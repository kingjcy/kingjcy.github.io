<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kingjcy blog </title>
    <link>https://kingjcy.github.io/</link>
    <language>en-us</language>
    <author></author>
    <rights>(C) 2021</rights>
    <updated>2021-02-04 20:22:22 &#43;0800 CST</updated>

    
      
        <item>
          <title>云计算K8s系列---- K8s autoscaler</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-autoscaler/</link>
          <pubDate>Thu, 04 Feb 2021 20:22:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-autoscaler/</guid>
          <description>&lt;p&gt;Pod 水平自动伸缩（Horizontal Pod Autoscaler）和垂直扩展（Vertical Pod Autoscaler）以及CA（ cluster-autoscaler）特性，可以说是很实用的特性，完全自动化实现了资源的充分利用，所以单独拿出来说说。&lt;/p&gt;

&lt;h1 id=&#34;hpa&#34;&gt;HPA&lt;/h1&gt;

&lt;p&gt;我们使用 kubectl scale 命令可以来实现 Pod 的扩缩容功能，但是这个毕竟是完全手动操作的，要应对线上的各种复杂情况，我们需要能够做到自动化去感知业务，来自动进行扩缩容。为此，Kubernetes 也为我们提供了这样的一个资源对象：Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称HPA&lt;/p&gt;

&lt;p&gt;HPA原来是k8s下面单独的一个项目，现在已经独立在github上了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/hpa&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Pod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。主要应用于无状态的服务的扩缩容，一般无状态的服务都是使用deployment来部署的，可以直接使用scale来指定副本的数量&amp;ndash;replicas的方式来完成扩缩容。pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets。&lt;/p&gt;

&lt;p&gt;Horizontal Pod Autoscaling，简称HPA, Kubernetes通过HPA的设定，实现了容器的弹性伸缩功能。对于Kubernetes中的POD集群来说，HPA可以实现很多自动化功能，比如当POD中业务负载上升的时候，可以创建新的POD来保证业务系统稳定运行，当POD中业务负载下降的时候，可以销毁POD来减少资源的浪费。当前的弹性伸缩的指标包括：CPU，内存，并发数，包传输大小。HPA控制器默认每隔15秒就会运行一次（Pod 水平自动伸缩的实现是一个控制循环，由 controller manager 的 &amp;ndash;horizontal-pod-autoscaler-sync-period 参数 指定周期（默认值为15秒）。），一旦创建的HPA，我们就可以通过命令查看获取到的当前指标信息。&lt;/p&gt;

&lt;p&gt;首先hpa要创建一个规则，就像我们之前创建ingress的规则一样，里面定义好一个扩容缩容的一个范围然后指定好对象，指定好它的预值，hpa本身就是一个控制器，循环的控制器，控制器将会不断从一系列的聚合 API（metrics.k8s.io、custom.metrics.k8s.io和external.metrics.k8s.io） 中获取指标数据。 metrics.k8s.io API 通常由 metrics-server（需要额外启动）提供。 可以从metrics-server 获取更多信息。，判断这个预值是不是到达你设置规则的预值，如果是的话，就会去执行这个scale帮你扩容这个副本，如果长期处于一个低使用率的情况下，它会帮你缩容这个副本，这个metrics server的资源来源是来自于cadvisor去拿的，想一下cadvisor可以提供那些指标，hpa可以拿到的，比如cpu,内存的使用率，主要采集你这些的利用率，所以hpa在早期已经支持了对CPU的弹性伸缩&lt;/p&gt;

&lt;p&gt;其实最早使用的不是metrics-server，而是heaspter，但是kubernetesv1.11以后不再支持通过heaspter采集监控数据，从 Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。替代方案如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server，metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。&lt;/li&gt;
&lt;li&gt;通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator。&lt;/li&gt;
&lt;li&gt;事件传输：使用第三方工具来传输、归档 kubernetes events。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;从最基本的角度来看，pod 水平自动缩放控制器跟据当前指标和期望指标来计算缩放比例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;期望副本数 = ceil[当前副本数 * ( 当前指标 / 期望指标 )]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如，当前指标为200m，目标设定值为100m,那么由于200.0 / 100.0 == 2.0， 副本数量将会翻倍。 如果当前指标为50m，副本数量将会减半，因为50.0 / 100.0 == 0.5。 如果计算出的缩放比例接近1.0（跟据&amp;ndash;horizontal-pod-autoscaler-tolerance 参数全局配置的容忍值，默认为0.1）， 将会放弃本次缩放。&lt;/p&gt;

&lt;p&gt;有一些规则说明一下&lt;/p&gt;

&lt;p&gt;1、如果 HorizontalPodAutoscaler 指定的是targetAverageValue 或 targetAverageUtilization， 那么将会把指定pod的平均指标做为currentMetricValue。 然而，在检查容忍度和决定最终缩放值前，我们仍然会把那些无法获取指标的pod统计进去。&lt;/p&gt;

&lt;p&gt;2、所有被标记了删除时间戳(Pod正在关闭过程中)的 pod 和 失败的 pod 都会被忽略。&lt;/p&gt;

&lt;p&gt;3、如果某个 pod 缺失指标信息，它将会被搁置，只在最终确定缩值时再考虑。&lt;/p&gt;

&lt;p&gt;4、当使用 CPU 指标来缩放时，任何还未就绪（例如还在初始化）状态的 pod 或 最近的指标为就绪状态前的 pod， 也会被搁置&lt;/p&gt;

&lt;p&gt;由于受技术限制，pod 水平缩放控制器无法准确的知道 pod 什么时候就绪， 也就无法决定是否暂时搁置该 pod。 &amp;ndash;horizontal-pod-autoscaler-initial-readiness-delay 参数（默认为30s），用于设置 pod 准备时间， 在此时间内的 pod 统统被认为未就绪。 &amp;ndash;horizontal-pod-autoscaler-cpu-initialization-period参数（默认为5分钟），用于设置 pod 的初始化时间， 在此时间内的 pod，CPU 资源指标将不会被采纳。&lt;/p&gt;

&lt;p&gt;5、在排除掉被搁置的 pod 后，缩放比例就会跟据currentMetricValue / desiredMetricValue计算出来。&lt;/p&gt;

&lt;p&gt;6、如果有任何 pod 的指标缺失，我们会更保守地重新计算平均值， 在需要缩小时假设这些 pod 消耗了目标值的 100%， 在需要放大时假设这些 pod 消耗了0%目标值。 这可以在一定程度上抑制伸缩的幅度。&lt;/p&gt;

&lt;p&gt;7、如果存在任何尚未就绪的pod，我们可以在不考虑遗漏指标或尚未就绪的pods的情况下进行伸缩， 我们保守地假设尚未就绪的pods消耗了试题指标的0%，从而进一步降低了伸缩的幅度。&lt;/p&gt;

&lt;p&gt;8、在缩放方向（缩小或放大）确定后，我们会把未就绪的 pod 和缺少指标的 pod 考虑进来再次计算使用率。 如果新的比率与缩放方向相反，或者在容忍范围内，则跳过缩放。 否则，我们使用新的缩放比例。&lt;/p&gt;

&lt;p&gt;注意，平均利用率的原始值会通过 HorizontalPodAutoscaler 的状态体现（ 即使使用了新的使用率，也不考虑未就绪 pod 和 缺少指标的 pod)。&lt;/p&gt;

&lt;p&gt;9、如果创建 HorizontalPodAutoscaler 时指定了多个指标， 那么会按照每个指标分别计算缩放副本数，取最大的进行缩放。 如果任何一个指标无法顺利的计算出缩放副本数（比如，通过 API 获取指标时出错）， 那么本次缩放会被跳过。&lt;/p&gt;

&lt;p&gt;10、在 HPA 控制器执行缩放操作之前，会记录缩放建议信息（scale recommendation）。 控制器会在操作时间窗口中考虑所有的建议信息，并从中选择得分最高的建议。 这个值可通过 kube-controller-manager 服务的启动参数 &amp;ndash;horizontal-pod-autoscaler-downscale-stabilization 进行配置， 默认值为 5min。 这个配置可以让系统更为平滑地进行缩容操作，从而消除短时间内指标值快速波动产生的影响。&lt;/p&gt;

&lt;h2 id=&#34;指标类型&#34;&gt;指标类型&lt;/h2&gt;

&lt;p&gt;hpa支持设置四种指标类型。&lt;/p&gt;

&lt;p&gt;1、资源度量指标，容器上指定资源的百分比，例如cpu使用率，在hap的资源配置清单中表现为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;type类型有两种&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、averageUtilization 百分比，比如cpu使用率
2、AverageValue  数值，比如内存
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、Pod 度量指标&lt;/p&gt;

&lt;p&gt;这些指标从某一方面描述了Pod，在不同Pod之间进行平均，并通过与一个目标值比对来确定副本的数量，它们的工作方式与资源度量指标非常相像。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type: Pods
pods:
  metric:
    name: packets-per-second
  target:
    type: AverageValue
    averageValue: 1k
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;仅支持target 类型为AverageValue。&lt;/p&gt;

&lt;p&gt;3、对象度量指标&lt;/p&gt;

&lt;p&gt;描述一个在相同名字空间(namespace)中的其他对象。 请注意这些度量指标用于描述这些对象，并非从对象中获取。 对象度量指标支持的target类型包括Value和AverageValue。如果是Value类型，target值将直接与API返回的度量指标比较， 而AverageValue类型，API返回的度量指标将按照 Pod 数量拆分，然后再与target值比较。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type: Object
object:
  metric:
    name: requests-per-second
  describedObject:
    apiVersion: networking.k8s.io/v1beta1
    kind: Ingress
    name: main-route
  target:
    type: Value
    value: 2k
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、外部的度量指标&lt;/p&gt;

&lt;p&gt;如果你的应用程序处理主机上的消息队列， 为了让每30个任务有1个worker，你可以将下面的内容添加到 HorizontalPodAutoscaler 的配置中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- type: External
  external:
    metric:
      name: queue_messages_ready
      selector: &amp;quot;queue=worker_tasks&amp;quot;
    target:
      type: AverageValue
      averageValue: 30
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;External metrics 同时支持Value和AverageValue类型&lt;/p&gt;

&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;

&lt;p&gt;上面的三种方案，我们获取数据都是访问对应的api，且每个api都是固定的，所以k8s会对直接到对应的api拉去指标，我们主要注册这个api并且指定对应的service服务去调用就可以了，当然监控服务还是要我们自己来部署的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、资源指标会使用 metrics.k8s.io API，一般由 metrics-server 提供。 它可以做为集群组件启动。
    esource metrics API 官方的说法是给 k8s 核心组件提供监控指标的，但是它只提供了 pod 和 node 的 CPU 和内存指标，功能实在有限。
    官方给出它可以做以下工作：
        HPA：CPU 指标可以拿来做 HPA。v1 版本的 HPA 也许依赖这个，现在已经无所谓了；
        pod 调度：官方的意思是这是个扩展的功能，因为现在的 pod 调度根本没有考虑到 node 的使用情况；
        集群联邦：同样是资源使用，但是现在没有使用；
        dashboard：出图，没用过 dashboard，也不知道是不是有效果；
        kubectl top：这算是最实用的功能吧。
2、用户指标会使用 custom.metrics.k8s.io API。 它由其他厂商的“适配器”API 服务器提供。 比如prometheus
3、外部指标会使用 external.metrics.k8s.io API。可能由上面的用户指标适配器提供。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;自Kubernetes 1.11版本起，K8s资源采集指标由Resource Metrics API（Metrics Server 实现）和Custom metrics api（Prometheus实现）两种API实现，传统Heapster监控被废弃。前者主要负责采集Node、Pod的核心资源数据，如内存、CPU等；而后者则主要负责自定义指标数据采集，如网卡流量，磁盘IOPS、HTTP请求数、数据库连接数等。&lt;/p&gt;

&lt;p&gt;所以这两种方式是合作的关系，但是其实promtheus能提供所有的指标，常规使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Core metrics(核心指标)：由metrics-server提供API，即 metrics.k8s.io，仅提供Node和Pod的CPU和内存使用情况。
Custom Metrics(自定义指标)：由Prometheus Adapter提供API，即 custom.metrics.k8s.io，由此可支持任意Prometheus采集到的自定义指标。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/hpa.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;metrics-server&#34;&gt;metrics-server&lt;/h3&gt;

&lt;p&gt;在 HPA 的第一个版本中，我们需要 Heapster 提供 CPU 和内存指标，在 HPA v2 过后就需要安装 Metrcis Server 了，Metrics Server 可以通过标准的 Kubernetes API 把监控数据暴露出来，有了 Metrics Server 之后，我们就完全可以通过标准的 Kubernetes API 来访问我们想要获取的监控数据了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://masterip/apis/metrics.k8s.io/v1beta1/namespaces/&amp;lt;namespace-name&amp;gt;/pods/&amp;lt;pod-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实就是声明了metrics.k8s.io的apiservice指向metrics-server。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/k8s-hpa&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过上图可以看出，数据是来自于 kubelet 的 Summary API 采集而来的，然后metrics server收集然后通过api对外提供，这边讲一下metrics server不是集成于apiserver，而是单独运行的，而上面的api是apiserver模式的，通过 Kubernetes 提供的 Aggregator 汇聚插件来实现的，Aggregator 允许开发人员编写一个自己的服务，把这个服务注册到 Kubernetes 的 APIServer 里面去，这样我们就可以像原生的 APIServer 提供的 API 使用自己的 API 了，然后 Kubernetes 的 Aggregator 通过 Service 名称就可以转发到我们自己写的 Service 里面去了。&lt;/p&gt;

&lt;p&gt;在这边简单提一下这种方式的好处，我自己在设计和开发代码的时候就是特别喜欢这种非侵入性的插件模式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;增加了 API 的扩展性，开发人员可以编写自己的 API 服务来暴露他们想要的 API。
丰富了 API，核心 kubernetes 团队阻止了很多新的 API 提案，通过允许开发人员将他们的 API 作为单独的服务公开，这样就无须社区繁杂的审查了。
开发分阶段实验性 API，新的 API 可以在单独的聚合服务中开发，当它稳定之后，在合并会 APIServer 就很容易了。
确保新 API 遵循 Kubernetes 约定，如果没有这里提出的机制，社区成员可能会被迫推出自己的东西，这样很可能造成社区成员和社区约定不一致。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要使用HPA，就需要先安装注册Metrics Server 服务，就需要开启 Aggregator，只要设计apiserver的启动参数就好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--requestheader-client-ca-file=&amp;lt;path to aggregator CA cert&amp;gt;
--requestheader-allowed-names=aggregator
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
--proxy-client-cert-file=&amp;lt;path to aggregator proxy cert&amp;gt;
--proxy-client-key-file=&amp;lt;path to aggregator proxy key&amp;gt;
--enable-aggregator-routing=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kubeadm 搭建的，默认已经开启了，minikube只要使用minikube addons enable metrics-server就好，二进制安装就要手动设置这些参数了。部署metrics server&lt;/p&gt;

&lt;p&gt;我们在minikube中直接用addons启动注册，我们来查看一下对应的情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get apiservices.apiregistration.k8s.io | grep metrics-server
v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        8d
MacBook-Pro:exercise chunyinjiang$ kubectl describe apiservices.apiregistration.k8s.io v1beta1.metrics.k8s.io
Name:         v1beta1.metrics.k8s.io
Namespace:
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              kubernetes.io/minikube-addons=metrics-server
Annotations:  API Version:  apiregistration.k8s.io/v1
Kind:         APIService
Metadata:
  Creation Timestamp:  2020-06-20T08:04:48Z
  Resource Version:    1088313
  Self Link:           /apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io
  UID:                 e870aa38-3091-4b62-967e-700823d7c215
Spec:
  Group:                     metrics.k8s.io
  Group Priority Minimum:    100
  Insecure Skip TLS Verify:  true
  Service:
    Name:            metrics-server
    Namespace:       kube-system
    Port:            443
  Version:           v1beta1
  Version Priority:  100
Status:
  Conditions:
    Last Transition Time:  2020-06-26T10:27:06Z
    Message:               all checks passed
    Reason:                Passed
    Status:                True
    Type:                  Available
Events:                    &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明已经注册上去了。然后k8s就会去/apis/metrics.k8s.io/v1beta1/拉去对应的指标。其实就是到对应的service：metrics-server拉去指标。&lt;/p&gt;

&lt;p&gt;Metrics Server 会通过 kubelet 的 10250 端口获取信息，使用的是 hostname，我们部署集群的时候在节点的 /etc/hosts 里面添加了节点的 hostname 和 ip 的映射，但是是我们的 Metrics Server 的 Pod 内部并没有这个 hosts 信息，当然也就不识别 hostname 了，要解决这个问题，有两种方法：&lt;/p&gt;

&lt;p&gt;1、DNS&lt;/p&gt;

&lt;p&gt;第一种方法就是在集群内部的 DNS 服务里面添加上 hostname 的解析，比如我们这里集群中使用的是 CoreDNS，我们就可以去修改下 CoreDNS 的 Configmap 信息，添加上 hosts 信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl edit configmap coredns -n kube-system
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health
        hosts {  # 添加集群节点hosts隐射信息
          10.151.30.11 ydzs-master
          10.151.30.57 ydzs-node3
          10.151.30.59 ydzs-node4
          10.151.30.22 ydzs-node1
          10.151.30.23 ydzs-node2
          fallthrough
        }
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           upstream
           fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        reload
    }
kind: ConfigMap
metadata:
  creationTimestamp: 2019-05-18T11:07:46Z
  name: coredns
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样当在集群内部访问集群的 hostname 的时候就可以解析到对应的 ip 了。&lt;/p&gt;

&lt;p&gt;2、指定ip&lt;/p&gt;

&lt;p&gt;另外一种方法就是在 metrics-server 的启动参数中修改 kubelet-preferred-address-types 参数，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;args:
- --cert-dir=/tmp
- --secure-port=4443
- --kubelet-insecure-tls #用于跳过验证
- --kubelet-preferred-address-types=InternalIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证metrics server是否安装成功，只要看有没有数据就行了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ kubectl top nodes
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
minikube   172m         8%     2969Mi          77%
MacBook-Pro:exercise chunyinjiang$ kubectl top pod
NAME                    CPU(cores)   MEMORY(bytes)
guestbook-clone-6dwl2   0m           1Mi
guestbook-clone-7dtxx   0m           1Mi
guestbook-clone-8tt9d   0m           1Mi
guestbook-clone-c2hhc   0m           1Mi
guestbook-clone-cbp59   0m           1Mi
guestbook-clone-dncmh   0m           1Mi
guestbook-clone-hl9jk   0m           1Mi
guestbook-clone-j2s28   0m           1Mi
guestbook-clone-j8jg9   0m           1Mi
guestbook-clone-j94s9   0m           1Mi
guestbook-clone-k9j5g   0m           1Mi
guestbook-clone-l52km   0m           1Mi
guestbook-clone-m6v7s   0m           1Mi
guestbook-clone-nvst8   0m           1Mi
guestbook-clone-q9dvt   0m           1Mi
guestbook-clone-rvjww   0m           1Mi
guestbook-clone-sdkg8   0m           1Mi
guestbook-clone-xjjq2   0m           1Mi
guestbook-clone-z4mwp   0m           1Mi
guestbook-clone-zcpkq   0m           1Mi
redis-master-fq8vp      0m           2Mi
redis-slave-srgsc       1m           2Mi
redis-slave-vglqg       0m           2Mi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见已经有数据返回了，就是成功了。&lt;/p&gt;

&lt;h4 id=&#34;基于cpu来实现hpa&#34;&gt;基于cpu来实现hpa&lt;/h4&gt;

&lt;p&gt;0、运行业务容器&lt;/p&gt;

&lt;p&gt;我们用 Deployment 来创建一个 Nginx Pod，资源清单如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpa-demo
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: 50Mi
            cpu: 50m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果要想让 HPA 生效，对应的 Pod 资源必须添加 requests 资源声明，创建deployment&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ kubectl apply -f nginx.yaml
deployment.apps/hpa-demo created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、生成HPA控制器&lt;/p&gt;

&lt;p&gt;生成了一个HPA的控制器，用于控制自动扩缩容，当deployment资源对象的CPU使用率达到10%时，就进行扩容，最多可以扩容到10个&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10
horizontalpodautoscaler.autoscaling/hpa-demo autoscaled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ kubectl get hpa
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   0%/10%    1         10        1          5m30s
MacBook-Pro:exercise chunyinjiang$ kubectl describe hpa hpa-demo
Name:                                                  hpa-demo
Namespace:                                             default
Labels:                                                &amp;lt;none&amp;gt;
Annotations:                                           &amp;lt;none&amp;gt;
CreationTimestamp:                                     Thu, 18 Jun 2020 16:58:16 +0800
Reference:                                             Deployment/hpa-demo
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (0) / 10%
Min replicas:                                          1
Max replicas:                                          10
Deployment pods:                                       1 current / 1 desired
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到hpa是正常的，如果在deployment中没有设置资源requset，就会影响计算，就会报错：failed to get cpu utilization: missing request for cpu&lt;/p&gt;

&lt;p&gt;再看看pod的数量，只有一个。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ kubectl get pod -o wide | grep hpa
hpa-demo-644d845b7f-bb4ww   1/1     Running   0          17m    172.17.0.33   minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、模拟消耗nginx的资源，并验证pod是否会自动扩容与缩容&lt;/p&gt;

&lt;p&gt;直接在宿主机器上执行一个死循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while true; do curl http://172.17.0.33 ; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认15s采集一次，每隔15S查看一下，可以很轻松的看出扩缩容，注意：当停止死循环请求后，也并不会立即减少pod数量，会等一段时间后减少pod数量，防止流量再次激增。至此，pod副本数量的自动扩缩容就实现了。扩容默认3m，所容默认持续5m&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:yaml chunyinjiang$ kubectl get hpa
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   38%/10%   1         10        4          19m
MacBook-Pro:yaml chunyinjiang$ kubectl get hpa
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   58%/10%   1         10        6          20m
MacBook-Pro:yaml chunyinjiang$ kubectl get hpa
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   0%/10%    1         10        10         23m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出最后并没有一下减少pod，等一会可以看到缩到1个了。基于内存实现自动扩缩容和cpu基本是一样，都是属于资源度量指标的设置。&lt;/p&gt;

&lt;h4 id=&#34;总结&#34;&gt;总结&lt;/h4&gt;

&lt;p&gt;默认就是使用metrics.k8s.io API来调用mertrics-server，因为metrics-server都是来源于kubelet集成的cadvisor，基本就是核心指标。你要是想用其他的两种API接口也是可以的，需要自己定义apiservice。&lt;/p&gt;

&lt;h3 id=&#34;生态监控系统&#34;&gt;生态监控系统&lt;/h3&gt;

&lt;p&gt;使用第三方监控系统，常规就是使用custom.metrics.k8s.io API来调用对应的服务。&lt;/p&gt;

&lt;h4 id=&#34;基于prometheus&#34;&gt;基于prometheus&lt;/h4&gt;

&lt;p&gt;Prometheus 用于监控应用和集群来获取应用负载和集群本身的各种指标，但是prometheus的指标并不能直接给k8s使用，所以k8s-prometheus-adapter 可以帮我们把 Prometheus 收集的指标转化为k8s可用的指标，我们只要注册custom.metrics.k8s.io API到k8s-prometheus-adapter 的service，使得HPA 资源对象也可以很轻易的直接使用。&lt;/p&gt;

&lt;p&gt;当HPA请求metrics时，kube-aggregator(apiservice的controller)会通过ustom.metrics.k8s.io API接口将请求转发到adapter，adapter作为kubernentes集群的pod，它会根据配置的rules从Prometheus抓取并处理metrics，在处理(如重命名metrics等)完后将metric通过custom metrics API返回给HPA。最后HPA通过获取的metrics的value对Deployment/ReplicaSet进行扩缩容。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/hpa2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们将 k8s-prometheus-adapter 安装到集群中，并添加一个规则来跟踪 Pod 的请求，这样我们就可以将 Prometheus 中的任何一个指标都用于 HPA，但是前提是你得通过查询语句将它拿到（包括指标名称和其对应的值）。&lt;/p&gt;

&lt;p&gt;1、部署prometheus，开启prometheus自动配置发现，我们直接使用&lt;a href=&#34;https://kingjcy.github.io/post/monitor/prometheus/prometheus-operater/#部署prometheus生态&#34;&gt;prometheus-operator&lt;/a&gt;就可以&lt;/p&gt;

&lt;p&gt;2、部署k8s-prometheus-adapter&lt;/p&gt;

&lt;p&gt;其实在&lt;a href=&#34;https://kingjcy.github.io/post/monitor/prometheus/prometheus-operater/#部署prometheus生态&#34;&gt;prometheus-operator&lt;/a&gt;中已经部署了k8s-prometheus-adapter，我们可以查看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get all -n monitoring | grep adapter
pod/prometheus-adapter-66b9c9dd58-6bdbm    1/1     Running   0          13h
service/prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      8d
deployment.apps/prometheus-adapter    1/1     1            1           8d
replicaset.apps/prometheus-adapter-5cdcdf9c8d    0         0         0       8d
replicaset.apps/prometheus-adapter-66b9c9dd58    1         1         1       13h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;k8s-prometheus-adapter是将prometheus的metrics数据格式转换成k8s API接口能识别的格式，同时通过apiservice扩展的模式注册到kube-apiserver来给k8s进行调用，我们查看对应的api-versions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl api-versions  | grep metrics
metrics.k8s.io/v1beta1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边只是声明了metrics.k8s.io这个api到prometheus-adapter去拉去核心指标，我们就可以重/apis/metrics.k8s.io/v1beta1这个URL来获取指标&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get --raw=&amp;quot;/apis/metrics.k8s.io/v1beta1&amp;quot;
{
    &amp;quot;kind&amp;quot;:&amp;quot;APIResourceList&amp;quot;,
    &amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,
    &amp;quot;groupVersion&amp;quot;:&amp;quot;metrics.k8s.io/v1beta1&amp;quot;,
    &amp;quot;resources&amp;quot;:[
        {
            &amp;quot;name&amp;quot;:&amp;quot;nodes&amp;quot;,
            &amp;quot;singularName&amp;quot;:&amp;quot;&amp;quot;,
            &amp;quot;namespaced&amp;quot;:false,
            &amp;quot;kind&amp;quot;:&amp;quot;NodeMetrics&amp;quot;,
            &amp;quot;verbs&amp;quot;:[
                &amp;quot;get&amp;quot;,
                &amp;quot;list&amp;quot;
            ]
        },
        {
            &amp;quot;name&amp;quot;:&amp;quot;pods&amp;quot;,
            &amp;quot;singularName&amp;quot;:&amp;quot;&amp;quot;,
            &amp;quot;namespaced&amp;quot;:true,
            &amp;quot;kind&amp;quot;:&amp;quot;PodMetrics&amp;quot;,
            &amp;quot;verbs&amp;quot;:[
                &amp;quot;get&amp;quot;,
                &amp;quot;list&amp;quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看出来这个接口主要获取了核心资源指标，比如nodes，pods，我们来看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get --raw &amp;quot;/apis/metrics.k8s.io/v1beta1/nodes&amp;quot;
{&amp;quot;kind&amp;quot;:&amp;quot;NodeMetricsList&amp;quot;,&amp;quot;apiVersion&amp;quot;:&amp;quot;metrics.k8s.io/v1beta1&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/nodes&amp;quot;},&amp;quot;items&amp;quot;:[]}
$ kubectl get --raw &amp;quot;/apis/metrics.k8s.io/v1beta1/pods&amp;quot;
{
    &amp;quot;kind&amp;quot;:&amp;quot;PodMetricsList&amp;quot;,
    &amp;quot;apiVersion&amp;quot;:&amp;quot;metrics.k8s.io/v1beta1&amp;quot;,
    &amp;quot;metadata&amp;quot;:{
        &amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/pods&amp;quot;
    },
    &amp;quot;items&amp;quot;:[
        {
            &amp;quot;metadata&amp;quot;:{
                &amp;quot;name&amp;quot;:&amp;quot;kube-state-metrics-957fd6c75-wlnm4&amp;quot;,
                &amp;quot;namespace&amp;quot;:&amp;quot;monitoring&amp;quot;,
                &amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/namespaces/monitoring/pods/kube-state-metrics-957fd6c75-wlnm4&amp;quot;,
                &amp;quot;creationTimestamp&amp;quot;:&amp;quot;2020-07-01T03:37:27Z&amp;quot;
            },
            &amp;quot;timestamp&amp;quot;:&amp;quot;2020-07-01T03:37:27Z&amp;quot;,
            &amp;quot;window&amp;quot;:&amp;quot;0s&amp;quot;,
            &amp;quot;containers&amp;quot;:[
                {
                    &amp;quot;name&amp;quot;:&amp;quot;&amp;quot;,
                    &amp;quot;usage&amp;quot;:{
                        &amp;quot;cpu&amp;quot;:&amp;quot;1m&amp;quot;,
                        &amp;quot;memory&amp;quot;:&amp;quot;49112Ki&amp;quot;
                    }
                }
            ]
        },
        {
            &amp;quot;metadata&amp;quot;:{
                &amp;quot;name&amp;quot;:&amp;quot;alertmanager-main-1&amp;quot;,
                &amp;quot;namespace&amp;quot;:&amp;quot;monitoring&amp;quot;,
                &amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/namespaces/monitoring/pods/alertmanager-main-1&amp;quot;,
                &amp;quot;creationTimestamp&amp;quot;:&amp;quot;2020-07-01T03:37:27Z&amp;quot;
            },
            &amp;quot;timestamp&amp;quot;:&amp;quot;2020-07-01T03:37:27Z&amp;quot;,
            &amp;quot;window&amp;quot;:&amp;quot;0s&amp;quot;,
            &amp;quot;containers&amp;quot;:[
                {
                    &amp;quot;name&amp;quot;:&amp;quot;&amp;quot;,
                    &amp;quot;usage&amp;quot;:{
                        &amp;quot;cpu&amp;quot;:&amp;quot;5m&amp;quot;,
                        &amp;quot;memory&amp;quot;:&amp;quot;32664Ki&amp;quot;
                    }
                }
            ]
        },
        {
            &amp;quot;metadata&amp;quot;:{
                &amp;quot;name&amp;quot;:&amp;quot;storage-provisioner&amp;quot;,
                &amp;quot;namespace&amp;quot;:&amp;quot;kube-system&amp;quot;,
                &amp;quot;selfLink&amp;quot;:&amp;quot;/apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/storage-provisioner&amp;quot;,
                &amp;quot;creationTimestamp&amp;quot;:&amp;quot;2020-07-01T03:37:27Z&amp;quot;
            },
            &amp;quot;timestamp&amp;quot;:&amp;quot;2020-07-01T03:37:27Z&amp;quot;,
            &amp;quot;window&amp;quot;:&amp;quot;0s&amp;quot;,
            &amp;quot;containers&amp;quot;:[
                {
                    &amp;quot;name&amp;quot;:&amp;quot;&amp;quot;,
                    &amp;quot;usage&amp;quot;:{
                        &amp;quot;cpu&amp;quot;:&amp;quot;0&amp;quot;,
                        &amp;quot;memory&amp;quot;:&amp;quot;30208Ki&amp;quot;
                    }
                }
            ]
        },
        。。。。。//很多数据，不展示了。
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么如果我们需要自定义一下其他的指标，我们就需要使用的custom.metrics.k8s.io API，我们就需要部署k8s-prometheus-adapter中的deploy，当然这个项目是在新的namespaces中创建新的相关配置，可能会存在冲突，可以在github上找到相关资源配置清单。&lt;/p&gt;

&lt;p&gt;我们直接查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get apiservice | grep monitoring/prometheus-adapter
v1beta1.custom.metrics.k8s.io          monitoring/prometheus-adapter   True        4m10s
v1beta1.metrics.k8s.io                 monitoring/prometheus-adapter   True        4m10s
$ kubectl api-versions | grep metrics
custom.metrics.k8s.io/v1beta1
metrics.k8s.io/v1beta1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和metrics.k8s.io/v1beta1一样，自定义接口api也是到k8s-prometheus-adapter去拉去数据。我们可以直接访问这个对应的接口获取数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get --raw &amp;quot;/apis/custom.metrics.k8s.io/v1beta1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以查到所有的在prometheus-adapter的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/prometheus/prometheus-operater/#adapter&#34;&gt;配置文件&lt;/a&gt;中指定查询的指标，如果只是查询相关的指标，可以访问 /apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/*/fs_usage_bytes 则是通过 metricsQuery 进行查询，从而获取每个 pod 的指标值。&lt;/p&gt;

&lt;p&gt;很多人会使用 metrics server 提供 resource metrics API，然后使用 Prometheus adapter 提供 custom metrics API。但是其实 Prometheus adapter 完全可以支持这两种 api，因此我们完全不需要 metrics server，只部署一个 Prometheus adapter 就行。我们上面就是这么部署的。&lt;/p&gt;

&lt;h3 id=&#34;实例&#34;&gt;实例&lt;/h3&gt;

&lt;p&gt;下面我们以nginx的请求总数为指标来做扩缩容&lt;/p&gt;

&lt;p&gt;1、先创建nginx服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpa-prom-demo
spec:
  selector:
    matchLabels:
      app: nginx-server
  template:
    metadata:
      labels:
        app: nginx-server
    spec:
      containers:
      - name: nginx-demo
        image: cnych/nginx-vts:v1.0
        resources:
          limits:
            cpu: 50m
          requests:
            cpu: 50m
        ports:
        - containerPort: 80
          name: http
---
apiVersion: v1
kind: Service
metadata:
  name: hpa-prom-demo
  annotations:
    prometheus.io/scrape: &amp;quot;true&amp;quot;
    prometheus.io/port: &amp;quot;80&amp;quot;
    prometheus.io/path: &amp;quot;/status/format/prometheus&amp;quot;
spec:
  ports:
  - port: 80
    targetPort: 80
    name: http
  selector:
    app: nginx-server
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我们部署的应用是在 80 端口的 /status/format/prometheus 这个端点暴露 nginx-vts 指标的，我们已经在 Prometheus 中配置了 Endpoints 的自动发现，所以我们直接在 Service 对象的 annotations 中进行配置，这样我们就可以在 Prometheus 中采集该指标数据了。&lt;/p&gt;

&lt;p&gt;2、配置hpa指标的查询配置在adapter中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rules:
- seriesQuery: &#39;nginx_vts_server_requests_total&#39;
  seriesFilters: []
  resources:
    overrides:
      kubernetes_namespace:
        resource: namespace
      kubernetes_pod_name:
        resource: pod
  name:
    matches: &amp;quot;^(.*)_total&amp;quot;
    as: &amp;quot;${1}_per_second&amp;quot;
  metricsQuery: (sum(rate(&amp;lt;&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}[1m])) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们就可以通过custom api来获取指标了，我们可以通过命令行查询一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get --raw &amp;quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/nginx_vts_server_requests_per_second&amp;quot; | jq .
{
  &amp;quot;kind&amp;quot;: &amp;quot;MetricValueList&amp;quot;,
  &amp;quot;apiVersion&amp;quot;: &amp;quot;custom.metrics.k8s.io/v1beta1&amp;quot;,
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;selfLink&amp;quot;: &amp;quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/nginx_vts_server_requests_per_second&amp;quot;
  },
  &amp;quot;items&amp;quot;: [
    {
      &amp;quot;describedObject&amp;quot;: {
        &amp;quot;kind&amp;quot;: &amp;quot;Pod&amp;quot;,
        &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;,
        &amp;quot;name&amp;quot;: &amp;quot;hpa-prom-demo-755bb56f85-lvksr&amp;quot;,
        &amp;quot;apiVersion&amp;quot;: &amp;quot;/v1&amp;quot;
      },
      &amp;quot;metricName&amp;quot;: &amp;quot;nginx_vts_server_requests_per_second&amp;quot;,
      &amp;quot;timestamp&amp;quot;: &amp;quot;2020-04-07T09:45:45Z&amp;quot;,
      &amp;quot;value&amp;quot;: &amp;quot;527m&amp;quot;,
      &amp;quot;selector&amp;quot;: null
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、部署hpa&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-custom-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hpa-prom-demo
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Pods
    pods:
      metricName: nginx_vts_server_requests_per_second
      targetAverageValue: 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f hpa-prome.yaml
horizontalpodautoscaler.autoscaling/nginx-custom-hpa created
$ kubectl describe hpa nginx-custom-hpa
Name:                                              nginx-custom-hpa
Namespace:                                         default
Labels:                                            &amp;lt;none&amp;gt;
Annotations:                                       kubectl.kubernetes.io/last-applied-configuration:
                                                     {&amp;quot;apiVersion&amp;quot;:&amp;quot;autoscaling/v2beta1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;HorizontalPodAutoscaler&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;nginx-custom-hpa&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;d...
CreationTimestamp:                                 Tue, 07 Apr 2020 17:54:55 +0800
Reference:                                         Deployment/hpa-prom-demo
Metrics:                                           ( current / target )
  &amp;quot;nginx_vts_server_requests_per_second&amp;quot; on pods:  &amp;lt;unknown&amp;gt; / 10
Min replicas:                                      2
Max replicas:                                      5
Deployment pods:                                   1 current / 2 desired
Conditions:
  Type         Status  Reason            Message
  ----         ------  ------            -------
  AbleToScale  True    SucceededRescale  the HPA controller was able to update the target scale to 2
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  7s    horizontal-pod-autoscaler  New size: 2; reason: Current number of replicas below Spec.MinReplicas
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、对应用进行压测&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while true; do wget -q -O- http://ip:port; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开另外一个终端观察 HPA 对象的变化：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get hpa
NAME               REFERENCE                  TARGETS     MINPODS   MAXPODS   REPLICAS   AGE
nginx-custom-hpa   Deployment/hpa-prom-demo   14239m/10   2         5         2          4m27s
$ kubectl describe hpa nginx-custom-hpa
Name:                                              nginx-custom-hpa
Namespace:                                         default
Labels:                                            &amp;lt;none&amp;gt;
Annotations:                                       kubectl.kubernetes.io/last-applied-configuration:
                                                     {&amp;quot;apiVersion&amp;quot;:&amp;quot;autoscaling/v2beta1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;HorizontalPodAutoscaler&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;nginx-custom-hpa&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;d...
CreationTimestamp:                                 Tue, 07 Apr 2020 17:54:55 +0800
Reference:                                         Deployment/hpa-prom-demo
Metrics:                                           ( current / target )
  &amp;quot;nginx_vts_server_requests_per_second&amp;quot; on pods:  14308m / 10
Min replicas:                                      2
Max replicas:                                      5
Deployment pods:                                   3 current / 3 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from pods metric nginx_vts_server_requests_per_second
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  5m2s  horizontal-pod-autoscaler  New size: 2; reason: Current number of replicas below Spec.MinReplicas
  Normal  SuccessfulRescale  61s   horizontal-pod-autoscaler  New size: 3; reason: pods metric nginx_vts_server_requests_per_second above target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到指标 nginx_vts_server_requests_per_second 的数据已经超过阈值了，触发扩容动作了，副本数变成了3，扩缩容和上面是一样的，就不多说了。&lt;/p&gt;

&lt;p&gt;到这里我们就完成了使用自定义的指标对应用进行自动扩缩容的操作。如果 Prometheus 安装在我们的 Kubernetes 集群之外，则只需要确保可以从集群访问到查询的端点，并在 adapter 的部署清单中对其进行更新即可。在更复杂的场景中，可以获取多个指标结合使用来制定扩展策略。&lt;/p&gt;

&lt;h1 id=&#34;vpa&#34;&gt;VPA&lt;/h1&gt;

&lt;p&gt;VPA 全称 Vertical Pod Autoscaler，即垂直 Pod 自动扩缩容，可以根据容器资源使用情况自动设置 CPU 和 内存 的request值，从而允许在节点上进行适当的调度，以便为每个 Pod 提供适当的资源。它既可以缩小过度请求资源的容器，也可以根据其使用情况随时提升资源不足的容量。注意：VPA 不会改变 Pod 的资源limit值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、因为 Pod 完全用其所需，所以集群节点使用效率高。
2、Pod 会被安排到具有适当可用资源的节点上。
3、不必运行耗时的基准测试任务来确定 CPU 和内存请求的合适值。
4、VPA 可以随时调整 CPU 和内存请求，而无需执行任何操作，因此可以减少维护时间。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;架构图&#34;&gt;架构图&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/vpa.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/vpa&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/vpa1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;原理-1&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;VPA 主要包含三个组件：&lt;/p&gt;

&lt;p&gt;1、Admission Controller 会拦截所有 Pod 的创建请求，如果 Pod 和某个 VPA 匹配且该 VPA 的更新策略不为 Off，Admission Controller 会使用推荐值修改 Pod 的资源请求，否则不会修改。&lt;/p&gt;

&lt;p&gt;Admission Controller 从 Recommender 获取资源的推荐值，如果获取超时或失败，则会使用缓存在对应 VPA 中的推荐值，如果这个推荐值也无法获取，则使用 Pod 指定的请求值。&lt;/p&gt;

&lt;p&gt;2、Recommender 负责计算推荐资源，该组件启动时会获取所有 Pod 的历史资源利用率（无论是否使用了VPA），以及历史存储（如Promethues，通过参数配置）中的 Pod OOM 事件的历史记录，然后聚合这些数据并存储在内存中。&lt;/p&gt;

&lt;p&gt;Recommender 会监听集群中的所有 Pod 和 VPA ，对于和某个 VPA 匹配的Pod，它会计算推荐的资源并在对应 VPA 中设置推荐值。&lt;/p&gt;

&lt;p&gt;3、Updater 监听集群中的所有 Pod 和 VPA，通过调用 Recommender API 定期获取 VPA 中的推荐值，当一个 Pod 的推荐资源与实际配置的资源相差较大时，Updater 会驱逐这个 Pod（注意：Updater并不负责 Pod 资源的更新），Pod 被其控制器重新创建时，Admission Controller 会拦截这个创建请求，并使用推荐值修改请求值，然后 Pod 使用推荐值被创建。&lt;/p&gt;

&lt;p&gt;VPA 有以下四种更新策略：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、Initial：仅在 Pod 创建时修改资源请求，以后都不再修改。
2、Auto：默认策略，在 Pod 创建时修改资源请求，并且在 Pod 更新时也会修改。
3、Recreate：类似 Auto，在 Pod 的创建和更新时都会修改资源请求，不同的是，只要Pod 中的请求值与新的推荐值不同，VPA 都会驱逐该 Pod，然后使用新的推荐值重新启一个。因此，一般不使用该策略，而是使用 Auto，除非你真的需要保证请求值是最新的推荐值。
4、Off：不改变 Pod 的资源请求，不过仍然会在 VPA 中设置资源的推荐值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;若要禁止 VPA 修改 Pod 的请求资源，有以下三种方式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、将 VPA 的更新策略改为 Off
2、删除 VPA
3、去除 Pod 的 label，使其不再被 VPA 匹配到
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实现-1&#34;&gt;实现&lt;/h2&gt;

&lt;p&gt;首先肯定要开启apiservice，来提供获取指标的api&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl api-versions | grep admissionregistration
admissionregistration.k8s.io/v1beta1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubernetes 版本从 1.9 开始，MutatingAdmissionWebhooks是默认启用的。默认应该也是重metrics-server中获取指标的，如果需要支持其他的监控生态，可以参考hpa。&lt;/p&gt;

&lt;p&gt;然后vpa的组件是要部署的，所以我们要部署vpa&lt;/p&gt;

&lt;p&gt;VPA 目前有两个版本，分别是 0.2.x和 0.3.x，0.2.x被称为 alpha版，0.3.x被称为 beta版，apiVersion也从 poc.autoscaling.k8s.io/v1alpha1 变为了 &lt;code&gt;autoscaling.k8s.io/v1beta1&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;安装步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/kubernetes/autoscaler.git
$ cd autoscaler/vertical-pod-autoscaler
$ ./hack/vpa-up.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vpa-up.sh 脚本会读取当前的环境变量：$REGISTRY 和 $TAG，分别是镜像仓库地址和镜像版本，默认分别是 k8s.gcr.io和 0.3.1。由于网络的原因，我们无法拉取k8s.gcr.io的镜像，因此建议修改 $REGISTRY为国内可访问的镜像仓库地址。&lt;/p&gt;

&lt;p&gt;若已经安装了 alpha版本的 VPA，想要升级到 beta版本，最安全的方法是通过 vpa-down.sh脚本删除老版本，然后通过 vpa-up.sh脚本安装新版本。&lt;/p&gt;

&lt;p&gt;若没有修改镜像地址，执行 vpa-up.sh脚本后，主要是获取下面三个镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;k8s.gcr.io/vpa-recommender:0.3.1
k8s.gcr.io/vpa-updater:0.3.1
k8s.gcr.io/vpa-admission-controller:0.3.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;实例-1&#34;&gt;实例&lt;/h3&gt;

&lt;p&gt;off策略：仅获取资源推荐不更新Pod&lt;/p&gt;

&lt;p&gt;1、创建 Deployment&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-rec-deployment
  labels:
    purpose: try-recommend
spec:
  replicas: 2
  template:
    metadata:
      labels:
        purpose: try-recommend
    spec:
      containers:
      - name: my-rec-container
        image: nginx:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、创建 VPA&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling.k8s.io/v1beta1
kind: VerticalPodAutoscaler
metadata:
  name: my-rec-vpa
spec:
  selector:
    matchLabels:
      purpose: try-recommend
  updatePolicy:
    updateMode: &amp;quot;Off&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、查看该 VPA 的详细信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
  recommendation:
    containerRecommendations:
    - containerName: my-rec-container
      lowerBound:
        cpu: 25m
        memory: 262144k
      target:
        cpu: 25m
        memory: 262144k
      upperBound:
        cpu: 25m
        memory: 262144k
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中lowerBound 、target、upperBound 分别表示 下限值、推荐值、上限值，上述结果表明，推荐的 Pod 的 CPU 请求为 25m，推荐的内存请求为 262144k 字节。&lt;/p&gt;

&lt;p&gt;VPA 使用 lowerBound 和 upperBound 来决定是否删除 Pod 并使用推荐值重新创建。如果 Pod 的请求小于下限或大于上限，则 VPA 将删除 Pod 并重新创建。&lt;/p&gt;

&lt;p&gt;auto策略：自动更新Pod资源请求&lt;/p&gt;

&lt;p&gt;1、创建deployment&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: my-deployment
labels:
  purpose: try-auto-requests
spec:
replicas: 2
template:
  metadata:
    labels:
      purpose: try-auto-requests
  spec:
    containers:
    - name: my-container
      image: alpine:latest
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      command: [&amp;quot;/bin/sh&amp;quot;]
      args: [&amp;quot;-c&amp;quot;, &amp;quot;while true; do timeout 0.5s yes &amp;gt;/dev/null; sleep 0.5s; done&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、创建vpa&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling.k8s.io/v1beta1
kind: VerticalPodAutoscaler
metadata:
name: my-vpa
spec:
selector:
  matchLabels:
    purpose: try-auto-requests
updatePolicy:
  updateMode: &amp;quot;Auto&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、获取该 VPA 的详细信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  recommendation:
    containerRecommendations:
    - containerName: my-container
      lowerBound:
        cpu: 25m
        memory: 262144k
      target:
        cpu: 35m
        memory: 262144k
      upperBound:
        cpu: 117m
        memory: 262144k
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、查看pod，pod发生了重启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod -w|grep my-deployment
my-deployment-79f7977c8-hrnt4        1/1       Running             0          44s
my-deployment-79f7977c8-r27kk        1/1       Running             0          44s
my-deployment-79f7977c8-r27kk   1/1       Terminating   0         2m
my-deployment-79f7977c8-r27kk   1/1       Terminating   0         2m
my-deployment-79f7977c8-29kl9   0/1       Pending   0         0s
my-deployment-79f7977c8-29kl9   0/1       Pending   0         1s
my-deployment-79f7977c8-29kl9   0/1       ContainerCreating   0         1s
my-deployment-79f7977c8-29kl9   1/1       Running   0         20s
my-deployment-79f7977c8-hrnt4   1/1       Terminating   0         3m
my-deployment-79f7977c8-hrnt4   1/1       Terminating   0         3m
my-deployment-79f7977c8-558bg   0/1       Pending   0         0s
my-deployment-79f7977c8-558bg   0/1       Pending   0         0s
my-deployment-79f7977c8-558bg   0/1       ContainerCreating   0         1s
my-deployment-79f7977c8-558bg   1/1       Running   0         16s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、查看重启后的pod的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  annotations:
    vpaUpdates: &#39;Pod resources updated by my-vpa: container 0: cpu request, memory request&#39;
spec:
    ...
    resources:
      requests:
        cpu: 35m
        memory: 262144k
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，Pod 的 CPU 和 内存 请求都已经改变，请求值就是 VPA 中 target，而且 Pod 的annotations也多了一行 vpaUpdates，表明该 Pod 是由 VPA 更新的。&lt;/p&gt;

&lt;p&gt;需要使用 VPA 的 Pod 必须属于副本集，比如属于 Deployment 或 StatefulSet，这样才能保证 Pod 被驱逐后能自动重启，也就是说部署了 Pod 类型的应用后，VPA 无法更新其资源请求，但 VPA 对象中仍然会显示推荐的资源，这时只能手动删除 Pod，然后重新创建，VPA Admission Controller拦截后才能更改 Pod 的请求值。&lt;/p&gt;

&lt;h1 id=&#34;ca&#34;&gt;CA&lt;/h1&gt;

&lt;p&gt;CA（ cluster-autoscaler）是用来弹性伸缩kubernetes集群的，cluster-autoscaler可以自动的根据部署的应用所请求的资源量来动态的伸缩集群，从而来保持集群合适的大小。&lt;/p&gt;

&lt;h2 id=&#34;架构&#34;&gt;架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/ca.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/ca&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;原理-2&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;ca主要是由以下几个核心组件完成的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、autoscaler：核心模块，负责整体扩缩容功能
2、Estimator：负责评估计算扩容节点
3、Simulator：负责模拟调度，计算缩容节点
4、CA Cloud-Provider：与云交互进行节点的增删操作。社区目前仅支持AWS和GCE，其他云厂商需要自己实现CloudProvider和NodeGroup相关接口。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实现-2&#34;&gt;实现&lt;/h2&gt;

&lt;p&gt;1、扩容： 由于资源不足，pod调度失败，即有pod一直处于Pending状态。&lt;/p&gt;

&lt;p&gt;2、缩容：node的资源利用率较低时，且此node上存在的pod都能被重新调度到其他node上运行。&lt;/p&gt;

&lt;p&gt;什么样的节点不会被CA删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;节点上有pod被PodDisruptionBudget控制器限制。
节点上有命名空间是kube-system的pods。
节点上的pod不是被控制器创建，例如不是被deployment, replica set, job, stateful set创建。
节点上有pod使用了本地存储
节点上pod驱逐后无处可去，即没有其他node能调度这个pod
节点有注解：”cluster-autoscaler.kubernetes.io/scale-down-disabled”: “true”，可以通过给节点打上特定注解保证节点不给CA删除：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CA与HPA协同工作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HPA（Horizontal Pod Autoscaling）是k8s中pod的水平自动扩展，HPA的操作对象是RC、RS或Deployment对应的Pod，根据观察到的CPU等实际使用量与用户的期望值进行比对，做出是否需要增减实例数量的决策。
当CPU负载增加，HPA扩容pod，如果此pod因为资源不足无法被调度，则此时CA出马扩容节点。
当CPU负载减小，HPA减少pod，CA发现有节点资源利用率低甚至已经是空时，CA就会删除此节点。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;部署使用CA&lt;/p&gt;

&lt;p&gt;直接在集群中部署即可，简化的yaml如下所示，启动参数按需添加，其中{{MIN}}是最小节点数，{{MAX}}是最大节点数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cluster-autoscaler
  labels:
    k8s-app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: cluster-autoscaler
  template:
    metadata:
      labels:
        k8s-app: cluster-autoscaler
    spec:
      containers:
        - image: cluster-autoscaler:latest
          name: cluster-autoscaler
          command:
            - ./cluster-autoscaler
            - --nodes={{MIN}}:{{MAX}}:k8s-worker-asg-1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;总结-1&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;目前 Kubernetes 的 Pod 水平自动伸缩（HPA，Horizontal Pod Autoscaler）已在业界广泛应用。但对一些特殊的 Pod（如一些有状态的 Pod），HPA 并不能很好地解决资源不足的问题。 这就引出 Pod 垂直自动伸缩（VPA，Vertical Pod Autoscaler）。目前在实际使用中，hpa使用比较多，vpa和ca也有使用，只是使用比较少。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- kruise</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-kruise/</link>
          <pubDate>Sun, 17 Jan 2021 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-kruise/</guid>
          <description>&lt;p&gt;OpenKruise 是阿里云开源的大规模应用自动化管理引擎，在功能上对标了 Kubernetes 原生的 Deployment / StatefulSet 等控制器，但 OpenKruise 提供了更多的增强功能如：优雅原地升级、发布优先级/打散策略、多可用区workload抽象管理、统一 sidecar 容器注入管理等，都是经历了阿里巴巴超大规模应用场景打磨出的核心能力。这些 feature 帮助我们应对更加多样化的部署环境和需求、为集群维护者和应用开发者带来更加灵活的部署发布组合策略。&lt;/p&gt;

&lt;h1 id=&#34;crd&#34;&gt;CRD&lt;/h1&gt;

&lt;p&gt;Kruise 是 cruise的谐音，&amp;rsquo;k&amp;rsquo; for Kubernetes. 字面意义巡航，豪华游艇。寓意Kubernetes上应用的自动巡航，满载阿里巴巴多年应用部署管理经验。&lt;/p&gt;

&lt;p&gt;Kruise 提供了以下 5 个 workload 控制器：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CloneSet: 提供了更加高效、确定可控的应用管理和部署能力，支持优雅原地升级、指定删除、发布顺序可配置、并行/灰度发布等丰富的策略，可以满足更多样化的应用场景。&lt;/li&gt;
&lt;li&gt;Advanced StatefulSet: 基于原生 StatefulSet 之上的增强版本，默认行为与原生完全一致，在此之外提供了原地升级、并行发布（最大不可用）、发布暂停等功能。&lt;/li&gt;
&lt;li&gt;SidecarSet: 对 sidecar 容器做统一管理，在满足 selector 条件的 Pod 中注入指定的 sidecar 容器。&lt;/li&gt;
&lt;li&gt;UnitedDeployment: 通过多个 subset workload 将应用部署到多个可用区。&lt;/li&gt;
&lt;li&gt;BroadcastJob: 配置一个 job，在集群中所有满足条件的 Node 上都跑一个 Pod 任务。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;h2 id=&#34;前置检查&#34;&gt;前置检查&lt;/h2&gt;

&lt;p&gt;使用 Kruise 需要在 kube-apiserver 启用一些 feature-gate 比如 MutatingAdmissionWebhook、ValidatingAdmissionWebhook （K8s 1.12以上默认开启）。 如果你的 K8s 版本低于 1.12，需要先执行以下命令来验证是否支持：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sh -c &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/openkruise/kruise/master/scripts/check_for_installation.sh)&amp;quot;（源文件在代码的scripts目录下）
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;helm安装&#34;&gt;helm安装&lt;/h2&gt;

&lt;p&gt;推荐使用 helm v3 安装 Kruise，helm 是一个简单的命令行工具可以从&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/CDCI/helm&#34;&gt;这里&lt;/a&gt;了解。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install kruise https://github.com/openkruise/kruise/releases/download/v0.5.0/kruise-chart.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意直接安装 chart 会使用默认的 template values，你也可以根据你的集群情况指定一些特殊配置，比如修改 resources 限制或者只启用某些特定的控制器能力。&lt;/p&gt;

&lt;h2 id=&#34;yaml安装&#34;&gt;yaml安装&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# Install CRDs（源文件在代码的config目录下）
kubectl apply -f https://raw.githubusercontent.com/kruiseio/kruise/master/config/crds/apps_v1alpha1_broadcastjob.yaml
kubectl apply -f https://raw.githubusercontent.com/kruiseio/kruise/master/config/crds/apps_v1alpha1_sidecarset.yaml
kubectl apply -f https://raw.githubusercontent.com/kruiseio/kruise/master/config/crds/apps_v1alpha1_statefulset.yaml
kubectl apply -f https://raw.githubusercontent.com/kruiseio/kruise/master/config/crds/apps_v1alpha1_uniteddeployment.yaml
kubectl apply -f https://raw.githubusercontent.com/kruiseio/kruise/master/config/crds/apps_v1alpha1_cloneset.yaml

# Install kruise-controller-manager
kubectl apply -f https://raw.githubusercontent.com/kruiseio/kruise/master/config/manager/all_in_one.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意 all_in_one.yaml 中包含的 Kruise-manager 镜像是每天周期性从 master 分支打出来的，无法保证功能的稳定性。 所以你可以通过 YAML 部署到测试集群做验证，但不推荐在生产环境使用。&lt;/p&gt;

&lt;h2 id=&#34;部分安装&#34;&gt;部分安装&lt;/h2&gt;

&lt;p&gt;如果你只需要使用某些 Kruise 中的控制器并关闭其他的控制器，你可以做以下两个方式或同时做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、只安装你需要使用的 CRD。

2、在 kruise-manager 容器中设置 CUSTOM_RESOURCE_ENABLE 环境变量，配置需要启用的功能，比如 CUSTOM_RESOURCE_ENABLE=CloneSet,StatefulSet。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果使用 helm chart 安装，可以通过以下参数来生效这个配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install kruise https://github.com/openkruise/kruise/releases/download/v0.5.0/kruise-chart.tgz --set manager.custom_resou
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;卸载&#34;&gt;卸载&lt;/h1&gt;

&lt;p&gt;卸载会导致所有 Kruise 下的资源都会删除掉，包括 webhook configurations, services, namespace, CRDs, CR instances 以及所有 Kruise workload 下的 Pod。 请务必谨慎操作！&lt;/p&gt;

&lt;p&gt;卸载使用 helm chart 安装的 Kruise：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm uninstall kruise
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;卸载使用 YAML files 安装的 Kruise:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sh -c &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/kruiseio/kruise/master/scripts/uninstall.sh)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;h2 id=&#34;cloneset&#34;&gt;cloneset&lt;/h2&gt;

&lt;p&gt;使用cloneset来创建Guestbook应用&lt;/p&gt;

&lt;p&gt;1、安装redis&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# install Redis by CloneSet
$ kubectl apply -f https://raw.githubusercontent.com/kruiseio/kruise/master/docs/tutorial/v1/cloneset-redis.yaml
service/redis-master created
service/redis-slave created
cloneset.apps.kruise.io/redis-master created
cloneset.apps.kruise.io/redis-slave created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、安装Guestbook&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# install Guestbook by CloneSet
$ kubectl apply -f https://raw.githubusercontent.com/kruiseio/kruise/master/docs/tutorial/v1/cloneset-guestbook.yaml
service/guestbook-clone-svc created
cloneset.apps.kruise.io/guestbook-clone created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、查看创建的cloneset&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:yaml chunyinjiang$ kubectl get pod
NAME                    READY   STATUS    RESTARTS   AGE
guestbook-clone-2rrv5   1/1     Running   0          17s
guestbook-clone-6dwl2   1/1     Running   0          17s
guestbook-clone-c2hhc   1/1     Running   0          17s
guestbook-clone-cbp59   1/1     Running   0          17s
guestbook-clone-dncmh   1/1     Running   0          17s
guestbook-clone-hl9jk   1/1     Running   0          17s
guestbook-clone-j94s9   1/1     Running   0          16s
guestbook-clone-k9j5g   1/1     Running   0          16s
guestbook-clone-nvst8   1/1     Running   0          16s
guestbook-clone-p9k65   1/1     Running   0          16s
guestbook-clone-p9tf8   1/1     Running   0          16s
guestbook-clone-q9dvt   1/1     Running   0          16s
guestbook-clone-qfbx6   1/1     Running   0          16s
guestbook-clone-rvjww   1/1     Running   0          16s
guestbook-clone-w675b   1/1     Running   0          16s
guestbook-clone-whgf4   1/1     Running   0          16s
guestbook-clone-x7gg7   1/1     Running   0          16s
guestbook-clone-xrg9p   1/1     Running   0          16s
guestbook-clone-xxwmn   1/1     Running   0          16s
guestbook-clone-xzxw9   1/1     Running   0          16s
redis-master-fq8vp      1/1     Running   0          5m9s
redis-slave-srgsc       1/1     Running   0          5m9s
redis-slave-vglqg       1/1     Running   0          5m9s


MacBook-Pro:yaml chunyinjiang$ kubectl get clonesets.apps.kruise.io
NAME              DESIRED   UPDATED   UPDATED_READY   READY   TOTAL   AGE
guestbook-clone   20        20        20              20      20      82s
redis-master      1         1         1               1       1       12m
redis-slave       2         2         2               2       2       12m

MacBook-Pro:yaml chunyinjiang$ kubectl get svc
NAME                  TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                         AGE
guestbook-clone-svc   LoadBalancer   10.103.56.2    &amp;lt;pending&amp;gt;     3000:31496/TCP,4000:32724/TCP   118s
kubernetes            ClusterIP      10.96.0.1      &amp;lt;none&amp;gt;        443/TCP                         7d6h
redis-master          ClusterIP      10.103.204.4   &amp;lt;none&amp;gt;        6379/TCP                        13m
redis-slave           ClusterIP      10.105.86.31   &amp;lt;none&amp;gt;        6379/TCP                        13m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、扩容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale --replicas=25 clone guestbook-clone
cloneset.apps.kruise.io/guestbook-clone scaled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、指定pod缩容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl edit clonesets.apps.kruise.io guestbook-clone
spec:
  replicas: 22
  scaleStrategy:
    podsToDelete:
    - guestbook-clone-k9796
    - guestbook-clone-nkn52
    - guestbook-clone-w9qgl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7、灰度升级&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:
  # ...
  template:
    spec:
      containers:
      - name: guestbook
        image: openkruise/guestbook:v2
      # ...
  updateStrategy:
    partition: 15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8、实际场景&lt;/p&gt;

&lt;p&gt;目前有openkruise/guestbook:v1的pod为10个。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:iamges chunyinjiang$ kubectl get clonesets.apps.kruise.io guestbook-clone
NAME              DESIRED   UPDATED   UPDATED_READY   READY   TOTAL   AGE
guestbook-clone   10        10        10              10      10      23h
MacBook-Pro:iamges chunyinjiang$ kubectl get pods -o jsonpath=&amp;quot;{.items[*].spec.containers[*].image}&amp;quot;| tr -s &#39;[[:space:]]&#39; &#39;\n&#39; |sort |uniq -c | grep  openkruise
  10 openkruise/guestbook:v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要想其中五个升级为版本v2。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:iamges chunyinjiang$ kubectl edit clonesets.apps.kruise.io guestbook-clone

    spec:
      containers:
      - image: openkruise/guestbook:v2
        imagePullPolicy: IfNotPresent
        name: guestbook
        ports:
        - containerPort: 3000
          name: http-server
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  updateStrategy:
    inPlaceUpdateStrategy: {}
    maxSurge: 0
    maxUnavailable: 3
    partition: 5
    type: InPlaceIfPossible

MacBook-Pro:iamges chunyinjiang$ kubectl edit clonesets.apps.kruise.io guestbook-clone
cloneset.apps.kruise.io/guestbook-clone edited
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:iamges chunyinjiang$ kubectl get clonesets.apps.kruise.io guestbook-clone
NAME              DESIRED   UPDATED   UPDATED_READY   READY   TOTAL   AGE
guestbook-clone   10        5         5               10      10      23h
MacBook-Pro:iamges chunyinjiang$ kubectl get pods -o jsonpath=&amp;quot;{.items[*].spec.containers[*].image}&amp;quot;| tr -s &#39;[[:space:]]&#39; &#39;\n&#39; |sort |uniq -c | grep  openkruise
   5 openkruise/guestbook:v1
   5 openkruise/guestbook:v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在又需要给v1版本和v2版本同时扩容，每个新增5个&lt;/p&gt;

&lt;p&gt;先在v2版本的情况下直接将replicas重10改成15，应该会新增5个v2的版本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:
  replicas: 15

MacBook-Pro:iamges chunyinjiang$ kubectl edit clonesets.apps.kruise.io guestbook-clone
cloneset.apps.kruise.io/guestbook-clone edited
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:iamges chunyinjiang$ kubectl get clonesets.apps.kruise.io guestbook-clone
NAME              DESIRED   UPDATED   UPDATED_READY   READY   TOTAL   AGE
guestbook-clone   15        10        10              15      15      23h
MacBook-Pro:iamges chunyinjiang$ kubectl get pods -o jsonpath=&amp;quot;{.items[*].spec.containers[*].image}&amp;quot;| tr -s &#39;[[:space:]]&#39; &#39;\n&#39; |sort |uniq -c | grep  openkruise
   5 openkruise/guestbook:v1
  10 openkruise/guestbook:v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后给版本切换到v1，同时要修改对应的partion到10，才能不更新&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:iamges chunyinjiang$ kubectl get pods -o jsonpath=&amp;quot;{.items[*].spec.containers[*].image}&amp;quot;| tr -s &#39;[[:space:]]&#39; &#39;\n&#39; |sort |uniq -c | grep  openkruise
   5 openkruise/guestbook:v1
  10 openkruise/guestbook:v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在v2版本的情况下直接将replicas重15改成20，应该会新增5个v1的版本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:iamges chunyinjiang$ kubectl get pods -o jsonpath=&amp;quot;{.items[*].spec.containers[*].image}&amp;quot;| tr -s &#39;[[:space:]]&#39; &#39;\n&#39; |sort |uniq -c | grep  openkruise
  10 openkruise/guestbook:v1
  10 openkruise/guestbook:v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面根据升级情况来设置v1和v2版本。&lt;/p&gt;

&lt;h3 id=&#34;cloneset对比deployment&#34;&gt;cloneset对比deployment&lt;/h3&gt;

&lt;p&gt;cloneset可以说是deployment的增强版本。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- 网络CNI</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network-cni/</link>
          <pubDate>Sun, 17 Jan 2021 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network-cni/</guid>
          <description>&lt;p&gt;CNI（Container Network Interface）是 CNCF 旗下的一个项目，最早是由CoreOS发起的容器网络规范，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;官方的CNI项目主要是接口的定义和实现，以及基本操作的注册集成，我们如果需要实现自己的插件，我们就要实现这个操作的注册（CNI项目提供）与逻辑实现即可，对应的官方也有实现，也就是CNI-plugin项目，很多第三方库也是和自研的一样实现了CNI规范的插件，当然在这些实现中也可以集成基本网络方案。其实CNI就是一个框架，对kubelet实现基本接口，对后端的CNI插件实现基本的操作逻辑，至于接口和操作的转化和执行，交互的内容都已经做了很好的封装。&lt;/p&gt;

&lt;p&gt;其基本思想为：Container Runtime在创建容器时，先创建好network namespace，然后调用CNI插件为这个netns配置网络，其后再启动容器内的进程。&lt;/p&gt;

&lt;p&gt;CNI目前已经发布到了0.4.0版本，实现了ADD，DEL，CHECK，VERSION四个基本操作，我们开发也就是需要实现这个操作的逻辑。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为什么会有CNI？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;各种各样的容器网络解决方案包括新的解决方案层出不穷。如果每出现一个新的解决方案，我们都需要进行两者的适配，那么工作量必然是巨大的，而且也是重复和不必要的。事实上，我们只要提供一个标准的接口，更准确的说是一种协议，就能完美地解决上述问题。一旦有新的网络方案出现时，只要它能满足这个标准的协议，那么它就能为同样满足该协议的所有容器平台提供网络功能，而CNI正是这样的标准接口协议。&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;h2 id=&#34;环境变量&#34;&gt;环境变量&lt;/h2&gt;

&lt;p&gt;运行cni的二进制文件的启动参数是通过下面的环境变量进行传递的&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CNI_PATH          cni二进制文件的路径，可以配置多个，在linux环境下使用&amp;rdquo;:&amp;ldquo;分割&lt;/li&gt;
&lt;li&gt;CNI_ARGS          cni二进制文件的启动参数，以&amp;rdquo;FOO=BAR;ABC=123&amp;rdquo;配置&lt;/li&gt;
&lt;li&gt;CNI_IFNAME        接口名&lt;/li&gt;
&lt;li&gt;CNI_COMMAND       操作类型，目前有ADD，DEL，CHECK，VERSION&lt;/li&gt;
&lt;li&gt;CNI_CONTAINERID   容器ID&lt;/li&gt;
&lt;li&gt;CNI_NETNS         网络命名空间的文件路径&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不同的操作，需要的参数不一样，可以在CNI的getCmdArgsFromEnv函数中查看，总结如下&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CMD\OPT&lt;/th&gt;
&lt;th&gt;CNI_COMMAND&lt;/th&gt;
&lt;th&gt;CNI_ARGS&lt;/th&gt;
&lt;th&gt;CNI_IFNAME&lt;/th&gt;
&lt;th&gt;CNI_PATH&lt;/th&gt;
&lt;th&gt;CNI_CONTAINERID&lt;/th&gt;
&lt;th&gt;CNI_NETNS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ADD&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DEL&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;CHECK&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;VERSION&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;启动&#34;&gt;启动&lt;/h2&gt;

&lt;p&gt;直接使用上面的环境变量加二进制文件就可以启动，官方提供来一个工具CNItool可以模拟运行，只有一个go文件直接编译就可以得到工具cnitool&lt;/p&gt;

&lt;p&gt;先新建一个network namespace：testing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo ip netns add testing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后新建一个网络配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p /etc/cni/net.d

$ cat &amp;gt;/etc/cni/net.d/10-mynet.conflist &amp;lt;&amp;lt;EOF
{
        &amp;quot;cniVersion&amp;quot;: &amp;quot;0.3.0&amp;quot;,
        &amp;quot;name&amp;quot;: &amp;quot;mynet&amp;quot;,
        &amp;quot;plugins&amp;quot;: [
          {
                &amp;quot;type&amp;quot;: &amp;quot;bridge&amp;quot;,
                &amp;quot;bridge&amp;quot;: &amp;quot;cni0&amp;quot;,
                &amp;quot;isGateway&amp;quot;: true,
                &amp;quot;ipMasq&amp;quot;: true,
                &amp;quot;ipam&amp;quot;: {
                        &amp;quot;type&amp;quot;: &amp;quot;host-local&amp;quot;,
                        &amp;quot;subnet&amp;quot;: &amp;quot;10.22.0.0/16&amp;quot;,
                        &amp;quot;routes&amp;quot;: [
                                { &amp;quot;dst&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot; }
                        ]
                }
          }
        ]
}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用cnitool将这个网络增加到testing中，使用plugins的bridge插件，所以指定CNI_PATH环境变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CNI_PATH=$GOPATH/src/github.com/containernetworking/plugins/bin ./cnitool add mynet /var/run/netns/ns
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他的环境变量其实都是在cni代码中转化了，其实可以都像CNI_PATH这样来启动插件。&lt;/p&gt;

&lt;h1 id=&#34;kubelet使用&#34;&gt;kubelet使用&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;安装配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;K8s 通过 CNI 配置文件来决定使用什么 CNI插件，基本的使用方法为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先在每个结点上配置 CNI 配置文件(/etc/cni/net.d/xxnet.conf)，其中 xxnet.conf 是某一个网络配置文件的名称；配置文件内容&lt;/li&gt;
&lt;li&gt;安装 CNI 配置文件中所对应的二进制插件（就是我们实现的CNI操作的插件，可以是官方维护的，也可以是第三方，也可以是自研的），就是将插件的二进制文件放在/opt/cni/bin/目录下；&lt;/li&gt;
&lt;li&gt;在这个节点上创建 Pod 之后，Kubelet 就会根据 CNI 配置文件执行前两步所安装的 CNI 插件；&lt;/li&gt;
&lt;li&gt;上步执行完之后，Pod 的网络就配置完成了。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;流程详解&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;具体的流程如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/cni&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在集群里面创建一个 Pod 的时候，首先会通过 apiserver 将 Pod 的配置写入。apiserver 的一些管控组件（比如 Scheduler）会调度到某个具体的节点上去。Kubelet 监听到这个 Pod 的创建之后，会在本地进行一些创建的操作。kubelet 先创建pause容器生成network namespace，执行到创建网络这一步骤时，它首先会读取刚才我们所说的配置目录中的配置文件，配置文件里面会声明所使用的是哪一个插件，然后去执行具体的 CNI 插件的二进制文件，再由 CNI 插件进入 Pod 的pause 容器网络空间去配置 Pod 的网络，pod 中其他的容器都使用 pause 容器的网络。配置完成之后，Kuberlet 也就完成了整个 Pod 的创建过程，这个 Pod 就在线了。&lt;/p&gt;

&lt;p&gt;使用 CNI 插件比较简单，因为很多 CNI 插件都已提供了一键安装的能力。以我们常用的 Flannel 为例，如下图所示：只需要我们使用 kubectl apply Flannel 的一个 Deploying 模板，它就能自动地将配置、二进制文件安装到每一个节点上去。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;配置详解&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CNI的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; mybridge.conf &amp;lt;&amp;lt;&amp;quot;EOF&amp;quot;
{
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.4.0&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;mybridge&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;bridge&amp;quot;,
    &amp;quot;bridge&amp;quot;: &amp;quot;cni_bridge0&amp;quot;,
    &amp;quot;isGateway&amp;quot;: true,
    &amp;quot;ipMasq&amp;quot;: true,
    &amp;quot;hairpinMode&amp;quot;:true,
    &amp;quot;ipam&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;host-local&amp;quot;,
        &amp;quot;subnet&amp;quot;: &amp;quot;10.15.20.0/24&amp;quot;,
        &amp;quot;routes&amp;quot;: [
            { &amp;quot;dst&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot; },
            { &amp;quot;dst&amp;quot;: &amp;quot;1.1.1.1/32&amp;quot;, &amp;quot;gw&amp;quot;:&amp;quot;10.15.20.1&amp;quot;}
        ]
    }
}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cniVersion： CNI规范的版本&lt;/li&gt;
&lt;li&gt;name： 这个网络的名字叫mybridge&lt;/li&gt;
&lt;li&gt;type：使用brige插件&lt;/li&gt;
&lt;li&gt;isGateway：如果是true，为网桥分配ip地址，以便连接到它的容器可以将其作为网关&lt;/li&gt;
&lt;li&gt;ipMasq：在插件支持的情况的，设置ip伪装。当宿主机充当的网关无法路由到分配给容器的IP子网的网关的时候，这个参数是必须有的。&lt;/li&gt;
&lt;li&gt;ipam：

&lt;ul&gt;
&lt;li&gt;type：IPAM可执行文件的名字&lt;/li&gt;
&lt;li&gt;subnet：要分配给容器的子网&lt;/li&gt;
&lt;li&gt;routes

&lt;ul&gt;
&lt;li&gt;dst： 目的子网&lt;/li&gt;
&lt;li&gt;gw：到达目的地址的下一跳ip地址，如果不指定则为默认网关&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;hairpinMode: 让网络设备能够让数据包从一个端口发进来一个端口发出去&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;各种插件&#34;&gt;各种插件&lt;/h1&gt;

&lt;p&gt;CNI可以支持很多类型的插件，CNI本也只是做网络的增删改查的基本操作，也就是实现了CNI的基本接口，当然也可以在这些接口里面实现组件的原来网络方案，也可以调用实现网络的组件，可以说是基于组件之上，我们也说过一些组件。&lt;/p&gt;

&lt;p&gt;我们来看一下CNI实现有哪些，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/cni2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;官方单独开辟了一个&lt;a href=&#34;https://github.com/containernetworking/plugins&#34;&gt;repo&lt;/a&gt;来表示官方实现的插件类型.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CNI社区

&lt;ul&gt;
&lt;li&gt;Main插件：用来创建具体的网络设备的二进制文件，包括：

&lt;ul&gt;
&lt;li&gt;bridge： 在宿主机上创建网桥然后通过veth pair的方式连接到容器&lt;/li&gt;
&lt;li&gt;macvlan：虚拟出多个macvtap，每个macvtap都有不同的mac地址&lt;/li&gt;
&lt;li&gt;ipvlan：和macvlan相似，也是通过一个主机接口虚拟出多个虚拟网络接口，不同的是ipvlan虚拟出来的是共享MAC地址，ip地址不同&lt;/li&gt;
&lt;li&gt;loopback： lo设备（将回环接口设置成up）&lt;/li&gt;
&lt;li&gt;ptp： Veth Pair设备&lt;/li&gt;
&lt;li&gt;vlan： 分配vlan设备&lt;/li&gt;
&lt;li&gt;host-device： 移动宿主上已经存在的设备到容器中&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;IPAM(IP Address Management)插件: 负责分配IP地址

&lt;ul&gt;
&lt;li&gt;dhcp： 宿主机上运行的守护进程，代表容器发出DHCP请求&lt;/li&gt;
&lt;li&gt;host-local： 使用提前分配好的IP地址段来分配&lt;/li&gt;
&lt;li&gt;static：用于为容器分配静态的IP地址，主要是调试使用&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Meta插件： 由CNI社区维护的内部插件

&lt;ul&gt;
&lt;li&gt;flannel: 专门为Flannel项目提供的插件&lt;/li&gt;
&lt;li&gt;tuning：通过sysctl调整网络设备参数的二进制文件&lt;/li&gt;
&lt;li&gt;portmap：通过iptables配置端口映射的二进制文件&lt;/li&gt;
&lt;li&gt;bandwidth：使用 Token Bucket Filter (TBF)来进行限流的二进制文件&lt;/li&gt;
&lt;li&gt;firewall：通过iptables或者firewalled添加规则控制容器的进出流量&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;3rd party plugins

&lt;ul&gt;
&lt;li&gt;Project Calico - a layer 3 virtual network&lt;/li&gt;
&lt;li&gt;Weave - a multi-host Docker network&lt;/li&gt;
&lt;li&gt;Contiv Networking - policy networking for various use cases&lt;/li&gt;
&lt;li&gt;SR-IOV&lt;/li&gt;
&lt;li&gt;Cilium - BPF &amp;amp; XDP for containers&lt;/li&gt;
&lt;li&gt;Infoblox - enterprise IP address management for containers&lt;/li&gt;
&lt;li&gt;Multus - a Multi plugin&lt;/li&gt;
&lt;li&gt;Romana - Layer 3 CNI plugin supporting network policy for Kubernetes&lt;/li&gt;
&lt;li&gt;CNI-Genie - generic CNI network plugin&lt;/li&gt;
&lt;li&gt;Nuage CNI - Nuage Networks SDN plugin for network policy kubernetes support&lt;/li&gt;
&lt;li&gt;Silk - a CNI plugin designed for Cloud Foundry&lt;/li&gt;
&lt;li&gt;Linen - a CNI plugin designed for overlay networks with Open vSwitch and fit in SDN/OpenFlow network environment&lt;/li&gt;
&lt;li&gt;Vhostuser - a Dataplane network plugin - Supports OVS-DPDK &amp;amp; VPP&lt;/li&gt;
&lt;li&gt;Amazon ECS CNI Plugins - a collection of CNI Plugins to configure containers with Amazon EC2 elastic network interfaces (ENIs)&lt;/li&gt;
&lt;li&gt;Bonding CNI - a Link aggregating plugin to address failover and high availability network&lt;/li&gt;
&lt;li&gt;ovn-kubernetes - an container network plugin built on Open vSwitch (OVS) and Open Virtual Networking (OVN) with support for both Linux and Windows&lt;/li&gt;
&lt;li&gt;Juniper Contrail / TungstenFabric - Provides overlay SDN solution, delivering multicloud networking, hybrid cloud networking, simultaneous overlay-underlay support, network policy enforcement, network isolation, service chaining and flexible load balancing&lt;/li&gt;
&lt;li&gt;Knitter - a CNI plugin supporting multiple networking for Kubernetes&lt;/li&gt;
&lt;li&gt;DANM - a CNI-compliant networking solution for TelCo workloads running on Kubernetes&lt;/li&gt;
&lt;li&gt;VMware NSX – a CNI plugin that enables automated NSX L2/L3 networking and L4/L7 Load Balancing; network isolation at the pod, node, and cluster level; and zero-trust security policy for your Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;cni-route-override - a meta CNI plugin that override route information&lt;/li&gt;
&lt;li&gt;Terway - a collection of CNI Plugins based on alibaba cloud VPC/ECS network product&lt;/li&gt;
&lt;li&gt;Cisco ACI CNI - for on-prem and cloud container networking with consistent policy and security model.&lt;/li&gt;
&lt;li&gt;Kube-OVN - a CNI plugin that bases on OVN/OVS and provides advanced features like subnet, static ip, ACL, QoS, etc.&lt;/li&gt;
&lt;li&gt;Project Antrea - an Open vSwitch k8s CNI&lt;/li&gt;
&lt;li&gt;OVN4NFV-K8S-Plugin - a OVN based CNI controller plugin to provide cloud native based Service function chaining (SFC), Multiple OVN overlay networking&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些插件都是直接使用项目中的编译脚本build_linux.sh就可以生成相关的二进制文件使用，当然自研的插件也一样编译成二进制文件放到上面使用的位置。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;CNI插件和组件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CNI网络插件是通过实现CNI的操作逻辑来调用相关组件实现网络通信，是基于组件之上的，可以说是组件的一个前置，当然也可以集成组件，最终实现网络的思路都是一样的，而且是目前比较推崇的一种标准，希望各大组件都能向它靠拢。&lt;/p&gt;

&lt;p&gt;组件是基于不同实现思路实现的容器网络解决方案，只不过不一定通过CNI插件调用来处理网络，比如直接启动守护进程就可以完成基本操作，目前各大组件正在极力拥抱CNI。&lt;/p&gt;

&lt;h1 id=&#34;源码解析&#34;&gt;源码解析&lt;/h1&gt;

&lt;h2 id=&#34;目录结构&#34;&gt;目录结构&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;├── Documentation               文档目录，存储这一些重要文档
│   ├── cnitool.md              cni工具的说明，cni工具可以在没有docker的情况来模拟测试
│   └── spec-upgrades.md        插件升级维护的一个简单指导
├── SPEC.md                     插件的使用说明
├── cnitool                     cni工具的实现
│   ├── README.md
│   └── cnitool.go
├── libcni
│   ├── api.go
│   ├── api_test.go
│   ├── backwards_compatibility_test.go
│   ├── conf.go
│   ├── conf_test.go
│   └── libcni_suite_test.go
├── pkg
│   ├── invoke
│   │   ├── args.go
│   │   ├── args_test.go
│   │   ├── delegate.go
│   │   ├── delegate_test.go
│   │   ├── exec.go
│   │   ├── exec_test.go
│   │   ├── fakes
│   │   │   ├── cni_args.go
│   │   │   ├── raw_exec.go
│   │   │   └── version_decoder.go
│   │   ├── find.go
│   │   ├── find_test.go
│   │   ├── get_version_integration_test.go
│   │   ├── invoke_suite_test.go
│   │   ├── os_unix.go
│   │   ├── os_windows.go
│   │   ├── raw_exec.go
│   │   └── raw_exec_test.go
│   ├── skel
│   │   ├── skel.go
│   │   ├── skel_suite_test.go
│   │   └── skel_test.go
│   ├── types
│   │   ├── 020
│   │   │   ├── types.go
│   │   │   ├── types_suite_test.go
│   │   │   └── types_test.go
│   │   ├── 040
│   │   │   ├── types.go
│   │   │   ├── types_suite_test.go
│   │   │   └── types_test.go
│   │   ├── 100
│   │   │   ├── types.go
│   │   │   ├── types_suite_test.go
│   │   │   └── types_test.go
│   │   ├── args.go
│   │   ├── args_test.go
│   │   ├── create
│   │   │   └── create.go
│   │   ├── internal
│   │   │   ├── convert.go
│   │   │   └── create.go
│   │   ├── types.go
│   │   ├── types_suite_test.go
│   │   └── types_test.go
│   ├── utils
│   │   ├── utils.go
│   │   └── utils_test.go
│   └── version
│       ├── conf.go
│       ├── conf_test.go
│       ├── legacy_examples
│       │   ├── example_runtime.go
│       │   ├── examples.go
│       │   ├── legacy_examples_suite_test.go
│       │   └── legacy_examples_test.go
│       ├── plugin.go
│       ├── plugin_test.go
│       ├── reconcile.go
│       ├── reconcile_test.go
│       ├── testhelpers
│       │   ├── testhelpers.go
│       │   ├── testhelpers_suite_test.go
│       │   └── testhelpers_test.go
│       ├── version.go
│       ├── version_suite_test.go
│       └── version_test.go
├── plugins
│   └── test
│       ├── noop
│       │   ├── debug
│       │   │   └── debug.go
│       │   ├── main.go
│       │   ├── noop_suite_test.go
│       │   └── noop_test.go
│       └── sleep
│           └── main.go
├── scripts
│   ├── docker-run.sh
│   ├── exec-plugins.sh
│   ├── priv-net-run.sh
│   └── release.sh
└── test.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;接口&#34;&gt;接口&lt;/h2&gt;

&lt;p&gt;CNI的核心是定义了网络操作的基本接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type CNI interface {
    AddNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)
    CheckNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error
    DelNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error
    GetNetworkListCachedResult(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)
    GetNetworkListCachedConfig(net *NetworkConfigList, rt *RuntimeConf) ([]byte, *RuntimeConf, error)

    AddNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) (types.Result, error)
    CheckNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error
    DelNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error
    GetNetworkCachedResult(net *NetworkConfig, rt *RuntimeConf) (types.Result, error)
    GetNetworkCachedConfig(net *NetworkConfig, rt *RuntimeConf) ([]byte, *RuntimeConf, error)

    ValidateNetworkList(ctx context.Context, net *NetworkConfigList) ([]string, error)
    ValidateNetwork(ctx context.Context, net *NetworkConfig) ([]string, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CNI库对这些接口进行实现包装，到插件使用的时候，直接就是对应的我们的ADD，DEL，CHECK，VERSION四个基本操作，CNI库就是这样连接kubelet和CNI插件的。&lt;/p&gt;

&lt;h2 id=&#34;kubelet&#34;&gt;kubelet&lt;/h2&gt;

&lt;p&gt;我们在&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-kubelet/#pod创建中的cni网络插件&#34;&gt;kubelet&lt;/a&gt;中已经将对应的参数和配置调用的CNIConfig的AddNetworkList。&lt;/p&gt;

&lt;p&gt;我们先来看一下CNIConfig&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type CNIConfig struct {
    Path     []string
    exec     invoke.Exec
    cacheDir string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用这个实例的AddNetworkList&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// AddNetworkList executes a sequence of plugins with the ADD command
func (c *CNIConfig) AddNetworkList(ctx context.Context, list *NetworkConfigList, rt *RuntimeConf) (types.Result, error) {
    var err error
    var result types.Result
    for _, net := range list.Plugins {
        result, err = c.addNetwork(ctx, list.Name, list.CNIVersion, net, result, rt)
        if err != nil {
            return nil, err
        }
    }

    if err = c.cacheAdd(result, list.Bytes, list.Name, rt); err != nil {
        return nil, fmt.Errorf(&amp;quot;failed to set network %q cached result: %v&amp;quot;, list.Name, err)
    }

    return result, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的是其还是addNetwork操作，只不过有一个list循环处理，我们还是看addNetwork&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *CNIConfig) addNetwork(ctx context.Context, name, cniVersion string, net *NetworkConfig, prevResult types.Result, rt *RuntimeConf) (types.Result, error) {
    c.ensureExec()
    pluginPath, err := c.exec.FindInPath(net.Network.Type, c.Path)
    if err != nil {
        return nil, err
    }
    if err := utils.ValidateContainerID(rt.ContainerID); err != nil {
        return nil, err
    }
    if err := utils.ValidateNetworkName(name); err != nil {
        return nil, err
    }
    if err := utils.ValidateInterfaceName(rt.IfName); err != nil {
        return nil, err
    }

    newConf, err := buildOneConfig(name, cniVersion, net, prevResult, rt)
    if err != nil {
        return nil, err
    }

    return invoke.ExecPluginWithResult(ctx, pluginPath, newConf.Bytes, c.args(&amp;quot;ADD&amp;quot;, rt), c.exec)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出先找出需要执行的文件，然后执行ADD操作，这边的ADD就是后面调用CNI插件的cmd操作，我们来看一下ExecPluginWithResult&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ExecPluginWithResult(ctx context.Context, pluginPath string, netconf []byte, args CNIArgs, exec Exec) (types.Result, error) {
    if exec == nil {
        exec = defaultExec
    }

    stdoutBytes, err := exec.ExecPlugin(ctx, pluginPath, netconf, args.AsEnv())
    if err != nil {
        return nil, err
    }

    // Plugin must return result in same version as specified in netconf
    versionDecoder := &amp;amp;version.ConfigDecoder{}
    confVersion, err := versionDecoder.Decode(netconf)
    if err != nil {
        return nil, err
    }

    return version.NewResult(confVersion, stdoutBytes)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是简单的执行二进制文件，将执行结果返回给kubelet。&lt;/p&gt;

&lt;h2 id=&#34;cni插件&#34;&gt;CNI插件&lt;/h2&gt;

&lt;p&gt;虽然各个CNI插件实现容器网络的方式是多种多样的，但是它们编写的套路基本是一致的。其中一定会存在三个函数：main（），cmdAdd（），cmdDel（）。这个就是CNI定义的规范，ADD，DEL，CHECK，VERSiON接口。我们就以CNI官方插件库的bridge插件为例，进一步说明CNI插件应该如何实现的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;main函数&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、CNI的skel.PluginMain这个函数将函数cmdAdd和cmdDel以及支持插件支持的CNI版本作为参数传递给它。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    skel.PluginMain(cmdAdd, cmdDel, version.All)
}
　　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、PluginMain函数是一个包裹函数，它直接对PluginMainWithError进行调用，当有错误发生的时候，会将错误以json的形式输出到标准输出，并退出插件的执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func PluginMain(cmdAdd, cmdDel func(_ *CmdArgs) error, versionInfo version.PluginInfo) {
    if e := PluginMainWithError(cmdAdd, cmdDel, versionInfo); e != nil {
        if err := e.Print(); err != nil {
            log.Print(&amp;quot;Error writing error JSON to stdout: &amp;quot;, err)
        }
        os.Exit(1)
    }
}
　　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、PluginMainWithError函数也非常简单，其实就是用环境变量，标准输入输出构造了一个dispatcher结构，再执行其中的pluginMain方法而已。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func PluginMainWithError(cmdAdd, cmdDel func(_ *CmdArgs) error, versionInfo version.PluginInfo) *types.Error {
    return (&amp;amp;dispatcher{
        Getenv: os.Getenv,
        Stdin:  os.Stdin,
        Stdout: os.Stdout,
        Stderr: os.Stderr,
    }).pluginMain(cmdAdd, cmdDel, versionInfo)
}
　　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;dispatcher结构如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type dispatcher struct {
    Getenv func(string) string
    Stdin  io.Reader
    Stdout io.Writer
    Stderr io.Writer

    ConfVersionDecoder version.ConfigDecoder
    VersionReconciler  version.Reconciler
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、接着dispatcher结构的pluginMain方法执行具体的操作。该函数的操作分为如下两步：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先调用cmd, cmdArgs, err := t.getCmdArgsFromEnv()从环境变量和标准输入中解析出操作信息cmd和配置信息cmdArgs，这个cmd就是在kubelet调用的时候封装的ADD。&lt;/li&gt;
&lt;li&gt;接着根据操作信息cmd的不同，调用checkVersionAndCall()，该函数会首先从标准输入中获取配置信息中的CNI版本，再和之前main函数中指定的插件支持的CNI版本信息进行比对。如果版本匹配，则调用相应的回调函数cmdAdd或cmdDel并以cmdArgs作为参数，否则，返回错误&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;看代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (t *dispatcher) pluginMain(cmdAdd, cmdDel func(_ *CmdArgs) error, versionInfo version.PluginInfo) *types.Error {
    cmd, cmdArgs, err := t.getCmdArgsFromEnv()
        .....
    switch cmd {
    case &amp;quot;ADD&amp;quot;:
        err = t.checkVersionAndCall(cmdArgs, versionInfo, cmdAdd)
    case &amp;quot;DEL&amp;quot;:
                ......
    }
        ......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、下面我们来看看dispatcher的getCmdArgsFromEnv()方法是如何从环境变量和标准输入中获取配置信息的。首先来看一下cmdArgs的具体结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type CmdArgs struct {
    ContainerID string
    Netns       string
    IfName      string
    Args        string
    Path        string
    StdinData   []byte
}
　　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分析了上述结构之后，不难想象，getCmdArgsFromEnv()所做的工作就是从环境变量中提取出配置信息用以填充CmdArgs，再将容器网络的配置信息，也就是标准输入中的内容，存入StdinData字段。具体代码如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (t *dispatcher) getCmdArgsFromEnv() (string, *CmdArgs, error) {
    var cmd, contID, netns, ifName, args, path string

    vars := []struct {
        name      string
        val       *string
        reqForCmd reqForCmdEntry
    }{
        {
            &amp;quot;CNI_COMMAND&amp;quot;,
            &amp;amp;cmd,
            reqForCmdEntry{
                &amp;quot;ADD&amp;quot;: true,
                &amp;quot;DEL&amp;quot;: true,
            },
        },
                ....
        {
            &amp;quot;CNI_NETNS&amp;quot;,
            &amp;amp;netns,
            reqForCmdEntry{
                &amp;quot;ADD&amp;quot;: true,
                &amp;quot;DEL&amp;quot;: false,
            },
        },
                ....
    }

    argsMissing := false
    for _, v := range vars {
        *v.val = t.Getenv(v.name)
        if *v.val == &amp;quot;&amp;quot; {
            if v.reqForCmd[cmd] || v.name == &amp;quot;CNI_COMMAND&amp;quot; {
                fmt.Fprintf(t.Stderr, &amp;quot;%v env variable missing\n&amp;quot;, v.name)
                argsMissing = true
            }
        }
    }

    if argsMissing {
        return &amp;quot;&amp;quot;, nil, fmt.Errorf(&amp;quot;required env variables missing&amp;quot;)
    }

    stdinData, err := ioutil.ReadAll(t.Stdin)
    if err != nil {
        return &amp;quot;&amp;quot;, nil, fmt.Errorf(&amp;quot;error reading from stdin: %v&amp;quot;, err)
    }

    cmdArgs := &amp;amp;CmdArgs{
        ContainerID: contID,
        Netns:       netns,
        IfName:      ifName,
        Args:        args,
        Path:        path,
        StdinData:   stdinData,
    }
    return cmd, cmdArgs, nil
}
　　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;虽然getCmdArgsFromEnv()要完成的工作非常简单，但仔细分析代码之后，我们可以发现它的实现非常精巧。首先，它定义了一系列想要获取的参数，例如cmd，contID，netns等等。之后再定义了一个匿名结构的数组，匿名结构中包含了环境变量的名字，一个字符串指针（把该环境变量对应的参数赋给它，例如cmd对应CNI_COMMAND）以及一个reqForCmdEntry类型的成员reqForCmd。类型reqForCmdEntry其实是一个map，它在这里的作用是定义该环境变量是否为对应操作所必须的。例如，上文中的环境变量&amp;rdquo;CNI_NETNS&amp;rdquo;，对于&amp;rdquo;ADD&amp;rdquo;操作为true，而对于&amp;rdquo;DEL&amp;rdquo;操作则为false，这说明在&amp;rdquo;ADD&amp;rdquo;操作时，该环境变量不能为空，否则会报错，但是在&amp;rdquo;DEL&amp;rdquo;操作时则无所谓。最后，遍历该数组进行参数的提取即可。&lt;/p&gt;

&lt;p&gt;到此为止，main函数的任务完成。总的来说它做了三件事&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CNI版本检查。&lt;/li&gt;
&lt;li&gt;提取配置参数构建cmdArgs。&lt;/li&gt;
&lt;li&gt;调用对应的回调函数，cmdAdd或者cmdDel。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我们以cmdAdd看看是怎么增加网络的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;cmdAdd函数&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、如下所示cmdAdd函数一般分为三个步骤执行：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先调用函数conf, err := loadNetConf(args.StdinData)（注：loadNetConf是插件自定义的，各个插件都不一样），从标准输入，也就是参数args.StdinData中获取容器网络配置信息&lt;/li&gt;
&lt;li&gt;接着根据具体的配置信息进行网络的配置工作，这一步就是调用相关的组件来完成相关的配置。&lt;/li&gt;
&lt;li&gt;最后，调用函数types.PrintResult(result, conf.CNIVersion)输出配置结果&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;代码如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func cmdAdd(args *skel.CmdArgs) error {
    n, cniVersion, err := loadNetConf(args.StdinData)
        ......
        return PrintResult(result, cniVersion)
}    　　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、接着我们对loadNetConf函数进行分析。因为每个CNI插件配置容器网络的方式各有不同，因此它们所需的配置信息一般也是不同的，除了大家共有的信息被包含在types.NetConf结构中，每个插件还定义了自己所需的字段。例如，对于bridge插件，它用于存储配置信息的结构如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type NetConf struct {
    types.NetConf
    BrName       string `json:&amp;quot;bridge&amp;quot;`
    IsGW         bool   `json:&amp;quot;isGateway&amp;quot;`
    IsDefaultGW  bool   `json:&amp;quot;isDefaultGateway&amp;quot;`
    ForceAddress bool   `json:&amp;quot;forceAddress&amp;quot;`
    IPMasq       bool   `json:&amp;quot;ipMasq&amp;quot;`
    MTU          int    `json:&amp;quot;mtu&amp;quot;`
    HairpinMode  bool   `json:&amp;quot;hairpinMode&amp;quot;`
    PromiscMode  bool   `json:&amp;quot;promiscMode&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而loadNetConf函数所做的操作也非常简单，就是调用json.Unmarshal(bytes, n)函数将配置信息从标准输入的字节流中解码到一个NetConf结构，具体代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func loadNetConf(bytes []byte) (*NetConf, string, error) {
    n := &amp;amp;NetConf{
        BrName: defaultBrName,
    }
    if err := json.Unmarshal(bytes, n); err != nil {
        return nil, &amp;quot;&amp;quot;, fmt.Errorf(&amp;quot;failed to load netconf: %v&amp;quot;, err)
    }
    return n, n.CNIVersion, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、最后，我们对配置结果的输出进行分析。由于不同的CNI版本要求的输出结果的内容是不太一样的，因此这部分内容其实是比较复杂的。下面我们就进入PrintResult函数一探究竟。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func PrintResult(result Result, version string) error {
    newResult, err := result.GetAsVersion(version)
    if err != nil {
        return err
    }
    return newResult.Print()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的代码中我们可以看出，该函数就做了两件事，一件是调用newResult, err := result.GetAsVersion(version)，根据指定的版本信息，进行结果信息的版本转换。第二件就是调用newResult.Print()将结果信息输出到标准输出。&lt;/p&gt;

&lt;p&gt;事实上，Result如下所示，是一个interface类型。每个版本的CNI都是定义了自己的Result结构的，而这些结构都是满足Result接口的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Result is an interface that provides the result of plugin execution
type Result interface {
    // The highest CNI specification result verison the result supports
    // without having to convert
    Version() string

    // Returns the result converted into the requested CNI specification
    // result version, or an error if conversion failed
    GetAsVersion(version string) (Result, error)

    // Prints the result in JSON format to stdout
    Print() error

    // Returns a JSON string representation of the result
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而其中的GetAsVersion()方法则用于将当前版本的CNI Result信息转化到对应的CNI Result信息。我们来举个具体的例子，应该就很清晰了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Result) GetAsVersion(version string) (types.Result, error) {
    switch version {
    case &amp;quot;0.3.0&amp;quot;, ImplementedSpecVersion:
        r.CNIVersion = version
        return r, nil
    case types020.SupportedVersions[0], types020.SupportedVersions[1], types020.SupportedVersions[2]:
        return r.convertTo020()
    }
    return nil, fmt.Errorf(&amp;quot;cannot convert version 0.3.x to %q&amp;quot;, version)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;假设现在我们的result的版本0.3.0， 但是插件要求返回的result版本是0.2.0的，根据上文中的代码，显然此时我们会调用r.convertTo020()函数进行转换，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Convert to the older 0.2.0 CNI spec Result type
func (r *Result) convertTo020() (*types020.Result, error) {
    oldResult := &amp;amp;types020.Result{
        CNIVersion: types020.ImplementedSpecVersion,
        DNS:        r.DNS,
    }

    for _, ip := range r.IPs {
        // Only convert the first IP address of each version as 0.2.0
        // and earlier cannot handle multiple IP addresses
               ......
    }

    for _, route := range r.Routes {
               ......
    }
        ......
    return oldResult, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数所做的操作，简单来说，就是定义了相应版本具体的Result结构，然后用当前版本的Result结构中的信息进行填充，从而完成Result版本的转化。&lt;/p&gt;

&lt;p&gt;而Print方法对于各个版本的Result都是一样的，都是将Result进行json编码后，输出到标准输出而已。&lt;/p&gt;

&lt;p&gt;到此为止，cmdAdd函数操作完成。其他的套路这个都是一样的，就不一一说明了。&lt;/p&gt;

&lt;h1 id=&#34;交互协议&#34;&gt;交互协议&lt;/h1&gt;

&lt;h2 id=&#34;add&#34;&gt;ADD&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Parameters:

&lt;ul&gt;
&lt;li&gt;Version. 调用者使用的CNI 配置的版本信息&lt;/li&gt;
&lt;li&gt;Container ID. 这个字段是可选的，但是建议使用，在容器活着的时候要求该字段全局唯一的。比如，存在IPAM的环境可能会要求每个container都分配一个独立的ID，这样每一个IP的分配都能和一个特定的容器相关联。例如，在appc implementations中，container ID其实就是pod ID&lt;/li&gt;
&lt;li&gt;Network namespace path. 这个字段表示要加入的network namespace的路径。例如，/proc/[pid]/ns/net或者对于该目录的bind-mount/link。&lt;/li&gt;
&lt;li&gt;Network configuration. 这是一个JSON文件用于描述container可以加入的network，具体内容在下文中描述&lt;/li&gt;
&lt;li&gt;Extra arguments. 该字段提供了可选的机制，从而允许基于每个容器进行CNI插件的简单配置&lt;/li&gt;
&lt;li&gt;Name of the interface inside the container. 该字段提供了在container （network namespace）中的interface的名字；因此，它也必须符合Linux对于网络命名的限制&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Result:

&lt;ul&gt;
&lt;li&gt;Interface list. 根据插件的不同，这个字段可以包括sandbox (container or hypervisor) interface的name，以及host interface的name，每个interface的hardware address，以及interface所在的sandbox（如果存在的话）的信息。&lt;/li&gt;
&lt;li&gt;IP configuration assigned to each interface. IPv4和/或者IPv6地址，gateways以及为sandbox或host interfaces中添加的路由&lt;/li&gt;
&lt;li&gt;DNS inormation. 包含nameservers，domains，search domains和options的DNS information的字典&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;del&#34;&gt;DEL&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Parameter:

&lt;ul&gt;
&lt;li&gt;Version. 调用者使用的CNI 配置的版本信息&lt;/li&gt;
&lt;li&gt;ContainerID. 定义同上&lt;/li&gt;
&lt;li&gt;Network namespace path. 定义同上&lt;/li&gt;
&lt;li&gt;Network configuration. 定义同上&lt;/li&gt;
&lt;li&gt;Extra argument. 定义同上&lt;/li&gt;
&lt;li&gt;Name of the interface inside the container. 定义同上&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;version&#34;&gt;VERSION&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Parameter: 无&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Result: 返回插件支持的所有CNI版本,如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;cniVersion&amp;quot;: &amp;quot;1.0.0&amp;quot;, // the version of the CNI spec in use for this output
  &amp;quot;supportedVersions&amp;quot;: [ &amp;quot;0.1.0&amp;quot;, &amp;quot;0.2.0&amp;quot;, &amp;quot;0.3.0&amp;quot;, &amp;quot;0.3.1&amp;quot;, &amp;quot;0.4.0&amp;quot;, &amp;quot;1.0.0&amp;quot; ] // the list of CNI spec versions that this plugin supports
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;check&#34;&gt;CHECK&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Parameter:

&lt;ul&gt;
&lt;li&gt;Version. 调用者使用的CNI 配置的版本信息&lt;/li&gt;
&lt;li&gt;ContainerID. 定义同上&lt;/li&gt;
&lt;li&gt;Network namespace path. 定义同上&lt;/li&gt;
&lt;li&gt;Network configuration. 定义同上&lt;/li&gt;
&lt;li&gt;Extra argument. 定义同上&lt;/li&gt;
&lt;li&gt;Name of the interface inside the container. 定义同上&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;result

&lt;ul&gt;
&lt;li&gt;The plugin must return either nothing or an error.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;如何开发自己的-cni-插件&#34;&gt;如何开发自己的 CNI 插件&lt;/h1&gt;

&lt;p&gt;首先就是注册我们的操作，比如ADD，DEL等调用的函数，然后在对应的操作中做网络的配置，这一部分我们在上面的源码解析中已经说明，主要看一下配置&lt;/p&gt;

&lt;p&gt;这一块的的实现一般都是组件实现的，我们也可以集成在CNI插件中，通常包含两个部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一个二进制的 CNI 插件去配置 Pod 网卡和 IP 地址。这一步配置完成之后相当于给 Pod 上插上了一条网线，就是说它已经有自己的 IP、有自己的网卡了；&lt;/li&gt;
&lt;li&gt;一个 Daemon 进程去管理 Pod 之间的网络打通。这一步相当于说将 Pod 真正连上网络，让 Pod 之间能够互相通信。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1、给 Pod 插上网线&lt;/p&gt;

&lt;p&gt;那么如何实现第一步，给 Pod 插上网线呢？通常是这样一个步骤：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/cni1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;给 Pod 准备一个网卡&lt;/p&gt;

&lt;p&gt;通常我们会用一个 &amp;ldquo;veth&amp;rdquo; 这种虚拟网卡，一端放到 Pod 的网络空间，一端放到主机的网络空间，这样就实现了 Pod 与主机这两个命名空间的打通。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;给 Pod 分配 IP 地址&lt;/p&gt;

&lt;p&gt;这个 IP 地址有一个要求，我们在之前介绍网络的时候也有提到，就是说这个 IP 地址在集群里需要是唯一的。如何保障集群里面给 Pod 分配的是个唯一的 IP 地址呢？&lt;/p&gt;

&lt;p&gt;一般来说我们在创建整个集群的时候会指定 Pod 的一个大网段，按照每个节点去分配一个 Node 网段。比如说上图右侧创建的是一个 172.16 的网段，我们再按照每个节点去分配一个 /24 的段，这样就能保障每个节点上的地址是互不冲突的。然后每个 Pod 再从一个具体的节点上的网段中再去顺序分配具体的 IP 地址，比如 Pod1 分配到了 172.16.0.1，Pod2 分配到了 172.16.0.2，这样就实现了在节点里面 IP 地址分配的不冲突，并且不同的 Node 又分属不同的网段，因此不会冲突。&lt;/p&gt;

&lt;p&gt;这样就给 Pod 分配了集群里面一个唯一的 IP 地址。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置 Pod 的 IP 和路由&lt;/p&gt;

&lt;p&gt;第一步，将分配到的 IP 地址配置给 Pod 的虚拟网卡；
第二步，在 Pod 的网卡上配置集群网段的路由，令访问的流量都走到对应的 Pod 网卡上去，并且也会配置默认路由的网段到这个网卡上，也就是说走公网的流量也会走到这个网卡上进行路由；
最后在宿主机上配置到 Pod 的 IP 地址的路由，指向到宿主机对端 veth1 这个虚拟网卡上。这样实现的是从 Pod 能够到宿主机上进行路由出去的，同时也实现了在宿主机上访问到 Pod 的 IP 地址也能路由到对应的 Pod 的网卡所对应的对端上去。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2、给 Pod 连上网络&lt;/p&gt;

&lt;p&gt;刚才我们是给 Pod 插上网线，也就是给它配了 IP 地址以及路由表。那怎么打通 Pod 之间的通信呢？也就是让每一个 Pod 的 IP 地址在集群里面都能被访问到。&lt;/p&gt;

&lt;p&gt;一般我们是在 CNI Daemon 进程中去做这些网络打通的事情。通常来说是这样一个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先 CNI 在每个节点上运行的 Daemon 进程会学习到集群所有 Pod 的 IP 地址及其所在节点信息。学习的方式通常是通过监听 K8s APIServer，拿到现有 Pod 的 IP 地址以及节点，并且新的节点和新的 Pod 的创建的时候也能通知到每个 Daemon；&lt;/li&gt;
&lt;li&gt;拿到 Pod 以及 Node 的相关信息之后，再去配置网络进行打通。

&lt;ul&gt;
&lt;li&gt;首先 Daemon 会创建到整个集群所有节点的通道。这里的通道是个抽象概念，具体实现一般是通过 Overlay 隧道、阿里云上的 VPC 路由表、或者是自己机房里的 BGP 路由完成的；&lt;/li&gt;
&lt;li&gt;第二步是将所有 Pod 的 IP 地址跟上一步创建的通道关联起来。关联也是个抽象概念，具体的实现通常是通过 Linux 路由、fdb 转发表或者OVS 流表等完成的。Linux 路由可以设定某一个 IP 地址路由到哪个节点上去。fdb 转发表是 forwarding database 的缩写，就是把某个 Pod 的 IP 转发到某一个节点的隧道端点上去（Overlay 网络）。OVS 流表是由 Open vSwitch 实现的，它可以把 Pod 的 IP 转发到对应的节点上。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于以CNI插件的方式来扩展网络并不是很复杂，很多公司都会自研自己的插件和组件。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算系列---- 云计算概念</title>
          <link>https://kingjcy.github.io/post/cloud/cncf/</link>
          <pubDate>Sat, 02 Jan 2021 19:51:38 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/cncf/</guid>
          <description>&lt;p&gt;目前云计算，云原生，微服务等概念已经不绝于耳，目前来说，上云已经是一种必然的趋势。&lt;/p&gt;

&lt;h1 id=&#34;云原生&#34;&gt;云原生&lt;/h1&gt;

&lt;p&gt;Pivotal 是云原生应用的提出者，早在2015年Pivotal公司的Matt Stine写了一本叫做迁移到云原生应用架构的小册子，其中探讨了云原生应用架构的几个主要特征：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;符合12因素应用&lt;/li&gt;
&lt;li&gt;面向微服务架构&lt;/li&gt;
&lt;li&gt;自服务敏捷架构&lt;/li&gt;
&lt;li&gt;基于API的协作&lt;/li&gt;
&lt;li&gt;抗脆弱性&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;并且在到了2015年Google主导成立了云原生计算基金会（CNCF），起初CNCF对云原生（Cloud Native）的定义包含以下三个方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;应用容器化&lt;/li&gt;
&lt;li&gt;面向微服务架构&lt;/li&gt;
&lt;li&gt;应用支持容器的编排调度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;到了2018年，由于云原生的生态发展，又更加广泛的定义包括容器、服务网格、微服务、不可变基础设施和声明式API。&lt;/p&gt;

&lt;p&gt;个人觉得最终的目的都是将应用中非业务代码部分最大程度的剥离，从而让云平台接管应用中大量非功能性的特性，比如弹性，灰度等，让业务不受非功能性的影响，具有轻量，敏捷的特性。&lt;/p&gt;

&lt;p&gt;云原生就是为了实现&lt;a href=&#34;#云计算&#34;&gt;云计算&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;云原生技术的核心思想是快速传递业务价值，为容器提供计算资源，将大型单体应用转化为微服务，形成一个敏感的操作系统。云原生技术改变了业务应用程序的生产模式，并释放了业务系统的核心价值。&lt;/p&gt;

&lt;p&gt;云原生可以说是一种开发模式，比如我们很多都是遵循k8s的设计模式来开发，就叫云原生，它有很多设计理念：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;面向分布式设计（Distribution）：容器、微服务、API 驱动的开发；&lt;/li&gt;
&lt;li&gt;面向配置设计（Configuration）：一个镜像，多个环境配置；&lt;/li&gt;
&lt;li&gt;面向韧性设计（Resistancy）：故障容忍和自愈；&lt;/li&gt;
&lt;li&gt;面向弹性设计（Elasticity）：弹性扩展和对环境变化（负载）做出响应；&lt;/li&gt;
&lt;li&gt;面向交付设计（Delivery）：自动拉起，缩短交付时间；&lt;/li&gt;
&lt;li&gt;面向性能设计（Performance）：响应式，并发和资源高效利用；&lt;/li&gt;
&lt;li&gt;面向自动化设计（Automation）：自动化的 DevOps；&lt;/li&gt;
&lt;li&gt;面向诊断性设计（Diagnosability）：集群级别的日志、metric 和追踪；&lt;/li&gt;
&lt;li&gt;面向安全性设计（Security）：安全端点、API Gateway、端到端加密；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实这些更像是在传统架构的基础上加了一层控制器，来实现智能化的架构基础，这是我对目前云的设计开发的一种理解。&lt;/p&gt;

&lt;h2 id=&#34;云原生架构&#34;&gt;云原生架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/compute/architecture.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这张图可以看出云原生的基本生态，很多重要组件就是我们组件云生态的核心。&lt;/p&gt;

&lt;h2 id=&#34;云原生过程&#34;&gt;云原生过程&lt;/h2&gt;

&lt;p&gt;云原生基础设施不等于在公有云上运行的基础设施。光是租用服务器并不会使您的基础设施云原生化，如果只是这样和管理 IaaS 的流程与运维物理数据中心没什么两样，将现有架构迁移到云上也未必能获得回报。&lt;/p&gt;

&lt;p&gt;云原生也不是指在容器中运行应用程序。在容器中运行只不过是改变应用程序的打包方式并不意味着就会增加自治系统的可扩展性和优势。即使应用程序是通过 CI/CD 渠道自动构建和部署的，也不意味着您就可以从增强 API 驱动部署的基础设施中受益。但是使用容器和容器编排工具我们可以实现动态调度的平台功能，这是一个很好的起步。&lt;/p&gt;

&lt;p&gt;云原生不是微服务或基础设施即代码。但是微服务意味着更快的开发周期和更小的独特功能，基础设施即代码以机器可解析语言或领域特定语言（DSL）定义、自动化您的基础设施。将代码应用于基础架构的传统工具包括配置管理工具（例如 Chef 和 Puppet）。这些工具在自动执行任务和提供一致性方面有很大帮助。这些都将解决人工作为部署和管理的瓶颈问题。&lt;/p&gt;

&lt;p&gt;所以云原生是发展到一定阶段的产物，使用容器，微服务，编排调度来实现基础设施的云原生，也是发展的必然趋势。&lt;/p&gt;

&lt;h2 id=&#34;云原生应用程序&#34;&gt;云原生应用程序&lt;/h2&gt;

&lt;p&gt;云原生应用程序被设计为在平台上运行，并设计用于弹性，敏捷性，可操作性和可观察性。弹性包含失败而不是试图阻止它们；它利用了在平台上运行的动态特性。敏捷性允许快速部署和快速迭代。可操作性从应用程序内部控制应用程序生命周期，而不是依赖外部进程和监视器。可观察性提供信息来回答有关应用程序状态的问题。&lt;/p&gt;

&lt;p&gt;云原生应用程序通过各种方法获取这些特征。以下是实现云原生应用程序所需特性的常用方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;微服务：服务切分&lt;/li&gt;
&lt;li&gt;健康报告：健康数据&lt;/li&gt;
&lt;li&gt;遥测数据：监控数据&lt;/li&gt;
&lt;li&gt;弹性：根据健康数据和监控数据来做自动伸缩&lt;/li&gt;
&lt;li&gt;声明式的，而不是命令式的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;云原生基础设施在提供自主应用管理的 IaaS 之上创建了一个平台，该平台建立在动态创建的基础设施之上，以抽象出单个服务器并促进动态资源分配调度，云原生应用程序期望在这样一个大多数自治系统的动态环境平台中运行。&lt;/p&gt;

&lt;p&gt;目前对于云原生应用的开发，还处于初始阶段，各大厂商还在致力于规范的定义，框架的开发，目前比较前沿的就是OAM（Open Application Model）规范，和目前Rudr，Crossplane项目，都是基于OAM开发的。&lt;/p&gt;

&lt;p&gt;OAM 的规范中定义了以下对象，它们既是 OAM 规范中的基本术语也是云原生应用的基本组成。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Workload（工作负载）：应用程序的工作负载类型，由平台提供。&lt;/li&gt;
&lt;li&gt;Component（组件）：定义了一个 Workload 的实例，并以基础设施中立的术语声明其运维特性。&lt;/li&gt;
&lt;li&gt;Trait（特征）：用于将运维特性分配给组件实例。&lt;/li&gt;
&lt;li&gt;ApplicationScope（应用作用域）：用于将组件分组成具有共同特性的松散耦合的应用。&lt;/li&gt;
&lt;li&gt;ApplicationConfiguration（应用配置）：描述 Component 的部署、Trait 和 ApplicationScope。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;云原生编程语言&#34;&gt;云原生编程语言&lt;/h2&gt;

&lt;p&gt;这边并不是说golang这个开发语言，而且针对yaml的语言。下面一段话说明了为什么要有云原生编程语言&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;对于每一个serverless函数来说，我都要写几十行的JSON或者YAML配置。要链接到一个API端点，我还要学习晦涩的概念，执行一系列复制-粘贴的低级工作。如果我想在本机上运行一个小的集群的话，那么Docker还是很棒的，但是如果要在生产上使用的话，那么就要手动管理etcd集群，配置网络和iptables路由表，还有一系列与我的应用程序本身不相干的事情。不过Kubernetes的出现至少让我可以配置一次下次就可以跨云平台重用，但这还是会分散开发人员的精力。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实编程的许多方面都经历了类似的转变过程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;汇编转c语言：在80年代初，我们使用汇编语言对微处理器进行了编程。最终，编译器技术进步了，我们可以同时处理多种常见的架构。像FORTRAN和C这样的Low-level的编程语言开始兴起。&lt;/li&gt;
&lt;li&gt;低级语言转高级语言：在90年代初期，我们直接针对低级别操作系统原语进行编程，无论是POSIX系统调用还是Win32 API，并进行手动内存和资源管理。最终，语言运行时技术和处理器速度提升到了可以使用更高级别语言的状态，如Java。除了动态语言之外，这种趋势已经加速，如JavaScript统治了Web。&lt;/li&gt;
&lt;li&gt;单线程转并发：在21世纪初期，我们的编程模型中的共享内存并发性最好是原始的（我花了很多时间在这个问题上）。现在，我们简单地假设OS具有高级线程共享、调度和异步IO功能，以及编程到更高级别的API，例如任务和承诺。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以云原生目前已在进行这种类似的转变，最终会有一场NoYAML运动的出现。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;云原生其实更多的是一种概念，可以是一种架构，容器化，微服务，编排调度，这也是cncf给出的定义，也可以是一种开发模式，使得我们的程序面向分布式，弹性，解耦，自动化等编程模式，其实也是我们开发云原生应用程序所需要考虑的因素，当然云原生更是一种文化，更是一种潮流，发展到一定阶段的产物。&lt;/strong&gt;它的意义在于让云成为云化战略成功的基石，而不是阻碍，如果业务应用上云之后开发和运维人员比原先还痛苦，成本还高的话，这样的云我们宁愿不上。自从云的概念开始普及，许多公司都部署了实施云化的策略，纷纷搭建起云平台，希望完成传统应用到云端的迁移。但是这个过程中会遇到一些技术难题，上云以后，效率并没有变得更高，故障也没有迅速定位。&lt;/p&gt;

&lt;p&gt;所以云原生并不是简单的上云，而是要实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;敏捷&lt;/li&gt;
&lt;li&gt;可靠&lt;/li&gt;
&lt;li&gt;高弹性&lt;/li&gt;
&lt;li&gt;易扩展&lt;/li&gt;
&lt;li&gt;故障隔离保护&lt;/li&gt;
&lt;li&gt;不中断业务持续更新&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;只有实现这这些上云才有意义，可以降成本，提效率，迅速定位问题。&lt;/p&gt;

&lt;h1 id=&#34;云计算&#34;&gt;云计算&lt;/h1&gt;

&lt;p&gt;云计算：一种利用互联网实现随时随地、按需、便捷地使用共享计算设施、存储设备、应用程序等资源的计算模式。当然在云原生的基础上可以实现资源的高效利用，云原生注重的是生态建设，云计算相对云原生来说更像一种商业的盈利的模式，也可以说是一种资源的高效整合使用。&lt;/p&gt;

&lt;p&gt;云计算是指基于互联网等网络，通过虚拟化方式共享IT资源的新型计算模式。其核心思想是通过网络统一管理和调度计算、存储、网络、软件等资源，实现资源整合与配置优化，以服务方式满足不同用户随时获取并扩展、按需使用并付费，最大限度地降低成本等各类需求。&lt;/p&gt;

&lt;p&gt;云的商业价值就在于降本：机器成本和人力成本。&lt;/p&gt;

&lt;p&gt;云计算系统由云平台、云存储、云终端、云安全四个基本部分组成。&lt;/p&gt;

&lt;p&gt;目前云计算提供的服务模式主要包含三大类：基础设施即服务（IaaS）、平台即服务（PaaS）、软件即服务（SaaS）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/compute/cloud&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基础设施即服务（IaaS）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;云计算服务商提供虚拟的硬件资源，如虚拟的主机、存储、网络、安全等资源，用户无需购买服务器、网络设备和存储设备，只需通过网络租赁即可搭建自己的应用系统。IaaS定位于底层，向用户提供可快速部署、按需分配、按需付费的高安全与高可靠的计算能力以及存储能力租用服务，并可为应用提供开放的云基础设施服务接口，用户可以根据业务需求灵活定制租用相应的基础设施资源。在这种服务模式下，用户无需考虑对琐碎的基础设施进行管理与维护，用户可直接在基础设施上面方便地加载应用。&lt;/p&gt;

&lt;p&gt;IaaS服务对应的用户是系统管理员。传统的虚拟机的应用，都是需要管理员进行管理，然后开发人员负责开发，运维人员负责部署，机器有系统管理员统一管理。&lt;/p&gt;

&lt;p&gt;IaaS已经很成熟了，各大厂商都能提供虚拟机，比如阿里云。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;平台即服务（PaaS）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PaaS提供商提供应用服务引擎，将软件研发测试和运维的平台作为一种服务提供，如应用程序接口（API）服务或应用运行时服务，用户基于这些服务构建业务应用。从用户角度来说，这意味着他们无需自行搭建开发，测试和运维平台，也不会在不同平台兼容性方面遇到困扰。&lt;/p&gt;

&lt;p&gt;PaaS服务对应的用户是应用的开发者和运维人员。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/paas&#34;&gt;paas的核心建设&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;软件即服务（SaaS）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;用户通过标准的 Web 浏览器来使用网络上的软件。从用户角度来说，这意味着前期无需在服务器或软件许可证授权上进行投资；从供应商角度来看，与常规的软件服务模式相比，维护一个应用软件的成本要相对低廉。SaaS供应商通常是按照客户所租用的软件模块来进行收费的，因此用户可以根据需求按需订购软件应用服务，而且SaaS的供应商会负责系统的部署、升级和维护。比如我们常用的邮箱服务等。&lt;/p&gt;

&lt;p&gt;SaaS提供商对应的用户是应用软件使用的终端用户。&lt;/p&gt;

&lt;p&gt;现在市场上也有saas应用已经很成熟了。其实就是我们开箱即用的服务，也可以理解为运行在paas平台上的应用。但是paas确实千变万化的。&lt;/p&gt;

&lt;h1 id=&#34;雾计算&#34;&gt;雾计算&lt;/h1&gt;

&lt;p&gt;边缘计算（雾计算）：&lt;/p&gt;

&lt;p&gt;和传统的中心化思维不同，他的主要计算节点以及应用分布式部署在靠近终端的数据中心，这使得在服务的响应性能、还是可靠性方面都是高于传统中心化的云计算概念，&lt;/p&gt;

&lt;p&gt;具体而言，边缘计算可以理解为是指利用靠近数据源的边缘地带来完成的运算程序。&lt;/p&gt;

&lt;p&gt;那么，边缘计算和云计算之间的区别是什么？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;其实如果说云计算是集中式大数据处理，边缘计算则可以理解为边缘式大数据处理。但不同的是，只是这一次，数据不用再传到遥远的云端，在边缘侧就能解决。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;边缘计算更适合实时的数据分析和智能化处理，相较单纯的云计算也更加高效而且安全！&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;边缘计算和云计算两者实际上都是处理大数据的计算运行的一种方式。边缘计算更准确的说应该是对云计算的一种补充和优化&lt;/p&gt;

&lt;p&gt;边缘计算而言有以下几个特质&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;分布式和低延时计算边缘计算聚焦实时、短周期数据的分析，能够更好地支撑本地业务的实时智能化处理与执行&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;效率更高由于边缘计算距离用户更近，在边缘节点处实现了对数据的过滤和分析，因此效率更高&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;更加智能化AI+边缘计算的组合出击让边缘计算不止于计算，更多了一份智能化&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;更加节能云计算和边缘计算结合，成本只有单独使用云计算的39%&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;缓解流量压力在进行云端传输时通过边缘节点进行一部分简单数据处理，进而能够设备响应时间，减少从设备到云端的数据流量&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;边缘计算主要针对物联网比较实用。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- istio</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-istio/</link>
          <pubDate>Thu, 17 Dec 2020 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-istio/</guid>
          <description>&lt;p&gt;istio是一款 Service Mesh 模式的落地实现，是目前servicemesh实现使用最多的一个方案，istio本身和平台无关，但是一般都是和k8s结合使用，首先k8s服务间通信和流控的相关操作。&lt;/p&gt;

&lt;h1 id=&#34;istio&#34;&gt;istio&lt;/h1&gt;

&lt;p&gt;istio是为了实现由 William Morgan 提出的微服务 Service Mesh 模式和诸多理念，Google , IBM 和 Lyft 这三家公司协同研发，并于 2017 年 6 月 8 日( 根据 Github 最后一次提交的时间 )发布了 Istio 的第一个发行版——Istio 0.1 版本。&lt;/p&gt;

&lt;p&gt;istio主要集成了服务发现、负载均衡、故障恢复、度量和监控等服务治理的基本功能。还满足了更复杂的运维需求，比如 A/B 测试、金丝雀发布、速率限制、访问控制和端到端认证。&lt;/p&gt;

&lt;p&gt;在k8s中实现有着负载均衡，路由等基本的服务治理功能，都是通过service实现的，service其实就是通过kube-proxy这个组件实现，核心是使用的iptables的路由功能，包括后来优化的ipvs，但是kube-proxy还有着很多的不足，比如流量控制，可视化，安全等等，所以istio诞生了。&lt;/p&gt;

&lt;h1 id=&#34;设计目标&#34;&gt;设计目标&lt;/h1&gt;

&lt;p&gt;Istio 最根本的设计目标就是实现 Service Mesh 的设计构想&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将“应用程序”与“网络”解藕: 将服务之间、服务与集群外部的网络通讯和安全机制从微服务的业务逻辑中解藕，并作为一个与平台无关的、独立运行的程序，以减少开发和运维人员的工作量。&lt;/li&gt;
&lt;li&gt;保障网络环节: 应用程序的目标是“将某些东西从A传送到B”，而 Service Mesh 所要做的就是实现这个目标，并处理传送过程中可能出现的任何故障。&lt;/li&gt;
&lt;li&gt;提供应用层面的可见性和可控性: 通过每个微服务中的 Sidecar ，Service Mesh 得以将服务间通信从底层的基础设施中分离出来，让它成为整个生态系统的一个独立部分——它不再是单纯的基础设施，更可以被监控、托管和控制。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此之外，Istio 还有着其他更加关键的设计目标，这些目标对于使系统能够应对大规模流量和高性能地服务处理至关重要。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;最大化透明度( 与“解藕”类似，但具体到基于 Pod 实现 ): Istio 使用 Sidecar 代理来捕获流量，并且在尽可能的地方自动编程网络层，以路由流量通过这些代理，而无需对已部署的应用程序代码进行任何改动。注入 sidecar 代理到 pod 中并且修改路由规则后，Istio 就能够调解所有流量。&lt;/li&gt;
&lt;li&gt;增量扩容策略: 扩展策略系统，集成其他策略和控制来源，并将网格行为信号传播到其他系统进行分析。策略运行时支持标准扩展机制以便插入到其他服务中。&lt;/li&gt;
&lt;li&gt;可移植性: Istio 必须能够以最少的代价运行在任何云或预置环境中。将基于 Istio 的服务移植到新环境应该是轻而易举的，而使用 Istio 将一个服务同时部署到多个环境中也是可行的（例如，在多个云上进行冗余部署）。&lt;/li&gt;
&lt;li&gt;策略一致性: 在服务间的 API 调用中，策略的应用使得可以对网格间行为进行全面的控制，但对于无需在 API 级别表达的资源来说，对资源应用策略也同样重要。策略系统作为独特的服务来维护，具有自己的 API，而不是将其放到代理或 Sidecar 中，这容许服务根据需要直接与其集成。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;核心功能&#34;&gt;核心功能&lt;/h1&gt;

&lt;p&gt;基于以上的设计目标，Istio 的核心功能有以下五点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;流量管理: Istio 通过 Pilot 所提供的 API 动态地配置所有 Pod 中 Sidecar 的路由规则，进而控制服务间的流量和 API 调用。Istio 简化了断路器、超时和重试等服务级别属性的配置，并且可以轻松设置 A/B 测试、金丝雀部署和基于百分比的流量分割的分阶段部署等重要任务。&lt;/li&gt;
&lt;li&gt;安全: Istio 提供给开发人员应用程序级别的安全性。Istio 提供底层安全通信信道，并大规模管理服务通信的认证、授权和加密。使用 Istio ，服务通信在默认情况下是安全的，它允许跨多种协议和运行时一致地实施策略——所有这些都很少或根本不需要应用程序更改。将 Istio 与 Kubernetes 的网络策略结合使用，其优势会更大，包括在网络和应用层保护 Pod 间或服务间通信的能力。&lt;/li&gt;
&lt;li&gt;可观察性: Istio 的 Mixer 组件负责策略控制和遥测收集。通过 Istio 的监控功能，可以了解服务性能如何影响上游和下游的功能；其自定义仪表板可以提供对所有服务性能的可视化，从而了解性能如何影响其他进程。&lt;/li&gt;
&lt;li&gt;平台独立: Istio 是独立于平台的，旨在运行在各种环境中，包括跨云、内部部署、Kubernetes、Mesos 等。您可以在 Kubernetes 上部署 Istio 或具有 Consul 的 Nomad 上部署。&lt;/li&gt;
&lt;li&gt;集成和定制: 策略执行组件可以扩展和定制，以便与现有的 ACL、日志、监控、配额、审计等方案集成。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基本概念和使用&#34;&gt;基本概念和使用&lt;/h1&gt;

&lt;h2 id=&#34;流量管理&#34;&gt;流量管理&lt;/h2&gt;

&lt;p&gt;流量管理包括服务发现和负载均衡（服务发现和负载均衡是流控的基本手段）的很多对流量进行控制和更细粒度规划的API：虚拟服务，目标规划，网关，服务入口，sidecar。这些API使用 Kubernetes 的自定义资源定义（CRDs）来声明使用。&lt;/p&gt;

&lt;h3 id=&#34;虚拟服务&#34;&gt;虚拟服务&lt;/h3&gt;

&lt;p&gt;虚拟服务（Virtual Service） 和目标规则（Destination Rule） 是 Istio 流量路由功能的关键，虚拟服务使得可以配置如何在服务网格内将请求路由到服务。每个虚拟服务包含一组路由规则，Istio 按顺序评估它们，Istio 将每个给定的请求匹配到虚拟服务指定的实际目标地址。&lt;/p&gt;

&lt;p&gt;如果没有虚拟服务，Envoy 会在所有的服务实例中使用轮询的负载均衡策略分发请求。在虚拟服务中使用路由规则，告诉 Envoy 如何发送虚拟服务的流量到适当的目标。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面的虚拟服务根据请求是否来自特定的用户，把它们路由到服务的不同版本。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: reviews
        subset: v2
  - route:
    - destination:
        host: reviews
        subset: v3
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;hosts：用户指定的目标或是路由规则设定的目标。这是客户端向服务发送请求时使用的一个或多个地址。可以是 IP 地址、DNS 名称，或者依赖于平台的一个简称（例如 Kubernetes 服务的短名称），隐式或显式地指向一个完全限定域名（FQDN）。&lt;/li&gt;
&lt;li&gt;http：需要路由的协议&lt;/li&gt;
&lt;li&gt;match：路由的匹配条件，在本例中，您希望此路由应用于来自 ”jason“ 用户的所有请求，所以使用 headers、end-user 和 exact 字段选择适当的请求。&lt;/li&gt;
&lt;li&gt;destination：符合此条件的流量的实际目标地址，与虚拟服务的 hosts 不同，destination 的 host 必须是存在于 Istio 服务注册中心的实际目标地址，否则 Envoy 不知道该将请求发送到哪里。&lt;/li&gt;
&lt;li&gt;路由规则按从上到下的顺序选择，虚拟服务中定义的第一条规则有最高优先级。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这边只是一个简单的实例，其实路由规则是将特定流量子集路由到指定目标地址的强大工具。您可以在流量端口、header 字段、URI 等内容上设置匹配条件，有些匹配条件可以使用精确的值，如前缀或正则，使用匹配条件您可以按百分比”权重“分发请求等都是很有用的功能。&lt;/p&gt;

&lt;h3 id=&#34;目标规则&#34;&gt;目标规则&lt;/h3&gt;

&lt;p&gt;目标规则是配置该目标的流量，在评估虚拟服务路由规则之后，目标规则将应用于流量的“真实”目标地址。&lt;/p&gt;

&lt;p&gt;默认情况下，Istio 使用轮询的负载均衡策略，当然我们还可以使用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;随机：请求以随机的方式转到池中的实例。&lt;/li&gt;
&lt;li&gt;权重：请求根据指定的百分比转到实例。&lt;/li&gt;
&lt;li&gt;最少请求：请求被转到最少被访问的实例。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在下面的示例中，目标规则为 my-svc 目标服务配置了 3 个具有不同负载均衡策略的子集：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: my-destination-rule
spec:
  host: my-svc
  trafficPolicy:
    loadBalancer:
      simple: RANDOM
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
  - name: v3
    labels:
      version: v3
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;lables：这些标签应用于 Kubernetes 服务的 Deployment 并作为 metadata 来识别不同的版本。&lt;/li&gt;
&lt;li&gt;目标规则对于所有子集都有默认的流量策略，而对于该子集，则有特定于子集的策略覆盖它。定义在 subsets 上的默认策略，为 v1 和 v3 子集设置了一个简单的随机负载均衡器。在 v2 策略中，轮询负载均衡器被指定在相应的子集字段上。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;网关&#34;&gt;网关&lt;/h3&gt;

&lt;p&gt;网关是网格来管理入站和出站流量，可以让指定要进入或离开网格的流量。&lt;/p&gt;

&lt;p&gt;网关配置被用于运行在网格边界的独立 Envoy 代理，而不是服务工作负载的 sidecar 代理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面的示例展示了一个外部 HTTPS 入口流量的网关配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: ext-host-gwy
spec:
  selector:
    app: my-gateway-controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    hosts:
    - ext-host.example.com
    tls:
      mode: SIMPLE
      serverCertificate: /tmp/tls.crt
      privateKey: /tmp/tls.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个网关配置让 HTTPS 流量从 ext-host.example.com 通过 443 端口流入网格，但没有为请求指定任何路由规则。&lt;/p&gt;

&lt;p&gt;如果想要工作的网关指定路由，您必须把网关绑定到虚拟服务上。正如下面的示例所示，使用虚拟服务的 gateways 字段进行设置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: virtual-svc
spec:
  hosts:
  - ext-host.example.com
  gateways:
    - ext-host-gwy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以为出口流量配置带有路由规则的虚拟服务。&lt;/p&gt;

&lt;h3 id=&#34;服务入口&#34;&gt;服务入口&lt;/h3&gt;

&lt;p&gt;服务入口（Service Entry） 就是添加一个入口到 Istio 内部维护的服务注册中心。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;为外部目标 redirect 和转发请求，例如来自 web 端的 API 调用，或者流向遗留老系统的服务。&lt;/li&gt;
&lt;li&gt;为外部目标定义重试、超时和故障注入策略。&lt;/li&gt;
&lt;li&gt;添加一个运行在虚拟机的服务来扩展您的网格。&lt;/li&gt;
&lt;li&gt;从逻辑上添加来自不同集群的服务到网格，在 Kubernetes 上实现一个多集群 Istio 网格。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面示例的 mesh-external 服务入口将 ext-resource 外部依赖项添加到 Istio 的服务注册中心&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: svc-entry
spec:
  hosts:
  - ext-svc.example.com
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  location: MESH_EXTERNAL
  resolution: DNS
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;hosts：指定的外部资源，可以使用完全限定名或通配符作为前缀域名。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;sidecar&#34;&gt;sidecar&lt;/h3&gt;

&lt;p&gt;可以指定将 sidecar 配置应用于特定命名空间中的所有工作负载，或者使用 workloadSelector 选择特定的工作负载。例如，下面的 sidecar 配置将 bookinfo 命名空间中的所有服务配置为仅能访问运行在相同命名空间和 Istio 控制平面中的服务（目前需要使用 Istio 的策略和遥测功能）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: Sidecar
metadata:
  name: default
  namespace: bookinfo
spec:
  egress:
  - hosts:
    - &amp;quot;./*&amp;quot;
    - &amp;quot;istio-system/*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;故障恢复和故障注入&#34;&gt;故障恢复和故障注入&lt;/h3&gt;

&lt;p&gt;除了为您的网格导流之外，Istio 还提供了可选的故障恢复和故障注入功能，您可以在运行时动态配置这些功能。使用这些特性可以让您的应用程序运行稳定，确保服务网格能够容忍故障节点，并防止局部故障级联影响到其他节点。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;超时&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;超时是 Envoy 代理等待来自给定服务的答复的时间量，以确保服务不会因为等待答复而无限期的挂起，并在可预测的时间范围内调用成功或失败。HTTP 请求的默认超时时间是 15 秒，这意味着如果服务在 15 秒内没有响应，调用将失败。&lt;/p&gt;

&lt;p&gt;下面的示例是一个虚拟服务，它对 ratings 服务的 v1 子集的调用指定 10 秒超时：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
spec:
  hosts:
  - ratings
  http:
  - route:
    - destination:
        host: ratings
        subset: v1
    timeout: 10s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;重试&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;重试设置指定如果初始调用失败，Envoy 代理尝试连接服务的最大次数。通过确保调用不会因为临时过载的服务或网络等问题而永久失败，重试可以提高服务可用性和应用程序的性能。重试之间的间隔（25ms+）是可变的，并由 Istio 自动确定，从而防止被调用服务被请求淹没。默认情况下，在第一次失败后，Envoy 代理不会重新尝试连接服务。&lt;/p&gt;

&lt;p&gt;下面的示例配置了在初始调用失败后最多重试 3 次来连接到服务子集，每个重试都有 2 秒的超时。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
spec:
  hosts:
  - ratings
  http:
  - route:
    - destination:
        host: ratings
        subset: v1
    retries:
      attempts: 3
      perTryTimeout: 2s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;熔断&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;熔断器是 Istio 为创建具有弹性的微服务应用提供的另一个有用的机制。在熔断器中，设置一个对服务中的单个主机调用的限制，例如并发连接的数量或对该主机调用失败的次数。一旦限制被触发，熔断器就会“跳闸”并停止连接到该主机。使用熔断模式可以快速失败而不必让客户端尝试连接到过载或有故障的主机。&lt;/p&gt;

&lt;p&gt;熔断适用于在负载均衡池中的“真实”网格目标地址，您可以在目标规则中配置熔断器阈值，让配置适用于服务中的每个主机。下面的示例将 v1 子集的reviews服务工作负载的并发连接数限制为 100：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: reviews
spec:
  host: reviews
  subsets:
  - name: v1
    labels:
      version: v1
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 100
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;故障注入&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;故障注入是一种将错误引入系统以确保系统能够承受并从错误条件中恢复的测试方法。&lt;/p&gt;

&lt;p&gt;您可以注入两种故障，它们都使用虚拟服务配置：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;延迟：延迟是时间故障。它们模拟增加的网络延迟或一个超载的上游服务。&lt;/li&gt;
&lt;li&gt;终止：终止是崩溃失败。他们模仿上游服务的失败。终止通常以 HTTP 错误码或 TCP 连接失败的形式出现。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面的虚拟服务为千分之一的访问 ratings 服务的请求配置了一个 5 秒的延迟：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
spec:
  hosts:
  - ratings
  http:
  - fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
    route:
    - destination:
        host: ratings
        subset: v1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;扩展性&#34;&gt;扩展性&lt;/h2&gt;

&lt;p&gt;扩展主要是对envoy的扩展。&lt;/p&gt;

&lt;h2 id=&#34;安全&#34;&gt;安全&lt;/h2&gt;

&lt;p&gt;Istio 安全功能提供强大的身份，强大的策略，透明的 TLS 加密，认证，授权和审计（AAA）工具来保护你的服务和数据。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;身份&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;身份是任何安全基础架构的基本概念。在工作负载间通信开始时，双方必须交换包含身份信息的凭证以进行双向验证。在客户端，根据安全命名信息检查服务器的标识，以查看它是否是该服务的授权运行程序。在服务器端，服务器可以根据授权策略确定客户端可以访问哪些信息，审计谁在什么时间访问了什么，根据他们使用的工作负载向客户收费，并拒绝任何未能支付账单的客户访问工作负载。&lt;/p&gt;

&lt;p&gt;Istio 身份模型使用 service identity （服务身份）来确定一个请求源端的身份。这种模型有极好的灵活性和粒度，可以用服务身份来标识人类用户、单个工作负载或一组工作负载。&lt;/p&gt;

&lt;p&gt;在不同平台上可以使用的服务身份：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Kubernetes: Kubernetes service account
GKE/GCE: GCP service account
GCP: GCP service account
AWS: AWS IAM user/role account
本地（非 Kubernetes）：用户帐户、自定义服务帐户、服务名称、Istio 服务帐户或 GCP 服务帐户。自定义服务帐户引用现有服务帐户，就像客户的身份目录管理的身份一样。
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;认证&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Istio 提供两种类型的认证：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Peer authentication：用于服务到服务的认证，以验证进行连接的客户端。Istio 提供双向 TLS 作为传输认证的全栈解决方案，无需更改服务代码就可以启用它。&lt;/li&gt;
&lt;li&gt;Request authentication：用于最终用户认证，以验证附加到请求的凭据。 Istio 使用 JSON Web Token（JWT）验证启用请求级认证，并使用自定义认证实现或任何 OpenID Connect 的认证实现（例如下面列举的）来简化的开发人员体验。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1、双向TLS认证&lt;/p&gt;

&lt;p&gt;当一个工作负载使用双向 TLS 认证向另一个工作负载发送请求时，该请求的处理方式如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Istio 将出站流量从客户端重新路由到客户端的本地 sidecar Envoy。&lt;/li&gt;
&lt;li&gt;客户端 Envoy 与服务器端 Envoy 开始双向 TLS 握手。在握手期间，客户端 Envoy 还做了安全命名检查，以验证服务器证书中显示的服务帐户是否被授权运行目标服务。&lt;/li&gt;
&lt;li&gt;客户端 Envoy 和服务器端 Envoy 建立了一个双向的 TLS 连接，Istio 将流量从客户端 Envoy 转发到服务器端 Envoy。&lt;/li&gt;
&lt;li&gt;授权后，服务器端 Envoy 通过本地 TCP 连接将流量转发到服务器服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;宽容模式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Istio 双向 TLS 具有一个宽容模式（permissive mode），允许服务同时接受纯文本流量和双向 TLS 流量。这样对于将服务移植到启用了双向 TLS 的 Istio 上时，许多非 Istio 客户端和非 Istio 服务端通信时会产生问题有了一个解决方案。&lt;/p&gt;

&lt;p&gt;通常情况下，运维人员无法同时为所有客户端安装 Istio sidecar，甚至没有这样做的权限。即使在服务端上安装了 Istio sidecar，运维人员也无法在不中断现有连接的情况下启用双向 TLS。启用宽容模式后，服务可以同时接受纯文本和双向 TLS 流量。这个模式为入门提供了极大的灵活性。服务中安装的 Istio sidecar 立即接受双向 TLS 流量而不会打断现有的纯文本流量。因此，运维人员可以逐步安装和配置客户端 Istio sidecar 发送双向 TLS 流量。一旦客户端配置完成，运维人员便可以将服务端配置为仅 TLS 模式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;安全命名&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;安全命名能够防止 HTTPS 流量受到一般性网络劫持，除了 DNS 欺骗外，它还可以保护 TCP 流量免受一般网络劫持。如果攻击者劫持了 DNS 并修改了目的地的 IP 地址，它将无法用于 TCP 通信。这是因为 TCP 流量不包含主机名信息，我们只能依靠 IP 地址进行路由，而且甚至在客户端 Envoy 收到流量之前，也可能发生 DNS 劫持。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;认证架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用 peer 和 request 认证策略为在 Istio 网格中接收请求的工作负载指定认证要求。网格运维人员使用 .yaml 文件来指定策略。部署后，策略将保存在 Istio 配置存储中。Istio 控制器监视配置存储。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/istio/istio1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;授权&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;您无需显式启用 Istio 的授权功能。只需将授权策略应用于工作负载即可实施访问控制。对于未应用授权策略的工作负载，Istio 不会执行访问控制，放行所有请求。&lt;/p&gt;

&lt;p&gt;授权策略支持 ALLOW 和 DENY 动作。 拒绝策略优先于允许策略。如果将任何允许策略应用于工作负载，则默认情况下将拒绝对该工作负载的访问，除非策略中的规则明确允许。当您将多个授权策略应用于相同的工作负载时，Istio 会累加地应用它们。&lt;/p&gt;

&lt;p&gt;要配置授权策略，请创建一个 AuthorizationPolicy 自定义资源。 一个授权策略包括选择器（selector），动作（action） 和一个规则（rules）列表：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;selector 字段指定策略的目标&lt;/li&gt;
&lt;li&gt;action 字段指定允许还是拒绝请求&lt;/li&gt;
&lt;li&gt;rules 指定何时触发动作

&lt;ul&gt;
&lt;li&gt;rules 下的 from 字段指定请求的来源&lt;/li&gt;
&lt;li&gt;rules 下的 to 字段指定请求的操作&lt;/li&gt;
&lt;li&gt;rules 下的 when 字段指定应用规则所需的条件&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以下示例显示了一个授权策略，该策略允许两个源（服务帐号 cluster.local/ns/default/sa/sleep 和命名空间 dev），在使用有效的 JWT 令牌发送请求时，可以访问命名空间 foo 中的带有标签 app: httpbin 和 version: v1 的工作负载。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: httpbin
 namespace: foo
spec:
 selector:
   matchLabels:
     app: httpbin
     version: v1
 action: ALLOW
 rules:
 - from:
   - source:
       principals: [&amp;quot;cluster.local/ns/default/sa/sleep&amp;quot;]
   - source:
       namespaces: [&amp;quot;dev&amp;quot;]
   to:
   - operation:
       methods: [&amp;quot;GET&amp;quot;]
   when:
   - key: request.auth.claims[iss]
     values: [&amp;quot;https://accounts.google.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下例显示了一个授权策略，如果请求来源不是命名空间 foo，请求将被拒绝。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: httpbin-deny
 namespace: foo
spec:
 selector:
   matchLabels:
     app: httpbin
     version: v1
 action: DENY
 rules:
 - from:
   - source:
       notNamespaces: [&amp;quot;foo&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;拒绝策略优先于允许策略。如果请求同时匹配上允许策略和拒绝策略，请求将被拒绝。Istio 首先评估拒绝策略，以确保允许策略不能绕过拒绝策略。很多使用匹配可以查看&lt;a href=&#34;https://istio.io/latest/zh/docs/concepts/security/#authentication-architecture&#34;&gt;官网&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;授权架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每个 Envoy 代理都运行一个授权引擎，该引擎在运行时授权请求。当请求到达代理时，授权引擎根据当前授权策略评估请求上下文，并返回授权结果 ALLOW 或 DENY。 运维人员使用 .yaml 文件指定 Istio 授权策略。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/istio/istio2.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;可视化&#34;&gt;可视化&lt;/h2&gt;

&lt;p&gt;stio 生成以下类型的遥测数据，以提供对整个服务网格的可观察性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;指标。Istio 基于 4 个监控的黄金标识（延迟、流量、错误、饱和）生成了一系列服务指标。Istio 还为网格控制平面提供了更详细的指标。除此以外还提供了一组默认的基于这些指标的网格监控仪表板。&lt;/li&gt;
&lt;li&gt;分布式追踪。Istio 为每个服务生成分布式追踪 span，运维人员可以理解网格内服务的依赖和调用流程。&lt;/li&gt;
&lt;li&gt;访问日志。当流量流入网格中的服务时，Istio 可以生成每个请求的完整记录，包括源和目标的元数据。此信息使运维人员能够将服务行为的审查控制到单个工作负载实例的级别。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;安装部署&#34;&gt;安装部署&lt;/h1&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;下载 Istio&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、访问 Istio release 页面下载与您操作系统对应的安装文件。在 macOS 或 Linux 系统中，也可以通过以下命令下载最新版本的 Istio：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -L https://istio.io/downloadIstio | sh -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;切换到 Istio 包所在目录下。例如：Istio 包名为 istio-1.6.8，则：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd istio-1.6.8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装目录包含如下内容：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;install/kubernetes 目录下，有 Kubernetes 相关的 YAML 安装文件&lt;/li&gt;
&lt;li&gt;samples/ 目录下，有示例应用程序&lt;/li&gt;
&lt;li&gt;bin/ 目录下，包含 istioctl 的客户端文件。istioctl 工具用于手动注入 Envoy sidecar 代理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、将 istioctl 客户端路径增加到 path 环境变量中，macOS 或 Linux 系统的增加方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export PATH=$PWD/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在使用 bash 或 ZSH 控制台时，可以选择启动 auto-completion option。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;安装 Istio&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、安装 demo 配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ istioctl manifest apply --set profile=demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了验证是否安装成功，需要先确保以下 Kubernetes 服务正确部署，然后验证除 jaeger-agent 服务外的其他服务，是否均有正确的 CLUSTER-IP：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc -n istio-system
NAME                     TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                                                                                                                      AGE
grafana                  ClusterIP      172.21.211.123   &amp;lt;none&amp;gt;          3000/TCP                                                                                                                                     2m
istio-citadel            ClusterIP      172.21.177.222   &amp;lt;none&amp;gt;          8060/TCP,15014/TCP                                                                                                                           2m
istio-egressgateway      ClusterIP      172.21.113.24    &amp;lt;none&amp;gt;          80/TCP,443/TCP,15443/TCP                                                                                                                     2m
istio-galley             ClusterIP      172.21.132.247   &amp;lt;none&amp;gt;          443/TCP,15014/TCP,9901/TCP                                                                                                                   2m
istio-ingressgateway     LoadBalancer   172.21.144.254   52.116.22.242   15020:31831/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:30318/TCP,15030:32645/TCP,15031:31933/TCP,15032:31188/TCP,15443:30838/TCP   2m
istio-pilot              ClusterIP      172.21.105.205   &amp;lt;none&amp;gt;          15010/TCP,15011/TCP,8080/TCP,15014/TCP                                                                                                       2m
istio-policy             ClusterIP      172.21.14.236    &amp;lt;none&amp;gt;          9091/TCP,15004/TCP,15014/TCP                                                                                                                 2m
istio-sidecar-injector   ClusterIP      172.21.155.47    &amp;lt;none&amp;gt;          443/TCP,15014/TCP                                                                                                                            2m
istio-telemetry          ClusterIP      172.21.196.79    &amp;lt;none&amp;gt;          9091/TCP,15004/TCP,15014/TCP,42422/TCP                                                                                                       2m
jaeger-agent             ClusterIP      None             &amp;lt;none&amp;gt;          5775/UDP,6831/UDP,6832/UDP                                                                                                                   2m
jaeger-collector         ClusterIP      172.21.135.51    &amp;lt;none&amp;gt;          14267/TCP,14268/TCP                                                                                                                          2m
jaeger-query             ClusterIP      172.21.26.187    &amp;lt;none&amp;gt;          16686/TCP                                                                                                                                    2m
kiali                    ClusterIP      172.21.155.201   &amp;lt;none&amp;gt;          20001/TCP                                                                                                                                    2m
prometheus               ClusterIP      172.21.63.159    &amp;lt;none&amp;gt;          9090/TCP                                                                                                                                     2m
tracing                  ClusterIP      172.21.2.245     &amp;lt;none&amp;gt;          80/TCP                                                                                                                                       2m
zipkin                   ClusterIP      172.21.182.245   &amp;lt;none&amp;gt;          9411/TCP                                                                                                                                     2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果集群运行在一个不支持外部负载均衡器的环境中（例如：minikube），istio-ingressgateway 的 EXTERNAL-IP 将显示为 &lt;pending&gt; 状态。请使用服务的 NodePort 或 端口转发来访问网关。&lt;/p&gt;

&lt;p&gt;2、请确保关联的 Kubernetes pod 已经部署，并且 STATUS 为 Running：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -n istio-system
NAME                                                           READY   STATUS      RESTARTS   AGE
grafana-f8467cc6-rbjlg                                         1/1     Running     0          1m
istio-citadel-78df5b548f-g5cpw                                 1/1     Running     0          1m
istio-egressgateway-78569df5c4-zwtb5                           1/1     Running     0          1m
istio-galley-74d5f764fc-q7nrk                                  1/1     Running     0          1m
istio-ingressgateway-7ddcfd665c-dmtqz                          1/1     Running     0          1m
istio-pilot-f479bbf5c-qwr28                                    1/1     Running     0          1m
istio-policy-6fccc5c868-xhblv                                  1/1     Running     2          1m
istio-sidecar-injector-78499d85b8-x44m6                        1/1     Running     0          1m
istio-telemetry-78b96c6cb6-ldm9q                               1/1     Running     2          1m
istio-tracing-69b5f778b7-s2zvw                                 1/1     Running     0          1m
kiali-99f7467dc-6rvwp                                          1/1     Running     0          1m
prometheus-67cdb66cbb-9w2hm                                    1/1     Running     0          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、安装 Istio 后，就可以部署您自己的服务，或部署安装程序中系统的任意一个示例应用。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;应用程序必须使用 HTTP/1.1 或 HTTP/2.0 协议用于 HTTP 通信；HTTP/1.0 不支持。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;当使用 kubectl apply 来部署应用时，如果 pod 启动在标有 istio-injection=enabled 的命名空间中，那么，Istio sidecar 注入器将自动注入 Envoy 容器到应用的 pod 中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl label namespace &amp;lt;namespace&amp;gt; istio-injection=enabled
$ kubectl create -n &amp;lt;namespace&amp;gt; -f &amp;lt;your-app-spec&amp;gt;.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在没有 istio-injection 标记的命名空间中，在部署前可以使用 istioctl kube-inject 命令将 Envoy 容器手动注入到应用的 pod 中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ istioctl kube-inject -f &amp;lt;your-app-spec&amp;gt;.yaml | kubectl apply -f -
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;卸载&#34;&gt;卸载&lt;/h2&gt;

&lt;p&gt;卸载程序将删除 RBAC 权限、istio-system 命名空间和所有相关资源。可以忽略那些不存在的资源的报错，因为它们可能已经被删除掉了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ istioctl manifest generate --set profile=demo | kubectl delete -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;p&gt;虽然istio是与平台无关的，但是我们大部分都是只是结合k8s平台应用，其他平台可以忽略。&lt;/p&gt;

&lt;h1 id=&#34;实战&#34;&gt;实战&lt;/h1&gt;

&lt;h2 id=&#34;bookinfo&#34;&gt;bookinfo&lt;/h2&gt;

&lt;p&gt;我们通过一个应用Bookinfo来展示istio的功能。这个应用模仿在线书店的一个分类，显示一本书的信息。 页面上会显示一本书的描述，书籍的细节（ISBN、页数等），以及关于这本书的一些评论。&lt;/p&gt;

&lt;p&gt;Bookinfo 应用分为四个单独的微服务：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;productpage. 这个微服务会调用 details 和 reviews 两个微服务，用来生成页面。&lt;/li&gt;
&lt;li&gt;details. 这个微服务中包含了书籍的信息。&lt;/li&gt;
&lt;li&gt;reviews. 这个微服务中包含了书籍相关的评论。它还会调用 ratings 微服务。&lt;/li&gt;
&lt;li&gt;ratings. 这个微服务中包含了由书籍评价组成的评级信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;reviews 微服务有 3 个版本：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;v1 版本不会调用 ratings 服务。&lt;/li&gt;
&lt;li&gt;v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。&lt;/li&gt;
&lt;li&gt;v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图展示了这个应用的端到端架构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/istio/istio.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Bookinfo 应用中的几个微服务是由不同的语言编写的。 这些服务对 Istio 并无依赖，但是构成了一个有代表性的服务网格的例子：它由多个服务、多个语言构成，并且 reviews 服务具有多个版本，很具有代表性。&lt;/p&gt;

&lt;h2 id=&#34;部署-1&#34;&gt;部署&lt;/h2&gt;

&lt;p&gt;1、进入 Istio 安装目录。Istio 默认自动注入 Sidecar. 请为 default 命名空间打上标签 istio-injection=enabled：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl label namespace default istio-injection=enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、使用 kubectl 部署应用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的命令会启动全部的四个服务，其中也包括了 reviews 服务的三个版本（v1、v2 以及 v3）。&lt;/p&gt;

&lt;p&gt;3、确认所有的服务和 Pod 都已经正确的定义和启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services
NAME                       CLUSTER-IP   EXTERNAL-IP   PORT(S)              AGE
details                    10.0.0.31    &amp;lt;none&amp;gt;        9080/TCP             6m
kubernetes                 10.0.0.1     &amp;lt;none&amp;gt;        443/TCP              7d
productpage                10.0.0.120   &amp;lt;none&amp;gt;        9080/TCP             6m
ratings                    10.0.0.15    &amp;lt;none&amp;gt;        9080/TCP             6m
reviews                    10.0.0.170   &amp;lt;none&amp;gt;        9080/TCP             6m

$ kubectl get pods
NAME                                        READY     STATUS    RESTARTS   AGE
details-v1-1520924117-48z17                 2/2       Running   0          6m
productpage-v1-560495357-jk1lz              2/2       Running   0          6m
ratings-v1-734492171-rnr5l                  2/2       Running   0          6m
reviews-v1-874083890-f0qf0                  2/2       Running   0          6m
reviews-v2-1343845940-b34q5                 2/2       Running   0          6m
reviews-v3-1813607990-8ch52                 2/2       Running   0          6m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、确认 Bookinfo 应用是否正在运行，请在某个 Pod 中用 curl 命令对应用发送请求，例如 ratings：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -c ratings -- curl productpage:9080/productpage | grep -o &amp;quot;&amp;lt;title&amp;gt;.*&amp;lt;/title&amp;gt;&amp;quot;
&amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、现在 Bookinfo 服务启动并运行中，需要使应用程序可以从外部访问 Kubernetes 集群，例如使用浏览器。可以用 Istio Gateway 来实现这个目标。为应用程序定义 Ingress 网关：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、确认网关创建完成：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get gateway
NAME               AGE
bookinfo-gateway   32s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7、根据文档设置访问网关的 INGRESS_HOST 和 INGRESS_PORT 变量。确认并设置。&lt;/p&gt;

&lt;p&gt;设置 GATEWAY_URL：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8、可以用 curl 命令来确认是否能够从集群外部访问 Bookinfo 应用程序：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -s http://${GATEWAY_URL}/productpage | grep -o &amp;quot;&amp;lt;title&amp;gt;.*&amp;lt;/title&amp;gt;&amp;quot;
&amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以用浏览器打开网址 http://$GATEWAY_URL/productpage，来浏览应用的 Web 页面。如果刷新几次应用的页面，就会看到 productpage 页面中会随机展示 reviews 服务的不同版本的效果（红色、黑色的星形或者没有显示）。reviews 服务出现这种情况是因为我们还没有使用 Istio 来控制版本的路由。&lt;/p&gt;

&lt;p&gt;9、在使用 Istio 控制 Bookinfo 版本路由之前，您需要在目标规则中定义好可用的版本，命名为 subsets，运行以下命令为 Bookinfo 服务创建的默认的目标规则：&lt;/p&gt;

&lt;p&gt;如果没有启用双向 TLS，请执行以下命令：&lt;/p&gt;

&lt;p&gt;如果您是 Istio 的新手，并且使用了 demo 配置文件，请选择此步。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果启用了双向 TLS，请执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等待几秒钟，以使目标规则生效。&lt;/p&gt;

&lt;p&gt;您可以使用以下命令查看目标规则：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get destinationrules -o yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10、到此部署就完成了，可以对这个系统进行其他的流控的操作，我们继续说明，我先看一下如何清理&lt;/p&gt;

&lt;p&gt;删除路由规则，并销毁应用的 Pod&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ samples/bookinfo/platform/kube/cleanup.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确认应用已经关停&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get virtualservices   #-- there should be no virtual services
$ kubectl get destinationrules  #-- there should be no destination rules
$ kubectl get gateway           #-- there should be no gateway
$ kubectl get pods              #-- the Bookinfo pods should be deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;流量管理-1&#34;&gt;流量管理&lt;/h2&gt;

&lt;p&gt;流量管理是基本功能，也就是我们常说的流控，主要包括&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置请求路由：将请求对应的动态路由到服务&lt;/li&gt;
&lt;li&gt;故障注入：注入故障来测试应用&lt;/li&gt;
&lt;li&gt;流量转移：将流量从旧版本迁移到新版本中&lt;/li&gt;
&lt;li&gt;设置超时时间&lt;/li&gt;
&lt;li&gt;熔断&lt;/li&gt;
&lt;li&gt;Ingress：控制入口流量&lt;/li&gt;
&lt;li&gt;Engress：控制出口流量&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;配置请求路由&#34;&gt;配置请求路由&lt;/h3&gt;

&lt;h2 id=&#34;安全-1&#34;&gt;安全&lt;/h2&gt;

&lt;p&gt;安全是网络服务中必要的功能，能够保护数据交互的安全。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;认证：管控网格服务间进行双向TLS认证和终端用户身份认证。&lt;/li&gt;
&lt;li&gt;授权：给用户身份角色，控制访问。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;策略&#34;&gt;策略&lt;/h2&gt;

&lt;p&gt;网络通信的一些功能，主要是完成一些逻辑的处理。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;速率限制：动态的限制流量&lt;/li&gt;
&lt;li&gt;黑白名单：通过黑白名单来控制访问&lt;/li&gt;
&lt;li&gt;修改请求头和路由&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;可视化-1&#34;&gt;可视化&lt;/h2&gt;

&lt;p&gt;就是让服务网格内部能够通过检测数据一目了然的看出来，便于控制和定位问题。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;指标：网格指标度量的配置、收集和处理。&lt;/li&gt;
&lt;li&gt;log： Istio 网格日志的配置、收集和处理。&lt;/li&gt;
&lt;li&gt;分布式追踪：服务调用链路的监控&lt;/li&gt;
&lt;li&gt;可视化： Istio 网格中可视化服务&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/istio/istio3.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Envoy&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Envoy 是用 C++ 开发的高性能代理，用于协调服务网格中所有服务的入站和出站流量。&lt;/p&gt;

&lt;p&gt;Envoy 代理是唯一与数据平面流量交互的 Istio 组件。&lt;/p&gt;

&lt;p&gt;Envoy 代理被部署为服务的 sidecar，这种 sidecar 部署允许 Istio 提取大量关于流量行为的信号作为属性。Istio 可以使用这些属性来实施策略决策，并将其发送到监视系统以提供有关整个网格行为的信息。在逻辑上为服务增加了 Envoy 的许多内置特性，例如:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;动态服务发现&lt;/li&gt;
&lt;li&gt;负载均衡&lt;/li&gt;
&lt;li&gt;TLS 终端&lt;/li&gt;
&lt;li&gt;HTTP/2 与 gRPC 代理&lt;/li&gt;
&lt;li&gt;熔断器&lt;/li&gt;
&lt;li&gt;健康检查&lt;/li&gt;
&lt;li&gt;基于百分比流量分割的分阶段发布&lt;/li&gt;
&lt;li&gt;故障注入&lt;/li&gt;
&lt;li&gt;丰富的指标&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上面这些特性可以完成下面的基本功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;流量控制功能：通过丰富的 HTTP、gRPC、WebSocket 和 TCP 流量路由规则来执行细粒度的流量控制。&lt;/li&gt;
&lt;li&gt;网络弹性特性：重试设置、故障转移、熔断器和故障注入。&lt;/li&gt;
&lt;li&gt;安全性和身份验证特性：执行安全性策略以及通过配置 API 定义的访问控制和速率限制。&lt;/li&gt;
&lt;li&gt;基于 WebAssembly 的可插拔扩展模型，允许通过自定义策略实施和生成网格流量的遥测。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Pilot&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pilot 为 Envoy sidecar 提供服务发现、用于智能路由的流量管理功能（例如，A/B 测试、金丝雀发布等）以及弹性功能（超时、重试、熔断器等）。&lt;/p&gt;

&lt;p&gt;通过下图的交互可以看出pilot的服务注册和发现功能&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/istio/istio4.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;平台启动一个服务的新实例，该实例通知其平台适配器。&lt;/li&gt;
&lt;li&gt;平台适配器使用 Pilot 抽象模型注册实例。&lt;/li&gt;
&lt;li&gt;Pilot 将流量规则和配置派发给 Envoy 代理，来传达此次更改。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;citadel&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Citadel 通过内置的身份和证书管理，可以支持强大的服务到服务以及最终用户的身份验证。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Galley&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Galley 是 Istio 的配置验证、提取、处理和分发组件。它负责将其余的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节隔离开来。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;逻辑架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Istio 服务网格从逻辑上分为数据平面和控制平面。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据平面 由一组智能代理（Envoy）组成，被部署为 sidecar。这些代理负责协调和控制微服务之间的所有网络通信。他们还收集和报告所有网格流量的遥测数据。同时负责和控制平面进行交互。&lt;/li&gt;
&lt;li&gt;控制平面 管理并配置代理来进行流量路由。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;流量管理及请求路由&#34;&gt;流量管理及请求路由&lt;/h2&gt;

&lt;p&gt;将流量从应用程序中解藕，使得 Istio 能提供各种流量管理的功能，例如：动态路由( 负载均衡，A/B 测试，金丝雀部署 )、故障处理( 超时，重试，熔断器，故障恢复 )以及故障注入( 测试服务之间的故障恢复策略的兼容性 )。这些都是通过 Pilot 与 Sidecar 共同协作完成的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/istio/istio5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一个 Pilot 主要由“平台适配器”、“抽象模型”、用于配置和调用 Envoy 的“Envoy API” 和 用于用户指定流量管理规则的“Rules API”所组成。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;平台适配器( Platform Adapter )从 Kubernetes API server 中获取 Pod 的注册信息。( 注意：Istio 本身并不具备服务注册的功能，它需要通过平台适配器和特定的平台结合才能具有完整的服务发现的功能。)&lt;/li&gt;
&lt;li&gt;用户通过 Pilot 的 Rules API 对所有被发现的服务的 Sidecar 进行各种高级特性( 包括路由规则、HTTP层的流量管理等 )的配置&lt;/li&gt;
&lt;li&gt;用户配置的这些规则被抽象模型翻译成低级配置&lt;/li&gt;
&lt;li&gt;Sidecar API(即 Envoy API) 将这些翻译好的低级配置通过 discovery API 分发到每个微服务上的 SIdecar(即 Envoy) 实例中&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;服务间通讯&#34;&gt;服务间通讯&lt;/h2&gt;

&lt;p&gt;运维人员可以用 Pilot 指定路由规则，而 Envoy 根据这些规则动态地确定其服务版本的实际选择。Envoy 拦截并转发客户端和服务器之间的所有请求和相应。路由规则让 Envoy 能够根据诸如 header、source/destination 或分配给每个版本的权重等标准来进行版本选择。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/istio/istio6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Service A 访问不同版本的 Service B 的工作流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;运维人员通过 Polit 的 Rules API 根据 destination 的相关标签配置分流规则&lt;/li&gt;
&lt;li&gt;Service A 运行时，其 Envoy 的配置规则更新&lt;/li&gt;
&lt;li&gt;Service A 根据新配置的规则访问带有不同标签的 Service B 的版本&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;集群出入站规则&#34;&gt;集群出入站规则&lt;/h2&gt;

&lt;p&gt;Istio 默认进入和离开网络的所有流量都会通过 Envoy 进行传输。通过 Envoy 将流量路由到外部 Web 服务( 例如访问 Maps API 或 视频服务 API )的方式，运维人员可以为这些服务添加超时控制、重试、熔断器等功能；还能从服务连接中获得各种细节指标。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/istio/istio7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;流量从外网进入集群再流出的流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过 Kubernetes 的准入控制，外部流量进入 Istio&lt;/li&gt;
&lt;li&gt;流量与 Service A 和 Service B 的 Envoy 交互，并由 Envoy 获取服务具体相应的内容&lt;/li&gt;
&lt;li&gt;流量通过 Egress gateway 或 直接流出的方式流出 Istio&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;服务发现和负载均衡&#34;&gt;服务发现和负载均衡&lt;/h2&gt;

&lt;p&gt;Istio 的服务注册功能是基于平台来实现的。Istio 默认存在用于跟踪 Pod 的服务注册表，而且还默认新的服务自动注册，不健康的服务被自动删除。&lt;/p&gt;

&lt;p&gt;Pilot 使用服务注册的信息，并提供与平台无关的 discovery API。网格中的 Envoy 提供服务发现功能，并相应地动态更新负载均衡池。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/istio/istio8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个微服务的 Pod 通过 Kubernetes 提供的服务注册机制进行注册&lt;/li&gt;
&lt;li&gt;已注册的 Service A 的 Envoy 通过 Pilot 中汇总的 Envoy 信息发现欲访问的 Service B&lt;/li&gt;
&lt;li&gt;Service A 的 Envoy 根据设置好的负载均衡或流量管理规则，对 Service B 的 Envoy 发起请求&lt;/li&gt;
&lt;li&gt;Service B 的 Envoy 根据设置好的规则确认是否接收 Service A 发出的请求。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 分布式存储</title>
          <link>https://kingjcy.github.io/post/distributed/store/store/</link>
          <pubDate>Tue, 15 Dec 2020 20:21:18 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/store/</guid>
          <description>&lt;p&gt;分布式存储是相对于集中式存储来说的，分布式存储是一个大的概念，其包含的种类繁多，除了传统意义上的分布式文件系统、分布式块存储和分布式对象存储外，还包括分布式数据库和分布式缓存等。&lt;/p&gt;

&lt;h1 id=&#34;块存储&#34;&gt;块存储&lt;/h1&gt;

&lt;p&gt;我们需要先理解一下块的概念：块级是指以扇区为基础，一个或我连续的扇区组成一个块，也叫物理块。它是在文件系统与块设备（例如：磁盘驱动器）之间。&lt;/p&gt;

&lt;p&gt;块存储主要是将裸磁盘空间整个映射给主机使用的。比如磁盘阵列里面有5块硬盘，每个硬盘1G，然后可以通过划逻辑盘、做Raid、或者LVM（逻辑卷）等种种方式逻辑划分出N个逻辑的硬盘。假设划分完的逻辑盘也是5个，每个也是1G，但是这5个1G的逻辑盘已经于原来的5个物理硬盘意义完全不同了。比如第一个逻辑硬盘A里面，可能第一个200M是来自物理硬盘1，第二个200M是来自物理硬盘2，所以逻辑硬盘A是由多个物理硬盘逻辑虚构出来的硬盘。&lt;/p&gt;

&lt;p&gt;块存储会采用映射的方式将这几个逻辑盘映射给主机，主机上面的操作系统会识别到有5块硬盘，但是操作系统是区分不出到底是逻辑还是物理的，它一概就认为只是5块裸的物理硬盘而已，跟直接拿一块物理硬盘挂载到操作系统没有区别的，至少操作系统感知上没有区别。&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;磁盘组合变大容量，几块磁盘可以并发读写，提供读写效率&lt;/li&gt;
&lt;li&gt;直接雨磁盘进行交互，能满足一些需要直接裸盘映射的需求，比如数据库&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要网络连接，成本高&lt;/li&gt;
&lt;li&gt;数据无法在不同主机，不同操作系统之间共享&lt;/li&gt;
&lt;li&gt;相对来说，效率低下&lt;/li&gt;
&lt;li&gt;不能形象展示&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单来说，块存储就是直接与磁盘打交道，比如数据库需要将数据直接映射到磁盘上，就需要用到块存储，一般现在就是数据库在使用块存储，其他的应用都使用更加高级的文件存储，甚至对象存储。通常使用块存储的都是系统而非用户，在市场上并没有很多的块存储的产品，大多数都是数据库直接使用，目前只知道&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/ceph/ceph/&#34;&gt;ceph&lt;/a&gt;在对象存储的基础上提供了块存储的能力RDB。&lt;/p&gt;

&lt;h1 id=&#34;文件存储&#34;&gt;文件存储&lt;/h1&gt;

&lt;p&gt;我们需要先理解一下文件的概念：文件级是指文件系统，单个文件可能由于一个或多个逻辑块组成，且逻辑块之间是不连续分布。逻辑块大于或等于物理块整数倍，与上面的块之间的关系是扇区→物理块→逻辑块→文件系统。&lt;/p&gt;

&lt;p&gt;计算机中所有的数据都是0和1，存储在硬件介质上的一连串的01组合对我们来说完全无法去分辨以及管理。因此我们用“文件”这个概念对这些数据进行组织，所有用于同一用途的数据，按照不同应用程序要求的结构方式组成不同类型的文件（通常用不同的后缀来指代不同的类型），然后我们给每一个文件起一个方便理解记忆的名字。而当文件很多的时候，我们按照某种划分方式给这些文件分组，每一组文件放在同一个目录（或者叫文件夹）里面，当然我们也需要给这些目录起一个容易理解和记忆的名字。而且目录下面除了文件还可以有下一级目录（称之为子目录或者子文件夹），所有的文件、目录形成一个树状结构&lt;/p&gt;

&lt;p&gt;把存储介质上的数据组织成目录-子目录-文件这种形式的数据结构，用于从这个结构中寻找、添加、修改、删除文件的程序，以及用于维护这个结构的程序，组成的系统有一个专用的名字：文件系统（File System）。&lt;/p&gt;

&lt;p&gt;文件系统有很多，常见的有Windows的FAT/FAT32/NTFS，Linux的EXT2/EXT3/EXT4/XFS/BtrFS等。而在网络存储中，底层数据并非存储在本地的存储介质，而是另外一台服务器上，不同的客户端都可以用类似文件系统的方式访问这台服务器上的文件，这样的系统叫网络文件系统（Network File System），常见的网络文件系统有Windows网络的CIFS（也叫SMB）、类Unix系统网络的NFS等。而文件存储除了网络文件系统外，FTP、HTTP其实也算是文件存储的某种特殊实现，都是可以通过某个url来访问一个文件。&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;形象，使用方便&lt;/li&gt;
&lt;li&gt;网络文件系统可以实现共享&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要文件系统的协调&lt;/li&gt;
&lt;li&gt;由于封装速度变慢&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实文件系统是我们最常使用的，分布式文件系统也是一步步演化过来的:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;单机时代&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;初创时期由于时间紧迫，在各种资源有限的情况下，通常就直接在项目目录下建立静态文件夹，用于用户存放项目中的文件资源。如果按不同类型再细分，可以在项目目录下再建立不同的子目录来区分。例如：resources\static\file、resources\static\img等。&lt;/p&gt;

&lt;p&gt;优点：这样做比较便利，项目直接引用就行，实现起来也简单，无需任何复杂技术，保存数据库记录和访问起来也很方便。&lt;/p&gt;

&lt;p&gt;缺点：如果只是后台系统的使用一般也不会有什么问题，但是作为一个前端网站使用的话就会存在弊端。一方面，文件和代码耦合在一起，文件越多存放越混乱；另一方面，如果流量比较大，静态文件访问会占据一定的资源，影响正常业务进行，不利于网站快速发展。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;独立文件服务器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;随着公司业务不断发展，将代码和文件放在同一服务器的弊端就会越来越明显。为了解决上面的问题引入独立图片服务器，工作流程如下：项目上传文件时，首先通过ftp或者ssh将文件上传到图片服务器的某个目录下，再通过ngnix或者apache来访问此目录下的文件，返回一个独立域名的图片URL地址，前端使用文件时就通过这个URL地址读取。&lt;/p&gt;

&lt;p&gt;优点：图片访问是很消耗服务器资源的（因为会涉及到操作系统的上下文切换和磁盘I/O操作），分离出来后，Web/App服务器可以更专注发挥动态处理的能力；独立存储，更方便做扩容、容灾和数据迁移；方便做图片访问请求的负载均衡，方便应用各种缓存策略（HTTP Header、Proxy Cache等），也更加方便迁移到CDN。&lt;/p&gt;

&lt;p&gt;缺点：单机存在性能瓶颈，容灾、垂直扩展性稍差&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式文件系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过独立文件服务器可以解决一些问题，如果某天存储文件的那台服务突然down了怎么办？可能你会说，定时将文件系统备份，这台down机的时候，迅速切换到另一台就OK了，但是这样处理需要人工来干预。另外，当存储的文件超过100T的时候怎么办？单台服务器的性能问题？这个时候我们就应该考虑分布式文件系统了。&lt;/p&gt;

&lt;p&gt;业务继续发展，单台服务器存储和响应也很快到达了瓶颈，新的业务需要文件访问具有高响应性、高可用性来支持系统。分布式文件系统，一般分为三块内容来配合，服务的存储、访问的仲裁系统，文件存储系统，文件的容灾系统来构成，仲裁系统相当于文件服务器的大脑，根据一定的算法来决定文件存储的位置，文件存储系统负责保存文件，容灾系统负责文件系统和自己的相互备份。&lt;/p&gt;

&lt;p&gt;优点：扩展能力: 毫无疑问，扩展能力是一个分布式文件系统最重要的特点；高可用性: 在分布式文件系统中，高可用性包含两层，一是整个文件系统的可用性，二是数据的完整和一致性；弹性存储: 可以根据业务需要灵活地增加或缩减数据存储以及增删存储池中的资源，而不需要中断系统运行&lt;/p&gt;

&lt;p&gt;缺点：系统复杂度稍高，需要更多服务器&lt;/p&gt;

&lt;p&gt;文件存储其实就是在块存储之上包装出来的，有了文件的概念，加上文件系统的管理，更加形象的记录展示使用，是我们最长使用的方式。其实在分布式文件系统之前，我们也是经常使用文件来存储的，比如日志，比如持久化数据等。只不过随着数据量的越来越多，数据共享需求越来越多，我们需要使用分布式存储系统，我们常用的分布式文件存储系统有很多，比如&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/nfs/&#34;&gt;NFS&lt;/a&gt;(nfs主要是在共享，还不能算分布式，不过确实是存在网络节点上的。)，&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/glusterfs/&#34;&gt;glusterfs&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/fastfs/&#34;&gt;fastdfs&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/hfds/&#34;&gt;hdfs&lt;/a&gt;等，包括上面块存储&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/ceph/ceph/&#34;&gt;ceph&lt;/a&gt;也提供了文件存储。&lt;/p&gt;

&lt;h1 id=&#34;对象存储&#34;&gt;对象存储&lt;/h1&gt;

&lt;p&gt;对象存储就是面向对象的存储，英文是Object-based Storage。现在很多云厂商，也直接称之为“云存储”。以下是对象存储出现的主要原因：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据量爆炸式增长。Web应用的崛起、社交需求的刺激，极大地推动了多媒体内容的创作和分享。人们开始上传大量的照片、音乐、视频，加剧了数据量的爆发。&lt;/li&gt;
&lt;li&gt;非结构化数据的占比显著增加。图像、音频、视频、word文章、演示胶片这样的数据，就是非结构化数据。2020年（也就是今年），全球数据总量的80%，将是非结构化数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以对象存储&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;能够处理大量非结构化数据的数据存储架构，我们日常生活中见到的文档、文本、图片、XML, HTML、各类报表、音视频信息等等都是非结构化数据。据统计，自社交网络发展以来，非结构化数据占总数据量的75%。。&lt;/li&gt;
&lt;li&gt;扁平结构：对象存储中没有文件夹的概念，所有数据均存储在同一个层级中，你不需要知道他存在哪里，只需要通过“凭证”就可以快速获取数据。&lt;/li&gt;
&lt;li&gt;弹性扩容：可以通过分布式多节点快速扩容。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前有&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/oss/&#34;&gt;很多公有云提供对象存储&lt;/a&gt;，比如阿里云的OSS,华为云的OBS，腾讯云的COS，七牛云Kodo，百度云BOS，网易云NOS。通过提供的SDK就可以访问。如果不想用公有云的话，也有一些开源方案可以自己搭建，比如ceph，minio。&lt;/p&gt;

&lt;p&gt;在对象存储系统里，你不能直接打开/修改文件，只能先下载、修改，再上传文件。（如果大家用过百度网盘或ftp服务，一定可以秒懂。），所以只是一个存储系统，而不是我们的文件系统。所以我们也需要块存储和文件存储。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为什么对象存储兼具块存储与文件存储的好处，还要使用块存储或文件存储呢？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、有一类应用是需要存储直接裸盘映射的，例如数据库。因为数据库需要存储裸盘映射给自己后，再根据自己的数据库文件系统来对裸盘进行格式化的，所以是不能够采用其他已经被格式化为某种文件系统的存储的。此类应用更适合使用块存储。&lt;/p&gt;

&lt;p&gt;2、对象存储的成本比起普通的文件存储还是较高，需要购买专门的对象存储软件以及大容量硬盘。如果对数据量要求不是海量，只是为了做文件共享的时候，直接用文件存储的形式好了，性价比高。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基本原理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对象存储最常用的方案，就是多台服务器内置大容量硬盘，再装上对象存储软件，然后再额外搞几台服务作为管理节点，安装上对象存储管理软件。管理节点可以管理其他服务器对外提供读写访问功能。其实就是一个文件存储或者块存储的分布式管理系统方案。&lt;/p&gt;

&lt;p&gt;比如AT32这种文件系统，是直接将一份文件的数据与metadata一起存储的，存储过程先将文件按照文件系统的最小块大小来打散（如4M的文件，假设文件系统要求一个块4K，那么就将文件打散成为1000个小块），再写进硬盘里面，过程中没有区分数据/metadata的。而每个块最后会告知你下一个要读取的块的地址，然后一直这样顺序地按图索骥，最后完成整份文件的所有块的读取。这种情况下读写速率很慢，因为就算你有100个机械手臂在读写，但是由于你只有读取到第一个块，才能知道下一个块在哪里，其实相当于只能有1个机械手臂在实际工作。&lt;/p&gt;

&lt;p&gt;而对象存储则将元数据独立了出来，控制节点叫元数据服务器（服务器+对象存储管理软件），里面主要负责存储对象的属性（主要是对象的数据被打散存放到了那几台分布式服务器中的信息），而其他负责存储数据的分布式服务器叫做OSD，主要负责存储文件的数据部分。当用户访问对象，会先访问元数据服务器，元数据服务器只负责反馈对象存储在哪些OSD，假设反馈文件A存储在B、C、D三台OSD，那么用户就会再次直接访问3台OSD服务器去读取数据。这时候由于是3台OSD同时对外传输数据，所以传输的速度就加快了。当OSD服务器数量越多，这种读写速度的提升就越大，通过此种方式，实现了读写快的目的。&lt;/p&gt;

&lt;p&gt;另一方面，对象存储软件是有专门的文件系统的，所以OSD对外又相当于文件服务器，那么就不存在文件共享方面的困难了，也解决了文件共享方面的问题。&lt;/p&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;1、所有的存储底层都是硬盘。&lt;/li&gt;
&lt;li&gt;2、块存储，操作对象是磁盘。存储协议是SCSI、iSCSI、FC，以 SCSI 为例，主要接口命令有 Read/Write/Read Capacity/Inquiry 等等。比如DAS和SAN，基于物理块的存储方式。&lt;/li&gt;
&lt;li&gt;3、文件存储，操作对象是文件和文件夹。存储协议是NFS、SAMBA（SMB）、POSIX等。以NFS（大家应该都用过“网上邻居”共享文件吧？就是那个）为例，文件相关的接口命令包括：READ/WRITE/CREATE/REMOVE/RENAME/LOOKUP/ACCESS 等等，文件夹相关的接口命令包括：MKDIR/RMDIR/READDIR 等等。&lt;/li&gt;
&lt;li&gt;4、对象存储，主要操作对象是对象（Object）。存储协议是S3、Swift等。以 S3 为例，主要接口命令有 PUT/GET/DELETE 等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;分布式存储的基本架构无非就是中心控制和无中心结构，无中心架构也是中心架构的中心瓶颈发展而成的，其实不管是分布式存储还是其他分布式实现，使用中心架构都会出现性能瓶颈，都在往无中心化架构的实现方法发展，使用最多的就是哈希映射思想。&lt;/p&gt;

&lt;h2 id=&#34;中间控制节点架构-hdfs&#34;&gt;中间控制节点架构（HDFS）&lt;/h2&gt;

&lt;p&gt;分布式存储最早是由谷歌提出的，其目的是通过廉价的服务器来提供使用与大规模，高并发场景下的Web访问问题。如图3是谷歌分布式存储（HDFS）的简化的模型。在该系统的整个架构中将服务器分为两种类型，一种名为namenode，这种类型的节点负责管理管理数据（元数据），另外一种名为datanode，这种类型的服务器负责实际数据的管理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/hdfs&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图分布式存储中，如果客户端需要从某个文件读取数据，首先从namenode获取该文件的位置（具体在哪个datanode），然后从该位置获取具体的数据。在该架构中namenode通常是主备部署，而datanode则是由大量节点构成一个集群。由于元数据的访问频度和访问量相对数据都要小很多，因此namenode通常不会成为性能瓶颈，而datanode集群可以分散客户端的请求。因此，通过这种分布式存储架构可以通过横向扩展datanode的数量来增加承载能力，也即实现了动态横向扩展的能力。&lt;/p&gt;

&lt;h2 id=&#34;完全无中心架构&#34;&gt;完全无中心架构&lt;/h2&gt;

&lt;h3 id=&#34;计算模式-ceph&#34;&gt;计算模式（Ceph）&lt;/h3&gt;

&lt;p&gt;如图是Ceph存储系统的架构，在该架构中与HDFS不同的地方在于该架构中没有中心节点。客户端是通过一个设备映射关系计算出来其写入数据的位置，这样客户端可以直接与存储节点通信，从而避免中心节点的性能瓶颈。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/ceph&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在Ceph存储系统架构中核心组件有Mon服务、OSD服务和MDS服务等。对于块存储类型只需要Mon服务、OSD服务和客户端的软件即可。其中Mon服务用于维护存储系统的硬件逻辑关系，主要是服务器和硬盘等在线信息。Mon服务通过集群的方式保证其服务的可用性。OSD服务用于实现对磁盘的管理，实现真正的数据读写，通常一个磁盘对应一个OSD服务。
客户端访问存储的大致流程是，客户端在启动后会首先从Mon服务拉取存储资源布局信息，然后根据该布局信息和写入数据的名称等信息计算出期望数据的位置（包含具体的物理服务器信息和磁盘信息），然后该位置信息直接通信，读取或者写入数据。&lt;/p&gt;

&lt;p&gt;底层是RADOS，这是个标准的对象存储。以RADOS为基础，Ceph 能够提供文件，块和对象三种存储服务。其中通过RBD提供出来的块存储是比较有价值的地方，毕竟因为市面上开源的分布式块存储少见嘛（以前倒是有个sheepdog，但是现在不当红了）。当然它也通过CephFS模块和相应的私有Client提供了文件服务，这也是很多人认为Ceph是个文件系统的原因。另外它自己原生的对象存储可以通过RadosGW存储网关模块向外提供对象存储服务，并且和对象存储的事实标准Amazon S3以及Swift兼容。所以能看出来这其实是个大一统解决方案，啥都齐全。&lt;/p&gt;

&lt;h3 id=&#34;一致性哈希-swift&#34;&gt;一致性哈希（Swift）&lt;/h3&gt;

&lt;p&gt;与Ceph的通过计算方式获得数据位置的方式不同，另外一种方式是通过一致性哈希的方式获得数据位置。一致性哈希的方式就是将设备做成一个哈希环，然后根据数据名称计算出的哈希值映射到哈希环的某个位置，从而实现数据的定位。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/swift&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如图是一致性哈希的基本原理，为了绘制简单，本文以一个服务器上的一个磁盘为例进行介绍。为了保证数据分配的均匀性及出现设备故障时数据迁移的均匀性，一致性哈希将磁盘划分为比较多的虚拟分区，每个虚拟分区是哈希环上的一个节点。整个环是一个从0到32位最大值的一个区间，并且首尾相接。当计算出数据（或者数据名称）的哈希值后，必然落到哈希环的某个区间，然后以顺时针，必然能够找到一个节点。那么，这个节点就是存储数据的位置。
Swift存储的整个数据定位算法就是基于上述一致性哈希实现的。在Swift对象存储中，通过账户名/容器名/对象名三个名称组成一个位置的标识，通过该唯一标识可以计算出一个整型数来。而在存储设备方面，Swift构建一个虚拟分区表，表的大小在创建集群是确定（通常为几十万），这个表其实就是一个数组。这样，根据上面计算的整数值，以及这个数组，通过一致性哈希算法就可以确定该整数在数组的位置。而数组中的每项内容是数据3个副本（也可以是其它副本数量）的设备信息（包含服务器和磁盘等信息）。也就是经过上述计算，可以确定一个数据存储的具体位置。这样，Swift就可以将请求重新定向到该设备进行处理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/swift2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上述计算过程是在一个名为Proxy的服务中进行的，该服务可以集群化部署。因此可以分摊请求的负载，不会成为性能瓶颈。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 文件存储系统glusterfs</title>
          <link>https://kingjcy.github.io/post/distributed/store/fs/glusterfs/</link>
          <pubDate>Tue, 15 Dec 2020 20:21:18 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/fs/glusterfs/</guid>
          <description>&lt;p&gt;GlusterFS (Gluster File System) 是一个开源的分布式文件系统，是 Scale-Out 存储解决方案 Gluster 的核心，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS 借助 TCP/IP 或 InfiniBand RDMA 网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据,基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;1、Brick：GlusterFS中的存储单元，通过是一个受信存储池中的服务器的一个导出目录。可以通过主机名和目录名来标识，如&amp;rsquo;SERVER:EXPORT&amp;rsquo;。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Brick是一个节点和一个导出目录的集合，e.g. node1:/brick1&lt;/li&gt;
&lt;li&gt;Brick是底层的RAID或磁盘经XFS或ext4文件系统格式化而来，所以继承了文件系统的限制&lt;/li&gt;
&lt;li&gt;每个节点上的brick数是不限的&lt;/li&gt;
&lt;li&gt;理想的状况是，一个集群的所有Brick大小都一样。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、Node：一个拥有若干brick的设备。&lt;/p&gt;

&lt;p&gt;3、Trusted Storage Pool：一堆存储节点的集合&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过一个节点“邀请”其他节点创建，这里叫probe&lt;/li&gt;
&lt;li&gt;成员可以动态加入，动态删除&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、GFID：GlusterFS卷中的每个文件或目录都有一个唯一的128位的数据相关联，其用于模拟inode&lt;/p&gt;

&lt;p&gt;5、Namespace：每个Gluster卷都导出单个ns作为POSIX的挂载点。&lt;/p&gt;

&lt;p&gt;6、Volume：一组bricks的逻辑集合，Volume是一个可挂载的目录。支持如下种类：&lt;/p&gt;

&lt;p&gt;a) 分布式卷：默认模式，DHT&lt;/p&gt;

&lt;p&gt;又称哈希卷，近似于RAID0，文件没有分片，文件根据hash算法写入各个节点的硬盘上，优点是容量大，缺点是没冗余。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;b) 条带卷&lt;/p&gt;

&lt;p&gt;相当于raid0，文件是分片均匀写在各个节点的硬盘上的，优点是分布式读写，性能整体较好。缺点是没冗余，分片随机读写可能会导致硬盘IOPS饱和。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume stripe 2 transport tcp server1:/exp1 server2:/exp2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;c) 复制卷（复制模式，AFR）&lt;/p&gt;

&lt;p&gt;相当于raid1，复制的份数，决定集群的大小，通常与分布式卷或者条带卷组合使用，解决前两种存储卷的冗余缺陷。缺点是磁盘利用率低。复本卷在创建时可指定复本的数量，通常为2或者3，复本在存储时会在卷的不同brick上，因此有几个复本就必须提供至少多个brick，当其中一台服务器失效后，可以从另一台服务器读取数据，因此复制GlusterFS卷提高了数据可靠性的同事，还提供了数据冗余的功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;d) 分布式复制卷&lt;/p&gt;

&lt;p&gt;最少需要4台服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;e) 分布式条带复制卷&lt;/p&gt;

&lt;p&gt;当单个文件的体型十分巨大，客户端数量更多时，条带卷已经无法满足需求，此时将分布式与条带化结合起来是一个比较好的选择。其性能与服务器数量有关。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume stripe 4 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 server7:/exp7 server8:/exp8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;传统分布式文件系统大都会设置元数据服务器或者功能相近的管理服务器，主要作用就是用来管理文件与数据区块之间的存储位置关系。相较其他分布式文件系统而言，GlusterFS并没有集中或者分布式的元数据的概念，取而代之的是弹性哈希算法。集群中的任何服务器和客户端都可以利用哈希算法、路径及文件名进行计算，就可以对数据进行定位，并执行读写访问操作。&lt;/p&gt;

&lt;p&gt;每个节点服务器都掌握了集群的配置信息，这样做的好处是每个节点度拥有节点的配置信息，高度自治，所有信息都可以在本地查询。每个节点的信息更新都会向其他节点通告，保证节点间信息的一致性。但如果集群规模较大，节点众多时，信息同步的效率就会下降，节点信息的非一致性概率就会大大提高。因此GlusterFS未来的版本有向集中式管理变化的趋势。&lt;/p&gt;

&lt;p&gt;通过GlusterFS的数据访问流程，我们可以看出其基本的实现&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先是在客户端， 用户通过glusterfs的mount point 来读写数据， 对于用户来说，集群系统的存在对用户是完全透明的，用户感觉不到是操作本地系统还是远端的集群系统。&lt;/li&gt;
&lt;li&gt;用户的这个操作被递交给 本地linux系统的VFS来处理。&lt;/li&gt;
&lt;li&gt;VFS 将数据递交给FUSE 内核文件系统:在启动 glusterfs 客户端以前，需要想系统注册一个实际的文件系统FUSE,如上图所示，该文件系统与ext3在同一个层次上面， ext3 是对实际的磁盘进行处理， 而fuse 文件系统则是将数据通过/dev/fuse 这个设备文件递交给了glusterfs client端。所以， 我们可以将 fuse文件系统理解为一个代理。&lt;/li&gt;
&lt;li&gt;数据被fuse 递交给Glusterfs client 后， client 对数据进行一些指定的处理（所谓的指定，是按照client 配置文件据来进行的一系列处理， 我们在启动glusterfs client 时需要指定这个文件。&lt;/li&gt;
&lt;li&gt;在glusterfs client的处理末端，通过网络将数据递交给 Glusterfs Server，并且将数据写入到服务器所控制的存储设备上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样， 整个数据流的处理就完成了。&lt;/p&gt;

&lt;h1 id=&#34;实践&#34;&gt;实践&lt;/h1&gt;

&lt;p&gt;理论和实践上分析，GlusterFS目前主要适用大文件存储场景，对于小文件尤其是海量小文件，存储效率和访问性能都表现不佳。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;元数据性能&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GlusterFS使用弹性哈希算法代替传统分布式文件系统中的集中或分布式元数据服务，这个是GlusterFS最核心的思想，从而获得了接近线性的高扩展性，同时也提高了系统性能和可靠性。GlusterFS使用算法进行数据定位，集群中的任何服务器和客户端只需根据路径和文件名就可以对数据进行定位和读写访问，文件定位可独立并行化进行。&lt;/p&gt;

&lt;p&gt;这种算法的特点是，给定确定的文件名，查找和定位会非常快。但是，如果事先不知道文件名，要列出文件目录（ls或ls -l），性能就会大幅下降。对于Distributed哈希卷，文件通过HASH算法分散到集群节点上，每个节点上的命名空间均不重叠，所有集群共同构成完整的命名空间，访问时使用HASH算法进行查找定位。列文件目录时，需要查询所有节点，并对文件目录信息及属性进行聚合。这时，哈希算法根本发挥不上作用，相对于有中心的元数据服务，查询效率要差很多。&lt;/p&gt;

&lt;p&gt;从接触的一些用户和实践来看，当集群规模变大以及文件数量达到百万级别时，ls文件目录和rm删除文件目录这两个典型元数据操作就会变得非常慢，创建和删除100万个空文件可能会花上15分钟。如何解决这个问题呢？我们建议合理组织文件目录，目录层次不要太深，单个目录下文件数量不要过多；增大服务器内存配置，并且增大GlusterFS目录缓存参数；网络配置方面，建议采用万兆或者InfiniBand。从研发角度看，可以考虑优化方法提升元数据性能。比如，可以构建全局统一的分布式元数据缓存系统；也可以将元数据与数据重新分离，每个节点上的元数据采用全内存或数据库设计，并采用SSD进行元数据持久化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;小文件问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;理论和实践上分析，GlusterFS目前主要适用大文件存储场景，对于小文件尤其是海量小文件，存储效率和访问性能都表现不佳。海量小文件LOSF问题是工业界和学术界公认的难题，GlusterFS作为通用的分布式文件系统，并没有对小文件作额外的优化措施，性能不好也是可以理解的。&lt;/p&gt;

&lt;p&gt;对于LOSF而言，IOPS/OPS是关键性能衡量指标，造成性能和存储效率低下的主要原因包括元数据管理、数据布局和I/O管理、Cache管理、网络开销等方面。从理论分析以及LOSF优化实践来看，优化应该从元数据管理、缓存机制、合并小文件等方面展开，而且优化是一个系统工程，结合硬件、软件，从多个层面同时着手，优化效果会更显著。GlusterFS小文件优化可以考虑这些方法，这里不再赘述，关于小文件问题请参考“海量小文件问题综述”一文。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;集群管理模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GlusterFS集群采用全对等式架构，每个节点在集群中的地位是完全对等的，集群配置信息和卷配置信息在所有节点之间实时同步。这种架构的优点是，每个节点都拥有整个集群的配置信息，具有高度的独立自治性，信息可以本地查询。但同时带来的问题的，一旦配置信息发生变化，信息需要实时同步到其他所有节点，保证配置信息一致性，否则GlusterFS就无法正常工作。在集群规模较大时，不同节点并发修改配置时，这个问题表现尤为突出。因为这个配置信息同步模型是网状的，大规模集群不仅信息同步效率差，而且出现数据不一致的概率会增加。&lt;/p&gt;

&lt;p&gt;实际上，大规模集群管理应该是采用集中式管理更好，不仅管理简单，效率也高。可能有人会认为集中式集群管理与GlusterFS的无中心架构不协调，其实不然。GlusterFS 2.0以前，主要通过静态配置文件来对集群进行配置管理，没有Glusterd集群管理服务，这说明glusterd并不是GlusterFS不可或缺的组成部分，它们之间是松耦合关系，可以用其他的方式来替换。从其他分布式系统管理实践来看，也都是采用集群式管理居多，这也算一个佐证，GlusterFS 4.0开发计划也表现有向集中式管理转变的趋势。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;容量负载均衡&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GlusterFS的哈希分布是以目录为基本单位的，文件的父目录利用扩展属性记录了子卷映射信息，子文件在父目录所属存储服务器中进行分布。由于文件目录事先保存了分布信息，因此新增节点不会影响现有文件存储分布，它将从此后的新创建目录开始参与存储分布调度。这种设计，新增节点不需要移动任何文件，但是负载均衡没有平滑处理，老节点负载较重。GlusterFS实现了容量负载均衡功能，可以对已经存在的目录文件进行Rebalance，使得早先创建的老目录可以在新增存储节点上分布，并可对现有文件数据进行迁移实现容量负载均衡。&lt;/p&gt;

&lt;p&gt;GlusterFS目前的容量负载均衡存在一些问题。由于采用Hash算法进行数据分布，容量负载均衡需要对所有数据重新进行计算并分配存储节点，对于那些不需要迁移的数据来说，这个计算是多余的。Hash分布具有随机性和均匀性的特点，数据重新分布之后，老节点会有大量数据迁入和迁出，这个多出了很多数据迁移量。相对于有中心的架构，可谓节点一变而动全身，增加和删除节点增加了大量数据迁移工作。GlusterFS应该优化数据分布，最小化容量负载均衡数据迁移。此外，GlusterFS容量负载均衡也没有很好考虑执行的自动化、智能化和并行化。目前，GlusterFS在增加和删除节点上，需要手工执行负载均衡，也没有考虑当前系统的负载情况，可能影响正常的业务访问。GlusterFS的容量负载均衡是通过在当前执行节点上挂载卷，然后进行文件复制、删除和改名操作实现的，没有在所有集群节点上并发进行，负载均衡性能差。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据分布问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Glusterfs主要有三种基本的集群模式，即分布式集群(Distributed cluster)、条带集群(Stripe cluster)、复制集群(Replica cluster)。这三种基本集群还可以采用类似堆积木的方式，构成更加复杂的复合集群。三种基本集群各由一个translator来实现，分别由自己独立的命名空间。对于分布式集群，文件通过HASH算法分散到集群节点上，访问时使用HASH算法进行查找定位。复制集群类似RAID1，所有节点数据完全相同，访问时可以选择任意个节点。条带集群与RAID0相似，文件被分成数据块以Round Robin方式分布到所有节点上，访问时根据位置信息确定节点。&lt;/p&gt;

&lt;p&gt;哈希分布可以保证数据分布式的均衡性，但前提是文件数量要足够多，当文件数量较少时，难以保证分布的均衡性，导致节点之间负载不均衡。这个对有中心的分布式系统是很容易做到的，但GlusteFS缺乏集中式的调度，实现起来比较复杂。复制卷包含多个副本，对于读请求可以实现负载均衡，但实际上负载大多集中在第一个副本上，其他副本负载很轻，这个是实现上问题，与理论不太相符。条带卷原本是实现更高性能和超大文件，但在性能方面的表现太差强人意，远远不如哈希卷和复制卷，没有被好好实现，连官方都不推荐应用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据可用性问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;副本（Replication）就是对原始数据的完全拷贝。通过为系统中的文件增加各种不同形式的副本，保存冗余的文件数据，可以十分有效地提高文件的可用性，避免在地理上广泛分布的系统节点由网络断开或机器故障等动态不可测因素而引起的数据丢失或不可获取。GlusterFS主要使用复制来提供数据的高可用性，通过的集群模式有复制卷和哈希复制卷两种模式。复制卷是文件级RAID1，具有容错能力，数据同步写到多个brick上，每个副本都可以响应读请求。当有副本节点发生故障，其他副本节点仍然正常提供读写服务，故障节点恢复后通过自修复服务或同步访问时自动进行数据同步。&lt;/p&gt;

&lt;p&gt;一般而言，副本数量越多，文件的可靠性就越高，但是如果为所有文件都保存较多的副本数量，存储利用率低（为副本数量分之一），并增加文件管理的复杂度。目前GlusterFS社区正在研发纠删码功能，通过冗余编码提高存储可用性，并且具备较低的空间复杂度和数据冗余度，存储利用率高。&lt;/p&gt;

&lt;p&gt;GlusterFS的复制卷以brick为单位进行镜像，这个模式不太灵活，文件的复制关系不能动态调整，在已经有副本发生故障的情况下会一定程度上降低系统的可用性。对于有元数据服务的分布式系统，复制关系可以是以文件为单位的，文件的不同副本动态分布在多个存储节点上；当有副本发生故障，可以重新选择一个存储节点生成一个新副本，从而保证副本数量，保证可用性。另外，还可以实现不同文件目录配置不同的副本数量，热点文件的动态迁移。对于无中心的GlusterFS系统来说，这些看起来理所当然的功能，实现起来都是要大费周折的。不过值得一提的是，4.0开发计划已经在考虑这方面的副本特性。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据安全问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GlusterFS以原始数据格式（如EXT4、XFS、ZFS）存储数据，并实现多种数据自动修复机制。因此，系统极具弹性，即使离线情形下文件也可以通过其他标准工具进行访问。如果用户需要从GlusterFS中迁移数据，不需要作任何修改仍然可以完全使用这些数据。&lt;/p&gt;

&lt;p&gt;然而，数据安全成了问题，因为数据是以平凡的方式保存的，接触数据的人可以直接复制和查看。这对很多应用显然是不能接受的，比如云存储系统，用户特别关心数据安全。私有存储格式可以保证数据的安全性，即使泄露也是不可知的。GlusterFS要实现自己的私有格式，在设计实现和数据管理上相对复杂一些，也会对性能产生一定影响。&lt;/p&gt;

&lt;p&gt;GlusterFS在访问文件目录时根据扩展属性判断副本是否一致，这个进行数据自动修复的前提条件。节点发生正常的故障，以及从挂载点进行正常的操作，这些情况下发生的数据不一致，都是可以判断和自动修复的。但是，如果直接从节点系统底层对原始数据进行修改或者破坏，GlusterFS大多情况下是无法判断的，因为数据本身也没有校验，数据一致性无法保证。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Cache一致性&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了简化Cache一致性，GlusterFS没有引入客户端写Cache，而采用了客户端只读Cache。GlusterFS采用简单的弱一致性，数据缓存的更新规则是根据设置的失效时间进行重置的。对于缓存的数据，客户端周期性询问服务器，查询文件最后被修改的时间，如果本地缓存的数据早于该时间，则让缓存数据失效，下次读取数据时就去服务器获取最新的数据。&lt;/p&gt;

&lt;p&gt;GlusterFS客户端读Cache刷新的时间缺省是1秒，可以通过重新设置卷参数Performance.cache-refresh-timeout进行调整。这意味着，如果同时有多个用户在读写一个文件，一个用户更新了数据，另一个用户在Cache刷新周期到来前可能读到非最新的数据，即无法保证数据的强一致性。因此实际应用时需要在性能和数据一致性之间进行折中，如果需要更高的数据一致性，就得调小缓存刷新周期，甚至禁用读缓存；反之，是可以把缓存周期调大一点，以提升读性能。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- K8s controller</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller/</link>
          <pubDate>Tue, 24 Nov 2020 20:22:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller/</guid>
          <description>&lt;p&gt;pod只是运行的最小单元，我们一般不会直接使用pod。大部分情况下我们都是使用deployment（RS），deamonset，statefulset，job等控制器来完成部署调度使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/pod3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;k8s原生控制器&#34;&gt;k8s原生控制器&lt;/h1&gt;

&lt;h2 id=&#34;rc&#34;&gt;RC&lt;/h2&gt;

&lt;p&gt;Replication Controller 保证了在所有时间内，都有特定数量的Pod副本正在运行，如果太多了，Replication Controller就杀死几个，如果太少了，Replication Controller会新建几个，和直接创建的pod不同的是，Replication Controller会替换掉那些删除的或者被终止的pod，不管删除的原因是什么（维护阿，更新啊，Replication Controller都不关心）。基于这个理由，我们建议即使是只创建一个pod，我们也要使用Replication Controller。Replication Controller 就像一个进程管理器，监管着不同node上的多个pod,而不是单单监控一个node上的pod,Replication Controller 会委派本地容器来启动一些节点上服务（Kubelet ,Docker）。&lt;/p&gt;

&lt;p&gt;后来这种kind和kube-controller-manager的模块同名，所以修改为ReplicaSet，一样的功能。&lt;/p&gt;

&lt;h2 id=&#34;depolymemt&#34;&gt;depolymemt&lt;/h2&gt;

&lt;p&gt;Deployment为Pod和ReplicaSet提供了一个声明式定义(declarative)方法，用来替代以前的ReplicationController来方便的管理应用。典型的应用场景包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;定义Deployment来创建Pod和ReplicaSet&lt;/li&gt;
&lt;li&gt;滚动升级和回滚应用&lt;/li&gt;
&lt;li&gt;扩容和缩容&lt;/li&gt;
&lt;li&gt;暂停和继续Deployment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;deploymemt就是使用ReplicaSet来实现的，现在基本都是使用这个。一般适用于无状态的应用部署。&lt;/p&gt;

&lt;p&gt;实例：一个简单的nginx应用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;扩容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;kubectl scale deployment nginx-deployment --replicas 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果集群支持 horizontal pod autoscaling 的话，还可以为Deployment设置自动扩展：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;回滚：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;kubectl rollout undo deployment/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f docs/user-guide/nginx-deployment.yaml --record
deployment &amp;quot;nginx-deployment&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将kubectl的 —record 的flag设置为 true可以在annotation中记录当前命令创建或者升级了该资源。&lt;/p&gt;

&lt;p&gt;获取信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         3         3            3           18s

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-2035384211   3         3         0       18s

$ kubectl get pods --show-labels
NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-2035384211-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
nginx-deployment-2035384211-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
nginx-deployment-2035384211-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见是基于rs的基础上实现的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;更新&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;更新镜像也比较简单:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以使用edit命令来编辑Deployment，修改 .spec.template.spec.containers[0].image ，将nginx:1.7.9 改写成nginx:1.9.1。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl edit deployment/nginx-deployment
deployment &amp;quot;nginx-deployment&amp;quot; edited
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看rollout的状态，只要执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl rollout status deployment/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment &amp;quot;nginx-deployment&amp;quot; successfully rolled out
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;检查Deployment升级的历史记录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先，检查下Deployment的revision：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl rollout history deployment/nginx-deployment
deployments &amp;quot;nginx-deployment&amp;quot;:
REVISION    CHANGE-CAUSE
1           kubectl create -f docs/user-guide/nginx-deployment.yaml --record
2           kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
3           kubectl set image deployment/nginx-deployment nginx=nginx:1.91
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;调度&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;deployment是基于rs的基础之上的，根据调度算法，选择合适的node，维持pod的数量，还可以指定调度和优化调度等。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;指定node(nodeselector)&lt;/p&gt;

&lt;p&gt;通过nodeSelector，一个Pod可以指定它所想要运行的Node节点。&lt;/p&gt;

&lt;p&gt;首先给Node加上标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl label nodes &amp;lt;your-node-name&amp;gt; disktype=ssd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着，指定该Pod只想运行在带有disktype=ssd标签的Node上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;亲和度调度（nodeaffinity）&lt;/p&gt;

&lt;p&gt;目前主要的node affinity：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;requiredDuringSchedulingIgnoredDuringExecution
表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。&lt;/li&gt;
&lt;li&gt;requiredDuringSchedulingRequiredDuringExecution
表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中RequiredDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，则重新选择符合要求的节点。&lt;/li&gt;
&lt;li&gt;preferredDuringSchedulingIgnoredDuringExecution
表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。&lt;/li&gt;
&lt;li&gt;preferredDuringSchedulingRequiredDuringExecution
表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中RequiredDuringExecution表示如果后面节点标签发生了变化，满足了条件，则重新调度到满足条件的节点。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: gcr.io/google_containers/pause:2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 pod 同时定义了 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 两种 nodeAffinity。第一个要求 pod 运行在特定 AZ 的节点上，第二个希望节点最好有对应的 another-node-label-key:another-node-label-value 标签。&lt;/p&gt;

&lt;p&gt;这里的匹配逻辑是label在某个列表中，可选的操作符有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In: label的值在某个列表中&lt;/li&gt;
&lt;li&gt;NotIn：label的值不在某个列表中&lt;/li&gt;
&lt;li&gt;Exists：某个label存在&lt;/li&gt;
&lt;li&gt;DoesNotExist：某个label不存在&lt;/li&gt;
&lt;li&gt;Gt：label的值大于某个值（字符串比较）&lt;/li&gt;
&lt;li&gt;Lt：label的值小于某个值（字符串比较）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果nodeAffinity中nodeSelector有多个选项，节点满足任何一个条件即可；如果matchExpressions有多个选项，则节点必须同时满足这些选项才能运行pod 。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不被调度&lt;/p&gt;

&lt;p&gt;Taints 和 tolerations&lt;/p&gt;

&lt;p&gt;Taints 和 tolerations 用于保证 Pod 不被调度到不合适的 Node 上，Taint 应用于 Node 上，而 toleration 则应用于 Pod 上（Toleration 是可选的）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;比如，可以使用 taint 命令给 node1 添加 taints：
kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value2:NoExecute
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@kube-manager01 rebuild]# kubectl cordon  kube-node02

node/kube-node02 already cordoned
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过这个命令会使得node那个字段被设置成node转维护状态，不会被调度新pod&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;statefulset&#34;&gt;StatefulSet&lt;/h2&gt;

&lt;p&gt;StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计），其应用场景包括&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现&lt;/li&gt;
&lt;li&gt;稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现&lt;/li&gt;
&lt;li&gt;有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现&lt;/li&gt;
&lt;li&gt;有序收缩，有序删除（即从N-1到0）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从上面的应用场景可以发现，StatefulSet由以下几个部分组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用于定义网络标志（DNS domain）的Headless Service&lt;/li&gt;
&lt;li&gt;用于创建PersistentVolumes的volumeClaimTemplates&lt;/li&gt;
&lt;li&gt;定义具体应用的StatefulSet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;StatefulSet中每个Pod的DNS格式为statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local，其中&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;serviceName为Headless Service的名字&lt;/li&gt;
&lt;li&gt;0..N-1为Pod所在的序号，从0开始到N-1&lt;/li&gt;
&lt;li&gt;statefulSetName为StatefulSet的名字&lt;/li&gt;
&lt;li&gt;namespace为服务所在的namespace，Headless Servic和StatefulSet必须在相同的namespace&lt;/li&gt;
&lt;li&gt;.cluster.local为Cluster Domain，&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;简单示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;以一个简单的nginx服务web.yaml为例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &amp;quot;nginx&amp;quot;
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: gcr.io/google_containers/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      resources:
        requests:
          storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f web.yaml
service &amp;quot;nginx&amp;quot; created
statefulset &amp;quot;web&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建的headless service和statefulset&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get service nginx
NAME      CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     None         &amp;lt;none&amp;gt;        80/TCP    1m
$ kubectl get statefulset web
NAME      DESIRED   CURRENT   AGE
web       2         2         2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据volumeClaimTemplates自动创建PVC（在GCE中会自动创建kubernetes.io/gce-pd类型的volume）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pvc
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-d064a004-d8d4-11e6-b521-42010a800002   1Gi        RWO           16s
www-web-1   Bound     pvc-d06a3946-d8d4-11e6-b521-42010a800002   1Gi        RWO           16s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建的Pod，他们都是有序的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          5m
web-1     1/1       Running   0          4m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用nslookup查看这些Pod的DNS&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh
/ # nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.2.10
/ # nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.3.12
/ # nslookup web-0.nginx.default.svc.cluster.local
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx.default.svc.cluster.local
Address 1: 10.244.2.10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以进行其他的操作&lt;/p&gt;

&lt;p&gt;扩容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale statefulset web --replicas=5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;缩容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl patch statefulset web -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:3}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;镜像更新（目前还不支持直接更新image，需要patch来间接实现）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl patch statefulset web --type=&#39;json&#39; -p=&#39;[{&amp;quot;op&amp;quot;: &amp;quot;replace&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/spec/template/spec/containers/0/image&amp;quot;, &amp;quot;value&amp;quot;:&amp;quot;gcr.io/google_containers/nginx-slim:0.7&amp;quot;}]&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除StatefulSet和Headless Service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete statefulset web
$ kubectl delete service nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StatefulSet删除后PVC还会保留着，数据不再使用的话也需要删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete pvc www-web-0 www-web-1
zookeeper
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外一个更能说明StatefulSet强大功能的示例为zookeeper.yaml。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: v1
kind: Service
metadata:
  name: zk-headless
  labels:
    app: zk-headless
spec:
  ports:
  - port: 2888
    name: server
  - port: 3888
    name: leader-election
  clusterIP: None
  selector:
    app: zk
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: zk-config
data:
  ensemble: &amp;quot;zk-0;zk-1;zk-2&amp;quot;
  jvm.heap: &amp;quot;2G&amp;quot;
  tick: &amp;quot;2000&amp;quot;
  init: &amp;quot;10&amp;quot;
  sync: &amp;quot;5&amp;quot;
  client.cnxns: &amp;quot;60&amp;quot;
  snap.retain: &amp;quot;3&amp;quot;
  purge.interval: &amp;quot;1&amp;quot;
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: zk-budget
spec:
  selector:
    matchLabels:
      app: zk
  minAvailable: 2
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: zk
spec:
  serviceName: zk-headless
  replicas: 3
  template:
    metadata:
      labels:
        app: zk
      annotations:
        pod.alpha.kubernetes.io/initialized: &amp;quot;true&amp;quot;
        scheduler.alpha.kubernetes.io/affinity: &amp;gt;
            {
              &amp;quot;podAntiAffinity&amp;quot;: {
                &amp;quot;requiredDuringSchedulingRequiredDuringExecution&amp;quot;: [{
                  &amp;quot;labelSelector&amp;quot;: {
                    &amp;quot;matchExpressions&amp;quot;: [{
                      &amp;quot;key&amp;quot;: &amp;quot;app&amp;quot;,
                      &amp;quot;operator&amp;quot;: &amp;quot;In&amp;quot;,
                      &amp;quot;values&amp;quot;: [&amp;quot;zk-headless&amp;quot;]
                    }]
                  },
                  &amp;quot;topologyKey&amp;quot;: &amp;quot;kubernetes.io/hostname&amp;quot;
                }]
              }
            }
    spec:
      containers:
      - name: k8szk
        imagePullPolicy: Always
        image: gcr.io/google_samples/k8szk:v1
        resources:
          requests:
            memory: &amp;quot;4Gi&amp;quot;
            cpu: &amp;quot;1&amp;quot;
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        env:
        - name : ZK_ENSEMBLE
          valueFrom:
            configMapKeyRef:
              name: zk-config
              key: ensemble
        - name : ZK_HEAP_SIZE
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: jvm.heap
        - name : ZK_TICK_TIME
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: tick
        - name : ZK_INIT_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: init
        - name : ZK_SYNC_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: tick
        - name : ZK_MAX_CLIENT_CNXNS
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: client.cnxns
        - name: ZK_SNAP_RETAIN_COUNT
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: snap.retain
        - name: ZK_PURGE_INTERVAL
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: purge.interval
        - name: ZK_CLIENT_PORT
          value: &amp;quot;2181&amp;quot;
        - name: ZK_SERVER_PORT
          value: &amp;quot;2888&amp;quot;
        - name: ZK_ELECTION_PORT
          value: &amp;quot;3888&amp;quot;
        command:
        - sh
        - -c
        - zkGenConfig.sh &amp;amp;&amp;amp; zkServer.sh start-foreground
        readinessProbe:
          exec:
            command:
            - &amp;quot;zkOk.sh&amp;quot;
          initialDelaySeconds: 15
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
            - &amp;quot;zkOk.sh&amp;quot;
          initialDelaySeconds: 15
          timeoutSeconds: 5
        volumeMounts:
        - name: datadir
          mountPath: /var/lib/zookeeper
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      resources:
        requests:
          storage: 20Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f zookeeper.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h2&gt;

&lt;p&gt;DaemonSet保证在每个Node上都运行一个容器副本，常用来部署一些集群的日志、监控或者其他系统管理应用。典型的应用包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日志收集，比如fluentd，logstash等&lt;/li&gt;
&lt;li&gt;系统监控，比如Prometheus Node Exporter，collectd，New Relic agent，Ganglia gmond等&lt;/li&gt;
&lt;li&gt;系统程序，比如kube-proxy, kube-dns, glusterd, ceph等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用Fluentd收集日志的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: fluentd
spec:
  template:
    metadata:
      labels:
        app: logging
        id: fluentd
      name: fluentd
    spec:
      containers:
      - name: fluentd-es
        image: gcr.io/google_containers/fluentd-elasticsearch:1.3
        env:
         - name: FLUENTD_ARGS
           value: -qq
        volumeMounts:
         - name: containers
           mountPath: /var/lib/docker/containers
         - name: varlog
           mountPath: /varlog
      volumes:
         - hostPath:
             path: /var/lib/docker/containers
           name: containers
         - hostPath:
             path: /var/log
           name: varlog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;daemonset的调度算法和deploymemt基本是差不多的，只不过它会调度到所有的节点上，当然也可以使用nodeselector或者nodeaffinity来进行范围性的调度。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;指定Node节点&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;DaemonSet会忽略Node的unschedulable状态，有两种方式来指定Pod只运行在指定的Node节点上：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nodeSelector：只调度到匹配指定label的Node上&lt;/li&gt;
&lt;li&gt;nodeAffinity：功能更丰富的Node选择器，比如支持集合操作&lt;/li&gt;
&lt;li&gt;podAffinity：调度到满足条件的Pod所在的Node上&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;nodeSelector示例&lt;/p&gt;

&lt;p&gt;首先给Node打上标签&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl label nodes node-01 disktype=ssd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在daemonset中指定nodeSelector为disktype=ssd：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:
  nodeSelector:
    disktype: ssd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;nodeAffinity示例&lt;/p&gt;

&lt;p&gt;nodeAffinity目前支持两种：requiredDuringSchedulingIgnoredDuringExecution和preferredDuringSchedulingIgnoredDuringExecution，分别代表必须满足条件和优选条件。比如下面的例子代表调度到包含标签kubernetes.io/e2e-az-name并且值为e2e-az1或e2e-az2的Node上，并且优选还带有标签another-node-label-key=another-node-label-value的Node。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: gcr.io/google_containers/pause:2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;podAffinity示例&lt;/p&gt;

&lt;p&gt;podAffinity基于Pod的标签来选择Node，仅调度到满足条件Pod所在的Node上，支持podAffinity和podAntiAffinity。这个功能比较绕，以下面的例子为例：&lt;/p&gt;

&lt;p&gt;如果一个“Node所在Zone中包含至少一个带有security=S1标签且运行中的Pod”，那么可以调度到该Node&lt;/p&gt;

&lt;p&gt;不调度到“包含至少一个带有security=S2标签且运行中Pod”的Node上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: kubernetes.io/hostname
  containers:
  - name: with-pod-affinity
    image: gcr.io/google_containers/pause:2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;静态Pod&lt;/p&gt;

&lt;p&gt;除了DaemonSet，还可以使用静态Pod来在每台机器上运行指定的Pod，这需要kubelet在启动的时候指定manifest目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubelet --pod-manifest-path=/etc/kubernetes/manifests
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将所需要的Pod定义文件放到指定的manifest目录中。&lt;/p&gt;

&lt;p&gt;注意：静态Pod不能通过API Server来删除，但可以通过删除manifest文件来自动删除对应的Pod。&lt;/p&gt;

&lt;h2 id=&#34;cronjob&#34;&gt;CronJob&lt;/h2&gt;

&lt;p&gt;CronJob即定时任务，就类似于Linux系统的crontab，在指定的时间周期运行指定的任务。在Kubernetes 1.5，使用CronJob需要开启batch/v2alpha1 API，即–runtime-config=batch/v2alpha1。&lt;/p&gt;

&lt;p&gt;CronJob Spec&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.spec.schedule指定任务运行周期，格式同Cron
.spec.jobTemplate指定需要运行的任务，格式同Job
.spec.startingDeadlineSeconds指定任务开始的截止期限
.spec.concurrencyPolicy指定任务的并发策略，支持Allow、Forbid和Replace三个选项
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: batch/v2alpha1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: &amp;quot;*/1 * * * *&amp;quot;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f cronjob.yaml
cronjob &amp;quot;hello&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，也可以用kubectl run来创建一个CronJob：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl run hello --schedule=&amp;quot;*/1 * * * *&amp;quot; --restart=OnFailure --image=busybox -- /bin/sh -c &amp;quot;date; echo Hello from the Kubernetes cluster&amp;quot;
$ kubectl get cronjob
NAME      SCHEDULE      SUSPEND   ACTIVE    LAST-SCHEDULE
hello     */1 * * * *   False     0         &amp;lt;none&amp;gt;
$ kubectl get jobs
NAME               DESIRED   SUCCESSFUL   AGE
hello-1202039034   1         1            49s
$ pods=$(kubectl get pods --selector=job-name=hello-1202039034 --output=jsonpath={.items..metadata.name} -a)
$ kubectl logs $pods
Mon Aug 29 21:34:09 UTC 2016
Hello from the Kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，删除cronjob的时候不会自动删除job，这些job可以用kubectl delete job来删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete cronjob hello
cronjob &amp;quot;hello&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;job&#34;&gt;job&lt;/h2&gt;

&lt;p&gt;Job负责批量处理短暂的一次性任务 (short lived one-off tasks)，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束。&lt;/p&gt;

&lt;p&gt;Kubernetes支持以下几种Job：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;非并行Job：通常创建一个Pod直至其成功结束&lt;/li&gt;
&lt;li&gt;固定结束次数的Job：设置.spec.completions，创建多个Pod，直到.spec.completions个Pod成功结束&lt;/li&gt;
&lt;li&gt;带有工作队列的并行Job：设置.spec.Parallelism但不设置.spec.completions，当所有Pod结束并且至少一个成功时，Job就认为是成功&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据.spec.completions和.spec.Parallelism的设置，可以将Job划分为以下几种pattern：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Job类型 使用示例    行为  completions Parallelism&lt;/li&gt;
&lt;li&gt;一次性Job    数据库迁移   创建一个Pod直至其成功结束  1   1&lt;/li&gt;
&lt;li&gt;固定结束次数的Job    处理工作队列的Pod  依次创建一个Pod运行直至completions个成功结束   2+  1&lt;/li&gt;
&lt;li&gt;固定结束次数的并行Job  多个Pod同时处理工作队列   依次创建多个Pod运行直至completions个成功结束   2+  2+&lt;/li&gt;
&lt;li&gt;并行Job 多个Pod同时处理工作队列   创建一个或多个Pod直至有一个成功结束 1   2+&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Job Spec格式&lt;/p&gt;

&lt;p&gt;spec.template格式同Pod&lt;/p&gt;

&lt;p&gt;RestartPolicy仅支持Never或OnFailure&lt;/p&gt;

&lt;p&gt;单个Pod时，默认Pod成功运行后Job即结束&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.spec.completions标志Job结束需要成功运行的Pod个数，默认为1
.spec.parallelism标志并行运行的Pod的个数，默认为1
spec.activeDeadlineSeconds标志失败Pod的重试最大时间，超过这个时间不会继续重试
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个简单的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: [&amp;quot;perl&amp;quot;,  &amp;quot;-Mbignum=bpi&amp;quot;, &amp;quot;-wle&amp;quot;, &amp;quot;print bpi(2000)&amp;quot;]
      restartPolicy: Never
$ kubectl create -f ./job.yaml
job &amp;quot;pi&amp;quot; created
$ pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath={.items..metadata.name})
$ kubectl logs $pods
3.141592653589793238462643383279502...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;固定结束次数的Job示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: busybox
spec:
  completions: 3
  template:
    metadata:
      name: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        command: [&amp;quot;echo&amp;quot;, &amp;quot;hello&amp;quot;]
      restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;这边讲解的就是原生的资源定义和使用，其实都是通过控制器来完成基本的控制的，控制器的源码在&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller-manager/&#34;&gt;kube-controller-manager&lt;/a&gt;中。&lt;/p&gt;

&lt;h1 id=&#34;k8s自定义控制器&#34;&gt;k8s自定义控制器&lt;/h1&gt;

&lt;p&gt;日常业务开发过程中，虽然常规的资源基本满足需求，但是这些常规的资源大多仅仅是代表相对底层、通用的概念的对象， 某些情况下我们总是想根据业务定制我们的资源类型，并且利用kubernetes的声明式API，对资源的增删改查进行监听并作出具体的业务功能。随着Kubernetes生态系统的持续发展，越来越多高层次的对象将会不断涌现，比起目前使用的对象，新对象将更加专业化。&lt;/p&gt;

&lt;p&gt;在这一块，目前业界比较多使用自定义的就是阿里云，其开源的项目&lt;a href=&#34;https://github.com/openkruise/kruise&#34;&gt;kruise&lt;/a&gt;包含来很多的使用场景。具体可以看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-kruise/&#34;&gt;我的关于kruise的文章&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;最初开发流程&#34;&gt;最初开发流程&lt;/h2&gt;

&lt;p&gt;作为一个k8s研发，会写控制器是一项最基本的工作，我们最初是参考官方提供的&lt;a href=&#34;https://github.com/kubernetes/sample-controller&#34;&gt;sample-controller&lt;/a&gt;来完成控制器开发的，k8s自定义controller开发的整个过程：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建自定义API对象&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-api/#crd-自定义资源类型&#34;&gt;CRD（Custom Resource Definition）&lt;/a&gt;，令k8s明白我们自定义的API对象；&lt;/li&gt;
&lt;li&gt;编写代码，将CRD的情况写入对应的代码中，然后通过自动代码生成工具，将controller之外的informer，client等内容较为固定的代码通过工具生成；&lt;/li&gt;
&lt;li&gt;编写controller，在里面判断实际情况是否达到了API对象的声明情况，如果未达到，就要进行实际业务处理，而这也是controller的通用做法；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实际编码过程并不复杂，动手编写的文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── controller.go
├── main.go
└── pkg
    ├── apis
    │   └── test
    │       ├── register.go
    │       └── v1
    │           ├── doc.go
    │           ├── register.go
    │           └── types.go
    └── signals
        ├── signal.go
        ├── signal_posix.go
        └── signal_windows.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面我们以实例来看看具体的开发。&lt;/p&gt;

&lt;h3 id=&#34;定义crd&#34;&gt;定义CRD&lt;/h3&gt;

&lt;p&gt;创建jcy.yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  # metadata.name的内容是由&amp;quot;复数名.分组名&amp;quot;构成，如下，jcys是复数名，test.k8s.io是分组名
  name: jcy.test.k8s.io
spec:
  # 分组名，在REST API中也会用到的，格式是: /apis/分组名/CRD版本
  group: test.k8s.io
  # list of versions supported by this CustomResourceDefinition
  versions:
    - name: v1
      # 是否有效的开关.
      served: true
      # 只有一个版本能被标注为storage
      storage: true
  # 范围是属于namespace的
  scope: Namespaced
  names:
    # 复数名
    plural: jcys
    # 单数名
    singular: jcy
    # 类型名
    kind: Jcy
    # 简称，就像service的简称是svc
    shortNames:
    - j
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;yaml中的关键字段&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;group：设置API所属的组，将其映射为API URL中的 “/apis/” 下一级目录。它是逻辑上相关的Kinds集合，自定义的&lt;/li&gt;
&lt;li&gt;scope：该API的生效范围，可选项为Namespaced和Cluster。&lt;/li&gt;
&lt;li&gt;version：每个 Group 可以存在多个版本。例如，v1alpha1，然后升为 v1beta1，最后稳定为 v1 版本。&lt;/li&gt;
&lt;li&gt;ames：CRD的名称，包括单数、复数、kind、所属组等名称定义&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在jcy.yaml所在目录执行命令kubectl apply -f jcy.yaml，即可在k8s环境创建Jcy的定义，今后如果发起对类型为Jcy的对象的处理，k8s的api server就能识别到该对象类型了，如下所示，可以用kubectl get crd和kubectl describe crd stu命令查看更多细节，stu是在jcy.yaml中定义的简称&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master custom_controller]# kubectl apply -f jcy.yaml
customresourcedefinition.apiextensions.k8s.io/jcy.test.k8s.io created
[root@master custom_controller]# kubectl get crd
NAME                            CREATED AT
jcy.test.k8s.io   2019-03-30T13:33:13Z
[root@master custom_controller]# kubectl describe crd stu
Name:         jcys.test.k8s.io
Namespace:
Labels:       &amp;lt;none&amp;gt;
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&amp;quot;apiVersion&amp;quot;:&amp;quot;apiextensions.k8s.io/v1beta1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;CustomResourceDefinition&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;jcys.test...
API Version:  apiextensions.k8s.io/v1beta1
Kind:         CustomResourceDefinition
Metadata:
  Creation Timestamp:  2019-03-30T13:33:13Z
  Generation:          1
  Resource Version:    292010
  Self Link:           /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/jcys.test.k8s.io
  UID:                 5e4ceb6e-52f0-11e9-96e1-000c29f1f9c9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果您已配置好etcdctl，可以访问k8s的etcd上存储的数据，那么执行以下命令，就可以看到新的CRD已经保存在etcd中了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get /registry/apiextensions.k8s.io/customresourcedefinitions/ --prefix

/registry/apiextensions.k8s.io/customresourcedefinitions/jcys.test.k8s.io{
    &amp;quot;kind&amp;quot;: &amp;quot;CustomResourceDefinition&amp;quot;,
    &amp;quot;apiVersion&amp;quot;: &amp;quot;apiextensions.k8s.io/v1beta1&amp;quot;,
    &amp;quot;metadata&amp;quot;: {
        &amp;quot;name&amp;quot;: &amp;quot;jcys.test.k8s.io&amp;quot;,
        &amp;quot;uid&amp;quot;: &amp;quot;5e4ceb6e-52f0-11e9-96e1-000c29f1f9c9&amp;quot;,
        &amp;quot;generation&amp;quot;: 1,
        &amp;quot;creationTimestamp&amp;quot;: &amp;quot;2019-03-30T13:33:13Z&amp;quot;,
        &amp;quot;annotations&amp;quot;: {
            &amp;quot;kubectl.kubernetes.io/last-applied-configuration&amp;quot;: &amp;quot;{\&amp;quot;apiVersion\&amp;quot;:\&amp;quot;apiextensions.k8s.io/v1beta1\&amp;quot;,\&amp;quot;kind\&amp;quot;:\&amp;quot;CustomResourceDefinition\&amp;quot;,\&amp;quot;metadata\&amp;quot;:{\&amp;quot;annotations\&amp;quot;:{},\&amp;quot;name\&amp;quot;:\&amp;quot;jcys.test.k8s.io\&amp;quot;},\&amp;quot;spec\&amp;quot;:{\&amp;quot;group\&amp;quot;:\&amp;quot;test.k8s.io\&amp;quot;,\&amp;quot;names\&amp;quot;:{\&amp;quot;kind\&amp;quot;:\&amp;quot;Jcy\&amp;quot;,\&amp;quot;plural\&amp;quot;:\&amp;quot;jcys\&amp;quot;,\&amp;quot;shortNames\&amp;quot;:[\&amp;quot;stu\&amp;quot;],\&amp;quot;singular\&amp;quot;:\&amp;quot;jcy\&amp;quot;},\&amp;quot;scope\&amp;quot;:\&amp;quot;Namespaced\&amp;quot;,\&amp;quot;versions\&amp;quot;:[{\&amp;quot;name\&amp;quot;:\&amp;quot;v1\&amp;quot;,\&amp;quot;served\&amp;quot;:true,\&amp;quot;storage\&amp;quot;:true}]}}\n&amp;quot;
        }
    },
    &amp;quot;spec&amp;quot;: {
        &amp;quot;group&amp;quot;: &amp;quot;test.k8s.io&amp;quot;,
        &amp;quot;version&amp;quot;: &amp;quot;v1&amp;quot;,
        &amp;quot;names&amp;quot;: {
            &amp;quot;plural&amp;quot;: &amp;quot;jcys&amp;quot;,
            &amp;quot;singular&amp;quot;: &amp;quot;jcy&amp;quot;,
            &amp;quot;shortNames&amp;quot;: [
                &amp;quot;stu&amp;quot;
            ],
            &amp;quot;kind&amp;quot;: &amp;quot;Jcy&amp;quot;,
            &amp;quot;listKind&amp;quot;: &amp;quot;JcyList&amp;quot;
        },
        &amp;quot;scope&amp;quot;: &amp;quot;Namespaced&amp;quot;,
        &amp;quot;versions&amp;quot;: [
            {
                &amp;quot;name&amp;quot;: &amp;quot;v1&amp;quot;,
                &amp;quot;served&amp;quot;: true,
                &amp;quot;storage&amp;quot;: true
            }
        ],
        &amp;quot;conversion&amp;quot;: {
            &amp;quot;strategy&amp;quot;: &amp;quot;None&amp;quot;
        }
    },
    &amp;quot;status&amp;quot;: {
        &amp;quot;conditions&amp;quot;: [
            {
                &amp;quot;type&amp;quot;: &amp;quot;NamesAccepted&amp;quot;,
                &amp;quot;status&amp;quot;: &amp;quot;True&amp;quot;,
                &amp;quot;lastTransitionTime&amp;quot;: &amp;quot;2019-03-30T13:33:13Z&amp;quot;,
                &amp;quot;reason&amp;quot;: &amp;quot;NoConflicts&amp;quot;,
                &amp;quot;message&amp;quot;: &amp;quot;no conflicts found&amp;quot;
            },
            {
                &amp;quot;type&amp;quot;: &amp;quot;Established&amp;quot;,
                &amp;quot;status&amp;quot;: &amp;quot;True&amp;quot;,
                &amp;quot;lastTransitionTime&amp;quot;: null,
                &amp;quot;reason&amp;quot;: &amp;quot;InitialNamesAccepted&amp;quot;,
                &amp;quot;message&amp;quot;: &amp;quot;the initial names have been accepted&amp;quot;
            }
        ],
        &amp;quot;acceptedNames&amp;quot;: {
            &amp;quot;plural&amp;quot;: &amp;quot;jcys&amp;quot;,
            &amp;quot;singular&amp;quot;: &amp;quot;jcy&amp;quot;,
            &amp;quot;shortNames&amp;quot;: [
                &amp;quot;stu&amp;quot;
            ],
            &amp;quot;kind&amp;quot;: &amp;quot;Jcy&amp;quot;,
            &amp;quot;listKind&amp;quot;: &amp;quot;JcyList&amp;quot;
        },
        &amp;quot;storedVersions&amp;quot;: [
            &amp;quot;v1&amp;quot;
        ]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面就可以创建stu类型的对象了，比如我们创建object-jcy.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: test.k8s.io/v1
kind: Jcy
metadata:
  name: object-jcy
spec:
  name: &amp;quot;张三&amp;quot;
  school: &amp;quot;深圳中学&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个资源对象跟定义pod差不多，它的主要信息都是来源上面的定义，Kind是Jcy，apiVersion就是group/version，除了这些设置，还需要在spec端设置相应的参数，一般是开发者自定义定制的。&lt;/p&gt;

&lt;p&gt;在object-jcy.yaml文件所在目录执行命令kubectl apply -f object-jcy.yaml，会看到提示创建成功&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master custom_controller]# kubectl apply -f object-jcy.yaml
jcy.test.k8s.io/object-jcy created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;行命令kubectl get stu可见已创建成功的Jcy对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master custom_controller]# kubectl get jcy
NAME             AGE
object-jcy   15s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;控制台输出的就是该Jcy对象存储在etcd中的内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;apiVersion&amp;quot;: &amp;quot;test.k8s.io/v1&amp;quot;,
    &amp;quot;kind&amp;quot;: &amp;quot;Jcy&amp;quot;,
    &amp;quot;metadata&amp;quot;: {
        &amp;quot;annotations&amp;quot;: {
            &amp;quot;kubectl.kubernetes.io/last-applied-configuration&amp;quot;: &amp;quot;{\&amp;quot;apiVersion\&amp;quot;:\&amp;quot;test.k8s.io/v1\&amp;quot;,\&amp;quot;kind\&amp;quot;:\&amp;quot;Jcy\&amp;quot;,\&amp;quot;metadata\&amp;quot;:{\&amp;quot;annotations\&amp;quot;:{},\&amp;quot;name\&amp;quot;:\&amp;quot;object-jcy\&amp;quot;,\&amp;quot;namespace\&amp;quot;:\&amp;quot;default\&amp;quot;},\&amp;quot;spec\&amp;quot;:{\&amp;quot;name\&amp;quot;:\&amp;quot;张三\&amp;quot;,\&amp;quot;school\&amp;quot;:\&amp;quot;深圳中学\&amp;quot;}}\n&amp;quot;
        },
        &amp;quot;creationTimestamp&amp;quot;: &amp;quot;2019-03-31T02:56:25Z&amp;quot;,
        &amp;quot;generation&amp;quot;: 1,
        &amp;quot;name&amp;quot;: &amp;quot;object-jcy&amp;quot;,
        &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;,
        &amp;quot;uid&amp;quot;: &amp;quot;92927d0d-5360-11e9-9d2a-000c29f1f9c9&amp;quot;
    },
    &amp;quot;spec&amp;quot;: {
        &amp;quot;name&amp;quot;: &amp;quot;张三&amp;quot;,
        &amp;quot;school&amp;quot;: &amp;quot;深圳中学&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，自定义API对象（也就是CRD）就创建成功了。&lt;/p&gt;

&lt;h3 id=&#34;crd控制器的原理&#34;&gt;CRD控制器的原理&lt;/h3&gt;

&lt;p&gt;在正式开发控制器之前，我们先理解一下自定义控制器的工作原理，因为很多东西直接是用工具生成的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为什么要做controller？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果仅仅是在etcd保存Jcy对象是没有什么意义的，试想通过deployment创建pod时，如果只在etcd创建pod对象，而不去node节点创建容器，那这个pod对象只是一条数据而已，没有什么实质性作用，其他对象如service、pv也是如此。&lt;/p&gt;

&lt;p&gt;controller的作用就是监听指定对象的新增、删除、修改等变化，针对这些变化做出相应的响应（例如新增pod的响应为创建docker容器），原理如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/develop&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;API对象的变化会通过Informer存入队列（WorkQueue），在Controller中消费队列的数据做出响应，响应相关的具体代码就是我们要做的真正业务逻辑，也就是我们要实现的控制器部分业务代码。&lt;/p&gt;

&lt;p&gt;CRD控制器的informer工作流，可分为监听、同步、触发三个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Controller 首先会通过Informer （所谓的 Informer，就是一个自带缓存和索引机制），从K8ss的API Server中获取它所关心的对象，举个例子，也就是我编写的Controller获取的应该是jcy对象。值得注意的是Informer在构建之前，会使用我们生成的client,再透过Reflector的ListAndWatch机制跟API Server建立连接，不断地监听jcy对象实例的变化。在 ListAndWatch 机制下，一旦 APIServer 端有新的 jcy 实例被创建、删除或者更新，Reflector 都会收到“事件通知”。该事件及它对应的 API 对象会被放进一个 Delta FIFO Queue中。&lt;/li&gt;
&lt;li&gt;Local Store 此时完成同步缓存操作&lt;/li&gt;
&lt;li&gt;Informer 根据这些事件的类型，触发我们编写并注册号的ResourceEventHandler，完成业务动作的触发。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;代码实际上可以通过&lt;a href=&#34;https://github.com/kubernetes/code-generator&#34;&gt;code-generator&lt;/a&gt;官方提供的工具生成的。&lt;/p&gt;

&lt;h3 id=&#34;将controller之外的informer-client等内容较为固定的代码通过工具生成&#34;&gt;将controller之外的informer，client等内容较为固定的代码通过工具生成&lt;/h3&gt;

&lt;p&gt;从上图可以发现整个逻辑还是比较复杂的，为了简化我们的自定义controller开发，k8s的大师们利用自动代码生成工具将controller之外的事情都做好了，我们只要专注于controller的开发就好。&lt;/p&gt;

&lt;p&gt;1、$GOPATH/src/目录下创建一个文件夹k8s_customize_controller：&lt;/p&gt;

&lt;p&gt;2、进入文件夹k8s_customize_controller，执行如下命令创建三层目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p pkg/apis/jcy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、在新建的jcy目录下创建文件register.go，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package jcy

const (
        GroupName = &amp;quot;test.k8s.io&amp;quot;
        Version   = &amp;quot;v1&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、在新建的jcy目录下创建名为v1的文件夹；&lt;/p&gt;

&lt;p&gt;5、在新建的v1文件夹下创建文件doc.go，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// +k8s:deepcopy-gen=package

// +groupName=test.k8s.io
package v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述代码中的两行注释，都是代码生成工具会用到的，一个是声明为整个v1包下的类型定义生成DeepCopy方法，另一个声明了这个包对应的API的组名，和CRD中的组名一致；&lt;/p&gt;

&lt;p&gt;6、在v1文件夹下创建文件types.go，里面定义了Jcy对象的具体内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package v1

import (
    metav1 &amp;quot;k8s.io/apimachinery/pkg/apis/meta/v1&amp;quot;
)

// +genclient
// +genclient:noStatus
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

type Jcy struct {
    metav1.TypeMeta   `json:&amp;quot;,inline&amp;quot;`
    metav1.ObjectMeta `json:&amp;quot;metadata,omitempty&amp;quot;`
    Spec              JcySpec `json:&amp;quot;spec&amp;quot;`
}

type JcySpec struct {
    name   string `json:&amp;quot;name&amp;quot;`
    school string `json:&amp;quot;school&amp;quot;`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// JcytList is a list of Jcy resources
type JcytList struct {
    metav1.TypeMeta `json:&amp;quot;,inline&amp;quot;`
    metav1.ListMeta `json:&amp;quot;metadata&amp;quot;`

    Items []Jcy `json:&amp;quot;items&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上述源码可见，Jcy对象的内容已经被设定好，主要有name和school这两个字段，表示学生的名字和所在学校，因此创建Jcy对象的时候内容就要和这里匹配了；&lt;/p&gt;

&lt;p&gt;在这个文件中也有几个k8s的Annotation 风格的注释&lt;/p&gt;

&lt;p&gt;+genclient 这段注解的意思是：请为下面资源类型生成对应的 Client 代码。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;+genclient 这段注解的意思是：请为下面资源类型生成对应的 Client 代码。&lt;/li&gt;
&lt;li&gt;+genclient:noStatus 的意思是：这个 API 资源类型定义里，没有 Status 字段，因为Mydemo才是主类型，所以 +genclient 要写在Mydemo之上，不用写在MydemoList之上，这时要细心注意的。&lt;/li&gt;
&lt;li&gt;+k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object 的意思是，请在生成 DeepCopy 的时候，实现 Kubernetes 提供的 runtime.Object 接口。否则，在某些版本的 Kubernetes 里，你的这个类型定义会出现编译错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;7、在v1目录下创建register.go文件，此文件的作用是通过addKnownTypes方法使得client可以知道Jcy类型的API对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package v1

import (
    metav1 &amp;quot;k8s.io/apimachinery/pkg/apis/meta/v1&amp;quot;
    &amp;quot;k8s.io/apimachinery/pkg/runtime&amp;quot;
    &amp;quot;k8s.io/apimachinery/pkg/runtime/schema&amp;quot;

    &amp;quot;k8s_customize_controller/pkg/apis/test&amp;quot;
)

var SchemeGroupVersion = schema.GroupVersion{
    Group:   Jcy.GroupName,
    Version: Jcy.Version,
}

var (
    SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)
    AddToScheme   = SchemeBuilder.AddToScheme
)

func Resource(resource string) schema.GroupResource {
    return SchemeGroupVersion.WithResource(resource).GroupResource()
}

func Kind(kind string) schema.GroupKind {
    return SchemeGroupVersion.WithKind(kind).GroupKind()
}

func addKnownTypes(scheme *runtime.Scheme) error {
    scheme.AddKnownTypes(
        SchemeGroupVersion,
        &amp;amp;Jcy{},
        &amp;amp;JcyList{},
    )

    // register the type in the scheme
    metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，为自动生成代码做的准备工作已经完成了，目前为止，$GOPATH/src目录下的文件和目录结构是这样的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@golang src]# tree
.
└── k8s_customize_controller
    └── pkg
        └── apis
            └── jcy
                ├── register.go
                └── v1
                    ├── doc.go
                    ├── register.go
                    └── types.go

5 directories, 4 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8、执行以下命令，会先下载依赖包，再下载代码生成工具&lt;a href=&#34;https://github.com/kubernetes/code-generator&#34;&gt;code-generator&lt;/a&gt;，再执行代码生成脚本generate-groups.sh：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd $GOPATH/src \
&amp;amp;&amp;amp; go get -u k8s.io/apimachinery/pkg/apis/meta/v1 \
&amp;amp;&amp;amp; go get -u k8s.io/code-generator/... \
&amp;amp;&amp;amp; cd $GOPATH/src/k8s.io/code-generator \
&amp;amp;&amp;amp; ./generate-groups.sh all \
k8s_customize_controller/pkg/client \
k8s_customize_controller/pkg/apis \
test:v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果代码写得没有问题，会看到以下输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Generating deepcopy funcs
Generating clientset for test:v1 at k8s_customize_controller/pkg/client/clientset
Generating listers for test:v1 at k8s_customize_controller/pkg/client/listers
Generating informers for test:v1 at k8s_customize_controller/pkg/client/informers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时再去$GOPATH/src/k8s_customize_controller目录下执行tree命令，可见已生成了很多内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master k8s_customize_controller]# tree
.
└── pkg
    ├── apis
    │   └── jcy
    │       ├── register.go
    │       └── v1
    │           ├── doc.go
    │           ├── register.go
    │           ├── types.go
    │           └── zz_generated.deepcopy.go
    └── client
        ├── clientset
        │   └── versioned
        │       ├── clientset.go
        │       ├── doc.go
        │       ├── fake
        │       │   ├── clientset_generated.go
        │       │   ├── doc.go
        │       │   └── register.go
        │       ├── scheme
        │       │   ├── doc.go
        │       │   └── register.go
        │       └── typed
        │           └── test
        │               └── v1
        │                   ├── test_client.go
        │                   ├── doc.go
        │                   ├── fake
        │                   │   ├── doc.go
        │                   │   ├── fake_test_client.go
        │                   │   └── fake_jcy.go
        │                   ├── generated_expansion.go
        │                   └── jcy.go
        ├── informers
        │   └── externalversions
        │       ├── test
        │       │   ├── interface.go
        │       │   └── v1
        │       │       ├── interface.go
        │       │       └── jcy.go
        │       ├── factory.go
        │       ├── generic.go
        │       └── internalinterfaces
        │           └── factory_interfaces.go
        └── listers
            └── test
                └── v1
                    ├── expansion_generated.go
                    └── jcy.go

21 directories, 27 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如上所示，zz_generated.deepcopy.go就是DeepCopy代码文件，client目录下的内容都是客户端相关代码，在开发controller时会用到；&lt;/p&gt;

&lt;p&gt;client目录下的clientset、informers、listers的身份和作用可以和前面的原理图中的不同模块结合来理解；&lt;/p&gt;

&lt;p&gt;至此，自动生成代码的步骤已经完成。下面就是写我们的controller的逻辑了&lt;/p&gt;

&lt;h3 id=&#34;编写controller代码&#34;&gt;编写controller代码&lt;/h3&gt;

&lt;p&gt;在k8s_customize_controller目录下创建controller.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;time&amp;quot;

    &amp;quot;github.com/golang/glog&amp;quot;
    corev1 &amp;quot;k8s.io/api/core/v1&amp;quot;
    &amp;quot;k8s.io/apimachinery/pkg/api/errors&amp;quot;
    &amp;quot;k8s.io/apimachinery/pkg/util/runtime&amp;quot;
    utilruntime &amp;quot;k8s.io/apimachinery/pkg/util/runtime&amp;quot;
    &amp;quot;k8s.io/apimachinery/pkg/util/wait&amp;quot;
    &amp;quot;k8s.io/client-go/kubernetes&amp;quot;
    &amp;quot;k8s.io/client-go/kubernetes/scheme&amp;quot;
    typedcorev1 &amp;quot;k8s.io/client-go/kubernetes/typed/core/v1&amp;quot;
    &amp;quot;k8s.io/client-go/tools/cache&amp;quot;
    &amp;quot;k8s.io/client-go/tools/record&amp;quot;
    &amp;quot;k8s.io/client-go/util/workqueue&amp;quot;

    jcy &amp;quot;github.com/kingjcy/k8s-controller-custom-resource/pkg/apis/jcy/v1&amp;quot;
    clientset &amp;quot;github.com/kingjcy/k8s-controller-custom-resource/pkg/client/clientset/versioned&amp;quot;
    jcyscheme &amp;quot;github.com/kingjcy/k8s-controller-custom-resource/pkg/client/clientset/versioned/scheme&amp;quot;
    informers &amp;quot;github.com/kingjcy/k8s-controller-custom-resource/pkg/client/informers/externalversions/jcy/v1&amp;quot;
    listers &amp;quot;github.com/kingjcy/k8s-controller-custom-resource/pkg/client/listers/jcy/v1&amp;quot;
)

const controllerAgentName = &amp;quot;jcy-controller&amp;quot;

const (
    SuccessSynced = &amp;quot;Synced&amp;quot;

    MessageResourceSynced = &amp;quot;Jcy synced successfully&amp;quot;
)

// Controller is the controller implementation for Jcy resources
type Controller struct {
    // kubeclientset is a standard kubernetes clientset
    kubeclientset kubernetes.Interface
    // jcyclientset is a clientset for our own API group
    jcyclientset clientset.Interface

    jcysLister listers.JcyLister
    jcysSynced cache.InformerSynced

    workqueue workqueue.RateLimitingInterface

    recorder record.EventRecorder
}

// NewController returns a new jcy controller
func NewController(
kubeclientset kubernetes.Interface,
jcyclientset clientset.Interface,
jcyInformer informers.jcyInformer) *Controller {
    utilruntime.Must(jcyscheme.AddToScheme(scheme.Scheme))
    glog.V(4).Info(&amp;quot;Creating event broadcaster&amp;quot;)
    eventBroadcaster := record.NewBroadcaster()
    eventBroadcaster.StartLogging(glog.Infof)
    eventBroadcaster.StartRecordingToSink(&amp;amp;typedcorev1.EventSinkImpl{Interface: kubeclientset.CoreV1().Events(&amp;quot;&amp;quot;)})
    recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: controllerAgentName})

    //使用client 和前面创建的 Informer，初始化了自定义控制器结构体
    controller := &amp;amp;Controller{
        kubeclientset:    kubeclientset,
        jcyclientset: jcyclientset,
        jcysLister:   jcyInformer.Lister(),
        jcysSynced:   jcyInformer.Informer().HasSynced,
        workqueue:        workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &amp;quot;Jcys&amp;quot;),
        recorder:         recorder,
    }


    //jcyInformer注册了三个 Handler（AddFunc、UpdateFunc 和 DeleteFunc）,分别对应 API 对象的“添加”“更新”和“删除”事件。而具体的处理操作，都是将该事件对应的 API 对象加入到工作队列中

    glog.Info(&amp;quot;Setting up event handlers&amp;quot;)
    // Set up an event handler for when Jcy resources change
    jcyInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: controller.enqueueJcy,
        UpdateFunc: func(old, new interface{}) {
            oldJcy := old.(*test.Jcy)
            newJcy := new.(*test.Jcy)
            if oldJcy.ResourceVersion == newJcy.ResourceVersion {
                //版本一致，就表示没有实际更新的操作，立即返回
                return
            }
            controller.enqueueJcy(new)
        },
        DeleteFunc: controller.enqueueJcyForDelete,
    })

    return controller
}

//在此处开始controller的业务，也就是原理图中的Control Loop的部分，启动控制循环的逻辑非常简单，就是同步+循环监听任务。而这个循环监听任务就是我们真正的业务实现部分了
func (c *Controller) Run(threadiness int, stopCh &amp;lt;-chan struct{}) error {
    defer runtime.HandleCrash()
    defer c.workqueue.ShutDown()

    glog.Info(&amp;quot;开始controller业务，开始一次缓存数据同步&amp;quot;)
    if ok := cache.WaitForCacheSync(stopCh, c.jcysSynced); !ok {
        return fmt.Errorf(&amp;quot;failed to wait for caches to sync&amp;quot;)
    }

    glog.Info(&amp;quot;worker启动&amp;quot;)
    for i := 0; i &amp;lt; threadiness; i++ {
        go wait.Until(c.runWorker, time.Second, stopCh)
    }

    glog.Info(&amp;quot;worker已经启动&amp;quot;)
    &amp;lt;-stopCh
    glog.Info(&amp;quot;worker已经结束&amp;quot;)

    return nil
}

//runWorker是一个不断运行的方法，并且一直会调用 c.processNextWorkItem 从workqueue读取和读取消息
func (c *Controller) runWorker() {
    for c.processNextWorkItem() {
    }
}

// 从workqueue取数据处理
func (c *Controller) processNextWorkItem() bool {

    obj, shutdown := c.workqueue.Get()

    if shutdown {
        return false
    }

    // We wrap this block in a func so we can defer c.workqueue.Done.
    err := func(obj interface{}) error {
        defer c.workqueue.Done(obj)
        var key string
        var ok bool

        if key, ok = obj.(string); !ok {

            c.workqueue.Forget(obj)
            runtime.HandleError(fmt.Errorf(&amp;quot;expected string in workqueue but got %#v&amp;quot;, obj))
            return nil
        }
        // 在syncHandler中处理业务
        if err := c.syncHandler(key); err != nil {
            return fmt.Errorf(&amp;quot;error syncing &#39;%s&#39;: %s&amp;quot;, key, err.Error())
        }

        c.workqueue.Forget(obj)
        glog.Infof(&amp;quot;Successfully synced &#39;%s&#39;&amp;quot;, key)
        return nil
    }(obj)

    if err != nil {
        runtime.HandleError(err)
        return true
    }

    return true
}

// 处理，尝试从 Informer 维护的缓存中拿到了它所对应的对象
func (c *Controller) syncHandler(key string) error {
    // Convert the namespace/name string into a distinct namespace and name
    namespace, name, err := cache.SplitMetaNamespaceKey(key)
    if err != nil {
        runtime.HandleError(fmt.Errorf(&amp;quot;invalid resource key: %s&amp;quot;, key))
        return nil
    }

    // 从缓存中取对象
    jcy, err := c.jcysLister.jcys(namespace).Get(name)
    if err != nil {
        // 如果Jcy对象被删除了，就会走到这里，所以应该在这里加入执行
        if errors.IsNotFound(err) {
            glog.Infof(&amp;quot;Jcy对象被删除，请在这里执行实际的删除业务: %s/%s ...&amp;quot;, namespace, name)

            return nil
        }

        runtime.HandleError(fmt.Errorf(&amp;quot;failed to list jcy by: %s/%s&amp;quot;, namespace, name))

        return err
    }

    glog.Infof(&amp;quot;这里是jcy对象的期望状态: %#v ...&amp;quot;, jcy)
    glog.Infof(&amp;quot;实际状态是从业务层面得到的，此处应该去的实际状态，与期望状态做对比，并根据差异做出响应(新增或者删除)&amp;quot;)

    c.recorder.Event(jcy, corev1.EventTypeNormal, SuccessSynced, MessageResourceSynced)
    return nil
}

// 数据先放入缓存，再入队列
func (c *Controller) enqueueJcy(obj interface{}) {
    var key string
    var err error
    // 将对象放入缓存
    if key, err = cache.MetaNamespaceKeyFunc(obj); err != nil {
        runtime.HandleError(err)
        return
    }

    // 将key放入队列
    c.workqueue.AddRateLimited(key)
}

// 删除操作
func (c *Controller) enqueueJcyForDelete(obj interface{}) {
    var key string
    var err error
    // 从缓存中删除指定对象
    key, err = cache.DeletionHandlingMetaNamespaceKeyFunc(obj)
    if err != nil {
        runtime.HandleError(err)
        return
    }
    //再将key放入队列
    c.workqueue.AddRateLimited(key)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述代码有以下几处关键点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建controller的NewController方法中，定义了收到Jcy对象的增删改消息时的具体处理逻辑，除了同步本地缓存，就是将该对象的key放入消息中；&lt;/li&gt;
&lt;li&gt;实际处理消息的方法是syncHandler，这里面可以添加实际的业务代码，来响应Jcy对象的增删改情况，达到业务目的；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、在$GOPATH/src/k8s_customize_controller/pkg目录下新建目录signals，在signals目录下新建文件signal_posix.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// +build !windows

package signals

import (
    &amp;quot;os&amp;quot;
    &amp;quot;syscall&amp;quot;
)

var shutdownSignals = []os.Signal{os.Interrupt, syscall.SIGTERM}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在signals目录下新建文件signal.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package signals

import (
        &amp;quot;os&amp;quot;
        &amp;quot;os/signal&amp;quot;
)

var onlyOneSignalHandler = make(chan struct{})

func SetupSignalHandler() (stopCh &amp;lt;-chan struct{}) {
        close(onlyOneSignalHandler) // panics when called twice

        stop := make(chan struct{})
        c := make(chan os.Signal, 2)
        signal.Notify(c, shutdownSignals...)
        go func() {
                &amp;lt;-c
                close(stop)
                &amp;lt;-c
                os.Exit(1) // second signal. Exit directly.
        }()

        return stop
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、接下来可以编写main.go了，在k8s_customize_controller目录下创建main.go文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;flag&amp;quot;
    &amp;quot;time&amp;quot;

    &amp;quot;github.com/golang/glog&amp;quot;
    &amp;quot;k8s.io/client-go/kubernetes&amp;quot;
    &amp;quot;k8s.io/client-go/tools/clientcmd&amp;quot;
    // Uncomment the following line to load the gcp plugin (only required to authenticate against GKE clusters).
    // _ &amp;quot;k8s.io/client-go/plugin/pkg/client/auth/gcp&amp;quot;

    clientset &amp;quot;k8s_customize_controller/pkg/client/clientset/versioned&amp;quot;
    informers &amp;quot;k8s_customize_controller/pkg/client/informers/externalversions&amp;quot;
    &amp;quot;k8s_customize_controller/pkg/signals&amp;quot;
)

var (
    masterURL  string
    kubeconfig string
)

func main() {
    flag.Parse()

    // 处理信号量
    stopCh := signals.SetupSignalHandler()

    // 处理入参
    cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)
    if err != nil {
        glog.Fatalf(&amp;quot;Error building kubeconfig: %s&amp;quot;, err.Error())
    }

    kubeClient, err := kubernetes.NewForConfig(cfg)
    if err != nil {
        glog.Fatalf(&amp;quot;Error building kubernetes clientset: %s&amp;quot;, err.Error())
    }

    jcyClient, err := clientset.NewForConfig(cfg)
    if err != nil {
        glog.Fatalf(&amp;quot;Error building example clientset: %s&amp;quot;, err.Error())
    }

    jcyInformerFactory := informers.NewSharedInformerFactory(jcyClient, time.Second*30)

    //得到controller
    controller := NewController(kubeClient, jcyClient,
        jcyInformerFactory.Bolingcavalry().V1().Jcys())

    //启动informer
    go jcyInformerFactory.Start(stopCh)

    //controller开始处理消息
    if err = controller.Run(2, stopCh); err != nil {
        glog.Fatalf(&amp;quot;Error running controller: %s&amp;quot;, err.Error())
    }
}

func init() {
    flag.StringVar(&amp;amp;kubeconfig, &amp;quot;kubeconfig&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;Path to a kubeconfig. Only required if out-of-cluster.&amp;quot;)
    flag.StringVar(&amp;amp;masterURL, &amp;quot;master&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;部署验证&#34;&gt;部署验证&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;编译和启动&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在$GOPATH/src/k8s_customize_controller目录下，执行以下命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get k8s.io/client-go/kubernetes/scheme \
&amp;amp;&amp;amp; go get github.com/golang/glog \
&amp;amp;&amp;amp; go get k8s.io/kube-openapi/pkg/util/proto \
&amp;amp;&amp;amp; go get k8s.io/utils/buffer \
&amp;amp;&amp;amp; go get k8s.io/utils/integer \
&amp;amp;&amp;amp; go get k8s.io/utils/trace
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;上述脚本将编译过程中依赖的库通过go get方式进行获取，属于笨办法，更好的方法是选用一种包依赖工具，具体的可以参照k8s的官方demo，这个代码中同时提供了godep和vendor两种方式来处理上面的包依赖问题，地址是：&lt;a href=&#34;https://github.com/kubernetes/sample-controller&#34;&gt;https://github.com/kubernetes/sample-controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;解决了包依赖问题后，在$GOPATH/src/k8s_customize_controller目录下执行命令go build，即可在当前目录生成k8s_customize_controller文件；&lt;/li&gt;
&lt;li&gt;将文件k8s_customize_controller复制到k8s环境中，记得通过chmod a+x命令给其可执行权限；&lt;/li&gt;
&lt;li&gt;执行命令./k8s_customize_controller -kubeconfig=$HOME/.kube/config -alsologtostderr=true，会立即启动controller，看到控制台输出如下&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master 31]# ./k8s_customize_controller -kubeconfig=$HOME/.kube/config -alsologtostderr=true
I0331 23:27:17.909265   21540 controller.go:72] Setting up event handlers
I0331 23:27:17.909450   21540 controller.go:96] 开始controller业务，开始一次缓存数据同步
I0331 23:27:18.110448   21540 controller.go:101] worker启动
I0331 23:27:18.110516   21540 controller.go:106] worker已经启动
I0331 23:27:18.110653   21540 controller.go:181] 这里是jcy对象的期望状态: &amp;amp;v1.Jcy{TypeMeta:v1.TypeMeta{Kind:&amp;quot;Jcy&amp;quot;, APIVersion:&amp;quot;test.k8s.io/v1&amp;quot;}, ObjectMeta:v1.ObjectMeta{Name:&amp;quot;object-jcy&amp;quot;, GenerateName:&amp;quot;&amp;quot;, Namespace:&amp;quot;default&amp;quot;, SelfLink:&amp;quot;/apis/test.k8s.io/v1/namespaces/default/jcys/object-jcy&amp;quot;, UID:&amp;quot;92927d0d-5360-11e9-9d2a-000c29f1f9c9&amp;quot;, ResourceVersion:&amp;quot;310395&amp;quot;, Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63689597785, loc:(*time.Location)(0x1f9c200)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{&amp;quot;kubectl.kubernetes.io/last-applied-configuration&amp;quot;:&amp;quot;{\&amp;quot;apiVersion\&amp;quot;:\&amp;quot;test.k8s.io/v1\&amp;quot;,\&amp;quot;kind\&amp;quot;:\&amp;quot;Jcy\&amp;quot;,\&amp;quot;metadata\&amp;quot;:{\&amp;quot;annotations\&amp;quot;:{},\&amp;quot;name\&amp;quot;:\&amp;quot;object-jcy\&amp;quot;,\&amp;quot;namespace\&amp;quot;:\&amp;quot;default\&amp;quot;},\&amp;quot;spec\&amp;quot;:{\&amp;quot;name\&amp;quot;:\&amp;quot;张三\&amp;quot;,\&amp;quot;school\&amp;quot;:\&amp;quot;深圳中学\&amp;quot;}}\n&amp;quot;}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:&amp;quot;&amp;quot;, ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.JcySpec{name:&amp;quot;&amp;quot;, school:&amp;quot;&amp;quot;}} ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，自定义controller已经启动成功了，并且从缓存中获取到了上一章中创建的对象的信息，接下来我们在k8s环境对Jcy对象做增删改，看看controller是否能做出响应；&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;验证controller&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;新开一个窗口连接到k8s环境，新建一个名为new-jcy.yaml的文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: test.k8s.io/v1
kind: Jcy
metadata:
  name: new-jcy
spec:
  name: &amp;quot;李四&amp;quot;
  school: &amp;quot;深圳小学&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在new-jcy.yaml所在目录执行命令kubectl apply -f new-jcy.yaml；&lt;/p&gt;

&lt;p&gt;返回controller所在的控制台窗口，发现新输出了如下内容，可见新增jcy对象的事件已经被controller监听并处理：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I0331 23:43:03.789894   21540 controller.go:181] 这里是jcy对象的期望状态: &amp;amp;v1.Jcy{TypeMeta:v1.TypeMeta{Kind:&amp;quot;&amp;quot;, APIVersion:&amp;quot;&amp;quot;}, ObjectMeta:v1.ObjectMeta{Name:&amp;quot;new-jcy&amp;quot;, GenerateName:&amp;quot;&amp;quot;, Namespace:&amp;quot;default&amp;quot;, SelfLink:&amp;quot;/apis/test.k8s.io/v1/namespaces/default/jcys/new-jcy&amp;quot;, UID:&amp;quot;abcd77d6-53cb-11e9-9d2a-000c29f1f9c9&amp;quot;, ResourceVersion:&amp;quot;370653&amp;quot;, Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63689643783, loc:(*time.Location)(0x1f9c200)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{&amp;quot;kubectl.kubernetes.io/last-applied-configuration&amp;quot;:&amp;quot;{\&amp;quot;apiVersion\&amp;quot;:\&amp;quot;test.k8s.io/v1\&amp;quot;,\&amp;quot;kind\&amp;quot;:\&amp;quot;Jcy\&amp;quot;,\&amp;quot;metadata\&amp;quot;:{\&amp;quot;annotations\&amp;quot;:{},\&amp;quot;name\&amp;quot;:\&amp;quot;new-jcy\&amp;quot;,\&amp;quot;namespace\&amp;quot;:\&amp;quot;default\&amp;quot;},\&amp;quot;spec\&amp;quot;:{\&amp;quot;name\&amp;quot;:\&amp;quot;李四\&amp;quot;,\&amp;quot;school\&amp;quot;:\&amp;quot;深圳小学\&amp;quot;}}\n&amp;quot;}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:&amp;quot;&amp;quot;, ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.JcySpec{name:&amp;quot;&amp;quot;, school:&amp;quot;&amp;quot;}} ...
I0331 23:43:03.790076   21540 controller.go:182] 实际状态是从业务层面得到的，此处应该去的实际状态，与期望状态做对比，并根据差异做出响应(新增或者删除)
I0331 23:43:03.790120   21540 controller.go:145] Successfully synced &#39;default/new-jcy&#39;
I0331 23:43:03.790141   21540 event.go:209] Event(v1.ObjectReference{Kind:&amp;quot;Jcy&amp;quot;, Namespace:&amp;quot;default&amp;quot;, Name:&amp;quot;new-jcy&amp;quot;, UID:&amp;quot;abcd77d6-53cb-11e9-9d2a-000c29f1f9c9&amp;quot;, APIVersion:&amp;quot;test.k8s.io/v1&amp;quot;, ResourceVersion:&amp;quot;370653&amp;quot;, FieldPath:&amp;quot;&amp;quot;}): type: &#39;Normal&#39; reason: &#39;Synced&#39; Jcy synced successfully
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来您也可以尝试修改和删除已有的Jcy对象，观察controller控制台的输出，确定是否已经监听到所有jcy变化的事件，例如删除的事件日志如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I0331 23:44:37.236090   21540 controller.go:171] Jcy对象被删除，请在这里执行实际的删除业务: default/new-jcy ...
I0331 23:44:37.236118   21540 controller.go:145] Successfully synced &#39;default/new-jcy&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kubebuilder&#34;&gt;kubebuilder&lt;/h2&gt;

&lt;p&gt;Kubebuilder 的工作流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建一个新的工程目录&lt;/li&gt;
&lt;li&gt;创建一个或多个资源 API CRD 然后将字段添加到资源&lt;/li&gt;
&lt;li&gt;在控制器中实现协调循环（reconcile loop），watch 额外的资源&lt;/li&gt;
&lt;li&gt;在集群中运行测试（自动安装 CRD 并自动启动控制器）&lt;/li&gt;
&lt;li&gt;更新引导集成测试测试新字段和业务逻辑&lt;/li&gt;
&lt;li&gt;使用用户提供的 Dockerfile 构建和发布容器&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;依赖&#34;&gt;依赖&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;go version v1.15+.&lt;/li&gt;
&lt;li&gt;docker version 17.03+.&lt;/li&gt;
&lt;li&gt;kubectl version v1.11.3+.&lt;/li&gt;
&lt;li&gt;kustomize v3.1.0+&lt;/li&gt;
&lt;li&gt;能够访问 Kubernetes v1.11.3+ 集群&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有一些kubebuilder依赖的重要库&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34;&gt;controller-runtime&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/controller-tools&#34;&gt;controller-tools&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;安装&#34;&gt;安装&lt;/h3&gt;

&lt;p&gt;直接去github项目上下载release对应的系统文件，或者使用命令下载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;os=$(go env GOOS)
arch=$(go env GOARCH)

# 下载 kubebuilder 并解压到 tmp 目录中
curl -L https://go.kubebuilder.io/dl/2.3.1/${os}/${arch} | tar -xz -C /tmp/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将其放到可执行的路径下，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo mv /tmp/kubebuilder_2.3.1_${os}_${arch} /usr/local/kubebuilder
export PATH=$PATH:/usr/local/kubebuilder/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;使用&#34;&gt;使用&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;创建一个项目&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;创建一个目录，然后在里面运行 kubebuilder init 命令，初始化一个新项目,&amp;ndash;domain flag arg 来指定 api group。示例如下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir $GOPATH/src/example
cd $GOPATH/src/example
kubebuilder init --domain test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当项目创建好之后，会提醒你是否下载依赖，然后你会发现大半个 Kubernetes 的代码已经在你 GOPATH 里了，这是的目录如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── Dockerfile                                  制作镜像的dockerfile
├── Makefile
├── PROJECT
├── bin
│   └── manager
├── config
│   ├── certmanager
│   │   ├── certificate.yaml
│   │   ├── kustomization.yaml
│   │   └── kustomizeconfig.yaml
│   ├── default
│   │   ├── kustomization.yaml
│   │   ├── manager_auth_proxy_patch.yaml
│   │   ├── manager_webhook_patch.yaml
│   │   └── webhookcainjection_patch.yaml
│   ├── manager
│   │   ├── kustomization.yaml
│   │   └── manager.yaml
│   ├── prometheus
│   │   ├── kustomization.yaml
│   │   └── monitor.yaml
│   ├── rbac
│   │   ├── auth_proxy_client_clusterrole.yaml
│   │   ├── auth_proxy_role.yaml
│   │   ├── auth_proxy_role_binding.yaml
│   │   ├── auth_proxy_service.yaml
│   │   ├── kustomization.yaml
│   │   ├── leader_election_role.yaml
│   │   ├── leader_election_role_binding.yaml
│   │   └── role_binding.yaml
│   └── webhook
│       ├── kustomization.yaml
│       ├── kustomizeconfig.yaml
│       └── service.yaml
├── go.mod
├── go.sum
├── hack
│   └── boilerplate.go.txt
└── main.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见是一堆yaml文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;创建API&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;运行下面的命令，创建一个新的 API（组/版本）为 “webapp.test(这个test就是前面init的domain)/v1”，并在上面创建新的 Kind(CRD) “Jcy”。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubebuilder create api --group webapp --version v1 --kind Jcy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看对应的生成的目录结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── Dockerfile
├── Makefile
├── PROJECT
├── api
│   └── v1
│       ├── groupversion_info.go                包含了关于 group-version 的一些元数据
│       ├── jcy_types.go                        数据
│       └── zz_generated.deepcopy.go            包含了前述 runtime.Object 接口的自动实现，这些实现标记了代表 Kinds 的所有根类型。
├── bin
│   └── manager
├── config
│   ├── certmanager
│   │   ├── certificate.yaml
│   │   ├── kustomization.yaml
│   │   └── kustomizeconfig.yaml
│   ├── crd                                     部署crd的yaml
│   │   ├── kustomization.yaml
│   │   ├── kustomizeconfig.yaml
│   │   └── patches
│   │       ├── cainjection_in_jcies.yaml
│   │       └── webhook_in_jcies.yaml
│   ├── default
│   │   ├── kustomization.yaml
│   │   ├── manager_auth_proxy_patch.yaml
│   │   ├── manager_webhook_patch.yaml
│   │   └── webhookcainjection_patch.yaml
│   ├── manager                                 部署controller的yaml，以pod形式启动
│   │   ├── kustomization.yaml
│   │   └── manager.yaml
│   ├── prometheus
│   │   ├── kustomization.yaml
│   │   └── monitor.yaml
│   ├── rbac                                        rbac权限运行的时候也是要用的
│   │   ├── auth_proxy_client_clusterrole.yaml
│   │   ├── auth_proxy_role.yaml
│   │   ├── auth_proxy_role_binding.yaml
│   │   ├── auth_proxy_service.yaml
│   │   ├── jcy_editor_role.yaml
│   │   ├── jcy_viewer_role.yaml
│   │   ├── kustomization.yaml
│   │   ├── leader_election_role.yaml
│   │   ├── leader_election_role_binding.yaml
│   │   └── role_binding.yaml
│   ├── samples                                                 cd的简单实例
│   │   └── webapp_v1_jcy.yaml
│   └── webhook
│       ├── kustomization.yaml
│       ├── kustomizeconfig.yaml
│       └── service.yaml
├── controllers                                     controller的逻辑
│   ├── jcy_controller.go
│   └── suite_test.go
├── go.mod
├── go.sum
├── hack
│   └── boilerplate.go.txt
└── main.go                                         入口函数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 api 你会发现 Kubebuilder 会帮你创建一些目录和源代码文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在 api 里面包含了资源 Jcy  的默认数据结构&lt;/li&gt;
&lt;li&gt;在 controllers 里面 Jcy 的默认 Controller&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;开发结构体&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们来看看这个结构体文件，首先导入meta/v1 API 组，通常本身并不会暴露该组，而是包含所有 Kubernetes 种类共有的元数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    metav1 &amp;quot;k8s.io/apimachinery/pkg/apis/meta/v1&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下Kubebuilder 已经帮你创建和默认的结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Jcy is the Schema for the Jcy API
// +k8s:openapi-gen=true
type Jcy struct {
    metav1.TypeMeta   `json:&amp;quot;,inline&amp;quot;`
    metav1.ObjectMeta `json:&amp;quot;metadata,omitempty&amp;quot;`

    Spec   JcySpec   `json:&amp;quot;spec,omitempty&amp;quot;`                期待的状态
    Status JcyStatus `json:&amp;quot;status,omitempty&amp;quot;`              实际的状态
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们需要做的就是扩展JcySpec和JcyStatus来定义我们需要使用的字段。&lt;/p&gt;

&lt;p&gt;还提供了批量操作LIST&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// JcyList contains a list of Jcy
type JcyList struct {
    metav1.TypeMeta `json:&amp;quot;,inline&amp;quot;`
    metav1.ListMeta `json:&amp;quot;metadata,omitempty&amp;quot;`
    Items           []Jcy `json:&amp;quot;items&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，我们将这个 Go 类型添加到 API 组中。这允许我们将这个 API 组中的类型可以添加到任何Scheme。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    SchemeBuilder.Register(&amp;amp;Jcy{}, &amp;amp;JcyList{})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;开发控制器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在来看看Controller默认 Jcy Controller 名为 ReconcileJcy，其只有一个主要方法就是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *ReconcileJcy) Reconcile(request reconcile.Request) (reconcile.Result, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在使这个 Controller Work 之前，需要 pkg.controller.app.app_controller.go 中的 add 方法中注册要关注的事件，当任何感兴趣的事件发生时，Reconcile 便会被调用，这个函数的职责，就像 Deployment Controller 的 syncHandler 一样，当有事件发生时，去比对当前资源的状态和预期的状态是否一致，如果不一致，就去矫正。&lt;/p&gt;

&lt;p&gt;我们来看看这个控制器文件，首先是import包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package controllers

import (
    &amp;quot;context&amp;quot;

    &amp;quot;github.com/go-logr/logr&amp;quot;
    &amp;quot;k8s.io/apimachinery/pkg/runtime&amp;quot;
    ctrl &amp;quot;sigs.k8s.io/controller-runtime&amp;quot;
    &amp;quot;sigs.k8s.io/controller-runtime/pkg/client&amp;quot;

    batchv1 &amp;quot;tutorial.kubebuilder.io/project/api/v1&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见核心的就是 controller-runtime 运行库，以及 client 包和我们的 API 类型包。&lt;/p&gt;

&lt;p&gt;接下来，kubebuilder 为我们搭建了一个基本的 reconciler 结构。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// JcyReconciler reconciles a Jcy object
type JcyReconciler struct {
    client.Client
    Log    logr.Logger
    Scheme *runtime.Scheme
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;大多数控制器需要一个日志句柄和一个上下文，所以我们在 Reconcile 中将他们初始化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *JcyReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    _ = context.Background()
    _ = r.Log.WithValues(&amp;quot;jcy&amp;quot;, req.NamespacedName)

    // your logic here

    return ctrl.Result{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，我们将 Reconcile 添加到 manager 中，这样当 manager 启动时它就会被启动。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *JcyReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&amp;amp;webappv1.Jcy{}).
        Complete(r)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;测试&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当 CR 的结构已经确定和 Controller 代码完成之后，便可以尝试试运行一下。Kubebuilder 可以通过本地 kubeconfig 配置的集群试运行（对于快速创建一个开发集群推荐 minikube）。&lt;/p&gt;

&lt;p&gt;首先记得在 main.go 中的 init 方法中添加 schema，确定监控的资源，可以根据需要来新增：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    _ = corev1.AddToScheme(scheme)
    _ = appsv1.AddToScheme(scheme)
    _ = extensionsv1beta1.AddToScheme(scheme)
    _ = apis.AddToScheme(scheme)
    // +kubebuilder:scaffold:scheme
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使 Kubebuilder 重新生成代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将 config/crd 下的 CRD yaml 应用到当前集群：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在本地运行 CRD Controller（直接执行 main 函数也可以）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make run
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;控制器开发&#34;&gt;控制器开发&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;main&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们的 main 文件最开始是 import 一些基本库，尤其是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;核心的 控制器运行时controller-runtime 库&lt;/li&gt;
&lt;li&gt;默认的控制器运行时日志库&amp;ndash; Zap&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;代码如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;flag&amp;quot;
    &amp;quot;os&amp;quot;

    &amp;quot;k8s.io/apimachinery/pkg/runtime&amp;quot;
    clientgoscheme &amp;quot;k8s.io/client-go/kubernetes/scheme&amp;quot;
    _ &amp;quot;k8s.io/client-go/plugin/pkg/client/auth/gcp&amp;quot;
    ctrl &amp;quot;sigs.k8s.io/controller-runtime&amp;quot;
    &amp;quot;sigs.k8s.io/controller-runtime/pkg/log/zap&amp;quot;

    webappv1 &amp;quot;example/api/v1&amp;quot;
    &amp;quot;example/controllers&amp;quot;
    // +kubebuilder:scaffold:imports
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每一组控制器都需要一个 Scheme，它提供了 Kinds 和相应的 Go 类型之间的映射。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    scheme   = runtime.NewScheme()
    setupLog = ctrl.Log.WithName(&amp;quot;setup&amp;quot;)
)

func init() {
    _ = clientgoscheme.AddToScheme(scheme)

    _ = webappv1.AddToScheme(scheme)
    // +kubebuilder:scaffold:scheme
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后即使main函数了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var metricsAddr string
    var enableLeaderElection bool
    flag.StringVar(&amp;amp;metricsAddr, &amp;quot;metrics-addr&amp;quot;, &amp;quot;:8080&amp;quot;, &amp;quot;The address the metric endpoint binds to.&amp;quot;)
    flag.BoolVar(&amp;amp;enableLeaderElection, &amp;quot;enable-leader-election&amp;quot;, false,
        &amp;quot;Enable leader election for controller manager. &amp;quot;+
            &amp;quot;Enabling this will ensure there is only one active controller manager.&amp;quot;)
    flag.Parse()

    ctrl.SetLogger(zap.New(zap.UseDevMode(true)))

    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
        Scheme:             scheme,
        MetricsBindAddress: metricsAddr,
        Port:               9443,
        LeaderElection:     enableLeaderElection,
        LeaderElectionID:   &amp;quot;8bf23ea1.test&amp;quot;,
    })
    if err != nil {
        setupLog.Error(err, &amp;quot;unable to start manager&amp;quot;)
        os.Exit(1)
    }

    if err = (&amp;amp;controllers.JcyReconciler{
        Client: mgr.GetClient(),
        Log:    ctrl.Log.WithName(&amp;quot;controllers&amp;quot;).WithName(&amp;quot;Jcy&amp;quot;),
        Scheme: mgr.GetScheme(),
    }).SetupWithManager(mgr); err != nil {
        setupLog.Error(err, &amp;quot;unable to create controller&amp;quot;, &amp;quot;controller&amp;quot;, &amp;quot;Jcy&amp;quot;)
        os.Exit(1)
    }
    // +kubebuilder:scaffold:builder

    setupLog.Info(&amp;quot;starting manager&amp;quot;)
    if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
        setupLog.Error(err, &amp;quot;problem running manager&amp;quot;)
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段代码的核心逻辑比较简单:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;我们通过 flag 库解析入参&lt;/li&gt;
&lt;li&gt;我们实例化了一个manager，它记录着我们所有控制器的运行情况，以及设置共享缓存和API服务器的客户端（注意，我们把我们的 Scheme 的信息告诉了 manager）。&lt;/li&gt;
&lt;li&gt;将 Manager 的 Client 传给 Controller，并且调用 SetupWithManager 方法传入 Manager 进行 Controller 的初始化&lt;/li&gt;
&lt;li&gt;运行 manager，它反过来运行我们所有的控制器和 webhooks。manager 状态被设置为 Running，直到它收到一个优雅停机 (graceful shutdown) 信号。这样一来，当我们在 Kubernetes 上运行时，我们就可以优雅地停止 pod。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;专业名称&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;groups（组）就是相关功能的集合，就是自定义的一个名&lt;/li&gt;
&lt;li&gt;versions（版本）就是版本，一个组可以有多个版本，一般版本都是重v1alpha1，然后升为 v1beta1，最后稳定为 v1 版本。&lt;/li&gt;
&lt;li&gt;Kinds（类型）就是API类型，就是我们定义的类型&lt;/li&gt;
&lt;li&gt;resources（资源）就是 API 中的一个 Kind 的使用方式，就是Kind 的对象标识。通常情况下，Kind 和 resources 之间有一个一对一的映射。 例如，pods 资源对应于 Pod 种类。&lt;/li&gt;
&lt;li&gt;GVK（Group Version Kind）就是当我们在一个特定的群组版本 (Group-Version) 中提到一个 Kind，每个 GVK 对应 Golang 代码中的到对应生成代码中的 Go type。&lt;/li&gt;
&lt;li&gt;GVR（Group Version Resources）就是就是当我们在一个特定的群组版本 (Group-Version) 中提到一个 resources&lt;/li&gt;
&lt;li&gt;Scheme是一种追踪 Go Type 的方法，它对应于给定的 GVK。提供了 Kinds 与对应 Go types 的映射，也就是说给定 Go type 就知道他的 GVK，给定 GVK 就知道他的 Go type。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;设计一个api&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实就是如何设计spec中的数据，这个是有一定的规范的&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;所有序列化的字段必须是 驼峰式&lt;/li&gt;
&lt;li&gt;使用omitempty 标签来标记一个字段在空的时候应该在序列化的时候省略。&lt;/li&gt;
&lt;li&gt;字段可以使用大多数的基本类型。数字是个例外：出于 API 兼容性的目的，我们只允许三种数字类型。对于整数，需要使用 int32 和 int64 类型；对于小数，使用 resource.Quantity 类型。&lt;/li&gt;
&lt;li&gt;还有一个我们使用的特殊类型：metav1.Time。 它有一个稳定的、可移植的序列化格式的功能，其他与 time.Time 相同。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;实现一个控制器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实就是实现我们对这个自定义资源的crud操作，并且保持期待状态和实际状态的统一。主要是在控制器文件中修改&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;main的注册&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在前面讲过，需要将我们关系的资源注册到scheme中去。&lt;/p&gt;

&lt;p&gt;到这里基本上一个crd资源就完成了定义和控制的所有开发，就可以使用了。&lt;/p&gt;

&lt;h2 id=&#34;源码解析&#34;&gt;源码解析&lt;/h2&gt;

&lt;p&gt;刚开始使用 Kubebuilder 的时候，因为封装程度很高，很多事情都是懵逼状态，剖析完之后很多问题就很明白了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;main&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从 main.go 开始。Kubebuilder 创建的 main.go 是整个项目的入口，逻辑十分简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    scheme   = runtime.NewScheme()
    setupLog = ctrl.Log.WithName(&amp;quot;setup&amp;quot;)
)
func init() {
    appsv1alpha1.AddToScheme(scheme)
    // +kubebuilder:scaffold:scheme
}
func main() {
    ...
        // 1、init Manager
    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{Scheme: scheme, MetricsBindAddress: metricsAddr})
    if err != nil {
        setupLog.Error(err, &amp;quot;unable to start manager&amp;quot;)
        os.Exit(1)
    }
        // 2、init Reconciler（Controller）
    err = (&amp;amp;controllers.ApplicationReconciler{
        Client: mgr.GetClient(),
        Log:    ctrl.Log.WithName(&amp;quot;controllers&amp;quot;).WithName(&amp;quot;Application&amp;quot;),
        Scheme: mgr.GetScheme(),
    }).SetupWithManager(mgr)
    if err != nil {
        setupLog.Error(err, &amp;quot;unable to create controller&amp;quot;, &amp;quot;controller&amp;quot;, &amp;quot;EDASApplication&amp;quot;)
        os.Exit(1)
    }
    // +kubebuilder:scaffold:builder
    setupLog.Info(&amp;quot;starting manager&amp;quot;)
        // 3、start Manager
    if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
        setupLog.Error(err, &amp;quot;problem running manager&amp;quot;)
        os.Exit(1)
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到在 init 方法里面我们将 appsv1alpha1 注册到 Scheme 里面去了，这样一来 Cache 就知道 watch 谁了，main 方法里面的逻辑基本都是 Manager 的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初始化了一个 Manager；&lt;/li&gt;
&lt;li&gt;将 Manager 的 Client 传给 Controller，并且调用 SetupWithManager 方法传入 Manager 进行 Controller 的初始化；&lt;/li&gt;
&lt;li&gt;启动 Manager。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们的核心就是看这 3 个流程。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Manager 初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Manager 初始化代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New returns a new Manager for creating Controllers.
func New(config *rest.Config, options Options) (Manager, error) {
    ...
    // Create the cache for the cached read client and registering informers
    cache, err := options.NewCache(config, cache.Options{Scheme: options.Scheme, Mapper: mapper, Resync: options.SyncPeriod, Namespace: options.Namespace})
    if err != nil {
        return nil, err
    }
    apiReader, err := client.New(config, client.Options{Scheme: options.Scheme, Mapper: mapper})
    if err != nil {
        return nil, err
    }
    writeObj, err := options.NewClient(cache, config, client.Options{Scheme: options.Scheme, Mapper: mapper})
    if err != nil {
        return nil, err
    }
    ...
    return &amp;amp;controllerManager{
        config:           config,
        scheme:           options.Scheme,
        errChan:          make(chan error),
        cache:            cache,
        fieldIndexes:     cache,
        client:           writeObj,
        apiReader:        apiReader,
        recorderProvider: recorderProvider,
        resourceLock:     resourceLock,
        mapper:           mapper,
        metricsListener:  metricsListener,
        internalStop:     stop,
        internalStopper:  stop,
        port:             options.Port,
        host:             options.Host,
        leaseDuration:    *options.LeaseDuration,
        renewDeadline:    *options.RenewDeadline,
        retryPeriod:      *options.RetryPeriod,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到主要是创建 Cache 与 Clients：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;创建 Cache&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Cache 初始化代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New initializes and returns a new Cache.
func New(config *rest.Config, opts Options) (Cache, error) {
    opts, err := defaultOpts(config, opts)
    if err != nil {
        return nil, err
    }
    im := internal.NewInformersMap(config, opts.Scheme, opts.Mapper, *opts.Resync, opts.Namespace)
    return &amp;amp;informerCache{InformersMap: im}, nil
}
// newSpecificInformersMap returns a new specificInformersMap (like
// the generical InformersMap, except that it doesn&#39;t implement WaitForCacheSync).
func newSpecificInformersMap(...) *specificInformersMap {
    ip := &amp;amp;specificInformersMap{
        Scheme:            scheme,
        mapper:            mapper,
        informersByGVK:    make(map[schema.GroupVersionKind]*MapEntry),
        codecs:            serializer.NewCodecFactory(scheme),
        resync:            resync,
        createListWatcher: createListWatcher,
        namespace:         namespace,
    }
    return ip
}
// MapEntry contains the cached data for an Informer
type MapEntry struct {
    // Informer is the cached informer
    Informer cache.SharedIndexInformer
    // CacheReader wraps Informer and implements the CacheReader interface for a single type
    Reader CacheReader
}
func createUnstructuredListWatch(gvk schema.GroupVersionKind, ip *specificInformersMap) (*cache.ListWatch, error) {
        ...
    // Create a new ListWatch for the obj
    return &amp;amp;cache.ListWatch{
        ListFunc: func(opts metav1.ListOptions) (runtime.Object, error) {
            if ip.namespace != &amp;quot;&amp;quot; &amp;amp;&amp;amp; mapping.Scope.Name() != meta.RESTScopeNameRoot {
                return dynamicClient.Resource(mapping.Resource).Namespace(ip.namespace).List(opts)
            }
            return dynamicClient.Resource(mapping.Resource).List(opts)
        },
        // Setup the watch function
        WatchFunc: func(opts metav1.ListOptions) (watch.Interface, error) {
            // Watch needs to be set to true separately
            opts.Watch = true
            if ip.namespace != &amp;quot;&amp;quot; &amp;amp;&amp;amp; mapping.Scope.Name() != meta.RESTScopeNameRoot {
                return dynamicClient.Resource(mapping.Resource).Namespace(ip.namespace).Watch(opts)
            }
            return dynamicClient.Resource(mapping.Resource).Watch(opts)
        },
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 Cache 主要就是创建了 InformersMap，Scheme 里面的每个 GVK 都创建了对应的 Informer，通过 informersByGVK 这个 map 做 GVK 到 Informer 的映射，每个 Informer 会根据 ListWatch 函数对对应的 GVK 进行 List 和 Watch。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;创建 Clients&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;创建 Clients 很简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// defaultNewClient creates the default caching client
func defaultNewClient(cache cache.Cache, config *rest.Config, options client.Options) (client.Client, error) {
    // Create the Client for Write operations.
    c, err := client.New(config, options)
    if err != nil {
        return nil, err
    }
    return &amp;amp;client.DelegatingClient{
        Reader: &amp;amp;client.DelegatingReader{
            CacheReader:  cache,
            ClientReader: c,
        },
        Writer:       c,
        StatusClient: c,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读操作使用上面创建的 Cache，写操作使用 K8s go-client 直连。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Controller 初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面看看 Controller 的启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *EDASApplicationReconciler) SetupWithManager(mgr ctrl.Manager) error {
    err := ctrl.NewControllerManagedBy(mgr).
        For(&amp;amp;appsv1alpha1.EDASApplication{}).
        Complete(r)
return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用的是 Builder 模式，NewControllerManagerBy 和 For 方法都是给 Builder 传参，最重要的是最后一个方法 Complete，其逻辑是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (blder *Builder) Build(r reconcile.Reconciler) (manager.Manager, error) {
...
    // Set the Manager
    if err := blder.doManager(); err != nil {
        return nil, err
    }
    // Set the ControllerManagedBy
    if err := blder.doController(r); err != nil {
        return nil, err
    }
    // Set the Watch
    if err := blder.doWatch(); err != nil {
        return nil, err
    }
...
    return blder.mgr, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要是看看 doController 和 doWatch 方法：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;doController 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func New(name string, mgr manager.Manager, options Options) (Controller, error) {
    if options.Reconciler == nil {
        return nil, fmt.Errorf(&amp;quot;must specify Reconciler&amp;quot;)
    }
    if len(name) == 0 {
        return nil, fmt.Errorf(&amp;quot;must specify Name for Controller&amp;quot;)
    }
    if options.MaxConcurrentReconciles &amp;lt;= 0 {
        options.MaxConcurrentReconciles = 1
    }
    // Inject dependencies into Reconciler
    if err := mgr.SetFields(options.Reconciler); err != nil {
        return nil, err
    }
    // Create controller with dependencies set
    c := &amp;amp;controller.Controller{
        Do:                      options.Reconciler,
        Cache:                   mgr.GetCache(),
        Config:                  mgr.GetConfig(),
        Scheme:                  mgr.GetScheme(),
        Client:                  mgr.GetClient(),
        Recorder:                mgr.GetEventRecorderFor(name),
        Queue:                   workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), name),
        MaxConcurrentReconciles: options.MaxConcurrentReconciles,
        Name:                    name,
    }
    // Add the controller as a Manager components
    return c, mgr.Add(c)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方法初始化了一个 Controller，传入了一些很重要的参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Do：Reconcile 逻辑；&lt;/li&gt;
&lt;li&gt;Cache：找 Informer 注册 Watch；&lt;/li&gt;
&lt;li&gt;Client：对 K8s 资源进行 CRUD；&lt;/li&gt;
&lt;li&gt;Queue：Watch 资源的 CUD 事件缓存；&lt;/li&gt;
&lt;li&gt;Recorder：事件收集。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;doWatch 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (blder *Builder) doWatch() error {
    // Reconcile type
    src := &amp;amp;source.Kind{Type: blder.apiType}
    hdler := &amp;amp;handler.EnqueueRequestForObject{}
    err := blder.ctrl.Watch(src, hdler, blder.predicates...)
    if err != nil {
        return err
    }
    // Watches the managed types
    for _, obj := range blder.managedObjects {
        src := &amp;amp;source.Kind{Type: obj}
        hdler := &amp;amp;handler.EnqueueRequestForOwner{
            OwnerType:    blder.apiType,
            IsController: true,
        }
        if err := blder.ctrl.Watch(src, hdler, blder.predicates...); err != nil {
            return err
        }
    }
    // Do the watch requests
    for _, w := range blder.watchRequest {
        if err := blder.ctrl.Watch(w.src, w.eventhandler, blder.predicates...); err != nil {
            return err
        }
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到该方法对本 Controller 负责的 CRD 进行了 watch，同时底下还会 watch 本 CRD 管理的其他资源，这个 managedObjects 可以通过 Controller 初始化 Buidler 的 Owns 方法传入，说到 Watch 我们关心两个逻辑：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;注册的 handler&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type EnqueueRequestForObject struct{}
// Create implements EventHandler
func (e *EnqueueRequestForObject) Create(evt event.CreateEvent, q workqueue.RateLimitingInterface) {
        ...
    q.Add(reconcile.Request{NamespacedName: types.NamespacedName{
        Name:      evt.Meta.GetName(),
        Namespace: evt.Meta.GetNamespace(),
    }})
}
// Update implements EventHandler
func (e *EnqueueRequestForObject) Update(evt event.UpdateEvent, q workqueue.RateLimitingInterface) {
    if evt.MetaOld != nil {
        q.Add(reconcile.Request{NamespacedName: types.NamespacedName{
            Name:      evt.MetaOld.GetName(),
            Namespace: evt.MetaOld.GetNamespace(),
        }})
    } else {
        enqueueLog.Error(nil, &amp;quot;UpdateEvent received with no old metadata&amp;quot;, &amp;quot;event&amp;quot;, evt)
    }
    if evt.MetaNew != nil {
        q.Add(reconcile.Request{NamespacedName: types.NamespacedName{
            Name:      evt.MetaNew.GetName(),
            Namespace: evt.MetaNew.GetNamespace(),
        }})
    } else {
        enqueueLog.Error(nil, &amp;quot;UpdateEvent received with no new metadata&amp;quot;, &amp;quot;event&amp;quot;, evt)
    }
}
// Delete implements EventHandler
func (e *EnqueueRequestForObject) Delete(evt event.DeleteEvent, q workqueue.RateLimitingInterface) {
        ...
    q.Add(reconcile.Request{NamespacedName: types.NamespacedName{
        Name:      evt.Meta.GetName(),
        Namespace: evt.Meta.GetNamespace(),
    }})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 Kubebuidler 为我们注册的 Handler 就是将发生变更的对象的 NamespacedName 入队列，如果在 Reconcile 逻辑中需要判断创建/更新/删除，需要有自己的判断逻辑。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;注册的流程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;// Watch implements controller.Controller
func (c *Controller) Watch(src source.Source, evthdler handler.EventHandler, prct ...predicate.Predicate) error {
    ...
    log.Info(&amp;quot;Starting EventSource&amp;quot;, &amp;quot;controller&amp;quot;, c.Name, &amp;quot;source&amp;quot;, src)
    return src.Start(evthdler, c.Queue, prct...)
}
// Start is internal and should be called only by the Controller to register an EventHandler with the Informer
// to enqueue reconcile.Requests.
func (is *Informer) Start(handler handler.EventHandler, queue workqueue.RateLimitingInterface,
    ...
    is.Informer.AddEventHandler(internal.EventHandler{Queue: queue, EventHandler: handler, Predicates: prct})
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们的 Handler 实际注册到 Informer 上面，这样整个逻辑就串起来了，通过 Cache 我们创建了所有 Scheme 里面 GVKs 的 Informers，然后对应 GVK 的 Controller 注册了 Watch Handler 到对应的 Informer，这样一来对应的 GVK 里面的资源有变更都会触发 Handler，将变更事件写到 Controller 的事件队列中，之后触发我们的 Reconcile 方法。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Manager 启动&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (cm *controllerManager) Start(stop &amp;lt;-chan struct{}) error {
    ...
    go cm.startNonLeaderElectionRunnables()
    ...
}
func (cm *controllerManager) startNonLeaderElectionRunnables() {
    ...
    // Start the Cache. Allow the function to start the cache to be mocked out for testing
    if cm.startCache == nil {
        cm.startCache = cm.cache.Start
    }
    go func() {
        if err := cm.startCache(cm.internalStop); err != nil {
            cm.errChan &amp;lt;- err
        }
    }()
        ...
        // Start Controllers
    for _, c := range cm.nonLeaderElectionRunnables {
        ctrl := c
        go func() {
            cm.errChan &amp;lt;- ctrl.Start(cm.internalStop)
        }()
    }
    cm.started = true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要就是启动 Cache，Controller，将整个事件流运转起来，我们下面来看看启动逻辑。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Cache 启动&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (ip *specificInformersMap) Start(stop &amp;lt;-chan struct{}) {
    func() {
        ...
        // Start each informer
        for _, informer := range ip.informersByGVK {
            go informer.Informer.Run(stop)
        }
    }()
}
func (s *sharedIndexInformer) Run(stopCh &amp;lt;-chan struct{}) {
        ...
        // informer push resource obj CUD delta to this fifo queue
    fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, s.indexer)
    cfg := &amp;amp;Config{
        Queue:            fifo,
        ListerWatcher:    s.listerWatcher,
        ObjectType:       s.objectType,
        FullResyncPeriod: s.resyncCheckPeriod,
        RetryOnError:     false,
        ShouldResync:     s.processor.shouldResync,
                // handler to process delta
        Process: s.HandleDeltas,
    }
    func() {
        s.startedLock.Lock()
        defer s.startedLock.Unlock()
                // this is internal controller process delta generate by reflector
        s.controller = New(cfg)
        s.controller.(*controller).clock = s.clock
        s.started = true
    }()
        ...
    wg.StartWithChannel(processorStopCh, s.processor.run)
    s.controller.Run(stopCh)
}
func (c *controller) Run(stopCh &amp;lt;-chan struct{}) {
    ...
    r := NewReflector(
        c.config.ListerWatcher,
        c.config.ObjectType,
        c.config.Queue,
        c.config.FullResyncPeriod,
    )
    ...
        // reflector is delta producer
    wg.StartWithChannel(stopCh, r.Run)
        // internal controller&#39;s processLoop is comsume logic
    wait.Until(c.processLoop, time.Second, stopCh)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cache 的初始化核心是初始化所有的 Informer，Informer 的初始化核心是创建了 reflector 和内部 controller，reflector 负责监听 Api Server 上指定的 GVK，将变更写入 delta 队列中，可以理解为变更事件的生产者，内部 controller 是变更事件的消费者，他会负责更新本地 indexer，以及计算出 CUD 事件推给我们之前注册的 Watch Handler。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Controller 启动&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;// Start implements controller.Controller
func (c *Controller) Start(stop &amp;lt;-chan struct{}) error {
    ...
    for i := 0; i &amp;lt; c.MaxConcurrentReconciles; i++ {
        // Process work items
        go wait.Until(func() {
            for c.processNextWorkItem() {
            }
        }, c.JitterPeriod, stop)
    }
    ...
}
func (c *Controller) processNextWorkItem() bool {
    ...
    obj, shutdown := c.Queue.Get()
    ...
    var req reconcile.Request
    var ok bool
    if req, ok = obj.(reconcile.Request);
        ...
    // RunInformersAndControllers the syncHandler, passing it the namespace/Name string of the
    // resource to be synced.
    if result, err := c.Do.Reconcile(req); err != nil {
        c.Queue.AddRateLimited(req)
        ...
    }
        ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Controller 的初始化是启动 goroutine 不断地查询队列，如果有变更消息则触发到我们自定义的 Reconcile 逻辑。&lt;/p&gt;

&lt;p&gt;Kubebuilder 作为工具已经为我们做了很多，到最后我们只需要实现 Reconcile 方法即可，相比一开始更加的自动化，其实&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-operator/&#34;&gt;operator&lt;/a&gt;也是一种特殊模式的控制器，也可以使用kubebuilder来开发。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 并发</title>
          <link>https://kingjcy.github.io/post/architecture/concurrence/</link>
          <pubDate>Mon, 09 Nov 2020 19:25:10 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/concurrence/</guid>
          <description>&lt;p&gt;高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;首先并发中的一些重要概念&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;并发度：指在同一个时间点发起的请求数量，比如 12306 统一在下午两点钟放票，100 个人在下午两点钟同时向 12306 发起请求，此时可以认为并发度为 100。&lt;/li&gt;
&lt;li&gt;响应时间：系统对请求做出响应的时间。例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。&lt;/li&gt;
&lt;li&gt;吞吐量：单位时间内处理的请求数量。&lt;/li&gt;
&lt;li&gt;TPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。TPS包括一条消息入和一条消息出，加上一次用户数据库访问。（业务TPS = CAPS × 每个呼叫平均TPS）TPS是软件测试结果的测量单位。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。一般的，评价系统性能均以每秒钟完成的技术交易的数量来衡量。系统整体处理能力取决于处理能力最低模块的TPS值。&lt;/li&gt;
&lt;li&gt;QPS：每秒查询率QPS是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。对应fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;高并发&#34;&gt;高并发&lt;/h2&gt;

&lt;p&gt;那么什么才叫高并发呢？&lt;/p&gt;

&lt;p&gt;高并发要根据场景来定义，多高的并发算高并发？对于阿里来说可能几千上万才算高并发，对于普通小公司来说，可能几十上百就算高并发了。&lt;/p&gt;

&lt;p&gt;高并发其实是和响应时间有很大关系的，你能把接口的响应时间做到1ms，你一个线程就能有1000QPS！一个服务怎么也能跑200个线程吧，QPS轻松地达到了20万呐！你看看有几个整天把高并发挂嘴上的能做到20万的QPS？可见响应时间是不那么容易快速响应的。&lt;/p&gt;

&lt;p&gt;其实高并发的场景多用于web端的请求处理。我们就web常见架构，对于百万级的的QPS怎么进行处理，常见web架构如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/concurrence/cc.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Qps如果不是太高，只要简单的使用上面的进行交互就能满足基本需要。但是如果在QPS达到百万级甚至千万级别的，就会在各种交互组件上出现瓶颈。这个时候就是经验的使用和积累，来使用不同的架构来完成这种需求。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;水平扩展&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、反向代理层的水平扩展&lt;/p&gt;

&lt;p&gt;反向代理层的水平扩展，是通过“DNS轮询”实现的：dns-server对于一个域名配置了多个解析ip，每次DNS解析请求来访问dns-server，会轮询返回这些ip。&lt;/p&gt;

&lt;p&gt;当nginx成为瓶颈的时候，只要增加服务器数量，新增nginx服务的部署，增加一个外网ip，就能扩展反向代理层的性能，做到理论上的无限高并发。&lt;/p&gt;

&lt;p&gt;2、站点层的水平扩展&lt;/p&gt;

&lt;p&gt;站点层的水平扩展，是通过“nginx”实现的。通过修改nginx.conf，可以设置多个web后端。&lt;/p&gt;

&lt;p&gt;当web后端成为瓶颈的时候，只要增加服务器数量，新增web服务的部署，在nginx配置中配置上新的web后端，就能扩展站点层的性能，做到理论上的无限高并发。&lt;/p&gt;

&lt;p&gt;3、服务层的水平扩展&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/concurrence/cc1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;服务层的水平扩展，是通过“服务连接池”实现的。&lt;/p&gt;

&lt;p&gt;站点层通过RPC-client调用下游的服务层RPC-server时，RPC-client中的连接池会建立与下游服务多个连接，当服务成为瓶颈的时候，只要增加服务器数量，新增服务部署，在RPC-client处建立新的下游服务连接，就能扩展服务层性能，做到理论上的无限高并发。如果需要优雅的进行服务层自动扩容，这里可能需要配置中心里服务自动发现功能的支持。&lt;/p&gt;

&lt;p&gt;4、数据层的水平扩展&lt;/p&gt;

&lt;p&gt;在数据量很大的情况下，数据层（缓存，数据库）涉及数据的水平扩展，将原本存储在一台服务器上的数据（缓存，数据库）水平拆分到不同服务器上去，以达到扩充系统性能的目的。&lt;/p&gt;

&lt;p&gt;互联网数据层常见的水平拆分方式有这么几种，以数据库为例：按照范围水平拆分，按照哈希水平拆分。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;垂直扩展&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、提供每台机器的配置&lt;/p&gt;

&lt;p&gt;2、使用多线程thread（正常是线程池）并发，也有使用fork出来多进行的进行处理，这边可以在逻辑里进行业务划分。&lt;/p&gt;

&lt;p&gt;3、优化业务，解决哪些很耗时间的操作，for循环什么的，数据库连接次数，&lt;/p&gt;

&lt;p&gt;4、数据库的优化，使用缓存数据库，优化数据库正常先对我们写的sql进行优化，可以看执行计划，然后数据库索引进行优化，然后就是分区，分表，分库的各种切分。&lt;/p&gt;

&lt;p&gt;其实数据库是很多需求的响应的瓶颈所在，在数据库上花功夫才是重点，一般数据库正常使用情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql 的合理上限不应该超过500万
oracle。20亿数据。  清单
HBase在50000条数据批量写的性能大概是在2s左右，单个查，5-10ms左右
redis qps 500-2000。     几百G
prometheus。  100W。30s 60。200G
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、完善架构，多使用缓存，减少io的交互等。&lt;/p&gt;

&lt;p&gt;在互联网业务发展非常迅猛的早期，如果预算不是问题，强烈建议使用“增强单机硬件性能”的方式提升系统并发能力，因为这个阶段，公司的战略往往是发展业务抢时间，而“增强单机硬件性能”往往是最快的方法。京东的架构说了句大并发，大数据下的业务其实还是靠堆机器保证的，我们现在研究的是如何在堆机器的情况下保证业务的连贯性，容错性，可用性。&lt;/p&gt;

&lt;h1 id=&#34;实践&#34;&gt;实践&lt;/h1&gt;

&lt;h2 id=&#34;java&#34;&gt;java&lt;/h2&gt;

&lt;p&gt;java在web并发中使用很多&lt;/p&gt;

&lt;p&gt;JVMJEE容器中运行的JVM参数配置参数的正确使用直接关系到整个系统的性能和处理能力，JVM的调优主要是对内存管理方面的调优，优化的方向分为以下4点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.HeapSize             堆的大小，也可以说Java虚拟机使用内存的策略，这点是非常关键的。
2.GarbageCollector  通过配置相关的参数进行Java中的垃圾收集器的4个算法(策略)进行使用。
3.StackSize             栈是JVM的内存指令区,每个线程都有他自己的Stack，Stack的大小限制着线程的数量。
4.DeBug/Log           在JVM中还可以设置对JVM运行时的日志和JVM挂掉后的日志输出，这点非常的关键，根据各类JVM的日志输出才能配置合适的参数。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;JDBC针对MySQL的JDBC的参数在之前的文章中也有介绍过，在单台机器或者集群的环境下合理的使用JDBC中的配置参数对操作数据库也有很大的影响。一些所谓高性能的 Java ORM开源框架也就是打开了很多JDBC中的默认参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.例如：autoReconnect、prepStmtCacheSize、cachePrepStmts、useNewIO、blobSendChunkSize 等，&lt;/li&gt;
&lt;li&gt;2.例如集群环境下：roundRobinLoadBalance、failOverReadOnly、autoReconnectForPools、secondsBeforeRetryMaster。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据库连接池(DataSource)应用程序与数据库连接频繁的交互会给系统带来瓶颈和大量的开销会影响到系统的性能，JDBC连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而再不是重新建立一个连接，因此应用程序不需要频繁的与数据库开关连接，并且可以释放空闲时间超过最大空闲时间的数据库连接来避免因为没有释放数据库连接而引起的数据库连接遗漏。这项技术能明显提高对数据库操作的性能。&lt;/p&gt;

&lt;p&gt;数据存取数据库服务器的优化和数据的存取，什么类型的数据放在什么地方更好是值得去思考的问题，将来的存储很可能是混用的，Cache，NOSQL，DFS，DataBase 在一个系统中都会有。&lt;/p&gt;

&lt;p&gt;缓存在宏观上看缓存一般分为2种：本地缓存和分布式缓存&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.本地缓存，对于Java的本地缓存而言就是讲数据放入静态(static)的数据结合中，然后需要用的时候就从静态数据结合中拿出来,对于高并发的环境建议使用 ConcurrentHashMap或者CopyOnWriteArrayList作为本地缓存。缓存的使用更具体点说就是对系统内存的使用，使用多少内存的资源需要有一个适当比例，如果超过适当的使用存储访问，将会适得其反，导致整个系统的运行效率低下。&lt;/li&gt;
&lt;li&gt;2.分布式缓存，一般用于分布式的环境，将每台机器上的缓存进行集中化的存储，并且不仅仅用于缓存的使用范畴，还可以作为分布式系统数据同步/传输的一种手段，一般被使用最多的就是Memcached和Redis。数据存储在不同的介质上读/写得到的效率是不同的，在系统中如何善用缓存，让你的数据更靠近cpu，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;并发/多线程在高并发环境下建议开发者使用JDK中自带的并发包(java.util.concurrent)，在JDK1.5以后使用java.util.concurrent下的工具类可以简化多线程开发，在java.util.concurrent的工具中主要分为以下几个主要部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.线程池，线程池的接口(Executor、ExecutorService)与实现类(ThreadPoolExecutor、 ScheduledThreadPoolExecutor），利用jdk自带的线程池框架可以管理任务的排队和安排，并允许受控制的关闭。因为运行一个线程需要消耗系统CPU资源，而创建、结束一个线程也对系统CPU资源有开销，使用线程池不仅仅可以有效的管理多线程的使用，还是可以提高线程的运行效率。&lt;/li&gt;
&lt;li&gt;2.本地队列，提供了高效的、可伸缩的、线程安全的非阻塞 FIFO 队列。java.util.concurrent 中的五个实现都支持扩展的 BlockingQueue 接口，该接口定义了 put 和 take 的阻塞版本：LinkedBlockingQueue、ArrayBlockingQueue、SynchronousQueue、PriorityBlockingQueue 和 DelayQueue。这些不同的类覆盖了生产者-使用者、消息传递、并行任务执行和相关并发设计的大多数常见使用的上下文。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;拒绝策略介绍&lt;/p&gt;

&lt;p&gt;线程池的拒绝策略，是指当任务添加到线程池中被拒绝，而采取的处理措施。
当任务添加到线程池中之所以被拒绝，可能是由于：第一，线程池异常关闭。第二，任务数量超过线程池的最大限制。&lt;/p&gt;

&lt;p&gt;线程池共包括4种拒绝策略，它们分别是：AbortPolicy, CallerRunsPolicy, DiscardOldestPolicy和DiscardPolicy。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AbortPolicy         -- 当任务添加到线程池中被拒绝时，它将抛出 RejectedExecutionException 异常。
CallerRunsPolicy    -- 当任务添加到线程池中被拒绝时，会在线程池当前正在运行的Thread线程池中处理被拒绝的任务。
DiscardOldestPolicy -- 当任务添加到线程池中被拒绝时，线程池会放弃等待队列中最旧的未处理任务，然后将被拒绝的任务添加到等待队列中。
DiscardPolicy       -- 当任务添加到线程池中被拒绝时，线程池将丢弃被拒绝的任务。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;线程池默认的处理策略是AbortPolicy！&lt;/p&gt;

&lt;h2 id=&#34;go&#34;&gt;go&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-concurrence/&#34;&gt;go并发&lt;/a&gt;是golang语言的核心能力。&lt;/p&gt;

&lt;h2 id=&#34;并发安全&#34;&gt;并发安全&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;是并发中核心需要解决的问题。&lt;/p&gt;

&lt;h2 id=&#34;高并发需要注意的事情&#34;&gt;高并发需要注意的事情&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;高并发下一定要减少锁的使用，这边也是channel在go中的重要作用之一。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;好的习惯是，稍大的类型存到map都存储指针而不是值。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;定义数据结构的时候，减少后面使用的转换&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;defer是性能杀手，我的原则是能不用尽量避开。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;能不在循环内部做的，就不要在循环内存处理&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;减少内存的频繁分配，减少使用全局锁的可能&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;设置并发数&#34;&gt;设置并发数&lt;/h2&gt;

&lt;p&gt;常规的并发模型就是我们使用的工作池模型，我们需要了解具体的工作模式，可以量化的分析并发，比如下图是一个典型的工作线程的处理过程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/concurrence/concurrence.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从开始处理start到结束处理end，该任务的处理共有7个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）从工作队列里拿出任务，进行一些本地初始化计算，例如http协议分析、参数解析、参数校验等&lt;/li&gt;
&lt;li&gt;2）访问cache拿一些数据&lt;/li&gt;
&lt;li&gt;3）拿到cache里的数据后，再进行一些本地计算，这些计算和业务逻辑相关&lt;/li&gt;
&lt;li&gt;4）通过RPC调用下游service再拿一些数据，或者让下游service去处理一些相关的任务&lt;/li&gt;
&lt;li&gt;5）RPC调用结束后，再进行一些本地计算，怎么计算和业务逻辑相关&lt;/li&gt;
&lt;li&gt;6）访问DB进行一些数据操作&lt;/li&gt;
&lt;li&gt;7）操作完数据库之后做一些收尾工作，同样这些收尾工作也是本地计算，和业务逻辑相关&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;分析整个处理的时间轴，会发现：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）其中1，3，5，7步骤中【上图中粉色时间轴】，线程进行本地业务逻辑计算时需要占用CPU&lt;/li&gt;
&lt;li&gt;2）而2，4，6步骤中【上图中橙色时间轴】，访问cache、service、DB过程中线程处于一个等待结果的状态，不需要占用CPU，进一步的分解，这个“等待结果”的时间共分为三部分：

&lt;ul&gt;
&lt;li&gt;请求在网络上传输到下游的cache、service、DB&lt;/li&gt;
&lt;li&gt;下游cache、service、DB进行任务处理&lt;/li&gt;
&lt;li&gt;cache、service、DB将报文在网络上传回工作线程&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上面的分析，Worker线程在执行的过程中，有一部计算时间需要占用CPU，另一部分等待时间不需要占用CPU，通过量化分析，例如打日志进行统计，可以统计出整个Worker线程执行过程中这两部分时间的比例，例如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）时间轴1，3，5，7【上图中粉色时间轴】的计算执行时间是100ms&lt;/li&gt;
&lt;li&gt;2）时间轴2，4，6【上图中橙色时间轴】的等待时间也是100ms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;得到的结果是，这个线程计算和等待的时间是1：1，即有50%的时间在计算（占用CPU），50%的时间在等待（不占用CPU）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）假设此时是单核，则设置为2个工作线程就可以把CPU充分利用起来，让CPU跑到100%&lt;/li&gt;
&lt;li&gt;2）假设此时是N核，则设置为2N个工作现场就可以把CPU充分利用起来，让CPU跑到N*100%&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;N核服务器，通过执行业务的单线程分析出本地计算时间为x，等待时间为y，则工作线程数（线程池线程数）设置为 N*(x+y)/x，能让CPU的利用率最大化。&lt;/p&gt;

&lt;p&gt;一般来说，非CPU密集型的业务（加解密、压缩解压缩、搜索排序等业务是CPU密集型的业务），瓶颈都在后端数据库，本地CPU计算的时间很少，所以设置几十或者几百个工作线程也都是可能的。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- Architecture 总结</title>
          <link>https://kingjcy.github.io/post/architecture/architecture/</link>
          <pubDate>Thu, 05 Nov 2020 10:04:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/architecture/</guid>
          <description>&lt;p&gt;架构就是解决问题给出的整体技术方案，既要掌握整体，也要知道局部瓶颈能够解决具体业务的方案。&lt;/p&gt;

&lt;p&gt;架构师，是一个既需要掌控整体又需要洞悉局部瓶颈并依据具体的业务场景给出解决方案的团队领导型人物。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;在软件工程和企业信息系统领域，又有很多细分，如所谓的系统架构师（System Architect）、应用架构师（Application Architect）、企业架构师（Enterprise Architect）以及基础设施架构师（Infrastructure Architect）等等。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;应用架构师负责构建一个以解决特定问题为目标的软件应用的内部结合结构，一般以满足各种功能性需求以及维护性需求为设计考虑目标；&lt;/li&gt;
&lt;li&gt;系统架构师则提供运营支撑软件应用的信息系统的结构设计，一般以满足各种非功能性需求或运营性需求为设计目标（如安全性、可伸缩性、可互操作性等等）；&lt;/li&gt;
&lt;li&gt;企业架构师，就不光只顾IT系统的架构了，他应以企业的持续经营目标为考虑要素来构建企业所需要的内在结构设计；&lt;/li&gt;
&lt;li&gt;基础设施架构师主要是负责基础平台的设计。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;架构设计&#34;&gt;架构设计&lt;/h1&gt;

&lt;p&gt;其实就是设计模式中的七大原则&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.开闭原则
2.依赖倒置原则
3.单一职责原则
4.接口隔离原则
5.迪米特法则（最小知道原则）
6.里氏替换原则
7.合成/聚合复用原则
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对我实践过程中总结来说：&lt;/p&gt;

&lt;p&gt;1、抽象，封装，可扩展，使用接口实现&lt;/p&gt;

&lt;p&gt;定义对外调用的接口，实现这些方法的结构体就是实现了接口，不需要关心具体的实现，只需要调用就好，其实也就是设计模式中的开闭原则和依赖倒置原则，接口隔离的原则，迪米特法则，实现依赖于接口，接口尽量细化，不互相依赖。&lt;/p&gt;

&lt;p&gt;2、尽量设计原子功能&lt;/p&gt;

&lt;p&gt;每个服务职责单一，可以服务化，不相互依赖，解耦，同时组合功能就是尽量模块清晰，只实现原子的功能，想实现大的功能就是将原子的功能组合起来，也就是合成复用原则&lt;/p&gt;

&lt;p&gt;3、关注性能、可用性、伸缩性、扩展性、安全性这5个架构要素的实现&lt;/p&gt;

&lt;h2 id=&#34;架构要素&#34;&gt;架构要素&lt;/h2&gt;

&lt;p&gt;在架构设计中一些常规的思想是我们设计的核心要素，我们必须拥有解决的能力，掌握其常规方案，结合实际场景有具体设计的能力。&lt;/p&gt;

&lt;h3 id=&#34;公共服务服务化&#34;&gt;公共服务服务化&lt;/h3&gt;

&lt;p&gt;其实就是一种共享服务的&lt;strong&gt;抽象&lt;/strong&gt;，可以实现架构和业务上的解耦，解决各种依赖问题，可以提高共同部分的优化效率和管控等等好处，没有具体的实现方案，视具体情况进行抽象设计。&lt;a href=&#34;https://kingjcy.github.io/post/architecture/coupling/&#34;&gt;架构解耦还有很多方案&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;高可用-容错机制&#34;&gt;高可用，容错机制&lt;/h3&gt;

&lt;p&gt;高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。&lt;/p&gt;

&lt;p&gt;高可用保证是通过&lt;strong&gt;冗余+自动故障转移&lt;/strong&gt;来保证系统的高可用特性。&lt;/p&gt;

&lt;p&gt;我们先来看一下正常的服务架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/webserver&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;常见互联网分布式架构如上，分为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）客户端层：典型调用方是浏览器browser或者手机应用APP&lt;/li&gt;
&lt;li&gt;（2）反向代理层：系统入口，反向代理&lt;/li&gt;
&lt;li&gt;（3）站点应用层：实现核心应用逻辑，返回html或者json&lt;/li&gt;
&lt;li&gt;（4）服务层：如果实现了服务化，就有这一层&lt;/li&gt;
&lt;li&gt;（5）数据-缓存层：缓存加速访问存储&lt;/li&gt;
&lt;li&gt;（6）数据-数据库层：数据库固化数据存储&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整个系统的高可用，又是通过每一层的冗余+自动故障转移来综合实现的。&lt;/p&gt;

&lt;p&gt;1、【客户端层-&amp;gt;反向代理层】的高可用&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【客户端层】到【反向代理层】的高可用，是通过反向&lt;strong&gt;代理层的冗余&lt;/strong&gt;来实现的。以nginx为例：有两台nginx，一台对线上提供服务，另一台冗余以保证高可用，常见的实践是keepalived存活探测，相同virtual IP提供服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当nginx挂了的时候，keepalived能够探测到，会自动的进行故障转移，将流量自动迁移到shadow-nginx，由于使用的是相同的virtual IP，这个切换过程对调用方是透明的。&lt;/p&gt;

&lt;p&gt;2、【反向代理层-&amp;gt;站点层】的高可用&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【反向代理层】到【站点层】的高可用，是通过&lt;strong&gt;站点层的冗余&lt;/strong&gt;来实现的。假设反向代理层是nginx，nginx.conf里能够配置多个web后端，并且nginx能够探测到多个后端的存活性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当web-server挂了的时候，nginx能够探测到，会自动的进行故障转移，将流量自动迁移到其他的web-server，整个过程由nginx自动完成，对调用方是透明的。&lt;/p&gt;

&lt;p&gt;3、【站点层-&amp;gt;服务层】的高可用&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【站点层】到【服务层】的高可用，是通过&lt;strong&gt;服务层的冗余&lt;/strong&gt;来实现的。“服务连接池”会建立与下游服务多个连接，每次请求会“随机”选取连接来访问下游服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当service挂了的时候，service-connection-pool能够探测到，会自动的进行故障转移，将流量自动迁移到其他的service，整个过程由连接池自动完成，对调用方是透明的（所以说RPC-client中的服务连接池是很重要的基础组件）。&lt;/p&gt;

&lt;p&gt;4、【服务层&amp;gt;缓存层】的高可用&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【服务层】到【缓存层】的高可用，是通过&lt;strong&gt;缓存数据的冗余&lt;/strong&gt;来实现的。&lt;/p&gt;

&lt;p&gt;缓存层的数据冗余又有几种方式：第一种是利用客户端的封装，service对cache进行双读或者双写。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;缓存层也可以通过支持主从同步的缓存集群来解决缓存层的高可用问题。以redis为例，redis天然支持主从同步，redis官方也有sentinel哨兵机制，来做redis的存活性检测。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当redis主挂了的时候，sentinel能够探测到，会通知调用方访问新的redis，整个过程由sentinel和redis集群配合完成，对调用方是透明的。&lt;/p&gt;

&lt;p&gt;5、【服务层&amp;gt;数据库层】的高可用&lt;/p&gt;

&lt;p&gt;大部分互联网技术，数据库层都用了“主从同步，读写分离”架构，所以数据库层的高可用，又分为“读库高可用”与“写库高可用”两类。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【服务层】到【数据库读】的高可用，是通过&lt;strong&gt;读库的冗余&lt;/strong&gt;来实现的。既然冗余了读库，一般来说就至少有2个从库，“数据库连接池”会建立与读库多个连接，每次请求会路由到这些读库。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive13.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当读库挂了的时候，db-connection-pool能够探测到，会自动的进行故障转移，将流量自动迁移到其他的读库，整个过程由连接池自动完成，对调用方是透明的（所以说DAO中的数据库连接池是很重要的基础组件）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive14.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【服务层】到【数据库写】的高可用，是通过&lt;strong&gt;写库的冗余&lt;/strong&gt;来实现的。以mysql为例，可以设置两个mysql双主同步，一台对线上提供服务，另一台冗余以保证高可用，常见的实践是keepalived存活探测，相同virtual IP提供服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive15.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;自动故障转移：当写库挂了的时候，keepalived能够探测到，会自动的进行故障转移，将流量自动迁移到shadow-db-master，由于使用的是相同的virtual IP，这个切换过程对调用方是透明的。&lt;/p&gt;

&lt;p&gt;通过整体架构的分析，我们可以看到高可用的思想就是通过&lt;strong&gt;冗余（多实例，集群化）和在调度（最常见的就是负载均衡）的基础上实现故障自动转移&lt;/strong&gt;来完成的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;负载均衡&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;提到高可用，不得不提负载均衡，其实高可用的实现还是依赖于负载均衡的，是在负载均衡的基础上实现的故障自动转移，没有冗余节点也即是多实例的负载均衡，也谈不上故障自动转移。但是负载均衡的重点还是在调度均衡，不光在
最前端的nginx，还有后端数据库连接池都存在这个均衡的问题。&lt;/p&gt;

&lt;p&gt;常见互联网分布式架构如上，分为客户端层、反向代理nginx层、站点层、服务层、数据层。可以看到，每一个下游都有多个上游调用，只需要做到，每一个上游都均匀访问每一个下游，就能实现“将请求/数据【均匀】分摊到多个操作单元上执行”。&lt;/p&gt;

&lt;p&gt;1、【客户端层-&amp;gt;反向代理层】的负载均衡&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/lb.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【客户端层】到【反向代理层】的负载均衡，是通过“DNS轮询”实现的：DNS-server对于一个域名配置了多个解析ip，每次DNS解析请求来访问DNS-server，会轮询返回这些ip，保证每个ip的解析概率是相同的。这些ip就是nginx的外网ip，以做到每台nginx的请求分配也是均衡的。&lt;/p&gt;

&lt;p&gt;2、【反向代理层-&amp;gt;站点层】的负载均衡&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/lb1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【反向代理层】到【站点层】的负载均衡，是通过“nginx”实现的。通过修改nginx.conf，可以实现多种负载均衡策略：轮询，最少连接路由，IPhash等&lt;/p&gt;

&lt;p&gt;3、【站点层-&amp;gt;服务层】的负载均衡&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/lb2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【站点层】到【服务层】的负载均衡，是通过“服务连接池”实现的。连接池需要涉及负载均衡、故障转移、超时处理的相关机制。&lt;/p&gt;

&lt;p&gt;4、【数据层】的负载均衡&lt;/p&gt;

&lt;p&gt;在数据量很大的情况下，由于数据层（db，cache）涉及数据的水平切分，所以数据层的负载均衡更为复杂一些，它分为“数据的均衡”，与“请求的均衡”。业内常见的水平切分方式有这么几种：按照range水平切分，按照id哈希水平切分&lt;/p&gt;

&lt;p&gt;提到负载均衡我们需要了解一些专业名称&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）nginx：一个高性能的web-server和实施反向代理的软件&lt;/li&gt;
&lt;li&gt;2）lvs：Linux Virtual Server，使用集群技术，实现在linux操作系统层面的一个高性能、高可用、负载均衡服务器&lt;/li&gt;
&lt;li&gt;3）keepalived：一款用来检测服务状态存活性的软件，常用来做高可用&lt;/li&gt;
&lt;li&gt;4）f5：一个高性能、高可用、负载均衡的硬件设备（听上去和lvs功能差不多？）&lt;/li&gt;
&lt;li&gt;5）DNS轮询：通过在DNS-server上对一个域名设置多个ip解析，来扩充web-server性能及实施负载均衡的技术&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来看一下负载均衡随着量的扩大不断演进的过程&lt;/p&gt;

&lt;p&gt;1、单机架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;浏览器通过DNS-server，域名解析到ip，直接访问webserver，这种情况下的缺点显而易见&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）非高可用，web-server挂了整个系统就挂了&lt;/li&gt;
&lt;li&gt;2）扩展性差，当吞吐量达到web-server上限时，无法扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、DNS轮询&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过dns轮询将流量分到不同的webserver。这种情况下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）非高可用：DNS-server只负责域名解析ip，这个ip对应的服务是否可用，DNS-server是不保证的，假设有一个web-server挂了，部分服务会受到影响&lt;/li&gt;
&lt;li&gt;2）扩容非实时：DNS解析有一个生效周期&lt;/li&gt;
&lt;li&gt;3）暴露了太多的外网ip&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、nginx&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个方案在站点层与浏览器层之间加入了一个反向代理层，利用高性能的nginx来做反向代理，利用nginx来实现流量分流，但是nginx也是单点。&lt;/p&gt;

&lt;p&gt;4、nginx+keepalived&lt;/p&gt;

&lt;p&gt;为了解决nginx高可用的问题，keepalived出场了，当nginx挂了，另一个nginx就顶上来了&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这种情况下，仍会存在问题&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）资源利用率只有50%&lt;/li&gt;
&lt;li&gt;2）nginx仍然是接入单点，如果接入吞吐量超过的nginx的性能上限怎么办&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、lvs/f5&lt;/p&gt;

&lt;p&gt;nginx毕竟是软件，性能比tomcat好，但总有个上限，超出了上限，还是扛不住。lvs就不一样了，它实施在操作系统层面；f5的性能又更好了，它实施在硬件层面；它们性能比nginx好很多，例如每秒可以抗10w，这样可以利用他们来扩容，常见的架构图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;基本上公司到这一步基本就能解决接入层高可用、扩展性、负载均衡的问题。当然如果业务量继续扩大，我们还可以进行扩展&lt;/p&gt;

&lt;p&gt;5、DNS轮询&lt;/p&gt;

&lt;p&gt;facebook，google，baidu的PV是不是超过80亿呢，它们的域名只对应一个ip么，终点又是起点，还是得通过DNS轮询来进行扩容：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;架构图可见，在lvs上再新增了水平扩展，能力更加强大了，其实在上面也可以使用这种方式，来水平扩展nginx，所以总体的思路就是&lt;strong&gt;水平扩展，垂直提升&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;前面将的都是相同服务实例的均衡，其实也存在异构服务器，也就是实例能力不相同的情况，也就是负载均衡的另一种体现，后端的service有可能部署在硬件条件不同的服务器上：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）如果对标最低配的服务器“均匀”分摊负载，高配的服务器的利用率不足；&lt;/li&gt;
&lt;li&gt;2）如果对标最高配的服务器“均匀”分摊负载，低配的服务器可能会扛不住；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;需要做到根据能力来实现负载均衡，其实在nginx中就有这个概念，我们通过权重来给服务器来实现分配&lt;/p&gt;

&lt;p&gt;1、静态权重&lt;/p&gt;

&lt;p&gt;为每个下游service设置一个“权重”，代表service的处理能力，来调整访问到每个service的概率，使用nginx做反向代理与负载均衡，就有类似的机制。&lt;/p&gt;

&lt;p&gt;这个方案的优点是：简单，能够快速的实现异构服务器的负载均衡。&lt;/p&gt;

&lt;p&gt;缺点也很明显：这个权重是固定的，无法自适应动态调整，而很多时候，服务器的处理能力是很难用一个固定的数值量化。&lt;/p&gt;

&lt;p&gt;2、动态权重&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）用一个动态权重来标识每个service的处理能力，默认初始处理能力相同，即分配给每个service的概率相等；&lt;/li&gt;
&lt;li&gt;2）每当service成功处理一个请求，认为service处理能力足够，权重动态+1；&lt;/li&gt;
&lt;li&gt;3）每当service超时处理一个请求，认为service处理能力可能要跟不上了，权重动态-10（权重下降会更快）；&lt;/li&gt;
&lt;li&gt;4）为了方便权重的处理，可以把权重的范围限定为[0, 100]，把权重的初始值设为60分。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这种情况下，我们需要使用过载保护，就是在服务器承受压力到瓶颈的时候，就能够维持在这个能力，而不是继续增加将其压垮，最简易的方式，服务端设定一个负载阈值，超过这个阈值的请求压过来，全部抛弃。这个方式不是特别优雅。&lt;/p&gt;

&lt;p&gt;我们在动态权重中，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）如果某一个service的连接上，连续3个请求都超时，即连续-10分三次，客户端就可以认为，服务器慢慢的要处理不过来了，得给这个service缓一小口气，于是设定策略：接下来的若干时间内，例如1秒（或者接下来的若干个请求），请求不再分配给这个service；防止其处理能力变为0&lt;/li&gt;
&lt;li&gt;2）如果某一个service的动态权重，降为了0（像连续10个请求超时，中间休息了3次还超时），客户端就可以认为，服务器完全处理不过来了，得给这个service喘一大口气，于是设定策略：接下来的若干时间内，例如1分钟（为什么是1分钟，根据经验，此时service一般在发生fullGC，差不多1分钟能回过神来），请求不再分配给这个service；但是要进行回复就要继续给他分配。&lt;/li&gt;
&lt;li&gt;3）可以有更复杂的保护策略…&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;单点系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;单点系统并不是一种架构思想，但是提到高可用，肯定就要知道单点系统，并不是所有的系统都能实现高可用，单点系统一般来说存在两个很大的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;非高可用：既然是单点，master一旦发生故障，服务就会受到影响&lt;/li&gt;
&lt;li&gt;性能瓶颈：既然是单点，不具备良好的扩展性，服务性能总有一个上限，这个单点的性能上限往往就是整个系统的性能上限&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如我们常用的架构中，也有避免不了的单点，在这个互联网架构中，站点层、服务层、数据库的从库都可以通过冗余的方式来保证高可用，但至少&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nginx层是一个潜在的单点&lt;/li&gt;
&lt;li&gt;数据库写库master也是一个潜在的单点&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;再比如GFS&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/gfs.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;GFS的系统架构里主要有这么几种角色：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）client，就是发起文件读写的调用端&lt;/li&gt;
&lt;li&gt;（2）master，这是一个单点服务，它有全局事业，掌握文件元信息&lt;/li&gt;
&lt;li&gt;（3）chunk-server，实际存储文件额服务器&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个系统里，master也是一个单点的服务，Map-reduce系统里也有类似的全局协调的master单点角色。系统架构设计中，像nginx，db-master，gfs-master这样的单点服务是存在的，也是我们需要解决的&lt;/p&gt;

&lt;p&gt;1、shadow-master解决单点高可用问题&lt;/p&gt;

&lt;p&gt;“影子master”，顾名思义，服务正常时，它只是单点master的一个影子，在master出现故障时，shadow-master会自动变成master，继续提供服务。&lt;/p&gt;

&lt;p&gt;shadow-master它能够解决高可用的问题，并且故障的转移是自动的，不需要人工介入，但不足是它使服务资源的利用率降为了50%，业内经常使用keepalived+vip的方式实现这类单点的高可用。&lt;/p&gt;

&lt;p&gt;以GFS的master为例，master正常时：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）client会连接正常的master，shadow-master不对外提供服务&lt;/li&gt;
&lt;li&gt;（2）master与shadow-master之间有一种存活探测机制&lt;/li&gt;
&lt;li&gt;（3）master与shadow-master有相同的虚IP（virtual-IP）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/gfs1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/gfs2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当发现master异常时：shadow-master会自动顶上成为master，虚IP机制可以保证这个过程对调用方是透明的，比如我们k8s的master的三主机制，比如数据库的双master机制都是使用了这种方案，只不过数据库还需要实现双主的数据同步，实现一致性的要求。&lt;/p&gt;

&lt;p&gt;2、减少与单点的交互，是存在单点的系统优化的核心方向&lt;/p&gt;

&lt;p&gt;既然知道单点存在性能上限，单点的性能（例如GFS中的master）有可能成为系统的瓶颈，那么，减少与单点的交互，便成了存在单点的系统优化的核心方向。怎么来减少与单点的交互，这里提两种常见的方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;批量写&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;批量写是一种常见的提升单点性能的方式。比如利用数据库写单点生成做“ID生成器”，常规方法就是利用数据库写单点的auto increament id来生成和返回ID，这样生成ID的并发上限，取决于单点数据库的写性能上限。如果我们使用批量的方式&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/pl.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）中间加一个服务，每次从数据库拿出100个id&lt;/li&gt;
&lt;li&gt;（2）业务方需要ID&lt;/li&gt;
&lt;li&gt;（3）服务直接返回100个id中的1个，100个分配完，再访问数据库&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样一来，每分配100个才会写数据库一次，分配id的性能可以认为提升了100倍。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;客户端缓存&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;客户端缓存也是一种降低与单点交互次数，提升系统整体性能的方法。比如上面的GFS的master使用了客户端缓存减少了与master的交互&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）GFS的调用客户端client要访问shenjian.txt，先查询本地缓存，miss了&lt;/li&gt;
&lt;li&gt;（2）client访问master问说文件在哪里，master告诉client在chunk3上&lt;/li&gt;
&lt;li&gt;（3）client把shenjian.txt存放在chunk3上记录到本地的缓存，然后进行文件的读写操作&lt;/li&gt;
&lt;li&gt;（4）未来client要访问文件，从本地缓存中查找到对应的记录，就不用再请求master了，可以直接访问chunk-server。如果文件发生了转移，chunk3返回client说“文件不在我这儿了”，client再访问master，询问文件所在的服务器。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据经验，这类缓存的命中非常非常高，可能在99.9%以上（因为文件的自动迁移是小概率事件），这样与master的交互次数就降低了1000倍。&lt;/p&gt;

&lt;p&gt;3、“DNS轮询”技术支持DNS-server返回不同的nginx外网IP，实现nginx负载均衡层的水平扩展。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/nginx.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;DNS-server部分，一个域名可以配置多个IP，每次DNS解析请求，轮询返回不同的IP，就能实现nginx的水平扩展，扩充负载均衡层的整体性能。&lt;/p&gt;

&lt;p&gt;数据库单点写库也是同样的道理，在数据量很大的情况下，可以通过水平拆分，来提升写入性能。&lt;/p&gt;

&lt;p&gt;遗憾的是，并不是所有的业务场景都可以水平拆分，例如秒杀业务，商品的条数可能不多，数据库的数据量不大，就不能通过水平拆分来提升秒杀系统的整体写性能&lt;/p&gt;

&lt;h3 id=&#34;高并发-高性能-扩展性&#34;&gt;高并发，高性能，扩展性&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrence/&#34;&gt;高并发（High Concurrency）&lt;/a&gt;是互联网分布式系统架构设计中必须考虑的因素之一，互联网分布式架构设计，提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;垂直扩展

&lt;ul&gt;
&lt;li&gt;机器配置升级，这种最原始的方式，也是提高性能最快的方式。所以是初期互联网业务发展非常迅猛的最推荐的方式，如果预算不是问题。&lt;/li&gt;
&lt;li&gt;架构完善，组件优化，其实也是属于垂直扩展，通过优化架构，能够提高单机的支持的能力，这个就需要大量的实践经验，调优经验，如：使用Cache来减少IO次数，使用异步来增加单服务吞吐量，使用无锁数据结构来减少响应时间；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;水平扩展，不管是提升单机硬件性能，还是提升单机架构性能，都有一个致命的不足：单机性能总是有极限的。所以互联网分布式架构设计高并发终极解决方案还是水平扩展：只要增加服务器数量，就能线性扩充系统性能。水平扩展在现在最直观的就是分布式集群的应用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;高并发是我们架构设计中最常见的，很多细节实现可以看&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrence/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;技术思想&#34;&gt;技术思想&lt;/h1&gt;

&lt;p&gt;1、技术和业务：技术和业务是相辅相成的，任何技术的初衷都是为了服务业务，发展技术的目的是为了更好的服务业务，一旦脱离业务，技术就失去了意义。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s组件系列（一）---- K8s apiserver 详解</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-apiserver/</link>
          <pubDate>Sat, 24 Oct 2020 20:22:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-apiserver/</guid>
          <description>&lt;p&gt;apiserver是集群的核心，kubernetes API server的核心功能是提供了kubernetes各类资源对象（pod、RC 、service等）的增、删、改、查以及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。&lt;/p&gt;

&lt;h1 id=&#34;接口&#34;&gt;接口&lt;/h1&gt;

&lt;p&gt;本地端口&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;用于http请求&lt;/li&gt;
&lt;li&gt;默认8080，可以通过启动参数“&amp;ndash;insecure-port”修改&lt;/li&gt;
&lt;li&gt;默认ip为localhost，可以通过启动参数“&amp;ndash;insecure-bind-address”来修改&lt;/li&gt;
&lt;li&gt;不需要认证或者授权&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;安全端口&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;用于https请求&lt;/li&gt;
&lt;li&gt;默认端口6443，可以通过启动参数“&amp;ndash;secure-port”修改&lt;/li&gt;
&lt;li&gt;默认ip为非本地网络接口，可以通过启动参数“&amp;ndash;bind-address”来修改&lt;/li&gt;
&lt;li&gt;需要认证或者授权&lt;/li&gt;
&lt;li&gt;默认不启动&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;api&#34;&gt;api&lt;/h1&gt;

&lt;p&gt;我们可以通过三种方式来访问apiserver提供的接口&lt;/p&gt;

&lt;h2 id=&#34;rest-api&#34;&gt;REST API&lt;/h2&gt;

&lt;p&gt;1、版本和资源对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl localhost:8080/api  #查看kubernetes API的版本信息
curl localhost:8080/api/v1  #查看kubernetes API支持的所有的资源对象
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、具体的资源操作&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;首先要找到具体的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;localhost:8080/api/v1/资源对象（ns，pod，service）
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后不同的资源需要不同的处理&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、proxy接口&lt;/p&gt;

&lt;p&gt;kubernetes API server还提供了一类很特殊的rest接口—proxy接口，这个结构就是代理REST请求，即kubernetes API server把收到的rest请求转发到某个node上的kubelet守护进程的rest端口上，由该kubelet进程负责相应。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;node
    masterIP:8080/api/v1/proxy/nodes/{node_name}/pods  #某个节点下所有pod信息
    masterIP:8080/api/v1/proxy/nodes/{node_name}/stats  #某个节点内物理资源的统计信息
    masterIP:8080/api/v1/proxy/nodes/{node_name}/spec  #某个节点的概要信息
pod
    masterIP:8080/api/v1/proxy/namespaces/{namespace}/pods/{pod_name}/{path:*} #访问pod的某个服务接口
    masterIP:8080/api/v1/proxy/namespaces/{namespace}/pods/{pod_name}  #访问pod
service
    masterIP:8080/api/v1/proxy/namespaces/{namespace}/services/{service_name}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在master的 API Server 进程同时提供了 swagger-ui 的访问地址：http://&lt;master-ip&gt;: &lt;master-port&gt;/swagger-ui/，可以查看所有的api，&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-api/&#34;&gt;更多api详细了解&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;各种语言的client-lib&#34;&gt;各种语言的client lib&lt;/h2&gt;

&lt;p&gt;也就是对上面api接口的封装。&lt;/p&gt;

&lt;h2 id=&#34;命令行kubectl&#34;&gt;命令行kubectl&lt;/h2&gt;

&lt;p&gt;kubectl的原理是将输入的转化为REST API来调用，将返回结果输出。只是对REST API的一种封装，可以说是apiserver的一个客户端&lt;/p&gt;

&lt;p&gt;常规使用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl [command] [options]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、command&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;help 帮助命令，可以查找所有的命令，在我们不会用的适合，要学会使用这个命令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl help
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;get   获取信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get po
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;describe  获取相关的详细信息,很多的资源信息都可以在这边获取，比如node的资源分配和使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe po rc-nginx-2-btv4j
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;create  创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f rc-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;replace  更新替换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl replace -f rc-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;patch  如果一个容器已经在运行，这时需要对一些容器属性进行修改，又不想删除容器，或不方便通过replace的方式进行更新。kubernetes还提供了一种在容器运行时，直接对容器进行修改的方式，就是patch命令.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl patch pod rc-nginx-2-kpiqt -p &#39;{&amp;quot;metadata&amp;quot;:{&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx-3&amp;quot;}}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;edit edit提供了另一种更新resource源的操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl edit po rc-nginx-btv4j
上面命令的效果等效于：
kubectl get po rc-nginx-btv4j -o yaml &amp;gt;&amp;gt; /tmp/nginx-tmp.yaml
vim /tmp/nginx-tmp.yaml
/*do some changes here */
kubectl replace -f /tmp/nginx-tmp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Delete  删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete -f rc-nginx.yaml
kubectl delete po rc-nginx-btv4j
kubectl delete po -lapp=nginx-2
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;logs    显示日志，跟docker的logs命令类似。如果要获得tail -f 的方式，也可以使用-f选项。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs rc-nginx-2-kpiqt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;rolling-update  滚动更新.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl rolling-update rc-nginx-2 -f rc-nginx.yaml，
这个还提供如果在升级过程中，发现有问题还可以中途停止update，并回滚到前面版本
kubectl rolling-update rc-nginx-2 —rollback
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;scale 扩容缩容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl scale rc rc-nginx-3 —replicas=4
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;cp将文件直接拷进容器内&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl cp 文件 pod：路径
kubectl cp ./test.war logtestjbossforone-7b89dd5c9-297pf:/opt/wildfly/standalone/deployments
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;exec进入某个容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl exec filebeat-27 -c container -n namespaces sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;以上都是常用的，其他的可以使用时通过help去使用&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2、options&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;-n=&amp;ndash;namespace 指定命名空间&lt;/li&gt;
&lt;li&gt;其他的可以通过kubectl options来查看使用&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-tutorial/#kubectl&#34;&gt;更多kubectl用法&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;apiserver的作用和原理&#34;&gt;apiserver的作用和原理&lt;/h1&gt;

&lt;h2 id=&#34;集群内通信&#34;&gt;集群内通信&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/k8s&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;apiserver负责各个模块之间的通信，集群里的功能模块通过apiserver将信息存入到etcd中，其他模块通过apiserver读取这些信息，实现来模块之间的交互，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;场景一（kubelet和API Server）：每个node节点上的kubelet每个一个时间周期，就会调用一次API Server的REST接口报告自身的状态，API Server接受到这些信息后，将节点信息更新到etcd中。还有，kubelet也通过API Server的Watch接口（读取etcd的数据）监听Pod信息，如果监听到新的Pod副本被调用绑定到本节点，则执行pod对应的容器的创建和启动；如果监听到Pod的删除操作，则删除本节点上相应的Pod容器；如果检测到修改操作，则kubelet会相应的修改本节点的Pod的容器。&lt;/li&gt;
&lt;li&gt;场景二（kube-controller-manager和API Server）：kube-controller-manager中的Node Controller模块通过API Server模块提供的WATCH接口（读取etcd的数据），实时监控Node信息。并做相应的处理。&lt;/li&gt;
&lt;li&gt;场景三（scheduler和API Server）：当Scheduler通过API Server的Watch接口（读取etcd的数据）监听到新建Pod副本的信息后，它会检索所有符合该Pod要求的Node列表，开始执行Pod调度逻辑，调度成功后将Pod绑定到目标节点上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里多的功能模块都会频繁的使用API Server，而且API Server这个服务也是如此的重要，长时间的压力工作，会不会容器挂掉。问得好，k8s为了缓解集群各模块对API Server的访问压力，各模块之间都采用了缓存机制。各个模块定时的从API Server获取制定资源对象信息，并缓存到本地，这样各个功能模块先从本地获取资源对象信息，本地没有时再访问API Server。&lt;/p&gt;

&lt;h2 id=&#34;api-server-list-watch-机制解析&#34;&gt;api server list watch 机制解析&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/api1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;api server 通过etcd的watch 接口监听资源的变更情况,当事件发生时 etcd 会通知 api server 比如步骤3, 为了让K8S的其他组件不依赖于etcd, api server 模仿etcd 提供了watch 机制, 当事件发生时,通知对应的组件 比如 4 8 12步骤中的实践 0 表示最开始进行watch 监控.&lt;/p&gt;

&lt;p&gt;Etcd 存储集群的数据信息，apiserver 作为统一入口，任何对数据的操作都必须经过 apiserver。客户端(kubelet/scheduler/ontroller-manager)通过 list-watch 监听 apiserver 中资源(pod/rs/rc 等等)的 create, update 和 delete 事件，并针对事件类型调用相应的事件处理函数。
list-watch 有两部分组成，分别是 list 和 watch。list 非常好理解，就是调用资源的 list API 罗列资源，基于 HTTP 短链接实现；watch 则是调用资源的 watch API 监听资源变更事件，基于 HTTP 长链接实现&lt;/p&gt;

&lt;p&gt;K8S 的 informer 模块(开发controller的时候也有informer模块)封装 list-watch API，用户只需要指定资源，编写事件处理函数，AddFunc, UpdateFunc 和 DeleteFunc 等。如下图所示，informer 首先通过 list API 罗列资源，然后调用 watch API 监听资源的变更事件，并将结果放入到一个 FIFO 队列，队列的另一头有协程从中取出事件，并调用对应的注册函数处理事件。Informer 还维护了一个只读的 Map Store 缓存，主要为了提升查询的效率，降低 apiserver 的负载。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当客户端调用 watch API 时，apiserver 在 response 的 HTTP Header 中设置 Transfer-Encoding 的值为 chunked，表示采用分块传输编码，客户端收到该信息后，便和服务端该链接，并等待下一个数据块，即资源的事件信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Curl –I 8080/api/v1/watch/pods?watch=yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;List-watch机制满足异步消息的系统&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;消息可靠性：保证消息不丢失，list获取全量数据，watch获取增量数据&lt;/li&gt;
&lt;li&gt;消息实时性：list-watch 机制下，每当 apiserver 的资源产生状态变更事件，都会将事件及时的推送给客户端，从而保证了消息的实时性。&lt;/li&gt;
&lt;li&gt;消息顺序性：K8S 在每个资源的事件中都带一个 resourceVersion 的标签，这个标签是递增的数字，所以当客户端并发处理同一个资源的事件时，它就可以对比 resourceVersion 来保证最终的状态和最新的事件所期望的状态保持一致。&lt;/li&gt;
&lt;li&gt;高性能&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;apiserver的内部结构&#34;&gt;apiserver的内部结构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/api&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;api层: 主要提供对外的 rest api&lt;/li&gt;
&lt;li&gt;访问控制层: 验证身份与鉴权,根据配置的各种资源访问许可逻辑(Adminssion control) ,判断是否允许访问&lt;/li&gt;
&lt;li&gt;注册表层: K8S 将所有对象都保存在registry 中, 针对 registry 中的各种资源对象, 都定义对象类型, 如何创建资源对象, 如何转换不同版本, 以及如何将资源编码和解码为json 或protobuf 格式进行存储.&lt;/li&gt;
&lt;li&gt;etcd 数据库: 用于持久化存储资源对象.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubernetes中api聚合与crd概要设计&#34;&gt;kubernetes中api聚合与CRD概要设计&lt;/h2&gt;

&lt;p&gt;kubernetes中apiserver的设计无疑是复杂的,其自身内部就包含了三种角色的api服务,今天我们一起来臆测下其内部的设计,搞明白aggregator、apiserver、apiExtensionsServer(crd server)的设计精要&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;从web服务到web网关到CRD&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;apiserver还是蛮复杂的，我们只讨论其kube-aggregator/apiserver/apiextensions三者架构上的设计，而不关注诸如请求认证、准入控制、权限等等&lt;/p&gt;

&lt;p&gt;1、最基础的REST服务&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一个最基础的Rest服务通常会包括一个resource资源和一组HTTP请求的方法, 在kubernetes中被称为一个REST，其内部还内嵌了一个Store(可以理解为继承)，其提供了针对某个具体资源的所有操作的集合，也就是我们常说的最终执行CRUD的具体操作的实现&lt;/p&gt;

&lt;p&gt;2、Service&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们有了Rest就可以提供各种k8s中资源的管理，但是如果我要进行扩展呢，如果要支持一些外部的资源k8s中不存在， 最简单的方式肯定就是在外部独立一个服务了，由这个服务自己管理数据存储、变更、控制等等逻辑&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;APIAggregator&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当通过外部服务来进行集群资源扩展的时候，针对这类资源我们如何集成到当前的apiserver中呢？为此k8s中设计了APIAggregator组件(其实APIAggreator组件还包括代理后端服务等功能)，来实现外部服务的集成，这样开发人员不用修改k8s代码，也可以来自定义服务信息&lt;/p&gt;

&lt;p&gt;1、一个服务的基本功能&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一个基础的业务服务通常包含数据模型、控制逻辑、持久化存储、基础功能(认证、监控、日志等等)等等，为了要创建一个服务，我们通常需要如下操作(不包含设计阶段)：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;选择合适的框架(完成基础功能)&lt;/li&gt;
&lt;li&gt;定义数据模型&lt;/li&gt;
&lt;li&gt;选择数据存储&lt;/li&gt;
&lt;li&gt;编写业务控制逻辑&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里面除了业务控制逻辑，其余部分在大多数情况下可能都是通用，比如框架、数据存储这些，那能不能简化下？来看大招CRD&lt;/p&gt;

&lt;p&gt;2、CustomResourceDefinitions&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;CRD中文被称为自定义资源类型，其核心在k8s中提供数据模型定义、数据存储、基础功能，这样如果我们要扩展服务就只需要编写一个业务逻辑控制器即可， 我们思考下其场景&lt;/p&gt;

&lt;p&gt;通常web请求的处理流程都是反序列化、验证字段、业务逻辑处理、数据存储，而在k8s中业务控制逻辑大多数由controller来进行，那为了支持CRD剩余工作肯定也是在k8s中完成的&lt;/p&gt;

&lt;p&gt;在我们完成定义模型之后，k8s的crd模块需要进行对应资源REST的构建、验证、转换、存储等操作这些无疑都是耗费资源的，而且在apiserver这种数据总线上，由此可以发行CRD并不支持大规模的数据存储&lt;/p&gt;

&lt;p&gt;3、CRD server&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;CRDServer主要就是负责CRD资源的管理，其会监听CRD资源的变更，并且为其创建对应的REST接口，完成对应的认证、转换、验证、存储等机制&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ServerChan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ServerChan从设计上更类似一种责任链的模式，简单来说如果我处理不了该请求，我就交给下个人处理，这种操作在k8s中被称为delegate(委托),接下来我们开始了解其关键实现&lt;/p&gt;

&lt;p&gt;1、服务的角色划分&lt;/p&gt;

&lt;p&gt;到目前我们已经有了三个server, 其中APIAggregator负责外部服务的集成和内部请求的转发，apiserver服务k8s汇总内部资源的控制，CRDServer则负责用户自定义资源的处理，然后我们就只需要将三者串联起来，就是我们最终的apiserver&lt;/p&gt;

&lt;p&gt;2、责任链上的层层委托&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当APIAggregator接收到请求之后，如果发现对应的是一个service的请求，则会直接转发到对应的服务上否则则委托给apiserver进行处理，apiserver中根据当前URL来选择对应的REST接口处理，如果未能找到对应的处理，则会交由CRD server处理， CRD server检测是否已经注册对应的CRD资源，如果注册就处理&lt;/p&gt;

&lt;p&gt;3、APIAggregator上的服务注册&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;APIAggreagtor中会通过informer 监听后端Service的变化，如果发现有新的服务，就会创建对应的代理转发，从而实现对应的服务注册&lt;/p&gt;

&lt;p&gt;4、CRD Server中的资源感知&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当在集群中创建了对应的CRD资源的时候，会通过内部的controller来感知对应的CRD资源信息，然后为其创建对应的REST处理接口，这样后续接收到对应的资源就可以进行处理了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基础概览图&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/apiserver8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;源码解析&#34;&gt;源码解析&lt;/h1&gt;

&lt;p&gt;apiserver是一套基于restful类型的接口，我们来通过源码解析看服务的启动和实现&lt;/p&gt;

&lt;p&gt;看源码，先了解kubernetes的源码结构，cmd是入口，pkg是主要实现。&lt;/p&gt;

&lt;p&gt;看apiserver的入口文件kubernetes-1.6.1/cmd/kube-apiserver/apiserver.go,比较简单，主要是初始化一些结构，然后调用run来实现apiserver的启动。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    rand.Seed(time.Now().UTC(). UnixNano())

    s := options.NewServerRunOptions()  //新建一个APIServer对象，APIServer结构体
    s.AddFlags(pflag.CommandLine)       //命令行参数输入

    flag.InitFlags()                    //解析并格式化传入的参数，填充kubeletserver结构体
    logs.InitLogs()                     //初始化日志
    defer logs.FlushLogs()              //刷新日志到磁盘，这边用了defer，可见是在进程推出后保存日志

    verflag.PrintAndExitIfRequested()

    if err := app.Run(s); err != nil {          //启动，run
        fmt.Fprintf(os.Stderr, &amp;quot;%v\n&amp;quot;, err)
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后主要在run函数中实现。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s组件系列（四）---- K8s kubelet 详解</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-kubelet/</link>
          <pubDate>Tue, 20 Oct 2020 20:22:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-kubelet/</guid>
          <description>&lt;p&gt;kubelet用于处理master节点下发到本节点的任务，管理Pod以及Pod中的容器。每个kubelet进程会在API Server上注册节点信息，定期向master节点汇报节点资源的使用情况，并通过cAdvisor监控容器和节点的资源。&lt;/p&gt;

&lt;h1 id=&#34;节点管理&#34;&gt;节点管理&lt;/h1&gt;

&lt;p&gt;节点通过设置kubelet的启动参数“&amp;ndash;register-node”来决定是否向API Server注册自己。如果该参数为true，那么kubelet将试着通过API Server注册自己。在自注册时，kubelet启动时还包括以下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--api-servers：API Server的位置
--kubeconfing：kubeconfig文件，用于访问API Server的安全配置文件
--cloud-provider：云服务商地址，仅用于共有云环境
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果没有选择自注册模式，用户需要手动去配置node的资源信息，同时告知ndoe上的kubelet API Server的位置。Kubelet在启动时通过API Server注册节点信息，并定时向API Server发送节点新消息，API Server在接受到这些消息之后，将这些信息写入etcd中。通过kubelet的启动参数“&amp;ndash;node-status-update-frequency”设置kubelet每个多长时间向API Server报告节点状态，默认为10s&lt;/p&gt;

&lt;h1 id=&#34;pod管理&#34;&gt;pod管理&lt;/h1&gt;

&lt;h2 id=&#34;资源获取&#34;&gt;资源获取&lt;/h2&gt;

&lt;p&gt;kubelet通过以下几种方式获取自身node上所要运行的pod清单：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文件：kubelet启动参数“&amp;ndash;config”指定的配置文件目录下的文件（默认为“/etc/Kubernetes/manifests”）通过&amp;ndash;file-check-frequency设置检查该文件的时间间隔，默认为20s&lt;/li&gt;
&lt;li&gt;HTTP端点：通过“&amp;ndash;manifest-url”参数设置。通过“&amp;ndash;http-check-frequency”设置检查该HTTP端点数据的时间间隔，默认为20s。&lt;/li&gt;
&lt;li&gt;API Server：kubelet通过API server监听etcd目录，同步pod列表&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：这里static pod，不是被API Server创建的，而是被kubelet创建，之前文章中提到了静态的pod是在kubelet的配置文件中编写，并且总在kubelet所在node上运行。
   &lt;/p&gt;

&lt;h2 id=&#34;创建流程&#34;&gt;创建流程&lt;/h2&gt;

&lt;p&gt;Kubelet监听etcd，所有针对pod的操作将会被kubelet监听到。如果是新的绑定到本节点的pod，则按照pod清单的要求创建pod，如果是删除pod，则kubelet通过docker client去删除pod中的容器，并删除该pod。&lt;/p&gt;

&lt;p&gt;具体的针对创建和修改pod任务，流程为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;为该pod创建一个目录&lt;/li&gt;
&lt;li&gt;从API Server读取该pod清单&lt;/li&gt;
&lt;li&gt;为该pod挂载外部volume&lt;/li&gt;
&lt;li&gt;下载pod用到的secret&lt;/li&gt;
&lt;li&gt;检查已经运行在节点中的pod,如果该pod没有容器或者Pause容器没有启动，则先停止pod里的所有容器的进程。如果pod中有需要删除的容器，则删除这些容器&lt;/li&gt;
&lt;li&gt;为pod中的每个容器做如下操作

&lt;ul&gt;
&lt;li&gt;为容器计算一个hash值，然后用容器的名字去查询docker容器的hash值。若查找到容器，且两者得到hash不同，则停止docker中的容器的进程，并且停止与之关联pause容器的进程；若两个相同，则不做任何处理&lt;/li&gt;
&lt;li&gt;如果容器被停止了，且容器没有指定restartPolicy(重启策略)，则不做任何处理&lt;/li&gt;
&lt;li&gt;调用docker client 下载容器镜像，调用docker client 运行容器&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;容器的健康检查&#34;&gt;容器的健康检查&lt;/h1&gt;

&lt;p&gt;Pod通过两类探针来检查容器的健康状态。一个是livenessProbe探针，用于判断容器是否健康，告诉kubelet一个容器什么时候处于不健康状态，如果livenessProbe探针探测到容器不健康，则kubelet将删除该容器，并根据容器的重启策略做相应的处理；如果一个容器不包含livenessProbe探针，那么kubelet认为livenessProbe探针的返回值永远为“success”。&lt;/p&gt;

&lt;p&gt;另一个探针为ReadinessProbe，用于判断容器是否启动完成，且准备接受请求。如果ReadinessProbe探针检测到失败，则pod的状态将被修改，endpoint controller将从service的endpoints中删除包含该容器所在pod的IP地址的endpoint条目。&lt;/p&gt;

&lt;h1 id=&#34;cadvisor资源监控&#34;&gt;cadvisor资源监控&lt;/h1&gt;

&lt;p&gt;cadcisor是为容器监控而生的监控工具，目前集成在kubelet中，以4194端口进行暴露。&lt;/p&gt;

&lt;h1 id=&#34;源码解析&#34;&gt;源码解析&lt;/h1&gt;

&lt;p&gt;目前是基于master来进行源码处理，目前release版本是1.20，最常用的是1.18&lt;/p&gt;

&lt;h2 id=&#34;启动分析&#34;&gt;启动分析&lt;/h2&gt;

&lt;p&gt;kubelet使用的是cobra第三方包来做启动包，同时做了命令行参数的处理，首先是NewKubeletCommand&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewKubeletCommand creates a *cobra.Command object with default parameters
func NewKubeletCommand() *cobra.Command {
    cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)
    cleanFlagSet.SetNormalizeFunc(cliflag.WordSepNormalizeFunc)
    kubeletFlags := options.NewKubeletFlags()
    kubeletConfig, err := options.NewKubeletConfiguration()
    // programmer error
    if err != nil {
        klog.Fatal(err)
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见kubelet的启动参数可以是命令行参数kubeletFlags，也可以是配置文件kubeletConfig，启动就是使用的cobra中的run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Run: func(cmd *cobra.Command, args []string) {
        // initial flag parse, since we disable cobra&#39;s flag parsing
        if err := cleanFlagSet.Parse(args); err != nil {
            cmd.Usage()
            klog.Fatal(err)
        }

        // check if there are non-flag arguments in the command line
        cmds := cleanFlagSet.Args()
        if len(cmds) &amp;gt; 0 {
            cmd.Usage()
            klog.Fatalf(&amp;quot;unknown command: %s&amp;quot;, cmds[0])
        }

        // short-circuit on help
        help, err := cleanFlagSet.GetBool(&amp;quot;help&amp;quot;)
        if err != nil {
            klog.Fatal(`&amp;quot;help&amp;quot; flag is non-bool, programmer error, please correct`)
        }
        if help {
            cmd.Help()
            return
        }

        // short-circuit on verflag
        verflag.PrintAndExitIfRequested()
        cliflag.PrintFlags(cleanFlagSet)

        // set feature gates from initial flags-based config
        if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil {
            klog.Fatal(err)
        }

        // validate the initial KubeletFlags
        if err := options.ValidateKubeletFlags(kubeletFlags); err != nil {
            klog.Fatal(err)
        }

        if kubeletFlags.ContainerRuntime == &amp;quot;remote&amp;quot; &amp;amp;&amp;amp; cleanFlagSet.Changed(&amp;quot;pod-infra-container-image&amp;quot;) {
            klog.Warning(&amp;quot;Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead&amp;quot;)
        }

        // load kubelet config file, if provided
        if configFile := kubeletFlags.KubeletConfigFile; len(configFile) &amp;gt; 0 {
            kubeletConfig, err = loadConfigFile(configFile)
            if err != nil {
                klog.Fatal(err)
            }
            // We must enforce flag precedence by re-parsing the command line into the new object.
            // This is necessary to preserve backwards-compatibility across binary upgrades.
            // See issue #56171 for more details.
            if err := kubeletConfigFlagPrecedence(kubeletConfig, args); err != nil {
                klog.Fatal(err)
            }
            // update feature gates based on new config
            if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil {
                klog.Fatal(err)
            }
        }

        // We always validate the local configuration (command line + config file).
        // This is the default &amp;quot;last-known-good&amp;quot; config for dynamic config, and must always remain valid.
        if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil {
            klog.Fatal(err)
        }

        if (kubeletConfig.KubeletCgroups != &amp;quot;&amp;quot; &amp;amp;&amp;amp; kubeletConfig.KubeReservedCgroup != &amp;quot;&amp;quot;) &amp;amp;&amp;amp; (0 != strings.Index(kubeletConfig.KubeletCgroups, kubeletConfig.KubeReservedCgroup)) {
            klog.Warning(&amp;quot;unsupported configuration:KubeletCgroups is not within KubeReservedCgroup&amp;quot;)
        }

        // use dynamic kubelet config, if enabled
        var kubeletConfigController *dynamickubeletconfig.Controller
        if dynamicConfigDir := kubeletFlags.DynamicConfigDir.Value(); len(dynamicConfigDir) &amp;gt; 0 {
            var dynamicKubeletConfig *kubeletconfiginternal.KubeletConfiguration
            dynamicKubeletConfig, kubeletConfigController, err = BootstrapKubeletConfigController(dynamicConfigDir,
                func(kc *kubeletconfiginternal.KubeletConfiguration) error {
                    // Here, we enforce flag precedence inside the controller, prior to the controller&#39;s validation sequence,
                    // so that we get a complete validation at the same point where we can decide to reject dynamic config.
                    // This fixes the flag-precedence component of issue #63305.
                    // See issue #56171 for general details on flag precedence.
                    return kubeletConfigFlagPrecedence(kc, args)
                })
            if err != nil {
                klog.Fatal(err)
            }
            // If we should just use our existing, local config, the controller will return a nil config
            if dynamicKubeletConfig != nil {
                kubeletConfig = dynamicKubeletConfig
                // Note: flag precedence was already enforced in the controller, prior to validation,
                // by our above transform function. Now we simply update feature gates from the new config.
                if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil {
                    klog.Fatal(err)
                }
            }
        }

        // construct a KubeletServer from kubeletFlags and kubeletConfig
        kubeletServer := &amp;amp;options.KubeletServer{
            KubeletFlags:         *kubeletFlags,
            KubeletConfiguration: *kubeletConfig,
        }

        // use kubeletServer to construct the default KubeletDeps
        kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate)
        if err != nil {
            klog.Fatal(err)
        }

        // add the kubelet config controller to kubeletDeps
        kubeletDeps.KubeletConfigController = kubeletConfigController

        // set up signal context here in order to be reused by kubelet and docker shim
        ctx := genericapiserver.SetupSignalContext()

        // run the kubelet
        klog.V(5).Infof(&amp;quot;KubeletConfiguration: %#v&amp;quot;, kubeletServer.KubeletConfiguration)
        if err := Run(ctx, kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate); err != nil {
            klog.Fatal(err)
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个方法虽然比较长，但是除了最终的Run方法，其余的步骤还是为kubelet的启动构建初始化的参数，无非就是换一个名称，换一个不同的结构体，并配置相依赖的参数，主要包括以下步骤：
- 解析参数，对参数的合法性进行判断；
- 根据kubeletConfig解析一些特殊的特性所需要配置的参数；
- 配置kubeletServer，包括KubeletFlags和KubeletConfiguration两个参数；
- 构造kubeletDeps结构体；
- 启动最终的Run方法。&lt;/p&gt;

&lt;p&gt;继续调用run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Run(ctx context.Context, s *options.KubeletServer, kubeDeps *kubelet.Dependencies, featureGate featuregate.FeatureGate) error {
    logOption := logs.NewOptions()
    logOption.LogFormat = s.Logging.Format
    logOption.LogSanitization = s.Logging.Sanitization
    logOption.Apply()
    // To help debugging, immediately log version
    klog.Infof(&amp;quot;Version: %+v&amp;quot;, version.Get())
    if err := initForOS(s.KubeletFlags.WindowsService, s.KubeletFlags.WindowsPriorityClass); err != nil {
        return fmt.Errorf(&amp;quot;failed OS init: %v&amp;quot;, err)
    }
    if err := run(ctx, s, kubeDeps, featureGate); err != nil {
        return fmt.Errorf(&amp;quot;failed to run Kubelet: %v&amp;quot;, err)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;initForOS通过对操作系统的判断，如果是windows系统需要做一些预先的特殊处理；然后继续执行run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func run(ctx context.Context, s *options.KubeletServer, kubeDeps *kubelet.Dependencies, featureGate featuregate.FeatureGate) (err error) {
    // Set global feature gates based on the value on the initial KubeletServer
    err = utilfeature.DefaultMutableFeatureGate.SetFromMap(s.KubeletConfiguration.FeatureGates)
    if err != nil {
        return err
    }
    // validate the initial KubeletServer (we set feature gates first, because this validation depends on feature gates)
    if err := options.ValidateKubeletServer(s); err != nil {
        return err
    }

    // Obtain Kubelet Lock File
    if s.ExitOnLockContention &amp;amp;&amp;amp; s.LockFilePath == &amp;quot;&amp;quot; {
        return errors.New(&amp;quot;cannot exit on lock file contention: no lock file specified&amp;quot;)
    }
    done := make(chan struct{})
    if s.LockFilePath != &amp;quot;&amp;quot; {
        klog.Infof(&amp;quot;acquiring file lock on %q&amp;quot;, s.LockFilePath)
        if err := flock.Acquire(s.LockFilePath); err != nil {
            return fmt.Errorf(&amp;quot;unable to acquire file lock on %q: %v&amp;quot;, s.LockFilePath, err)
        }
        if s.ExitOnLockContention {
            klog.Infof(&amp;quot;watching for inotify events for: %v&amp;quot;, s.LockFilePath)
            if err := watchForLockfileContention(s.LockFilePath, done); err != nil {
                return err
            }
        }
    }

    // Register current configuration with /configz endpoint
    err = initConfigz(&amp;amp;s.KubeletConfiguration)
    if err != nil {
        klog.Errorf(&amp;quot;unable to register KubeletConfiguration with configz, error: %v&amp;quot;, err)
    }

    if len(s.ShowHiddenMetricsForVersion) &amp;gt; 0 {
        metrics.SetShowHidden()
    }

    // About to get clients and such, detect standaloneMode
    standaloneMode := true
    if len(s.KubeConfig) &amp;gt; 0 {
        standaloneMode = false
    }

    if kubeDeps == nil {
        kubeDeps, err = UnsecuredDependencies(s, featureGate)
        if err != nil {
            return err
        }
    }

    if kubeDeps.Cloud == nil {
        if !cloudprovider.IsExternal(s.CloudProvider) {
            cloudprovider.DeprecationWarningForProvider(s.CloudProvider)
            cloud, err := cloudprovider.InitCloudProvider(s.CloudProvider, s.CloudConfigFile)
            if err != nil {
                return err
            }
            if cloud != nil {
                klog.V(2).Infof(&amp;quot;Successfully initialized cloud provider: %q from the config file: %q\n&amp;quot;, s.CloudProvider, s.CloudConfigFile)
            }
            kubeDeps.Cloud = cloud
        }
    }

    hostName, err := nodeutil.GetHostname(s.HostnameOverride)
    if err != nil {
        return err
    }
    nodeName, err := getNodeName(kubeDeps.Cloud, hostName)
    if err != nil {
        return err
    }

    // if in standalone mode, indicate as much by setting all clients to nil
    switch {
    case standaloneMode:
        kubeDeps.KubeClient = nil
        kubeDeps.EventClient = nil
        kubeDeps.HeartbeatClient = nil
        klog.Warningf(&amp;quot;standalone mode, no API client&amp;quot;)

    case kubeDeps.KubeClient == nil, kubeDeps.EventClient == nil, kubeDeps.HeartbeatClient == nil:
        clientConfig, closeAllConns, err := buildKubeletClientConfig(ctx, s, nodeName)
        if err != nil {
            return err
        }
        if closeAllConns == nil {
            return errors.New(&amp;quot;closeAllConns must be a valid function other than nil&amp;quot;)
        }
        kubeDeps.OnHeartbeatFailure = closeAllConns

        kubeDeps.KubeClient, err = clientset.NewForConfig(clientConfig)
        if err != nil {
            return fmt.Errorf(&amp;quot;failed to initialize kubelet client: %v&amp;quot;, err)
        }

        // make a separate client for events
        eventClientConfig := *clientConfig
        eventClientConfig.QPS = float32(s.EventRecordQPS)
        eventClientConfig.Burst = int(s.EventBurst)
        kubeDeps.EventClient, err = v1core.NewForConfig(&amp;amp;eventClientConfig)
        if err != nil {
            return fmt.Errorf(&amp;quot;failed to initialize kubelet event client: %v&amp;quot;, err)
        }

        // make a separate client for heartbeat with throttling disabled and a timeout attached
        heartbeatClientConfig := *clientConfig
        heartbeatClientConfig.Timeout = s.KubeletConfiguration.NodeStatusUpdateFrequency.Duration
        // The timeout is the minimum of the lease duration and status update frequency
        leaseTimeout := time.Duration(s.KubeletConfiguration.NodeLeaseDurationSeconds) * time.Second
        if heartbeatClientConfig.Timeout &amp;gt; leaseTimeout {
            heartbeatClientConfig.Timeout = leaseTimeout
        }

        heartbeatClientConfig.QPS = float32(-1)
        kubeDeps.HeartbeatClient, err = clientset.NewForConfig(&amp;amp;heartbeatClientConfig)
        if err != nil {
            return fmt.Errorf(&amp;quot;failed to initialize kubelet heartbeat client: %v&amp;quot;, err)
        }
    }

    if kubeDeps.Auth == nil {
        auth, runAuthenticatorCAReload, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)
        if err != nil {
            return err
        }
        kubeDeps.Auth = auth
        runAuthenticatorCAReload(ctx.Done())
    }

    var cgroupRoots []string
    nodeAllocatableRoot := cm.NodeAllocatableRoot(s.CgroupRoot, s.CgroupsPerQOS, s.CgroupDriver)
    cgroupRoots = append(cgroupRoots, nodeAllocatableRoot)
    kubeletCgroup, err := cm.GetKubeletContainer(s.KubeletCgroups)
    if err != nil {
        klog.Warningf(&amp;quot;failed to get the kubelet&#39;s cgroup: %v.  Kubelet system container metrics may be missing.&amp;quot;, err)
    } else if kubeletCgroup != &amp;quot;&amp;quot; {
        cgroupRoots = append(cgroupRoots, kubeletCgroup)
    }

    runtimeCgroup, err := cm.GetRuntimeContainer(s.ContainerRuntime, s.RuntimeCgroups)
    if err != nil {
        klog.Warningf(&amp;quot;failed to get the container runtime&#39;s cgroup: %v. Runtime system container metrics may be missing.&amp;quot;, err)
    } else if runtimeCgroup != &amp;quot;&amp;quot; {
        // RuntimeCgroups is optional, so ignore if it isn&#39;t specified
        cgroupRoots = append(cgroupRoots, runtimeCgroup)
    }

    if s.SystemCgroups != &amp;quot;&amp;quot; {
        // SystemCgroups is optional, so ignore if it isn&#39;t specified
        cgroupRoots = append(cgroupRoots, s.SystemCgroups)
    }

    if kubeDeps.CAdvisorInterface == nil {
        imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)
        kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cgroupRoots, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))
        if err != nil {
            return err
        }
    }

    // Setup event recorder if required.
    makeEventRecorder(kubeDeps, nodeName)

    if kubeDeps.ContainerManager == nil {
        if s.CgroupsPerQOS &amp;amp;&amp;amp; s.CgroupRoot == &amp;quot;&amp;quot; {
            klog.Info(&amp;quot;--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /&amp;quot;)
            s.CgroupRoot = &amp;quot;/&amp;quot;
        }

        var reservedSystemCPUs cpuset.CPUSet
        if s.ReservedSystemCPUs != &amp;quot;&amp;quot; {
            // is it safe do use CAdvisor here ??
            machineInfo, err := kubeDeps.CAdvisorInterface.MachineInfo()
            if err != nil {
                // if can&#39;t use CAdvisor here, fall back to non-explicit cpu list behavor
                klog.Warning(&amp;quot;Failed to get MachineInfo, set reservedSystemCPUs to empty&amp;quot;)
                reservedSystemCPUs = cpuset.NewCPUSet()
            } else {
                var errParse error
                reservedSystemCPUs, errParse = cpuset.Parse(s.ReservedSystemCPUs)
                if errParse != nil {
                    // invalid cpu list is provided, set reservedSystemCPUs to empty, so it won&#39;t overwrite kubeReserved/systemReserved
                    klog.Infof(&amp;quot;Invalid ReservedSystemCPUs \&amp;quot;%s\&amp;quot;&amp;quot;, s.ReservedSystemCPUs)
                    return errParse
                }
                reservedList := reservedSystemCPUs.ToSlice()
                first := reservedList[0]
                last := reservedList[len(reservedList)-1]
                if first &amp;lt; 0 || last &amp;gt;= machineInfo.NumCores {
                    // the specified cpuset is outside of the range of what the machine has
                    klog.Infof(&amp;quot;Invalid cpuset specified by --reserved-cpus&amp;quot;)
                    return fmt.Errorf(&amp;quot;Invalid cpuset %q specified by --reserved-cpus&amp;quot;, s.ReservedSystemCPUs)
                }
            }
        } else {
            reservedSystemCPUs = cpuset.NewCPUSet()
        }

        if reservedSystemCPUs.Size() &amp;gt; 0 {
            // at cmd option valication phase it is tested either --system-reserved-cgroup or --kube-reserved-cgroup is specified, so overwrite should be ok
            klog.Infof(&amp;quot;Option --reserved-cpus is specified, it will overwrite the cpu setting in KubeReserved=\&amp;quot;%v\&amp;quot;, SystemReserved=\&amp;quot;%v\&amp;quot;.&amp;quot;, s.KubeReserved, s.SystemReserved)
            if s.KubeReserved != nil {
                delete(s.KubeReserved, &amp;quot;cpu&amp;quot;)
            }
            if s.SystemReserved == nil {
                s.SystemReserved = make(map[string]string)
            }
            s.SystemReserved[&amp;quot;cpu&amp;quot;] = strconv.Itoa(reservedSystemCPUs.Size())
            klog.Infof(&amp;quot;After cpu setting is overwritten, KubeReserved=\&amp;quot;%v\&amp;quot;, SystemReserved=\&amp;quot;%v\&amp;quot;&amp;quot;, s.KubeReserved, s.SystemReserved)
        }
        kubeReserved, err := parseResourceList(s.KubeReserved)
        if err != nil {
            return err
        }
        systemReserved, err := parseResourceList(s.SystemReserved)
        if err != nil {
            return err
        }
        var hardEvictionThresholds []evictionapi.Threshold
        // If the user requested to ignore eviction thresholds, then do not set valid values for hardEvictionThresholds here.
        if !s.ExperimentalNodeAllocatableIgnoreEvictionThreshold {
            hardEvictionThresholds, err = eviction.ParseThresholdConfig([]string{}, s.EvictionHard, nil, nil, nil)
            if err != nil {
                return err
            }
        }
        experimentalQOSReserved, err := cm.ParseQOSReserved(s.QOSReserved)
        if err != nil {
            return err
        }

        devicePluginEnabled := utilfeature.DefaultFeatureGate.Enabled(features.DevicePlugins)

        kubeDeps.ContainerManager, err = cm.NewContainerManager(
            kubeDeps.Mounter,
            kubeDeps.CAdvisorInterface,
            cm.NodeConfig{
                RuntimeCgroupsName:    s.RuntimeCgroups,
                SystemCgroupsName:     s.SystemCgroups,
                KubeletCgroupsName:    s.KubeletCgroups,
                ContainerRuntime:      s.ContainerRuntime,
                CgroupsPerQOS:         s.CgroupsPerQOS,
                CgroupRoot:            s.CgroupRoot,
                CgroupDriver:          s.CgroupDriver,
                KubeletRootDir:        s.RootDirectory,
                ProtectKernelDefaults: s.ProtectKernelDefaults,
                NodeAllocatableConfig: cm.NodeAllocatableConfig{
                    KubeReservedCgroupName:   s.KubeReservedCgroup,
                    SystemReservedCgroupName: s.SystemReservedCgroup,
                    EnforceNodeAllocatable:   sets.NewString(s.EnforceNodeAllocatable...),
                    KubeReserved:             kubeReserved,
                    SystemReserved:           systemReserved,
                    ReservedSystemCPUs:       reservedSystemCPUs,
                    HardEvictionThresholds:   hardEvictionThresholds,
                },
                QOSReserved:                           *experimentalQOSReserved,
                ExperimentalCPUManagerPolicy:          s.CPUManagerPolicy,
                ExperimentalCPUManagerReconcilePeriod: s.CPUManagerReconcilePeriod.Duration,
                ExperimentalPodPidsLimit:              s.PodPidsLimit,
                EnforceCPULimits:                      s.CPUCFSQuota,
                CPUCFSQuotaPeriod:                     s.CPUCFSQuotaPeriod.Duration,
                ExperimentalTopologyManagerPolicy:     s.TopologyManagerPolicy,
                ExperimentalTopologyManagerScope:      s.TopologyManagerScope,
            },
            s.FailSwapOn,
            devicePluginEnabled,
            kubeDeps.Recorder)

        if err != nil {
            return err
        }
    }

    if err := checkPermissions(); err != nil {
        klog.Error(err)
    }

    utilruntime.ReallyCrash = s.ReallyCrashForTesting

    // TODO(vmarmol): Do this through container config.
    oomAdjuster := kubeDeps.OOMAdjuster
    if err := oomAdjuster.ApplyOOMScoreAdj(0, int(s.OOMScoreAdj)); err != nil {
        klog.Warning(err)
    }

    err = kubelet.PreInitRuntimeService(&amp;amp;s.KubeletConfiguration,
        kubeDeps, &amp;amp;s.ContainerRuntimeOptions,
        s.ContainerRuntime,
        s.RuntimeCgroups,
        s.RemoteRuntimeEndpoint,
        s.RemoteImageEndpoint,
        s.NonMasqueradeCIDR)
    if err != nil {
        return err
    }

    if err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil {
        return err
    }

    // If the kubelet config controller is available, and dynamic config is enabled, start the config and status sync loops
    if utilfeature.DefaultFeatureGate.Enabled(features.DynamicKubeletConfig) &amp;amp;&amp;amp; len(s.DynamicConfigDir.Value()) &amp;gt; 0 &amp;amp;&amp;amp;
        kubeDeps.KubeletConfigController != nil &amp;amp;&amp;amp; !standaloneMode &amp;amp;&amp;amp; !s.RunOnce {
        if err := kubeDeps.KubeletConfigController.StartSync(kubeDeps.KubeClient, kubeDeps.EventClient, string(nodeName)); err != nil {
            return err
        }
    }

    if s.HealthzPort &amp;gt; 0 {
        mux := http.NewServeMux()
        healthz.InstallHandler(mux)
        go wait.Until(func() {
            err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), mux)
            if err != nil {
                klog.Errorf(&amp;quot;Starting healthz server failed: %v&amp;quot;, err)
            }
        }, 5*time.Second, wait.NeverStop)
    }

    if s.RunOnce {
        return nil
    }

    // If systemd is used, notify it that we have started
    go daemon.SdNotify(false, &amp;quot;READY=1&amp;quot;)

    select {
    case &amp;lt;-done:
        break
    case &amp;lt;-ctx.Done():
        break
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要执行对参数的再一次验证，以及新的结构体的初始化。后续开始构建一些重要的客户端，包括eventClient主要处理事件的上报，与apiserver打交道；heartbeatClient主要处理心跳操作，与之后的PLEG相关；csiClient主要与CSI接口相关。配置完成之后，最终进入RunKubelet方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// RunKubelet is responsible for setting up and running a kubelet.  It is used in three different applications:
//   1 Integration tests
//   2 Kubelet binary
//   3 Standalone &#39;kubernetes&#39; binary
// Eventually, #2 will be replaced with instances of #3
func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error {
    hostname, err := nodeutil.GetHostname(kubeServer.HostnameOverride)
    if err != nil {
        return err
    }
    // Query the cloud provider for our node name, default to hostname if kubeDeps.Cloud == nil
    nodeName, err := getNodeName(kubeDeps.Cloud, hostname)
    if err != nil {
        return err
    }
    hostnameOverridden := len(kubeServer.HostnameOverride) &amp;gt; 0
    // Setup event recorder if required.
    makeEventRecorder(kubeDeps, nodeName)

    var nodeIPs []net.IP
    if kubeServer.NodeIP != &amp;quot;&amp;quot; {
        for _, ip := range strings.Split(kubeServer.NodeIP, &amp;quot;,&amp;quot;) {
            parsedNodeIP := net.ParseIP(strings.TrimSpace(ip))
            if parsedNodeIP == nil {
                klog.Warningf(&amp;quot;Could not parse --node-ip value %q; ignoring&amp;quot;, ip)
            } else {
                nodeIPs = append(nodeIPs, parsedNodeIP)
            }
        }
    }
    if !utilfeature.DefaultFeatureGate.Enabled(features.IPv6DualStack) &amp;amp;&amp;amp; len(nodeIPs) &amp;gt; 1 {
        return fmt.Errorf(&amp;quot;dual-stack --node-ip %q not supported in a single-stack cluster&amp;quot;, kubeServer.NodeIP)
    } else if len(nodeIPs) &amp;gt; 2 || (len(nodeIPs) == 2 &amp;amp;&amp;amp; utilnet.IsIPv6(nodeIPs[0]) == utilnet.IsIPv6(nodeIPs[1])) {
        return fmt.Errorf(&amp;quot;bad --node-ip %q; must contain either a single IP or a dual-stack pair of IPs&amp;quot;, kubeServer.NodeIP)
    } else if len(nodeIPs) == 2 &amp;amp;&amp;amp; kubeServer.CloudProvider != &amp;quot;&amp;quot; {
        return fmt.Errorf(&amp;quot;dual-stack --node-ip %q not supported when using a cloud provider&amp;quot;, kubeServer.NodeIP)
    } else if len(nodeIPs) == 2 &amp;amp;&amp;amp; (nodeIPs[0].IsUnspecified() || nodeIPs[1].IsUnspecified()) {
        return fmt.Errorf(&amp;quot;dual-stack --node-ip %q cannot include &#39;0.0.0.0&#39; or &#39;::&#39;&amp;quot;, kubeServer.NodeIP)
    }

    capabilities.Initialize(capabilities.Capabilities{
        AllowPrivileged: true,
    })

    credentialprovider.SetPreferredDockercfgPath(kubeServer.RootDirectory)
    klog.V(2).Infof(&amp;quot;Using root directory: %v&amp;quot;, kubeServer.RootDirectory)

    if kubeDeps.OSInterface == nil {
        kubeDeps.OSInterface = kubecontainer.RealOS{}
    }

    k, err := createAndInitKubelet(&amp;amp;kubeServer.KubeletConfiguration,
        kubeDeps,
        &amp;amp;kubeServer.ContainerRuntimeOptions,
        kubeServer.ContainerRuntime,
        hostname,
        hostnameOverridden,
        nodeName,
        nodeIPs,
        kubeServer.ProviderID,
        kubeServer.CloudProvider,
        kubeServer.CertDirectory,
        kubeServer.RootDirectory,
        kubeServer.ImageCredentialProviderConfigFile,
        kubeServer.ImageCredentialProviderBinDir,
        kubeServer.RegisterNode,
        kubeServer.RegisterWithTaints,
        kubeServer.AllowedUnsafeSysctls,
        kubeServer.ExperimentalMounterPath,
        kubeServer.KernelMemcgNotification,
        kubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount,
        kubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold,
        kubeServer.MinimumGCAge,
        kubeServer.MaxPerPodContainerCount,
        kubeServer.MaxContainerCount,
        kubeServer.MasterServiceNamespace,
        kubeServer.RegisterSchedulable,
        kubeServer.KeepTerminatedPodVolumes,
        kubeServer.NodeLabels,
        kubeServer.SeccompProfileRoot,
        kubeServer.NodeStatusMaxImages)
    if err != nil {
        return fmt.Errorf(&amp;quot;failed to create kubelet: %v&amp;quot;, err)
    }

    // NewMainKubelet should have set up a pod source config if one didn&#39;t exist
    // when the builder was run. This is just a precaution.
    if kubeDeps.PodConfig == nil {
        return fmt.Errorf(&amp;quot;failed to create kubelet, pod source config was nil&amp;quot;)
    }
    podCfg := kubeDeps.PodConfig

    if err := rlimit.SetNumFiles(uint64(kubeServer.MaxOpenFiles)); err != nil {
        klog.Errorf(&amp;quot;Failed to set rlimit on max file handles: %v&amp;quot;, err)
    }

    // process pods and exit.
    if runOnce {
        if _, err := k.RunOnce(podCfg.Updates()); err != nil {
            return fmt.Errorf(&amp;quot;runonce failed: %v&amp;quot;, err)
        }
        klog.Info(&amp;quot;Started kubelet as runonce&amp;quot;)
    } else {
        startKubelet(k, podCfg, &amp;amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableCAdvisorJSONEndpoints, kubeServer.EnableServer)
        klog.Info(&amp;quot;Started kubelet&amp;quot;)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RunKubelet方法最重要的方法有两个：CreateAndInitKubelet和startKubelet，可以理解为CreateAndInitKubelet为参数的配置，startKubelet为最终的启动，其实最后说来说去还是把参数封装一遍，重新构造新的结构体来运行。&lt;/p&gt;

&lt;p&gt;CreateAndInitKubelet方法通过调用NewMainKubelet返回Kubelet结构体。在NewMainKubelet中，主要的配置有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PodConfig。通过makePodSourceConfig可以发现kubelet获取Pod的来源有以下途径：静态Pod、静态Pod的URL地址以及kube-apiserver；&lt;/li&gt;
&lt;li&gt;容器与镜像的GC参数。&lt;/li&gt;
&lt;li&gt;驱逐Pod策略。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最终通过参数填充Kubelet结构体，完成kubelet结构体参数的最终配置。&lt;/p&gt;

&lt;p&gt;然后就是启动startKubelet，在启动之前，判断是以后台daemon进程一直运行还是只启动一次，即runOnce，基本上都是以后台daemon启动的方式，所以大部分调用的是startKubelet方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableCAdvisorJSONEndpoints, enableServer bool) {
    // start the kubelet
    go k.Run(podCfg.Updates())

    // start the kubelet server
    if enableServer {
        go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth,
            enableCAdvisorJSONEndpoints, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling, kubeCfg.EnableSystemLogHandler)

    }
    if kubeCfg.ReadOnlyPort &amp;gt; 0 {
        go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort), enableCAdvisorJSONEndpoints)
    }
    if utilfeature.DefaultFeatureGate.Enabled(features.KubeletPodResources) {
        go k.ListenAndServePodResources()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见开来goroutine，调用run方法，上面构建的是kubelet结构体，所以这边调用的也是kubelet的run方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Run starts the kubelet reacting to config updates
func (kl *Kubelet) Run(updates &amp;lt;-chan kubetypes.PodUpdate) {
    if kl.logServer == nil {
        kl.logServer = http.StripPrefix(&amp;quot;/logs/&amp;quot;, http.FileServer(http.Dir(&amp;quot;/var/log/&amp;quot;)))
    }
    if kl.kubeClient == nil {
        klog.Warning(&amp;quot;No api server defined - no node status update will be sent.&amp;quot;)
    }

    // Start the cloud provider sync manager
    if kl.cloudResourceSyncManager != nil {
        go kl.cloudResourceSyncManager.Run(wait.NeverStop)
    }

    if err := kl.initializeModules(); err != nil {
        kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())
        klog.Fatal(err)
    }

    // Start volume manager
    go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)

    if kl.kubeClient != nil {
        // Start syncing node status immediately, this may set up things the runtime needs to run.
        go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)
        go kl.fastStatusUpdateOnce()

        // start syncing lease
        go kl.nodeLeaseController.Run(wait.NeverStop)
    }
    go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)

    // Set up iptables util rules
    if kl.makeIPTablesUtilChains {
        kl.initNetworkUtil()
    }

    // Start a goroutine responsible for killing pods (that are not properly
    // handled by pod workers).
    go wait.Until(kl.podKiller.PerformPodKillingWork, 1*time.Second, wait.NeverStop)

    // Start component sync loops.
    kl.statusManager.Start()
    kl.probeManager.Start()

    // Start syncing RuntimeClasses if enabled.
    if kl.runtimeClassManager != nil {
        kl.runtimeClassManager.Start(wait.NeverStop)
    }

    // Start the pod lifecycle event generator.
    kl.pleg.Start()
    kl.syncLoop(updates, kl)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到这边是kubelet的调度核心，在该方法内，通过多个goroutine完成最终的kubelet的任务：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;volumeManager，volume相关管理；&lt;/li&gt;
&lt;li&gt;syncNodeStatus，定时同步Node状态；&lt;/li&gt;
&lt;li&gt;updateRuntimeUp，定时更新Runtime状态；&lt;/li&gt;
&lt;li&gt;syncNetworkUtil，定时同步网络状态；&lt;/li&gt;
&lt;li&gt;podKiller，定时清理死亡的pod；&lt;/li&gt;
&lt;li&gt;statusManager，pod状态管理；&lt;/li&gt;
&lt;li&gt;probeManager，pod探针管理；&lt;/li&gt;
&lt;li&gt;启动PLEG；&lt;/li&gt;
&lt;li&gt;syncLoop，最重要的主进程，不停监听外部数据的变化执行pod的相应操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;至此，kubelet启动过程完成。启动主要完成的任务就是参数的配置和多个任务的启动，通过构造一个循环进程不停监听外部事件的变化，执行对应的pod处理工作，这也就是kubelet所需要负责的任务。&lt;/p&gt;

&lt;h2 id=&#34;pod启动流程&#34;&gt;Pod启动流程&lt;/h2&gt;

&lt;p&gt;在上面主进程syncLoop中，不停监听外部数据的变化执行pod的相应操作，那么如何启动pod的，我们来看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (kl *Kubelet) syncLoop(updates &amp;lt;-chan kubetypes.PodUpdate, handler SyncHandler) {
    klog.Info(&amp;quot;Starting kubelet main sync loop.&amp;quot;)
    // The syncTicker wakes up kubelet to checks if there are any pod workers
    // that need to be sync&#39;d. A one-second period is sufficient because the
    // sync interval is defaulted to 10s.
    syncTicker := time.NewTicker(time.Second)
    defer syncTicker.Stop()
    housekeepingTicker := time.NewTicker(housekeepingPeriod)
    defer housekeepingTicker.Stop()
    plegCh := kl.pleg.Watch()
    const (
        base   = 100 * time.Millisecond
        max    = 5 * time.Second
        factor = 2
    )
    duration := base
    // Responsible for checking limits in resolv.conf
    // The limits do not have anything to do with individual pods
    // Since this is called in syncLoop, we don&#39;t need to call it anywhere else
    if kl.dnsConfigurer != nil &amp;amp;&amp;amp; kl.dnsConfigurer.ResolverConfig != &amp;quot;&amp;quot; {
        kl.dnsConfigurer.CheckLimitsForResolvConf()
    }

    for {
        if err := kl.runtimeState.runtimeErrors(); err != nil {
            klog.Errorf(&amp;quot;skipping pod synchronization - %v&amp;quot;, err)
            // exponential backoff
            time.Sleep(duration)
            duration = time.Duration(math.Min(float64(max), factor*float64(duration)))
            continue
        }
        // reset backoff if we have a success
        duration = base

        kl.syncLoopMonitor.Store(kl.clock.Now())
        if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {
            break
        }
        kl.syncLoopMonitor.Store(kl.clock.Now())
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心处理函数syncLoopIteration，它有五个参数&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;configCh：获取Pod信息的channel，关于Pod相关的事件都从该channel获取；&lt;/li&gt;
&lt;li&gt;handler：处理Pod的handler；&lt;/li&gt;
&lt;li&gt;syncCh：同步所有等待同步的Pod；&lt;/li&gt;
&lt;li&gt;houseKeepingCh：清理Pod的channel；&lt;/li&gt;
&lt;li&gt;plegCh：获取PLEG信息，同步Pod。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过select判断某个channel获取到信息，处理相应的操作。Pod的启动显然与configCh相关。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (kl *Kubelet) syncLoopIteration(configCh &amp;lt;-chan kubetypes.PodUpdate, handler SyncHandler,
    syncCh &amp;lt;-chan time.Time, housekeepingCh &amp;lt;-chan time.Time, plegCh &amp;lt;-chan *pleg.PodLifecycleEvent) bool {
    select {
    case u, open := &amp;lt;-configCh:
        // Update from a config source; dispatch it to the right handler
        // callback.
        if !open {
            klog.Errorf(&amp;quot;Update channel is closed. Exiting the sync loop.&amp;quot;)
            return false
        }

        switch u.Op {
        case kubetypes.ADD:
            klog.V(2).Infof(&amp;quot;SyncLoop (ADD, %q): %q&amp;quot;, u.Source, format.Pods(u.Pods))
            // After restarting, kubelet will get all existing pods through
            // ADD as if they are new pods. These pods will then go through the
            // admission process and *may* be rejected. This can be resolved
            // once we have checkpointing.
            handler.HandlePodAdditions(u.Pods)
        case kubetypes.UPDATE:
            klog.V(2).Infof(&amp;quot;SyncLoop (UPDATE, %q): %q&amp;quot;, u.Source, format.PodsWithDeletionTimestamps(u.Pods))
            handler.HandlePodUpdates(u.Pods)
        case kubetypes.REMOVE:
            klog.V(2).Infof(&amp;quot;SyncLoop (REMOVE, %q): %q&amp;quot;, u.Source, format.Pods(u.Pods))
            handler.HandlePodRemoves(u.Pods)
        case kubetypes.RECONCILE:
            klog.V(4).Infof(&amp;quot;SyncLoop (RECONCILE, %q): %q&amp;quot;, u.Source, format.Pods(u.Pods))
            handler.HandlePodReconcile(u.Pods)
        case kubetypes.DELETE:
            klog.V(2).Infof(&amp;quot;SyncLoop (DELETE, %q): %q&amp;quot;, u.Source, format.Pods(u.Pods))
            // DELETE is treated as a UPDATE because of graceful deletion.
            handler.HandlePodUpdates(u.Pods)
        case kubetypes.SET:
            // TODO: Do we want to support this?
            klog.Errorf(&amp;quot;Kubelet does not support snapshot update&amp;quot;)
        default:
            klog.Errorf(&amp;quot;Invalid event type received: %d.&amp;quot;, u.Op)
        }

        kl.sourcesReady.AddSource(u.Source)

    case e := &amp;lt;-plegCh:
        if e.Type == pleg.ContainerStarted {
            // record the most recent time we observed a container start for this pod.
            // this lets us selectively invalidate the runtimeCache when processing a delete for this pod
            // to make sure we don&#39;t miss handling graceful termination for containers we reported as having started.
            kl.lastContainerStartedTime.Add(e.ID, time.Now())
        }
        if isSyncPodWorthy(e) {
            // PLEG event for a pod; sync it.
            if pod, ok := kl.podManager.GetPodByUID(e.ID); ok {
                klog.V(2).Infof(&amp;quot;SyncLoop (PLEG): %q, event: %#v&amp;quot;, format.Pod(pod), e)
                handler.HandlePodSyncs([]*v1.Pod{pod})
            } else {
                // If the pod no longer exists, ignore the event.
                klog.V(4).Infof(&amp;quot;SyncLoop (PLEG): ignore irrelevant event: %#v&amp;quot;, e)
            }
        }

        if e.Type == pleg.ContainerDied {
            if containerID, ok := e.Data.(string); ok {
                kl.cleanUpContainersInPod(e.ID, containerID)
            }
        }
    case &amp;lt;-syncCh:
        // Sync pods waiting for sync
        podsToSync := kl.getPodsToSync()
        if len(podsToSync) == 0 {
            break
        }
        klog.V(4).Infof(&amp;quot;SyncLoop (SYNC): %d pods; %s&amp;quot;, len(podsToSync), format.Pods(podsToSync))
        handler.HandlePodSyncs(podsToSync)
    case update := &amp;lt;-kl.livenessManager.Updates():
        if update.Result == proberesults.Failure {
            // The liveness manager detected a failure; sync the pod.

            // We should not use the pod from livenessManager, because it is never updated after
            // initialization.
            pod, ok := kl.podManager.GetPodByUID(update.PodUID)
            if !ok {
                // If the pod no longer exists, ignore the update.
                klog.V(4).Infof(&amp;quot;SyncLoop (container unhealthy): ignore irrelevant update: %#v&amp;quot;, update)
                break
            }
            klog.V(1).Infof(&amp;quot;SyncLoop (container unhealthy): %q&amp;quot;, format.Pod(pod))
            handler.HandlePodSyncs([]*v1.Pod{pod})
        }
    case &amp;lt;-housekeepingCh:
        if !kl.sourcesReady.AllReady() {
            // If the sources aren&#39;t ready or volume manager has not yet synced the states,
            // skip housekeeping, as we may accidentally delete pods from unready sources.
            klog.V(4).Infof(&amp;quot;SyncLoop (housekeeping, skipped): sources aren&#39;t ready yet.&amp;quot;)
        } else {
            klog.V(4).Infof(&amp;quot;SyncLoop (housekeeping)&amp;quot;)
            if err := handler.HandlePodCleanups(); err != nil {
                klog.Errorf(&amp;quot;Failed cleaning pods: %v&amp;quot;, err)
            }
        }
    }
    return true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出重configCh获取的pod的信息包含操作，不同的操作有这边不同的处理函数，关于操作和函数的定义如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    // SET is the current pod configuration.
    SET PodOperation = iota
    // ADD signifies pods that are new to this source.
    ADD
    // DELETE signifies pods that are gracefully deleted from this source.
    DELETE
    // REMOVE signifies pods that have been removed from this source.
    REMOVE
    // UPDATE signifies pods have been updated in this source.
    UPDATE
    // RECONCILE signifies pods that have unexpected status in this source,
    // kubelet should reconcile status with this source.
    RECONCILE
)

// SyncHandler is an interface implemented by Kubelet, for testability
type SyncHandler interface {
    HandlePodAdditions(pods []*v1.Pod)
    HandlePodUpdates(pods []*v1.Pod)
    HandlePodRemoves(pods []*v1.Pod)
    HandlePodReconcile(pods []*v1.Pod)
    HandlePodSyncs(pods []*v1.Pod)
    HandlePodCleanups() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建pod就应该是ADD操作，其对应的处理方法为HandlePodAdditions，我们handler其实传递的就是kubelet的结构体，所以其实就是kubelet的HandlePodAdditions函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {
    start := kl.clock.Now()
    sort.Sort(sliceutils.PodsByCreationTime(pods))
    for _, pod := range pods {
        existingPods := kl.podManager.GetPods()
        // Always add the pod to the pod manager. Kubelet relies on the pod
        // manager as the source of truth for the desired state. If a pod does
        // not exist in the pod manager, it means that it has been deleted in
        // the apiserver and no action (other than cleanup) is required.
        kl.podManager.AddPod(pod)

        if kubetypes.IsMirrorPod(pod) {
            kl.handleMirrorPod(pod, start)
            continue
        }

        if !kl.podIsTerminated(pod) {
            // Only go through the admission process if the pod is not
            // terminated.

            // We failed pods that we rejected, so activePods include all admitted
            // pods that are alive.
            activePods := kl.filterOutTerminatedPods(existingPods)

            // Check if we can admit the pod; if not, reject it.
            if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok {
                kl.rejectPod(pod, reason, message)
                continue
            }
        }
        mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)
        kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)
        kl.probeManager.AddPod(pod)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要以下几个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;根据Pod的创建时间对Pod进行排序；&lt;/li&gt;
&lt;li&gt;podManager添加Pod；（对Pod的管理依赖于podManager）&lt;/li&gt;
&lt;li&gt;处理mirrorPod，即静态Pod的处理；&lt;/li&gt;
&lt;li&gt;通过dispatchWork方法分发任务，处理Pod的创建；&lt;/li&gt;
&lt;li&gt;probeManager添加Pod。（readiness和liveness探针）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心就是dispatchWork做的，它调用了kl.podWorkers.UpdatePod方法对Pod进行创建。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) {
    // check whether we are ready to delete the pod from the API server (all status up to date)
    containersTerminal, podWorkerTerminal := kl.podAndContainersAreTerminal(pod)
    if pod.DeletionTimestamp != nil &amp;amp;&amp;amp; containersTerminal {
        klog.V(4).Infof(&amp;quot;Pod %q has completed execution and should be deleted from the API server: %s&amp;quot;, format.Pod(pod), syncType)
        kl.statusManager.TerminatePod(pod)
        return
    }

    // optimization: avoid invoking the pod worker if no further changes are possible to the pod definition
    if podWorkerTerminal {
        klog.V(4).Infof(&amp;quot;Pod %q has completed, ignoring remaining sync work: %s&amp;quot;, format.Pod(pod), syncType)
        return
    }

    // Run the sync in an async worker.
    kl.podWorkers.UpdatePod(&amp;amp;UpdatePodOptions{
        Pod:        pod,
        MirrorPod:  mirrorPod,
        UpdateType: syncType,
        OnCompleteFunc: func(err error) {
            if err != nil {
                metrics.PodWorkerDuration.WithLabelValues(syncType.String()).Observe(metrics.SinceInSeconds(start))
            }
        },
    })
    // Note the number of containers for new pods.
    if syncType == kubetypes.SyncPodCreate {
        metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))
    }
}

func (p *podWorkers) UpdatePod(options *UpdatePodOptions) {
    pod := options.Pod
    uid := pod.UID
    var podUpdates chan UpdatePodOptions
    var exists bool

    p.podLock.Lock()
    defer p.podLock.Unlock()
    if podUpdates, exists = p.podUpdates[uid]; !exists {
        // We need to have a buffer here, because checkForUpdates() method that
        // puts an update into channel is called from the same goroutine where
        // the channel is consumed. However, it is guaranteed that in such case
        // the channel is empty, so buffer of size 1 is enough.
        podUpdates = make(chan UpdatePodOptions, 1)
        p.podUpdates[uid] = podUpdates

        // Creating a new pod worker either means this is a new pod, or that the
        // kubelet just restarted. In either case the kubelet is willing to believe
        // the status of the pod for the first pod worker sync. See corresponding
        // comment in syncPod.
        go func() {
            defer runtime.HandleCrash()
            p.managePodLoop(podUpdates)
        }()
    }
    if !p.isWorking[pod.UID] {
        p.isWorking[pod.UID] = true
        podUpdates &amp;lt;- *options
    } else {
        // if a request to kill a pod is pending, we do not let anything overwrite that request.
        update, found := p.lastUndeliveredWorkUpdate[pod.UID]
        if !found || update.UpdateType != kubetypes.SyncPodKill {
            p.lastUndeliveredWorkUpdate[pod.UID] = *options
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;UpdatePod方法通过podUpdates的map类型获取相对应的Pod，map的key为Pod的UID，value为UpdatePodOptions的结构体channel。通过获取到需要创建的Pod之后，单独起一个goroutine调用managePodLoop方法完成Pod的创建，managePodLoop方法最终调用syncPodFn完成Pod的创建，syncPodFn对应的就是Kubelet的syncPod方法，位于kubernetes/pkg/kubelet/kubelet.go下。经过层层环绕，syncPod就是最终处理Pod创建的方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *podWorkers) managePodLoop(podUpdates &amp;lt;-chan UpdatePodOptions) {
    var lastSyncTime time.Time
    for update := range podUpdates {
        err := func() error {
            podUID := update.Pod.UID
            // This is a blocking call that would return only if the cache
            // has an entry for the pod that is newer than minRuntimeCache
            // Time. This ensures the worker doesn&#39;t start syncing until
            // after the cache is at least newer than the finished time of
            // the previous sync.
            status, err := p.podCache.GetNewerThan(podUID, lastSyncTime)
            if err != nil {
                // This is the legacy event thrown by manage pod loop
                // all other events are now dispatched from syncPodFn
                p.recorder.Eventf(update.Pod, v1.EventTypeWarning, events.FailedSync, &amp;quot;error determining status: %v&amp;quot;, err)
                return err
            }
            err = p.syncPodFn(syncPodOptions{
                mirrorPod:      update.MirrorPod,
                pod:            update.Pod,
                podStatus:      status,
                killPodOptions: update.KillPodOptions,
                updateType:     update.UpdateType,
            })
            lastSyncTime = time.Now()
            return err
        }()
        // notify the call-back function if the operation succeeded or not
        if update.OnCompleteFunc != nil {
            update.OnCompleteFunc(err)
        }
        if err != nil {
            // IMPORTANT: we do not log errors here, the syncPodFn is responsible for logging errors
            klog.Errorf(&amp;quot;Error syncing pod %s (%q), skipping: %v&amp;quot;, update.Pod.UID, format.Pod(update.Pod), err)
        }
        p.wrapUp(update.Pod.UID, err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;syncPod主要的工作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (kl *Kubelet) syncPod(o syncPodOptions) error {
    // pull out the required options
    pod := o.pod
    mirrorPod := o.mirrorPod
    podStatus := o.podStatus
    updateType := o.updateType

    // if we want to kill a pod, do it now!
    if updateType == kubetypes.SyncPodKill {
        killPodOptions := o.killPodOptions
        if killPodOptions == nil || killPodOptions.PodStatusFunc == nil {
            return fmt.Errorf(&amp;quot;kill pod options are required if update type is kill&amp;quot;)
        }
        apiPodStatus := killPodOptions.PodStatusFunc(pod, podStatus)
        kl.statusManager.SetPodStatus(pod, apiPodStatus)
        // we kill the pod with the specified grace period since this is a termination
        if err := kl.killPod(pod, nil, podStatus, killPodOptions.PodTerminationGracePeriodSecondsOverride); err != nil {
            kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, &amp;quot;error killing pod: %v&amp;quot;, err)
            // there was an error killing the pod, so we return that error directly
            utilruntime.HandleError(err)
            return err
        }
        return nil
    }

    // If the pod is a static pod and its mirror pod is still gracefully terminating,
    // we do not want to start the new static pod until the old static pod is gracefully terminated.
    podFullName := kubecontainer.GetPodFullName(pod)
    if kl.podKiller.IsMirrorPodPendingTerminationByPodName(podFullName) {
        return fmt.Errorf(&amp;quot;pod %q is pending termination&amp;quot;, podFullName)
    }

    // Latency measurements for the main workflow are relative to the
    // first time the pod was seen by the API server.
    var firstSeenTime time.Time
    if firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok {
        firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get()
    }

    // Record pod worker start latency if being created
    // TODO: make pod workers record their own latencies
    if updateType == kubetypes.SyncPodCreate {
        if !firstSeenTime.IsZero() {
            // This is the first time we are syncing the pod. Record the latency
            // since kubelet first saw the pod if firstSeenTime is set.
            metrics.PodWorkerStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime))
        } else {
            klog.V(3).Infof(&amp;quot;First seen time not recorded for pod %q&amp;quot;, pod.UID)
        }
    }

    // Generate final API pod status with pod and status manager status
    apiPodStatus := kl.generateAPIPodStatus(pod, podStatus)
    // The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576)
    // TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and
    // set pod IP to hostIP directly in runtime.GetPodStatus
    podStatus.IPs = make([]string, 0, len(apiPodStatus.PodIPs))
    for _, ipInfo := range apiPodStatus.PodIPs {
        podStatus.IPs = append(podStatus.IPs, ipInfo.IP)
    }

    if len(podStatus.IPs) == 0 &amp;amp;&amp;amp; len(apiPodStatus.PodIP) &amp;gt; 0 {
        podStatus.IPs = []string{apiPodStatus.PodIP}
    }

    // Record the time it takes for the pod to become running.
    existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID)
    if !ok || existingStatus.Phase == v1.PodPending &amp;amp;&amp;amp; apiPodStatus.Phase == v1.PodRunning &amp;amp;&amp;amp;
        !firstSeenTime.IsZero() {
        metrics.PodStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime))
    }

    runnable := kl.canRunPod(pod)
    if !runnable.Admit {
        // Pod is not runnable; update the Pod and Container statuses to why.
        apiPodStatus.Reason = runnable.Reason
        apiPodStatus.Message = runnable.Message
        // Waiting containers are not creating.
        const waitingReason = &amp;quot;Blocked&amp;quot;
        for _, cs := range apiPodStatus.InitContainerStatuses {
            if cs.State.Waiting != nil {
                cs.State.Waiting.Reason = waitingReason
            }
        }
        for _, cs := range apiPodStatus.ContainerStatuses {
            if cs.State.Waiting != nil {
                cs.State.Waiting.Reason = waitingReason
            }
        }
    }

    // Update status in the status manager
    kl.statusManager.SetPodStatus(pod, apiPodStatus)

    // Kill pod if it should not be running
    if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed {
        var syncErr error
        if err := kl.killPod(pod, nil, podStatus, nil); err != nil {
            kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, &amp;quot;error killing pod: %v&amp;quot;, err)
            syncErr = fmt.Errorf(&amp;quot;error killing pod: %v&amp;quot;, err)
            utilruntime.HandleError(syncErr)
        } else {
            if !runnable.Admit {
                // There was no error killing the pod, but the pod cannot be run.
                // Return an error to signal that the sync loop should back off.
                syncErr = fmt.Errorf(&amp;quot;pod cannot be run: %s&amp;quot;, runnable.Message)
            }
        }
        return syncErr
    }

    // If the network plugin is not ready, only start the pod if it uses the host network
    if err := kl.runtimeState.networkErrors(); err != nil &amp;amp;&amp;amp; !kubecontainer.IsHostNetworkPod(pod) {
        kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, &amp;quot;%s: %v&amp;quot;, NetworkNotReadyErrorMsg, err)
        return fmt.Errorf(&amp;quot;%s: %v&amp;quot;, NetworkNotReadyErrorMsg, err)
    }

    // Create Cgroups for the pod and apply resource parameters
    // to them if cgroups-per-qos flag is enabled.
    pcm := kl.containerManager.NewPodContainerManager()
    // If pod has already been terminated then we need not create
    // or update the pod&#39;s cgroup
    if !kl.podIsTerminated(pod) {
        // When the kubelet is restarted with the cgroups-per-qos
        // flag enabled, all the pod&#39;s running containers
        // should be killed intermittently and brought back up
        // under the qos cgroup hierarchy.
        // Check if this is the pod&#39;s first sync
        firstSync := true
        for _, containerStatus := range apiPodStatus.ContainerStatuses {
            if containerStatus.State.Running != nil {
                firstSync = false
                break
            }
        }
        // Don&#39;t kill containers in pod if pod&#39;s cgroups already
        // exists or the pod is running for the first time
        podKilled := false
        if !pcm.Exists(pod) &amp;amp;&amp;amp; !firstSync {
            if err := kl.killPod(pod, nil, podStatus, nil); err == nil {
                podKilled = true
            } else {
                klog.Errorf(&amp;quot;killPod for pod %q (podStatus=%v) failed: %v&amp;quot;, format.Pod(pod), podStatus, err)
            }
        }
        // Create and Update pod&#39;s Cgroups
        // Don&#39;t create cgroups for run once pod if it was killed above
        // The current policy is not to restart the run once pods when
        // the kubelet is restarted with the new flag as run once pods are
        // expected to run only once and if the kubelet is restarted then
        // they are not expected to run again.
        // We don&#39;t create and apply updates to cgroup if its a run once pod and was killed above
        if !(podKilled &amp;amp;&amp;amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) {
            if !pcm.Exists(pod) {
                if err := kl.containerManager.UpdateQOSCgroups(); err != nil {
                    klog.V(2).Infof(&amp;quot;Failed to update QoS cgroups while syncing pod: %v&amp;quot;, err)
                }
                if err := pcm.EnsureExists(pod); err != nil {
                    kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, &amp;quot;unable to ensure pod container exists: %v&amp;quot;, err)
                    return fmt.Errorf(&amp;quot;failed to ensure that the pod: %v cgroups exist and are correctly applied: %v&amp;quot;, pod.UID, err)
                }
            }
        }
    }

    // Create Mirror Pod for Static Pod if it doesn&#39;t already exist
    if kubetypes.IsStaticPod(pod) {
        deleted := false
        if mirrorPod != nil {
            if mirrorPod.DeletionTimestamp != nil || !kl.podManager.IsMirrorPodOf(mirrorPod, pod) {
                // The mirror pod is semantically different from the static pod. Remove
                // it. The mirror pod will get recreated later.
                klog.Infof(&amp;quot;Trying to delete pod %s %v&amp;quot;, podFullName, mirrorPod.ObjectMeta.UID)
                var err error
                deleted, err = kl.podManager.DeleteMirrorPod(podFullName, &amp;amp;mirrorPod.ObjectMeta.UID)
                if deleted {
                    klog.Warningf(&amp;quot;Deleted mirror pod %q because it is outdated&amp;quot;, format.Pod(mirrorPod))
                } else if err != nil {
                    klog.Errorf(&amp;quot;Failed deleting mirror pod %q: %v&amp;quot;, format.Pod(mirrorPod), err)
                }
            }
        }
        if mirrorPod == nil || deleted {
            node, err := kl.GetNode()
            if err != nil || node.DeletionTimestamp != nil {
                klog.V(4).Infof(&amp;quot;No need to create a mirror pod, since node %q has been removed from the cluster&amp;quot;, kl.nodeName)
            } else {
                klog.V(4).Infof(&amp;quot;Creating a mirror pod for static pod %q&amp;quot;, format.Pod(pod))
                if err := kl.podManager.CreateMirrorPod(pod); err != nil {
                    klog.Errorf(&amp;quot;Failed creating a mirror pod for %q: %v&amp;quot;, format.Pod(pod), err)
                }
            }
        }
    }

    // Make data directories for the pod
    if err := kl.makePodDataDirs(pod); err != nil {
        kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, &amp;quot;error making pod data directories: %v&amp;quot;, err)
        klog.Errorf(&amp;quot;Unable to make pod data directories for pod %q: %v&amp;quot;, format.Pod(pod), err)
        return err
    }

    // Volume manager will not mount volumes for terminated pods
    if !kl.podIsTerminated(pod) {
        // Wait for volumes to attach/mount
        if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {
            kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, &amp;quot;Unable to attach or mount volumes: %v&amp;quot;, err)
            klog.Errorf(&amp;quot;Unable to attach or mount volumes for pod %q: %v; skipping pod&amp;quot;, format.Pod(pod), err)
            return err
        }
    }

    // Fetch the pull secrets for the pod
    pullSecrets := kl.getPullSecretsForPod(pod)

    // Call the container runtime&#39;s SyncPod callback
    result := kl.containerRuntime.SyncPod(pod, podStatus, pullSecrets, kl.backOff)
    kl.reasonCache.Update(pod.UID, result)
    if err := result.Error(); err != nil {
        // Do not return error if the only failures were pods in backoff
        for _, r := range result.SyncResults {
            if r.Error != kubecontainer.ErrCrashLoopBackOff &amp;amp;&amp;amp; r.Error != images.ErrImagePullBackOff {
                // Do not record an event here, as we keep all event logging for sync pod failures
                // local to container runtime so we get better errors
                return err
            }
        }

        return nil
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码主要做了如下事情&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;更新Pod的状态，对应generateAPIPodStatus和statusManager.SetPodStatus方法；&lt;/li&gt;
&lt;li&gt;创建Pod存储的目录，对应makePodDataDirs方法；&lt;/li&gt;
&lt;li&gt;挂载对应的volume，对应volumeManager.WaitForAttachAndMount方法；&lt;/li&gt;
&lt;li&gt;获取ImagePullSecrets，对应getPullSecretsForPod方法；&lt;/li&gt;
&lt;li&gt;创建容器，对应containerRuntime.SyncPod方法&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;创建容器containerRuntime.SyncPod方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {
    // Step 1: Compute sandbox and container changes.
    podContainerChanges := m.computePodActions(pod, podStatus)
    klog.V(3).Infof(&amp;quot;computePodActions got %+v for pod %q&amp;quot;, podContainerChanges, format.Pod(pod))
    if podContainerChanges.CreateSandbox {
        ref, err := ref.GetReference(legacyscheme.Scheme, pod)
        if err != nil {
            klog.Errorf(&amp;quot;Couldn&#39;t make a ref to pod %q: &#39;%v&#39;&amp;quot;, format.Pod(pod), err)
        }
        if podContainerChanges.SandboxID != &amp;quot;&amp;quot; {
            m.recorder.Eventf(ref, v1.EventTypeNormal, events.SandboxChanged, &amp;quot;Pod sandbox changed, it will be killed and re-created.&amp;quot;)
        } else {
            klog.V(4).Infof(&amp;quot;SyncPod received new pod %q, will create a sandbox for it&amp;quot;, format.Pod(pod))
        }
    }

    // Step 2: Kill the pod if the sandbox has changed.
    if podContainerChanges.KillPod {
        if podContainerChanges.CreateSandbox {
            klog.V(4).Infof(&amp;quot;Stopping PodSandbox for %q, will start new one&amp;quot;, format.Pod(pod))
        } else {
            klog.V(4).Infof(&amp;quot;Stopping PodSandbox for %q because all other containers are dead.&amp;quot;, format.Pod(pod))
        }

        killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil)
        result.AddPodSyncResult(killResult)
        if killResult.Error() != nil {
            klog.Errorf(&amp;quot;killPodWithSyncResult failed: %v&amp;quot;, killResult.Error())
            return
        }

        if podContainerChanges.CreateSandbox {
            m.purgeInitContainers(pod, podStatus)
        }
    } else {
        // Step 3: kill any running containers in this pod which are not to keep.
        for containerID, containerInfo := range podContainerChanges.ContainersToKill {
            klog.V(3).Infof(&amp;quot;Killing unwanted container %q(id=%q) for pod %q&amp;quot;, containerInfo.name, containerID, format.Pod(pod))
            killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name)
            result.AddSyncResult(killContainerResult)
            if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil {
                killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error())
                klog.Errorf(&amp;quot;killContainer %q(id=%q) for pod %q failed: %v&amp;quot;, containerInfo.name, containerID, format.Pod(pod), err)
                return
            }
        }
    }

    // Keep terminated init containers fairly aggressively controlled
    // This is an optimization because container removals are typically handled
    // by container garbage collector.
    m.pruneInitContainersBeforeStart(pod, podStatus)

    // We pass the value of the PRIMARY podIP and list of podIPs down to
    // generatePodSandboxConfig and generateContainerConfig, which in turn
    // passes it to various other functions, in order to facilitate functionality
    // that requires this value (hosts file and downward API) and avoid races determining
    // the pod IP in cases where a container requires restart but the
    // podIP isn&#39;t in the status manager yet. The list of podIPs is used to
    // generate the hosts file.
    //
    // We default to the IPs in the passed-in pod status, and overwrite them if the
    // sandbox needs to be (re)started.
    var podIPs []string
    if podStatus != nil {
        podIPs = podStatus.IPs
    }

    // Step 4: Create a sandbox for the pod if necessary.
    // 创建使用 pause 镜像创建的 sandbox
    podSandboxID := podContainerChanges.SandboxID
    if podContainerChanges.CreateSandbox {
        var msg string
        var err error

        klog.V(4).Infof(&amp;quot;Creating PodSandbox for pod %q&amp;quot;, format.Pod(pod))
        createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod))
        result.AddSyncResult(createSandboxResult)
        podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)
        if err != nil {
            createSandboxResult.Fail(kubecontainer.ErrCreatePodSandbox, msg)
            klog.Errorf(&amp;quot;createPodSandbox for pod %q failed: %v&amp;quot;, format.Pod(pod), err)
            ref, referr := ref.GetReference(legacyscheme.Scheme, pod)
            if referr != nil {
                klog.Errorf(&amp;quot;Couldn&#39;t make a ref to pod %q: &#39;%v&#39;&amp;quot;, format.Pod(pod), referr)
            }
            m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedCreatePodSandBox, &amp;quot;Failed to create pod sandbox: %v&amp;quot;, err)
            return
        }
        klog.V(4).Infof(&amp;quot;Created PodSandbox %q for pod %q&amp;quot;, podSandboxID, format.Pod(pod))

        podSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)
        if err != nil {
            ref, referr := ref.GetReference(legacyscheme.Scheme, pod)
            if referr != nil {
                klog.Errorf(&amp;quot;Couldn&#39;t make a ref to pod %q: &#39;%v&#39;&amp;quot;, format.Pod(pod), referr)
            }
            m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedStatusPodSandBox, &amp;quot;Unable to get pod sandbox status: %v&amp;quot;, err)
            klog.Errorf(&amp;quot;Failed to get pod sandbox status: %v; Skipping pod %q&amp;quot;, err, format.Pod(pod))
            result.Fail(err)
            return
        }

        // If we ever allow updating a pod from non-host-network to
        // host-network, we may use a stale IP.
        if !kubecontainer.IsHostNetworkPod(pod) {
            // Overwrite the podIPs passed in the pod status, since we just started the pod sandbox.
            podIPs = m.determinePodSandboxIPs(pod.Namespace, pod.Name, podSandboxStatus)
            klog.V(4).Infof(&amp;quot;Determined the ip %v for pod %q after sandbox changed&amp;quot;, podIPs, format.Pod(pod))
        }
    }

    // the start containers routines depend on pod ip(as in primary pod ip)
    // instead of trying to figure out if we have 0 &amp;lt; len(podIPs)
    // everytime, we short circuit it here
    podIP := &amp;quot;&amp;quot;
    if len(podIPs) != 0 {
        podIP = podIPs[0]
    }

    // Get podSandboxConfig for containers to start.
    configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)
    result.AddSyncResult(configPodSandboxResult)
    podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)
    if err != nil {
        message := fmt.Sprintf(&amp;quot;GeneratePodSandboxConfig for pod %q failed: %v&amp;quot;, format.Pod(pod), err)
        klog.Error(message)
        configPodSandboxResult.Fail(kubecontainer.ErrConfigPodSandbox, message)
        return
    }

    // Helper containing boilerplate common to starting all types of containers.
    // typeName is a label used to describe this type of container in log messages,
    // currently: &amp;quot;container&amp;quot;, &amp;quot;init container&amp;quot; or &amp;quot;ephemeral container&amp;quot;
    start := func(typeName string, spec *startSpec) error {
        startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, spec.container.Name)
        result.AddSyncResult(startContainerResult)

        isInBackOff, msg, err := m.doBackOff(pod, spec.container, podStatus, backOff)
        if isInBackOff {
            startContainerResult.Fail(err, msg)
            klog.V(4).Infof(&amp;quot;Backing Off restarting %v %+v in pod %v&amp;quot;, typeName, spec.container, format.Pod(pod))
            return err
        }

        klog.V(4).Infof(&amp;quot;Creating %v %+v in pod %v&amp;quot;, typeName, spec.container, format.Pod(pod))
        // NOTE (aramase) podIPs are populated for single stack and dual stack clusters. Send only podIPs.
        if msg, err := m.startContainer(podSandboxID, podSandboxConfig, spec, pod, podStatus, pullSecrets, podIP, podIPs); err != nil {
            startContainerResult.Fail(err, msg)
            // known errors that are logged in other places are logged at higher levels here to avoid
            // repetitive log spam
            switch {
            case err == images.ErrImagePullBackOff:
                klog.V(3).Infof(&amp;quot;%v %+v start failed in pod %v: %v: %s&amp;quot;, typeName, spec.container, format.Pod(pod), err, msg)
            default:
                utilruntime.HandleError(fmt.Errorf(&amp;quot;%v %+v start failed in pod %v: %v: %s&amp;quot;, typeName, spec.container, format.Pod(pod), err, msg))
            }
            return err
        }

        return nil
    }

    // Step 5: start ephemeral containers
    // These are started &amp;quot;prior&amp;quot; to init containers to allow running ephemeral containers even when there
    // are errors starting an init container. In practice init containers will start first since ephemeral
    // containers cannot be specified on pod creation.
    if utilfeature.DefaultFeatureGate.Enabled(features.EphemeralContainers) {
        for _, idx := range podContainerChanges.EphemeralContainersToStart {
            start(&amp;quot;ephemeral container&amp;quot;, ephemeralContainerStartSpec(&amp;amp;pod.Spec.EphemeralContainers[idx]))
        }
    }

    // Step 6: start the init container.
    if container := podContainerChanges.NextInitContainerToStart; container != nil {
        // Start the next init container.
        if err := start(&amp;quot;init container&amp;quot;, containerStartSpec(container)); err != nil {
            return
        }

        // Successfully started the container; clear the entry in the failure
        klog.V(4).Infof(&amp;quot;Completed init container %q for pod %q&amp;quot;, container.Name, format.Pod(pod))
    }

    // Step 7: start containers in podContainerChanges.ContainersToStart.
    for _, idx := range podContainerChanges.ContainersToStart {
        start(&amp;quot;container&amp;quot;, containerStartSpec(&amp;amp;pod.Spec.Containers[idx]))
    }

    return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要做了以下的事情&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compute sandbox and container changes.&lt;/li&gt;
&lt;li&gt;Kill pod sandbox if necessary.&lt;/li&gt;
&lt;li&gt;Kill any containers that should not be running.&lt;/li&gt;
&lt;li&gt;Create sandbox if necessary.&lt;/li&gt;
&lt;li&gt;Create ephemeral containers.&lt;/li&gt;
&lt;li&gt;Create init containers.&lt;/li&gt;
&lt;li&gt;Create normal containers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;至此，Pod的启动到创建过程完成。&lt;/p&gt;

&lt;h3 id=&#34;pod创建中的cni网络插件&#34;&gt;pod创建中的CNI网络插件&lt;/h3&gt;

&lt;p&gt;在上面的Create sandbox调用createPodSandbox来创建 pause 镜像创建的 sandbox过程中会调用CRI相关接口来实现容器的运行，其中就包括了网络的设置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// createPodSandbox creates a pod sandbox and returns (podSandBoxID, message, error).
func (m *kubeGenericRuntimeManager) createPodSandbox(pod *v1.Pod, attempt uint32) (string, string, error) {
    podSandboxConfig, err := m.generatePodSandboxConfig(pod, attempt)
    if err != nil {
        message := fmt.Sprintf(&amp;quot;GeneratePodSandboxConfig for pod %q failed: %v&amp;quot;, format.Pod(pod), err)
        klog.Error(message)
        return &amp;quot;&amp;quot;, message, err
    }

    // Create pod logs directory
    err = m.osInterface.MkdirAll(podSandboxConfig.LogDirectory, 0755)
    if err != nil {
        message := fmt.Sprintf(&amp;quot;Create pod log directory for pod %q failed: %v&amp;quot;, format.Pod(pod), err)
        klog.Errorf(message)
        return &amp;quot;&amp;quot;, message, err
    }

    runtimeHandler := &amp;quot;&amp;quot;
    if m.runtimeClassManager != nil {
        runtimeHandler, err = m.runtimeClassManager.LookupRuntimeHandler(pod.Spec.RuntimeClassName)
        if err != nil {
            message := fmt.Sprintf(&amp;quot;CreatePodSandbox for pod %q failed: %v&amp;quot;, format.Pod(pod), err)
            return &amp;quot;&amp;quot;, message, err
        }
        if runtimeHandler != &amp;quot;&amp;quot; {
            klog.V(2).Infof(&amp;quot;Running pod %s with RuntimeHandler %q&amp;quot;, format.Pod(pod), runtimeHandler)
        }
    }

    podSandBoxID, err := m.runtimeService.RunPodSandbox(podSandboxConfig, runtimeHandler)
    if err != nil {
        message := fmt.Sprintf(&amp;quot;CreatePodSandbox for pod %q failed: %v&amp;quot;, format.Pod(pod), err)
        klog.Error(message)
        return &amp;quot;&amp;quot;, message, err
    }

    return podSandBoxID, &amp;quot;&amp;quot;, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见调用来m.runtimeService.RunPodSandbox来创建容器，PodSandboxManager就是基本容器的操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PodSandboxManager interface {
    // RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure
    // the sandbox is in ready state.
    RunPodSandbox(config *runtimeapi.PodSandboxConfig, runtimeHandler string) (string, error)
    // StopPodSandbox stops the sandbox. If there are any running containers in the
    // sandbox, they should be force terminated.
    StopPodSandbox(podSandboxID string) error
    // RemovePodSandbox removes the sandbox. If there are running containers in the
    // sandbox, they should be forcibly removed.
    RemovePodSandbox(podSandboxID string) error
    // PodSandboxStatus returns the Status of the PodSandbox.
    PodSandboxStatus(podSandboxID string) (*runtimeapi.PodSandboxStatus, error)
    // ListPodSandbox returns a list of Sandbox.
    ListPodSandbox(filter *runtimeapi.PodSandboxFilter) ([]*runtimeapi.PodSandbox, error)
    // PortForward prepares a streaming endpoint to forward ports from a PodSandbox, and returns the address.
    PortForward(*runtimeapi.PortForwardRequest) (*runtimeapi.PortForwardResponse, error)
}

type RuntimeService interface {
    RuntimeVersioner
    ContainerManager
    PodSandboxManager
    ContainerStatsManager

    // UpdateRuntimeConfig updates runtime configuration if specified
    UpdateRuntimeConfig(runtimeConfig *runtimeapi.RuntimeConfig) error
    // Status returns the status of the runtime.
    Status() (*runtimeapi.RuntimeStatus, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;dockerService 类实现了上面 CRI 接口，dockerService创建的时候传入的 cni 配置目录和bin目录，初始化插件，并供后续选择，所以我们直接看dockerService的RunPodSandbox函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) {
    config := r.GetConfig()

    // Step 1: Pull the image for the sandbox.
    image := defaultSandboxImage
    podSandboxImage := ds.podSandboxImage
    if len(podSandboxImage) != 0 {
        image = podSandboxImage
    }

    // NOTE: To use a custom sandbox image in a private repository, users need to configure the nodes with credentials properly.
    // see: http://kubernetes.io/docs/user-guide/images/#configuring-nodes-to-authenticate-to-a-private-repository
    // Only pull sandbox image when it&#39;s not present - v1.PullIfNotPresent.
    if err := ensureSandboxImageExists(ds.client, image); err != nil {
        return nil, err
    }

    // Step 2: Create the sandbox container.
    if r.GetRuntimeHandler() != &amp;quot;&amp;quot; &amp;amp;&amp;amp; r.GetRuntimeHandler() != runtimeName {
        return nil, fmt.Errorf(&amp;quot;RuntimeHandler %q not supported&amp;quot;, r.GetRuntimeHandler())
    }
    createConfig, err := ds.makeSandboxDockerConfig(config, image)
    if err != nil {
        return nil, fmt.Errorf(&amp;quot;failed to make sandbox docker config for pod %q: %v&amp;quot;, config.Metadata.Name, err)
    }
    createResp, err := ds.client.CreateContainer(*createConfig)
    if err != nil {
        createResp, err = recoverFromCreationConflictIfNeeded(ds.client, *createConfig, err)
    }

    if err != nil || createResp == nil {
        return nil, fmt.Errorf(&amp;quot;failed to create a sandbox for pod %q: %v&amp;quot;, config.Metadata.Name, err)
    }
    resp := &amp;amp;runtimeapi.RunPodSandboxResponse{PodSandboxId: createResp.ID}

    ds.setNetworkReady(createResp.ID, false)
    defer func(e *error) {
        // Set networking ready depending on the error return of
        // the parent function
        if *e == nil {
            ds.setNetworkReady(createResp.ID, true)
        }
    }(&amp;amp;err)

    // Step 3: Create Sandbox Checkpoint.
    if err = ds.checkpointManager.CreateCheckpoint(createResp.ID, constructPodSandboxCheckpoint(config)); err != nil {
        return nil, err
    }

    // Step 4: Start the sandbox container.
    // Assume kubelet&#39;s garbage collector would remove the sandbox later, if
    // startContainer failed.
    err = ds.client.StartContainer(createResp.ID)
    if err != nil {
        return nil, fmt.Errorf(&amp;quot;failed to start sandbox container for pod %q: %v&amp;quot;, config.Metadata.Name, err)
    }

    // Rewrite resolv.conf file generated by docker.
    // NOTE: cluster dns settings aren&#39;t passed anymore to docker api in all cases,
    // not only for pods with host network: the resolver conf will be overwritten
    // after sandbox creation to override docker&#39;s behaviour. This resolv.conf
    // file is shared by all containers of the same pod, and needs to be modified
    // only once per pod.
    if dnsConfig := config.GetDnsConfig(); dnsConfig != nil {
        containerInfo, err := ds.client.InspectContainer(createResp.ID)
        if err != nil {
            return nil, fmt.Errorf(&amp;quot;failed to inspect sandbox container for pod %q: %v&amp;quot;, config.Metadata.Name, err)
        }

        if err := rewriteResolvFile(containerInfo.ResolvConfPath, dnsConfig.Servers, dnsConfig.Searches, dnsConfig.Options); err != nil {
            return nil, fmt.Errorf(&amp;quot;rewrite resolv.conf failed for pod %q: %v&amp;quot;, config.Metadata.Name, err)
        }
    }

    // Do not invoke network plugins if in hostNetwork mode.
    if config.GetLinux().GetSecurityContext().GetNamespaceOptions().GetNetwork() == runtimeapi.NamespaceMode_NODE {
        return resp, nil
    }

    // Step 5: Setup networking for the sandbox.
    // All pod networking is setup by a CNI plugin discovered at startup time.
    // This plugin assigns the pod ip, sets up routes inside the sandbox,
    // creates interfaces etc. In theory, its jurisdiction ends with pod
    // sandbox networking, but it might insert iptables rules or open ports
    // on the host as well, to satisfy parts of the pod spec that aren&#39;t
    // recognized by the CNI standard yet.
    cID := kubecontainer.BuildContainerID(runtimeName, createResp.ID)
    networkOptions := make(map[string]string)
    if dnsConfig := config.GetDnsConfig(); dnsConfig != nil {
        // Build DNS options.
        dnsOption, err := json.Marshal(dnsConfig)
        if err != nil {
            return nil, fmt.Errorf(&amp;quot;failed to marshal dns config for pod %q: %v&amp;quot;, config.Metadata.Name, err)
        }
        networkOptions[&amp;quot;dns&amp;quot;] = string(dnsOption)
    }
    err = ds.network.SetUpPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID, config.Annotations, networkOptions)
    if err != nil {
        errList := []error{fmt.Errorf(&amp;quot;failed to set up sandbox container %q network for pod %q: %v&amp;quot;, createResp.ID, config.Metadata.Name, err)}

        // Ensure network resources are cleaned up even if the plugin
        // succeeded but an error happened between that success and here.
        err = ds.network.TearDownPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID)
        if err != nil {
            errList = append(errList, fmt.Errorf(&amp;quot;failed to clean up sandbox container %q network for pod %q: %v&amp;quot;, createResp.ID, config.Metadata.Name, err))
        }

        err = ds.client.StopContainer(createResp.ID, defaultSandboxGracePeriod)
        if err != nil {
            errList = append(errList, fmt.Errorf(&amp;quot;failed to stop sandbox container %q for pod %q: %v&amp;quot;, createResp.ID, config.Metadata.Name, err))
        }

        return resp, utilerrors.NewAggregate(errList)
    }

    return resp, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用ds.network.SetUpPod来对容器的网络进行设置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pm *PluginManager) SetUpPod(podNamespace, podName string, id kubecontainer.ContainerID, annotations, options map[string]string) error {
    const operation = &amp;quot;set_up_pod&amp;quot;
    defer recordOperation(operation, time.Now())
    fullPodName := kubecontainer.BuildPodFullName(podName, podNamespace)
    pm.podLock(fullPodName).Lock()
    defer pm.podUnlock(fullPodName)

    klog.V(3).Infof(&amp;quot;Calling network plugin %s to set up pod %q&amp;quot;, pm.plugin.Name(), fullPodName)
    if err := pm.plugin.SetUpPod(podNamespace, podName, id, annotations, options); err != nil {
        recordError(operation)
        return fmt.Errorf(&amp;quot;networkPlugin %s failed to set up pod %q network: %v&amp;quot;, pm.plugin.Name(), fullPodName, err)
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边就调用的插件管理模式的SetUpPod函数。我们配置的时候是cni模式，是有这边就是调用cniNetworkPlugin的SetUpPod函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (plugin *cniNetworkPlugin) SetUpPod(namespace string, name string, id kubecontainer.ContainerID, annotations, options map[string]string) error {
    if err := plugin.checkInitialized(); err != nil {
        return err
    }
    netnsPath, err := plugin.host.GetNetNS(id.ID)
    if err != nil {
        return fmt.Errorf(&amp;quot;CNI failed to retrieve network namespace path: %v&amp;quot;, err)
    }

    // Todo get the timeout from parent ctx
    cniTimeoutCtx, cancelFunc := context.WithTimeout(context.Background(), network.CNITimeoutSec*time.Second)
    defer cancelFunc()
    // Windows doesn&#39;t have loNetwork. It comes only with Linux
    if plugin.loNetwork != nil {
        if _, err = plugin.addToNetwork(cniTimeoutCtx, plugin.loNetwork, name, namespace, id, netnsPath, annotations, options); err != nil {
            return err
        }
    }

    _, err = plugin.addToNetwork(cniTimeoutCtx, plugin.getDefaultNetwork(), name, namespace, id, netnsPath, annotations, options)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就调用插件实现的接口来实现网络。pod创建的时候就是调用的AddNetworkList接口，也就是CNI包中的CNIConfig实现的AddNetworkList接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (plugin *cniNetworkPlugin) addToNetwork(ctx context.Context, network *cniNetwork, podName string, podNamespace string, podSandboxID kubecontainer.ContainerID, podNetnsPath string, annotations, options map[string]string) (cnitypes.Result, error) {
    rt, err := plugin.buildCNIRuntimeConf(podName, podNamespace, podSandboxID, podNetnsPath, annotations, options)
    if err != nil {
        klog.Errorf(&amp;quot;Error adding network when building cni runtime conf: %v&amp;quot;, err)
        return nil, err
    }

    pdesc := podDesc(podNamespace, podName, podSandboxID)
    netConf, cniNet := network.NetworkConfig, network.CNIConfig
    klog.V(4).Infof(&amp;quot;Adding %s to network %s/%s netns %q&amp;quot;, pdesc, netConf.Plugins[0].Network.Type, netConf.Name, podNetnsPath)
    res, err := cniNet.AddNetworkList(ctx, netConf, rt)
    if err != nil {
        klog.Errorf(&amp;quot;Error adding %s to network %s/%s: %v&amp;quot;, pdesc, netConf.Plugins[0].Network.Type, netConf.Name, err)
        return nil, err
    }
    klog.V(4).Infof(&amp;quot;Added %s to network %s: %v&amp;quot;, pdesc, netConf.Name, res)
    return res, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CNIConfig实现的AddNetworkList接口会将对应的配置和参数，转化为环境变量执行CNI插件的二进制文件，完成对容器的网络配置，下面的步骤也就是&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network-cni/#kubelet&#34;&gt;CNI的实现&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算容器系列---- Docker network</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/docker/docker-network/</link>
          <pubDate>Wed, 14 Oct 2020 16:29:55 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/docker/docker-network/</guid>
          <description>&lt;p&gt;容器网络的建立和控制是一种结合了network namespace、iptables、linux网桥、route table等多种Linux内核技术的综合方案,在宿主机和容器内分别创建虚拟接口，并让它们彼此连通。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;network namespace：主要提供了关于网络资源的隔离，包括网络设备、IPv4和IPv6协议栈、IP路由表、防火墙、/proc/net目录、/sys/class/net目录、端口（socket）等。&lt;/li&gt;
&lt;li&gt;linux Bridge：功能相当于物理交换机，为连在其上的设备（容器）转发数据帧。如docker0网桥。&lt;/li&gt;
&lt;li&gt;iptables：主要为容器提供NAT以及容器网络安全。&lt;/li&gt;
&lt;li&gt;veth pair：两个虚拟网卡组成的数据通道。在Docker中，用于连接Docker容器和Linux Bridge。一端在容器中作为eth0网卡，另一端在Linux Bridge中作为网桥的一个端口。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基础理论&#34;&gt;基础理论&lt;/h1&gt;

&lt;p&gt;在顶层设计中，Docker 网络架构由 3 个主要部分构成：CNM、Libnetwork 和驱动。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CNM 是设计标准。在 CNM 中，规定了 Docker 网络架构的基础组成要素。&lt;/li&gt;
&lt;li&gt;Libnetwork 是 CNM 的具体实现，并且被 Docker 采用，Libnetwork 通过 Go 语言编写，并实现了 CNM 中列举的核心组件。&lt;/li&gt;
&lt;li&gt;驱动通过实现特定网络拓扑的方式来拓展该模型的能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;cnm&#34;&gt;CNM&lt;/h2&gt;

&lt;p&gt;Docker 网络架构的设计规范是 CNM。CNM 中规定了 Docker 网络的基础组成要素，完整内容见&lt;a href=&#34;https://github.com/moby/libnetwork&#34;&gt;GitHub&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;CNM 定义了 3 个基本要素：沙盒（Sandbox）、终端（Endpoint）和网络（Network）。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;沙盒是一个独立的网络栈。其中包括以太网接口、端口、路由表以及 DNS 配置。可以看为一个独立的nanmespace。&lt;/li&gt;
&lt;li&gt;终端就是虚拟网络接口。就像普通网络接口一样，终端主要职责是负责创建连接。在 CNM 中，终端负责将沙盒连接到网络。可以看作我们使用的veth pair对。&lt;/li&gt;
&lt;li&gt;网络是 802.1d 网桥（类似大家熟知的交换机）的软件实现。因此，网络就是需要交互的终端的集合，并且终端之间相互独立。可以看作我们docker0网桥。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图展示了 3 个组件是如何连接的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/network&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Docker 环境中最小的调度单位就是容器，而 CNM 也恰如其名，负责为容器提供网络功能。&lt;/p&gt;

&lt;p&gt;下图展示了 CNM 组件是如何与容器进行关联的——沙盒被放置在容器内部，为容器提供网络连接。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/network1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;容器 A 只有一个接口（终端）并连接到了网络 A。容器 B 有两个接口（终端）并且分别接入了网络 A 和网络 B。容器 A 与 B 之间是可以相互通信的，因为都接入了网络 A。但是，如果没有三层路由器的支持，容器 B 的两个终端之间是不能进行通信的。&lt;/p&gt;

&lt;p&gt;需要重点理解的是，终端与常见的网络适配器类似，这意味着终端只能接入某一个网络。因此，如果容器需要接入到多个网络，就需要多个终端。&lt;/p&gt;

&lt;p&gt;下图对前面的内容进行拓展，加上了 Docker 主机。虽然容器 A 和容器 B 运行在同一个主机上，但其网络堆栈在操作系统层面是互相独立的，这一点由沙盒机制保证。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/network2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;libnetwork&#34;&gt;Libnetwork&lt;/h2&gt;

&lt;p&gt;CNM 是设计规范文档，Libnetwork 是标准的实现。Libnetwork 是开源的，采用 Go 语言编写，它跨平台（Linux 以及 Windows），并且被 Docker 所使用。&lt;/p&gt;

&lt;p&gt;在 Docker 早期阶段，网络部分代码都存在于 daemon 当中。daemon 变得臃肿，并且不符合 UNIX 工具模块化设计原则，即既能独立工作，又易于集成到其他项目，所以，Docker 将该网络部分从 daemon 中拆分，并重构为一个叫作 Libnetwork 的外部类库。&lt;/p&gt;

&lt;p&gt;现在，Docker 核心网络架构代码都在 Libnetwork 当中。Libnetwork 实现了 CNM 中定义的全部 3 个组件。此外它还实现了本地服务发现（Service Discovery）、基于 Ingress 的容器负载均衡，以及网络控制层和管理层功能。&lt;/p&gt;

&lt;h3 id=&#34;服务发现-service-discovery&#34;&gt;服务发现（Service Discovery）&lt;/h3&gt;

&lt;p&gt;其底层实现是利用了 Docker 内置的 DNS 服务器，为每个容器提供 DNS 解析功能。&lt;/p&gt;

&lt;p&gt;我们通过一个实例来了解如何使用dns做服务发现的&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/sd&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1) ping c2 命令调用本地 DNS 解析器，尝试将“c2”解析为具体 IP 地址。每个 Docker 容器都有本地 DNS 解析器。&lt;/p&gt;

&lt;p&gt;2) 如果本地解析器在本地缓存中没有找到“c2”对应的 IP 地址，本地解析器会向 Docker DNS 服务器发起一个递归查询。本地服务解析器是预先配置好并知道 Docker DNS 服务器细节的。&lt;/p&gt;

&lt;p&gt;3) Docker DNS 服务器记录了全部容器名称和 IP 地址的映射关系，其中容器名称是容器在创建时通过 &amp;ndash;name 或者 &amp;ndash;net-alias 参数设置的。这意味着 Docker DNS 服务器知道容器“c2”的 IP 地址。&lt;/p&gt;

&lt;p&gt;4) DNS 服务返回“c2”对应的 IP 地址到“c1”本地 DNS 解析器。之所以会这样是因为两个容器位于相同的网络当中，如果所处网络不同则该命令不可行。&lt;/p&gt;

&lt;p&gt;5) ping 命令被发往“c2”对应的 IP 地址。&lt;/p&gt;

&lt;p&gt;每个启动时使用了 &amp;ndash;name 参数的 Swarm 服务或者独立的容器，都会将自己的名称和 IP 地址注册到 Docker DNS 服务。这意味着容器和服务副本可以通过 Docker DNS 服务互相发现。&lt;/p&gt;

&lt;p&gt;但是，服务发现是受网络限制的。这意味着名称解析只对位于同一网络中的容器和服务生效。如果两个容器在不同的网络，那么就不能互相解析。&lt;/p&gt;

&lt;h2 id=&#34;驱动&#34;&gt;驱动&lt;/h2&gt;

&lt;p&gt;如果说 Libnetwork 实现了控制层和管理层功能，那么驱动就负责实现数据层。比如，网络连通性和隔离性是由驱动来处理的，驱动层实际创建网络对象也是如此，其关系如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/network3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Docker 封装了若干内置驱动，通常被称作原生驱动或者本地驱动。&lt;/p&gt;

&lt;p&gt;在 Linux 上包括 Bridge、Overlay 以及 Macvlan，在 Windows 上包括 NAT、Overlay、Transport 以及 L2 Bridge。&lt;/p&gt;

&lt;p&gt;第三方也可以编写 Docker 网络驱动。这些驱动叫作远程驱动，例如 Calico、Contiv、Kuryr 以及 Weave。&lt;/p&gt;

&lt;p&gt;每个驱动都负责其上所有网络资源的创建和管理。举例说明，一个叫作“prod-fe-cuda”的覆盖网络由 Overlay 驱动所有并管理。这意味着 Overlay 驱动会在创建、管理和删除其上网络资源的时候被调用。&lt;/p&gt;

&lt;p&gt;为了满足复杂且不固定的环境需求，Libnetwork 支持同时激活多个网络驱动。这意味着 Docker 环境可以支持一个庞大的异构网络。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;桥接网络&#34;&gt;桥接网络&lt;/h2&gt;

&lt;p&gt;桥接网络(Bridge Network)，在同一宿主机上的多个容器可以连接到同一桥接网络来实现彼此网络通信，之前提到了Docker默认的虚拟网桥为docker0，当然也可以创建自定义的网络生成新的虚拟网桥，而将容器连接到的该虚拟网桥上形成的网络称为桥接网络。在运行容器时可指定&amp;ndash;net=bridge使用桥接模式，将容器桥接到宿主机的虚拟网桥上，从虚拟网桥的空闲网段中划分出一个子网私有IP分配给该容器。&lt;/p&gt;

&lt;h2 id=&#34;覆盖网络&#34;&gt;覆盖网络&lt;/h2&gt;

&lt;p&gt;重叠网络(Overly Network，也有的译成叠加或覆盖网络)，在物理网络之上，通过软件定义网络(Software Defined Network, SDN) 创建虚拟网络抽象，使得应用之间在逻辑上彼此隔离，但共享相同的底层物理网络，同时简化网络管理。&lt;/p&gt;

&lt;p&gt;重叠网技术的基本思路是在互联网承载层和应用层之间增加一个中间层，结合IP承载网技术，为上层业务和应用提供有针对性的服务，对已有的应用和业务进行适当控制，是对现有互联网体系架构的系统性修补。在软件定义网络领域中，重叠网络的实现大多采用隧道(Tunneling)技术实现，它可以基于现行的IP网络进行叠加部署，消除传统二层网络的限制，如VXLAN、NVGRE等，都是基于隧道原理实现网络通信，利用叠加在三层网络之上的虚拟网络传递二层数据包，实现了可以跨越三层物理网络进行通信的二层逻辑网络，突破了传统二层网络中存在的物理位置受限、VLAN数量有限等障碍，大幅度降低管理成本和运营风险。&lt;/p&gt;

&lt;p&gt;在Docker 1.9之后的版本中，Docker官方集成了重叠网络的特性支持，用户可以在Swarm中使用它或者将其作为Compose工具。通过创建虚拟网络并将其连接到容器上，可实现多个主机上容器相互通信，并且能够实现不同的应用程序或者应用程序不同部分的相互隔离。&lt;/p&gt;

&lt;h1 id=&#34;docker网络方式&#34;&gt;Docker网络方式&lt;/h1&gt;

&lt;p&gt;Docker 的子网络是可插拔的，默认支持几种驱动程序，提供核心网络功能。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;bridge: 默认的网络驱动。当你的应用程序需要在单独的容器中运行时，通常会使用桥接网络。&lt;/li&gt;
&lt;li&gt;host: 对于独立容器，和主机之间没有网络隔离，直接使用主机的网络。 host 仅适用于 Docker 17.06 以及更高版本的 swarm 服务。&lt;/li&gt;
&lt;li&gt;container模式网络 使用–net =container:指定容器名，可以多个容器共用一个网络&lt;/li&gt;
&lt;li&gt;none: 容器禁用所有网络。通常与自定义网络驱动一起使用。 none 对于 swarm 服务不可用。&lt;/li&gt;
&lt;li&gt;overlay: Overlay 将多个 Docker 守护进程连接在一起，并且可以在 swarms 服务内部相互通信。还可以使用 Overlay 网络在 swarm 服务和独立容器之间通信，或者在不同的 Docker 守护进程上两个独立容器之间通信。该策略无需在这些容器之间在操作系统级别路由。&lt;/li&gt;
&lt;li&gt;macvlan: Macvlan 网络允许你为容器分配 MAC 地址，使其显示为网络上的物理设备。Docker 守护程序通过 MAC 地址将流量路由到容器。 macvlan 一般用于处理希望可以连接到屋里网络的陈旧的应用程序，而不是通过 Docker 的主机网络来路由。&lt;/li&gt;
&lt;li&gt;网络插件: 你可以使用 Docker 安装和使用第三方插件。在 DockerHub 或者第三方的供应商可以找到可用的插件。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以在docker run时通过&amp;ndash;net参数来指定容器的网络配置。&lt;/p&gt;

&lt;h2 id=&#34;bridge模式&#34;&gt;bridge模式&lt;/h2&gt;

&lt;p&gt;使用&amp;ndash;net=bridge指定，为默认模式，连接到默认的网桥docker0，docker0是虚拟网桥，使用软件技术模拟的一套内部网络。我们此模式会为每个容器分配一个独立的网络命名空间，使用该设置可以看到在容器中创建了eth0。&lt;/p&gt;

&lt;p&gt;当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上，所以有默认地址172.17.0.0/16的地址。&lt;/p&gt;

&lt;p&gt;从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。可以通过brctl show命令查看。所以Docker创建一个容器的时候，会执行如下操作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建一对虚拟接口，即veth pair，分别放到宿主机和容器中;&lt;/li&gt;
&lt;li&gt;主机一端桥接到默认的docker0或指定网桥上，具有一个唯一的名字，如veth0ac844e;&lt;/li&gt;
&lt;li&gt;另一端放到新容器中，并修改名字为eth0，该接口只在容器的命名空间可见;&lt;/li&gt;
&lt;li&gt;从网桥可用地址段中获取一个空闲地址分配给容器的eth0，并配置默认路由到桥接网卡veth0ac844e。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结构图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/bridge&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# docker run --name b1 -it --network bridge --rm busybox:latest     默认使用桥接模式，使用docker0虚拟网桥，创建一对虚拟接口eth0和veth
/ # ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02
          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:12 errors:0 dropped:0 overruns:0 frame:0
          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:1016 (1016.0 B)  TX bytes:508 (508.0 B)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
/ # route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.17.0.1      0.0.0.0         UG    0      0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 eth0
/ # ping 10.11.55.5   正常访问宿主机
PING 10.11.55.5 (10.11.55.5): 56 data bytes
64 bytes from 10.11.55.5: seq=0 ttl=64 time=0.292 ms
/ # exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;连接在同一个网桥上的IP都是相互通的。但是对于不同的网桥，哪怕是在一台主机上也是不通的，如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/bridge1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们在看主机的网络&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:69:06:60 brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.6/20 brd 172.16.15.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fe69:660/64 scope link
       valid_lft forever preferred_lft forever
3: docker0: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:74:13:f5:ff brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:74ff:fe13:f5ff/64 scope link
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当运行一个容器时，我们可以看到在docker主机上多了一个网卡（vethd579570@if38），而且master指向docker0&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# docker run -d -P --net=bridge nginx:1.9.1
59abbf7dbeedf974349fd971970300e311d4ff56aef7e17f8071ec84e8b8e795
# ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:69:06:60 brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.6/20 brd 172.16.15.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fe69:660/64 scope link
       valid_lft forever preferred_lft forever
3: docker0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:74:13:f5:ff brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:74ff:fe13:f5ff/64 scope link
       valid_lft forever preferred_lft forever
39: vethd579570@if38: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master docker0 state UP group default
    link/ether 7e:79:90:55:97:1d brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::7c79:90ff:fe55:971d/64 scope link
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为bridge模式是Docker的默认设置，所以你也可以使用&lt;code&gt;docker run -d -P nginx:1.9.1&lt;/code&gt;。如果你没有使用-P（发布该容器暴露的所有端口）或者-p
host_port:container_port（发布某个特定端口），IP数据包就不能从宿主机之外路由到容器中。&lt;/p&gt;

&lt;p&gt;这时候我们在查看该容器的网络信息（ip地址和网关）。发现它的ip地址和docker0一个网段，网关则是docker0的地址&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@VM_0_6_centos ~]# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS                                           NAMES
59abbf7dbeed        nginx:1.9.1         &amp;quot;nginx -g &#39;daemon of…&amp;quot;   About a minute ago   Up About a minute   0.0.0.0:32771-&amp;gt;80/tcp, 0.0.0.0:32770-&amp;gt;443/tcp   competent_cori
[root@VM_0_6_centos ~]# docker inspect -f {{&amp;quot;.NetworkSettings.Networks.bridge.IPAddress&amp;quot;}} 59abbf7dbeed
172.17.0.2
[root@VM_0_6_centos ~]# docker inspect -f {{&amp;quot;.NetworkSettings.Networks.bridge.Gateway&amp;quot;}} 59abbf7dbeed
172.17.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们也可以使用自定义桥接网络，创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker network create my-net
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以指定子网，IP地址范围，网关和其他选项。然后我们就可以将容器连接到这个网桥上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//未启动
# docker create --name my-nginx \
&amp;gt; --network my-net \
&amp;gt; --publish 8080:80 \
&amp;gt; nginx:latest
//已启动
# docker network connect my-net my-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker network disconnect my-net my-nginx
$ docker network rm my-net
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;桥接网络适用于在同一个 Docker 守护程序的主机容器进行通信，对于跨机器或者集群通信需要使用其他网络。&lt;/p&gt;

&lt;h2 id=&#34;host模式&#34;&gt;host模式&lt;/h2&gt;

&lt;p&gt;使用&amp;ndash;net=host指定，那么这个容器将不会获得一个独立的Network Namespace，Docker使用的网络和宿主机的一样的Network Namespace，该方式创建出来的容器，可以看到主机上所有的网络设备，容器中对这些设备有全部的访问权限。&lt;/p&gt;

&lt;p&gt;因此，使用这个选项的时候要非常小心，在非隔离的环境中使用host模式是不安全的，如果进一步的使用&amp;ndash;privileged=true，容器可以直接配置主机的网络堆栈。&lt;/p&gt;

&lt;p&gt;结构图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/host&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# docker run --name b2 -it --network host --rm busybox:latest
/ # ifconfig -a   和宿主机一样
docker0   Link encap:Ethernet  HWaddr 02:42:41:C4:5D:6E
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:41ff:fec4:5d6e/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:90 errors:0 dropped:0 overruns:0 frame:0
          TX packets:26 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:5903 (5.7 KiB)  TX bytes:2381 (2.3 KiB)

eth0      Link encap:Ethernet  HWaddr 00:0C:29:AB:D2:DA
          inet addr:10.11.55.5  Bcast:10.11.55.255  Mask:255.255.255.0
          inet6 addr: fe80::20c:29ff:feab:d2da/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:3913 errors:0 dropped:0 overruns:0 frame:0
          TX packets:3327 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:354314 (346.0 KiB)  TX bytes:919096 (897.5 KiB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出来和主机一摸一样。&lt;/p&gt;

&lt;h2 id=&#34;container模式&#34;&gt;container模式&lt;/h2&gt;

&lt;p&gt;使用&amp;ndash;net=container:CONTAINER_ID/CONTAINER_NAME指定，多个容器使用共同的网络，即两者的网络完全相同，将新建容器的进程放到一个已存在容器的网络栈中，新容器进程有自己的文件系统、进程列表和资源限制，但会和已存在的容器共享IP地址和端口等网络资源，两者进程可以直接通过 环回接口 通信。&lt;/p&gt;

&lt;p&gt;结构图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/container&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;在一个终端，使用bridge网络模式启动容器b1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@along ~]# docker run --name b1 -it --rm busybox:latest
/ # ifconfig   b1的ip为172.17.0.2
eth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02
          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:6 errors:0 dropped:0 overruns:0 frame:0
          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:508 (508.0 B)  TX bytes:508 (508.0 B)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
/ # echo &amp;quot;hello world b1&amp;quot; &amp;gt; /tmp/index.html
/ # httpd -h /tmp/  在b1上启动httpd服务
/ # netstat -nutl
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 :::80                   :::*                    LISTEN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在另一个终端使用Container 网络模式创建容器b2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# docker run --name b2 -it --network container:b1 --rm busybox:latest
/ # ifconfig -a   b2的ip和b1一样
eth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02
          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:8 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
/ # wget -O - -q 127.0.0.1   b1启动的httpd服务，在b2上直接访问
hello world b1
/ # ls /tmp/   但是文件系统并不共享，只共享网络
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;none模式&#34;&gt;none模式&lt;/h2&gt;

&lt;p&gt;使用&amp;ndash;net=none指定，使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息，只有lo 网络接口。需要我们自己为Docker容器添加网卡、配置IP等。　　&lt;/p&gt;

&lt;p&gt;不参与网络通信，运行于此类容器中的进程仅能访问本地回环接口；仅适用于进程无须网络通信的场景中，例如：备份、进程诊断及各种离线任务等。&lt;/p&gt;

&lt;p&gt;结构图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/none&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# docker run --name b1 -it --network none --rm busybox:latest
/ # ifconfig
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
/ # route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
/ # exit
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;overlay&#34;&gt;overlay&lt;/h2&gt;

&lt;p&gt;用户自定义模式主要可选的有三种网络驱动：bridge、overlay、macvlan。bridge驱动用于创建类似于前面提到的bridge网络；overlay和macvlan驱动用于创建跨主机的网络。&lt;/p&gt;

&lt;p&gt;overlay网络就是在上面基本概念解说过的覆盖网络，就是在已有网络基础之上应用之下构建的一层单独通信的网络，使得容器能够点对点通信，至于真正的通过是通过三次网络进行的，我们不必关心。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/overlay&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们先举个例子来加深理解&lt;/p&gt;

&lt;p&gt;首先在在 Swarm 模式下有两个节点如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/overlay4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;node1 节点上运行 docker swarm init 命令使其成为管理节点，然后在 node2 节点上运行 docker swarm join 命令来使其成为工作节点。&lt;/p&gt;

&lt;p&gt;现在创建一个名为 uber-net 的覆盖网络。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker network create -d overlay uber-net
c740ydi1lm89khn5kd52skrd9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;刚刚创建了一个崭新的覆盖网络，能连接 Swarm 集群内的所有主机，在node1上使用docker network ls可以看到刚刚创建的overlay网络，如果在 node2 节点上运行 docker network ls 命令，就会发现无法看到 uber-net 网络。这是因为只有当运行中的容器连接到覆盖网络的时候，该网络才变为可用状态。这种延迟生效策略通过减少网络梳理，提升了网络的扩展性。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker network ls
NETWORK ID NAME DRIVER SCOPE
ddac4ff813b7 bridge bridge local
389a7e7e8607 docker_gwbridge bridge local
a09f7e6b2ac6 host host local
ehw16ycy980s ingress overlay swarm
2b26c11d3469 none null local
c740ydi1lm89 uber-net overlay swarm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只要将服务连接到覆盖网络就好，我们新建一个Docker 服务会包含两个副本（容器），一个运行 node1 节点上，一个运行在 node2 节点上。这样会自动将 node2 节点接入 uber-net 网络。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker service create --name test \
--network uber-net \
--replicas 2 \
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在使用 ping 命令来测试覆盖网络。如下图所示，在两个独立的网络中分别有一台 Docker 主机，并且两者都接入了同一个覆盖网络。目前在每个节点上都有一个容器接入了覆盖网络。测试一下两个容器之间是否可以 ping 通。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/overlay3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由图可知，一个二层覆盖网络横跨两台主机，并且每个容器在覆盖网络中都有自己的 IP 地址。这意味着 node1 节点上的容器可以通过 node2 节点上容器的 IP 地址 10.0.0.4 来 ping 通，该 IP 地址属于覆盖网络。尽管两个节点分属于不同的二层网络，还是可以直接 ping 通。&lt;/p&gt;

&lt;p&gt;根据实例我们来看看overlay的原理，Docker 使用 VXLAN 隧道技术创建了虚拟二层覆盖网络。所以，在详解之前，先快速了解一下 VXLAN。&lt;/p&gt;

&lt;p&gt;在 VXLAN 的设计中，允许用户基于已经存在的三层网络结构创建虚拟的二层网络。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/overlay1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;VXLAN 的美妙之处在于它是一种封装技术，能使现存的路由器和网络架构看起来就像普通的 IP/UDP 包一样，并且处理起来毫无问题。&lt;/p&gt;

&lt;p&gt;为了创建二层覆盖网络，VXLAN 基于现有的三层 IP 网络创建了隧道。小伙伴可能听过基础网络（Underlay Network）这个术语，它用于指代三层之下的基础部分。&lt;/p&gt;

&lt;p&gt;VXLAN 隧道两端都是 VXLAN 隧道终端（VXLAN Tunnel Endpoint, VTEP）。VTEP 完成了封装和解压的步骤，以及一些功能实现所必需的操作，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/overlay2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在前面的示例中，读者通过 IP 网络将两台主机连接起来。每个主机运行了一个容器，之后又为容器连接创建了一个 VXLAN 覆盖网络。&lt;/p&gt;

&lt;p&gt;为了实现上述场景，在每台主机上都新建了一个 Sandbox（网络命名空间）。正如前文所讲，Sandbox 就像一个容器，但其中运行的不是应用，而是当前主机上独立的网络栈。&lt;/p&gt;

&lt;p&gt;在 Sandbox 内部创建了一个名为 Br0 的虚拟交换机（又称做虚拟网桥）。同时 Sandbox 内部还创建了一个 VTEP，其中一端接入到名为 Br0 的虚拟交换机当中，另一端接入主机网络栈（VTEP）。&lt;/p&gt;

&lt;p&gt;在主机网络栈中的终端从主机所连接的基础网络中获取到 IP 地址，并以 UDP Socket 的方式绑定到 4789 端口。不同主机上的两个 VTEP 通过 VXLAN 隧道创建了一个覆盖网络，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/overlay5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这是 VXLAN 上层网络创建和使用所必需的。&lt;/p&gt;

&lt;p&gt;接下来每个容器都会有自己的虚拟以太网（veth）适配器，并接入本地 Br0 虚拟交换机。目前拓扑结构如下图所示，虽然是在主机所属网络互相独立的情况下，但这样能更容易看出两个分别位于不同主机上的容器之间是如何通过 VXLAN 上层网络进行通信的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/overlay6.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们再来看一下上面ping通的过程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/overlay7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;C1 发起 ping 请求，目标 IP 为 C2 的地址 10.0.0.4。该请求的流量通过连接到 Br0 虚拟交换机 veth 接口发出。虚拟交换机并不知道将包发送到哪里，因为在虚拟交换机的 MAC 地址映射表（ARP 映射表）中并没有与当前目的 IP 对应的 MAC 地址。&lt;/p&gt;

&lt;p&gt;所以虚拟交换机会将该包发送到其上的全部端口。连接到 Br0 的 VTEP 接口知道如何转发这个数据帧，所以会将自己的 MAC 地址返回。这就是一个代理 ARP 响应，并且虚拟交换机 Br0 根据返回结果学会了如何转发该包。接下来虚拟交换机会更新自己的 ARP 映射表，将 10.0.0.4 映射到本地 VTEP 的 MAC 地址上。&lt;/p&gt;

&lt;p&gt;现在 Br0 交换机已经学会如何转发目标为 C2 的流量，接下来所有发送到 C2 的包都会被直接转发到 VTEP 接口。VTEP 接口知道 C2，是因为所有新启动的容器都会将自己的网络详情采用网络内置 Gossip 协议发送给相同 Swarm 集群内的其他节点。&lt;/p&gt;

&lt;p&gt;交换机会将包转发到 VTEP 接口，VTEP 完成数据帧的封装，这样就能在底层网络传输。具体来说，封装操作就是把 VXLAN Header 信息添加以太帧当中。&lt;/p&gt;

&lt;p&gt;VXLAN Header 信息包含了 VXLAN 网络 ID（VNID），其作用是记录 VLAN 到 VXLAN 的映射关系。每个 VLAN 都对应一个 VNID，以便包可以在解析后被转发到正确的 VLAN。&lt;/p&gt;

&lt;p&gt;封装的时候会将数据帧放到 UDP 包中，并设置 UDP 的目的 IP 字段为 node2 节点的 VTEP 的 IP 地址，同时设置 UDP Socket 端口为 4789。这种封装方式保证了底层网络即使不知道任何关于 VXLAN 的信息，也可以完成数据传输。&lt;/p&gt;

&lt;p&gt;当包到达 node2 之后，内核发现目的端口为 UDP 端口 4789，同时还知道存在 VTEP 接口绑定到该 Socket。所以内核将包发给 VTEP，由 VTEP 读取 VNID，解压包信息，并根据 VNID 发送到本地名为 Br0 的连接到 VLAN 的交换机。在该交换机上，包被发送给容器 C2。&lt;/p&gt;

&lt;h2 id=&#34;macvlan&#34;&gt;Macvlan&lt;/h2&gt;

&lt;p&gt;Macvlan通过为容器提供 MAC 和 IP 地址，让容器在物理网络上成为“一等公民”，无须端口映射或者额外桥接，可以直接通过主机接口（或者子接口）访问容器接口。&lt;/p&gt;

&lt;p&gt;Macvlan 的缺点是需要将主机网卡（NIC）设置为混杂模式（Promiscuous Mode），这在大部分公有云平台上是不允许的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/macvlan&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;假设有一个物理网络，其上配置了两个 VLAN——VLAN 100：10.0.0.0/24 和 VLAN 200：192.168.3.0/24，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/macvlan1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接下来，添加一个 Docker 主机并连接到该网络，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/macvlan2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;下面的命令会创建一个名为 macvlan100 的 Macvlan 网络，该网络会连接到 VLAN 100。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker network create -d macvlan \
--subnet=10.0.0.0/24 \
--ip-range=10.0.00/25 \
--gateway=10.0.0.1 \
-o parent=eth0.100 \
macvlan100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该命令会创建 macvlan100 网络以及 eth0.100 子接口。macvlan100 网络已为容器准备就绪，执行以下命令将容器部署到该网络中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker container run -d --name mactainer1 \
--network macvlan100 \
alpine sleep 1d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当前配置如下图所示。但是切记，下层网络（VLAN 100）对 Macvlan 的魔法毫不知情，只能看到容器的 MAC 和 IP 地址。在该基础之上，mactainer1 容器可以 ping 通任何加入 VLAN 100 的系统，并进行通信。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/macvlan3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;网络插件&#34;&gt;网络插件&lt;/h2&gt;

&lt;p&gt;网络插件主要是解决的跨主机的通信，就像overlay和macvlan解决的问题一样，基本也适用于k8s的网络实现，网络插件基本都是支持k8s网络实现，可以查看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network/&#34;&gt;k8s网络实现&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;当需要在同一个 Docker 主机上多个容器通信时，&lt;em&gt;使用用户定义的 bridge网络&lt;/em&gt; 。&lt;/li&gt;
&lt;li&gt;当网络堆栈不应与 Docker 主机隔离时（但是你希望隔离容器的其他方面），&lt;em&gt;使用 host 网络&lt;/em&gt; 。&lt;/li&gt;
&lt;li&gt;当需要在不同的 Docker 主机上多个容器互通时，*使用 Overlay 网络*，或者使用 swarm 服务多个应用一起工作时。&lt;/li&gt;
&lt;li&gt;当你从 VM 迁移业务时，需要容器看起来像网络上的物理主机时，每个都有唯一的 MAC 地址，*使用 Macvlan 网络*。&lt;/li&gt;
&lt;li&gt;第三方的网络插件 允许将 Docker 与专用网络堆栈集成。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实这些方式都是早期在使用docker时候的网络设置，后来都使用了编排工具，比如k8s，都有相应的解决方案。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s组件系列（五）---- K8s proxy 详解</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-proxy/</link>
          <pubDate>Tue, 13 Oct 2020 20:22:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-proxy/</guid>
          <description>&lt;p&gt;kube-proxy是Kubernetes的核心组件，部署在每个Node节点上，它是实现Kubernetes Service的通信与负载均衡机制的重要组件; kube-proxy负责为Pod创建代理服务，从apiserver获取所有server信息，并根据server信息创建代理服务，实现server到Pod的请求路由和转发，从而实现K8s层级的虚拟转发网络。&lt;/p&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;p&gt;直接使用二进制文件进行启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/bin/kube-proxy \
  --bind-address=10.12.51.171 \
  --hostname-override=10.12.51.171 \
  --cluster-cidr=10.254.0.0/16 \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
  --logtostderr=true \
  --v=2 \
  --ipvs-scheduler=wrr \
  --ipvs-min-sync-period=5s \
  --ipvs-sync-period=5s \
  --proxy-mode=ipvs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们主要看一下几个核心的启动参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--alsologtostderr   设置true则日志输出到stderr，也输出到日志文件
--bind-address 0.0.0.0  监听主机IP地址，0.0.0.0监听主机所有主机接口 (default 0.0.0.0)
--cleanup   如果设置为true，则清除iptables和ipvs规则并退出
--cleanup-ipvs  如果设置为true，在运行前kube-proxy将清除ipvs规则(default true)
--cluster-cidr string   集群中 Pod 的CIDR范围。集群外的发送到服务集群IP的流量将被伪装，从pod发送到外部 LoadBalancer IP的流量将被定向到相应的集群IP
--config string 配置文件路径
--config-sync-period duration   从apiserver同步配置的时间间隔(default 15m0s)
--conntrack-max-per-core int32  每个CPU核跟踪的最大NAT连接数（0按原来保留限制并忽略conntrack-min）(default 32768)
--conntrack-min int32   分配的最小conntrack条目，无视conntrack-max-per-core选项（设置conntrack-max-per-core=0保持原始限制）(default 131072)
--conntrack-tcp-timeout-close-wait duration 对于TCP连接处于CLOSE_WAIT阶段的NAT超时时间(default 1h0m0s)
--conntrack-tcp-timeout-established duration    TCP连接的空闲超时(default 24h0m0s)
--feature-gates mapStringBool   key = value对，用于试验
--healthz-bind-address 0.0.0.0  健康检查服务器提供服务的IP地址及端口(default 0.0.0.0:10256)
--healthz-port int32    配置健康检查服务的端口，0表示禁止 (default 10256)
--hostname-override string  使用该名字作为标识而不是实际的主机名
--iptables-masquerade-bit int32 对于纯iptables代理，则表示fwmark space的位数，用于标记需要SNAT的数据包。[0,31]范围 (default 14)
--iptables-min-sync-period duration 当endpoints和service变化，刷新iptables规则的最小时间间隔 
--iptables-sync-period duration iptables刷新的最大时间间隔 (default 30s)
 --ipvs-exclude-cidrs strings   ipvs proxier清理IPVS规则时不触及的CIDR以逗号分隔的列表
--ipvs-min-sync-period duration 当endpoints和service变化，刷新ipvs规则的最小时间间隔 
--ipvs-scheduler string 当proxy模式设置为ipvs，ipvs调度的类型
--ipvs-sync-period duration ipvs刷新的最大时间间隔 (default 30s)
--kube-api-burst int32  发送到kube-apiserver每秒请求量 （default 10）
--kube-api-content-type string  发送到kube-apiserver请求内容类型（default &amp;quot;application/vnd.kubernetes.protobuf&amp;quot;）
--kube-api-qps float32  与kube-apiserver通信的qps（default 5）
--kubeconfig string 具有授权信息的kubeconfig文件的路径
--log-backtrace-at traceLocation    when logging hits line file:N, emit a stack trace (default :0)
--log-dir string    If non-empty, write log files in this directory
--log-flush-frequency duration  Maximum number of seconds between log flushes (default 5s)
--logtostderr   log to standard error instead of files (default true)
--masquerade-all    纯 iptables 代理，对所有通过集群 service IP发送的流量进行 SNAT（通常不配置）
--master string Kubernetes API server地址，覆盖kubeconfig的配置
--metrics-bind-address 0.0.0.0  对于metrics服务地址和端口 (default 127.0.0.1:10249)
--nodeport-addresses strings    A string slice of values which specify the addresses to use for NodePorts. Values may be valid IP blocks (e.g. 1.2.3.0/24, 1.2.3.4/32). The default empty string slice ([]) means to use all local addresses.
--oom-score-adj int32   kube-proxy进程的oom-score-adj值，合法值范围[-1000, 1000]  (default -999)
--profiling 设置为true，通过web接口/debug/pprof查看性能分析
--proxy-mode ProxyMode  userspace / iptables / ipvs (默认为iptables)
--proxy-port-range port-range   Range of host ports (beginPort-endPort, single port or beginPort+offset, inclusive) that may be consumed in order to proxy service traffic. If (unspecified, 0, or 0-0) then ports will be randomly chosen.
--stderrthreshold severity  logs at or above this threshold go to stderr (default 2)
--udp-timeout duration  ow long an idle UDP connection will be kept open (e.g. &#39;250ms&#39;, &#39;2s&#39;).  Must be greater than 0. Only applicable for proxy-mode=userspace (default 250ms)
-v, --v Level   log level for V logs
--version version[=true]    Print version information and quit
--vmodule moduleSpec    逗号分隔的模式=N的列表文件，用以筛选日志记录
--write-config-to string    If set, write the default configuration values to this file and exit.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;kube-proxy&#34;&gt;kube-proxy&lt;/h1&gt;

&lt;p&gt;容器的特点是快速创建、快速销毁，Kubernetes Pod和容器一样只具有临时的生命周期，一个Pod随时有可能被终止或者漂移，随着集群的状态变化而变化，一旦Pod变化，则该Pod提供的服务也就无法访问，如果直接访问Pod则无法实现服务的连续性和高可用性，因此显然不能使用Pod地址作为服务暴露端口。&lt;/p&gt;

&lt;p&gt;解决这个问题的办法和传统数据中心解决无状态服务高可用的思路完全一样，通过负载均衡和VIP实现后端真实服务的自动转发、故障转移。&lt;/p&gt;

&lt;p&gt;这个负载均衡在Kubernetes中称为Service，VIP即Service ClusterIP，因此可以认为Kubernetes的Service就是一个四层负载均衡，Kubernetes对应的还有七层负载均衡Ingress&lt;/p&gt;

&lt;h2 id=&#34;services&#34;&gt;services&lt;/h2&gt;

&lt;p&gt;这个Service就是由kube-proxy实现的，ClusterIP不会因为Pod状态改变而变，需要注意的是VIP即ClusterIP是个假的IP，这个IP在整个集群中根本不存在，这个IP在其他节点以及集群外均无法访问。&lt;/p&gt;

&lt;p&gt;所以在你创建service的时候，kube-proxy会为你分配一个clusterIp并且为这个IP实现路由(根据label查询到具体pod，然后根据模式设置路由规则，比如iptables规则)，这个路由有很多方式，取决于使用kube-proxy的配置使用方式，具体模式，我们下面详细讲解。关于service资源可以看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-principle/#service&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;ingress&#34;&gt;ingress&lt;/h2&gt;

&lt;p&gt;ingress是七层的负载均衡，主要实现给service对外访问的转发机制，具体可以看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-principle/#ingress&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;模式&#34;&gt;模式&lt;/h1&gt;

&lt;p&gt;kubernetes里kube-proxy支持三种模式，其中userspace mode是v1.0及之前版本的默认模式，从v1.1版本中开始增加了iptables mode，在v1.2版本中正式替代userspace模式成为默认模式，在v1.8之前我们使用的是iptables 以及 userspace两种模式，在kubernetes 1.8之后引入了ipvs模式，并且在v1.11中正式使用，其中iptables和ipvs都是内核态也就是基于netfilter，只有userspace模式是用户态。&lt;/p&gt;

&lt;h2 id=&#34;userspace&#34;&gt;userspace&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/proxy2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;userspace 模式下service的请求会先从用户空间进入内核iptables，然后再回到用户空间，由kube-proxy完成后端Endpoints的选择和代理工作，这样流量从用户空间进出内核带来的性能损耗是不可接受的。这也是k8s v1.0及之前版本中对kube-proxy质疑最大的一点，因此社区就开始研究iptables mode。&lt;/p&gt;

&lt;h2 id=&#34;iptables&#34;&gt;iptables&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/proxy3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kube-Proxy 监听 Kubernetes Master 增加和删除 Service 以及 Endpoint 的消息。对于每一个 Service，Kube Proxy 创建相应的 IPtables 规则，并将发送到 Service Cluster IP 的流量转发到 Service 后端提供服务的 Pod 的相应端口上。并且流量的转发都是在内核态进行的，所以性能更高更加可靠。&lt;/p&gt;

&lt;p&gt;但是iptables mode因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗。另外，如果集群中存在上万的Service/Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。
这也导致，当时大部分企业用k8s上生产时，都不会直接用kube-proxy作为服务代理，而是通过自己开发或者通过Ingress Controller来集成HAProxy, Nginx来代替kube-proxy。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;我们来分析下面这个service的iptables规则。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
meadata:
  labels:
    name: tomcat
  name: tomcat
space:
  ports:
    - port: 6080
      targetPort: 6080
      nodePort: 30005
  type: NodePort
  selector:
    app: tomcat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如上所示，在本例中我们创建了一个NodePort类型名为tomcat的服务。该服务的端口为6080，NodePort为30005，对应后端Pod的端口也为6080。它的Cluster IP为10.254.0.40。tomcat服务有两个后端Pod，IP分别是192.168.20.1和192.168.20.2，这边我们就不展示了。&lt;/p&gt;

&lt;p&gt;kube-proxy 为该服务创建的iptables规则如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# iptables -S -t nat
-A PREROUTING -m -comment --comment &amp;quot;kubernetes service portals&amp;quot; -j KUBE-SERVICES
-A OUTPUT -m -comment --comment &amp;quot;kubernetes service portals&amp;quot; -j KUBE-SERVICES
-A POSTROUTING -m -comment --comment &amp;quot;kubernetes postrouting rules &amp;quot; -j KUBE-POSTROUTING
-A KUBE-MARK-MASQ -j MARK  --set-xmark  0x4000/0x4000

-A KUBE-NODEPORTS -p tcp -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-m tcp --dport 30005  -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-m -tpc --dport 30005 -j KUBE-SVC-67RLXXX

-A KUBE-SEP-ID6YXXX -s 192.168.20.1/32 -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-j KUBE-MARK-MASQ
-A KUBE-SEP-ID6YXXX -p tcp -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-m tcp -j DANT --to-destination 192.168.20.1:6080

-A KUBE-SEP-IN2YXXX -s 192.168.20.2/32 -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-j KUBE-MARK-MASQ
-A KUBE-SEP-IN2YXXX -p tcp -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-m tcp -j DANT --to-destination 192.168.20.2:6080

-A KUBE-SERVICES -d 10.254.0.40/32 -p tcp -m -comment --comment \
&amp;quot;default/tomcat: cluster ip&amp;quot; -m tcp --dport 6080 -j KUBE-SVC-67RLXXX
-A KUBE-SERVICES -m comment --comment &amp;quot;kubernetes service nodeport; NOTE: this must be the \
last rule in this chain &amp;quot; -m addrtype  --dst-type LOCAL -j KUBE-NODEPORTS

-A KUBE-SVC-67RLXXX  -m comment --comment &amp;quot;default/tomcat:&amp;quot; -m \
statistic --mode random --probability 0.500000000 -j KUBE-SEP-ID6YXXX
-A KUBE-SVC-67RLXXX -m comment --comment &amp;quot;default/tomcat:&amp;quot; -j \
KUBE-SEP-IN2YXXX
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先我们先看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-A KUBE-NODEPORTS -p tcp -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-m tcp --dport 30005  -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-m -tpc --dport 30005 -j KUBE-SVC-67RLXXX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过节点的30005端口访问NodePort，则会进入以下链,kube-proxy针对NodePort流量入口创建了KUBE-NODEPORTS 链。在我们这个例子中，KUBE-NODEPORTS 链进一步跳转到KUBE-SVC-67RLXXX链。&lt;/p&gt;

&lt;p&gt;在看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-A KUBE-SVC-67RLXXX  -m comment --comment &amp;quot;default/tomcat:&amp;quot; -m \
statistic --mode random --probability 0.500000000 -j KUBE-SEP-ID6YXXX
-A KUBE-SVC-67RLXXX -m comment --comment &amp;quot;default/tomcat:&amp;quot; -j \
KUBE-SEP-IN2YXXX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里采用了iptables的random模块，使连接有50%的概率进入KUBE-SEP-ID6YXXX链，50%的概率进入KUBE-SEP-IN2YXXX链。因此，kube-proxy的iptables模式采用随机数实现了服务的负载均衡。&lt;/p&gt;

&lt;p&gt;在看KUBE-SEP-ID6YXXX 链&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-A KUBE-SEP-ID6YXXX -s 192.168.20.1/32 -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-j KUBE-MARK-MASQ
-A KUBE-SEP-ID6YXXX -p tcp -m -comment --comment &amp;quot;default/tomcat:&amp;quot;\
-m tcp -j DANT --to-destination 192.168.20.1:6080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;KUBE-SEP-ID6YXXX 链的具体作用就是将请求通过DNAT发送到192.168.20.1的6080端口，到这里就完成了基本的转发。&lt;/p&gt;

&lt;h2 id=&#34;ipvs&#34;&gt;ipvs&lt;/h2&gt;

&lt;p&gt;iptables的数量限制和损耗的缺点，使得ipvs得到了发展。kubernetes从1.8开始增加了IPVS支持，并在Kubernetes 1.12成为kube-proxy的默认代理模式，IPVS相对于iptables来说效率会更加高，使用ipvs模式需要在允许proxy的节点上安装ipvsadm，ipset工具包加载ipvs的内核模块。并且ipvs可以轻松处理每秒 10 万次以上的转发请求。可以说是目前比较流行的使用方式。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/proxy4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kube-Proxy 会监视 Kubernetes Service 对象 和 Endpoints，调用 Netlink 接口以相应地创建 IPVS 规则并定期与 Kubernetes Service 对象 和 Endpoints 对象同步 IPVS 规则，以确保 IPVS 状态与期望一致。访问服务时，流量将被重定向到其中一个后端 Pod。&lt;/p&gt;

&lt;p&gt;ipvs模式也是基于netfilter，对比iptables模式在大规模Kubernetes集群有更好的扩展性和性能，支持更加复杂的负载均衡算法(如：最小负载、最少连接、加权等)，支持Server的健康检查和连接重试等功能。ipvs依赖于iptables，使用iptables进行包过滤、SNAT、masquared。ipvs将使用ipset需要被DROP或MASQUARED的源地址或目标地址，这样就能保证iptables规则数量的固定，我们不需要关心集群中有多少个Service了。&lt;/p&gt;

&lt;p&gt;在运行基于IPVS的kube-proxy时，需要注意以下参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;–proxy-mode：除了现有的userspace和iptables模式，IPVS模式通过–proxymode=ipvs进行配置。
–ipvs-scheduler:用来指定ipvs负载均衡算法，如果不配置则默认使用round-robin（rr）算法。
如果不配置则默认使用round-robin（rr）算法。支持配置的负载均衡算法有：
        — rr：轮询，这种算法是最简单的，就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是简单。轮询算法假设所有的服务器处理请求的能力都是一样的，调度器会将所有的请求平均分配给每个真实服务器，不管后端 RS 配置和处理能力，非常均衡地分发下去。
        — lc：最小连接，这个算法会根据后端 RS 的连接数来决定把请求分发给谁，比如 RS1 连接数比 RS2 连接数少，那么请求就优先发给 RS1
        — dh：目的地址哈希，该算法是根据目标 IP 地址通过散列函数将目标 IP 与服务器建立映射关系，出现服务器不可用或负载过高的情况下，发往该目标 IP 的请求会固定发给该服务器。
        — sh：原地址哈希，与目标地址散列调度算法类似，但它是根据源地址散列算法进行静态分配固定的服务器资源。
        — sed：最短时延
        （未来。kube-proxy可能实现在service的annotations 配置负载均衡策略，这个功能应该只能在IPVS模式下才支持）
–cleanup-ipvs：类似于–cleanup-iptables参数。如果设置为true，则清除在IPVS模式下创建的IPVS规则；
–ipvs-sync-period：表示kube-proxy刷新IPVS规则的最大间隔时间，例如5秒。1分钟等，要求大于0；
–ipvs-min-sync-period：表示kube-proxy刷新IPVS规则最小时间间隔，例如5秒，1分钟等，要求大于0
–ipvs-exclude-cidrs：用于清除IPVS规则时告知kube-proxy不要清理该参数配置的网段的IPVS规则。因为我们无法区别某条IPVS规则到底是kube-proxy创建的，还是其他用户进程的，配置该参数是为了避免删除用户自己的IPVS规则。
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s组件系列（二）---- K8s scheduler 详解</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-scheduler/</link>
          <pubDate>Thu, 24 Sep 2020 20:22:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-scheduler/</guid>
          <description>&lt;p&gt;kube-scheduler是 kubernetes 系统的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源。&lt;/p&gt;

&lt;h1 id=&#34;部署使用&#34;&gt;部署使用&lt;/h1&gt;

&lt;h2 id=&#34;物理部署&#34;&gt;物理部署&lt;/h2&gt;

&lt;p&gt;直接使用二进制文件启动就可以&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kube-scheduler [flags]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; /usr/bin/kube-scheduler --logtostderr=true --v=4 --master=http://10.243.129.252:8080 --address=0.0.0.0 --master=http://10.243.129.252:8080 --leader-elect=true --v=5 --log-dir=/k8s_log/kubernetes --use-legacy-policy-config=true --policy-config-file=/etc/kubernetes/scheduler-policy.config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;flags有很多，具体使用的时候可以查看&lt;a href=&#34;https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-scheduler/&#34;&gt;官网&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;这边简单的说几个常用的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//配置文件
--config=/etc/kubernetes/config/kube-scheduler.yaml
//策略文件
--use-legacy-policy-config=true
--policy-config-file=/etc/kubernetes/scheduler-policy.config
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置文件&#34;&gt;配置文件&lt;/h2&gt;

&lt;p&gt;根据上面参数配置配置文件，我们来看看配置文件内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
schedulerName: my-kube-scheduler
algorithmSource:
  policy:
    configMap:
      namespace: kube-system
      name: my-scheduler-policy
leaderElection:
  leaderElect: false
  lockObjectName: my-kube-scheduler
  lockObjectNamespace: kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置文件应该包含一个 KubeSchedulerConfiguration 对象，yaml文件指定了调度器的一些参数，包括leader选举，调度算法策略的选择（可以是configmaps，也可以是具体的json或者yaml文件），以及指定调度器的名称为my-kube-scheduler。&lt;/p&gt;

&lt;p&gt;这边我们还需要了解一下配置文件中指定的策略问题，也可以直接使用启动参数进行配置&amp;ndash;use-legacy-policy-config=true &amp;ndash;policy-config-file=/etc/kubernetes/scheduler-policy.config，这个是scheduler运行的关键。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat /etc/kubernetes/scheduler-policy.config
{
&amp;quot;kind&amp;quot; : &amp;quot;Policy&amp;quot;,
&amp;quot;apiVersion&amp;quot; : &amp;quot;v1&amp;quot;,
&amp;quot;predicates&amp;quot; : [
    {&amp;quot;name&amp;quot; : &amp;quot;CheckNodeUnschedulable&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;GeneralPredicates&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;NoDiskConflict&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;PodToleratesNodeTaints&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;LimitSRIOVQuantity&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;Reschedule&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;Mutex&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;AppLimit&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;HAschedule&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;CheckVolumeBinding&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;MaxOssBucketCount&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;MaxCSIVolumeCountPred&amp;quot;},
    {&amp;quot;name&amp;quot; : &amp;quot;MatchInterPodAffinity&amp;quot;}
    ],
&amp;quot;priorities&amp;quot; : [
    {&amp;quot;name&amp;quot; : &amp;quot;AppLimitPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;AppServiceSpreadPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;BalancedResourceAllocation&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;DiskIOPSPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;GpuBinpackPriority&amp;quot;, &amp;quot;weight&amp;quot; : 20},
    {&amp;quot;name&amp;quot; : &amp;quot;HAschedulerPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;ImageLocalityPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;InterPodAffinityPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;NetworkBandwidthPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;NodeAffinityPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;ReschedulePriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;SelectorSpreadPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;LeastRequestedPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
    {&amp;quot;name&amp;quot; : &amp;quot;TaintTolerationPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1}
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到配置预选和优选策略，也是调度的算法，默认有一个启动配置，可以根据需要进行修改和扩展。&lt;/p&gt;

&lt;h2 id=&#34;容器部署&#34;&gt;容器部署&lt;/h2&gt;

&lt;p&gt;1、创建kube-scheduler的配置文件和策略文件的configmap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: my-scheduler-config
  namespace: kube-system
data:
  config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1alpha1
    kind: KubeSchedulerConfiguration
    schedulerName: my-kube-scheduler
    algorithmSource:
      policy:
        configMap:
          namespace: kube-system
          name: my-scheduler-policy
    leaderElection:
      leaderElect: false
      lockObjectName: my-kube-scheduler
      lockObjectNamespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-scheduler-policy
  namespace: kube-system
data:
 policy.cfg : |
  {
    &amp;quot;kind&amp;quot; : &amp;quot;Policy&amp;quot;,
    &amp;quot;apiVersion&amp;quot; : &amp;quot;v1&amp;quot;,
    &amp;quot;predicates&amp;quot; : [
      {&amp;quot;name&amp;quot; : &amp;quot;PodFitsHostPorts&amp;quot;},
      {&amp;quot;name&amp;quot; : &amp;quot;PodFitsResources&amp;quot;},
      {&amp;quot;name&amp;quot; : &amp;quot;NoDiskConflict&amp;quot;},
      {&amp;quot;name&amp;quot; : &amp;quot;MatchNodeSelector&amp;quot;},
      {&amp;quot;name&amp;quot; : &amp;quot;HostName&amp;quot;}
    ],
    &amp;quot;priorities&amp;quot; : [
      {&amp;quot;name&amp;quot; : &amp;quot;LeastRequestedPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
      {&amp;quot;name&amp;quot; : &amp;quot;BalancedResourceAllocation&amp;quot;, &amp;quot;weight&amp;quot; : 1},
      {&amp;quot;name&amp;quot; : &amp;quot;ServiceSpreadingPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1},
      {&amp;quot;name&amp;quot; : &amp;quot;EqualPriority&amp;quot;, &amp;quot;weight&amp;quot; : 1}
    ],
    &amp;quot;extenders&amp;quot; : [{
      &amp;quot;urlPrefix&amp;quot;: &amp;quot;http://10.168.107.12:80/scheduler&amp;quot;,
      &amp;quot;filterVerb&amp;quot;: &amp;quot;predicates/always_true&amp;quot;,
      &amp;quot;prioritizeVerb&amp;quot;: &amp;quot;priorities/zero_score&amp;quot;,
      &amp;quot;preemptVerb&amp;quot;: &amp;quot;preemption&amp;quot;,
      &amp;quot;bindVerb&amp;quot;: &amp;quot;&amp;quot;,
      &amp;quot;weight&amp;quot;: 1,
      &amp;quot;enableHttps&amp;quot;: false,
      &amp;quot;nodeCacheCapable&amp;quot;: false
    }],
    &amp;quot;hardPodAffinitySymmetricWeight&amp;quot; : 10
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个名为my-scheduler-config的configmaps，该配置文件应该包含一个 KubeSchedulerConfiguration 对象，data下的config.yaml文件指定了调度器的一些参数，包括leader选举，调度算法策略的选择（指定另一个configmaps），以及指定调度器的名称为my-kube-scheduler。&lt;/p&gt;

&lt;p&gt;相应的创建一个my-scheduler-policy的configmaps，里面指定了选择哪些预选、优选策略，以及外部扩展调度程序的urlPrefix、扩展预选URI、扩展优选URI、扩展pod优先级抢占URI、扩展bind URI、扩展优选算法的权重等，可以根据需求进行调整。&lt;/p&gt;

&lt;p&gt;2、yaml文件中将configmaps：my-scheduler-config以文件的形式挂载到容器内/my-scheduler目录下，并在启动参数中指定&amp;ndash;config=/my-scheduler/config.yaml，启动kube-scheduler。&lt;/p&gt;

&lt;h2 id=&#34;高可用&#34;&gt;高可用&lt;/h2&gt;

&lt;p&gt;k8s中kube-scheuler的高可用是通过leaderElection实现的，一般三个master，哪一个先起来就是leader, 虽然两台机器上都安装了scheduler, 但是只有leader提供服务, 另外两个上面的scheduler是处于等待状态, 并没有真正运行自己的逻辑。&lt;/p&gt;

&lt;p&gt;当leader异常后，其他的scheduler服务就会成为leaeder，继续提供服务。&lt;/p&gt;

&lt;p&gt;当我们部署多个调度器的时候，每个调度器都会各自调度属于自己的pod。&lt;/p&gt;

&lt;h1 id=&#34;调度流程&#34;&gt;调度流程&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/scheduler&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;核心：待调度的pod列表、可有的合适的node列表、调度算法和策略。&lt;/p&gt;

&lt;p&gt;1、首先，客户端通过 API Server 的 REST API 或者 kubectl 工具创建 Pod 资源&lt;/p&gt;

&lt;p&gt;2、API Server 收到用户请求后，存储相关数据到 etcd 数据库中&lt;/p&gt;

&lt;p&gt;3、watch apiserver，将 spec.nodeName 为空的 Pod 放入调度器内部的调度队列中&lt;/p&gt;

&lt;p&gt;4、调度器监听 API Server 查看为调度(bind)的 Pod 列表，循环遍历地为每个 Pod 尝试分配节点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从调度队列中 Pop 出一个 Pod，开始一个标准的调度周期&lt;/li&gt;
&lt;li&gt;预选阶段(Predicates)，过滤节点，调度器用一组规则过滤掉不符合要求的 Node 节点，比如 Pod 设置了资源的 request，那么可用资源比 Pod 需要的资源少的主机显然就会被过滤掉&lt;/li&gt;
&lt;li&gt;优选阶段(Priorities)，为节点的优先级打分，将上一阶段过滤出来的 Node 列表进行打分，调度器会考虑一些整体的优化策略，比如把 Deployment 控制的多个 Pod 副本分布到不同的主机上，使用最低负载的主机等等策略&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、经过上面的阶段过滤后选择打分最高的 Node 节点和 Pod 进行 binding 操作，然后将结果存储到 etcd 中&lt;/p&gt;

&lt;p&gt;5、最后被选择出来的 Node 节点对应的 kubelet 去执行创建 Pod 的相关操作&lt;/p&gt;

&lt;h2 id=&#34;优先级调度&#34;&gt;优先级调度&lt;/h2&gt;

&lt;p&gt;Pod 的优先级，高优先级的 Pod 会优先被调度，或者在资源不足低情况牺牲低优先级的 Pod，以便于重要的 Pod 能够得到资源部署。&lt;/p&gt;

&lt;p&gt;要定义 Pod 优先级，就需要先定义PriorityClass对象，该对象没有 Namespace 的限制：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: &amp;quot;This priority class should be used for XYZ service pods only.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;value为 32 位整数的优先级，该值越大，优先级越高
globalDefault用于未配置 PriorityClassName 的 Pod，整个集群中应该只有一个PriorityClass将其设置为 true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过在 Pod 的spec.priorityClassName中指定已定义的PriorityClass名称即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外一个值得注意的是当节点没有足够的资源供调度器调度 Pod，导致 Pod 处于 pending 时，抢占（preemption）逻辑就会被触发。Preemption会尝试从一个节点删除低优先级的 Pod，从而释放资源使高优先级的 Pod 得到节点资源进行部署。&lt;/p&gt;

&lt;h2 id=&#34;预选策略&#34;&gt;预选策略&lt;/h2&gt;

&lt;p&gt;我们在部署应用的时候，如果发现有 Pod 一直处于 Pending 状态，那么就是没有满足调度条件的节点，这个时候可以去检查下节点资源是否可用。&lt;/p&gt;

&lt;h3 id=&#34;nodiskconflict-磁盘冲突&#34;&gt;NoDiskconflict：磁盘冲突&lt;/h3&gt;

&lt;p&gt;判断备选pod的gcePersistentDisk或者AWSElasticBlockStore和备选的节点中已存在的pod是否存在冲突具体检测过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先，读取备选pod的所有的volume信息，对每一个volume执行一下步骤的冲突检测&lt;/li&gt;
&lt;li&gt;如果该volume是gcePersistentDisk，则将volume和备选节点上的所有pod的每个volume进行比较，如果发现相同的gcePersistentDisk，则返回false，表明磁盘冲突，检测结束，反馈给调度器该备选节点不合适作为备选的pod，如果volume是AWSElasticBlockStore，则将volume和备选节点上的所有pod的每个volume进行比较，如果发现相同的AWSElasticBlockStore，则返回false，表明磁盘冲突，检测结束，反馈给调度器该备选节点不合适作为备选的pod&lt;/li&gt;
&lt;li&gt;最终，检查备选pod的所有的volume均为发现冲突，则返回true，表明不存在磁盘冲突，反馈给调度器该备选节点合适备选pod&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;podfistresources-资源要求&#34;&gt;podFistResources：资源要求&lt;/h3&gt;

&lt;p&gt;判断备选节点资源是否满足备选pod的需求，检测过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算备选pod和节点中已存在的pod的所有容器的需求资源（CPU 和内存）的总和&lt;/li&gt;
&lt;li&gt;获得备选节点的状态信息，其中包括节点的资源信息&lt;/li&gt;
&lt;li&gt;如果备选pod和节点中已存在pod的所有容器的需求资源（CPU和内存）的总和超出了备选节点拥有的资源，则返回false，表明备选节点不适合备选pod，否则返回true,表明备选节点适合备选pod&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;podselectormatches-标签匹配&#34;&gt;PodSelectorMatches：标签匹配&lt;/h3&gt;

&lt;p&gt;判断备选节点是否包含备选pod的标签选择器指定的标签：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果pod没有指定spec.nodeSelector标签选择器，则返回true&lt;/li&gt;
&lt;li&gt;如果获得备选节点的标签信息，判断节点是否包含备选pod的标签选择器所指的标签，如果包含返回true，不包含返回false&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;podfitshost&#34;&gt;PodFitsHost&lt;/h3&gt;

&lt;p&gt;判断备选pod的spec.nodeName域所指定的节点名称和备选节点的名称是否一致，如果一致返回true，否则返回false。&lt;/p&gt;

&lt;h3 id=&#34;podfitsports&#34;&gt;PodFitsPorts&lt;/h3&gt;

&lt;p&gt;判断备选pod所用的端口列表汇中的端口是否在备选节点中被占用，如果被占用，则返回false，否则返回true。&lt;/p&gt;

&lt;h3 id=&#34;podfitshostports&#34;&gt;PodFitsHostPorts&lt;/h3&gt;

&lt;p&gt;节点上已经使用的 port 是否和 Pod 申请的 port 冲突&lt;/p&gt;

&lt;h3 id=&#34;其他&#34;&gt;其他&lt;/h3&gt;

&lt;p&gt;Predicates过滤有一系列的算法可以使用，上面就是简单的列举几个，还有很多，比如如下等等&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CheckNodeDiskPressure：检查节点磁盘空间是否符合要求
CheckNodeMemoryPressure：检查节点内存是否够用
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多更详细的我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/predicates.go。&lt;/p&gt;

&lt;h2 id=&#34;优选策略&#34;&gt;优选策略&lt;/h2&gt;

&lt;h3 id=&#34;leastrequestedpriority&#34;&gt;leastRequestedPriority&lt;/h3&gt;

&lt;p&gt;该策略用于从备选节点列表中选出资源消耗最小的节点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算出所有备选节点上运行的pod和备选pod的CPU占用量&lt;/li&gt;
&lt;li&gt;计算出所有备选节点上运行的pod和备选pod的memory占用量&lt;/li&gt;
&lt;li&gt;根据特定的算法，计算每个节点的得分&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;calculatenodelabelpriority&#34;&gt;CalculateNodeLabelPriority&lt;/h3&gt;

&lt;p&gt;如果用户在配置中指定了该策略，则scheduler会通过registerCustomPriorityFunction方法注册该策略。该策略用于判断策略列出的标签在备选节点中存在时，是否选择该备选节点。如果备选节点的标签在优选策略的标签列表中且优选策略的presence值为true，或者备选节点的标签不在优选策略的标签列表中且优选策略的presence值为false，则备选节点score=10，否则等于0。&lt;/p&gt;

&lt;h3 id=&#34;balancedresourceallocation&#34;&gt;BalancedResourceAllocation&lt;/h3&gt;

&lt;p&gt;该优选策略用于从备选节点列表中选出各项资源使用率最均衡的节点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算出所有备选节点上运行的pod和备选pod的CPU占用量&lt;/li&gt;
&lt;li&gt;计算出所有备选节点上运行的pod和备选pod的memory占用量&lt;/li&gt;
&lt;li&gt;根据特定的算法，计算每个节点的得分&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;其他-1&#34;&gt;其他&lt;/h3&gt;

&lt;p&gt;还有很多其他的策略，比如如下等等&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SelectorSpreadPriority：为了更好的高可用，对同属于一个 Deployment 或者 RC 下面的多个 Pod 副本，尽量调度到多个不同的节点上，当一个 Pod 被调度的时候，会先去查找该 Pod 对应的 controller，然后查看该 controller 下面的已存在的 Pod，运行 Pod 越少的节点权重越高
ImageLocalityPriority：就是如果在某个节点上已经有要使用的镜像节点了，镜像总大小值越大，权重就越高
NodeAffinityPriority：这个就是根据节点的亲和性来计算一个权重值，后面我们会详细讲解亲和性的使用方法
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样我们可以查看源码文件：k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/ 了解更多信息。每一个优先级函数会返回一个0-10的分数，分数越高表示节点越优，同时每一个函数也会对应一个表示权重的值。最终主机的得分用以下公式计算得出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;finalScoreNode = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + … + (weightn * priorityFuncn)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;priority functions 集合中的每一个函数都有一个权重 (weight)，最终的值为 weight 和 priority functions 的乘积，而一个节点的 weight 就是所有 priority functions 结果的加和&lt;/p&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;p&gt;在我们实际使用中，有着很多的使用的地方，比如指定node，屏蔽node。&lt;/p&gt;

&lt;h3 id=&#34;指定-node-节点调度&#34;&gt;指定 Node 节点调度&lt;/h3&gt;

&lt;p&gt;有三种方式指定 Pod 只运行在指定的 Node 节点上&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nodeSelector：只调度到匹配指定 label 的 Node 上&lt;/li&gt;
&lt;li&gt;nodeAffinity：功能更丰富的 Node 选择器，比如支持集合操作&lt;/li&gt;
&lt;li&gt;podAffinity：调度到满足条件的 Pod 所在的 Node 上&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1、nodeSelector&lt;/p&gt;

&lt;p&gt;首先给 Node 打上标签&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl label nodes node-01 disktype=ssd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在 daemonset 中指定 nodeSelector 为 disktype=ssd：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:
  nodeSelector:
    disktype: ssd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据默认调度器的PodSelectorMatches策略选出合适的节点，然后打分进行分配。&lt;/p&gt;

&lt;p&gt;2、nodeAffinity&lt;/p&gt;

&lt;p&gt;nodeAffinity 目前支持两种：requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution，分别代表必须满足条件和优选条件。比如下面的例子代表调度到包含标签 kubernetes.io/e2e-az-name 并且值为 e2e-az1 或 e2e-az2 的 Node 上，并且优选还带有标签 another-node-label-key=another-node-label-value 的 Node。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: gcr.io/google_containers/pause:2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、podAffinity&lt;/p&gt;

&lt;p&gt;podAffinity 基于 Pod 的标签来选择 Node，仅调度到满足条件 Pod 所在的 Node 上，支持 podAffinity 和 podAntiAffinity。这个功能比较绕，以下面的例子为例：&lt;/p&gt;

&lt;p&gt;如果一个 “Node 所在 Zone 中包含至少一个带有 security=S1 标签且运行中的 Pod”，那么可以调度到该 Node&lt;/p&gt;

&lt;p&gt;不调度到 “包含至少一个带有 security=S2 标签且运行中 Pod” 的 Node 上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: kubernetes.io/hostname
  containers:
  - name: with-pod-affinity
    image: gcr.io/google_containers/pause:2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出基本上都是根据label进行调度选择。&lt;/p&gt;

&lt;h3 id=&#34;屏蔽-node-节点调度&#34;&gt;屏蔽 Node 节点调度&lt;/h3&gt;

&lt;p&gt;Taints 和 tolerations 用于保证 Pod 不被调度到不合适的 Node 上，其中 Taint 应用于 Node 上，而 toleration 则应用于 Pod 上。&lt;/p&gt;

&lt;p&gt;目前支持的 taint 类型&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NoSchedule：新的 Pod 不调度到该 Node 上，不影响正在运行的 Pod&lt;/li&gt;
&lt;li&gt;PreferNoSchedule：soft 版的 NoSchedule，尽量不调度到该 Node 上&lt;/li&gt;
&lt;li&gt;NoExecute：新的 Pod 不调度到该 Node 上，并且删除（evict）已在运行的 Pod。Pod 可以增加一个时间（tolerationSeconds），&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然而，当 Pod 的 Tolerations 匹配 Node 的所有 Taints 的时候可以调度到该 Node 上；当 Pod 是已经运行的时候，也不会被删除（evicted）。另外对于 NoExecute，如果 Pod 增加了一个 tolerationSeconds，则会在该时间之后才删除 Pod。&lt;/p&gt;

&lt;p&gt;比如，假设 node1 上应用以下几个 taint&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value1:NoExecute
kubectl taint nodes node1 key2=value2:NoSchedule
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面的这个 Pod 由于没有 toleratekey2=value2:NoSchedule 无法调度到 node1 上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tolerations:
- key: &amp;quot;key1&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;value1&amp;quot;
  effect: &amp;quot;NoSchedule&amp;quot;
- key: &amp;quot;key1&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;value1&amp;quot;
  effect: &amp;quot;NoExecute&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而正在运行且带有 tolerationSeconds 的 Pod 则会在 600s 之后删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tolerations:
- key: &amp;quot;key1&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;value1&amp;quot;
  effect: &amp;quot;NoSchedule&amp;quot;
- key: &amp;quot;key1&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;value1&amp;quot;
  effect: &amp;quot;NoExecute&amp;quot;
  tolerationSeconds: 600
- key: &amp;quot;key2&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;value2&amp;quot;
  effect: &amp;quot;NoSchedule&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，DaemonSet 创建的 Pod 会自动加上对 node.alpha.kubernetes.io/unreachable 和 node.alpha.kubernetes.io/notReady 的 NoExecute Toleration，以避免它们因此被删除。&lt;/p&gt;

&lt;h1 id=&#34;自定义调度器&#34;&gt;自定义调度器&lt;/h1&gt;

&lt;p&gt;要编写一个优秀的调度器却不容易，因为要考虑的东西很多：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;尽可能地将 workload 平均到不同的节点，减少单个节点宕机造成的损失&lt;/li&gt;
&lt;li&gt;可扩展性。随着集群规模的增加，怎么保证调度器不会成为性能的瓶颈&lt;/li&gt;
&lt;li&gt;高可用。调度器能做组成集群，任何一个调度器出现问题，不会影响整个集群的调度&lt;/li&gt;
&lt;li&gt;灵活性。不同的用户有不同的调度需求，一个优秀的调度器还要允许用户能配置不同的调度算法&lt;/li&gt;
&lt;li&gt;资源合理和高效利用。调度器应该尽可能地提高集群的资源利用率，防止资源的浪费&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般来说，我们有4种扩展 Kubernetes 调度器的方法。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;直接 clone 官方的 kube-scheduler 源代码，在合适的位置直接修改代码，然后重新编译运行修改后的程序，当然这种方法是最不建议使用的，也不实用，因为需要花费大量额外的精力来和上游的调度程序更改保持一致。&lt;/li&gt;
&lt;li&gt;和默认的调度程序一起运行独立的调度程序，默认的调度器和我们自定义的调度器可以通过 Pod 的 spec.schedulerName 来覆盖各自的 Pod，默认是使用 default 默认的调度器，但是多个调度程序共存的情况下也比较麻烦，比如当多个调度器将 Pod 调度到同一个节点的时候，可能会遇到一些问题，因为很有可能两个调度器都同时将两个 Pod 调度到同一个节点上去，但是很有可能其中一个 Pod 运行后其实资源就消耗完了，并且维护一个高质量的自定义调度程序也不是很容易的，因为我们需要全面了解默认的调度程序，整体 Kubernetes 的架构知识以及各种 Kubernetes API 对象的各种关系或限制。&lt;/li&gt;
&lt;li&gt;调度器扩展程序，这个方案目前是一个可行的方案，可以和上游调度程序兼容，所谓的调度器扩展程序其实就是一个可配置的 Webhook 而已，里面包含 过滤器 和 优先级 两个端点，分别对应调度周期中的两个主要阶段（过滤和打分）。&lt;/li&gt;
&lt;li&gt;通过调度框架（Scheduling Framework），Kubernetes v1.15 版本中引入了可插拔架构的调度框架，使得定制调度器这个任务变得更加的容易。调库框架向现有的调度器中添加了一组插件化的 API，该 API 在保持调度程序“核心”简单且易于维护的同时，使得大部分的调度功能以插件的形式存在，而且在我们现在的 v1.16 版本中上面的 调度器扩展程序 也已经被废弃了，所以以后调度框架才是自定义调度器的核心方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们可以简单介绍下后面三种方式的实现。&lt;/p&gt;

&lt;h2 id=&#34;调度器扩展程序&#34;&gt;调度器扩展程序&lt;/h2&gt;

&lt;p&gt;1、实现扩展程序&lt;/p&gt;

&lt;p&gt;我们直接用 golang 来实现一个简单的调度器扩展程序，当然你可以使用其他任何编程语言，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    router := httprouter.New()
    router.GET(&amp;quot;/&amp;quot;, Index)
    router.POST(&amp;quot;/filter&amp;quot;, Filter)
    router.POST(&amp;quot;/prioritize&amp;quot;, Prioritize)

    log.Fatal(http.ListenAndServe(&amp;quot;:8888&amp;quot;, router))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后接下来我们需要实现 /filter 和 /prioritize 两个端点的处理程序。&lt;/p&gt;

&lt;p&gt;其中 Filter 这个扩展函数接收一个输入类型为 schedulerapi.ExtenderArgs 的参数，然后返回一个类型为 *schedulerapi.ExtenderFilterResult 的值。在函数中，我们可以进一步过滤输入的节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// filter 根据扩展程序定义的预选规则来过滤节点
func filter(args schedulerapi.ExtenderArgs) *schedulerapi.ExtenderFilterResult {
    var filteredNodes []v1.Node
    failedNodes := make(schedulerapi.FailedNodesMap)
    pod := args.Pod

    for _, node := range args.Nodes.Items {
        fits, failReasons, _ := podFitsOnNode(pod, node)
        if fits {
            filteredNodes = append(filteredNodes, node)
        } else {
            failedNodes[node.Name] = strings.Join(failReasons, &amp;quot;,&amp;quot;)
        }
    }

    result := schedulerapi.ExtenderFilterResult{
        Nodes: &amp;amp;v1.NodeList{
            Items: filteredNodes,
        },
        FailedNodes: failedNodes,
        Error:       &amp;quot;&amp;quot;,
    }

    return &amp;amp;result
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在过滤函数中，我们循环每个节点然后用我们自己实现的业务逻辑来判断是否应该批准该节点，这里我们实现比较简单，在 podFitsOnNode() 函数中我们只是简单的检查随机数是否为偶数来判断即可，如果是的话我们就认为这是一个幸运的节点，否则拒绝批准该节点。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var predicatesSorted = []string{LuckyPred}

var predicatesFuncs = map[string]FitPredicate{
    LuckyPred: LuckyPredicate,
}

type FitPredicate func(pod *v1.Pod, node v1.Node) (bool, []string, error)

func podFitsOnNode(pod *v1.Pod, node v1.Node) (bool, []string, error) {
    fits := true
    var failReasons []string
    for _, predicateKey := range predicatesSorted {
        fit, failures, err := predicatesFuncs[predicateKey](pod, node)
        if err != nil {
            return false, nil, err
        }
        fits = fits &amp;amp;&amp;amp; fit
        failReasons = append(failReasons, failures...)
    }
    return fits, failReasons, nil
}

func LuckyPredicate(pod *v1.Pod, node v1.Node) (bool, []string, error) {
    lucky := rand.Intn(2) == 0
    if lucky {
        log.Printf(&amp;quot;pod %v/%v is lucky to fit on node %v\n&amp;quot;, pod.Name, pod.Namespace, node.Name)
        return true, nil, nil
    }
    log.Printf(&amp;quot;pod %v/%v is unlucky to fit on node %v\n&amp;quot;, pod.Name, pod.Namespace, node.Name)
    return false, []string{LuckyPredFailMsg}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样的打分功能用同样的方式来实现，我们在每个节点上随机给出一个分数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// it&#39;s webhooked to pkg/scheduler/core/generic_scheduler.go#PrioritizeNodes()
// 这个函数输出的分数会被添加会默认的调度器
func prioritize(args schedulerapi.ExtenderArgs) *schedulerapi.HostPriorityList {
    pod := args.Pod
    nodes := args.Nodes.Items

    hostPriorityList := make(schedulerapi.HostPriorityList, len(nodes))
    for i, node := range nodes {
        score := rand.Intn(schedulerapi.MaxPriority + 1)  // 在最大优先级内随机取一个值
        log.Printf(luckyPrioMsg, pod.Name, pod.Namespace, score)
        hostPriorityList[i] = schedulerapi.HostPriority{
            Host:  node.Name,
            Score: score,
        }
    }

    return &amp;amp;hostPriorityList
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们可以使用下面的命令来编译打包我们的应用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOOS=linux GOARCH=amd64 go build -o app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获得我们的扩展调度器扩展程序的二进制文件app，构建完成后，将应用 app 拷贝到 kube-scheduler 所在的节点直接运行即可，当然也可以使用pod运行，复制一个调度器的 YAML 文件然后更改下 schedulerName 来部署，这样就不会影响默认的调度器了，然后在需要使用这个测试的调度器的 Pod 上面指定 spec.schedulerName 即可。这就是第二种方法的多调度器。&lt;/p&gt;

&lt;p&gt;2、注册扩展程序&lt;/p&gt;

&lt;p&gt;我们只要在策略的配置文件中注册就可以格式是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Policy
extenders:
- urlPrefix: &amp;quot;http://127.0.0.1:8888/&amp;quot;
  filterVerb: &amp;quot;filter&amp;quot;
  prioritizeVerb: &amp;quot;prioritize&amp;quot;
  weight: 1
  enableHttps: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将其写到默认的policy文件中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;kind&amp;quot;: &amp;quot;Policy&amp;quot;,
    &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
    &amp;quot;predicates&amp;quot;: [{
        &amp;quot;name&amp;quot;: &amp;quot;MatchNodeSelector&amp;quot;
    }, {
        &amp;quot;name&amp;quot;: &amp;quot;PodFitsResources&amp;quot;
    }, {
        &amp;quot;name&amp;quot;: &amp;quot;PodFitsHostPorts&amp;quot;
    },{
        &amp;quot;name&amp;quot;: &amp;quot;HostName&amp;quot;
    }
    ],
    &amp;quot;priorities&amp;quot;: [{
        &amp;quot;name&amp;quot;: &amp;quot;EqualPriority&amp;quot;,
        &amp;quot;weight&amp;quot;: 2
    }, {
        &amp;quot;name&amp;quot;: &amp;quot;ImageLocalityPriority&amp;quot;,
        &amp;quot;weight&amp;quot;: 4
    }, {
        &amp;quot;name&amp;quot;: &amp;quot;LeastRequestedPriority&amp;quot;,
        &amp;quot;weight&amp;quot;: 2
    }, {
        &amp;quot;name&amp;quot;: &amp;quot;BalancedResourceAllocation&amp;quot;,
        &amp;quot;weight&amp;quot;: 2
    }
    ],
    &amp;quot;extenders&amp;quot;: [{
        &amp;quot;urlPrefix&amp;quot;: &amp;quot;http://127.0.0.1:8888/prefix&amp;quot;,
        &amp;quot;filterVerb&amp;quot;: &amp;quot;filter&amp;quot;,
        &amp;quot;prioritizeVerb&amp;quot;: &amp;quot;prioritize&amp;quot;,
        &amp;quot;weight&amp;quot;: 1,
        &amp;quot;bindVerb&amp;quot;: &amp;quot;bind&amp;quot;,
        &amp;quot;enableHttps&amp;quot;: false
    }]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动kube-scheduler就行。这时候我们在调度的是时候，是重上到下，这样在过滤和打分阶段结束后，可以将结果分别传递给该扩展程序的端点，可以进一步过滤并确定优先级，以适应我们的特定业务需求。同一个调度器就是这样，多个调度器时候，每一个调度器都是这么个重下倒下过滤调度的流程。&lt;/p&gt;

&lt;p&gt;3、验证&lt;/p&gt;

&lt;p&gt;现在我们来运行一个 Deployment 查看其工作原理，我们准备一个包含20个副本的部署 Yaml：(test-scheduler.yaml)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: pause
spec:
  replicas: 20
  selector:
    matchLabels:
      app: pause
  template:
    metadata:
      labels:
        app: pause
    spec:
      containers:
      - name: pause
        image: gcr.azk8s.cn/google_containers/pause:3.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接创建上面的资源对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kuectl apply -f test-scheduler.yaml
deployment.apps/pause created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候我们去查看下我们编写的调度器扩展程序日志：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./app
......
2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is unlucky to fit on node ydzs-node1
2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is lucky to get score 7
2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is lucky to get score 9
2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is unlucky to fit on node ydzs-node3
2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is unlucky to fit on node ydzs-node4
2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to fit on node ydzs-node1
2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to fit on node ydzs-node2
2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to get score 4
2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to get score 8
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到 Pod 调度的过程，另外默认调度程序会定期重试失败的 Pod，因此它们将一次又一次地重新传递到我们的调度扩展程序上，我们的逻辑是检查随机数是否为偶数，所以最终所有 Pod 都将处于运行状态。&lt;/p&gt;

&lt;p&gt;调度器扩展程序可能是在一些情况下可以满足我们的需求，但是他仍然有一些限制和缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通信成本：数据在默认调度程序和调度器扩展程序之间以 http（s）传输，在执行序列化和反序列化的时候有一定成本&lt;/li&gt;
&lt;li&gt;有限的扩展点：扩展程序只能在某些阶段的末尾参与，例如“ Filter”和“ Prioritize”，它们不能在任何阶段的开始或中间被调用&lt;/li&gt;
&lt;li&gt;减法优于加法：与默认调度程序传递的节点候选列表相比，我们可能有一些需求需要添加新的候选节点列表，但这是比较冒险的操作，因为不能保证新节点可以通过其他要求，所以，调度器扩展程序最好执行“减法”（进一步过滤），而不是“加法”（添加节点）&lt;/li&gt;
&lt;li&gt;缓存共享：上面只是一个简单的测试示例，但在真实的项目中，我们是需要通过查看整个集群的状态来做出调度决策的，默认调度程序可以很好地调度决策，但是无法共享其缓存，这意味着我们必须构建和维护自己的缓存&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于这些局限性，Kubernetes 调度小组就提出了上面第四种方法来进行更好的扩展，也就是调度框架（Scheduler Framework），它基本上可以解决我们遇到的所有难题，现在也已经成官方推荐的扩展方式，所以这将是以后扩展调度器的最主流的方式。&lt;/p&gt;

&lt;h2 id=&#34;多调度器&#34;&gt;多调度器&lt;/h2&gt;

&lt;p&gt;上面我们已经开发了一个调度器，我们只要将对应的二进制文件制作成镜像，然后运行在k8s上，并将这个调度器命一个名，比如my-scheduler，给后来的pod进行指定。当然可以参考&lt;a href=&#34;https://kubernetes.io/zh/docs/tasks/administer-cluster/configure-multiple-schedulers/&#34;&gt;官方文档&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;在整个集群中还可以同时运行多个调度器实例，通过podSpec.schedulerName 来选择使用哪一个调度器（默认使用内置的调度器）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  schedulerName: my-scheduler  # 选择使用自定义调度器 my-scheduler
  containers:
  - name: nginx
    image: nginx:1.10
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;调度框架&#34;&gt;调度框架&lt;/h2&gt;

&lt;p&gt;Scheduler Framework 将 Pod 的调度过程分为两步：调度和绑定。&lt;/p&gt;

&lt;p&gt;调度是为 Pod 选择一个合适的节点，而绑定则是将调度结果提交给集群。调度是顺序执行的，绑定并发执行。无论是在调度还是绑定过程中，如果发生错误或者判断 Pod 不可调度，那么 Pod 就会被重新放回调度队列，等待重新调度。&lt;/p&gt;

&lt;p&gt;调度框架定义了一组扩展点，用户可以实现扩展点定义的接口来定义自己的调度逻辑，并将扩展注册到扩展点上，调度框架在执行调度工作流时，遇到对应的扩展点时，将调用用户注册的扩展。调度框架在预留扩展点时，都是有特定的目的，有些扩展点上的扩展可以改变调度程序的决策方法，有些扩展点上的扩展只是发送一个通知。&lt;/p&gt;

&lt;p&gt;下图展示了调度框架中的调度上下文及其中的扩展点，一个扩展可以注册多个扩展点，以便可以执行更复杂的有状态的任务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/scheduler1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;做一个简单的说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;QueueSort 扩展用于对 Pod 的待调度队列进行排序，以决定先调度哪个 Pod，QueueSort 扩展本质上只需要实现一个方法 Less(Pod1, Pod2) 用于比较两个 Pod 谁更优先获得调度即可，同一时间点只能有一个 QueueSort 插件生效。&lt;/li&gt;
&lt;li&gt;Pre-filter 扩展用于对 Pod 的信息进行预处理，或者检查一些集群或 Pod 必须满足的前提条件，如果 pre-filter 返回了 error，则调度过程终止。&lt;/li&gt;
&lt;li&gt;Filter 扩展用于排除那些不能运行该 Pod 的节点，对于每一个节点，调度器将按顺序执行 filter 扩展；如果任何一个 filter 将节点标记为不可选，则余下的 filter 扩展将不会被执行。调度器可以同时对多个节点执行 filter 扩展。&lt;/li&gt;
&lt;li&gt;Post-filter 是一个通知类型的扩展点，调用该扩展的参数是 filter 阶段结束后被筛选为可选节点的节点列表，可以在扩展中使用这些信息更新内部状态，或者产生日志或 metrics 信息。&lt;/li&gt;
&lt;li&gt;Scoring 扩展用于为所有可选节点进行打分，调度器将针对每一个节点调用 Soring 扩展，评分结果是一个范围内的整数。在 normalize scoring 阶段，调度器将会把每个 scoring 扩展对具体某个节点的评分结果和该扩展的权重合并起来，作为最终评分结果。&lt;/li&gt;
&lt;li&gt;Normalize scoring 扩展在调度器对节点进行最终排序之前修改每个节点的评分结果，注册到该扩展点的扩展在被调用时，将获得同一个插件中的 scoring 扩展的评分结果作为参数，调度框架每执行一次调度，都将调用所有插件中的一个 normalize scoring 扩展一次。&lt;/li&gt;
&lt;li&gt;Reserve 是一个通知性质的扩展点，有状态的插件可以使用该扩展点来获得节点上为 Pod 预留的资源，该事件发生在调度器将 Pod 绑定到节点之前，目的是避免调度器在等待 Pod 与节点绑定的过程中调度新的 Pod 到节点上时，发生实际使用资源超出可用资源的情况。（因为绑定 Pod 到节点上是异步发生的）。这是调度过程的最后一个步骤，Pod 进入 reserved 状态以后，要么在绑定失败时触发 Unreserve 扩展，要么在绑定成功时，由 Post-bind 扩展结束绑定过程。&lt;/li&gt;
&lt;li&gt;Permit 扩展用于阻止或者延迟 Pod 与节点的绑定。Permit 扩展可以做下面三件事中的一项：

&lt;ul&gt;
&lt;li&gt;approve（批准）：当所有的 permit 扩展都 approve 了 Pod 与节点的绑定，调度器将继续执行绑定过程&lt;/li&gt;
&lt;li&gt;deny（拒绝）：如果任何一个 permit 扩展 deny 了 Pod 与节点的绑定，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展&lt;/li&gt;
&lt;li&gt;wait（等待）：如果一个 permit 扩展返回了 wait，则 Pod 将保持在 permit 阶段，直到被其他扩展 approve，如果超时事件发生，wait 状态变成 deny，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Pre-bind 扩展用于在 Pod 绑定之前执行某些逻辑。例如，pre-bind 扩展可以将一个基于网络的数据卷挂载到节点上，以便 Pod 可以使用。如果任何一个 pre-bind 扩展返回错误，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展。&lt;/li&gt;
&lt;li&gt;Bind 扩展用于将 Pod 绑定到节点上：

&lt;ul&gt;
&lt;li&gt;只有所有的 pre-bind 扩展都成功执行了，bind 扩展才会执行&lt;/li&gt;
&lt;li&gt;调度框架按照 bind 扩展注册的顺序逐个调用 bind 扩展&lt;/li&gt;
&lt;li&gt;具体某个 bind 扩展可以选择处理或者不处理该 Pod&lt;/li&gt;
&lt;li&gt;如果某个 bind 扩展处理了该 Pod 与节点的绑定，余下的 bind 扩展将被忽略&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Post-bind 是一个通知性质的扩展：

&lt;ul&gt;
&lt;li&gt;Post-bind 扩展在 Pod 成功绑定到节点上之后被动调用&lt;/li&gt;
&lt;li&gt;Post-bind 扩展是绑定过程的最后一个步骤，可以用来执行资源清理的动作&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Unreserve 是一个通知性质的扩展，如果为 Pod 预留了资源，Pod 又在被绑定过程中被拒绝绑定，则 unreserve 扩展将被调用。Unreserve 扩展应该释放已经为 Pod 预留的节点上的计算资源。在一个插件中，reserve 扩展和 unreserve 扩展应该成对出现。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个 Plugin 可以实现多个扩展点。即在一个 Plugin 中既可以实现 Filter，又可以实现 Scoring，也可以再实现 Pre-Bind，看具体需求和场景，避免了一个需求实现多个 Plugin 的情况。&lt;/p&gt;

&lt;p&gt;如果我们要实现自己的插件，必须向调度框架注册插件并完成配置，另外还必须实现扩展点接口，对应的扩展点接口我们可以在源码 pkg/scheduler/framework/v1alpha1/interface.go 文件中找到，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Plugin is the parent type for all the scheduling framework plugins.
type Plugin interface {
    Name() string
}

type QueueSortPlugin interface {
    Plugin
    Less(*PodInfo, *PodInfo) bool
}

// PreFilterPlugin is an interface that must be implemented by &amp;quot;prefilter&amp;quot; plugins.
// These plugins are called at the beginning of the scheduling cycle.
type PreFilterPlugin interface {
    Plugin
    PreFilter(pc *PluginContext, p *v1.Pod) *Status
}

// FilterPlugin is an interface for Filter plugins. These plugins are called at the
// filter extension point for filtering out hosts that cannot run a pod.
// This concept used to be called &#39;predicate&#39; in the original scheduler.
// These plugins should return &amp;quot;Success&amp;quot;, &amp;quot;Unschedulable&amp;quot; or &amp;quot;Error&amp;quot; in Status.code.
// However, the scheduler accepts other valid codes as well.
// Anything other than &amp;quot;Success&amp;quot; will lead to exclusion of the given host from
// running the pod.
type FilterPlugin interface {
    Plugin
    Filter(pc *PluginContext, pod *v1.Pod, nodeName string) *Status
}

// PostFilterPlugin is an interface for Post-filter plugin. Post-filter is an
// informational extension point. Plugins will be called with a list of nodes
// that passed the filtering phase. A plugin may use this data to update internal
// state or to generate logs/metrics.
type PostFilterPlugin interface {
    Plugin
    PostFilter(pc *PluginContext, pod *v1.Pod, nodes []*v1.Node, filteredNodesStatuses NodeToStatusMap) *Status
}

// ScorePlugin is an interface that must be implemented by &amp;quot;score&amp;quot; plugins to rank
// nodes that passed the filtering phase.
type ScorePlugin interface {
    Plugin
    Score(pc *PluginContext, p *v1.Pod, nodeName string) (int, *Status)
}

// ScoreWithNormalizePlugin is an interface that must be implemented by &amp;quot;score&amp;quot;
// plugins that also need to normalize the node scoring results produced by the same
// plugin&#39;s &amp;quot;Score&amp;quot; method.
type ScoreWithNormalizePlugin interface {
    ScorePlugin
    NormalizeScore(pc *PluginContext, p *v1.Pod, scores NodeScoreList) *Status
}

// ReservePlugin is an interface for Reserve plugins. These plugins are called
// at the reservation point. These are meant to update the state of the plugin.
// This concept used to be called &#39;assume&#39; in the original scheduler.
// These plugins should return only Success or Error in Status.code. However,
// the scheduler accepts other valid codes as well. Anything other than Success
// will lead to rejection of the pod.
type ReservePlugin interface {
    Plugin
    Reserve(pc *PluginContext, p *v1.Pod, nodeName string) *Status
}

// PreBindPlugin is an interface that must be implemented by &amp;quot;prebind&amp;quot; plugins.
// These plugins are called before a pod being scheduled.
type PreBindPlugin interface {
    Plugin
    PreBind(pc *PluginContext, p *v1.Pod, nodeName string) *Status
}

// PostBindPlugin is an interface that must be implemented by &amp;quot;postbind&amp;quot; plugins.
// These plugins are called after a pod is successfully bound to a node.
type PostBindPlugin interface {
    Plugin
    PostBind(pc *PluginContext, p *v1.Pod, nodeName string)
}

// UnreservePlugin is an interface for Unreserve plugins. This is an informational
// extension point. If a pod was reserved and then rejected in a later phase, then
// un-reserve plugins will be notified. Un-reserve plugins should clean up state
// associated with the reserved Pod.
type UnreservePlugin interface {
    Plugin
    Unreserve(pc *PluginContext, p *v1.Pod, nodeName string)
}

// PermitPlugin is an interface that must be implemented by &amp;quot;permit&amp;quot; plugins.
// These plugins are called before a pod is bound to a node.
type PermitPlugin interface {
    Plugin
    Permit(pc *PluginContext, p *v1.Pod, nodeName string) (*Status, time.Duration)
}

// BindPlugin is an interface that must be implemented by &amp;quot;bind&amp;quot; plugins. Bind
// plugins are used to bind a pod to a Node.
type BindPlugin interface {
    Plugin
    Bind(pc *PluginContext, p *v1.Pod, nodeName string) *Status
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们要实现对应的插件只要实现上面对应的插件接口，也可以在一个进程中实现那多个插件。&lt;/p&gt;

&lt;p&gt;对于调度框架插件的启用或者禁用，我们同样可以使用上面的 KubeSchedulerConfiguration 资源对象来进行配置。下面的例子中的配置启用了一个实现了 reserve 和 preBind 扩展点的插件，并且禁用了另外一个插件，同时为插件 foo 提供了一些配置信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration

...

plugins:
  reserve:
    enabled:
    - name: foo
    - name: bar
    disabled:
    - name: baz
  preBind:
    enabled:
    - name: foo
    disabled:
    - name: baz

pluginConfig:
- name: foo
  args: &amp;gt;
    foo插件可以解析的任意内容
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;扩展的调用顺序如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果某个扩展点没有配置对应的扩展，调度框架将使用默认插件中的扩展&lt;/li&gt;
&lt;li&gt;如果为某个扩展点配置且激活了扩展，则调度框架将先调用默认插件的扩展，再调用配置中的扩展&lt;/li&gt;
&lt;li&gt;默认插件的扩展始终被最先调用，然后按照 KubeSchedulerConfiguration 中扩展的激活 enabled 顺序逐个调用扩展点的扩展&lt;/li&gt;
&lt;li&gt;可以先禁用默认插件的扩展，然后在 enabled 列表中的某个位置激活默认插件的扩展，这种做法可以改变默认插件的扩展被调用时的顺序&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;假设默认插件 foo 实现了 reserve 扩展点，此时我们要添加一个插件 bar，想要在 foo 之前被调用，则应该先禁用 foo 再按照 bar foo 的顺序激活。示例配置如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration

...

plugins:
  reserve:
    enabled:
    - name: bar
    - name: foo
    disabled:
    - name: foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在源码目录 pkg/scheduler/framework/plugins/examples 中有几个示范插件，我们可以参照其实现方式。&lt;/p&gt;

&lt;h3 id=&#34;简单实现一个调度扩展插件&#34;&gt;简单实现一个调度扩展插件&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;注册&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实要实现一个调度框架的插件，并不难，我们只要实现对应的扩展点，然后将插件注册到调度器中即可，我们来看一下注册：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    rand.Seed(time.Now().UTC().UnixNano())

    command := app.NewSchedulerCommand(
        app.WithPlugin(sample.Name, sample.New),
    )

    logs.InitLogs()
    defer logs.FlushLogs()

    if err := command.Execute(); err != nil {
        _, _ = fmt.Fprintf(os.Stderr, &amp;quot;%v\n&amp;quot;, err)
        os.Exit(1)
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 app.WithPlugin(sample.Name, sample.New) 就是注册一个名为sample.Name的插件，接下来就是我们接下来要实现的插件，从 WithPlugin 函数的参数也可以看出我们这里的 sample.New 必须是一个 framework.PluginFactory 类型的值，而 PluginFactory 的定义就是一个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们简单看一下 WithPlugin 的函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// WithPlugin creates an Option based on plugin name and factory.
func WithPlugin(name string, factory framework.PluginFactory) Option {
    return func(registry framework.Registry) error {
        return registry.Register(name, factory)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建的就是Option 的列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Option configures a framework.Registry.
type Option func(framework.Registry) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后给NewSchedulerCommand调用，也就是main函数中的调用的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以 sample.New 实际上就是上面的这个函数，在这个函数中我们可以获取到插件中的一些数据然后进行逻辑处理即可，插件实现如下所示，我们这里只是简单获取下数据打印日志，如果你有实际需求的可以根据获取的数据就行处理即可，我们这里只是实现了 PreFilter、Filter、PreBind 三个扩展点，其他的可以用同样的方式来扩展即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 插件名称
const Name = &amp;quot;sample-plugin&amp;quot;

type Args struct {
    FavoriteColor  string `json:&amp;quot;favorite_color,omitempty&amp;quot;`
    FavoriteNumber int    `json:&amp;quot;favorite_number,omitempty&amp;quot;`
    ThanksTo       string `json:&amp;quot;thanks_to,omitempty&amp;quot;`
}

type Sample struct {
    args   *Args
    handle framework.FrameworkHandle
}

func (s *Sample) Name() string {
    return Name
}

func (s *Sample) PreFilter(pc *framework.PluginContext, pod *v1.Pod) *framework.Status {
    klog.V(3).Infof(&amp;quot;prefilter pod: %v&amp;quot;, pod.Name)
    return framework.NewStatus(framework.Success, &amp;quot;&amp;quot;)
}

func (s *Sample) Filter(pc *framework.PluginContext, pod *v1.Pod, nodeName string) *framework.Status {
    klog.V(3).Infof(&amp;quot;filter pod: %v, node: %v&amp;quot;, pod.Name, nodeName)
    return framework.NewStatus(framework.Success, &amp;quot;&amp;quot;)
}

func (s *Sample) PreBind(pc *framework.PluginContext, pod *v1.Pod, nodeName string) *framework.Status {
    if nodeInfo, ok := s.handle.NodeInfoSnapshot().NodeInfoMap[nodeName]; !ok {
        return framework.NewStatus(framework.Error, fmt.Sprintf(&amp;quot;prebind get node info error: %+v&amp;quot;, nodeName))
    } else {
        klog.V(3).Infof(&amp;quot;prebind node info: %+v&amp;quot;, nodeInfo.Node())
        return framework.NewStatus(framework.Success, &amp;quot;&amp;quot;)
    }
}

//type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)
func New(configuration *runtime.Unknown, f framework.FrameworkHandle) (framework.Plugin, error) {
    args := &amp;amp;Args{}
    if err := framework.DecodeInto(configuration, args); err != nil {
        return nil, err
    }
    klog.V(3).Infof(&amp;quot;get plugin config args: %+v&amp;quot;, args)
    return &amp;amp;Sample{
        args: args,
        handle: f,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完整代码可以前往仓库 &lt;a href=&#34;https://github.com/cnych/sample-scheduler-framework&#34;&gt;https://github.com/cnych/sample-scheduler-framework&lt;/a&gt; 获取。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;打包运行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;实现完成后，编译打包成镜像即可，然后我们就可以当成普通的应用用一个 Deployment 控制器来部署即可，由于我们需要去获取集群中的一些资源对象，所以当然需要申请 RBAC 权限，然后同样通过 &amp;ndash;config 参数来配置我们的调度器，同样还是使用一个 KubeSchedulerConfiguration 资源对象配置，可以通过 plugins 来启用或者禁用我们实现的插件，也可以通过 pluginConfig 来传递一些参数值给插件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: sample-scheduler-clusterrole
rules:
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - endpoints
      - events
    verbs:
      - create
      - get
      - update
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - pods
    verbs:
      - delete
      - get
      - list
      - watch
      - update
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - bindings
      - pods/binding
    verbs:
      - create
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - pods/status
    verbs:
      - patch
      - update
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - replicationcontrollers
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
      - extensions
    resources:
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - persistentvolumeclaims
      - persistentvolumes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - &amp;quot;storage.k8s.io&amp;quot;
    resources:
      - storageclasses
      - csinodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - &amp;quot;coordination.k8s.io&amp;quot;
    resources:
      - leases
    verbs:
      - create
      - get
      - list
      - update
  - apiGroups:
      - &amp;quot;events.k8s.io&amp;quot;
    resources:
      - events
    verbs:
      - create
      - patch
      - update
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sample-scheduler-sa
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: sample-scheduler-clusterrolebinding
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sample-scheduler-clusterrole
subjects:
- kind: ServiceAccount
  name: sample-scheduler-sa
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-config
  namespace: kube-system
data:
  scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1alpha1
    kind: KubeSchedulerConfiguration
    schedulerName: sample-scheduler
    leaderElection:
      leaderElect: true
      lockObjectName: sample-scheduler
      lockObjectNamespace: kube-system
    plugins:
      preFilter:
        enabled:
        - name: &amp;quot;sample-plugin&amp;quot;
      filter:
        enabled:
        - name: &amp;quot;sample-plugin&amp;quot;
      preBind:
        enabled:
        - name: &amp;quot;sample-plugin&amp;quot;
    pluginConfig:
    - name: &amp;quot;sample-plugin&amp;quot;
      args:
        favorite_color: &amp;quot;#326CE5&amp;quot;
        favorite_number: 7
        thanks_to: &amp;quot;thockin&amp;quot;
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-scheduler
  namespace: kube-system
  labels:
    component: sample-scheduler
spec:
  replicas: 1
  selector:
    matchLabels:
      component: sample-scheduler
  template:
    metadata:
      labels:
        component: sample-scheduler
    spec:
      serviceAccount: sample-scheduler-sa
      priorityClassName: system-cluster-critical
      volumes:
        - name: scheduler-config
          configMap:
            name: scheduler-config
      containers:
        - name: scheduler-ctrl
          image: cnych/sample-scheduler:v0.1.6
          imagePullPolicy: IfNotPresent
          args:
            - sample-scheduler-framework
            - --config=/etc/kubernetes/scheduler-config.yaml
            - --v=3
          resources:
            requests:
              cpu: &amp;quot;50m&amp;quot;
          volumeMounts:
            - name: scheduler-config
              mountPath: /etc/kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接部署上面的资源对象即可，这样我们就部署了一个名为 sample-scheduler 的调度器了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;测试&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;接下来我们可以部署一个应用来使用这个调度器进行调度：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-scheduler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-scheduler
  template:
    metadata:
      labels:
        app: test-scheduler
    spec:
      schedulerName: sample-scheduler
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里需要注意的是我们现在手动指定了一个 schedulerName 的字段，将其设置成上面我们自定义的调度器名称 sample-scheduler。&lt;/p&gt;

&lt;p&gt;我们直接创建这个资源对象，创建完成后查看我们自定义调度器的日志信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -n kube-system -l component=sample-scheduler
NAME                               READY   STATUS    RESTARTS   AGE
sample-scheduler-7c469787f-rwhhd   1/1     Running   0          13m
$ kubectl logs -f sample-scheduler-7c469787f-rwhhd -n kube-system
I0104 08:24:22.087881       1 scheduler.go:530] Attempting to schedule pod: default/test-scheduler-6d779d9465-rq2bb
I0104 08:24:22.087992       1 plugins.go:23] prefilter pod: test-scheduler-6d779d9465-rq2bb
I0104 08:24:22.088657       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-node1
I0104 08:24:22.088797       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-node2
I0104 08:24:22.088871       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-node3
I0104 08:24:22.088946       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-node4
I0104 08:24:22.088992       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-master
I0104 08:24:22.090653       1 plugins.go:36] prebind node info: &amp;amp;Node{ObjectMeta:{ydzs-node3   /api/v1/nodes/ydzs-node3 1ff6e228-4d98-4737-b6d3-30a5d55ccdc2 15466372 0 2019-11-10 09:05:09 +0000 UTC &amp;lt;nil&amp;gt; &amp;lt;nil&amp;gt; ......}
I0104 08:24:22.091761       1 factory.go:610] Attempting to bind test-scheduler-6d779d9465-rq2bb to ydzs-node3
I0104 08:24:22.104994       1 scheduler.go:667] pod default/test-scheduler-6d779d9465-rq2bb is bound successfully on node &amp;quot;ydzs-node3&amp;quot;, 5 nodes evaluated, 4 nodes were found feasible. Bound node resource: &amp;quot;Capacity: CPU&amp;lt;4&amp;gt;|Memory&amp;lt;8008820Ki&amp;gt;|Pods&amp;lt;110&amp;gt;|StorageEphemeral&amp;lt;17921Mi&amp;gt;; Allocatable: CPU&amp;lt;4&amp;gt;|Memory&amp;lt;7906420Ki&amp;gt;|Pods&amp;lt;110&amp;gt;|StorageEphemeral&amp;lt;16912377419&amp;gt;.&amp;quot;.
可以看到当我们创建完 Pod 后，在我们自定义的调度器中就出现了对应的日志，并且在我们定义的扩展点上面都出现了对应的日志，证明我们的示例成功了，也可以通过查看 Pod 的 schedulerName 来验证：

$ kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
test-scheduler-6d779d9465-rq2bb           1/1     Running   0          22m
$ kubectl get pod test-scheduler-6d779d9465-rq2bb -o yaml
......
restartPolicy: Always
schedulerName: sample-scheduler
securityContext: {}
serviceAccount: default
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在最新的 Kubernetes v1.17 版本中，Scheduler Framework 内置的预选和优选函数已经全部插件化，所以要扩展调度器我们应该掌握并理解调度框架这种方式。所以以后调度框架是必然的趋势，我们所要做的工作就是将我们开发的调度器或扩展程序移植到我们对应的调度框架中去。&lt;/p&gt;

&lt;h1 id=&#34;代码&#34;&gt;代码&lt;/h1&gt;

&lt;p&gt;kubernetes 调度器的源码位于 kubernetes/pkg/scheduler 中，大体的代码目录结构如下所示：(不同的版本目录结构可能不太一样)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubernetes/pkg/scheduler
-- scheduler.go         //调度相关的具体实现
|-- algorithm
|   |-- predicates      //节点筛选策略
|   |-- priorities      //节点打分策略
|-- algorithmprovider
|   |-- defaults         //定义默认的调度器
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 Scheduler 创建和运行的核心程序，对应的代码在 pkg/scheduler/scheduler.go，如果要查看kube-scheduler的入口程序，对应的代码在 cmd/kube-scheduler/scheduler.go。主要调度就是上面的流程。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 企业架构</title>
          <link>https://kingjcy.github.io/post/architecture/enterprise-architecture/</link>
          <pubDate>Fri, 04 Sep 2020 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/enterprise-architecture/</guid>
          <description>&lt;p&gt;重下往上：基础平台，应用&lt;/p&gt;

&lt;h1 id=&#34;基础平台&#34;&gt;基础平台&lt;/h1&gt;

&lt;p&gt;目前主要是&lt;a href=&#34;https://kingjcy.github.io/post/cloud/compute/&#34;&gt;云计算&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;iaas基础设施建设&#34;&gt;IAAS基础设施建设&lt;/h2&gt;

&lt;p&gt;其实就是服务器，已经很成熟，大部分厂商，包括阿里云都是直接一台什么配置的服务器多少钱，其实就是一台vm，当天有钱也可以卖物理机，还可以买物理机回来自己搭建数据中心。&lt;/p&gt;

&lt;h2 id=&#34;paas平台建设&#34;&gt;PAAS平台建设&lt;/h2&gt;

&lt;p&gt;PAAS平台其实是基于vm基础上一套申请机器，扩缩容的流程，也可以基于vm搭建基础应用。AWS就是典型的&lt;/p&gt;

&lt;p&gt;随着docker虚拟化技术的发展，出现了一种新型的paas平台，就是基于docker的，这个其实比较倾向于应用，利用镜像能够快速的部署应用，扩缩容，十分轻量级，不像vm部署可能需要很长的时间，所以这个就比较倾向于应用了，而不是docker的分配。&lt;/p&gt;

&lt;p&gt;其实上面讲的都是应用型的PAAS，也就是APaaS，APAAS是一种面向IT企业和机构的云计算应用开发与部署平台。APaaS主要为应用提供运行环境和数据存储，能够将本地部署的传统应用直接部署到APaaS上。容器厂商和IaaS厂商的PaaS大致为APaaS。&lt;/p&gt;

&lt;p&gt;还有一种平台访问型的PASS，也就是IPAAS，大数据厂商的PaaS实际上是属于IPaaS。&lt;/p&gt;

&lt;p&gt;IPaaS是用于集成和协同的PaaS平台，不仅可以支持与现有云服务间的连接性，而且可以以安全的方式提供企业应用的访问能力。IPaaS主要用于集成和构建复合应用。&lt;/p&gt;

&lt;p&gt;基础平台并不是所有的企业都会建设的，只有大型的企业会建设这些东西，比如阿里云，他们有自己的数据中心，并且机器并不是全年都在使用的，而小企业只要在大企业搭建的云平台的基础上进行业务中台的建设就可以
一般paas平台都是以卖自己的产品并且在自己的加上运行的为主的企业会在发展的过程中会需要发展&lt;/p&gt;

&lt;p&gt;1、对外提供saas服务，但是机器会在一段时间内有使用峰值，正常情况下不需要那么多的机器，这些企业一般都是在云上租用的服务器，所以能够快速扩缩容能够应对需求，不用的时候不租用服务器可以降低成本&lt;/p&gt;

&lt;p&gt;2、使用云平台提供的paas服务，但是随着规模的扩大，本来昂贵的paas方案，越来越高，需要自己构建一套方案&lt;/p&gt;

&lt;p&gt;所以现在以docker为核心的paas平台是核心与主流，&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/paas&#34;&gt;核心流程&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;saas服务&#34;&gt;Saas服务&lt;/h2&gt;

&lt;p&gt;用户通过标准的 Web 浏览器来使用网络上的软件。从用户角度来说，这意味着前期无需在服务器或软件许可证授权上进行投资；从供应商角度来看，与常规的软件服务模式相比，维护一个应用软件的成本要相对低廉。SaaS供应商通常是按照客户所租用的软件模块来进行收费的，因此用户可以根据需求按需订购软件应用服务，而且SaaS的供应商会负责系统的部署、升级和维护。比如我们常用的邮箱服务等。&lt;/p&gt;

&lt;p&gt;SaaS提供商对应的用户是应用软件使用的终端用户。其实和我们下面说的应用是息息相关的。&lt;/p&gt;

&lt;p&gt;现在市场上也有saas应用已经很成熟了。其实就是我们开箱即用的服务，也可以理解为运行在paas平台上的应用。但是paas确实千变万化的。&lt;/p&gt;

&lt;h1 id=&#34;应用&#34;&gt;应用&lt;/h1&gt;

&lt;p&gt;其实就是运行在基础平台上的业务系统，可以是单体系统，可以是分布式系统。比如说使用最多的购物系统和打车系统。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/shopping/&#34;&gt;购物系统&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/car/&#34;&gt;打车系统&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其实系统架构都是演进的，并不是所有的企业都要走到最后的架构，必须以业务驱动为核心，比如你的企业并不需要大并发，单体系统就可以，有的微服务的SOA架构既可以，但是像淘宝等就需要服务化，微服务。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/architecture-evolution/&#34;&gt;架构的演进&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在不同的架构思想和体系中需要考虑的问题也不一样&lt;/p&gt;

&lt;p&gt;1、单体架构&lt;/p&gt;

&lt;p&gt;也就是我们常用的前后台体系&lt;/p&gt;

&lt;p&gt;应用的开发无非就是解决如下问题,其实就是我们&lt;a href=&#34;https://kingjcy.github.io/post/architecture/architecture/#架构要素&#34;&gt;常规的架构思想&lt;/a&gt;所要解决的问题：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;安全性&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net-http/&#34;&gt;web开发&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2、&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-web-safe/&#34;&gt;安全访问&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;高并发高性能&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-context/&#34;&gt;并发控制&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2、&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、异步处理：定一个channel，以goroutine来运行，用于接受异步返回的结果&lt;/p&gt;

&lt;p&gt;4、&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrence/&#34;&gt;并发&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;5、&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-goroutinechannel/&#34;&gt;goroutine并发&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;高可用&lt;/p&gt;

&lt;p&gt;可扩展&lt;/p&gt;

&lt;p&gt;容错性&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2、第一代微服务架构&amp;ndash;SOA&lt;/p&gt;

&lt;p&gt;考虑单体架构的所有问题，以及分布式带来的&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed/&#34;&gt;分布式问题&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、第一代微服务架构&amp;ndash;大中台&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;统一抽象规划在中台，中台建设&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;作为一种组织架构模式，“中台”突出的是规划控制和协调的能力，主要是将企业总线的瓶颈转化为中台服务的相互调用，而“前台”强调的是创新和灵活多变。这是一种快速设计和迭代的方法。&lt;/p&gt;

&lt;p&gt;4、下一代微服务架构&lt;/p&gt;

&lt;p&gt;去中心化&lt;/p&gt;

&lt;h2 id=&#34;小前台&#34;&gt;小前台&lt;/h2&gt;

&lt;p&gt;灵活多变，适应很多需求&lt;/p&gt;

&lt;h2 id=&#34;大中台&#34;&gt;大中台&lt;/h2&gt;

&lt;p&gt;其实就是将一些能够统一的业务进行统一规划，所以系统的接入和流出都是标准化的操作。&lt;/p&gt;

&lt;p&gt;可见随着中台的发展，中台已经开始分为业务中台，数据中台，技术中台等，其实就是中台越来越庞大，需要每个领域进行专注，每一种中台都往平台化的方向发展，便于使用。具体中台可以看&lt;a href=&#34;https://kingjcy.github.io/post/architecture/electronic-commerce/&#34;&gt;中台建设&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;轻后台&#34;&gt;轻后台&lt;/h2&gt;

&lt;p&gt;主要的实现业务的不能抽象统一的逻辑系统还是在后台，比如ERP系统等。
在建设好的中台上进行业务处理，和服务端开发，或者平台建设。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- channel</title>
          <link>https://kingjcy.github.io/post/golang/go-channel/</link>
          <pubDate>Mon, 24 Aug 2020 14:49:00 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-channel/</guid>
          <description>&lt;p&gt;goroutine和channel是go语言的两大基石，这边主要来研究一下channel，&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-goroutinechannel/&#34;&gt;goroutine可以查看这里&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;channel核心设计思想：不是通过共享内存来通信，而是通过通信来共享内存。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;声明&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ch chan int
var m map[ string] chan bool
ch := make(chan int)
c := make(chan int, 1024)--带缓冲的channel，后面的参数是缓冲大小
c := make([]chan int, 1024)--这个是是数组
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;单向channel&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ch1 chan int // ch1是一个正常的 channel，不是单向的
var ch2 chan&amp;lt;- float64// ch2是单向channel，只用于写float64数据
var ch3 &amp;lt;-chan int // ch3 是单向channel，只用于读取int数据

ch4 := make( chan int)
ch5 := &amp;lt;-chan int(ch4) // ch5就是一个单向的读取channel
ch6 := chan&amp;lt;- int(ch4) // ch6 是一个单向的写入channel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读写&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;写 ch &amp;lt;- 1           向channel写入数据通常会导致程序阻塞，直到有其他goroutine从这个channel中读取数据
读 value := &amp;lt;-ch    如果channel之前没有写入数据，那么从channel中读取数据也会导致程序阻塞，直到channel中被写入数据为止——这两点可以用于数据同步。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关闭&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;关闭close（）----x, ok := &amp;lt;-ch，可以通过ok来判断channel是否关闭，一个非空的通道也是可以关闭的，但是通道中剩下的值仍然可以被接收到。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ok的结果和含义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- `true`：读到通道数据，不确定是否关闭，可能channel还有保存的数据，但channel已关闭。
- `false`：通道关闭，无数据读到。
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;读已经关闭的 chan 能一直读到东西，但是读到的内容根据通道内关闭前是否有元素而不同。

&lt;ul&gt;
&lt;li&gt;如果 chan 关闭前，buffer 内有元素还未读 , 会正确读到 chan 内的值，且返回的第二个 bool 值（是否读成功）为 true。&lt;/li&gt;
&lt;li&gt;如果 chan 关闭前，buffer 内有元素已经被读完，chan 内无值，接下来所有接收的值都会非阻塞直接成功，返回 channel 元素的零值，但是第二个 bool 值一直为 false。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;写已经关闭的 chan 会 panic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
)

func producer(chnl chan int) {
    for i := 0; i &amp;lt; 10; i++ {
        chnl &amp;lt;- i
    }
    close(chnl)
}

func main() {
    ch := make(chan int)
    go producer(ch)
    for {
        v, ok := &amp;lt;-ch
        if ok == false {
            break
        }
        fmt.Println(&amp;quot;Received &amp;quot;, v, ok)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;实现原理&#34;&gt;实现原理&lt;/h1&gt;

&lt;h2 id=&#34;具体实现&#34;&gt;具体实现&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hchan.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;源码位于/runtime/chan.go中(目前版本：1.11)。结构体为hchan。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type hchan struct {
    qcount   uint           // total data in the queue
    dataqsiz uint           // size of the circular queue
    buf      unsafe.Pointer // points to an array of dataqsiz elements
    elemsize uint16
    closed   uint32
    elemtype *_type // element type
    sendx    uint   // send index
    recvx    uint   // receive index
    recvq    waitq  // list of recv waiters
    sendq    waitq  // list of send waiters

    // lock protects all fields in hchan, as well as several
    // fields in sudogs blocked on this channel.
    //
    // Do not change another G&#39;s status while holding this lock
    // (in particular, do not ready a G), as this can deadlock
    // with stack shrinking.
    lock mutex
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;详细说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;环形队列&lt;/p&gt;

&lt;p&gt;chan内部实现了一个环形队列作为其缓冲区，队列的长度是创建chan时指定的。&lt;/p&gt;

&lt;p&gt;下图展示了一个可缓存6个元素的channel示意图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/channel/channel.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dataqsiz指示了队列长度为6，即可缓存6个元素；&lt;/li&gt;
&lt;li&gt;buf指向队列的内存，队列是一个环形队列，队列中还剩余两个元素；&lt;/li&gt;
&lt;li&gt;qcount表示队列中还有两个元素；&lt;/li&gt;
&lt;li&gt;sendx指示后续写入的数据存储的位置，取值[0, 6)；&lt;/li&gt;
&lt;li&gt;recvx指示从该位置读取数据, 取值[0, 6)；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;等待队列&lt;/p&gt;

&lt;p&gt;recvq和sendq分别是接收(&amp;lt;-channel)或者发送(channel &amp;lt;- xxx)的goroutine抽象出来的结构体(sudog)的队列。是个双向链表&lt;/p&gt;

&lt;p&gt;从channel读数据，如果channel缓冲区为空或者没有缓冲区，当前goroutine会被阻塞。&lt;/p&gt;

&lt;p&gt;向channel写数据，如果channel缓冲区已满或者没有缓冲区，当前goroutine会被阻塞。&lt;/p&gt;

&lt;p&gt;被阻塞的goroutine将会挂在channel的等待队列中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;因读阻塞的goroutine会被向channel写入数据的goroutine唤醒；
因写阻塞的goroutine会被从channel读数据的goroutine唤醒；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下图展示了一个没有缓冲区的channel，有几个goroutine阻塞等待读数据：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/channel/channel1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;注意，一般情况下recvq和sendq至少有一个为空。只有一个例外，那就是同一个goroutine使用select语句向channel一边写数据，一边读数据。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;类型信息&lt;/p&gt;

&lt;p&gt;一个channel只能传递一种类型的值，类型信息存储在hchan数据结构中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;elemtype代表类型，用于数据传递过程中的赋值；
elemsize代表类型大小，用于在buf中定位元素位置。
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;锁&lt;/p&gt;

&lt;p&gt;一个channel同时仅允许被一个goroutine读写，为简单起见，本章后续部分说明读写过程时不再涉及加锁和解锁。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;实现流程&#34;&gt;实现流程&lt;/h2&gt;

&lt;p&gt;下面我们来详细介绍hchan中各部分是如何使用的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;先从创建开始&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们首先创建一个channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ch := make(chan int, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建channel实际上就是在内存中实例化了一个hchan的结构体，并返回一个ch指针，我们使用过程中channel在函数之间的传递都是用的这个指针，这就是为什么函数传递中无需使用channel的指针，而直接用channel就行了，因为channel本身就是一个指针。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;向channel写数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;向一个channel中写数据简单过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果等待接收队列recvq不为空，说明缓冲区中没有数据或者没有缓冲区，此时直接从recvq取出G,并把数据写入，最后把该G唤醒，结束发送过程；&lt;/li&gt;
&lt;li&gt;如果缓冲区中有空余位置，将数据写入缓冲区，结束发送过程；&lt;/li&gt;
&lt;li&gt;如果缓冲区中没有空余位置，将待发送数据写入G，将当前G加入sendq，进入睡眠，等待被读goroutine唤醒；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单流程图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/channel/channel2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据写入buf&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一，加锁&lt;/li&gt;
&lt;li&gt;第二，把数据从goroutine中copy到“队列”中(或者从队列中copy到goroutine中）。&lt;/li&gt;
&lt;li&gt;第三，释放锁&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hchan_gif2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;从channel读数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从一个channel读数据简单过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果等待发送队列sendq不为空，且没有缓冲区，直接从sendq中取出G，把G中数据读出，最后把G唤醒，结束读取过程；&lt;/li&gt;
&lt;li&gt;如果等待发送队列sendq不为空，此时说明缓冲区已满，从缓冲区中首部读出数据，把G中数据写入缓冲区尾部，把G唤醒，结束读取过程；&lt;/li&gt;
&lt;li&gt;如果缓冲区中有数据，则从缓冲区取出数据，结束读取过程；&lt;/li&gt;
&lt;li&gt;将当前goroutine加入recvq，进入睡眠，等待被写goroutine唤醒；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单流程图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/channel/channel3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据读取buf&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一，加锁&lt;/li&gt;
&lt;li&gt;第二，把数据从goroutine中copy到“队列”中(或者从队列中copy到goroutine中）。&lt;/li&gt;
&lt;li&gt;第三，释放锁&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hchan_gif6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;至于为什么channel会使用循环链表作为缓存结构，我个人认为是在缓存列表在动态的send和recv过程中，定位当前send或者recvx的位置、选择send的和recvx的位置比较方便吧，只要顺着链表顺序一直旋转操作就好。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;关闭channel&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关闭channel时会把recvq中的G全部唤醒，本该写入G的数据位置为nil。把sendq中的G全部唤醒，但这些G会panic。&lt;/p&gt;

&lt;p&gt;除此之外，panic出现的常见场景还有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;关闭值为nil的channel&lt;/li&gt;
&lt;li&gt;关闭已经被关闭的channel&lt;/li&gt;
&lt;li&gt;向已经关闭的channel写数据&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;h2 id=&#34;在prometheus中的使用&#34;&gt;在prometheus中的使用&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// Describe simply sends the two Descs in the struct to the channel.
func (e *exporter) Describe(ch chan&amp;lt;- *prometheus.Desc) {
    metricCh := make(chan prometheus.Metric)
    doneCh := make(chan struct{})
    go func() {
        for m := range metricCh {
            ch &amp;lt;- m.Desc()
        }
        close(doneCh)
    }()

    e.Collect(metricCh)
    close(metricCh)
    &amp;lt;-doneCh
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;range取数据阻塞，等待数据传入channel&lt;/p&gt;

&lt;p&gt;donech防止程序结束，类似于控制并发等待的效果。&lt;/p&gt;

&lt;h2 id=&#34;阻塞&#34;&gt;阻塞&lt;/h2&gt;

&lt;p&gt;发送与接收默认是阻塞的。这是什么意思？当把数据发送到信道时，程序控制会在发送数据的语句处发生阻塞，直到有其它 Go 协程从信道读取到数据，才会解除阻塞。与此类似，当读取信道的数据时，如果没有其它的协程把数据写入到这个信道，那么读取过程就会一直阻塞着。&lt;/p&gt;

&lt;p&gt;信道的这种特性能够帮助 Go 协程之间进行高效的通信，不需要用到其他编程语言常见的显式锁或条件变量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
)

func hello(done chan bool) {
    fmt.Println(&amp;quot;Hello world goroutine&amp;quot;)
    done &amp;lt;- true
}
func main() {
    done := make(chan bool)
    go hello(done)
    &amp;lt;-done
    fmt.Println(&amp;quot;main function&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上述程序里，我们在第 12 行创建了一个 bool 类型的信道 done，并把 done 作为参数传递给了 hello 协程。在第 14 行，我们通过信道 done 接收数据。这一行代码发生了阻塞，除非有协程向 done 写入数据，否则程序不会跳到下一行代码。于是，这就不需要用以前的 time.Sleep 来阻止 Go 主协程退出了。&lt;/p&gt;

&lt;h2 id=&#34;死锁&#34;&gt;死锁&lt;/h2&gt;

&lt;p&gt;使用信道需要考虑的一个重点是死锁。当 Go 协程给一个信道发送数据时，照理说会有其他 Go 协程来接收数据。如果没有的话，程序就会在运行时触发 panic，形成死锁。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;有写无读（正常需要两个goroutine）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;同理，当有 Go 协程等着从一个信道接收数据时，我们期望其他的 Go 协程会向该信道写入数据，要不然程序就会触发 panic。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

func main() {
    var ch = make(chan int)
    ch &amp;lt;- 1
    println(&amp;lt;-ch)
}


运行打印结果为：

fatal error: all goroutines are asleep - deadlock!

goroutine 1 [chan send]:
main.main()
    /tmp/sandbox117018544/main.go:5 +0x60
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;死锁了，为什么会这样呢，因为ch是一个无缓冲的channel,在执行到ch &amp;lt;- 1就阻塞了当前goroutine（也就是main函数所在的goroutine）,后面打印语句根本没机会执行&lt;/p&gt;

&lt;p&gt;稍加修改即能正常运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

func main() {
    var ch = make(chan int)
    go func() {
        ch &amp;lt;- 1
        println(&amp;quot;sender&amp;quot;)
    }()
    println(&amp;lt;-ch)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为此时ch既有发送也有接收而且不在同一个goroutine里面，此时它们不会相互阻塞.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
)

func main() {
    ch := make(chan string, 2)
    ch &amp;lt;- &amp;quot;naveen&amp;quot;
    ch &amp;lt;- &amp;quot;paul&amp;quot;
    ch &amp;lt;- &amp;quot;steve&amp;quot;
    fmt.Println(&amp;lt;-ch)
    fmt.Println(&amp;lt;-ch)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的程序，我们想向容量为2的channel中写入３个字符串。程序执行到１１行时候将会被阻塞，因为此时channel缓冲区已经满了。如果没有其他goroutine从中读取数据，程序将会死锁。报错如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fatal error: all goroutines are asleep - deadlock!

goroutine 1 [chan send]:
main.main()
    /tmp/sandbox274756028/main.go:11 +0x100
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;相互读，相互等待的死锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通道1中调用了通道2，通道2中调用通道1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    c1,c2:=make(chan int),make(chan int)
    go func() {
        for  {
            select{
                case &amp;lt;-c1:
                    c2&amp;lt;-10
            }
        }
    }()
    for  {
        select{
        case &amp;lt;-c2:
            c1&amp;lt;-10
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;大体正常就是这两种情况，但是死锁的出现的情况很多，但都不外乎是争抢资源和数据通信引起。&lt;/p&gt;

&lt;h2 id=&#34;select的使用&#34;&gt;select的使用&lt;/h2&gt;

&lt;p&gt;在执行select语句的时候，运行时系统会自上而下地判断每个case中的发送或接收操作是否可以被立即执行(立即执行：意思是当前Goroutine不会因此操作而被阻塞)&lt;/p&gt;

&lt;p&gt;在 select 中使用发送操作并且有 default可以确保发送不被阻塞！如果没有 case，select 就会一直阻塞。&lt;/p&gt;

&lt;p&gt;为了便于理解，我们首先给出一个代码片段：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://talks.golang.org/2012/concurrency.slide#32
select {
case v1 := &amp;lt;-c1:
    fmt.Printf(&amp;quot;received %v from c1\n&amp;quot;, v1)
case v2 := &amp;lt;-c2:
    fmt.Printf(&amp;quot;received %v from c2\n&amp;quot;, v1)
case c3 &amp;lt;- 23:
    fmt.Printf(&amp;quot;sent %v to c3\n&amp;quot;, 23)
default:
    fmt.Printf(&amp;quot;no one was ready to communicate\n&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这段代码中，select 语句有四个 case 子语句，前两个是 receive 操作，很好理解就是channel中没有数据的时候一直等待，直到有数据直接读出来赋值，第三个是 send 操作，这个其实也是用的channel的阻塞，往channel中写数据，channel要有缓存才能写进去，满了就会一直阻塞，最后一个是默认操作。代码执行到 select 时，case 语句会按照源代码的顺序被评估，且只评估一次，评估的结果会出现下面这几种情况：&lt;/p&gt;

&lt;p&gt;除 default 外，如果只有一个 case 语句评估通过，那么就执行这个case里的语句；&lt;/p&gt;

&lt;p&gt;除 default 外，如果有多个 case 语句评估通过，那么通过伪随机的方式随机选一个；&lt;/p&gt;

&lt;p&gt;如果 default 外的 case 语句都没有通过评估，那么执行 default 里的语句；&lt;/p&gt;

&lt;p&gt;如果没有 default，那么 代码块会被阻塞，直到有一个 case 通过评估；否则一直阻塞，所以select是很实用的方式，我们在很多地方都使用：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;GO select用法详解&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;golang 的 select 就是监听 IO 操作，当 IO 操作发生时，触发相应的动作。&lt;/p&gt;

&lt;p&gt;在执行select语句的时候，运行时系统会&lt;strong&gt;自上而下&lt;/strong&gt;地判断每个case中的发送或接收操作是否可以被立即执行(立即执行：意思是当前Goroutine不会因此操作而被阻塞)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;select的用法与switch非常类似，由select开始一个新的选择块，每个选择条件由case语句来描述。与switch语句可以选择任何可使用相等比较的条件相比，select有比较多的限制，其中最大的一条限制就是每个case语句里必须是一个IO操作，确切的说，应该是一个面向channel的IO操作。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;下面这段话来自官方文档：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A &amp;quot;select&amp;quot; statement chooses which of a set of possible send or receive operations will proceed. It looks similar to a &amp;quot;switch&amp;quot; statement but with the cases all referring to communication operations.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;语法格式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select {

case SendStmt:

  //statements

case RecvStmt:

  //statements

default:

  //statements

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SendStmt : channelVariable &amp;lt;- value

RecvStmt : variable &amp;lt;-channelVariable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A case with a RecvStmt may assign the result of a RecvExpr to one or two variables, which may be declared using a short variable declaration(IdentifierList := value). The RecvExpr must be a (possibly parenthesized) receive operation(&amp;lt;-channelVariable). There can be at most one default case and it may appear anywhere in the list of cases.&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ch1 := make(chan int, 1)
ch2 := make(chan int, 1)
ch1 &amp;lt;- 1
select {
case e1 := &amp;lt;-ch1:
    //如果ch1通道成功读取数据，则执行该case处理语句
    fmt.Printf(&amp;quot;1th case is selected. e1=%v&amp;quot;, e1)
case e2 := &amp;lt;-ch2:
    //如果ch2通道成功读取数据，则执行该case处理语句
    fmt.Printf(&amp;quot;2th case is selected. e2=%v&amp;quot;, e2)
default:
    //如果上面case都没有成功，则进入default处理流程
    fmt.Println(&amp;quot;default!.&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;超时机制也可以使用上面的方式用select来实现。&lt;/p&gt;

&lt;p&gt;Execution of a &amp;ldquo;select&amp;rdquo; statement proceeds in several steps:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、For all the cases in the statement, the channel operands of receive operations and the channel and right-hand-side expressions of send statements are evaluated exactly once, in source order, upon entering the &amp;quot;select&amp;quot; statement.(所有channel表达式都会被求值、所有被发送的表达式都会被求值。求值顺序：自上而下、从左到右)

2、If one or more of the communications can proceed, a single one that can proceed is chosen via a uniform pseudo-random selection. Otherwise, if there is a default case, that case is chosen. If there is no default case, the &amp;quot;select&amp;quot; statement blocks until at least one of the communications can proceed.（如果有一个或多个IO操作可以完成，则Go运行时系统会随机的选择一个执行，否则的话，如果有default分支，则执行default分支语句，如果连default都没有，则select语句会一直阻塞，直到至少有一个IO操作可以进行）

3、Unless the selected case is the default case, the respective communication operation is executed.

4、If the selected case is a RecvStmt with a short variable declaration or an assignment, the left-hand side expressions are evaluated and the received value (or values) are assigned.

5、The statement list of the selected case is executed.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例1：select语句会一直等待，直到某个case里的IO操作可以进行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//main.go


package main
import &amp;quot;fmt&amp;quot;
import &amp;quot;time&amp;quot;
func f1(ch chan int) {
    time.Sleep(time.Second * 5)
    ch &amp;lt;- 1
}
func f2(ch chan int) {
    time.Sleep(time.Second * 10)
    ch &amp;lt;- 1
}
func main() {
    var ch1 = make(chan int)
    var ch2 = make(chan int)
    go f1(ch1)
    go f2(ch2)
    select {
    case &amp;lt;-ch1:
        fmt.Println(&amp;quot;The first case is selected.&amp;quot;)
    case &amp;lt;-ch2:
        fmt.Println(&amp;quot;The second case is selected.&amp;quot;)
    }
}
编译运行：

C:/go/bin/go.exe run test14.go [E:/project/go/proj/src/test]

The first case is selected.

成功: 进程退出代码 0.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例2：所有跟在case关键字右边的发送语句或接收语句中的通道表达式和元素表达式都会先被求值。无论它们所在的case是否有可能被选择都会这样。&lt;/p&gt;

&lt;p&gt;求值顺序：自上而下、从左到右&lt;/p&gt;

&lt;p&gt;此示例使用空值channel进行验证。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//main.go

package main
import (
    &amp;quot;fmt&amp;quot;
)
//定义几个变量，其中chs和numbers分别代表通道列表和整数列表
var ch1 chan int
var ch2 chan int
var chs = []chan int{ch1, ch2}
var numbers = []int{1, 2, 3, 4, 5}
func main() {
    select {
    case getChan(0) &amp;lt;- getNumber(2):
        fmt.Println(&amp;quot;1th case is selected.&amp;quot;)
    case getChan(1) &amp;lt;- getNumber(3):
        fmt.Println(&amp;quot;2th case is selected.&amp;quot;)
    default:
        fmt.Println(&amp;quot;default!.&amp;quot;)
    }
}
func getNumber(i int) int {
    fmt.Printf(&amp;quot;numbers[%d]\n&amp;quot;, i)
    return numbers[i]
}
func getChan(i int) chan int {
    fmt.Printf(&amp;quot;chs[%d]\n&amp;quot;, i)
    return chs[i]
}
编译运行：
C:/go/bin/go.exe run test4.go [E:/project/go/proj/src/test]

chs[0]

numbers[2]

chs[1]

numbers[3]

default!.

成功: 进程退出代码 0.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的案例，之所以输出default!.是因为chs[0]和chs[1]都是空值channel，和空值channel通信永远都不会成功。&lt;/p&gt;

&lt;p&gt;此示例使用非空值channel进行验证。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//main.go

package main
import (
    &amp;quot;fmt&amp;quot;
)
//定义几个变量，其中chs和numbers分别代表通道列表和整数列表
var ch1 chan int = make(chan int, 1)  //声明并初始化channel变量
var ch2 chan int = make(chan int, 1)  //声明并初始化channel变量
var chs = []chan int{ch1, ch2}
var numbers = []int{1, 2, 3, 4, 5}
func main() {
    select {
    case getChan(0) &amp;lt;- getNumber(2):
        fmt.Println(&amp;quot;1th case is selected.&amp;quot;)
    case getChan(1) &amp;lt;- getNumber(3):
        fmt.Println(&amp;quot;2th case is selected.&amp;quot;)
    default:
        fmt.Println(&amp;quot;default!.&amp;quot;)
    }
}
func getNumber(i int) int {
    fmt.Printf(&amp;quot;numbers[%d]\n&amp;quot;, i)
    return numbers[i]
}
func getChan(i int) chan int {
    fmt.Printf(&amp;quot;chs[%d]\n&amp;quot;, i)
    return chs[i]
}
编译运行：
C:/go/bin/go.exe run test4.go [E:/project/go/proj/src/test]

chs[0]

numbers[2]

chs[1]

numbers[3]

1th case is selected.

成功: 进程退出代码 0.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此示例，使用非空值channel进行IO操作，所以可以成功，没有走default分支。&lt;/p&gt;

&lt;p&gt;示例4：如果有多个case同时可以运行，go会随机选择一个case执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//main.go
package main
import (
    &amp;quot;fmt&amp;quot;
)
func main() {
    chanCap := 5
    ch := make(chan int, chanCap) //创建channel，容量为5
    for i := 0; i &amp;lt; chanCap; i++ { //通过for循环，向channel里填满数据
        select { //通过select随机的向channel里追加数据
        case ch &amp;lt;- 1:
        case ch &amp;lt;- 2:
        case ch &amp;lt;- 3:
        }
    }
    for i := 0; i &amp;lt; chanCap; i++ {
        fmt.Printf(&amp;quot;%v\n&amp;quot;, &amp;lt;-ch)
    }
}
编译运行：
C:/go/bin/go.exe run test5.go [E:/project/go/proj/src/test]



成功: 进程退出代码 0.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：上面的案例每次运行结果都不一样。&lt;/p&gt;

&lt;p&gt;示例5：使用break终止select语句的执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import &amp;quot;fmt&amp;quot;
func main() {
    var ch = make(chan int, 1)
    ch &amp;lt;- 1
    select {
    case &amp;lt;-ch:
        fmt.Println(&amp;quot;This case is selected.&amp;quot;)
        break //The following statement in this case will not execute.
        fmt.Println(&amp;quot;After break statement&amp;quot;)
    default:
        fmt.Println(&amp;quot;This is the default case.&amp;quot;)
    }
    fmt.Println(&amp;quot;After select statement.&amp;quot;)
}

编译运行：

C:/go/bin/go.exe run test15.go [E:/project/go/proj/src/test]

This case is selected.

After select statement.

成功: 进程退出代码 0.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;关于带缓冲的channel&#34;&gt;关于带缓冲的channel&lt;/h2&gt;

&lt;p&gt;带缓冲的channel是我们经常作为队列使用的，不带缓冲的一般都是作为信号使用，是阻塞的，带缓冲的在满了之前是非阻塞的，满了也是阻塞性的&lt;/p&gt;

&lt;p&gt;创建一个带缓冲区的channel需要一个额外的参数容量来表明缓冲区大小：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ch := make(chan type, capacity)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中的 capacity　需要大于０，如果等于０的话则是之前学习的无缓冲区channel。&lt;/p&gt;

&lt;p&gt;例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
)


func main() {
    ch := make(chan string, 2)
    ch &amp;lt;- &amp;quot;naveen&amp;quot;
    ch &amp;lt;- &amp;quot;paul&amp;quot;
    fmt.Println(&amp;lt;- ch)
    fmt.Println(&amp;lt;- ch)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的例子中，我们创建了一个容量为2的channel，所以在写入２个字符串之前的写操作不会被阻塞。&lt;/p&gt;

&lt;p&gt;我们再来看一个例子，我们在并发执行的goroutine中进行写操作，然后在main goroutine中读取，这个例子帮助我们更好的理解缓冲区channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;time&amp;quot;
)

func write(ch chan int) {
    for i := 0; i &amp;lt; 5; i++ {
        ch &amp;lt;- i
        fmt.Println(&amp;quot;successfully wrote&amp;quot;, i, &amp;quot;to ch&amp;quot;)
    }
    close(ch)
}
func main() {
    ch := make(chan int, 2)
    go write(ch)
    time.Sleep(2 * time.Second)
    for v := range ch {
        fmt.Println(&amp;quot;read value&amp;quot;, v,&amp;quot;from ch&amp;quot;)
        time.Sleep(2 * time.Second)

    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的代码，我们创建了一个容量是2的缓冲区channel，并把它作为参数传递给write函数，接下来sleep2秒钟。同时write函数并发的执行，在函数中使用for循环向ch写入0-4。由于容量是2，所以可以立即向channel中写入0和１，然后阻塞等待至少一个值被读取。所以程序会立即输出下面２行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;successfully wrote 0 to ch
successfully wrote 1 to ch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当main函数中sleep２秒后，进入for range循环中开始读取数据，然后继续sleep２秒钟。所以程序接下来会输出:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;read value 0 from ch
successfully wrote 2 to ch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如此循环直到channel被关闭为止，程序最终输出结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;successfully wrote 0 to ch
successfully wrote 1 to ch
read value 0 from ch
successfully wrote 2 to ch
read value 1 from ch
successfully wrote 3 to ch
read value 2 from ch
successfully wrote 4 to ch
read value 3 from ch
read value 4 from ch
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;长度和容量&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;容量是指一个有缓冲区的channel能够最多同时存储多少数据，这个值在使用make关键字用在创建channel时。而长度则是指当前channel中已经存放了多少个数据。我们看下面的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
)

func main() {
    ch := make(chan string, 3)
    ch &amp;lt;- &amp;quot;naveen&amp;quot;
    ch &amp;lt;- &amp;quot;paul&amp;quot;
    fmt.Println(&amp;quot;capacity is&amp;quot;, cap(ch))
    fmt.Println(&amp;quot;length is&amp;quot;, len(ch))
    fmt.Println(&amp;quot;read value&amp;quot;, &amp;lt;-ch)
    fmt.Println(&amp;quot;new length is&amp;quot;, len(ch))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的代码中我们创建了一个容量为3的channel，然后向里面写入2个字符串，因此现在channel的长度是２。接下来从channel中读取１个字符串，所以现在长度是１。程序输出如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;capacity is 3
length is 2
read value naveen
new length is 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;有状态的channel&#34;&gt;有状态的channel&lt;/h2&gt;

&lt;p&gt;我们正常在使用channel的时候，希望一个全局使用一个channel来分配到对应的goroutine中去，这个时候就需要使用到有状态的channel&lt;/p&gt;

&lt;p&gt;正常有状态的channel都是使用map的k来标识，通常使用如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;

func main() {
    things := make(map[string](chan int))

    things[&amp;quot;stuff&amp;quot;] = make(chan int, 2)
    things[&amp;quot;stuff&amp;quot;] &amp;lt;- 2
    mything := &amp;lt;-things[&amp;quot;stuff&amp;quot;]
    fmt.Printf(&amp;quot;my thing: %d&amp;quot;, mything)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;吞吐极限&#34;&gt;吞吐极限&lt;/h2&gt;

&lt;p&gt;吞吐极限10,000,000(1千万)&lt;/p&gt;

&lt;h2 id=&#34;异步接受返回结果&#34;&gt;异步接受返回结果&lt;/h2&gt;

&lt;p&gt;针对来一个请求，启动一个groutine来处理，需要拿到返回结果的或者错误结果的。&lt;/p&gt;

&lt;p&gt;你需要预先创建一个用于处理返回值的公共管道. 然后定义一个一直在读取该管道的函数, 该函数需要预先以单独的goroutine形式启动.&lt;/p&gt;

&lt;p&gt;最后当执行到并发任务时, 每个并发任务得到结果后, 都会将结果通过管道传递到之前预先启动的goroutine中.&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控系列---- log</title>
          <link>https://kingjcy.github.io/post/monitor/log/log-scheme/</link>
          <pubDate>Thu, 13 Aug 2020 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/log-scheme/</guid>
          <description>&lt;p&gt;日志是设备或者程序对自身状态和运作行为的记录，日志监控平台是包括日志采集，存储，分析，索引查询，告警以及各种流程管理的一站式日志服务，日志监控是监控体系中核心的建设，而且可以说是量最大的一项监控。&lt;/p&gt;

&lt;h1 id=&#34;日志&#34;&gt;日志&lt;/h1&gt;

&lt;p&gt;日志是设备或者程序对自身状态和运作行为的记录。日志记录了事件，通过日志就可以看到设备和程序运行的历史信息，通过这些信息，可以了解设备和程序运行情况的变化，以更好的对于设备和程序进行维护。主要是在系统出现问题的时候，通过对于运行过程中发生的历史事件，可以查找问题出现的原因。&lt;/p&gt;

&lt;p&gt;我们可以通过下图来对日志有一个直观的概念&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/log/log&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;日志平台&#34;&gt;日志平台&lt;/h1&gt;

&lt;p&gt;业内最常见的日志采集方案就是 ELK，在 ELK 出来之前，日志管理基本上都是通过登陆日志所在机器然后使用 Linux 命令或人为查看和统计 ，这样是非常没有效率的。&lt;/p&gt;

&lt;h2 id=&#34;架构&#34;&gt;架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/log/log1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这是一个最简化版的日志收集架构，很多基于ELK的日志架构是从它演化而来，比如中加上kafka等队列缓存，核心的问题就是日志数据都保存到ElasticSearch中。其实核心的是四大模块&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据采集模块：负责从各节点上实时采集数据，建议选用filebeat来实现。&lt;/li&gt;
&lt;li&gt;数据接入模块：由于采集数据的速度和数据处理的速度不一定同步，因此添加一个消息中间件来作为缓冲，建议选用Kafka来实现。&lt;/li&gt;
&lt;li&gt;存储计算模块：对采集到的数据进行实时存储分析，建议选用ES来实现。&lt;/li&gt;
&lt;li&gt;数据输出模块：对分析后的结果展示，一般使用kibana。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;采集&#34;&gt;采集&lt;/h2&gt;

&lt;p&gt;在日志采集方面，可以说是有很多项目的支持，从以一开始的logstash，Rsyslog到后来Flume，Fluentd，Filebeat等。采集越来越倾向于轻量级，性能越来越高。容器日志采集和寻常的采集也不一样，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/collect-scheme/&#34;&gt;不同的方案&lt;/a&gt;有不同的适用场景。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.02.20&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;今天调研了grafana推出的loki也是处理日志,这边采集是promtail，具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/loki/loki/&#34;&gt;loki调研&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;存储&#34;&gt;存储&lt;/h2&gt;

&lt;p&gt;在日志存储索引查询方面，目前只有&lt;a href=&#34;https://kingjcy.github.io/post/database/elasticsearch/&#34;&gt;ES&lt;/a&gt;一个核心技术站，并没有过多的选择。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.02.20&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;今天调研了grafana推出的loki也是处理日志，借鉴了prometheus的label和metrics理念，通过label完成检索，具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/loki/loki/&#34;&gt;loki调研&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;展示&#34;&gt;展示&lt;/h2&gt;

&lt;p&gt;在数据展示报表方面，目前对日志也没有什么选择，只有kibana。Kibana主要负责读取ElasticSearch中的数据，并进行可视化展示。并且，它还自带Tool，可以方便调用ElasticSearch的Rest API。在日志平台中，我们通过Kibana查看日志。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- Filebeat原理</title>
          <link>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/</link>
          <pubDate>Sat, 08 Aug 2020 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/</guid>
          <description>&lt;p&gt;Filebeat 是使用 Golang 实现的轻量型日志采集器，也是 Elasticsearch stack 里面的一员。本质上是一个 agent，可以安装在各个节点上，根据配置读取对应位置的日志，并上报到相应的地方去。&lt;/p&gt;

&lt;p&gt;filebeat源码归属于beats项目，而beats项目的设计初衷是为了采集各类的数据，所以beats抽象出了一个libbeat库，基于libbeat我们可以快速的开发实现一个采集的工具，除了filebeat，还有像metricbeat、packetbeat等官方的项目也是在beats工程中。libbeat已经实现了内存缓存队列memqueue、几种output日志发送客户端，数据的过滤处理processor,配置解析、日志打印、事件处理和发送等通用功能，而filebeat只需要实现日志文件的读取等和日志相关的逻辑即可。&lt;/p&gt;

&lt;h1 id=&#34;beats&#34;&gt;beats&lt;/h1&gt;

&lt;p&gt;对于任一种beats来说，主要逻辑都包含两个部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;收集数据并转换成事件&lt;/li&gt;
&lt;li&gt;发送事件到指定的输出&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中第二点已由libbeat实现，因此各个beats实际只需要关心如何收集数据并生成事件后发送给libbeat的Publisher。&lt;/p&gt;

&lt;h1 id=&#34;filebeat整体架构&#34;&gt;filebeat整体架构&lt;/h1&gt;

&lt;h2 id=&#34;架构图&#34;&gt;架构图&lt;/h2&gt;

&lt;p&gt;下图是 Filebeat 官方提供的架构图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/filebeat.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;下图是看代码的一些模块组合&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/filebeat1.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实我个人觉得这一幅图是最形象的说明了filebeat的功能&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/filebeat2.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;模块&#34;&gt;模块&lt;/h2&gt;

&lt;p&gt;除了图中提到的各个模块，整个 filebeat 主要包含以下重要模块：&lt;/p&gt;

&lt;p&gt;1.filebeat主要模块&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Crawler: 负责管理和启动各个Input,管理所有Input收集数据并发送事件到libbeat的Publisher
Input: 负责管理和解析输入源的信息，以及为每个文件启动 Harvester。可由配置文件指定输入源信息。
    Harvester: 负责读取一个文件的数据,对应一个输入源，是收集数据的实际工作者。配置中，一个具体的Input可以包含多个输入源（Harvester）
module: 简化了一些常见程序日志（比如nginx日志）收集、解析、可视化（kibana dashboard）配置项
    fileset: module下具体的一种Input定义（比如nginx包括access和error log），包含：
        1）输入配置；
        2）es ingest node pipeline定义；
        3）事件字段定义；
        4）示例kibana dashboard
Registrar：接收libbeat反馈回来的ACK, 作相应的持久化，管理记录每个文件处理状态，包括偏移量、文件名等信息。当 Filebeat 启动时，会从 Registrar 恢复文件处理状态。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.libbeat主要模块&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Pipeline（publisher）: 负责管理缓存、Harvester 的信息写入以及 Output 的消费等，是 Filebeat 最核心的组件。
    client: 提供Publish接口让filebeat将事件发送到Publisher。在发送到队列之前，内部会先调用processors（包括input 内部的processors和全局processors）进行处理。
    processor: 事件处理器，可对事件按照配置中的条件进行各种处理（比如删除事件、保留指定字段，过滤添加字段，多行合并等）。配置项
    queue: 事件队列，有memqueue（基于内存）和spool（基于磁盘文件）两种实现。配置项
    outputs: 事件的输出端，比如ES、Logstash、kafka等。配置项
    acker: 事件确认回调，在事件发送成功后进行回调
autodiscover：用于自动发现容器并将其作为输入源
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;filebeat 的整个生命周期，几个组件共同协作，完成了日志从采集到上报的整个过程。&lt;/p&gt;

&lt;h1 id=&#34;基本原理-源码解析&#34;&gt;基本原理（源码解析）&lt;/h1&gt;

&lt;h2 id=&#34;文件目录组织&#34;&gt;文件目录组织&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;├── autodiscover        # 包含filebeat的autodiscover适配器（adapter），当autodiscover发现新容器时创建对应类型的输入
├── beater              # 包含与libbeat库交互相关的文件
├── channel             # 包含filebeat输出到pipeline相关的文件
├── config              # 包含filebeat配置结构和解析函数
├── crawler             # 包含Crawler结构和相关函数
├── fileset             # 包含module和fileset相关的结构
├── harvester           # 包含Harvester接口定义、Reader接口及实现等
├── input               # 包含所有输入类型的实现（比如: log, stdin, syslog）
├── inputsource         # 在syslog输入类型中用于读取tcp或udp syslog
├── module              # 包含各module和fileset配置
├── modules.d           # 包含各module对应的日志路径配置文件，用于修改默认路径
├── processor           # 用于从容器日志的事件字段source中提取容器id
├── prospector          # 包含旧版本的输入结构Prospector，现已被Input取代
├── registrar           # 包含Registrar结构和方法
└── util                # 包含beat事件和文件状态的通用结构Data
└── ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这些目录中还有一些重要的文件&lt;/p&gt;

&lt;p&gt;/beater：包含与libbeat库交互相关的文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acker.go: 包含在libbeat设置的ack回调函数，事件成功发送后被调用
channels.go: 包含在ack回调函数中被调用的记录者（logger），包括：
    registrarLogger: 将已确认事件写入registrar运行队列
    finishedLogger: 统计已确认事件数量
filebeat.go: 包含实现了beater接口的filebeat结构，接口函数包括：
    New：创建了filebeat实例
    Run：运行filebeat
    Stop: 停止filebeat运行
signalwait.go：基于channel实现的等待函数，在filebeat中用于：
    等待fileebat结束
    等待确认事件被写入registry文件
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/channel：filebeat输出（到pipeline）相关的文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;factory.go: 包含OutletFactory，用于创建输出器Outleter对象
interface.go: 定义输出接口Outleter
outlet.go: 实现Outleter，封装了libbeat的pipeline client，其在harvester中被调用用于将事件发送给pipeline
util.go: 定义ack回调的参数结构data，包含beat事件和文件状态
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/input：包含Input接口及各种输入类型的Input和Harvester实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input：对应配置中的一个Input项，同个Input下可包含多个输入源（比如文件）
Harvester：每个输入源对应一个Harvester，负责实际收集数据、并发送事件到pipeline
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/harvester：包含Harvester接口定义、Reader接口及实现等&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;forwarder.go: Forwarder结构（包含outlet）定义，用于转发事件
harvester.go: Harvester接口定义，具体实现则在/input目录下
registry.go: Registry结构，用于在Input中管理多个Harvester（输入源）的启动和停止
source.go: Source接口定义，表示输入源。目前仅有Pipe一种实现（包含os.File），用在log、stdin和docker输入类型中。btw，这三种输入类型都是用的log input的实现。
/reader目录: Reader接口定义和各种Reader实现
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;重要数据结构&#34;&gt;重要数据结构&lt;/h2&gt;

&lt;p&gt;beats通用事件结构(libbeat/beat/event.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Event struct {
    Timestamp time.Time     // 收集日志时记录的时间戳，对应es文档中的@timestamp字段
    Meta      common.MapStr // meta信息，outpus可选的将其作为事件字段输出。比如输出为es且指定了pipeline时，其pipeline id就被包含在此字段中
    Fields    common.MapStr // 默认输出字段定义在field.yml，其他字段可以在通过fields配置项指定
    Private   interface{} // for beats private use
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Crawler(filebeat/crawler/crawler.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Crawler 负责抓取日志并发送到libbeat pipeline
type Crawler struct {
    inputs          map[uint64]*input.Runner // 包含所有输入的runner
    inputConfigs    []*common.Config
    out             channel.Factory
    wg              sync.WaitGroup
    InputsFactory   cfgfile.RunnerFactory
    ModulesFactory  cfgfile.RunnerFactory
    modulesReloader *cfgfile.Reloader
    inputReloader   *cfgfile.Reloader
    once            bool
    beatVersion     string
    beatDone        chan struct{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;log类型Input(filebeat/input/log/input.go)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Input contains the input and its config
type Input struct {
    cfg           *common.Config
    config        config
    states        *file.States
    harvesters    *harvester.Registry   // 包含Input所有Harvester
    outlet        channel.Outleter      // Input共享的Publisher client
    stateOutlet   channel.Outleter
    done          chan struct{}
    numHarvesters atomic.Uint32
    meta          map[string]string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;log类型Harvester(filebeat/input/log/harvester.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Harvester struct {
    id     uuid.UUID
    config config
    source harvester.Source // the source being watched

    // shutdown handling
    done     chan struct{}
    stopOnce sync.Once
    stopWg   *sync.WaitGroup
    stopLock sync.Mutex

    // internal harvester state
    state  file.State
    states *file.States
    log    *Log

    // file reader pipeline
    reader          reader.Reader
    encodingFactory encoding.EncodingFactory
    encoding        encoding.Encoding

    // event/state publishing
    outletFactory OutletFactory
    publishState  func(*util.Data) bool

    onTerminate func()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Registrar(filebeat/registrar/registrar.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Registrar struct {
    Channel      chan []file.State
    out          successLogger
    done         chan struct{}
    registryFile string      // Path to the Registry File
    fileMode     os.FileMode // Permissions to apply on the Registry File
    wg           sync.WaitGroup

    states               *file.States // Map with all file paths inside and the corresponding state
    gcRequired           bool         // gcRequired is set if registry state needs to be gc&#39;ed before the next write
    gcEnabled            bool         // gcEnabled indictes the registry contains some state that can be gc&#39;ed in the future
    flushTimeout         time.Duration
    bufferedStateUpdates int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;libbeat Pipeline(libbeat/publisher/pipeline/pipeline.go)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Pipeline struct {
    beatInfo beat.Info

    logger *logp.Logger
    queue  queue.Queue
    output *outputController

    observer observer

    eventer pipelineEventer

    // wait close support
    waitCloseMode    WaitCloseMode
    waitCloseTimeout time.Duration
    waitCloser       *waitCloser

    // pipeline ack
    ackMode    pipelineACKMode
    ackActive  atomic.Bool
    ackDone    chan struct{}
    ackBuilder ackBuilder // pipelineEventsACK
    eventSema  *sema

    processors pipelineProcessors
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;启动&#34;&gt;启动&lt;/h2&gt;

&lt;p&gt;filebeat启动流程图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/f1.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每个 beat 的构建是独立的。从 filebeat 的入口文件filebeat/main.go可以看到，它向libbeat传递了名字、版本和构造函数来构造自身。跟着走到libbeat/beater/beater.go，我们可以看到程序的启动时的主要工作都是在这里完成的，包括命令行参数的处理、通用配置项的解析，以及最为重要的：调用象征一个beat的生命周期的若干方法&lt;/p&gt;

&lt;p&gt;我们来看filebeat的启动过程。&lt;/p&gt;

&lt;p&gt;1、执行root命令&lt;/p&gt;

&lt;p&gt;在filebeat/main.go文件中，main函数调用了cmd.RootCmd.Execute()，而RootCmd则是在cmd/root.go中被init函数初始化，其中就注册了filebeat.go:New函数以创建实现了beater接口的filebeat实例&lt;/p&gt;

&lt;p&gt;对于任意一个beats来说，都需要有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实现Beater接口的具体Beater（如Filebeat）;&lt;/li&gt;
&lt;li&gt;创建该具体Beater的(New)函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;beater接口定义（beat/beat.go）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Beater interface {
    // The main event loop. This method should block until signalled to stop by an
    // invocation of the Stop() method.
    Run(b *Beat) error

    // Stop is invoked to signal that the Run method should finish its execution.
    // It will be invoked at most once.
    Stop()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、初始化和运行Filebeat&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建libbeat/cmd/instance/beat.go:Beat结构&lt;/li&gt;
&lt;li&gt;执行(*Beat).launch方法

&lt;ul&gt;
&lt;li&gt;(*Beat).Init() 初始化Beat：加载beats公共config&lt;/li&gt;
&lt;li&gt;(*Beat).createBeater&lt;/li&gt;
&lt;li&gt;registerTemplateLoading: 当输出为es时，注册加载es模板的回调函数&lt;/li&gt;
&lt;li&gt;pipeline.Load: 创建Pipeline：包含队列、事件处理器、输出等&lt;/li&gt;
&lt;li&gt;setupMetrics: 安装监控&lt;/li&gt;
&lt;li&gt;filebeat.New: 解析配置(其中输入配置包括配置文件中的Input和module Input)等&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;loadDashboards 加载kibana dashboard&lt;/li&gt;
&lt;li&gt;(*Filebeat).Run: 运行filebeat&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、Filebeat运行&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;设置加载es pipeline的回调函数&lt;/li&gt;
&lt;li&gt;初始化registrar和crawler&lt;/li&gt;
&lt;li&gt;设置事件完成的回调函数&lt;/li&gt;
&lt;li&gt;启动Registrar、启动Crawler、启动Autodiscover&lt;/li&gt;
&lt;li&gt;等待filebeat运行结束&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们再重代码看一下这个启动过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;main.go

    package main
    import (
        &amp;quot;os&amp;quot;
        &amp;quot;github.com/elastic/beats/filebeat/cmd&amp;quot;
    )
    func main() {
        if err := cmd.RootCmd.Execute(); err != nil {
            os.Exit(1)
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入到filebeat/cmd执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package cmd

import (
    &amp;quot;flag&amp;quot;

    &amp;quot;github.com/spf13/pflag&amp;quot;

    &amp;quot;github.com/elastic/beats/filebeat/beater&amp;quot;

    cmd &amp;quot;github.com/elastic/beats/libbeat/cmd&amp;quot;
)

// Name of this beat
var Name = &amp;quot;filebeat&amp;quot;

// RootCmd to handle beats cli
var RootCmd *cmd.BeatsRootCmd

func init() {
    var runFlags = pflag.NewFlagSet(Name, pflag.ExitOnError)
    runFlags.AddGoFlag(flag.CommandLine.Lookup(&amp;quot;once&amp;quot;))
    runFlags.AddGoFlag(flag.CommandLine.Lookup(&amp;quot;modules&amp;quot;))

    RootCmd = cmd.GenRootCmdWithRunFlags(Name, &amp;quot;&amp;quot;, beater.New, runFlags)
    RootCmd.PersistentFlags().AddGoFlag(flag.CommandLine.Lookup(&amp;quot;M&amp;quot;))
    RootCmd.TestCmd.Flags().AddGoFlag(flag.CommandLine.Lookup(&amp;quot;modules&amp;quot;))
    RootCmd.SetupCmd.Flags().AddGoFlag(flag.CommandLine.Lookup(&amp;quot;modules&amp;quot;))
    RootCmd.AddCommand(cmd.GenModulesCmd(Name, &amp;quot;&amp;quot;, buildModulesManager))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RootCmd 在这一句初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RootCmd = cmd.GenRootCmdWithRunFlags(Name, &amp;quot;&amp;quot;, beater.New, runFlags)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;beater.New跟进去看到是filebeat.go，这个函数会在后面进行调用，来创建filebeat结构体，传递filebeat相关的配置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func New(b beat.Beat, rawConfig common.Config) (beat.Beater, error) {...}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在进入GenRootCmdWithRunFlags方法，一路跟进去到GenRootCmdWithSettings，真正的初始化是在这个方法里面。&lt;/p&gt;

&lt;p&gt;忽略前面的一段初始化值方法，看到RunCmd的初始化在：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rootCmd.RunCmd = genRunCmd(settings, beatCreator, runFlags)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入getRunCmd，看到执行代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err := instance.Run(settings, beatCreator)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;跟到\elastic\beats\libbeat\cmd\instance\beat.go的Run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b, err := NewBeat(name, idxPrefix, version)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里新建了beat结构体，同时将filebeat的New方法也传递了进来，就是参数beatCreator，我们可以看到在beat通过launch函数创建了filebeat结构体类型的beater&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return b.launch(settings, bt)---&amp;gt;beater, err := b.createBeater(bt)---&amp;gt;beater, err := bt(&amp;amp;b.Beat, sub)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入launch后，还做了很多的事情&lt;/p&gt;

&lt;p&gt;1、还初始化了配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err := b.InitWithSettings(settings)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、&lt;a href=&#34;#pipeline初始化&#34;&gt;pipeline的初始化&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pipeline, err := pipeline.Load(b.Info,
        pipeline.Monitors{
            Metrics:   reg,
            Telemetry: monitoring.GetNamespace(&amp;quot;state&amp;quot;).GetRegistry(),
            Logger:    logp.L().Named(&amp;quot;publisher&amp;quot;),
        },
        b.Config.Pipeline,
        b.processing,
        b.makeOutputFactory(b.Config.Output),
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在launch的末尾，还调用了beater启动方法，也就是filebeat的run函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return beater.Run(&amp;amp;b.Beat)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为启动的是filebeat，我们到filebeat.go的Run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (fb *Filebeat) Run(b *beat.Beat) error {
       var err error
       config := fb.config

       if !fb.moduleRegistry.Empty() {
              err = fb.loadModulesPipelines(b)
              if err != nil {
                     return err
              }
       }

       waitFinished := newSignalWait()
       waitEvents := newSignalWait()

       // count active events for waiting on shutdown
       wgEvents := &amp;amp;eventCounter{
              count: monitoring.NewInt(nil, &amp;quot;filebeat.events.active&amp;quot;),
              added: monitoring.NewUint(nil, &amp;quot;filebeat.events.added&amp;quot;),
              done:  monitoring.NewUint(nil, &amp;quot;filebeat.events.done&amp;quot;),
       }
       finishedLogger := newFinishedLogger(wgEvents)

       // Setup registrar to persist state
       registrar, err := registrar.New(config.RegistryFile, config.RegistryFilePermissions, config.RegistryFlush, finishedLogger)
       if err != nil {
              logp.Err(&amp;quot;Could not init registrar: %v&amp;quot;, err)
              return err
       }

       // Make sure all events that were published in
       registrarChannel := newRegistrarLogger(registrar)

       err = b.Publisher.SetACKHandler(beat.PipelineACKHandler{
              ACKEvents: newEventACKer(finishedLogger, registrarChannel).ackEvents,
       })
       if err != nil {
              logp.Err(&amp;quot;Failed to install the registry with the publisher pipeline: %v&amp;quot;, err)
              return err
       }

       outDone := make(chan struct{}) // outDone closes down all active pipeline connections
       crawler, err := crawler.New(
              channel.NewOutletFactory(outDone, wgEvents).Create,
              config.Inputs,
              b.Info.Version,
              fb.done,
              *once)
       if err != nil {
              logp.Err(&amp;quot;Could not init crawler: %v&amp;quot;, err)
              return err
       }

       // The order of starting and stopping is important. Stopping is inverted to the starting order.
       // The current order is: registrar, publisher, spooler, crawler
       // That means, crawler is stopped first.

       // Start the registrar
       err = registrar.Start()
       if err != nil {
              return fmt.Errorf(&amp;quot;Could not start registrar: %v&amp;quot;, err)
       }

       // Stopping registrar will write last state
       defer registrar.Stop()

       // Stopping publisher (might potentially drop items)
       defer func() {
              // Closes first the registrar logger to make sure not more events arrive at the registrar
              // registrarChannel must be closed first to potentially unblock (pretty unlikely) the publisher
              registrarChannel.Close()
              close(outDone) // finally close all active connections to publisher pipeline
       }()

       // Wait for all events to be processed or timeout
       defer waitEvents.Wait()

       // Create a ES connection factory for dynamic modules pipeline loading
       var pipelineLoaderFactory fileset.PipelineLoaderFactory
       if b.Config.Output.Name() == &amp;quot;elasticsearch&amp;quot; {
              pipelineLoaderFactory = newPipelineLoaderFactory(b.Config.Output.Config())
       } else {
              logp.Warn(pipelinesWarning)
       }

       if config.OverwritePipelines {
              logp.Debug(&amp;quot;modules&amp;quot;, &amp;quot;Existing Ingest pipelines will be updated&amp;quot;)
       }

       err = crawler.Start(b.Publisher, registrar, config.ConfigInput, config.ConfigModules, pipelineLoaderFactory, config.OverwritePipelines)
       if err != nil {
              crawler.Stop()
              return err
       }

       // If run once, add crawler completion check as alternative to done signal
       if *once {
              runOnce := func() {
                     logp.Info(&amp;quot;Running filebeat once. Waiting for completion ...&amp;quot;)
                     crawler.WaitForCompletion()
                     logp.Info(&amp;quot;All data collection completed. Shutting down.&amp;quot;)
              }
              waitFinished.Add(runOnce)
       }

       // Register reloadable list of inputs and modules
       inputs := cfgfile.NewRunnerList(management.DebugK, crawler.InputsFactory, b.Publisher)
       reload.Register.MustRegisterList(&amp;quot;filebeat.inputs&amp;quot;, inputs)

       modules := cfgfile.NewRunnerList(management.DebugK, crawler.ModulesFactory, b.Publisher)
       reload.Register.MustRegisterList(&amp;quot;filebeat.modules&amp;quot;, modules)

       var adiscover *autodiscover.Autodiscover
       if fb.config.Autodiscover != nil {
              adapter := fbautodiscover.NewAutodiscoverAdapter(crawler.InputsFactory, crawler.ModulesFactory)
              adiscover, err = autodiscover.NewAutodiscover(&amp;quot;filebeat&amp;quot;, b.Publisher, adapter, config.Autodiscover)
              if err != nil {
                     return err
              }
       }
       adiscover.Start()

       // Add done channel to wait for shutdown signal
       waitFinished.AddChan(fb.done)
       waitFinished.Wait()

       // Stop reloadable lists, autodiscover -&amp;gt; Stop crawler -&amp;gt; stop inputs -&amp;gt; stop harvesters
       // Note: waiting for crawlers to stop here in order to install wgEvents.Wait
       //       after all events have been enqueued for publishing. Otherwise wgEvents.Wait
       //       or publisher might panic due to concurrent updates.
       inputs.Stop()
       modules.Stop()
       adiscover.Stop()
       crawler.Stop()

       timeout := fb.config.ShutdownTimeout
       // Checks if on shutdown it should wait for all events to be published
       waitPublished := fb.config.ShutdownTimeout &amp;gt; 0 || *once
       if waitPublished {
              // Wait for registrar to finish writing registry
              waitEvents.Add(withLog(wgEvents.Wait,
                     &amp;quot;Continue shutdown: All enqueued events being published.&amp;quot;))
              // Wait for either timeout or all events having been ACKed by outputs.
              if fb.config.ShutdownTimeout &amp;gt; 0 {
                     logp.Info(&amp;quot;Shutdown output timer started. Waiting for max %v.&amp;quot;, timeout)
                     waitEvents.Add(withLog(waitDuration(timeout),
                            &amp;quot;Continue shutdown: Time out waiting for events being published.&amp;quot;))
              } else {
                     waitEvents.AddChan(fb.done)
              }
       }

       return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构造了&lt;a href=&#34;#registry和ack-机制&#34;&gt;registrar&lt;/a&gt;和crawler，用于监控文件状态变更和数据采集。然后&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err = crawler.Start(b.Publisher, registrar, config.ConfigInput, config.ConfigModules, pipelineLoaderFactory, config.OverwritePipelines)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;crawler开始启动采集数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for _, inputConfig := range c.inputConfigs {
       err := c.startInput(pipeline, inputConfig, r.GetStates())
       if err != nil {
              return err
       }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;crawler的Start方法里面根据每个配置的输入调用一次startInput&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Crawler) startInput(
       pipeline beat.Pipeline,
       config *common.Config,
       states []file.State,
) error {
       if !config.Enabled() {
              return nil
       }

       connector := channel.ConnectTo(pipeline, c.out)
       p, err := input.New(config, connector, c.beatDone, states, nil)
       if err != nil {
              return fmt.Errorf(&amp;quot;Error in initing input: %s&amp;quot;, err)
       }
       p.Once = c.once

       if _, ok := c.inputs[p.ID]; ok {
              return fmt.Errorf(&amp;quot;Input with same ID already exists: %d&amp;quot;, p.ID)
       }

       c.inputs[p.ID] = p

       p.Start()

       return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据配置的input，构造log/input&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Input) Run() {
       logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Start next scan&amp;quot;)

       // TailFiles is like ignore_older = 1ns and only on startup
       if p.config.TailFiles {
              ignoreOlder := p.config.IgnoreOlder

              // Overwrite ignore_older for the first scan
              p.config.IgnoreOlder = 1
              defer func() {
                     // Reset ignore_older after first run
                     p.config.IgnoreOlder = ignoreOlder
                     // Disable tail_files after the first run
                     p.config.TailFiles = false
              }()
       }
       p.scan()

       // It is important that a first scan is run before cleanup to make sure all new states are read first
       if p.config.CleanInactive &amp;gt; 0 || p.config.CleanRemoved {
              beforeCount := p.states.Count()
              cleanedStates, pendingClean := p.states.Cleanup()
              logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;input states cleaned up. Before: %d, After: %d, Pending: %d&amp;quot;,
                     beforeCount, beforeCount-cleanedStates, pendingClean)
       }

       // Marking removed files to be cleaned up. Cleanup happens after next scan to make sure all states are updated first
       if p.config.CleanRemoved {
              for _, state := range p.states.GetStates() {
                     // os.Stat will return an error in case the file does not exist
                     stat, err := os.Stat(state.Source)
                     if err != nil {
                            if os.IsNotExist(err) {
                                   p.removeState(state)
                                   logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Remove state for file as file removed: %s&amp;quot;, state.Source)
                            } else {
                                   logp.Err(&amp;quot;input state for %s was not removed: %s&amp;quot;, state.Source, err)
                            }
                     } else {
                            // Check if existing source on disk and state are the same. Remove if not the case.
                            newState := file.NewState(stat, state.Source, p.config.Type, p.meta)
                            if !newState.FileStateOS.IsSame(state.FileStateOS) {
                                   p.removeState(state)
                                   logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Remove state for file as file removed or renamed: %s&amp;quot;, state.Source)
                            }
                     }
              }
       }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;input开始根据配置的输入路径扫描所有符合的文件，并启动harvester&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Input) scan() {
       var sortInfos []FileSortInfo
       var files []string

       paths := p.getFiles()

       var err error

       if p.config.ScanSort != &amp;quot;&amp;quot; {
              sortInfos, err = getSortedFiles(p.config.ScanOrder, p.config.ScanSort, getSortInfos(paths))
              if err != nil {
                     logp.Err(&amp;quot;Failed to sort files during scan due to error %s&amp;quot;, err)
              }
       }

       if sortInfos == nil {
              files = getKeys(paths)
       }

       for i := 0; i &amp;lt; len(paths); i++ {

              var path string
              var info os.FileInfo

              if sortInfos == nil {
                     path = files[i]
                     info = paths[path]
              } else {
                     path = sortInfos[i].path
                     info = sortInfos[i].info
              }

              select {
              case &amp;lt;-p.done:
                     logp.Info(&amp;quot;Scan aborted because input stopped.&amp;quot;)
                     return
              default:
              }

              newState, err := getFileState(path, info, p)
              if err != nil {
                     logp.Err(&amp;quot;Skipping file %s due to error %s&amp;quot;, path, err)
              }

              // Load last state
              lastState := p.states.FindPrevious(newState)

              // Ignores all files which fall under ignore_older
              if p.isIgnoreOlder(newState) {
                     err := p.handleIgnoreOlder(lastState, newState)
                     if err != nil {
                            logp.Err(&amp;quot;Updating ignore_older state error: %s&amp;quot;, err)
                     }
                     continue
              }

              // Decides if previous state exists
              if lastState.IsEmpty() {
                     logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Start harvester for new file: %s&amp;quot;, newState.Source)
                     err := p.startHarvester(newState, 0)
                     if err == errHarvesterLimit {
                            logp.Debug(&amp;quot;input&amp;quot;, harvesterErrMsg, newState.Source, err)
                            continue
                     }
                     if err != nil {
                            logp.Err(harvesterErrMsg, newState.Source, err)
                     }
              } else {
                     p.harvestExistingFile(newState, lastState)
              }
       }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在harvest的Run看到一个死循环读取message，预处理之后交由forwarder发送到目标输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;message, err := h.reader.Next()
h.sendEvent(data, forwarder)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，整个filebeat的启动到发送数据就理完了&lt;/p&gt;

&lt;h2 id=&#34;配置文件解析&#34;&gt;配置文件解析&lt;/h2&gt;

&lt;p&gt;在libbeat中实现了通用的配置文件解析，在启动的过程中，在每次createbeater时候就会进行config。&lt;/p&gt;

&lt;p&gt;调用 cfgfile.Load方法解析到cfg对象，进入load方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Load(path string, beatOverrides *common.Config) (*common.Config, error) {
       var config *common.Config
       var err error

       cfgpath := GetPathConfig()

       if path == &amp;quot;&amp;quot; {
              list := []string{}
              for _, cfg := range configfiles.List() {
                     if !filepath.IsAbs(cfg) {
                            list = append(list, filepath.Join(cfgpath, cfg))
                     } else {
                            list = append(list, cfg)
                     }
              }
              config, err = common.LoadFiles(list...)
       } else {
              if !filepath.IsAbs(path) {
                     path = filepath.Join(cfgpath, path)
              }
              config, err = common.LoadFile(path)
       }
       if err != nil {
              return nil, err
       }

       if beatOverrides != nil {
              config, err = common.MergeConfigs(
                     defaults,
                     beatOverrides,
                     config,
                     overwrites,
              )
              if err != nil {
                     return nil, err
              }
       } else {
              config, err = common.MergeConfigs(
                     defaults,
                     config,
                     overwrites,
              )
       }

       config.PrintDebugf(&amp;quot;Complete configuration loaded:&amp;quot;)
       return config, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果不输入配置文件，使用configfiles定义文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;configfiles = common.StringArrFlag(nil, &amp;quot;c&amp;quot;, &amp;quot;beat.yml&amp;quot;, &amp;quot;Configuration file, relative to path.config&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果输入配置文件进入else分支&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config, err = common.LoadFile(path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据配置文件构造config对象，使用的是yaml解析库。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c, err := yaml.NewConfigWithFile(path, configOpts...)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pipeline初始化&#34;&gt;pipeline初始化&lt;/h2&gt;

&lt;p&gt;pipeline的初始化是在libbeat的创建对于的filebeat 的结构体的时候进行的在func (b *Beat) createBeater(bt beat.Creator) (beat.Beater, error) {}函数中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pipeline, err := pipeline.Load(b.Info,
    pipeline.Monitors{
        Metrics:   reg,
        Telemetry: monitoring.GetNamespace(&amp;quot;state&amp;quot;).GetRegistry(),
        Logger:    logp.L().Named(&amp;quot;publisher&amp;quot;),
    },
    b.Config.Pipeline,
    b.processing,
    b.makeOutputFactory(b.Config.Output),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看load函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load uses a Config object to create a new complete Pipeline instance with
// configured queue and outputs.
func Load(
    beatInfo beat.Info,
    monitors Monitors,
    config Config,
    processors processing.Supporter,
    makeOutput func(outputs.Observer) (string, outputs.Group, error),
) (*Pipeline, error) {
    log := monitors.Logger
    if log == nil {
        log = logp.L()
    }

    if publishDisabled {
        log.Info(&amp;quot;Dry run mode. All output types except the file based one are disabled.&amp;quot;)
    }

    name := beatInfo.Name
    settings := Settings{
        WaitClose:     0,
        WaitCloseMode: NoWaitOnClose,
        Processors:    processors,
    }

    queueBuilder, err := createQueueBuilder(config.Queue, monitors)
    if err != nil {
        return nil, err
    }

    out, err := loadOutput(monitors, makeOutput)
    if err != nil {
        return nil, err
    }

    p, err := New(beatInfo, monitors, queueBuilder, out, settings)
    if err != nil {
        return nil, err
    }

    log.Infof(&amp;quot;Beat name: %s&amp;quot;, name)
    return p, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要是初始化queue，output，并创建对应的pipeline。&lt;/p&gt;

&lt;p&gt;1、queue&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;queueBuilder, err := createQueueBuilder(config.Queue, monitors)
    if err != nil {
        return nil, err
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入createQueueBuilder&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func createQueueBuilder(
    config common.ConfigNamespace,
    monitors Monitors,
) (func(queue.Eventer) (queue.Queue, error), error) {
    queueType := defaultQueueType
    if b := config.Name(); b != &amp;quot;&amp;quot; {
        queueType = b
    }

    queueFactory := queue.FindFactory(queueType)
    if queueFactory == nil {
        return nil, fmt.Errorf(&amp;quot;&#39;%v&#39; is no valid queue type&amp;quot;, queueType)
    }

    queueConfig := config.Config()
    if queueConfig == nil {
        queueConfig = common.NewConfig()
    }

    if monitors.Telemetry != nil {
        queueReg := monitors.Telemetry.NewRegistry(&amp;quot;queue&amp;quot;)
        monitoring.NewString(queueReg, &amp;quot;name&amp;quot;).Set(queueType)
    }

    return func(eventer queue.Eventer) (queue.Queue, error) {
        return queueFactory(eventer, monitors.Logger, queueConfig)
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据queueType（有默认类型mem）找到创建的方法，一般mem就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    queue.RegisterType(&amp;quot;mem&amp;quot;, create)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下create函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func create(eventer queue.Eventer, logger *logp.Logger, cfg *common.Config) (queue.Queue, error) {
    config := defaultConfig
    if err := cfg.Unpack(&amp;amp;config); err != nil {
        return nil, err
    }

    if logger == nil {
        logger = logp.L()
    }

    return NewBroker(logger, Settings{
        Eventer:        eventer,
        Events:         config.Events,
        FlushMinEvents: config.FlushMinEvents,
        FlushTimeout:   config.FlushTimeout,
    }), nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建了一个broker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewBroker creates a new broker based in-memory queue holding up to sz number of events.
// If waitOnClose is set to true, the broker will block on Close, until all internal
// workers handling incoming messages and ACKs have been shut down.
func NewBroker(
    logger logger,
    settings Settings,
) *Broker {
    // define internal channel size for producer/client requests
    // to the broker
    chanSize := 20

    var (
        sz           = settings.Events
        minEvents    = settings.FlushMinEvents
        flushTimeout = settings.FlushTimeout
    )

    if minEvents &amp;lt; 1 {
        minEvents = 1
    }
    if minEvents &amp;gt; 1 &amp;amp;&amp;amp; flushTimeout &amp;lt;= 0 {
        minEvents = 1
        flushTimeout = 0
    }
    if minEvents &amp;gt; sz {
        minEvents = sz
    }

    if logger == nil {
        logger = logp.NewLogger(&amp;quot;memqueue&amp;quot;)
    }

    b := &amp;amp;Broker{
        done:   make(chan struct{}),
        logger: logger,

        // broker API channels
        events:    make(chan pushRequest, chanSize),
        requests:  make(chan getRequest),
        pubCancel: make(chan producerCancelRequest, 5),

        // internal broker and ACK handler channels
        acks:          make(chan int),
        scheduledACKs: make(chan chanList),

        waitOnClose: settings.WaitOnClose,

        eventer: settings.Eventer,
    }

    var eventLoop interface {
        run()
        processACK(chanList, int)
    }

    if minEvents &amp;gt; 1 {
        eventLoop = newBufferingEventLoop(b, sz, minEvents, flushTimeout)
    } else {
        eventLoop = newDirectEventLoop(b, sz)
    }

    b.bufSize = sz
    ack := newACKLoop(b, eventLoop.processACK)

    b.wg.Add(2)
    go func() {
        defer b.wg.Done()
        eventLoop.run()
    }()
    go func() {
        defer b.wg.Done()
        ack.run()
    }()

    return b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;broker就是我们的queue，同时创建了一个eventLoop（根据是否有缓存创建不同的结构体，根据配置min_event是否大于1创建BufferingEventLoop或者DirectEventLoop，一般默认都是BufferingEventLoop，即带缓冲的队列。）和ack，调用他们的run函数进行监听&lt;/p&gt;

&lt;p&gt;这边特别说明一下eventLoop的new，我们看带缓存的newBufferingEventLoop&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBufferingEventLoop(b *Broker, size int, minEvents int, flushTimeout time.Duration) *bufferingEventLoop {
    l := &amp;amp;bufferingEventLoop{
        broker:       b,
        maxEvents:    size,
        minEvents:    minEvents,
        flushTimeout: flushTimeout,

        events:    b.events,
        get:       nil,
        pubCancel: b.pubCancel,
        acks:      b.acks,
    }
    l.buf = newBatchBuffer(l.minEvents)

    l.timer = time.NewTimer(flushTimeout)
    if !l.timer.Stop() {
        &amp;lt;-l.timer.C
    }

    return l
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把broker的值很多都赋给了bufferingEventLoop，不知道为什么这么做。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go func() {
    defer b.wg.Done()
    eventLoop.run()
}()
go func() {
    defer b.wg.Done()
    ack.run()
}()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下有缓存的事件处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) run() {
    var (
        broker = l.broker
    )

    for {
        select {
        case &amp;lt;-broker.done:
            return

        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)

        case req := &amp;lt;-l.pubCancel: // producer cancelling active events
            l.handleCancel(&amp;amp;req)

        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)

        case l.schedACKS &amp;lt;- l.pendingACKs:
            l.schedACKS = nil
            l.pendingACKs = chanList{}

        case count := &amp;lt;-l.acks:
            l.handleACK(count)

        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里就可以监听队列中的事件了，BufferingEventLoop是一个实现了Broker、带有各种channel的结构，主要用于将日志发送至consumer消费。 BufferingEventLoop的run方法中，同样是一个无限循环，这里可以认为是一个日志事件的调度中心。&lt;/p&gt;

&lt;p&gt;再来看看ack的调度&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *ackLoop) run() {
    var (
        // log = l.broker.logger

        // Buffer up acked event counter in acked. If acked &amp;gt; 0, acks will be set to
        // the broker.acks channel for sending the ACKs while potentially receiving
        // new batches from the broker event loop.
        // This concurrent bidirectionally communication pattern requiring &#39;select&#39;
        // ensures we can not have any deadlock between the event loop and the ack
        // loop, as the ack loop will not block on any channel
        acked int
        acks  chan int
    )

    for {
        select {
        case &amp;lt;-l.broker.done:
            // TODO: handle pending ACKs?
            // TODO: panic on pending batches?
            return

        case acks &amp;lt;- acked:
            acks, acked = nil, 0

        case lst := &amp;lt;-l.broker.scheduledACKs:
            count, events := lst.count()
            l.lst.concat(&amp;amp;lst)

            // log.Debug(&amp;quot;ACK List:&amp;quot;)
            // for current := l.lst.head; current != nil; current = current.next {
            //  log.Debugf(&amp;quot;  ack entry(seq=%v, start=%v, count=%v&amp;quot;,
            //      current.seq, current.start, current.count)
            // }

            l.batchesSched += uint64(count)
            l.totalSched += uint64(events)

        case &amp;lt;-l.sig:
            acked += l.handleBatchSig()
            if acked &amp;gt; 0 {
                acks = l.broker.acks
            }
        }

        // log.Debug(&amp;quot;ackloop INFO&amp;quot;)
        // log.Debug(&amp;quot;ackloop:   total events scheduled = &amp;quot;, l.totalSched)
        // log.Debug(&amp;quot;ackloop:   total events ack = &amp;quot;, l.totalACK)
        // log.Debug(&amp;quot;ackloop:   total batches scheduled = &amp;quot;, l.batchesSched)
        // log.Debug(&amp;quot;ackloop:   total batches ack = &amp;quot;, l.batchesACKed)

        l.sig = l.lst.channel()
        // if l.sig == nil {
        //  log.Debug(&amp;quot;ackloop: no ack scheduled&amp;quot;)
        // } else {
        //  log.Debug(&amp;quot;ackloop: schedule ack: &amp;quot;, l.lst.head.seq)
        // }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有处理信号就发送给regestry进行记录，关于&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/#registry和ack-机制&#34;&gt;registry在下面详细说明&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;2、output&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;out, err := loadOutput(monitors, makeOutput)
if err != nil {
    return nil, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入loadOutput&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func loadOutput(
    monitors Monitors,
    makeOutput OutputFactory,
) (outputs.Group, error) {
    log := monitors.Logger
    if log == nil {
        log = logp.L()
    }

    if publishDisabled {
        return outputs.Group{}, nil
    }

    if makeOutput == nil {
        return outputs.Group{}, nil
    }

    var (
        metrics  *monitoring.Registry
        outStats outputs.Observer
    )
    if monitors.Metrics != nil {
        metrics = monitors.Metrics.GetRegistry(&amp;quot;output&amp;quot;)
        if metrics != nil {
            metrics.Clear()
        } else {
            metrics = monitors.Metrics.NewRegistry(&amp;quot;output&amp;quot;)
        }
        outStats = outputs.NewStats(metrics)
    }

    outName, out, err := makeOutput(outStats)
    if err != nil {
        return outputs.Fail(err)
    }

    if metrics != nil {
        monitoring.NewString(metrics, &amp;quot;type&amp;quot;).Set(outName)
    }
    if monitors.Telemetry != nil {
        telemetry := monitors.Telemetry.GetRegistry(&amp;quot;output&amp;quot;)
        if telemetry != nil {
            telemetry.Clear()
        } else {
            telemetry = monitors.Telemetry.NewRegistry(&amp;quot;output&amp;quot;)
        }
        monitoring.NewString(telemetry, &amp;quot;name&amp;quot;).Set(outName)
    }

    return out, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边是根据load传进来的makeOutput函数来进行创建的，我们看一下load这个参数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Beat) makeOutputFactory(
    cfg common.ConfigNamespace,
) func(outputs.Observer) (string, outputs.Group, error) {
    return func(outStats outputs.Observer) (string, outputs.Group, error) {
        out, err := b.createOutput(outStats, cfg)
        return cfg.Name(), out, err
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建createOutput&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Beat) createOutput(stats outputs.Observer, cfg common.ConfigNamespace) (outputs.Group, error) {
    if !cfg.IsSet() {
        return outputs.Group{}, nil
    }

    return outputs.Load(b.IdxSupporter, b.Info, stats, cfg.Name(), cfg.Config())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看load&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load creates and configures a output Group using a configuration object..
func Load(
    im IndexManager,
    info beat.Info,
    stats Observer,
    name string,
    config *common.Config,
) (Group, error) {
    factory := FindFactory(name)
    if factory == nil {
        return Group{}, fmt.Errorf(&amp;quot;output type %v undefined&amp;quot;, name)
    }

    if stats == nil {
        stats = NewNilObserver()
    }
    return factory(im, info, stats, config)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见根据配置文件的配置的output的类型进行创建，比如我们用kafka做为output，我们看一下创建函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    sarama.Logger = kafkaLogger{}

    outputs.RegisterType(&amp;quot;kafka&amp;quot;, makeKafka)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是makeKafka&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeKafka(
    _ outputs.IndexManager,
    beat beat.Info,
    observer outputs.Observer,
    cfg *common.Config,
) (outputs.Group, error) {
    debugf(&amp;quot;initialize kafka output&amp;quot;)

    config, err := readConfig(cfg)
    if err != nil {
        return outputs.Fail(err)
    }

    topic, err := outil.BuildSelectorFromConfig(cfg, outil.Settings{
        Key:              &amp;quot;topic&amp;quot;,
        MultiKey:         &amp;quot;topics&amp;quot;,
        EnableSingleOnly: true,
        FailEmpty:        true,
    })
    if err != nil {
        return outputs.Fail(err)
    }

    libCfg, err := newSaramaConfig(config)
    if err != nil {
        return outputs.Fail(err)
    }

    hosts, err := outputs.ReadHostList(cfg)
    if err != nil {
        return outputs.Fail(err)
    }

    codec, err := codec.CreateEncoder(beat, config.Codec)
    if err != nil {
        return outputs.Fail(err)
    }

    client, err := newKafkaClient(observer, hosts, beat.IndexPrefix, config.Key, topic, codec, libCfg)
    if err != nil {
        return outputs.Fail(err)
    }

    retry := 0
    if config.MaxRetries &amp;lt; 0 {
        retry = -1
    }
    return outputs.Success(config.BulkMaxSize, retry, client)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接创建了kafka的client给发送的时候使用。&lt;/p&gt;

&lt;p&gt;最后利用上面的两个构建函数来创建我们的pipeline&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p, err := New(beatInfo, monitors, queueBuilder, out, settings)
if err != nil {
    return nil, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实上面函数有的调用是在这个new中进行的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New create a new Pipeline instance from a queue instance and a set of outputs.
// The new pipeline will take ownership of queue and outputs. On Close, the
// queue and outputs will be closed.
func New(
    beat beat.Info,
    monitors Monitors,
    queueFactory queueFactory,
    out outputs.Group,
    settings Settings,
) (*Pipeline, error) {
    var err error

    if monitors.Logger == nil {
        monitors.Logger = logp.NewLogger(&amp;quot;publish&amp;quot;)
    }

    p := &amp;amp;Pipeline{
        beatInfo:         beat,
        monitors:         monitors,
        observer:         nilObserver,
        waitCloseMode:    settings.WaitCloseMode,
        waitCloseTimeout: settings.WaitClose,
        processors:       settings.Processors,
    }
    p.ackBuilder = &amp;amp;pipelineEmptyACK{p}
    p.ackActive = atomic.MakeBool(true)

    if monitors.Metrics != nil {
        p.observer = newMetricsObserver(monitors.Metrics)
    }
    p.eventer.observer = p.observer
    p.eventer.modifyable = true

    if settings.WaitCloseMode == WaitOnPipelineClose &amp;amp;&amp;amp; settings.WaitClose &amp;gt; 0 {
        p.waitCloser = &amp;amp;waitCloser{}

        // waitCloser decrements counter on queue ACK (not per client)
        p.eventer.waitClose = p.waitCloser
    }

    p.queue, err = queueFactory(&amp;amp;p.eventer)
    if err != nil {
        return nil, err
    }

    if count := p.queue.BufferConfig().Events; count &amp;gt; 0 {
        p.eventSema = newSema(count)
    }

    maxEvents := p.queue.BufferConfig().Events
    if maxEvents &amp;lt;= 0 {
        // Maximum number of events until acker starts blocking.
        // Only active if pipeline can drop events.
        maxEvents = 64000
    }
    p.eventSema = newSema(maxEvents)

    p.output = newOutputController(beat, monitors, p.observer, p.queue)
    p.output.Set(out)

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个output的控制器outputController&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newOutputController(
    beat beat.Info,
    monitors Monitors,
    observer outputObserver,
    b queue.Queue,
) *outputController {
    c := &amp;amp;outputController{
        beat:     beat,
        monitors: monitors,
        observer: observer,
        queue:    b,
    }

    ctx := &amp;amp;batchContext{}
    c.consumer = newEventConsumer(monitors.Logger, b, ctx)
    c.retryer = newRetryer(monitors.Logger, observer, nil, c.consumer)
    ctx.observer = observer
    ctx.retryer = c.retryer

    c.consumer.sigContinue()

    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时初始化了eventConsumer和retryer。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventConsumer(
    log *logp.Logger,
    queue queue.Queue,
    ctx *batchContext,
) *eventConsumer {
    c := &amp;amp;eventConsumer{
        logger: log,
        done:   make(chan struct{}),
        sig:    make(chan consumerSignal, 3),
        out:    nil,

        queue:    queue,
        consumer: queue.Consumer(),
        ctx:      ctx,
    }

    c.pause.Store(true)
    go c.loop(c.consumer)
    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在eventConsumer中启动了一个监听程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *eventConsumer) loop(consumer queue.Consumer) {
    log := c.logger

    log.Debug(&amp;quot;start pipeline event consumer&amp;quot;)

    var (
        out    workQueue
        batch  *Batch
        paused = true
    )

    handleSignal := func(sig consumerSignal) {
        switch sig.tag {
        case sigConsumerCheck:

        case sigConsumerUpdateOutput:
            c.out = sig.out

        case sigConsumerUpdateInput:
            consumer = sig.consumer
        }

        paused = c.paused()
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; batch != nil {
            out = c.out.workQueue
        } else {
            out = nil
        }
    }

    for {
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; consumer != nil &amp;amp;&amp;amp; batch == nil {
            out = c.out.workQueue
            queueBatch, err := consumer.Get(c.out.batchSize)
            if err != nil {
                out = nil
                consumer = nil
                continue
            }
            if queueBatch != nil {
                batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
            }

            paused = c.paused()
            if paused || batch == nil {
                out = nil
            }
        }

        select {
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
            continue
        default:
        }

        select {
        case &amp;lt;-c.done:
            log.Debug(&amp;quot;stop pipeline event consumer&amp;quot;)
            return
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
        case out &amp;lt;- batch:
            batch = nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用于消费队列中的事件event，并将其构建成Batch，放到处理队列中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBatch(ctx *batchContext, original queue.Batch, ttl int) *Batch {
    if original == nil {
        panic(&amp;quot;empty batch&amp;quot;)
    }

    b := batchPool.Get().(*Batch)
    *b = Batch{
        original: original,
        ctx:      ctx,
        ttl:      ttl,
        events:   original.Events(),
    }
    return b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看retryer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newRetryer(
    log *logp.Logger,
    observer outputObserver,
    out workQueue,
    c *eventConsumer,
) *retryer {
    r := &amp;amp;retryer{
        logger:     log,
        observer:   observer,
        done:       make(chan struct{}),
        sig:        make(chan retryerSignal, 3),
        in:         retryQueue(make(chan batchEvent, 3)),
        out:        out,
        consumer:   c,
        doneWaiter: sync.WaitGroup{},
    }
    r.doneWaiter.Add(1)
    go r.loop()
    return r
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样启动了监听程序，用于重试。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *retryer) loop() {
    defer r.doneWaiter.Done()
    var (
        out             workQueue
        consumerBlocked bool

        active     *Batch
        activeSize int
        buffer     []*Batch
        numOutputs int

        log = r.logger
    )

    for {
        select {
        case &amp;lt;-r.done:
            return
        case evt := &amp;lt;-r.in:
            var (
                countFailed  int
                countDropped int
                batch        = evt.batch
                countRetry   = len(batch.events)
            )

            if evt.tag == retryBatch {
                countFailed = len(batch.events)
                r.observer.eventsFailed(countFailed)

                decBatch(batch)

                countRetry = len(batch.events)
                countDropped = countFailed - countRetry
                r.observer.eventsDropped(countDropped)
            }

            if len(batch.events) == 0 {
                log.Info(&amp;quot;Drop batch&amp;quot;)
                batch.Drop()
            } else {
                out = r.out
                buffer = append(buffer, batch)
                out = r.out
                active = buffer[0]
                activeSize = len(active.events)
                if !consumerBlocked {
                    consumerBlocked = blockConsumer(numOutputs, len(buffer))
                    if consumerBlocked {
                        log.Info(&amp;quot;retryer: send wait signal to consumer&amp;quot;)
                        r.consumer.sigWait()
                        log.Info(&amp;quot;  done&amp;quot;)
                    }
                }
            }

        case out &amp;lt;- active:
            r.observer.eventsRetry(activeSize)

            buffer = buffer[1:]
            active, activeSize = nil, 0

            if len(buffer) == 0 {
                out = nil
            } else {
                active = buffer[0]
                activeSize = len(active.events)
            }

            if consumerBlocked {
                consumerBlocked = blockConsumer(numOutputs, len(buffer))
                if !consumerBlocked {
                    log.Info(&amp;quot;retryer: send unwait-signal to consumer&amp;quot;)
                    r.consumer.sigUnWait()
                    log.Info(&amp;quot;  done&amp;quot;)
                }
            }

        case sig := &amp;lt;-r.sig:
            switch sig.tag {
            case sigRetryerUpdateOutput:
                r.out = sig.channel
            case sigRetryerOutputAdded:
                numOutputs++
            case sigRetryerOutputRemoved:
                numOutputs--
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后对out进行了设置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *outputController) Set(outGrp outputs.Group) {
    // create new outputGroup with shared work queue
    clients := outGrp.Clients
    queue := makeWorkQueue()
    worker := make([]outputWorker, len(clients))
    for i, client := range clients {
        worker[i] = makeClientWorker(c.observer, queue, client)
    }
    grp := &amp;amp;outputGroup{
        workQueue:  queue,
        outputs:    worker,
        timeToLive: outGrp.Retry + 1,
        batchSize:  outGrp.BatchSize,
    }

    // update consumer and retryer
    c.consumer.sigPause()
    if c.out != nil {
        for range c.out.outputs {
            c.retryer.sigOutputRemoved()
        }
    }
    c.retryer.updOutput(queue)
    for range clients {
        c.retryer.sigOutputAdded()
    }
    c.consumer.updOutput(grp)

    // close old group, so events are send to new workQueue via retryer
    if c.out != nil {
        for _, w := range c.out.outputs {
            w.Close()
        }
    }

    c.out = grp

    // restart consumer (potentially blocked by retryer)
    c.consumer.sigContinue()

    c.observer.updateOutputGroup()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边就是对上面创建的kafka的每个client创建一个监控程序makeClientWorker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeClientWorker(observer outputObserver, qu workQueue, client outputs.Client) outputWorker {
    if nc, ok := client.(outputs.NetworkClient); ok {
        c := &amp;amp;netClientWorker{observer: observer, qu: qu, client: nc}
        go c.run()
        return c
    }
    c := &amp;amp;clientWorker{observer: observer, qu: qu, client: client}
    go c.run()
    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就用于监控workQueue的数据，有数据就通过client的push发送到kafka，到这边pipeline的初始化也就结束了。&lt;/p&gt;

&lt;h2 id=&#34;日志收集&#34;&gt;日志收集&lt;/h2&gt;

&lt;p&gt;Filebeat 不仅支持普通文本日志的作为输入源，还内置支持了 redis 的慢查询日志、stdin、tcp 和 udp 等作为输入源。&lt;/p&gt;

&lt;p&gt;本文只分析下普通文本日志的处理方式，对于普通文本日志，可以按照以下配置方式，指定 log 的输入源信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/*.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 Input 也可以指定多个, 每个 Input 下的 Log 也可以指定多个。&lt;/p&gt;

&lt;p&gt;从收集日志、到发送事件到publisher，其数据流如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/collect.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;filebeat 启动时会开启 Crawler，filebeat抽象出一个Crawler的结构体，对于配置中的每条 Input，Crawler 都会启动一个 Input 进行处理，代码如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Crawler) Start(...){
    ...
    for _, inputConfig := range c.inputConfigs {
        err := c.startInput(pipeline, inputConfig, r.GetStates())
        if err != nil {
            return err
        }
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是创建input，比如我们采集的是log类型的，就是调用log的NewInput来创建，并且启动，定时进行扫描&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p, err := input.New(config, connector, c.beatDone, states, nil)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会根据采集日志的类型来进行注册调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewInput instantiates a new Log
func NewInput(
    cfg *common.Config,
    outlet channel.Connector,
    context input.Context,
) (input.Input, error) {
    cleanupNeeded := true
    cleanupIfNeeded := func(f func() error) {
        if cleanupNeeded {
            f()
        }
    }

    // Note: underlying output.
    //  The input and harvester do have different requirements
    //  on the timings the outlets must be closed/unblocked.
    //  The outlet generated here is the underlying outlet, only closed
    //  once all workers have been shut down.
    //  For state updates and events, separate sub-outlets will be used.
    out, err := outlet.ConnectWith(cfg, beat.ClientConfig{
        Processing: beat.ProcessingConfig{
            DynamicFields: context.DynamicFields,
        },
    })
    if err != nil {
        return nil, err
    }
    defer cleanupIfNeeded(out.Close)

    // stateOut will only be unblocked if the beat is shut down.
    // otherwise it can block on a full publisher pipeline, so state updates
    // can be forwarded correctly to the registrar.
    stateOut := channel.CloseOnSignal(channel.SubOutlet(out), context.BeatDone)
    defer cleanupIfNeeded(stateOut.Close)

    meta := context.Meta
    if len(meta) == 0 {
        meta = nil
    }

    p := &amp;amp;Input{
        config:      defaultConfig,
        cfg:         cfg,
        harvesters:  harvester.NewRegistry(),
        outlet:      out,
        stateOutlet: stateOut,
        states:      file.NewStates(),
        done:        context.Done,
        meta:        meta,
    }

    if err := cfg.Unpack(&amp;amp;p.config); err != nil {
        return nil, err
    }
    if err := p.config.resolveRecursiveGlobs(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to resolve recursive globs in config: %v&amp;quot;, err)
    }
    if err := p.config.normalizeGlobPatterns(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to normalize globs patterns: %v&amp;quot;, err)
    }

    // Create empty harvester to check if configs are fine
    // TODO: Do config validation instead
    _, err = p.createHarvester(file.State{}, nil)
    if err != nil {
        return nil, err
    }

    if len(p.config.Paths) == 0 {
        return nil, fmt.Errorf(&amp;quot;each input must have at least one path defined&amp;quot;)
    }

    err = p.loadStates(context.States)
    if err != nil {
        return nil, err
    }

    logp.Info(&amp;quot;Configured paths: %v&amp;quot;, p.config.Paths)

    cleanupNeeded = false
    go p.stopWhenDone()

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后会调用这个结构体的run函数进行扫描，主要是调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.scan()

// Scan starts a scanGlob for each provided path/glob
func (p *Input) scan() {
    var sortInfos []FileSortInfo
    var files []string

    paths := p.getFiles()

    var err error

    if p.config.ScanSort != &amp;quot;&amp;quot; {
        sortInfos, err = getSortedFiles(p.config.ScanOrder, p.config.ScanSort, getSortInfos(paths))
        if err != nil {
            logp.Err(&amp;quot;Failed to sort files during scan due to error %s&amp;quot;, err)
        }
    }

    if sortInfos == nil {
        files = getKeys(paths)
    }

    for i := 0; i &amp;lt; len(paths); i++ {

        var path string
        var info os.FileInfo

        if sortInfos == nil {
            path = files[i]
            info = paths[path]
        } else {
            path = sortInfos[i].path
            info = sortInfos[i].info
        }

        select {
        case &amp;lt;-p.done:
            logp.Info(&amp;quot;Scan aborted because input stopped.&amp;quot;)
            return
        default:
        }

        newState, err := getFileState(path, info, p)
        if err != nil {
            logp.Err(&amp;quot;Skipping file %s due to error %s&amp;quot;, path, err)
        }



        // Load last state
        lastState := p.states.FindPrevious(newState)

        // Ignores all files which fall under ignore_older
        if p.isIgnoreOlder(newState) {
            logp.Debug(&amp;quot;input&amp;quot;,&amp;quot;ignore&amp;quot;)
            err := p.handleIgnoreOlder(lastState, newState)
            if err != nil {
                logp.Err(&amp;quot;Updating ignore_older state error: %s&amp;quot;, err)
            }
            //close(p.done)
            continue
        }

        // Decides if previous state exists
        if lastState.IsEmpty() {
            logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Start harvester for new file: %s&amp;quot;, newState.Source)
            err := p.startHarvester(newState, 0)
            if err == errHarvesterLimit {
                logp.Debug(&amp;quot;input&amp;quot;, harvesterErrMsg, newState.Source, err)
                continue
            }
            if err != nil {
                logp.Err(harvesterErrMsg, newState.Source, err)
            }
        } else {
            p.harvestExistingFile(newState, lastState)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进行扫描过滤。由于指定的 paths 可以配置多个，而且可以是 Glob 类型，因此 Filebeat 将会匹配到多个配置文件。&lt;/p&gt;

&lt;p&gt;根据配置获取匹配的日志文件，需要注意的是，这里的匹配方式并非正则，而是采用linux glob的规则，和正则还是有一些区别。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;matches, err := filepath.Glob(path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取到了所有匹配的日志文件之后，会经过一些复杂的过滤，例如如果配置了exclude_files则会忽略这类文件，同时还会查询文件的状态，如果文件的最近一次修改时间大于ignore_older的配置，也会不去采集该文件。&lt;/p&gt;

&lt;p&gt;还会对文件进行处理，获取每个文件的状态，构建新的state结构，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;newState, err := getFileState(path, info, p)
if err != nil {
    logp.Err(&amp;quot;Skipping file %s due to error %s&amp;quot;, path, err)
}

func getFileState(path string, info os.FileInfo, p *Input) (file.State, error) {
    var err error
    var absolutePath string
    absolutePath, err = filepath.Abs(path)
    if err != nil {
        return file.State{}, fmt.Errorf(&amp;quot;could not fetch abs path for file %s: %s&amp;quot;, absolutePath, err)
    }
    logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Check file for harvesting: %s&amp;quot;, absolutePath)
    // Create new state for comparison
    newState := file.NewState(info, absolutePath, p.config.Type, p.meta, p.cfg.GetField(&amp;quot;brokerlist&amp;quot;), p.cfg.GetField(&amp;quot;topic&amp;quot;))
    return newState, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时在已经存在的状态中进行对比，如果获取到对于的状态就不重新启动协程进行采集，如果获取一个新的状态就开启新的协程进行采集。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load last state
lastState := p.states.FindPrevious(newState)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Input对象创建时会从registry读取文件状态(主要是offset)， 对于每个匹配到的文件，都会开启一个 Harvester 进行逐行读取，每个 Harvester 都工作在自己的的 goroutine 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err := p.startHarvester(newState, 0)
if err == errHarvesterLimit {
    logp.Debug(&amp;quot;input&amp;quot;, harvesterErrMsg, newState.Source, err)
    continue
}
if err != nil {
    logp.Err(harvesterErrMsg, newState.Source, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看startHarvester&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// startHarvester starts a new harvester with the given offset
// In case the HarvesterLimit is reached, an error is returned
func (p *Input) startHarvester(state file.State, offset int64) error {
    if p.numHarvesters.Inc() &amp;gt; p.config.HarvesterLimit &amp;amp;&amp;amp; p.config.HarvesterLimit &amp;gt; 0 {
        p.numHarvesters.Dec()
        harvesterSkipped.Add(1)
        return errHarvesterLimit
    }
    // Set state to &amp;quot;not&amp;quot; finished to indicate that a harvester is running
    state.Finished = false
    state.Offset = offset

    // Create harvester with state
    h, err := p.createHarvester(state, func() { p.numHarvesters.Dec() })
    if err != nil {
        p.numHarvesters.Dec()
        return err
    }

    err = h.Setup()
    if err != nil {
        p.numHarvesters.Dec()
        return fmt.Errorf(&amp;quot;error setting up harvester: %s&amp;quot;, err)
    }

    // Update state before staring harvester
    // This makes sure the states is set to Finished: false
    // This is synchronous state update as part of the scan
    h.SendStateUpdate()

    if err = p.harvesters.Start(h); err != nil {
        p.numHarvesters.Dec()
    }
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先创建了Harvester&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// createHarvester creates a new harvester instance from the given state
func (p *Input) createHarvester(state file.State, onTerminate func()) (*Harvester, error) {
    // Each wraps the outlet, for closing the outlet individually
    h, err := NewHarvester(
        p.cfg,
        state,
        p.states,
        func(state file.State) bool {
            return p.stateOutlet.OnEvent(beat.Event{Private: state})
        },
        subOutletWrap(p.outlet),
    )
    if err == nil {
        h.onTerminate = onTerminate
    }
    return h, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;harvester启动时会通过Setup方法创建一系列reader形成读处理链&lt;/p&gt;

&lt;p&gt;关于log类型的reader处理链，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/read.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;opt表示根据配置决定是否创建该reader&lt;/p&gt;

&lt;p&gt;Reader包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Line: 包含os.File，用于从指定offset开始读取日志行。虽然位于处理链的最内部，但其Next函数中实际的处理逻辑（读文件行）却是最新被执行的。&lt;/li&gt;
&lt;li&gt;Encode: 包含Line Reader，将其读取到的行生成Message结构后返回&lt;/li&gt;
&lt;li&gt;JSON, DockerJSON: 将json形式的日志内容decode成字段&lt;/li&gt;
&lt;li&gt;StripNewLine：去除日志行尾部的空白符&lt;/li&gt;
&lt;li&gt;Multiline: 用于读取多行日志&lt;/li&gt;
&lt;li&gt;Limit: 限制单行日志字节数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了Line Reader外，这些reader都实现了Reader接口：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader interface {
    Next() (Message, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reader通过内部包含Reader对象的方式，使Reader形成一个处理链，其实这就是设计模式中的责任链模式。&lt;/p&gt;

&lt;p&gt;各Reader的Next方法的通用形式像是这样：Next方法调用内部Reader对象的Next方法获取Message，然后处理后返回。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *SomeReader) Next() (Message, error) {
    message, err := r.reader.Next()
    if err != nil {
        return message, err
    }

    // do some processing...

    return message, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实Harvester 的工作流程非常简单，harvester从registry记录的文件位置开始读取，就是逐行读取文件，并更新该文件暂时在 Input 中的文件偏移量（注意，并不是 Registrar 中的偏移量），读取完成（读到文件的EOF末尾），组装成事件（beat.Event）后发给Publisher。主要是调用了Harvester的run方法，部分如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    select {
    case &amp;lt;-h.done:
        return nil
    default:
    }

    message, err := h.reader.Next()
    if err != nil {
        switch err {
        case ErrFileTruncate:
            logp.Info(&amp;quot;File was truncated. Begin reading file from offset 0: %s&amp;quot;, h.state.Source)
            h.state.Offset = 0
            filesTruncated.Add(1)
        case ErrRemoved:
            logp.Info(&amp;quot;File was removed: %s. Closing because close_removed is enabled.&amp;quot;, h.state.Source)
        case ErrRenamed:
            logp.Info(&amp;quot;File was renamed: %s. Closing because close_renamed is enabled.&amp;quot;, h.state.Source)
        case ErrClosed:
            logp.Info(&amp;quot;Reader was closed: %s. Closing.&amp;quot;, h.state.Source)
        case io.EOF:
            logp.Info(&amp;quot;End of file reached: %s. Closing because close_eof is enabled.&amp;quot;, h.state.Source)
        case ErrInactive:
            logp.Info(&amp;quot;File is inactive: %s. Closing because close_inactive of %v reached.&amp;quot;, h.state.Source, h.config.CloseInactive)
        case reader.ErrLineUnparsable:
            logp.Info(&amp;quot;Skipping unparsable line in file: %v&amp;quot;, h.state.Source)
            //line unparsable, go to next line
            continue
        default:
            logp.Err(&amp;quot;Read line error: %v; File: %v&amp;quot;, err, h.state.Source)
        }
        return nil
    }

    // Get copy of state to work on
    // This is important in case sending is not successful so on shutdown
    // the old offset is reported
    state := h.getState()
    startingOffset := state.Offset
    state.Offset += int64(message.Bytes)

    // Stop harvester in case of an error
    if !h.onMessage(forwarder, state, message, startingOffset) {
        return nil
    }

    // Update state of harvester as successfully sent
    h.state = state
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，reader.Next()方法会不停的读取日志，如果没有返回异常，则发送日志数据到缓存队列中。&lt;/p&gt;

&lt;p&gt;返回的异常有几种类型，除了读取到EOF外，还会有例如文件一段时间不活跃等情况发生会使harvester goroutine退出，不再采集该文件，并关闭文件句柄。 filebeat为了防止占据过多的采集日志文件的文件句柄，默认的close_inactive参数为5min，如果日志文件5min内没有被修改，上面代码会进入ErrInactive的case，之后该harvester goroutine会被关闭。 这种场景下还需要注意的是，如果某个文件日志采集中被移除了，但是由于此时被filebeat保持着文件句柄，文件占据的磁盘空间会被保留直到harvester goroutine结束。&lt;/p&gt;

&lt;p&gt;不同的harvester goroutine采集到的日志数据都会发送至一个全局的队列queue中，queue的实现有两种：基于内存和基于磁盘的队列，目前基于磁盘的队列还是处于alpha阶段，filebeat默认启用的是基于内存的缓存队列。 每当队列中的数据缓存到一定的大小或者超过了定时的时间（默认1s)，会被注册的client从队列中消费，发送至配置的后端。目前可以设置的client有kafka、elasticsearch、redis等。&lt;/p&gt;

&lt;p&gt;同时，我们需要考虑到，日志型的数据其实是在不断增长和变化的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;会有新的日志在不断产生
可能一个日志文件对应的 Harvester 退出后，又再次有了内容更新。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了解决这两个情况，filebeat 采用了 Input 定时扫描的方式。代码如下，可以看出，Input 扫描的频率是由用户指定的 scan_frequency 配置来决定的 (默认 10s 扫描一次)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Runner) Run() {
    p.input.Run()

    if p.Once {
        return
    }

    for {
        select {
        case &amp;lt;-p.done:
            logp.Info(&amp;quot;input ticker stopped&amp;quot;)
            return
        case &amp;lt;-time.After(p.config.ScanFrequency): // 定时扫描
            logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Run input&amp;quot;)
            p.input.Run()
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外，如果用户启动时指定了 –once 选项，则扫描只会进行一次，就退出了。&lt;/p&gt;

&lt;p&gt;使用一个简单的流程图可以这样表示&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/collect1.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;处理文件重命名，删除，截断&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取文件信息时会获取文件的device id + indoe作为文件的唯一标识;&lt;/li&gt;
&lt;li&gt;文件收集进度会被持久化，这样当创建Harvester时，首先会对文件作openFile, 以 device id + inode为key在持久化文件中查看当前文件是否被收集过，收集到了什么位置，然后断点续传&lt;/li&gt;
&lt;li&gt;在读取过程中，如果文件被截断，认为文件已经被同名覆盖，将从头开始读取文件&lt;/li&gt;
&lt;li&gt;如果文件被删除，因为原文件已被打开，不影响继续收集，但如果设置了CloseRemoved， 则不会再继续收集&lt;/li&gt;
&lt;li&gt;如果文件被重命名，因为原文件已被打开，不影响继续收集，但如果设置了CloseRenamed ， 则不会再继续收集&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;pipeline调度&#34;&gt;pipeline调度&lt;/h2&gt;

&lt;p&gt;至此，我们可以清楚的知道，Filebeat 是如何采集日志文件。而日志采集过程，Harvest 会将数据写到 Pipeline 中。我们接下来看下数据是如何写入到 Pipeline 中的。&lt;/p&gt;

&lt;p&gt;Haveseter 会将数据写入缓存中，而另一方面 Output 会从缓存将数据读走。整个生产消费的过程都是由 Pipeline 进行调度的，而整个调度过程也非常复杂。&lt;/p&gt;

&lt;p&gt;不同的harvester goroutine采集到的日志数据都会发送至一个全局的队列queue中，Filebeat 的缓存queue目前分为 memqueue 和 spool。memqueue 顾名思义就是内存缓存，spool 则是将数据缓存到磁盘中。本文将基于 memqueue 讲解整个调度过程。&lt;/p&gt;

&lt;p&gt;在下面的pipeline的写入和消费中，在client.go在(*client) publish方法中我们可以看到，事件是通过调用c.producer.Publish(pubEvent)被实际发送的，而producer则通过具体Queue的Producer方法生成。&lt;/p&gt;

&lt;p&gt;队列对象被包含在pipeline.go:Pipeline结构中，其接口的定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Queue interface {
    io.Closer
    BufferConfig() BufferConfig
    Producer(cfg ProducerConfig) Producer
    Consumer() Consumer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要的，Producer方法生成Producer对象，用于向队列中push事件；Consumer方法生成Consumer对象，用于从队列中取出事件。Producer和Consumer接口定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Producer interface {
    Publish(event publisher.Event) bool
    TryPublish(event publisher.Event) bool
    Cancel() int
}

type Consumer interface {
    Get(sz int) (Batch, error)
    Close() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在配置中没有指定队列配置时，默认使用了memqueue作为队列实现，下面我们来看看memqueue及其对应producer和consumer定义：&lt;/p&gt;

&lt;p&gt;Broker结构(memqueue在代码中实际对应的结构名是Broker)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Broker struct {
    done chan struct{}

    logger logger

    bufSize int
    // buf         brokerBuffer
    // minEvents   int
    // idleTimeout time.Duration

    // api channels
    events    chan pushRequest
    requests  chan getRequest
    pubCancel chan producerCancelRequest

    // internal channels
    acks          chan int
    scheduledACKs chan chanList

    eventer queue.Eventer

    // wait group for worker shutdown
    wg          sync.WaitGroup
    waitOnClose bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据是否需要ack分为forgetfullProducer和ackProducer两种producer：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type forgetfullProducer struct {
    broker    *Broker
    openState openState
}

type ackProducer struct {
    broker    *Broker
    cancel    bool
    seq       uint32
    state     produceState
    openState openState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;consumer结构:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type consumer struct {
    broker *Broker
    resp   chan getResponse

    done   chan struct{}
    closed atomic.Bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;queue、producer、consumer三者关系的运作方式如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/queue.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Producer通过Publish或TryPublish事件放入Broker的队列，即结构中的channel对象evetns&lt;/li&gt;
&lt;li&gt;Broker的主事件循环EventLoop将（请求）事件从events channel取出，放入自身结构体对象ringBuffer中。主事件循环有两种类型：

&lt;ul&gt;
&lt;li&gt;直接（不带buffer）事件循环结构directEventLoop：收到事件后尽可能快的转发；&lt;/li&gt;
&lt;li&gt;带buffer事件循环结构bufferingEventLoop：当buffer满或刷新超时时转发。具体使用哪一种取决于memqueue配置项flush.min_events，大于1时使用后者，否则使用前者。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;eventConsumer调用Consumer的Get方法获取事件：

&lt;ul&gt;
&lt;li&gt;首先将获取事件请求（包括请求事件数和用于存放其响应事件的channel resp）放入Broker的请求队列requests中，等待主事件循环EventLoop处理后将事件放入resp；&lt;/li&gt;
&lt;li&gt;获取resp的事件，组装成batch结构后返回&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;eventConsumer将事件放入output对应队列中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;这部分关于事件在队列中各种channel间的流转，笔者认为是比较消耗性能的，但不清楚设计者这样设计的考量是什么。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;另外值得思考的是，在多个go routine使用队列交互的场景下，libbeat中都使用了go语言channel作为其底层的队列，它是否可以完全替代加锁队列的使用呢？&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;pipeline-的写入&#34;&gt;Pipeline 的写入&lt;/h3&gt;

&lt;p&gt;在Crawler收集日志并转换成事件后，我们继续发送数据，其就会通过调用Publisher对应client的Publish接口将事件送到Publisher，后续的处理流程也都将由libbeat完成，事件的流转如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/event.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们首先看一下事件处理器processor&lt;/p&gt;

&lt;p&gt;在harvester调用client.Publish接口时，其内部会使用配置中定义的processors对事件进行处理，然后才将事件发送到Publisher队列。&lt;/p&gt;

&lt;p&gt;processor包含两种：在Input内定义作为局部（Input独享）的processor，其只对该Input产生的事件生效；在顶层配置中定义作为全局processor，其对全部事件生效。其对应的代码实现方式是： filebeat在使用libbeat pipeline的ConnectWith接口创建client时（factory.go中(*OutletFactory)Create函数），会将Input内部的定义processor作为参数传递给ConnectWith接口。而在ConnectWith实现中，会将参数中的processor和全局processor（在创建pipeline时生成）合并。从这里读者也可以发现，实际上每个Input都独享一个client，其包含一些Input自身的配置定义逻辑。&lt;/p&gt;

&lt;p&gt;任一Processor都实现了Processor接口：Run函数包含处理逻辑，String返回Processor名。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Processor interface {
    Run(event *beat.Event) (*beat.Event, error)
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再看下 Haveseter 是如何将数据写入缓存中的，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/produce-to-pipeline.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Harvester 通过 pipeline 提供的 pipelineClient 将数据写入到 pipeline 中，Haveseter 会将读到的数据会包装成一个 Event 结构体，再递交给 pipeline。&lt;/p&gt;

&lt;p&gt;在 Filebeat 的实现中，pipelineClient 并不直接操作缓存，而是将 event 先写入一个 events channel 中。&lt;/p&gt;

&lt;p&gt;同时，有一个 eventloop 组件，会监听 events channel 的事件到来，等 event 到达时，eventloop 会将其放入缓存中。&lt;/p&gt;

&lt;p&gt;当缓存满的时候，eventloop 直接移除对该 channel 的监听。&lt;/p&gt;

&lt;p&gt;每次 event ACK 或者取消后，缓存不再满了，则 eventloop 会重新监听 events channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// onMessage processes a new message read from the reader.
// This results in a state update and possibly an event would be send.
// A state update first updates the in memory state held by the prospector,
// and finally sends the file.State indirectly to the registrar.
// The events Private field is used to forward the file state update.
//
// onMessage returns &#39;false&#39; if it was interrupted in the process of sending the event.
// This normally signals a harvester shutdown.
func (h *Harvester) onMessage(
    forwarder *harvester.Forwarder,
    state file.State,
    message reader.Message,
    messageOffset int64,
) bool {
    if h.source.HasState() {
        h.states.Update(state)
    }

    text := string(message.Content)
    if message.IsEmpty() || !h.shouldExportLine(text) {
        // No data or event is filtered out -&amp;gt; send empty event with state update
        // only. The call can fail on filebeat shutdown.
        // The event will be filtered out, but forwarded to the registry as is.
        err := forwarder.Send(beat.Event{Private: state})
        return err == nil
    }

    fields := common.MapStr{
        &amp;quot;log&amp;quot;: common.MapStr{
            &amp;quot;offset&amp;quot;: messageOffset, // Offset here is the offset before the starting char.
            &amp;quot;file&amp;quot;: common.MapStr{
                &amp;quot;path&amp;quot;: state.Source,
            },
        },
    }
    fields.DeepUpdate(message.Fields)

    // Check if json fields exist
    var jsonFields common.MapStr
    if f, ok := fields[&amp;quot;json&amp;quot;]; ok {
        jsonFields = f.(common.MapStr)
    }

    var meta common.MapStr
    timestamp := message.Ts
    if h.config.JSON != nil &amp;amp;&amp;amp; len(jsonFields) &amp;gt; 0 {
        id, ts := readjson.MergeJSONFields(fields, jsonFields, &amp;amp;text, *h.config.JSON)
        if !ts.IsZero() {
            // there was a `@timestamp` key in the event, so overwrite
            // the resulting timestamp
            timestamp = ts
        }

        if id != &amp;quot;&amp;quot; {
            meta = common.MapStr{
                &amp;quot;id&amp;quot;: id,
            }
        }
    } else if &amp;amp;text != nil {
        if fields == nil {
            fields = common.MapStr{}
        }
        fields[&amp;quot;message&amp;quot;] = text
    }

    err := forwarder.Send(beat.Event{
        Timestamp: timestamp,
        Fields:    fields,
        Meta:      meta,
        Private:   state,
    })
    return err == nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将数据包装成event直接通过send方法将数据发出去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Send updates the input state and sends the event to the spooler
// All state updates done by the input itself are synchronous to make sure no states are overwritten
func (f *Forwarder) Send(event beat.Event) error {
    ok := f.Outlet.OnEvent(event)
    if !ok {
        logp.Info(&amp;quot;Input outlet closed&amp;quot;)
        return errors.New(&amp;quot;input outlet closed&amp;quot;)
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用Outlet.OnEvent发送data&lt;/p&gt;

&lt;p&gt;点进去发现是一个接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Outlet interface {
       OnEvent(data *util.Data) bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过调试观察，elastic\beats\filebeat\channel\outlet.go实现了这个接口&lt;/p&gt;

&lt;p&gt;outlet在Harvester的run一开始就创建了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;outlet := channel.CloseOnSignal(h.outletFactory(), h.done)
forwarder := harvester.NewForwarder(outlet)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以调用的OnEvent&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (o *outlet) OnEvent(event beat.Event) bool {
    if !o.isOpen.Load() {
        return false
    }

    if o.wg != nil {
        o.wg.Add(1)
    }

    o.client.Publish(event)

    // Note: race condition on shutdown:
    //  The underlying beat.Client is asynchronous. Without proper ACK
    //  handler we can not tell if the event made it &#39;through&#39; or the client
    //  close has been completed before sending. In either case,
    //  we report &#39;false&#39; here, indicating the event eventually being dropped.
    //  Returning false here, prevents the harvester from updating the state
    //  to the most recently published events. Therefore, on shutdown the harvester
    //  might report an old/outdated state update to the registry, overwriting the
    //  most recently
    //  published offset in the registry on shutdown.
    return o.isOpen.Load()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过client.Publish发送数据，client也是一个接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Client interface {
       Publish(Event)
       PublishAll([]Event)
       Close() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调试之后，client使用的是elastic\beats\libbeat\publisher\pipeline\client.go的client对象&lt;/p&gt;

&lt;p&gt;我们来看一下这个client是通过Harvester的参数outletFactory来初始化的，我们来看一下NewHarvester初始化的时候也就是在createHarvester的时候传递的参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// createHarvester creates a new harvester instance from the given state
func (p *Input) createHarvester(state file.State, onTerminate func()) (*Harvester, error) {
    // Each wraps the outlet, for closing the outlet individually
    h, err := NewHarvester(
        p.cfg,
        state,
        p.states,
        func(state file.State) bool {
            return p.stateOutlet.OnEvent(beat.Event{Private: state})
        },
        subOutletWrap(p.outlet),
    )
    if err == nil {
        h.onTerminate = onTerminate
    }
    return h, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是subOutletWrap中的参数p.outlet那就要看以下Input初始化的的时候NewInput传递的参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewInput instantiates a new Log
func NewInput(
    cfg *common.Config,
    outlet channel.Connector,
    context input.Context,
) (input.Input, error) {
    cleanupNeeded := true
    cleanupIfNeeded := func(f func() error) {
        if cleanupNeeded {
            f()
        }
    }

    // Note: underlying output.
    //  The input and harvester do have different requirements
    //  on the timings the outlets must be closed/unblocked.
    //  The outlet generated here is the underlying outlet, only closed
    //  once all workers have been shut down.
    //  For state updates and events, separate sub-outlets will be used.
    out, err := outlet.ConnectWith(cfg, beat.ClientConfig{
        Processing: beat.ProcessingConfig{
            DynamicFields: context.DynamicFields,
        },
    })
    if err != nil {
        return nil, err
    }
    defer cleanupIfNeeded(out.Close)

    // stateOut will only be unblocked if the beat is shut down.
    // otherwise it can block on a full publisher pipeline, so state updates
    // can be forwarded correctly to the registrar.
    stateOut := channel.CloseOnSignal(channel.SubOutlet(out), context.BeatDone)
    defer cleanupIfNeeded(stateOut.Close)

    meta := context.Meta
    if len(meta) == 0 {
        meta = nil
    }

    p := &amp;amp;Input{
        config:      defaultConfig,
        cfg:         cfg,
        harvesters:  harvester.NewRegistry(),
        outlet:      out,
        stateOutlet: stateOut,
        states:      file.NewStates(),
        done:        context.Done,
        meta:        meta,
    }

    if err := cfg.Unpack(&amp;amp;p.config); err != nil {
        return nil, err
    }
    if err := p.config.resolveRecursiveGlobs(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to resolve recursive globs in config: %v&amp;quot;, err)
    }
    if err := p.config.normalizeGlobPatterns(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to normalize globs patterns: %v&amp;quot;, err)
    }

    // Create empty harvester to check if configs are fine
    // TODO: Do config validation instead
    _, err = p.createHarvester(file.State{}, nil)
    if err != nil {
        return nil, err
    }

    if len(p.config.Paths) == 0 {
        return nil, fmt.Errorf(&amp;quot;each input must have at least one path defined&amp;quot;)
    }

    err = p.loadStates(context.States)
    if err != nil {
        return nil, err
    }

    logp.Info(&amp;quot;Configured paths: %v&amp;quot;, p.config.Paths)

    cleanupNeeded = false
    go p.stopWhenDone()

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要看outlet，这个是在Crawler的startInput的时候进行初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Crawler) startInput(
    pipeline beat.Pipeline,
    config *common.Config,
    states []file.State,
) error {
    if !config.Enabled() {
        return nil
    }

    connector := c.out(pipeline)
    p, err := input.New(config, connector, c.beatDone, states, nil)
    if err != nil {
        return fmt.Errorf(&amp;quot;Error while initializing input: %s&amp;quot;, err)
    }
    p.Once = c.once

    if _, ok := c.inputs[p.ID]; ok {
        return fmt.Errorf(&amp;quot;Input with same ID already exists: %d&amp;quot;, p.ID)
    }

    c.inputs[p.ID] = p

    p.Start()

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看out就是crawler创建new的时候传递的值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;crawler, err := crawler.New(
        channel.NewOutletFactory(outDone, wgEvents, b.Info).Create,
        config.Inputs,
        b.Info.Version,
        fb.done,
        *once)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是create返回的pipelineConnector结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *OutletFactory) Create(p beat.Pipeline) Connector {
    return &amp;amp;pipelineConnector{parent: f, pipeline: p}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看pipelineConnector的ConnectWith函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *pipelineConnector) ConnectWith(cfg *common.Config, clientCfg beat.ClientConfig) (Outleter, error) {
    config := inputOutletConfig{}
    if err := cfg.Unpack(&amp;amp;config); err != nil {
        return nil, err
    }

    procs, err := processorsForConfig(c.parent.beatInfo, config, clientCfg)
    if err != nil {
        return nil, err
    }

    setOptional := func(to common.MapStr, key string, value string) {
        if value != &amp;quot;&amp;quot; {
            to.Put(key, value)
        }
    }

    meta := clientCfg.Processing.Meta.Clone()
    fields := clientCfg.Processing.Fields.Clone()

    serviceType := config.ServiceType
    if serviceType == &amp;quot;&amp;quot; {
        serviceType = config.Module
    }

    setOptional(meta, &amp;quot;pipeline&amp;quot;, config.Pipeline)
    setOptional(fields, &amp;quot;fileset.name&amp;quot;, config.Fileset)
    setOptional(fields, &amp;quot;service.type&amp;quot;, serviceType)
    setOptional(fields, &amp;quot;input.type&amp;quot;, config.Type)
    if config.Module != &amp;quot;&amp;quot; {
        event := common.MapStr{&amp;quot;module&amp;quot;: config.Module}
        if config.Fileset != &amp;quot;&amp;quot; {
            event[&amp;quot;dataset&amp;quot;] = config.Module + &amp;quot;.&amp;quot; + config.Fileset
        }
        fields[&amp;quot;event&amp;quot;] = event
    }

    mode := clientCfg.PublishMode
    if mode == beat.DefaultGuarantees {
        mode = beat.GuaranteedSend
    }

    // connect with updated configuration
    clientCfg.PublishMode = mode
    clientCfg.Processing.EventMetadata = config.EventMetadata
    clientCfg.Processing.Meta = meta
    clientCfg.Processing.Fields = fields
    clientCfg.Processing.Processor = procs
    clientCfg.Processing.KeepNull = config.KeepNull
    client, err := c.pipeline.ConnectWith(clientCfg)
    if err != nil {
        return nil, err
    }

    outlet := newOutlet(client, c.parent.wgEvents)
    if c.parent.done != nil {
        return CloseOnSignal(outlet, c.parent.done), nil
    }
    return outlet, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边获取到了pipeline的客户端client&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// ConnectWith create a new Client for publishing events to the pipeline.
// The client behavior on close and ACK handling can be configured by setting
// the appropriate fields in the passed ClientConfig.
// If not set otherwise the defaut publish mode is OutputChooses.
func (p *Pipeline) ConnectWith(cfg beat.ClientConfig) (beat.Client, error) {
    var (
        canDrop      bool
        dropOnCancel bool
        eventFlags   publisher.EventFlags
    )

    err := validateClientConfig(&amp;amp;cfg)
    if err != nil {
        return nil, err
    }

    p.eventer.mutex.Lock()
    p.eventer.modifyable = false
    p.eventer.mutex.Unlock()

    switch cfg.PublishMode {
    case beat.GuaranteedSend:
        eventFlags = publisher.GuaranteedSend
        dropOnCancel = true
    case beat.DropIfFull:
        canDrop = true
    }

    waitClose := cfg.WaitClose
    reportEvents := p.waitCloser != nil

    switch p.waitCloseMode {
    case NoWaitOnClose:

    case WaitOnClientClose:
        if waitClose &amp;lt;= 0 {
            waitClose = p.waitCloseTimeout
        }
    }

    processors, err := p.createEventProcessing(cfg.Processing, publishDisabled)
    if err != nil {
        return nil, err
    }

    client := &amp;amp;client{
        pipeline:     p,
        closeRef:     cfg.CloseRef,
        done:         make(chan struct{}),
        isOpen:       atomic.MakeBool(true),
        eventer:      cfg.Events,
        processors:   processors,
        eventFlags:   eventFlags,
        canDrop:      canDrop,
        reportEvents: reportEvents,
    }

    acker := p.makeACKer(processors != nil, &amp;amp;cfg, waitClose, client.unlink)
    producerCfg := queue.ProducerConfig{
        // Cancel events from queue if acker is configured
        // and no pipeline-wide ACK handler is registered.
        DropOnCancel: dropOnCancel &amp;amp;&amp;amp; acker != nil &amp;amp;&amp;amp; p.eventer.cb == nil,
    }

    if reportEvents || cfg.Events != nil {
        producerCfg.OnDrop = func(event beat.Event) {
            if cfg.Events != nil {
                cfg.Events.DroppedOnPublish(event)
            }
            if reportEvents {
                p.waitCloser.dec(1)
            }
        }
    }

    if acker != nil {
        producerCfg.ACK = acker.ackEvents
    } else {
        acker = newCloseACKer(nilACKer, client.unlink)
    }

    client.acker = acker
    client.producer = p.queue.Producer(producerCfg)

    p.observer.clientConnected()

    if client.closeRef != nil {
        p.registerSignalPropagation(client)
    }

    return client, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的就是client的Publish函数来发送数据，publish方法即发送日志的方法，如果需要在发送前改造日志格式，可在这里添加代码，如下面的解析日志代码。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) Publish(e beat.Event) {
    c.mutex.Lock()
    defer c.mutex.Unlock()

    c.publish(e)
}

func (c *client) publish(e beat.Event) {
    var (
        event   = &amp;amp;e
        publish = true
        log     = c.pipeline.monitors.Logger
    )

    c.onNewEvent()

    if !c.isOpen.Load() {
        // client is closing down -&amp;gt; report event as dropped and return
        c.onDroppedOnPublish(e)
        return
    }

    if c.processors != nil {
        var err error

        event, err = c.processors.Run(event)
        publish = event != nil
        if err != nil {
            // TODO: introduce dead-letter queue?

            log.Errorf(&amp;quot;Failed to publish event: %v&amp;quot;, err)
        }
    }

    if event != nil {
        e = *event
    }

    open := c.acker.addEvent(e, publish)
    if !open {
        // client is closing down -&amp;gt; report event as dropped and return
        c.onDroppedOnPublish(e)
        return
    }

    if !publish {
        c.onFilteredOut(e)
        return
    }

    e = *event
    pubEvent := publisher.Event{
        Content: e,
        Flags:   c.eventFlags,
    }

    if c.reportEvents {
        c.pipeline.waitCloser.inc()
    }

    var published bool
    if c.canDrop {
        published = c.producer.TryPublish(pubEvent)
    } else {
        published = c.producer.Publish(pubEvent)
    }

    if published {
        c.onPublished()
    } else {
        c.onDroppedOnPublish(e)
        if c.reportEvents {
            c.pipeline.waitCloser.dec(1)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上面创建clinet的时候，创建了队列的生产者，也就是之前broker的Producer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) Producer(cfg queue.ProducerConfig) queue.Producer {
    return newProducer(b, cfg.ACK, cfg.OnDrop, cfg.DropOnCancel)
}

func newProducer(b *Broker, cb ackHandler, dropCB func(beat.Event), dropOnCancel bool) queue.Producer {
    openState := openState{
        log:    b.logger,
        isOpen: atomic.MakeBool(true),
        done:   make(chan struct{}),
        events: b.events,
    }

    if cb != nil {
        p := &amp;amp;ackProducer{broker: b, seq: 1, cancel: dropOnCancel, openState: openState}
        p.state.cb = cb
        p.state.dropCB = dropCB
        return p
    }
    return &amp;amp;forgetfulProducer{broker: b, openState: openState}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是forgetfulProducer结构体，调用这个的Publish函数来发送数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *forgetfulProducer) Publish(event publisher.Event) bool {
    return p.openState.publish(p.makeRequest(event))
}

func (st *openState) publish(req pushRequest) bool {
    select {
    case st.events &amp;lt;- req:
        return true
    case &amp;lt;-st.done:
        st.events = nil
        return false
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将数据放到了forgetfulProducer的openState的events中。到此数据就算发送到pipeline中了。&lt;/p&gt;

&lt;p&gt;上文在pipeline的初始化的时候，queue初始化一般默认都是BufferingEventLoop，即带缓冲的队列。BufferingEventLoop是一个实现了Broker、带有各种channel的结构，主要用于将日志发送至consumer消费。 BufferingEventLoop的run方法中，同样是一个无限循环，这里可以认为是一个日志事件的调度中心。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
        select {
        case &amp;lt;-broker.done:
            return
        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)
        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)
        case count := &amp;lt;-l.acks:
            l.handleACK(count)
        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上文中harvester goroutine每次读取到日志数据之后，最终会被发送至bufferingEventLoop中的events chan pushRequest 的channel中，然后触发上面req := &amp;lt;-l.events的case，handleInsert方法会把数据添加至bufferingEventLoop的buf中，buf即memqueue实际缓存日志数据的队列，如果buf长度超过配置的最大值或者bufferingEventLoop中的timer定时器（默认1S）触发了case &amp;lt;-l.idleC，均会调用flushBuffer()方法。
flushBuffer()又会触发req := &amp;lt;-l.get的case，然后运行handleConsumer方法，该方法中最重要的是这一句代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;req.resp &amp;lt;- getResponse{ackChan, events}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里获取到了consumer消费者的response channel，然后发送数据给这个channel。真正到这，才会触发consumer对memqueue的消费。所以，其实memqueue并非一直不停的在被consumer消费，而是在memqueue通知consumer的时候才被消费，我们可以理解为一种脉冲式的发送&lt;/p&gt;

&lt;p&gt;简单的来说就是，每当队列中的数据缓存到一定的大小或者超过了定时的时间（默认1s)，会被注册的client从队列中消费，发送至配置的后端。&lt;/p&gt;

&lt;p&gt;以上是 Pipeline 的写入过程，此时 event 已被写入到了缓存中。&lt;/p&gt;

&lt;p&gt;但是 Output 是如何从缓存中拿到 event 数据的？&lt;/p&gt;

&lt;h3 id=&#34;pipeline-的消费过程&#34;&gt;Pipeline 的消费过程&lt;/h3&gt;

&lt;p&gt;在上文已经提到过，filebeat初始化的时候，就已经创建了一个eventConsumer并在loop无限循环方法里试图从Broker中其实也就是上面的resp中获取日志数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; consumer != nil &amp;amp;&amp;amp; batch == nil {
            out = c.out.workQueue
            queueBatch, err := consumer.Get(c.out.batchSize)
            ...
            batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
        }
        ...
        select {
        case &amp;lt;-c.done:
            return
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
        case out &amp;lt;- batch:
            batch = nil
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面consumer.Get就是消费者consumer从Broker中获取日志数据，然后发送至out的channel中被output client发送，我们看一下Get方法里的核心代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select {
    case c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:
    case &amp;lt;-c.done:
        return nil, io.EOF
    }

    // if request has been send, we do have to wait for a response
    resp := &amp;lt;-c.resp
    return &amp;amp;batch{
        consumer: c,
        events:   resp.buf,
        ack:      resp.ack,
        state:    batchActive,
    }, nil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;getRequest的结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type getRequest struct {
    sz   int              // request sz events from the broker
    resp chan getResponse // channel to send response to
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;getResponse的结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type getResponse struct {
    ack *ackChan
    buf []publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;getResponse里包含了日志的数据，而getRequest包含了一个发送至消费者的channel。
在上文bufferingEventLoop缓冲队列的handleConsumer方法里接收到的参数为getRequest，里面包含了consumer请求的getResponse channel。
如果handleConsumer不发送数据，consumer.Get方法会一直阻塞在select中，直到flushBuffer，consumer的getResponse channel才会接收到日志数据。&lt;/p&gt;

&lt;p&gt;我们来看看bufferingEventLoop的调度中心&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) run() {
    var (
        broker = l.broker
    )

    for {
        select {
        case &amp;lt;-broker.done:
            return

        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)

        case req := &amp;lt;-l.pubCancel: // producer cancelling active events
            l.handleCancel(&amp;amp;req)

        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)

        case l.schedACKS &amp;lt;- l.pendingACKs:
            l.schedACKS = nil
            l.pendingACKs = chanList{}

        case count := &amp;lt;-l.acks:
            l.handleACK(count)

        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}时候，l.get会得到信息，为什么l.get会得到信息，因为l.get = l.broker.requests&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) flushBuffer() {
    l.buf.flushed = true

    if l.buf.length() == 0 {
        panic(&amp;quot;flushing empty buffer&amp;quot;)
    }

    l.flushList.add(l.buf)
    l.get = l.broker.requests
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看handleConsumer如何处理信息的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) handleConsumer(req *getRequest) {
    buf := l.flushList.head
    if buf == nil {
        panic(&amp;quot;get from non-flushed buffers&amp;quot;)
    }

    count := buf.length()
    if count == 0 {
        panic(&amp;quot;empty buffer in flush list&amp;quot;)
    }

    if sz := req.sz; sz &amp;gt; 0 {
        if sz &amp;lt; count {
            count = sz
        }
    }

    if count == 0 {
        panic(&amp;quot;empty batch returned&amp;quot;)
    }

    events := buf.events[:count]
    clients := buf.clients[:count]
    ackChan := newACKChan(l.ackSeq, 0, count, clients)
    l.ackSeq++

    req.resp &amp;lt;- getResponse{ackChan, events}
    l.pendingACKs.append(ackChan)
    l.schedACKS = l.broker.scheduledACKs

    buf.events = buf.events[count:]
    buf.clients = buf.clients[count:]
    if buf.length() == 0 {
        l.advanceFlushList()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;处理req，并且将数据发送给req的resp，在发送的时候c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:就将c.resp赋值给了req的resp。所以可以获得返回值getResponse，组装成batch发送出去，其实就是放到type workQueue chan *Batch这个barch类型的channel中。&lt;/p&gt;

&lt;p&gt;看一下getResponse&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type getResponse struct {
    ack *ackChan
    buf []publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见ack就是channel，buf就是发送的日志。&lt;/p&gt;

&lt;p&gt;整个消费的过程非常复杂，数据会在多个 channel 之间传递流转，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/consume-from-pipeline.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;首先再介绍两个角色：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;consumer： pipeline 在创建的时候，会同时创建一个 consumer。consumer 负责从缓存中取数据
client worker：负责接收 consumer 传来的数据，并调用 Output 的 Publish 函数进行上报。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;与 producer 类似，consumer 也不直接操作缓存，而是会向 get channel 中写入消费请求。&lt;/p&gt;

&lt;p&gt;consumer 本身是个后台 loop 的过程，这个消费请求会不断进行。&lt;/p&gt;

&lt;p&gt;eventloop 监听 get channel, 拿到之后会从缓存中取数据。并将数据写入到 resp channel 中。&lt;/p&gt;

&lt;p&gt;consumer 从 resp channel 中拿到 event 数据后，又会将其写入到 workQueue。&lt;/p&gt;

&lt;p&gt;workQueue 也是个 channel。client worker 会监听该 channel 上的数据到来，将数据交给 Output client 进行 Publish 上报。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type workQueue chan *Batch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而且，Output 收到的是 Batch Events，即会一次收到一批 Events。BatchSize 由各个 Output 自行决定。&lt;/p&gt;

&lt;p&gt;至此，消息已经递交给了 Output 组件。&lt;/p&gt;

&lt;h3 id=&#34;output&#34;&gt;output&lt;/h3&gt;

&lt;p&gt;Filebeat 并不依赖于 Elasticsearch，可以单独存在。我们可以单独使用 Filebeat 进行日志的上报和搜集。filebeat 内置了常用的 Output 组件, 例如 kafka、Elasticsearch、redis 等。出于调试考虑，也可以输出到 console 和 file。我们可以利用现有的 Output 组件，将日志进行上报。&lt;/p&gt;

&lt;p&gt;当然，我们也可以自定义 Output 组件，让 Filebeat 将日志转发到我们想要的地方。&lt;/p&gt;

&lt;p&gt;在上文提到过，在pipeline初始化的时候，就会设置output的clinet，会创建一个clientWorker或者netClientWorker（可重连，默认就是这个），clientWorker的run方法中，会不停的从consumer发送的channel（就是上面的workQueue）里读取日志数据，然后调用client.Publish批量发送日志。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (w *netClientWorker) run() {
    for !w.closed.Load() {
        reconnectAttempts := 0

        // start initial connect loop from first batch, but return
        // batch to pipeline for other outputs to catch up while we&#39;re trying to connect
        for batch := range w.qu {
            batch.Cancelled()

            if w.closed.Load() {
                logp.Info(&amp;quot;Closed connection to %v&amp;quot;, w.client)
                return
            }

            if reconnectAttempts &amp;gt; 0 {
                logp.Info(&amp;quot;Attempting to reconnect to %v with %d reconnect attempt(s)&amp;quot;, w.client, reconnectAttempts)
            } else {
                logp.Info(&amp;quot;Connecting to %v&amp;quot;, w.client)
            }

            err := w.client.Connect()
            if err != nil {
                logp.Err(&amp;quot;Failed to connect to %v: %v&amp;quot;, w.client, err)
                reconnectAttempts++
                continue
            }

            logp.Info(&amp;quot;Connection to %v established&amp;quot;, w.client)
            reconnectAttempts = 0
            break
        }

        // send loop
        for batch := range w.qu {
            if w.closed.Load() {
                if batch != nil {
                    batch.Cancelled()
                }
                return
            }

            err := w.client.Publish(batch)
            if err != nil {
                logp.Err(&amp;quot;Failed to publish events: %v&amp;quot;, err)
                // on error return to connect loop
                break
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;libbeats库中包含了kafka、elasticsearch、logstash等几种client，它们均实现了client接口：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Client interface {
    Close() error
    Publish(publisher.Batch) error
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然最重要的是实现Publish接口，然后将日志发送出去。比如我们看一下kafka的Publish接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) Publish(batch publisher.Batch) error {
    events := batch.Events()
    c.observer.NewBatch(len(events))

    ref := &amp;amp;msgRef{
        client: c,
        count:  int32(len(events)),
        total:  len(events),
        failed: nil,
        batch:  batch,
    }

    ch := c.producer.Input()
    for i := range events {
        d := &amp;amp;events[i]
        msg, err := c.getEventMessage(d)
        if err != nil {
            logp.Err(&amp;quot;Dropping event: %v&amp;quot;, err)
            ref.done()
            c.observer.Dropped(1)
            continue
        }

        msg.ref = ref
        msg.initProducerMessage()
        ch &amp;lt;- &amp;amp;msg.msg
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是基本的kafka客户端的使用方法，到此为止，数据也就发送的kakfa了。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;其实在pipleline调度的时候就说明了queue的生产消费的关系，数据在各个channel中进行传输，整个日志数据流转的过程还是表复杂的，在各个channel中进行流转，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/datastream.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;registry和ack-机制&#34;&gt;registry和Ack 机制&lt;/h2&gt;

&lt;p&gt;Filebeat 的可靠性很强，可以保证日志 At least once 的上报，同时也考虑了日志搜集中的各类问题，例如日志断点续读、文件名更改、日志 Truncated 等。&lt;/p&gt;

&lt;p&gt;filebeat 之所以可以保证日志可以 at least once 的上报，就是基于其 Ack 机制。&lt;/p&gt;

&lt;p&gt;简单来说，Ack 机制就是，当 Output Publish 成功之后会调用 ACK，最终 Registrar 会收到 ACK，并修改偏移量。&lt;/p&gt;

&lt;p&gt;而且, Registrar 只会在 Output 调用 batch 的相关信号时，才改变文件偏移量。其中 Batch 对外提供了这些信号：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Batch interface {
    Events() []Event

    // signals
    ACK()
    Drop()
    Retry()
    RetryEvents(events []Event)
    Cancelled()
    CancelledEvents(events []Event)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output 在 Publish 之后，无论失败，必须调用这些函数中的其中一个。&lt;/p&gt;

&lt;p&gt;以下是 Output Publish 成功后调用 Ack 的流程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/ack.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到其中起核心作用的组件是 Ackloop。AckLoop 中有一个 ackChanList，其中每一个 ackChan，对应于转发给 Output 的一个 Batch。
每次新建一个 Batch，同时会建立一个 ackChan，该 ackChan 会被 append 到 ackChanList 中。&lt;/p&gt;

&lt;p&gt;而 AckLoop 每次只监听处于 ackChanList 最头部的 ackChan。&lt;/p&gt;

&lt;p&gt;当 Batch 被 Output 调用 Ack 后，AckLoop 会收到对应 ackChan 上的事件，并将其最终转发给 Registrar。同时，ackChanList 将会 pop 头部的 ackChan，继续监听接下来的 Ack 事件。&lt;/p&gt;

&lt;p&gt;由于 FileBeat 是 At least once 的上报，但并不保证 Exactly once, 因此一条数据可能会被上报多次，所以接收端需要自行进行去重过滤。&lt;/p&gt;

&lt;p&gt;上面状态的修改，主要是filebeat维护了一个registry文件在本地的磁盘，该registry文件维护了所有已经采集的日志文件的状态。 实际上，每当日志数据发送至后端成功后，会返回ack事件。filebeat启动了一个独立的registry协程负责监听该事件，接收到ack事件后会将日志文件的State状态更新至registry文件中，State中的Offset表示读取到的文件偏移量，所以filebeat会保证Offset记录之前的日志数据肯定被后端的日志存储接收到。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;pipeline初始化的ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先是pipeline初始化只有一次，在这个时候只是简单的初始化了pipeline的ack相关信息，这边也创建的一个queue，原始是使用一个queue，这边初始化broker的时候会创建ack.run()来监听，后来改造多kafka发送后这一条queue
是不用的，而且每次连接kafka的时候创建一个新queue的时候，会都会创建一个ack.run()来监听，流程是一样的，改造可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/#支持多kafka的发送&#34;&gt;支持多kafka的发送&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.ackBuilder = &amp;amp;pipelineEmptyACK{p}
p.ackActive = atomic.MakeBool(true)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是在创建queue的时候，默认是使用mem的queue，会创建ack。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ack := newACKLoop(b, eventLoop.processACK)

b.wg.Add(2)
go func() {
    defer b.wg.Done()
    eventLoop.run()
}()
go func() {
    defer b.wg.Done()
    ack.run()
}()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在创建newBufferingEventLoop队列的同时，会newACKLoop并且调用相应结构体的run函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newACKLoop(b *Broker, processACK func(chanList, int)) *ackLoop {
    l := &amp;amp;ackLoop{broker: b}
    l.processACK = processACK
    return l
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ackLoop struct {
    broker *Broker
    sig    chan batchAckMsg
    lst    chanList

    totalACK   uint64
    totalSched uint64

    batchesSched uint64
    batchesACKed uint64

    processACK func(chanList, int)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看一下run函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *ackLoop) run() {
    var (
        // log = l.broker.logger

        // Buffer up acked event counter in acked. If acked &amp;gt; 0, acks will be set to
        // the broker.acks channel for sending the ACKs while potentially receiving
        // new batches from the broker event loop.
        // This concurrent bidirectionally communication pattern requiring &#39;select&#39;
        // ensures we can not have any deadlock between the event loop and the ack
        // loop, as the ack loop will not block on any channel
        acked int
        acks  chan int
    )

    for {
        select {
        case &amp;lt;-l.broker.done:
            // TODO: handle pending ACKs?
            // TODO: panic on pending batches?
            return

        case acks &amp;lt;- acked:
            acks, acked = nil, 0

        case lst := &amp;lt;-l.broker.scheduledACKs:
            count, events := lst.count()
            l.lst.concat(&amp;amp;lst)

            // log.Debug(&amp;quot;ACK List:&amp;quot;)
            // for current := l.lst.head; current != nil; current = current.next {
            //  log.Debugf(&amp;quot;  ack entry(seq=%v, start=%v, count=%v&amp;quot;,
            //      current.seq, current.start, current.count)
            // }

            l.batchesSched += uint64(count)
            l.totalSched += uint64(events)

        case &amp;lt;-l.sig:
            acked += l.handleBatchSig()
            if acked &amp;gt; 0 {
                acks = l.broker.acks
            }
        }

        // log.Debug(&amp;quot;ackloop INFO&amp;quot;)
        // log.Debug(&amp;quot;ackloop:   total events scheduled = &amp;quot;, l.totalSched)
        // log.Debug(&amp;quot;ackloop:   total events ack = &amp;quot;, l.totalACK)
        // log.Debug(&amp;quot;ackloop:   total batches scheduled = &amp;quot;, l.batchesSched)
        // log.Debug(&amp;quot;ackloop:   total batches ack = &amp;quot;, l.batchesACKed)

        l.sig = l.lst.channel()
        // if l.sig == nil {
        //  log.Debug(&amp;quot;ackloop: no ack scheduled&amp;quot;)
        // } else {
        //  log.Debug(&amp;quot;ackloop: schedule ack: &amp;quot;, l.lst.head.seq)
        // }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边其实就是对ack信号的调度处理中心。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;registrar初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后就是启动filebeat的时候可能是要初始化registrar&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Setup registrar to persist state
registrar, err := registrar.New(config.Registry, finishedLogger)
if err != nil {
    logp.Err(&amp;quot;Could not init registrar: %v&amp;quot;, err)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;config.Registry就是registry文件的配置信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Registry struct {
    Path         string        `config:&amp;quot;path&amp;quot;`
    Permissions  os.FileMode   `config:&amp;quot;file_permissions&amp;quot;`
    FlushTimeout time.Duration `config:&amp;quot;flush&amp;quot;`
    MigrateFile  string        `config:&amp;quot;migrate_file&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建方法很简单，就是对文件的一些描述赋值给了这个结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New creates a new Registrar instance, updating the registry file on
// `file.State` updates. New fails if the file can not be opened or created.
func New(cfg config.Registry, out successLogger) (*Registrar, error) {
    home := paths.Resolve(paths.Data, cfg.Path)
    migrateFile := cfg.MigrateFile
    if migrateFile != &amp;quot;&amp;quot; {
        migrateFile = paths.Resolve(paths.Data, migrateFile)
    }

    err := ensureCurrent(home, migrateFile, cfg.Permissions)
    if err != nil {
        return nil, err
    }

    dataFile := filepath.Join(home, &amp;quot;filebeat&amp;quot;, &amp;quot;data.json&amp;quot;)
    r := &amp;amp;Registrar{
        registryFile: dataFile,
        fileMode:     cfg.Permissions,
        done:         make(chan struct{}),
        states:       file.NewStates(),
        Channel:      make(chan []file.State, 1),
        flushTimeout: cfg.FlushTimeout,
        out:          out,
        wg:           sync.WaitGroup{},
    }
    return r, r.Init()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后返回Registrar结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Registrar struct {
    Channel      chan []file.State
    out          successLogger
    done         chan struct{}
    registryFile string      // Path to the Registry File
    fileMode     os.FileMode // Permissions to apply on the Registry File
    wg           sync.WaitGroup

    states               *file.States // Map with all file paths inside and the corresponding state
    gcRequired           bool         // gcRequired is set if registry state needs to be gc&#39;ed before the next write
    gcEnabled            bool         // gcEnabled indicates the registry contains some state that can be gc&#39;ed in the future
    flushTimeout         time.Duration
    bufferedStateUpdates int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还进行了初始化，主要是对文件进行了一些检查。然后就是启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Start the registrar
err = registrar.Start()
if err != nil {
    return fmt.Errorf(&amp;quot;Could not start registrar: %v&amp;quot;, err)
}

func (r *Registrar) Start() error {
    // Load the previous log file locations now, for use in input
    err := r.loadStates()
    if err != nil {
        return fmt.Errorf(&amp;quot;Error loading state: %v&amp;quot;, err)
    }

    r.wg.Add(1)
    go r.Run()

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先就是加载了目前存在的文件的状态来赋值给结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// loadStates fetches the previous reading state from the configure RegistryFile file
// The default file is `registry` in the data path.
func (r *Registrar) loadStates() error {
    f, err := os.Open(r.registryFile)
    if err != nil {
        return err
    }

    defer f.Close()

    logp.Info(&amp;quot;Loading registrar data from %s&amp;quot;, r.registryFile)

    states, err := readStatesFrom(f)
    if err != nil {
        return err
    }
    r.states.SetStates(states)
    logp.Info(&amp;quot;States Loaded from registrar: %+v&amp;quot;, len(states))

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后开始监听来更新文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Registrar) Run() {
    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Starting Registrar&amp;quot;)
    // Writes registry on shutdown
    defer func() {
        r.writeRegistry()
        r.wg.Done()
    }()

    var (
        timer  *time.Timer
        flushC &amp;lt;-chan time.Time
    )

    for {
        select {
        case &amp;lt;-r.done:
            logp.Info(&amp;quot;Ending Registrar&amp;quot;)
            return
        case &amp;lt;-flushC:
            flushC = nil
            timer.Stop()
            r.flushRegistry()
        case states := &amp;lt;-r.Channel:
            r.onEvents(states)
            if r.flushTimeout &amp;lt;= 0 {
                r.flushRegistry()
            } else if flushC == nil {
                timer = time.NewTimer(r.flushTimeout)
                flushC = timer.C
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当接受到Registrar中channel的发来的文件状态，就更新结构体的值，如果到时间了就将内存中的值刷新到本地文件中，如果没有就定一个timeout时间后刷新到本地文件中。&lt;/p&gt;

&lt;p&gt;我们可以简单的看一下这个channel&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Channel:      make(chan []file.State, 1),
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是一个文件状态的channel，关于文件状态的结构体如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// State is used to communicate the reading state of a file
type State struct {
    Id          string            `json:&amp;quot;-&amp;quot;` // local unique id to make comparison more efficient
    Finished    bool              `json:&amp;quot;-&amp;quot;` // harvester state
    Fileinfo    os.FileInfo       `json:&amp;quot;-&amp;quot;` // the file info
    Source      string            `json:&amp;quot;source&amp;quot;`
    Offset      int64             `json:&amp;quot;offset&amp;quot;`
    Timestamp   time.Time         `json:&amp;quot;timestamp&amp;quot;`
    TTL         time.Duration     `json:&amp;quot;ttl&amp;quot;`
    Type        string            `json:&amp;quot;type&amp;quot;`
    Meta        map[string]string `json:&amp;quot;meta&amp;quot;`
    FileStateOS file.StateOS
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记录在registry文件中的数据大致如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[{&amp;quot;source&amp;quot;:&amp;quot;/tmp/aa.log&amp;quot;,&amp;quot;offset&amp;quot;:48,&amp;quot;timestamp&amp;quot;:&amp;quot;2019-07-03T13:54:01.298995+08:00&amp;quot;,&amp;quot;ttl&amp;quot;:-1,&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;meta&amp;quot;:null,&amp;quot;FileStateOS&amp;quot;:{&amp;quot;inode&amp;quot;:7048952,&amp;quot;device&amp;quot;:16777220}}]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于文件可能会被改名或移动，filebeat会根据inode和设备号来标志每个日志文件。&lt;/p&gt;

&lt;p&gt;到这边registrar启动也结束了，下面就是监控registrar中channel的数据，在启动的时候还做了一件事情，那就是把channel设置到pipeline中去。&lt;/p&gt;

&lt;p&gt;在构建registrar的时候，通过registrar中channel构建一个结构体registrarLogger&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type registrarLogger struct {
    done chan struct{}
    ch   chan&amp;lt;- []file.State
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是用来交互的结构体,这个结构体中的channel获取的文件状态就是给上面的监听程序进行处理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Make sure all events that were published in
registrarChannel := newRegistrarLogger(registrar)

func newRegistrarLogger(reg *registrar.Registrar) *registrarLogger {
    return &amp;amp;registrarLogger{
        done: make(chan struct{}),
        ch:   reg.Channel,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过这个registrarLogger结构体，做了如下的调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err = b.Publisher.SetACKHandler(beat.PipelineACKHandler{
    ACKEvents: newEventACKer(finishedLogger, registrarChannel).ackEvents,
})
if err != nil {
    logp.Err(&amp;quot;Failed to install the registry with the publisher pipeline: %v&amp;quot;, err)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先看一下newEventACKer(finishedLogger, registrarChannel)这个结构体的ackEvents函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACKer(stateless statelessLogger, stateful statefulLogger) *eventACKer {
    return &amp;amp;eventACKer{stateless: stateless, stateful: stateful, log: logp.NewLogger(&amp;quot;acker&amp;quot;)}
}

func (a *eventACKer) ackEvents(data []interface{}) {
    stateless := 0
    states := make([]file.State, 0, len(data))
    for _, datum := range data {
        if datum == nil {
            stateless++
            continue
        }

        st, ok := datum.(file.State)
        if !ok {
            stateless++
            continue
        }

        states = append(states, st)
    }

    if len(states) &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateful ack&amp;quot;, &amp;quot;count&amp;quot;, len(states))
        a.stateful.Published(states)
    }

    if stateless &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateless ack&amp;quot;, &amp;quot;count&amp;quot;, stateless)
        a.stateless.Published(stateless)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见是一个eventACKer结构体的函数赋值给了beat.PipelineACKHandler的成员函数，我们再来看一下beat.PipelineACKHandler&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// PipelineACKHandler configures some pipeline-wide event ACK handler.
type PipelineACKHandler struct {
    // ACKCount reports the number of published events recently acknowledged
    // by the pipeline.
    ACKCount func(int)

    // ACKEvents reports the events recently acknowledged by the pipeline.
    // Only the events &#39;Private&#39; field will be reported.
    ACKEvents func([]interface{})

    // ACKLastEvent reports the last ACKed event per pipeline client.
    // Only the events &#39;Private&#39; field will be reported.
    ACKLastEvents func([]interface{})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是一个这样的结构体作为参数，最后我们来看一下SetACKHandler这个函数的调用，首先b的就是libbeat中创建的beat，其中的Publisher就是对应的初始化的Pipeline，看一下Pipeline的SetACKHandler方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// SetACKHandler sets a global ACK handler on all events published to the pipeline.
// SetACKHandler must be called before any connection is made.
func (p *Pipeline) SetACKHandler(handler beat.PipelineACKHandler) error {
    p.eventer.mutex.Lock()
    defer p.eventer.mutex.Unlock()

    if !p.eventer.modifyable {
        return errors.New(&amp;quot;can not set ack handler on already active pipeline&amp;quot;)
    }

    // TODO: check only one type being configured

    cb, err := newPipelineEventCB(handler)
    if err != nil {
        return err
    }

    if cb == nil {
        p.ackBuilder = &amp;amp;pipelineEmptyACK{p}
        p.eventer.cb = nil
        return nil
    }

    p.eventer.cb = cb
    if cb.mode == countACKMode {
        p.ackBuilder = &amp;amp;pipelineCountACK{
            pipeline: p,
            cb:       cb.onCounts,
        }
    } else {
        p.ackBuilder = &amp;amp;pipelineEventsACK{
            pipeline: p,
            cb:       cb.onEvents,
        }
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;newPipelineEventCB是根据传递的不同函数，创建不同mode的pipelineEventCB结构体，启动goroutine来work。我们这边传递的是ACKEvents，设置mode为eventsACKMode&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newPipelineEventCB(handler beat.PipelineACKHandler) (*pipelineEventCB, error) {
    mode := noACKMode
    if handler.ACKCount != nil {
        mode = countACKMode
    }
    if handler.ACKEvents != nil {
        if mode != noACKMode {
            return nil, errors.New(&amp;quot;only one callback can be set&amp;quot;)
        }
        mode = eventsACKMode
    }
    if handler.ACKLastEvents != nil {
        if mode != noACKMode {
            return nil, errors.New(&amp;quot;only one callback can be set&amp;quot;)
        }
        mode = lastEventsACKMode
    }

    // yay, no work
    if mode == noACKMode {
        return nil, nil
    }

    cb := &amp;amp;pipelineEventCB{
        acks:          make(chan int),
        mode:          mode,
        handler:       handler,
        events:        make(chan eventsDataMsg),
        droppedEvents: make(chan eventsDataMsg),
    }
    go cb.worker()
    return cb, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看worker工作协程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) worker() {
    defer close(p.acks)
    defer close(p.events)
    defer close(p.droppedEvents)

    for {
        select {
        case count := &amp;lt;-p.acks:
            exit := p.collect(count)
            if exit {
                return
            }

            // short circuit dropped events, but have client block until all events
            // have been processed by pipeline ack handler
        case msg := &amp;lt;-p.droppedEvents:
            p.reportEventsData(msg.data, msg.total)
            if msg.sig != nil {
                close(msg.sig)
            }

        case &amp;lt;-p.done:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时对于不同的mode对p.ackBuilder进行了重新构建，因为是代码设置mode为eventsACKMode&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.ackBuilder = &amp;amp;pipelineEventsACK{
    pipeline: p,
    cb:       cb.onEvents,
}

type pipelineEventsACK struct {
    pipeline *Pipeline
    cb       func([]interface{}, int)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里启动就结束了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;input初始化的ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;crawler启动后构建新的input构建的时候，需要获取到pipeline的client，在使用ConnectWith进行构建的时候，会构建client的acker，第一次参数是processors != nil，影响后的结构体的创建，一般是true&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acker := p.makeACKer(processors != nil, &amp;amp;cfg, waitClose, client.unlink)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下makeACKer这个函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Pipeline) makeACKer(
    canDrop bool,
    cfg *beat.ClientConfig,
    waitClose time.Duration,
    afterClose func(),
) acker {
    var (
        bld   = p.ackBuilder
        acker acker
    )

    sema := p.eventSema
    switch {
    case cfg.ACKCount != nil:
        acker = bld.createCountACKer(canDrop, sema, cfg.ACKCount)
    case cfg.ACKEvents != nil:
        acker = bld.createEventACKer(canDrop, sema, cfg.ACKEvents)
    case cfg.ACKLastEvent != nil:
        cb := lastEventACK(cfg.ACKLastEvent)
        acker = bld.createEventACKer(canDrop, sema, cb)
    default:
        if waitClose &amp;lt;= 0 {
            acker = bld.createPipelineACKer(canDrop, sema)
        } else {
            acker = bld.createCountACKer(canDrop, sema, func(_ int) {})
        }
    }

    if waitClose &amp;lt;= 0 {
        return newCloseACKer(acker, afterClose)
    }
    return newWaitACK(acker, waitClose, afterClose)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要使用p.ackBuilder的create函数，我们在上面SetACKHandler的时候构建了p.ackBuilder，根据cfg配置调用，默认调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acker = bld.createEventACKer(canDrop, sema, cfg.ACKEvents)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *pipelineEventsACK) createEventACKer(canDrop bool, sema *sema, fn func([]interface{})) acker {
    return buildClientEventACK(b.pipeline, canDrop, sema, func(guard *clientACKer) func([]interface{}, int) {
        return func(data []interface{}, acked int) {
            b.cb(data, acked)
            if guard.Active() {
                fn(data)
            }
        }
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用函数buildClientEventACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func buildClientEventACK(
    pipeline *Pipeline,
    canDrop bool,
    sema *sema,
    mk func(*clientACKer) func([]interface{}, int),
) acker {
    guard := &amp;amp;clientACKer{}
    guard.lift(newEventACK(pipeline, canDrop, sema, mk(guard)))
    return guard
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下返回值clientACKer结构体，其成员acker的赋值就是eventDataACK。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACK(
    pipeline *Pipeline,
    canDrop bool,
    sema *sema,
    fn func([]interface{}, int),
) *eventDataACK {
    a := &amp;amp;eventDataACK{pipeline: pipeline, fn: fn}
    a.acker = makeCountACK(pipeline, canDrop, sema, a.onACK)

    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个eventDataACK的结构体，fn就是mk(guard)就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func(data []interface{}, acked int) {
    b.cb(data, acked)
    if guard.Active() {
        fn(data)
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后调用makeCountACK来赋值给eventDataACK的acker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeCountACK(pipeline *Pipeline, canDrop bool, sema *sema, fn func(int, int)) acker {
    if canDrop {
        return newBoundGapCountACK(pipeline, sema, fn)
    }
    return newCountACK(pipeline, fn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;canDrop之前说过了，就是ture，所以创建newBoundGapCountACK，将eventDataACK的onACK当参数传递进来。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *eventDataACK) onACK(total, acked int) {
    n := total

    a.mutex.Lock()
    data := a.data[:n]
    a.data = a.data[n:]
    a.mutex.Unlock()

    if len(data) &amp;gt; 0 &amp;amp;&amp;amp; a.pipeline.ackActive.Load() {
        a.fn(data, acked)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;继续newBoundGapCountACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBoundGapCountACK(
    pipeline *Pipeline,
    sema *sema,
    fn func(total, acked int),
) *boundGapCountACK {
    a := &amp;amp;boundGapCountACK{active: true, sema: sema, fn: fn}
    a.acker.init(pipeline, a.onACK)
    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建了一个boundGapCountACK，调用初始化函数，将这个结构体的onACK传进去就是fn&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) init(pipeline *Pipeline, fn func(int, int)) {
    *a = gapCountACK{
        pipeline: pipeline,
        fn:       fn,
        done:     make(chan struct{}),
        drop:     make(chan struct{}),
        acks:     make(chan int, 1),
    }

    init := &amp;amp;gapInfo{}
    a.lst.head = init
    a.lst.tail = init

    go a.ackLoop()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) ackLoop() {
    // close channels, as no more events should be ACKed:
    // - once pipeline is closed
    // - all events of the closed client have been acked/processed by the pipeline

    acks, drop := a.acks, a.drop
    closing := false

    for {
        select {
        case &amp;lt;-a.done:
            closing = true
            a.done = nil
            if a.events.Load() == 0 {
                // stop worker, if all events accounted for have been ACKed.
                // If new events are added after this acker won&#39;t handle them, which may
                // result in duplicates
                return
            }

        case &amp;lt;-a.pipeline.ackDone:
            return

        case n := &amp;lt;-acks:
            empty := a.handleACK(n)
            if empty &amp;amp;&amp;amp; closing &amp;amp;&amp;amp; a.events.Load() == 0 {
                // stop worker, if and only if all events accounted for have been ACKed
                return
            }

        case &amp;lt;-drop:
            // TODO: accumulate multiple drop events + flush count with timer
            a.events.Sub(1)
            a.fn(1, 0)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边启动了一个acker的监听，然后使用这个clientACKer结构体又构建了一个新的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newWaitACK(acker acker, timeout time.Duration, afterClose func()) *waitACK {
    return &amp;amp;waitACK{
        acker:      acker,
        signalAll:  make(chan struct{}, 1),
        signalDone: make(chan struct{}),
        waitClose:  timeout,
        active:     atomic.MakeBool(true),
        afterClose: afterClose,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里连接创建就结束了，创建的acker就是waitACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;client.acker = acker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以pipeline的client的acker就是waitACK。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;publish数据的ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面是有数据的时候将数据发送到pipeline，调用的client的publish函数，在发送数据的时候调用了addEvent。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open := c.acker.addEvent(e, publish)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c.acker也就是上面waitACK的addEvent函数，e就是对应发送的事件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *waitACK) addEvent(event beat.Event, published bool) bool {
    if published {
        a.events.Inc()
    }
    return a.acker.addEvent(event, published)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的是结构体成员acker的addEvent，也就是eventDataACK的addEvent。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *eventDataACK) addEvent(event beat.Event, published bool) bool {
    a.mutex.Lock()
    active := a.pipeline.ackActive.Load()
    if active {
        a.data = append(a.data, event.Private)
    }
    a.mutex.Unlock()

    if active {
        return a.acker.addEvent(event, published)
    }
    return false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边就是将数据传输到了data中，同时调用了其对应的acker的addEvent函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACK(
    pipeline *Pipeline,
    canDrop bool,
    sema *sema,
    fn func([]interface{}, int),
) *eventDataACK {
    a := &amp;amp;eventDataACK{pipeline: pipeline, fn: fn}
    a.acker = makeCountACK(pipeline, canDrop, sema, a.onACK)

    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看到a.acker = makeCountACK(pipeline, canDrop, sema, a.onACK)，再看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeCountACK(pipeline *Pipeline, canDrop bool, sema *sema, fn func(int, int)) acker {
    if canDrop {
        return newBoundGapCountACK(pipeline, sema, fn)
    }
    return newCountACK(pipeline, fn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;canDrop上面说明过了，所以是newBoundGapCountACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBoundGapCountACK(
    pipeline *Pipeline,
    sema *sema,
    fn func(total, acked int),
) *boundGapCountACK {
    a := &amp;amp;boundGapCountACK{active: true, sema: sema, fn: fn}
    a.acker.init(pipeline, a.onACK)
    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以返回的结构体是boundGapCountACK，调用的也是这个结构体的addEvent&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *boundGapCountACK) addEvent(event beat.Event, published bool) bool {
    a.sema.inc()
    return a.acker.addEvent(event, published)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有acker.addEvent，再看boundGapCountACK的acker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type boundGapCountACK struct {
    active bool
    fn     func(total, acked int)

    acker gapCountACK
    sema  *sema
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;是一个gapCountACK的结构体，调用初始化a.acker.init(pipeline, a.onACK)来赋值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) init(pipeline *Pipeline, fn func(int, int)) {
    *a = gapCountACK{
        pipeline: pipeline,
        fn:       fn,
        done:     make(chan struct{}),
        drop:     make(chan struct{}),
        acks:     make(chan int, 1),
    }

    init := &amp;amp;gapInfo{}
    a.lst.head = init
    a.lst.tail = init

    go a.ackLoop()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时启动了一个监听程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) ackLoop() {
    // close channels, as no more events should be ACKed:
    // - once pipeline is closed
    // - all events of the closed client have been acked/processed by the pipeline

    acks, drop := a.acks, a.drop
    closing := false

    for {
        select {
        case &amp;lt;-a.done:
            closing = true
            a.done = nil
            if a.events.Load() == 0 {
                // stop worker, if all events accounted for have been ACKed.
                // If new events are added after this acker won&#39;t handle them, which may
                // result in duplicates
                return
            }

        case &amp;lt;-a.pipeline.ackDone:
            return

        case n := &amp;lt;-acks:
            empty := a.handleACK(n)
            if empty &amp;amp;&amp;amp; closing &amp;amp;&amp;amp; a.events.Load() == 0 {
                // stop worker, if and only if all events accounted for have been ACKed
                return
            }

        case &amp;lt;-drop:
            // TODO: accumulate multiple drop events + flush count with timer
            a.events.Sub(1)
            a.fn(1, 0)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当drop获取到信号的时候，就会调用fn也就是boundGapCountACK的onACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *boundGapCountACK) onACK(total, acked int) {
    a.sema.release(total)
    a.fn(total, acked)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边会调用到fn也就是eventDataACK的onACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *eventDataACK) onACK(total, acked int) {
    n := total

    a.mutex.Lock()
    data := a.data[:n]
    a.data = a.data[n:]
    a.mutex.Unlock()

    if len(data) &amp;gt; 0 &amp;amp;&amp;amp; a.pipeline.ackActive.Load() {
        a.fn(data, acked)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边会调用fn也就是我们创建的ackBuilder的cb成员也就是我们的ackBuilder结构的cb函数，也就是我们的onEvents&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) onEvents(data []interface{}, acked int) {
    p.pushMsg(eventsDataMsg{data: data, total: len(data), acked: acked})
}

func (p *pipelineEventCB) onCounts(total, acked int) {
    p.pushMsg(eventsDataMsg{total: total, acked: acked})
}

func (p *pipelineEventCB) pushMsg(msg eventsDataMsg) {
    if msg.acked == 0 {
        p.droppedEvents &amp;lt;- msg
    } else {
        msg.sig = make(chan struct{})
        p.events &amp;lt;- msg
        &amp;lt;-msg.sig
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取到了file的具体数据。到这边就是继续监听，我们先看addEvent，published肯定是true，正常都是有事件的publish = event != nil&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) addEvent(_ beat.Event, published bool) bool {
    // if gapList is empty and event is being dropped, forward drop event to ack
    // loop worker:

    a.events.Inc()
    if !published {
        a.addDropEvent()
    } else {
        a.addPublishedEvent()
    }

    return true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以看addPublishedEvent，只是给结构体成员send加一&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) addPublishedEvent() {
    // event is publisher -&amp;gt; add a new gap list entry if gap is present in current
    // gapInfo

    a.lst.Lock()

    current := a.lst.tail
    current.Lock()

    if current.dropped &amp;gt; 0 {
        tmp := &amp;amp;gapInfo{}
        a.lst.tail.next = tmp
        a.lst.tail = tmp

        current.Unlock()
        tmp.Lock()
        current = tmp
    }

    a.lst.Unlock()

    current.send++
    current.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边调用的addevent就结束了，下面就是等待output的publish后的返回调用。上面已经有四个相关ack的监听，一个queue消费的监听，一个registry监听，一个是pipeline的监听，一个gapCountACK的ackLoop。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;publish后回调ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后就是output的publish的时候进行回调了，我们使用的是kafka，kafka在connect的时候会新建两个协程，来监听发送的情况，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) Connect() error {
    c.mux.Lock()
    defer c.mux.Unlock()

    debugf(&amp;quot;connect: %v&amp;quot;, c.hosts)

    // try to connect
    producer, err := sarama.NewAsyncProducer(c.hosts, &amp;amp;c.config)
    if err != nil {
        logp.Err(&amp;quot;Kafka connect fails with: %v&amp;quot;, err)
        return err
    }

    c.producer = producer

    c.wg.Add(2)
    go c.successWorker(producer.Successes())
    go c.errorWorker(producer.Errors())

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们一共可以看到两个处理方式，一个成功一个失败，producer.Successes()和producer.Errors()为这个producer的成功和错误返回channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) successWorker(ch &amp;lt;-chan *sarama.ProducerMessage) {
    defer c.wg.Done()
    defer debugf(&amp;quot;Stop kafka ack worker&amp;quot;)

    for libMsg := range ch {
        msg := libMsg.Metadata.(*message)
        msg.ref.done()
    }
}

func (c *client) errorWorker(ch &amp;lt;-chan *sarama.ProducerError) {
    defer c.wg.Done()
    defer debugf(&amp;quot;Stop kafka error handler&amp;quot;)

    for errMsg := range ch {
        msg := errMsg.Msg.Metadata.(*message)
        msg.ref.fail(msg, errMsg.Err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下成功的响应，失败也是一样的，只不过多了一个错误处理，有兴趣可以自己看一下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *msgRef) done() {
    r.dec()
}

func (r *msgRef) dec() {
    i := atomic.AddInt32(&amp;amp;r.count, -1)
    if i &amp;gt; 0 {
        return
    }

    debugf(&amp;quot;finished kafka batch&amp;quot;)
    stats := r.client.observer

    err := r.err
    if err != nil {
        failed := len(r.failed)
        success := r.total - failed
        r.batch.RetryEvents(r.failed)

        stats.Failed(failed)
        if success &amp;gt; 0 {
            stats.Acked(success)
        }

        debugf(&amp;quot;Kafka publish failed with: %v&amp;quot;, err)
    } else {
        r.batch.ACK()
        stats.Acked(r.total)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在else中也就是接受到成功发送信号后调用了batch.ACK()。我们来看一下batch，首先是msg的类型转化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;msg := errMsg.Msg.Metadata.(*message)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;转化为我们定义的kafka的message的结构体message&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type message struct {
    msg sarama.ProducerMessage

    topic string
    key   []byte
    value []byte
    ref   *msgRef
    ts    time.Time

    hash      uint32
    partition int32

    data publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在kafka的client使用publish的时候初始化了ref，给batch赋值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ref := &amp;amp;msgRef{
        client: c,
        count:  int32(len(events)),
        total:  len(events),
        failed: nil,
        batch:  batch,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个batch就是重netClientWorker的qu workQueue中获取的，看一下这个channel是bantch类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type workQueue chan *Batch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个batch就是我们需要找的结构，发送kafka成功后就是调用这个结构他的ACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Batch struct {
    original queue.Batch
    ctx      *batchContext
    ttl      int
    events   []publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下Batch的ACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Batch) ACK() {
    b.ctx.observer.outBatchACKed(len(b.events))
    b.original.ACK()
    releaseBatch(b)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边继续调用ACK，我们需要看一下b.original赋值，赋值都会调用newBatch函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBatch(ctx *batchContext, original queue.Batch, ttl int) *Batch {
    if original == nil {
        panic(&amp;quot;empty batch&amp;quot;)
    }

    b := batchPool.Get().(*Batch)
    *b = Batch{
        original: original,
        ctx:      ctx,
        ttl:      ttl,
        events:   original.Events(),
    }
    return b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只在eventConsumer消费的时候调用了newBatch，通过get方法获取的queueBatch给了他&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;queueBatch, err := consumer.Get(c.out.batchSize)
if err != nil {
    out = nil
    consumer = nil
    continue
}
if queueBatch != nil {
    batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先consumer是基于mem的，看一下get方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *consumer) Get(sz int) (queue.Batch, error) {
    // log := c.broker.logger

    if c.closed.Load() {
        return nil, io.EOF
    }

    select {
    case c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:
    case &amp;lt;-c.done:
        return nil, io.EOF
    }

    // if request has been send, we do have to wait for a response
    resp := &amp;lt;-c.resp
    return &amp;amp;batch{
        consumer: c,
        events:   resp.buf,
        ack:      resp.ack,
        state:    batchActive,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到返回使用的是结构体batch如下，我们简单看一下需要先向队列请求channel发送getRequest结构体，等待resp的返回来创建下面的结构体。具体的处理逻辑可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/#pipeline-的消费过程&#34;&gt;pipeline的消费&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type batch struct {
    consumer     *consumer
    events       []publisher.Event
    clientStates []clientState
    ack          *ackChan
    state        ackState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下batch的ACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *batch) ACK() {
    if b.state != batchActive {
        switch b.state {
        case batchACK:
            panic(&amp;quot;Can not acknowledge already acknowledged batch&amp;quot;)
        default:
            panic(&amp;quot;inactive batch&amp;quot;)
        }
    }

    b.report()
}

func (b *batch) report() {
    b.ack.ch &amp;lt;- batchAckMsg{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是给batch的ack也就是getResponse的ack的ch发送了一个信号batchAckMsg{}。这个ch接收到信号，牵涉到一个完整的消费的调度过程。&lt;/p&gt;

&lt;p&gt;我们先看一下正常的消费调度，在上面说过，首先在有数据发送到queue的时候，consumer会获取这个数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; consumer != nil &amp;amp;&amp;amp; batch == nil {
            out = c.out.workQueue
            queueBatch, err := consumer.Get(c.out.batchSize)
            ...
            batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
        }
        ...
        select {
        case &amp;lt;-c.done:
            return
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
        case out &amp;lt;- batch:
            batch = nil
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面consumer.Get就是消费者consumer从Broker中获取日志数据，然后发送至out的channel中被output client发送，我们看一下Get方法里的核心代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *consumer) Get(sz int) (queue.Batch, error) {
    // log := c.broker.logger

    if c.closed.Load() {
        return nil, io.EOF
    }

    select {
    case c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:
    case &amp;lt;-c.done:
        return nil, io.EOF
    }

    // if request has been send, we do have to wait for a response
    resp := &amp;lt;-c.resp
    return &amp;amp;batch{
        consumer: c,
        events:   resp.buf,
        ack:      resp.ack,
        state:    batchActive,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}时候,我们需要看一下bufferingEventLoop的调度中心的响应。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) run() {
    var (
        broker = l.broker
    )

    for {
        select {
        case &amp;lt;-broker.done:
            return

        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)

        case req := &amp;lt;-l.pubCancel: // producer cancelling active events
            l.handleCancel(&amp;amp;req)

        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)

        case l.schedACKS &amp;lt;- l.pendingACKs:
            l.schedACKS = nil
            l.pendingACKs = chanList{}

        case count := &amp;lt;-l.acks:
            l.handleACK(count)

        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;l.get会得到信息，为什么l.get会得到信息，因为l.get = l.broker.requests&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) flushBuffer() {
    l.buf.flushed = true

    if l.buf.length() == 0 {
        panic(&amp;quot;flushing empty buffer&amp;quot;)
    }

    l.flushList.add(l.buf)
    l.get = l.broker.requests
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;l.get得到信息后handleConsumer如何处理信息的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) handleConsumer(req *getRequest) {
    buf := l.flushList.head
    if buf == nil {
        panic(&amp;quot;get from non-flushed buffers&amp;quot;)
    }

    count := buf.length()
    if count == 0 {
        panic(&amp;quot;empty buffer in flush list&amp;quot;)
    }

    if sz := req.sz; sz &amp;gt; 0 {
        if sz &amp;lt; count {
            count = sz
        }
    }

    if count == 0 {
        panic(&amp;quot;empty batch returned&amp;quot;)
    }

    events := buf.events[:count]
    clients := buf.clients[:count]
    ackChan := newACKChan(l.ackSeq, 0, count, clients)
    l.ackSeq++

    req.resp &amp;lt;- getResponse{ackChan, events}
    l.pendingACKs.append(ackChan)
    l.schedACKS = l.broker.scheduledACKs

    buf.events = buf.events[count:]
    buf.clients = buf.clients[count:]
    if buf.length() == 0 {
        l.advanceFlushList()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到获取到数据封装getResponse发送给output，我们这边不看这个数据具体发送到workqueue，而是关心的是ack。&lt;/p&gt;

&lt;p&gt;先看ack构建的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newACKChan(seq uint, start, count int, states []clientState) *ackChan {
    ch := ackChanPool.Get().(*ackChan)
    ch.next = nil
    ch.seq = seq
    ch.start = start
    ch.count = count
    ch.states = states
    return ch
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将这个ackChan新增到chanlist的链表中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;l.pendingACKs.append(ackChan)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将l.schedACKS = l.broker.scheduledACKs，一开始调度的时候l.schedACKS是阻塞的，l.pendingACKs不能写入到l.schedACKS，但是这边进行赋值后就是将l.pendingACKs写入到l.broker.scheduledACKs，这个在初始化的时候是有缓存的。就直接写入了，然后bufferingEventLoop调度中心将这个数据清空，bufferingEventLoop的ack的调度获取到这个chanenl&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *ackLoop) run() {
    var (
        // log = l.broker.logger

        // Buffer up acked event counter in acked. If acked &amp;gt; 0, acks will be set to
        // the broker.acks channel for sending the ACKs while potentially receiving
        // new batches from the broker event loop.
        // This concurrent bidirectionally communication pattern requiring &#39;select&#39;
        // ensures we can not have any deadlock between the event loop and the ack
        // loop, as the ack loop will not block on any channel
        acked int
        acks  chan int
    )

    for {
        select {
        case &amp;lt;-l.broker.done:
            // TODO: handle pending ACKs?
            // TODO: panic on pending batches?
            return

        case acks &amp;lt;- acked:
            acks, acked = nil, 0

        case lst := &amp;lt;-l.broker.scheduledACKs:
            count, events := lst.count()
            l.lst.concat(&amp;amp;lst)

            // log.Debug(&amp;quot;ACK List:&amp;quot;)
            // for current := l.lst.head; current != nil; current = current.next {
            //  log.Debugf(&amp;quot;  ack entry(seq=%v, start=%v, count=%v&amp;quot;,
            //      current.seq, current.start, current.count)
            // }

            l.batchesSched += uint64(count)
            l.totalSched += uint64(events)

        case &amp;lt;-l.sig:
            acked += l.handleBatchSig()
            if acked &amp;gt; 0 {
                acks = l.broker.acks
            }
        }

        // log.Debug(&amp;quot;ackloop INFO&amp;quot;)
        // log.Debug(&amp;quot;ackloop:   total events scheduled = &amp;quot;, l.totalSched)
        // log.Debug(&amp;quot;ackloop:   total events ack = &amp;quot;, l.totalACK)
        // log.Debug(&amp;quot;ackloop:   total batches scheduled = &amp;quot;, l.batchesSched)
        // log.Debug(&amp;quot;ackloop:   total batches ack = &amp;quot;, l.batchesACKed)

        l.sig = l.lst.channel()
        // if l.sig == nil {
        //  log.Debug(&amp;quot;ackloop: no ack scheduled&amp;quot;)
        // } else {
        //  log.Debug(&amp;quot;ackloop: schedule ack: &amp;quot;, l.lst.head.seq)
        // }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;lst := &amp;lt;-l.broker.scheduledACKs就是获取到那个请求是新建的ackchan的channel，也就是获取到了batch回调的ack的函数的信号。也就是&amp;lt;-l.sig获取到了信号，调用handleBatchSig&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// handleBatchSig collects and handles a batch ACK/Cancel signal. handleBatchSig
// is run by the ackLoop.
func (l *ackLoop) handleBatchSig() int {
    lst := l.collectAcked()

    count := 0
    for current := lst.front(); current != nil; current = current.next {
        count += current.count
    }

    if count &amp;gt; 0 {
        if e := l.broker.eventer; e != nil {
            e.OnACK(count)
        }

        // report acks to waiting clients
        l.processACK(lst, count)
    }

    for !lst.empty() {
        releaseACKChan(lst.pop())
    }

    // return final ACK to EventLoop, in order to clean up internal buffer
    l.broker.logger.Debug(&amp;quot;ackloop: return ack to broker loop:&amp;quot;, count)

    l.totalACK += uint64(count)
    l.broker.logger.Debug(&amp;quot;ackloop:  done send ack&amp;quot;)
    return count
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;l.broker.eventer我们可以追溯一下，broker就是创建queue的是newACKLoop的时候传递的，broker也是在这个时候初始化的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b := &amp;amp;Broker{
        done:   make(chan struct{}),
        logger: logger,

        // broker API channels
        events:    make(chan pushRequest, chanSize),
        requests:  make(chan getRequest),
        pubCancel: make(chan producerCancelRequest, 5),

        // internal broker and ACK handler channels
        acks:          make(chan int),
        scheduledACKs: make(chan chanList),

        waitOnClose: settings.WaitOnClose,

        eventer: settings.Eventer,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;eventer是create的时候传递的，create回传的时候是create的方法，真正调用是在pipeline初始化的时候new新建pipeline的时候&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.queue, err = queueFactory(&amp;amp;p.eventer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以说l.broker.eventer就是p.eventer也就是结构体pipelineEventer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type pipelineEventer struct {
    mutex      sync.Mutex
    modifyable bool

    observer  queueObserver
    waitClose *waitCloser
    cb        *pipelineEventCB
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在pipeline中设置registry的时候p.eventer.cb就是我们创建的pipelineEventCB：p.eventer.cb = cb&lt;/p&gt;

&lt;p&gt;到这边可以看出e.OnACK(count)就是调用pipelineEventer的成员函数OnACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (e *pipelineEventer) OnACK(n int) {
    e.observer.queueACKed(n)

    if wc := e.waitClose; wc != nil {
        wc.dec(n)
    }
    if e.cb != nil {
        e.cb.reportQueueACK(n)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候就调用我们最初用的pipelineEventCB的reportQueueACK函数，就是将acked发送到了p.acks &amp;lt;- acked中，这个时候pipelineEventCB的监听程序监听到acks信号。&lt;/p&gt;

&lt;p&gt;收到ack的channel信息，调用collect函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) collect(count int) (exit bool) {
    var (
        signalers []chan struct{}
        data      []interface{}
        acked     int
        total     int
    )

    for acked &amp;lt; count {
        var msg eventsDataMsg
        select {
        case msg = &amp;lt;-p.events:
        case msg = &amp;lt;-p.droppedEvents:
        case &amp;lt;-p.done:
            exit = true
            return
        }

        if msg.sig != nil {
            signalers = append(signalers, msg.sig)
        }
        total += msg.total
        acked += msg.acked

        if count-acked &amp;lt; 0 {
            panic(&amp;quot;ack count mismatch&amp;quot;)
        }

        switch p.mode {
        case eventsACKMode:
            data = append(data, msg.data...)

        case lastEventsACKMode:
            if L := len(msg.data); L &amp;gt; 0 {
                data = append(data, msg.data[L-1])
            }
        }
    }

    // signal clients we processed all active ACKs, as reported by queue
    for _, sig := range signalers {
        close(sig)
    }
    p.reportEventsData(data, total)
    return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重pipelineEventCB中的events和droppedEvents中读取数据信息，然后进行上报reportEventsData&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) reportEventsData(data []interface{}, total int) {
    // report ACK back to the beat
    switch p.mode {
    case countACKMode:
        p.handler.ACKCount(total)
    case eventsACKMode:
        p.handler.ACKEvents(data)
    case lastEventsACKMode:
        p.handler.ACKLastEvents(data)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边就调用到一开始的ACKEvents函数，对数据进行处理，其实这些数据就是[]file.State文件信息。在创建pipelineEventCB的时候，也就是在pipeline使用set函数的时候，我们这边传递的是ACKEvents，所以调用的是newEventACKer(finishedLogger, registrarChannel)这个结构体的ackEvents函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACKer(stateless statelessLogger, stateful statefulLogger) *eventACKer {
    return &amp;amp;eventACKer{stateless: stateless, stateful: stateful, log: logp.NewLogger(&amp;quot;acker&amp;quot;)}
}

func (a *eventACKer) ackEvents(data []interface{}) {
    stateless := 0
    states := make([]file.State, 0, len(data))
    for _, datum := range data {
        if datum == nil {
            stateless++
            continue
        }

        st, ok := datum.(file.State)
        if !ok {
            stateless++
            continue
        }

        states = append(states, st)
    }

    if len(states) &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateful ack&amp;quot;, &amp;quot;count&amp;quot;, len(states))
        a.stateful.Published(states)
    }

    if stateless &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateless ack&amp;quot;, &amp;quot;count&amp;quot;, stateless)
        a.stateless.Published(stateless)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的这个registrarLogger的Published来完成文件状态的推送&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *registrarLogger) Published(states []file.State) {
    select {
    case &amp;lt;-l.done:
        // set ch to nil, so no more events will be send after channel close signal
        // has been processed the first time.
        // Note: nil channels will block, so only done channel will be actively
        //       report &#39;closed&#39;.
        l.ch = nil
    case l.ch &amp;lt;- states:
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;文件持久化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;registrarLogger的channel获取的信息是如何处理的？其实是Registrar的channel接受到了信息，在一开始Registrar就启动了监听channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    select {
    case &amp;lt;-r.done:
        logp.Info(&amp;quot;Ending Registrar&amp;quot;)
        return
    case &amp;lt;-flushC:
        flushC = nil
        timer.Stop()
        r.flushRegistry()
    case states := &amp;lt;-r.Channel:
        r.onEvents(states)
        if r.flushTimeout &amp;lt;= 0 {
            r.flushRegistry()
        } else if flushC == nil {
            timer = time.NewTimer(r.flushTimeout)
            flushC = timer.C
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接收到文件状态后调用onEvents&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// onEvents processes events received from the publisher pipeline
func (r *Registrar) onEvents(states []file.State) {
    r.processEventStates(states)
    r.bufferedStateUpdates += len(states)

    // check if we need to enable state cleanup
    if !r.gcEnabled {
        for i := range states {
            if states[i].TTL &amp;gt;= 0 || states[i].Finished {
                r.gcEnabled = true
                break
            }
        }
    }

    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Registrar state updates processed. Count: %v&amp;quot;, len(states))

    // new set of events received -&amp;gt; mark state registry ready for next
    // cleanup phase in case gc&#39;able events are stored in the registry.
    r.gcRequired = r.gcEnabled
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过processEventStates来处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// processEventStates gets the states from the events and writes them to the registrar state
func (r *Registrar) processEventStates(states []file.State) {
    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Processing %d events&amp;quot;, len(states))

    ts := time.Now()
    for i := range states {
        r.states.UpdateWithTs(states[i], ts)
        statesUpdate.Add(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是states的更新UpdateWithTs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// UpdateWithTs updates a state, assigning the given timestamp.
// If previous state didn&#39;t exist, new one is created
func (s *States) UpdateWithTs(newState State, ts time.Time) {
    s.Lock()
    defer s.Unlock()

    id := newState.ID()
    index := s.findPrevious(id)
    newState.Timestamp = ts

    if index &amp;gt;= 0 {
        s.states[index] = newState
    } else {
        // No existing state found, add new one
        s.idx[id] = len(s.states)
        s.states = append(s.states, newState)
        logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;New state added for %s&amp;quot;, newState.Source)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边states的状态就发生变化，再来看看states的初始化操作，其实就是在Registrar的New的时候调用了NewStates进行了初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r := &amp;amp;Registrar{
    registryFile: dataFile,
    fileMode:     cfg.Permissions,
    done:         make(chan struct{}),
    states:       file.NewStates(),
    Channel:      make(chan []file.State, 1),
    flushTimeout: cfg.FlushTimeout,
    out:          out,
    wg:           sync.WaitGroup{},
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Registrar的for循环的时候，定时会对状态进行写文件操作，调用flushRegistry的writeRegistry来完成文件的持久化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Registrar) flushRegistry() {
    if err := r.writeRegistry(); err != nil {
        logp.Err(&amp;quot;Writing of registry returned error: %v. Continuing...&amp;quot;, err)
    }

    if r.out != nil {
        r.out.Published(r.bufferedStateUpdates)
    }
    r.bufferedStateUpdates = 0
}

// writeRegistry writes the new json registry file to disk.
func (r *Registrar) writeRegistry() error {
    // First clean up states
    r.gcStates()
    states := r.states.GetStates()
    statesCurrent.Set(int64(len(states)))

    registryWrites.Inc()

    tempfile, err := writeTmpFile(r.registryFile, r.fileMode, states)
    if err != nil {
        registryFails.Inc()
        return err
    }

    err = helper.SafeFileRotate(r.registryFile, tempfile)
    if err != nil {
        registryFails.Inc()
        return err
    }

    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Registry file updated. %d states written.&amp;quot;, len(states))
    registrySuccess.Inc()

    return nil
}

func writeTmpFile(baseName string, perm os.FileMode, states []file.State) (string, error) {
    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Write registry file: %s (%v)&amp;quot;, baseName, len(states))

    tempfile := baseName + &amp;quot;.new&amp;quot;
    f, err := os.OpenFile(tempfile, os.O_RDWR|os.O_CREATE|os.O_TRUNC|os.O_SYNC, perm)
    if err != nil {
        logp.Err(&amp;quot;Failed to create tempfile (%s) for writing: %s&amp;quot;, tempfile, err)
        return &amp;quot;&amp;quot;, err
    }

    defer f.Close()

    encoder := json.NewEncoder(f)

    if err := encoder.Encode(states); err != nil {
        logp.Err(&amp;quot;Error when encoding the states: %s&amp;quot;, err)
        return &amp;quot;&amp;quot;, err
    }

    // Commit the changes to storage to avoid corrupt registry files
    if err = f.Sync(); err != nil {
        logp.Err(&amp;quot;Error when syncing new registry file contents: %s&amp;quot;, err)
        return &amp;quot;&amp;quot;, err
    }

    return tempfile, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对以上的过程最一个简单的总结&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/registry.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;特殊情况&#34;&gt;特殊情况&lt;/h3&gt;

&lt;p&gt;1.如果filebeat异常重启，每次采集harvester启动的时候都会读取registry文件，从上次记录的状态继续采集，确保不会从头开始重复发送所有的日志文件。
当然，如果日志发送过程中，还没来得及返回ack，filebeat就挂掉，registry文件肯定不会更新至最新的状态，那么下次采集的时候，这部分的日志就会重复发送，所以这意味着filebeat只能保证at least once，无法保证不重复发送。
还有一个比较异常的情况是，linux下如果老文件被移除，新文件马上创建，很有可能它们有相同的inode，而由于filebeat根据inode来标志文件记录采集的偏移，会导致registry里记录的其实是被移除的文件State状态，这样新的文件采集却从老的文件Offset开始，从而会遗漏日志数据。
为了尽量避免inode被复用的情况，同时防止registry文件随着时间增长越来越大，建议使用clean_inactive和clean_remove配置将长时间未更新或者被删除的文件State从registry中移除。&lt;/p&gt;

&lt;p&gt;2.在harvester读取日志中，会更新registry的状态处理一些异常场景。例如，如果一个日志文件被清空，filebeat会在下一次Reader.Next方法中返回ErrFileTruncate异常，将inode标志文件的Offset置为0，结束这次harvester，重新启动新的harvester，虽然文件不变，但是registry中的Offset为0，采集会从头开始。&lt;/p&gt;

&lt;p&gt;3.如果使用容器部署filebeat，需要将registry文件挂载到宿主机上，否则容器重启后registry文件丢失，会使filebeat从头开始重复采集日志文件。&lt;/p&gt;

&lt;h4 id=&#34;日志重复&#34;&gt;日志重复&lt;/h4&gt;

&lt;p&gt;Filebeat对于收集到的数据（即event）的传输保证的是&amp;rdquo;at least once&amp;rdquo;，而不是&amp;rdquo;exactly once&amp;rdquo;，也就是Filebeat传输的数据是有可能有重复的。这里我们讨论一下可能产生重复数据的一些场景，我大概将其分为两类。&lt;/p&gt;

&lt;p&gt;第一类：Filebeat重传导致数据重复。重传是因为Filebeat要保证数据至少发送一次，进而避免数据丢失。具体来说就是每条event发送到output后都要等待ack，只有收到ack了才会认为数据发送成功，然后将状态记录到registry。当然实际操作的时候为了高效是批量发送，批量确认的。而造成重传的场景（也就是没有收到ack）非常多，而且很多都不可避免，比如后端不可达、网络传输失败、程序突然挂掉等等。&lt;/p&gt;

&lt;p&gt;第二类：配置不当或操作不当导致文件重复收集。Filebeat感知文件有没有被收集过靠的是registry文件里面记录的状态，如果一个文件已经被收集过了，但因为各种原因它的状态从registry文件中被移除了，而恰巧这个文件还在收集范围内，那就会再收集一次。&lt;/p&gt;

&lt;p&gt;对于第一类产生的数据重复一般不可避免，而第二类可以避免，但总的来说，Filebeat提供的是at least once的机制，所以我们在使用时要明白数据是可能重复的。如果业务上不能接受数据重复，那就要在Filebeat之后的流程中去重。&lt;/p&gt;

&lt;h4 id=&#34;数据丢失&#34;&gt;数据丢失&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;inode重用的问题&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果一个文件达到了限制（比如大小），不是重新创建一个新的文件写，而是将这个文件truncate掉继续复用（当然实际中这种场景好像比较少，但也并非没有），Filebeat下次来检查这个文件是否有变动的时候，这个文件的大小如果大于之前记录的offset，也会发生上面的情况。这个问题在github上面是有issue的，但目前还没有解决，官方回复是Filebeat的整个机制在重构中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;还有一些其它情况，比如文件数太多，Filebeat的处理能力有限，在还没来得及处理的时候这些文件就被删掉了（比如rotate给老化掉了）也会造成数据丢失。还有就是后端不可用，所以Filebeat还在重试，但源文件被删了，那数据也就丢了。因为Filebeat的重试并非一直发送已经收集到内存里面的event，必要的时候会重新从源文件读，比如程序重启。这些情况的话，只要不限制Filebeat的收集能力，同时保证后端的可用性，网络的可用性，一般问题不大。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;总结-1&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;其实重数据发送到内存队列中这一套完整的功能就是由libbeat完成的，正常流程如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/output.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;日志采集状态监控&#34;&gt;日志采集状态监控&lt;/h2&gt;

&lt;p&gt;我们之前讲到 Registrar 会记录每个文件的状态，当 Filebeat 启动时，会从 Registrar 恢复文件处理状态。&lt;/p&gt;

&lt;p&gt;其实在 filebeat 运行过程中，Input 组件也记录了文件状态。不一样的是，Registrar 是持久化存储，而 Input 中的文件状态仅表示当前文件的读取偏移量，且修改时不会同步到磁盘中。&lt;/p&gt;

&lt;p&gt;每次，Filebeat 刚启动时，Input 都会载入 Registrar 中记录的文件状态，作为初始状态。Input 中的状态有两个非常重要：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;offset: 代表文件当前读取的 offset，从 Registrar 中初始化。Harvest 读取文件后，会同时修改 offset。
finished: 代表该文件对应的 Harvester 是否已经结束，Harvester 开始时置为 false，结束时置为 true。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于每次定时扫描到的文件，概括来说，会有三种大的情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input 找不到该文件状态的记录, 说明是新增文件，则开启一个 Harvester，从头开始解析该文件
如果可以找到文件状态，且 finished 等于 false。这个说明已经有了一个 Harvester 在处理了，这种情况直接忽略就好了。
如果可以找到文件状态，且 finished 等于 true。说明之前有 Harvester 处理过，但已经处理结束了。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于这种第三种情况，我们需要考虑到一些异常情况，Filebeat 是这么处理的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;如果 offset 大于当前文件大小：说明文件被 Truncate 过，此时按做一个新文件处理，直接从头开始解析该文件
如果 offset 小于当前文件大小，说明文件内容有新增，则从上次 offset 处继续读即可。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于第二种情况，Filebeat 似乎有一个逻辑上的问题: 如果文件被 Truncate 过，后来又新增了数据，且文件大小也比之前 offset 大，那么 Filebeat 是检查不出来这个问题的。&lt;/p&gt;

&lt;h2 id=&#34;句柄保持&#34;&gt;句柄保持&lt;/h2&gt;

&lt;p&gt;Filebeat 甚至可以处理文件名修改的问题。即使一个日志的文件名被修改过，Filebeat 重启后，也能找到该文件，从上次读过的地方继续读。&lt;/p&gt;

&lt;p&gt;这是因为 Filebeat 除了在 Registrar 存储了文件名，还存储了文件的唯一标识。对于 Linux 来说，这个文件的唯一标识就是该文件的 inode ID + device ID。&lt;/p&gt;

&lt;h2 id=&#34;重载&#34;&gt;重载&lt;/h2&gt;

&lt;p&gt;重载是在Crawler中启动的，首先是新建的InputsFactory的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c.InputsFactory = input.NewRunnerFactory(c.out, r, c.beatDone)

// RunnerFactory is a factory for registrars
type RunnerFactory struct {
    outlet    channel.Factory
    registrar *registrar.Registrar
    beatDone  chan struct{}
}

// NewRunnerFactory instantiates a new RunnerFactory
func NewRunnerFactory(outlet channel.Factory, registrar *registrar.Registrar, beatDone chan struct{}) *RunnerFactory {
    return &amp;amp;RunnerFactory{
        outlet:    outlet,
        registrar: registrar,
        beatDone:  beatDone,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后如果配置了重载，就会新建Reloader结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Reloader is used to register and reload modules
type Reloader struct {
    pipeline beat.Pipeline
    config   DynamicConfig
    path     string
    done     chan struct{}
    wg       sync.WaitGroup
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新建的过程如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if configInputs.Enabled() {
    c.inputReloader = cfgfile.NewReloader(pipeline, configInputs)
    if err := c.inputReloader.Check(c.InputsFactory); err != nil {
        return err
    }

    go func() {
        c.inputReloader.Run(c.InputsFactory)
    }()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动reloader的run方法并且将RunnerFactory作为参数传递进去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Run runs the reloader
func (rl *Reloader) Run(runnerFactory RunnerFactory) {
    logp.Info(&amp;quot;Config reloader started&amp;quot;)

    list := NewRunnerList(&amp;quot;reload&amp;quot;, runnerFactory, rl.pipeline)

    rl.wg.Add(1)
    defer rl.wg.Done()

    // Stop all running modules when method finishes
    defer list.Stop()

    gw := NewGlobWatcher(rl.path)

    // If reloading is disable, config files should be loaded immediately
    if !rl.config.Reload.Enabled {
        rl.config.Reload.Period = 0
    }

    overwriteUpdate := true

    for {
        select {
        case &amp;lt;-rl.done:
            logp.Info(&amp;quot;Dynamic config reloader stopped&amp;quot;)
            return

        case &amp;lt;-time.After(rl.config.Reload.Period):
            debugf(&amp;quot;Scan for new config files&amp;quot;)
            configReloads.Add(1)

            //扫描所有的配置文件
            files, updated, err := gw.Scan()
            if err != nil {
                // In most cases of error, updated == false, so will continue
                // to next iteration below
                logp.Err(&amp;quot;Error fetching new config files: %v&amp;quot;, err)
            }

            // no file changes
            if !updated &amp;amp;&amp;amp; !overwriteUpdate {
                overwriteUpdate = false
                continue
            }

            // Load all config objects 加载所有配置文件
            configs, _ := rl.loadConfigs(files)

            debugf(&amp;quot;Number of module configs found: %v&amp;quot;, len(configs))

            //启动加载程序
            if err := list.Reload(configs); err != nil {
                // Make sure the next run also updates because some runners were not properly loaded
                overwriteUpdate = true
            }
        }

        // Path loading is enabled but not reloading. Loads files only once and then stops.
        if !rl.config.Reload.Enabled {
            logp.Info(&amp;quot;Loading of config files completed.&amp;quot;)
            select {
            case &amp;lt;-rl.done:
                logp.Info(&amp;quot;Dynamic config reloader stopped&amp;quot;)
                return
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见有一个定时的循环程序来获取所有的配置文件，交给RunnerList的reload的来加载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Reload the list of runners to match the given state
func (r *RunnerList) Reload(configs []*reload.ConfigWithMeta) error {
    r.mutex.Lock()
    defer r.mutex.Unlock()

    var errs multierror.Errors

    startList := map[uint64]*reload.ConfigWithMeta{}
    //获取正在运行的runner到stopList
    stopList := r.copyRunnerList()

    r.logger.Debugf(&amp;quot;Starting reload procedure, current runners: %d&amp;quot;, len(stopList))

    // diff current &amp;amp; desired state, create action lists
    for _, config := range configs {
        hash, err := HashConfig(config.Config)
        if err != nil {
            r.logger.Errorf(&amp;quot;Unable to hash given config: %s&amp;quot;, err)
            errs = append(errs, errors.Wrap(err, &amp;quot;Unable to hash given config&amp;quot;))
            continue
        }

        //如果配置文件还在，就重stopList中删除，继续采集，剩下的在stopList中的下面会停止采集
        if _, ok := stopList[hash]; ok {
            delete(stopList, hash)
        } else {
            //如果不在stopList中，说明是新的文件，如果不是重复的就加入到startList中，下来开始采集
            if _,ok := r.runners[hash]; !ok{
                startList[hash] = config
            }
        }
    }

    r.logger.Debugf(&amp;quot;Start list: %d, Stop list: %d&amp;quot;, len(startList), len(stopList))

    // Stop removed runners
    for hash, runner := range stopList {
        r.logger.Debugf(&amp;quot;Stopping runner: %s&amp;quot;, runner)
        delete(r.runners, hash)
        go runner.Stop()
    }

    // Start new runners
    for hash, config := range startList {
        // Pass a copy of the config to the factory, this way if the factory modifies it,
        // that doesn&#39;t affect the hash of the original one.
        c, _ := common.NewConfigFrom(config.Config)
        runner, err := r.factory.Create(r.pipeline, c, config.Meta)
        if err != nil {
            r.logger.Errorf(&amp;quot;Error creating runner from config: %s&amp;quot;, err)
            errs = append(errs, errors.Wrap(err, &amp;quot;Error creating runner from config&amp;quot;))
            continue
        }

        r.logger.Debugf(&amp;quot;Starting runner: %s&amp;quot;, runner)
        r.runners[hash] = runner
        runner.Start()
    }

    return errs.Err()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边的create就是上面传进来的InputsFactory的结构体的成员函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Create creates a input based on a config
func (r *RunnerFactory) Create(
    pipeline beat.Pipeline,
    c *common.Config,
    meta *common.MapStrPointer,
) (cfgfile.Runner, error) {
    connector := r.outlet(pipeline)
    p, err := New(c, connector, r.beatDone, r.registrar.GetStates(), meta)
    if err != nil {
        // In case of error with loading state, input is still returned
        return p, err
    }

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看看new函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New instantiates a new Runner
func New(
    conf *common.Config,
    connector channel.Connector,
    beatDone chan struct{},
    states []file.State,
    dynFields *common.MapStrPointer,
) (*Runner, error) {
    input := &amp;amp;Runner{
        config:   defaultConfig,
        wg:       &amp;amp;sync.WaitGroup{},
        done:     make(chan struct{}),
        Once:     false,
        beatDone: beatDone,
    }

    var err error
    if err = conf.Unpack(&amp;amp;input.config); err != nil {
        return nil, err
    }

    var h map[string]interface{}
    conf.Unpack(&amp;amp;h)
    input.ID, err = hashstructure.Hash(h, nil)
    if err != nil {
        return nil, err
    }

    var f Factory
    f, err = GetFactory(input.config.Type)
    if err != nil {
        return input, err
    }

    context := Context{
        States:        states,
        Done:          input.done,
        BeatDone:      input.beatDone,
        DynamicFields: dynFields,
        Meta:          nil,
    }
    var ipt Input
    ipt, err = f(conf, connector, context)
    if err != nil {
        return input, err
    }
    input.input = ipt

    return input, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看见就是新建流程中的新建runner，下面调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;runner.Start()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是正常的采集流程，到这边重载也就结束了。&lt;/p&gt;

&lt;h1 id=&#34;基本使用与特性&#34;&gt;基本使用与特性&lt;/h1&gt;

&lt;h2 id=&#34;filebeat自动reload更新&#34;&gt;filebeat自动reload更新&lt;/h2&gt;

&lt;p&gt;目前filebeat支持reload input配置，module配置，但reload的机制只有定时更新。&lt;/p&gt;

&lt;p&gt;在配置中打开reload.enable之后，还可以配置reload.period表示自动reload配置的时间间隔。&lt;/p&gt;

&lt;p&gt;filebeat在启动时，会创建一个专门用于reload的协程。对于每个正在运行的harvester，filebeat会将其加入一个全局的Runner列表，每次到了定时的间隔后，会触发一次配置文件的diff判断，如果是需要停止的加入stopRunner列表，然后逐个关闭，新的则加入startRunner列表，启动新的Runner。&lt;/p&gt;

&lt;h2 id=&#34;filebeat对kubernetes的支持&#34;&gt;filebeat对kubernetes的支持&lt;/h2&gt;

&lt;p&gt;filebeat官方文档提供了在kubernetes下基于daemonset的部署方式，最主要的一个配置如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- type: docker
      containers.ids:
      - &amp;quot;*&amp;quot;
      processors:
        - add_kubernetes_metadata:
            in_cluster: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即设置输入input为docker类型。由于所有的容器的标准输出日志默认都在节点的/var/lib/docker/containers/&lt;containerId&gt;/*-json.log路径，所以本质上采集的是这类日志文件。&lt;/p&gt;

&lt;p&gt;和传统的部署方式有所区别的是，如果服务部署在kubernetes上，我们查看和检索日志的维度不能仅仅局限于节点和服务，还需要有podName，containerName等，所以每条日志我们都需要打标增加kubernetes的元信息才发送至后端。&lt;/p&gt;

&lt;p&gt;filebeat会在配置中增加了add_kubernetes_metadata的processor的情况下，启动监听kubernetes的watch服务，监听所有kubernetes pod的变更，然后将归属本节点的pod最新的事件同步至本地的缓存中。&lt;/p&gt;

&lt;p&gt;节点上一旦发生容器的销毁创建，/var/lib/docker/containers/下会有目录的变动，filebeat根据路径提取出containerId，再根据containerId从本地的缓存中找到pod信息，从而可以获取到podName、label等数据，并加到日志的元信息fields中。&lt;/p&gt;

&lt;p&gt;filebeat还有一个beta版的功能autodiscover，autodiscover的目的是把分散到不同节点上的filebeat配置文件集中管理。目前也支持kubernetes作为provider，本质上还是监听kubernetes事件然后采集docker的标准输出文件。&lt;/p&gt;

&lt;p&gt;大致架构如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/k8slog.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但是在实际生产环境使用中，仅采集容器的标准输出日志还是远远不够，我们往往还需要采集容器挂载出来的自定义日志目录，还需要控制每个服务的日志采集方式以及更多的定制化功能。&lt;/p&gt;

&lt;h2 id=&#34;性能分析与调优&#34;&gt;性能分析与调优&lt;/h2&gt;

&lt;p&gt;虽然beats系列主打轻量级，虽然用golang写的filebeat的内存占用确实比较基于jvm的logstash等好太多，但是事实告诉我们其实没那么简单。&lt;/p&gt;

&lt;p&gt;正常启动filebeat，一般确实只会占用3、40MB内存，但是在轻舟容器云上偶发性的我们也会发现某些节点上的filebeat容器内存占用超过配置的pod limit限制（一般设置为200MB），并且不停的触发的OOM。&lt;/p&gt;

&lt;p&gt;究其原因，一般容器化环境中，特别是裸机上运行的容器个数可能会比较多，导致创建大量的harvester去采集日志。如果没有很好的配置filebeat，会有较大概率导致内存急剧上升。
当然，filebeat内存占据较大的部分还是memqueue，所有采集到的日志都会先发送至memqueue聚集，再通过output发送出去。每条日志的数据在filebeat中都被组装为event结构，filebeat默认配置的memqueue缓存的event个数为4096，可通过queue.mem.events设置。默认最大的一条日志的event大小限制为10MB，可通过max_bytes设置。4096 * 10MB = 40GB，可以想象，极端场景下，filebeat至少占据40GB的内存。特别是配置了multiline多行模式的情况下，如果multiline配置有误，单个event误采集为上千条日志的数据，很可能导致memqueue占据了大量内存，致使内存爆炸。&lt;/p&gt;

&lt;p&gt;所以，合理的配置日志文件的匹配规则，限制单行日志大小，根据实际情况配置memqueue缓存的个数，才能在实际使用中规避filebeat的内存占用过大的问题。&lt;/p&gt;

&lt;p&gt;有些文章说filebeat内存消耗很少,不会超过100M, 这简直是不负责任的胡说,假如带着这样的认识把filebeat部署到生产服务器上就等着哭吧.&lt;/p&gt;

&lt;p&gt;那怎么样才能避免以上内存灾难呢?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;每个日志生产环境生产的日志大小,爆发量都不一样, 要根据自己的日志特点设定合适的event值;什么叫合适,至少能避免内存&amp;gt;200MB的灾难;&lt;/li&gt;
&lt;li&gt;在不知道日志实际情况(单条大小,爆发量), 务必把event设置上,建议128或者256;&lt;/li&gt;
&lt;li&gt;合理的配置日志文件的匹配规则，是否因为通配符的原因，造成同时监控数量巨大的文件，这种情况应该避免用通配符监控无用的文件。&lt;/li&gt;
&lt;li&gt;规范日志，限制单行日志大小，是否文件的单行内容巨大，确定是否需要改造文件内容，或者将其过滤&lt;/li&gt;
&lt;li&gt;限制cpu&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面的一系列操作可以做如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max_procs: 2
queue:
  mem:
    events: 512
    flush.min_events: 256
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;限制cpu为2core，内存最大为512*10M～=5G&lt;/p&gt;

&lt;p&gt;限制cpu的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max_procs，限制filebeat的进程数量，其实是内核数，建议手动设为1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;限制内存的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;queue.mem.events消息队列的大小，默认值是4096，这个参数在6.0以前的版本是spool-size，通过命令行，在启动时进行配置
max_message_bytes 单条消息的大小, 默认值是10M
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;filebeat最大的可能占用的内存是max_message_bytes * queue.mem.events = 40G，考虑到这个queue是用于存储encode过的数据，raw数据也是要存储的，所以，在没有对内存进行限制的情况下，最大的内存占用情况是可以达到超过80G。&lt;/p&gt;

&lt;h2 id=&#34;内存使用过多的情况&#34;&gt;内存使用过多的情况&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;非常频繁的rotate日志&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于实时大量产生内容的文件，比如日志，常用的做法往往是将日志文件进行rotate，根据策略的不同，每隔一段时间或者达到固定大小之后，将日志rotate。
这样，在文件目录下可能会产生大量的日志文件。
如果我们使用通配符的方式，去监控该目录，则filebeat会启动大量的harvester实例去采集文件。但是，请记住，我这里不是说这样一定会产生内存泄漏，只是在这里观测到了内存泄漏而已，不是说这是造成内存泄漏的原因。&lt;/p&gt;

&lt;p&gt;当filebeat运行了几个月之后，占用了超过10个G的内存。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;因为multiline导致内存占用过多&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;multiline.pattern: &amp;lsquo;^[[:space:]]+|^Caused by:|^.+Exception:|^\d+\serror，比如这个配置，认为空格或者制表符开头的line是上一行的附加内容，需要作为多行模式，存储到同一个event当中。当你监控的文件刚巧在文件的每一行带有一个空格时，会错误的匹配多行，造成filebeat解析过后，单条event的行数达到了上千行，大小达到了10M，并且在这过程中使用的是正则表达式，每一条event的处理都会极大的消耗内存。因为大多数的filebeat output是需应答的，buffer这些event必然会大量的消耗内存。&lt;/p&gt;

&lt;h2 id=&#34;解读日志中的监控数据&#34;&gt;解读日志中的监控数据&lt;/h2&gt;

&lt;p&gt;其实filebeat的日志，已经包含了很多参数用于实时观测filebeat的资源使用情况，（下面是6.0版本的，6.5版本之后，整个日志格式变了，从kv格式变成了json对象格式）&lt;/p&gt;

&lt;p&gt;里面的参数主要分成三个部分：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;beat.*，包含memstats.gc_next，memstats.memory_alloc，memstats.memory_total，这个是所有beat组件都有的指标，是filebeat继承来的，主要是内存相关的，我们这里特别关注memstats.memory_alloc，alloc的越多，占用内存越大
filebeat.*，这部分是filebeat特有的指标，通过event相关的指标，我们知道吞吐，通过harvester，我们知道正在监控多少个文件，未消费event堆积的越多，havester创建的越多，消耗内存越大
libbeat.*，也是beats组件通用的指标，包含outputs和pipeline等信息。这里要主要当outputs发生阻塞的时候，会直接影响queue里面event的消费，造成内存堆积
registrar，filebeat将监控文件的状态放在registry文件里面，当监控文件非常多的时候，比如10万个，而且没有合理的设置close_inactive参数，这个文件能达到100M，载入内存后，直接占用内存
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在6.5之后都是json，但也是kv结构，可以对应查看。&lt;/p&gt;

&lt;h2 id=&#34;如何对filebeat进行扩展开发&#34;&gt;如何对filebeat进行扩展开发&lt;/h2&gt;

&lt;p&gt;一般情况下filebeat可满足大部分的日志采集需求，但是仍然避免不了一些特殊的场景需要我们对filebeat进行定制化开发，当然filebeat本身的设计也提供了良好的扩展性。
beats目前只提供了像elasticsearch、kafka、logstash等几类output客户端，如果我们想要filebeat直接发送至其他后端，需要定制化开发自己的output。同样，如果需要对日志做过滤处理或者增加元信息，也可以自制processor插件。
无论是增加output还是写个processor，filebeat提供的大体思路基本相同。一般来讲有3种方式：&lt;/p&gt;

&lt;p&gt;1.直接fork filebeat，在现有的源码上开发。output或者processor都提供了类似Run、Stop等的接口，只需要实现该类接口，然后在init方法中注册相应的插件初始化方法即可。当然，由于golang中init方法是在import包时才被调用，所以需要在初始化filebeat的代码中手动import。&lt;/p&gt;

&lt;p&gt;2.filebeat还提供了基于golang plugin的插件机制，需要把自研的插件编译成.so共享链接库，然后在filebeat启动参数中通过-plugin指定库所在路径。不过实际上一方面golang plugin还不够成熟稳定，一方面自研的插件依然需要依赖相同版本的libbeat库，而且还需要相同的golang版本编译，坑可能更多，不太推荐。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- Filebeat</title>
          <link>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat/</link>
          <pubDate>Wed, 08 Jul 2020 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat/</guid>
          <description>&lt;p&gt;Filebeat 是使用 Golang 实现的轻量型日志采集器，是基于原先 logstash-forwarder 的源码改造出来的，没有任何依赖，可以单独存在的搞性能采集工具。&lt;/p&gt;

&lt;h1 id=&#34;认识beats&#34;&gt;认识beats&lt;/h1&gt;

&lt;p&gt;Beats是轻量级（资源高效，无依赖性，小型）采集程序的集合。这些可以是日志文件（Filebeat），网络数据（Packetbeat），服务器指标（Metricbeat）等，Beats建立在名为libbeat的Go框架之上，该框架主要用于数据转发，Elastic和社区开发的越来越多的Beats可以收集的任何其他类型的数据，收集后，数据将直接发送到Elasticsearch或Logstash中进行其他处理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Filebeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;顾名思义，Filebeat用于收集和传送日志文件，它也是最常用的Beat。 Filebeat如此高效的事实之一就是它处理背压的方式，如果Logstash繁忙，Filebeat会减慢其读取速率，并在减速结束后加快节奏。
Filebeat几乎可以安装在任何操作系统上，包括作为Docker容器安装，还随附用于特定平台（例如Apache，MySQL，Docker等）的内部模块，其中包含这些平台的默认配置和Kibana对象。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Packetbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;网络数据包分析器Packetbeat是第一个引入的beat。 Packetbeat捕获服务器之间的网络流量，因此可用于应用程序和性能监视。
Packetbeat可以安装在受监视的服务器上，也可以安装在其专用服务器上。 Packetbeat跟踪网络流量，解码协议并记录每笔交易的数据。 Packetbeat支持的协议包括：DNS，HTTP，ICMP，Redis，MySQL，MongoDB，Cassandra等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Metricbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Metricbeat是一种非常受欢迎的beat，它收集并报告各种系统和平台的各种系统级度量。 Metricbeat还支持用于从特定平台收集统计信息的内部模块。您可以使用这些模块和称为指标集的metricsets来配置Metricbeat收集指标的频率以及要收集哪些特定指标。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Heartbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Heartbeat是用于“uptime monitoring”的。本质上，Heartbeat是探测服务以检查它们是否可访问的功能，例如，它可以用来验证服务的正常运行时间是否符合您的SLA。 您要做的就是为Heartbeat提供URL和正常运行时间指标的列表。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Auditbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Auditbeat可用于操作Linux服务器上的用户和进程活动。 与其他传统的系统工具（systemd，auditd）类似，Auditbeat可用于识别安全漏洞-文件更改，配置更改，恶意行为等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Winlogbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Winlogbeat仅会引起Windows系统管理员或工程师的兴趣，因为它是专门为收集Windows事件日志而设计的。 它可用于分析安全事件，已安装的更新等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Functionbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Functionbeat主要是为“serverless”而设计，可以将其部署为收集数据并将其发送到ELK堆栈的功能。 Functionbeat专为监视云环境而设计，目前已针对Amazon设置量身定制，可以部署为Amazon Lambda函数，以从Amazon CloudWatch，Kinesis和SQS收集数据。&lt;/p&gt;

&lt;h1 id=&#34;filebeat&#34;&gt;filebeat&lt;/h1&gt;

&lt;p&gt;为什么选择filebeat&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;性能好&lt;/li&gt;
&lt;li&gt;基于golang的技术站，对于容器生态友好&lt;/li&gt;
&lt;li&gt;使用部署方便，功能齐全&lt;/li&gt;
&lt;li&gt;其他技术方案，主要是社区的fluentd，使用的ruby+c，使用起来很复杂，各种功能都需要写插件来完成。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;基本使用&#34;&gt;基本使用&lt;/h2&gt;

&lt;h3 id=&#34;下载&#34;&gt;下载&lt;/h3&gt;

&lt;p&gt;直接去github上可以下载二进制文件，可以直接运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.10.2-linux-x86_64.tar.gz
tar xzvf filebeat-7.10.2-linux-x86_64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以使用源码编译&lt;/p&gt;

&lt;p&gt;After installing Go, set the GOPATH environment variable to point to your workspace location, and make sure $GOPATH/bin is in your PATH.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p ${GOPATH}/src/github.com/elastic
git clone https://github.com/elastic/beats ${GOPATH}/src/github.com/elastic/beats
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have multiple go paths, use ${GOPATH%%:*} instead of ${GOPATH}.&lt;/p&gt;

&lt;p&gt;Then you can compile a particular Beat by using the Makefile. For example, for Packetbeat:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd beats/packetbeat
make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of the Beats might have extra development requirements, in which case you’ll find a CONTRIBUTING.md file in the Beat directory.&lt;/p&gt;

&lt;p&gt;We use an EditorConfig file in the beats repository to standardise how different editors handle whitespace, line endings, and other coding styles in our files. Most popular editors have a plugin for EditorConfig and we strongly recommend that you install it.&lt;/p&gt;

&lt;h3 id=&#34;配置文件&#34;&gt;配置文件&lt;/h3&gt;

&lt;p&gt;FileBeat 的配置文件定义了在读取文件的位置，输出流的位置以及相应的性能参数，本实例是以 Kafka 消息中间件作为缓冲，所有的日志收集器都向 Kafka 输送日志流，相应的最简单的配置项如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim fileat.yml

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /wls/applogs/rtlog/app.log
  fields:
    log_topic: appName
  multiline:
        # pattern for error log, if start with space or cause by
        pattern: &#39;^[[:space:]]+(at|\.{3})\b|^Caused by:&#39;
        negate:  false
        match:   after

output.kafka:
   enabled: true
   hosts: [&amp;quot;kafka-1:9092&amp;quot;,&amp;quot;kafka-2:9092&amp;quot;]
   topic: applog
   version: &amp;quot;0.10.2.0&amp;quot;
   compression: gzip

processors:
- drop_fields:
   fields: [&amp;quot;beat&amp;quot;, &amp;quot;input&amp;quot;, &amp;quot;source&amp;quot;, &amp;quot;offset&amp;quot;]

logging.level: error
name: app-server-ip
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;paths:定义了日志文件路径，可以采用模糊匹配模式，如*.log&lt;/li&gt;
&lt;li&gt;fields：topic 对应的消息字段或自定义增加的字段。&lt;/li&gt;
&lt;li&gt;output.kafka：filebeat 支持多种输出，支持向 kafka，logstash，elasticsearch 输出数据，此处设置数据输出到 kafka。&lt;/li&gt;
&lt;li&gt;enabled：这个启动这个模块。&lt;/li&gt;
&lt;li&gt;topic：指定要发送数据给 kafka 集群的哪个 topic，若指定的 topic 不存在，则会自动创建此 topic。&lt;/li&gt;
&lt;li&gt;version：指定 kafka 的版本。&lt;/li&gt;
&lt;li&gt;drop_fields：舍弃字段，filebeat 会 json 日志信息，适当舍弃无用字段节省空间资源。&lt;/li&gt;
&lt;li&gt;name：收集日志中对应主机的名字，建议 name 这里设置为 IP，便于区分多台主机的日志信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每一个不同的输出都用不同的配置项，还有很多功能性能的配置项，比如&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;合并规则&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;multiline:
        pattern: &#39;^[[:space:]]+(at|\.{3})\b|^Caused by:&#39;
        negate:  false
        match:   after
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用的合并规则&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[0-9]{4}-[0-9]{2}-[0-9]{2} 按照yyyy-mm-dd时间戳合并，日志不是以这个时间戳开头的都合并
[[0-9]{4}-[0-9]{2}-[0-9]{2} 按照[yyyy-mm-dd时间戳合并，日志不是以这个时间戳开头的都合并
* 不合并----不合并最后处理出来的结果就是没有这一段的配置
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;过滤规则&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;include_lines: [&amp;quot;FATAL&amp;quot;,&amp;quot;ERROR&amp;quot;,&amp;quot;WARN&amp;quot;,&amp;quot;INFO&amp;quot;,&amp;quot;fatal&amp;quot;,&amp;quot;error&amp;quot;,&amp;quot;warn&amp;quot;,&amp;quot;info&amp;quot;,&amp;quot;Fatal&amp;quot;,&amp;quot;Error&amp;quot;,&amp;quot;Warn&amp;quot;,&amp;quot;Info&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常用于日志级别的选择，比如上面只是采集包含这些字段的日志，其实也就是INFO级别的日志的采集。当然还是使用exclude_lines，不包含，和白名单黑名单一样的概念。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;资源限制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;logging.level: debug    日志级别
max_procs: 2            cpu核数限制

queue:
      mem:
        events: 32768               pipeline队列长度
        flush.min_events: 1024
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;采集配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;close_eof: false
close_inactive: 5m（5分钟没有活跃，就会停止采集）
close_removed: false
close_renamed: false
ignore_older: 48h（即将开启的采集文件如果大于48H，就不要采集了）
# State options
clean_removed: true
clean_inactive: 72h（如果文件72h没有活跃就删除采集记录）
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;输出配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;output.kafka:
      topic: &amp;quot;%{[topic]}&amp;quot;
      version: &amp;quot;0.8.2.2&amp;quot;
      codec.format:
        ignoreNotFound: true
        string: &#39;%{[message]}&#39;
      metadata:
        retry.max: 2
        full: true
      worker: 10（The number of concurrent load-balanced Kafka output workers.）
      channel_buffer_size: 30000（每一个连接可以缓存消息的长度，默认是256）
      ##bulk_max_size: 20480（一次发送kafka请求最多的事件数量，默认是2048）
      #keep_alive: 0（是否保持连接，默认0，不保持）
      ##required_acks: 0（代表kafka是否需要等待回复，有1，0，-1，默认是1，需要等待主节点回复）
      compression: none（代表压缩级别，none代表不压缩，默认压缩是gzip）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多详细的配置可以去查看完整的配置文件说明。&lt;/p&gt;

&lt;h3 id=&#34;启动&#34;&gt;启动&lt;/h3&gt;

&lt;p&gt;调试模式下采用：终端启动（退出终端或 ctrl+c 会退出运行）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./filebeat -e -c filebeat.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;线上环境配合 error 级别使用：以后台守护进程启动启动 filebeats&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup ./filebeat -e -c filebeat.yml &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;零输出启动（不推荐）：将所有标准输出及标准错误输出到/dev/null空设备，即没有任何输出信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup ./filebeat -e -c filebeat.yml &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止运行 FileBeat 进程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ps -ef | grep filebeat
Kill -9 线程号
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;特性&#34;&gt;特性&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;采集路径可以使用正则表达式，来采集当前目录下所有的文件，包括子目录下，比如/k8s_log/*&lt;em&gt;/&lt;/em&gt;.log*&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可以动态加载，可以在配置文件中配置reload的时间，filebeat本身自动加载，但是这个加载不能更新output&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filebeat本身日志支持备份切换，默认一个文件10M，保留8个文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filebaet支持句柄保持和checkpoint功能。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filebeat输出到kafka可以不支持多个kafka集群，可以改造多pipeline来发送到不同的Kafka集群。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;h3 id=&#34;filebeat创建一个beater&#34;&gt;filebeat创建一个beater&lt;/h3&gt;

&lt;p&gt;filebaet创建了一个beater实例启动&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.启动了一个Crawler，用于

&lt;ul&gt;
&lt;li&gt;1.启动静态的 input (写在主配置里的)&lt;/li&gt;
&lt;li&gt;2.启动 reloader，动态的 input 由 reloader 管理。&lt;/li&gt;
&lt;li&gt;3.启动Registrar&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2.每个input又启动了Harvester，Harvester就是负责采集日志&lt;/li&gt;
&lt;li&gt;3.Harvester 连接 pipeline 时，调用 outlet factory 创建一个 outleter，Outleter 封装了 pipeline 的 producer，调用 outleter OnEvent 方法发送数据到 pipeline&lt;/li&gt;
&lt;li&gt;4.Registrar 负责 checkpoint 文件的更新&lt;/li&gt;
&lt;li&gt;5.启动Pipeline 模块，Pipeline 是一个大的功能模块，包含 &lt;code&gt;queue&lt;/code&gt;, &lt;code&gt;outputController&lt;/code&gt;, &lt;code&gt;consumer&lt;/code&gt;, &lt;code&gt;output&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;1.Queue (memqueue)
真正的 queue 对象时 Broker。创建 broker 时启动事件循环，负责处理 publish 和 consume 等各种请求

&lt;ul&gt;
&lt;li&gt;创建 broker&lt;/li&gt;
&lt;li&gt;事件循环&lt;/li&gt;
&lt;li&gt;Consumer&lt;/li&gt;
&lt;li&gt;Producer 真实创建的是 ackProducer&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2.Output Controller
负责管理 consumer。Consumer 负责从 queue 中获取 batch，然后发送到 workQueue(chan publisher.Batch)。&lt;/li&gt;
&lt;li&gt;3.Consumer
Consumer 会启动多个 outputWorker。outputWorker 负责从 workQueue 获取 batch，然后调用 output client 的 Publish 方法发送出去。&lt;/li&gt;
&lt;li&gt;4.Retryer
Retry 负责重试发送失败的请求&lt;/li&gt;
&lt;li&gt;5.Output(kafka)
Connect 调用 sarama 库，创建一个 kafka AsyncProducer。连接时会启动两个循环，处理成功响应和失败响应。&lt;/li&gt;
&lt;li&gt;6.msgRef
msgRef 注入到发送给 AsyncProducer 的事件中。在处理响应的时候回调。如果成功就调用 batch.ACK()。失败就调用 batch.OnRetry 重试。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;核心代码模块&#34;&gt;核心代码模块&lt;/h3&gt;

&lt;h4 id=&#34;filebeat-1&#34;&gt;filebeat&lt;/h4&gt;

&lt;p&gt;beater 启动
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/beater/filebeat.go#L273&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/beater/filebeat.go#L273&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Crawler 启动时加载配置文件中的 inputs。启动 reloader，定期加载动态配置文件目录中的 inputs。启动Registrar，负责checkpoint
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/crawler/crawler.go#L33&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/crawler/crawler.go#L33&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动静态的 input (写在主配置里的)&lt;/li&gt;
&lt;li&gt;启动 reloader，动态的 input 由 reloader 管理&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;InputInput 是一个用来包装 &lt;code&gt;harvester&lt;/code&gt; 的数据结构，对外提供生命周期管理接口。Input 在建立起来时，会调用 pipeline 的 &lt;code&gt;ConnectWith&lt;/code&gt; 方法获取一个 client，用于发送 events。
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/input.go#L35&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/input.go#L35&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Harvester
负责采集日志
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/log/harvester.go#L88&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/log/harvester.go#L88&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Outleter：
Harvester 连接 pipeline 时，调用 outlet factory 创建一个 outleter
Outleter 封装了 pipeline 的 producer，调用 outleter OnEvent 方法发送数据到 pipeline&lt;/p&gt;

&lt;p&gt;Outleter 创建
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/channel/factory.go#L70&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/channel/factory.go#L70&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Registrar
负责 checkpoint 文件的更新
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/registrar/registrar.go#L19&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/registrar/registrar.go#L19&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;libbeat&#34;&gt;Libbeat&lt;/h4&gt;

&lt;p&gt;Pipeline 模块启动
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L30&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L30&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;加载 output &lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L85&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L85&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Pipeline
Pipeline 包含 queue, output controller, consumer, output&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/pipeline.go#L135&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/pipeline.go#L135&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Output Controller

&lt;ul&gt;
&lt;li&gt;负责管理 consumer。Consumer 负责从 queue 中获取 batch，然后发送到 workQueue(chan publisher.Batch)。&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/controller.go#L13&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/controller.go#L13&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Consumer

&lt;ul&gt;
&lt;li&gt;Consumer 会启动多个 outputWorker。outputWorker 负责从 workQueue 获取 batch，然后调用 output client 的 Publish 方法发送出去。
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/consumer.go#L43&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/consumer.go#L43&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Retryer

&lt;ul&gt;
&lt;li&gt;Retry 负责重试发送失败的请求
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/retry.go#L14&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/retry.go#L14&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Output(kafka)

&lt;ul&gt;
&lt;li&gt;创建 &lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/kafka.go#L114&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/kafka.go#L114&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Connect
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L68&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L68&lt;/a&gt;
调用 sarama 库，创建一个 kafka AsyncProducer。连接时会启动两个循环，处理成功响应和失败响应。&lt;/li&gt;
&lt;li&gt;msgRef
msgRef 注入到发送给 AsyncProducer 的事件中。在处理响应的时候回调。如果成功就调用 batch.ACK()。失败就调用 batch.OnRetry 重试。
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L222&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L222&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Queue (memqueue)
真正的 queue 对象时 Broker。创建 broker 时启动事件循环，负责处理 publish 和 consume 等各种请求&lt;/li&gt;
&lt;li&gt;创建 broker
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/broker.go#L63&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/broker.go#L63&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;事件循环
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/eventloop.go#L30&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/eventloop.go#L30&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consumer
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/consume.go#L12&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/consume.go#L12&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Producer
真实创建的是 ackProducer
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/produce.go#L39&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/produce.go#L39&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;工作机制:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;队列容量为 &lt;code&gt;Events&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;当队列中的 events 数量大于 &lt;code&gt;FlushMinEvents&lt;/code&gt; 开始 flush&lt;/li&gt;
&lt;li&gt;当队列中有 events 并且离上一次 flush 过了 &lt;code&gt;FlushTimeout&lt;/code&gt; 时间，开始 flush&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这边简单梳理了filebeat的模块，核心的原理包括一些beats的原理可以看我写的另一篇&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/&#34;&gt;filebeat原理&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s组件系列—- 存储CSI</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-store-csi/</link>
          <pubDate>Mon, 29 Jun 2020 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-store-csi/</guid>
          <description>&lt;p&gt;CSI是Container Storage Interface的简称，旨在能为容器编排引擎和存储系统间建立一套标准的存储调用接口，实现解耦，通过该接口能为容器编排引擎提供存储服务。&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;p&gt;以csi-hostpath插件为例，演示部署CSI插件、用户使用CSI插件提供的存储资源。&lt;/p&gt;

&lt;h2 id=&#34;开启csi&#34;&gt;开启csi&lt;/h2&gt;

&lt;p&gt;设置Kubernetes服务启动参数，为kube-apiserver、kubecontroller-manager和kubelet服务的启动参数添加。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@k8smaster01 ~]# vi /etc/kubernetes/manifests/kube-apiserver.yaml
……
    - --allow-privileged=true
    - --feature-gates=CSIPersistentVolume=true
    - --runtime-config=storage.k8s.io/v1alpha1=true
……
[root@k8smaster01 ~]# vi /etc/kubernetes/manifests/kube-controller-manager.yaml
……
    - --feature-gates=CSIPersistentVolume=true
……
[root@k8smaster01 ~]# vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment=&amp;quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --feature-gates=CSIPersistentVolume=true&amp;quot;
……
[root@k8smaster01 ~]# systemctl daemon-reload
[root@k8smaster01 ~]# systemctl restart kubelet.service
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;设计架构&#34;&gt;设计架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;大致看上去，这个和原生存储架构并没有太大的变化，其实不然，原生中Plugin和driver之间的调用使用的是操作系统命令行接口，而CSI采用的是grpc调用，grpc调用的一个优势就是可以将grpc服务运行在socket上，这样服务端就可以运行在socket端点的任何地方，换句话说就是可以被隔离单独运行。&lt;/p&gt;

&lt;p&gt;其中组件的主要作用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PV Controller：负责 PV/PVC 绑定及周期管理，根据需求进行数据卷的 Provision/Delete 操作；&lt;/li&gt;
&lt;li&gt;AD Controller：负责数据卷的 Attach/Detach 操作，将设备挂接到目标节点；&lt;/li&gt;
&lt;li&gt;Kubelet：Kubelet 是在每个 Node 节点上运行的主要 “节点代理”，功能是 Pod 生命周期管理、容器健康检查、容器监控等；&lt;/li&gt;
&lt;li&gt;Volume Manager：Kubelet 中的组件，负责管理数据卷的 Mount/Umount 操作（也负责数据卷的 Attach/Detach 操作，需配置 kubelet 相关参数开启该特性）、卷设备的格式化等等；&lt;/li&gt;
&lt;li&gt;Volume Plugins：存储插件，由存储供应商开发，目的在于扩展各种存储类型的卷管理能力，实现第三方存储的各种操作能力，即是上面蓝色操作的实现。Volume Plugins 有 in-tree 和 out-of-tree 两种；&lt;/li&gt;
&lt;li&gt;External Provioner：External Provioner 是一种 sidecar 容器，作用是调用 Volume Plugins 中的 CreateVolume 和 DeleteVolume 函数来执行 Provision/Delete 操作。因为 K8s 的 PV 控制器无法直接调用 Volume Plugins 的相关函数，故由 External Provioner 通过 gRPC 来调用；&lt;/li&gt;
&lt;li&gt;External Attacher：External Attacher 是一种 sidecar 容器，作用是调用 Volume Plugins 中的 ControllerPublishVolume 和 ControllerUnpublishVolume 函数来执行 Attach/Detach 操作。因为 K8s 的 AD 控制器无法直接调用 Volume Plugins 的相关函数，故由 External Attacher 通过 gRPC 来调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见csi提供了一套标准的接口集成在k8s的源码（kube-controller-manager，kubelet）中，然后第三方的存储插件只要实现这些接口并注册就可以调用对应的函数进行pv和pvc的自动创建，提供了可扩展的机会。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store14.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;所以部署架构如下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;右边一个StatefulSet或Deployment的pod，可以说是csi controller，提供存储服务视角对存储资源和存储卷进行管理和操作。在Kubernetes中建议将其部署为单实例Pod，可以使用StatefulSet或Deployment控制器进行部署，设置副本数量为1，保证为一种存储插件只运行一个控制器实例。

&lt;ul&gt;
&lt;li&gt;用户实现的 CSI 插件，也就是CSI Driver存储驱动容器（正常和下面的CSI 插件是同一个程序，也可以分开做一个控制插件，一个操作插件）&lt;/li&gt;
&lt;li&gt;与Master（kube-controller-manager）通信的辅助sidecar容器。

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher&#34;&gt;External Attacher&lt;/a&gt;：Kubernetes 提供的 sidecar 容器，它监听 VolumeAttachment 和 PersistentVolume 对象的变化情况，并调用 CSI 插件的 ControllerPublishVolume 和 ControllerUnpublishVolume 等 API 将 Volume 挂载或卸载到指定的 Node 上（官网提供）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34;&gt;External Provisioner&lt;/a&gt;：Kubernetes 提供的 sidecar 容器，它监听 PersistentVolumeClaim 对象的变化情况，并调用 CSI 插件的 ControllerPublish 和 ControllerUnpublish 等 API 管理 Volume（官网提供）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;这两个容器通过本地Socket（Unix DomainSocket，UDS），并使用gPRC协议进行通信。&lt;/li&gt;
&lt;li&gt;sidecar容器通过Socket调用CSI Driver容器的CSI接口，CSI Driver容器负责具体的存储卷操作。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;左边一个Daemonset的pod：对主机（Node）上的Volume进行管理和操作。在Kubernetes中建议将其部署为DaemonSet，在每个Node上都运行一个Pod，以便 Kubelet 可以调用。它包含 2 个容器

&lt;ul&gt;
&lt;li&gt;用户实现的 CSI 插件，也就是CSI Driver存储驱动容器，主要功能是接收kubelet的调用，需要实现一系列与Node相关的CSI接口，例如NodePublishVolume接口（用于将Volume挂载到容器内的目标路径）、NodeUnpublishVolume接口（用于从容器中卸载Volume），等等。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/driver-registrar&#34;&gt;Driver Registrar&lt;/a&gt;：注册 CSI 插件到 kubelet 中，并初始化 NodeId（即给 Node 对象增加一个 Annotation csi.volume.kubernetes.io/nodeid）（官网提供）&lt;/li&gt;
&lt;li&gt;node-driver-registrar容器与kubelet通过Node主机的一个hostPath目录下的unixsocket进行通信。CSI Driver容器与kubelet通过Node主机的另一个hostPath目录下的unixsocket进行通信，同时需要将kubelet的工作目录（默认为/var/lib/kubelet）挂载给CSIDriver容器，用于为Pod进行Volume的管理操作（包括mount、umount等）。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以重点就是用户自己实现的插件逻辑，&lt;a href=&#34;https://github.com/kubernetes-csi&#34;&gt;官方&lt;/a&gt;已经支持实现了很多的插件，我们在开发的时候可以参考，如何开发一个csi插件，我们需要先了解一下插件的使用过程。&lt;/p&gt;

&lt;h1 id=&#34;存储流程&#34;&gt;存储流程&lt;/h1&gt;

&lt;h2 id=&#34;pv创建方式&#34;&gt;pv创建方式&lt;/h2&gt;

&lt;p&gt;我们先了解一下创建 PV 的两种方式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一种是集群管理员通过手动方式静态创建应用所需要的 PV，也就是我们在&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-store/&#34;&gt;存储&lt;/a&gt;中说明的内置的原生的方式。&lt;/li&gt;
&lt;li&gt;另一种是用户手动创建 PVC 并由 Provisioner 组件动态创建对应的 PV。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;静态创建存储卷&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;集群管理员创建 NFS PV，NFS 属于 K8s 原生支持的 in-tree 存储类型。&lt;/li&gt;
&lt;li&gt;用户创建 PVC，通过 kubectl get pv 命令可看到 PV 和 PVC 自动绑定&lt;/li&gt;
&lt;li&gt;用户创建应用，并使用第二步创建的 PVC。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;就完成了静态创建存储卷并且使用，也就是我们最初使用的方式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;动态创建存储卷&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;动态创建存储卷相比静态创建存储卷，少了集群管理员的干预，动态创建存储卷，要求集群中部署有 provisioner 以及对应的 storageclass。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store8.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;集群管理员只需要保证环境中有相关的 storageclass 即可&lt;/li&gt;
&lt;li&gt;用户创建 PVC，此处 PVC 的 storageClassName 指定为上面的 storageclass 名称，集群中的 provisioner 会动态创建相应 PV。&lt;/li&gt;
&lt;li&gt;用户创建应用，并使用第二步创建的 PVC，同静态创建存储卷的第三步。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;流程&#34;&gt;流程&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;pod创建过程中调用存储的相关流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户创建了一个包含 PVC 的 Pod，该 PVC 要求使用动态存储卷；&lt;/li&gt;
&lt;li&gt;Scheduler 根据 Pod 配置、节点状态、PV 配置等信息，把 Pod 调度到一个合适的 Worker 节点上；（图上23换一下）&lt;/li&gt;
&lt;li&gt;PV 控制器 watch 到该 Pod 使用的 PVC 处于 Pending 状态，于是调用 Volume Plugin（in-tree）创建存储卷，并创建 PV 对象（out-of-tree 由 External Provisioner 来处理）；&lt;/li&gt;
&lt;li&gt;AD 控制器发现 Pod 和 PVC 处于待挂接状态，于是调用 Volume Plugin 挂接存储设备到目标 Worker 节点上&lt;/li&gt;
&lt;li&gt;在 Worker 节点上，Kubelet 中的 Volume Manager 等待存储设备挂接完成，并通过 Volume Plugin 将设备挂载到全局目录：/var/lib/kubelet/pods/[pod uid]/volumes/kubernetes.io~iscsi/[PV name]（以 iscsi 为例）；&lt;/li&gt;
&lt;li&gt;Kubelet 通过 Docker 启动 Pod 的 Containers，用 bind mount 方式将已挂载到本地全局目录的卷映射到容器中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以通过时序图看的更加清晰&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store10.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见，存储卷从创建到提供应用使用共分为三个阶段：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Provision/Delete（创建/删除存储卷，处理pv和pvc之间的关系）&lt;/li&gt;
&lt;li&gt;Attach/Detach（挂接和摘除存储卷，处理的是volumes和node上目录之间的关系）&lt;/li&gt;
&lt;li&gt;Mount/Unmount（挂载和摘除目录，处理的是volumes和pod之间的关系）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;provisioning-volumes&#34;&gt;provisioning volumes&lt;/h3&gt;

&lt;p&gt;先了解一些pv控制器的基本概念&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store11.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;重图上可见pv控制器中主要有两个worker&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ClaimWorker：处理 PVC 的 add / update / delete 相关事件以及 PVC 的状态迁移；&lt;/li&gt;
&lt;li&gt;VolumeWorker：负责 PV 的状态迁移。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;PV 状态迁移（UpdatePVStatus）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;PV 初始状态为 Available，当 PV 与 PVC 绑定后，状态变为 Bound；&lt;/li&gt;
&lt;li&gt;与 PV 绑定的 PVC 删除后，状态变为 Released；&lt;/li&gt;
&lt;li&gt;当 PV 回收策略为 Recycled 或手动删除 PV 的 .Spec.ClaimRef 后，PV 状态变为 Available；&lt;/li&gt;
&lt;li&gt;当 PV 回收策略未知或 Recycle 失败或存储卷删除失败，PV 状态变为 Failed；&lt;/li&gt;
&lt;li&gt;手动删除 PV 的 .Spec.ClaimRef，PV 状态变为 Available。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;PVC 状态迁移（UpdatePVCStatus）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;当集群中不存在满足 PVC 条件的 PV 时，PVC 状态为 Pending。在 PV 与 PVC 绑定后，PVC 状态由 Pending 变为 Bound；&lt;/li&gt;
&lt;li&gt;与 PVC 绑定的 PV 在环境中被删除，PVC 状态变为 Lost；&lt;/li&gt;
&lt;li&gt;再次与一个同名 PV 绑定后，PVC 状态变为 Bound。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;再来看Provisioning 流程就十分清晰简单了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;先寻找静态存储卷（FindBestMatch）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PV 控制器首先在环境中筛选一个状态为 Available 的 PV 与新 PVC 匹配。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DelayBinding：PV 控制器判断该 PVC 是否需要延迟绑定：

&lt;ul&gt;
&lt;li&gt;查看 PVC 的 annotation 中是否包含 volume.kubernetes.io/selected-node，若存在则表示该 PVC 已经被调度器指定好了节点（属于 ProvisionVolume），故不需要延迟绑定；&lt;/li&gt;
&lt;li&gt;若 PVC 的 annotation 中不存在 volume.kubernetes.io/selected-node，同时没有 StorageClass，默认表示不需要延迟绑定；若有 StorageClass，查看其 VolumeBindingMode 字段，若为 WaitForFirstConsumer 则需要延迟绑定，若为 Immediate 则不需要延迟绑定；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;FindBestMatchPVForClaim：PV 控制器尝试找一个满足 PVC 要求的环境中现有的 PV。PV 控制器会将所有的 PV 进行一次筛选，并会从满足条件的 PV 中选择一个最佳匹配的 PV。筛选规则：

&lt;ul&gt;
&lt;li&gt;VolumeMode 是否匹配；&lt;/li&gt;
&lt;li&gt;PV 是否已绑定到 PVC 上；&lt;/li&gt;
&lt;li&gt;PV 的 .Status.Phase 是否为 Available；&lt;/li&gt;
&lt;li&gt;LabelSelector 检查，PV 与 PVC 的 label 要保持一致；&lt;/li&gt;
&lt;li&gt;PV 与 PVC 的 StorageClass 是否一致；&lt;/li&gt;
&lt;li&gt;每次迭代更新最小满足 PVC requested size 的 PV，并作为最终结果返回；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Bind：PV 控制器对选中的 PV、PVC 进行绑定：

&lt;ul&gt;
&lt;li&gt;更新 PV 的 .Spec.ClaimRef 信息为当前 PVC；&lt;/li&gt;
&lt;li&gt;更新 PV 的 .Status.Phase 为 Bound；&lt;/li&gt;
&lt;li&gt;新增 PV 的 annotation ：pv.kubernetes.io/bound-by-controller: “yes”；&lt;/li&gt;
&lt;li&gt;更新 PVC 的 .Spec.VolumeName 为 PV 名称；&lt;/li&gt;
&lt;li&gt;更新 PVC 的 .Status.Phase 为 Bound；&lt;/li&gt;
&lt;li&gt;新增 PVC 的 annotation：pv.kubernetes.io/bound-by-controller: “yes” 和 pv.kubernetes.io/bind-completed: “yes”；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;再动态创建存储卷（ProvisionVolume）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;若环境中没有合适的 PV，则进入动态 Provisioning 场景：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Before Provisioning：

&lt;ul&gt;
&lt;li&gt;PV 控制器首先判断 PVC 使用的 StorageClass 是 in-tree 还是 out-of-tree：通过查看 StorageClass 的 Provisioner 字段是否包含 “kubernetes.io/” 前缀来判断；&lt;/li&gt;
&lt;li&gt;PV 控制器更新 PVC 的 annotation：&lt;code&gt;claim.Annotations[“volume.beta.kubernetes.io/storage-provisioner”] = storageClass.Provisioner&lt;/code&gt;；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;in-tree Provisioning（internal provisioning）：

&lt;ul&gt;
&lt;li&gt;in-tree 的 Provioner 会实现 ProvisionableVolumePlugin 接口的 NewProvisioner 方法，用来返回一个新的 Provisioner；&lt;/li&gt;
&lt;li&gt;PV 控制器调用 Provisioner 的 Provision 函数，该函数会返回一个 PV 对象；&lt;/li&gt;
&lt;li&gt;PV 控制器创建上一步返回的 PV 对象，将其与 PVC 绑定，Spec.ClaimRef 设置为 PVC，.Status.Phase 设置为 Bound，.Spec.StorageClassName 设置为与 PVC 相同的 StorageClassName；同时新增 annotation：“pv.kubernetes.io/bound-by-controller”=“yes” 和 “pv.kubernetes.io/provisioned-by”=plugin.GetPluginName()；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;out-of-tree Provisioning（external provisioning）：

&lt;ul&gt;
&lt;li&gt;External Provisioner 检查 PVC 中的 claim.Spec.VolumeName 是否为空，不为空则直接跳过该 PVC；&lt;/li&gt;
&lt;li&gt;External Provisioner 检查 PVC 中的 &lt;code&gt;claim.Annotations[“volume.beta.kubernetes.io/storage-provisioner”]&lt;/code&gt;是否等于自己的 Provisioner Name（External Provisioner 在启动时会传入 &amp;ndash;provisioner 参数来确定自己的 Provisioner Name）；&lt;/li&gt;
&lt;li&gt;若 PVC 的 VolumeMode=Block，检查 External Provisioner 是否支持块设备；&lt;/li&gt;
&lt;li&gt;External Provisioner 调用 Provision 函数：通过 gRPC 调用 CSI 存储插件的 CreateVolume 接口；&lt;/li&gt;
&lt;li&gt;External Provisioner 创建一个 PV 来代表该 volume，同时将该 PV 与之前的 PVC 做绑定。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;deleting-volumes&#34;&gt;deleting volumes&lt;/h3&gt;

&lt;p&gt;用户删除 PVC，PV 控制器改变 PV.Status.Phase 为 Released。&lt;/p&gt;

&lt;p&gt;当 PV.Status.Phase == Released 时，PV 控制器首先检查 Spec.PersistentVolumeReclaimPolicy 的值，为 Retain 时直接跳过，为 Delete 时：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;in-tree Deleting：

&lt;ul&gt;
&lt;li&gt;in-tree 的 Provioner 会实现 DeletableVolumePlugin 接口的 NewDeleter 方法，用来返回一个新的 Deleter；&lt;/li&gt;
&lt;li&gt;控制器调用 Deleter 的 Delete 函数，删除对应 volume；&lt;/li&gt;
&lt;li&gt;在 volume 删除后，PV 控制器会删除 PV 对象；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;out-of-tree Deleting：

&lt;ul&gt;
&lt;li&gt;External Provisioner 调用 Delete 函数，通过 gRPC 调用 CSI 插件的 DeleteVolume 接口；&lt;/li&gt;
&lt;li&gt;在 volume 删除后，External Provisioner 会删除 PV 对象&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;attaching-volumes&#34;&gt;Attaching Volumes&lt;/h3&gt;

&lt;p&gt;Kubelet 组件和 AD 控制器都可以做 attach/detach 操作，若 Kubelet 的启动参数中指定了 &amp;ndash;enable-controller-attach-detach，则由 Kubelet 来做；否则默认由 AD 控制起来做。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;AD 控制器中有两个核心变量：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DesiredStateOfWorld（DSW）：集群中预期的数据卷挂接状态，包含了 nodes-&amp;gt;volumes-&amp;gt;pods 的信息；&lt;/li&gt;
&lt;li&gt;ActualStateOfWorld（ASW）：集群中实际的数据卷挂接状态，包含了 volumes-&amp;gt;nodes 的信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Attaching 流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AD 控制器根据集群中的资源信息，初始化 DSW 和 ASW。&lt;/li&gt;
&lt;li&gt;AD 控制器内部有三个组件周期性更新 DSW 和 ASW：

&lt;ul&gt;
&lt;li&gt;Reconciler。通过一个 GoRoutine 周期性运行，确保 volume 挂接 / 摘除完毕。此期间不断更新 ASW：

&lt;ul&gt;
&lt;li&gt;in-tree attaching：

&lt;ul&gt;
&lt;li&gt;in-tree 的 Attacher 会实现 AttachableVolumePlugin 接口的 NewAttacher 方法，用来返回一个新的 Attacher；&lt;/li&gt;
&lt;li&gt;AD 控制器调用 Attacher 的 Attach 函数进行设备挂接；&lt;/li&gt;
&lt;li&gt;更新 ASW。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;out-of-tree attaching：

&lt;ul&gt;
&lt;li&gt;调用 in-tree 的 CSIAttacher 创建一个 VolumeAttachement（VA）对象，该对象包含了 Attacher 信息、节点名称、待挂接 PV 信息；&lt;/li&gt;
&lt;li&gt;External Attacher 会 watch 集群中的 VolumeAttachement 资源，发现有需要挂接的数据卷时，调用 Attach 函数，通过 gRPC 调用 CSI 插件的 ControllerPublishVolume 接口。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;DesiredStateOfWorldPopulator。通过一个 GoRoutine 周期性运行，主要功能是更新 DSW：

&lt;ul&gt;
&lt;li&gt;findAndRemoveDeletedPods - 遍历所有 DSW 中的 Pods，若其已从集群中删除则从 DSW 中移除；&lt;/li&gt;
&lt;li&gt;findAndAddActivePods - 遍历所有 PodLister 中的 Pods，若 DSW 中不存在该 Pod 则添加至 DSW。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;PVC Worker。watch PVC 的 add/update 事件，处理 PVC 相关的 Pod，并实时更新 DSW。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;detaching-volumes&#34;&gt;Detaching Volumes&lt;/h3&gt;

&lt;p&gt;Detaching 流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当 Pod 被删除，AD 控制器会 watch 到该事件。首先 AD 控制器检查 Pod 所在的 Node 资源是否包含&amp;rdquo;volumes.kubernetes.io/keep-terminated-pod-volumes&amp;rdquo;标签，若包含则不做操作；不包含则从 DSW 中去掉该 volume；&lt;/li&gt;
&lt;li&gt;AD 控制器通过 Reconciler 使 ActualStateOfWorld 状态向 DesiredStateOfWorld 状态靠近，当发现 ASW 中有 DSW 中不存在的 volume 时，会做 Detach 操作：

&lt;ul&gt;
&lt;li&gt;in-tree detaching：

&lt;ul&gt;
&lt;li&gt;AD 控制器会实现 AttachableVolumePlugin 接口的 NewDetacher 方法，用来返回一个新的 Detacher；&lt;/li&gt;
&lt;li&gt;控制器调用 Detacher 的 Detach 函数，detach 对应 volume；&lt;/li&gt;
&lt;li&gt;AD 控制器更新 ASW。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;out-of-tree detaching：

&lt;ul&gt;
&lt;li&gt;AD 控制器调用 in-tree 的 CSIAttacher 删除相关 VolumeAttachement 对象；&lt;/li&gt;
&lt;li&gt;External Attacher 会 watch 集群中的 VolumeAttachement（VA）资源，发现有需要摘除的数据卷时，调用 Detach 函数，通过 gRPC 调用 CSI 插件的 ControllerUnpublishVolume 接口；&lt;/li&gt;
&lt;li&gt;AD 控制器更新 ASW。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;mounting-unmounting-volumes&#34;&gt;Mounting/Unmounting Volumes&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store13.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Volume Manager 中同样也有两个核心变量：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DesiredStateOfWorld（DSW）：集群中预期的数据卷挂载状态，包含了 volumes-&amp;gt;pods 的信息；&lt;/li&gt;
&lt;li&gt;ActualStateOfWorld（ASW）：集群中实际的数据卷挂载状态，包含了 volumes-&amp;gt;pods 的信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;全局目录（global mount path）存在的目的：块设备在 Linux 上只能挂载一次，而在 K8s 场景中，一个 PV 可能被挂载到同一个 Node 上的多个 Pod 实例中。若块设备格式化后先挂载至 Node 上的一个临时全局目录，然后再使用 Linux 中的 bind mount 技术把这个全局目录挂载进 Pod 中对应的目录上，就可以满足要求。上述流程图中，全局目录即 /var/lib/kubelet/pods/[pod uid]/volumes/kubernetes.io~iscsi/[PV name]&lt;/p&gt;

&lt;p&gt;Mounting/UnMounting 流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VolumeManager 根据集群中的资源信息，初始化 DSW 和 ASW。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;VolumeManager 内部有两个组件周期性更新 DSW 和 ASW：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DesiredStateOfWorldPopulator：通过一个 GoRoutine 周期性运行，主要功能是更新 DSW；&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Reconciler：通过一个 GoRoutine 周期性运行，确保 volume 挂载 / 卸载完毕。此期间不断更新 ASW：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;unmountVolumes：确保 Pod 删除后 volumes 被 unmount。遍历一遍所有 ASW 中的 Pod，若其不在 DSW 中（表示 Pod 被删除），此处以 VolumeMode=FileSystem 举例，则执行如下操作：

&lt;ul&gt;
&lt;li&gt;Remove all bind-mounts：调用 Unmounter 的 TearDown 接口（若为 out-of-tree 则调用 CSI 插件的 NodeUnpublishVolume 接口）；&lt;/li&gt;
&lt;li&gt;Unmount volume：调用 DeviceUnmounter 的 UnmountDevice 函数（若为 out-of-tree 则调用 CSI 插件的 NodeUnstageVolume 接口）；&lt;/li&gt;
&lt;li&gt;更新 ASW。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;mountAttachVolumes：确保 Pod 要使用的 volumes 挂载成功。遍历一遍所有 DSW 中的 Pod，若其不在 ASW 中（表示目录待挂载映射到 Pod 上），此处以 VolumeMode=FileSystem 举例，执行如下操作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;等待 volume 挂接到节点上（由 External Attacher or Kubelet 本身挂接）；&lt;/li&gt;
&lt;li&gt;挂载 volume 到全局目录：调用 DeviceMounter 的 MountDevice 函数（若为 out-of-tree 则调用 CSI 插件的 NodeStageVolume 接口）；&lt;/li&gt;
&lt;li&gt;更新 ASW：该 volume 已挂载到全局目录；&lt;/li&gt;
&lt;li&gt;bind-mount volume 到 Pod 上：调用 Mounter 的 SetUp 接口（若为 out-of-tree 则调用 CSI 插件的 NodePublishVolume 接口）；&lt;/li&gt;
&lt;li&gt;更新 ASW。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;unmountDetachDevices：确保需要 unmount 的 volumes 被 unmount。遍历一遍所有 ASW 中的 UnmountedVolumes，若其不在 DSW 中（表示 volume 已无需使用），执行如下操作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Unmount volume：调用 DeviceUnmounter 的 UnmountDevice 函数（若为 out-of-tree 则调用 CSI 插件的 NodeUnstageVolume 接口）；&lt;/li&gt;
&lt;li&gt;更新 ASW。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;在这6个流程中，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;其中mount/umount, 在kubelet中触发相应操作后由CSI plugin调用CSI driver（由厂商自己提供的CSI接口实现，类似于flexvolume的driver）做相应的mount/umount操作；&lt;/li&gt;
&lt;li&gt;provision/delete操作由外置controller通过监听API-Server，做相应的创卷、删卷操作并更新API中实体状态。&lt;/li&gt;
&lt;li&gt;当AD Controller的reconciler检测要做相应的attach或者detach操作时，是直接通过调用volume plugin来实现，volume plugin调用CSI Driver就将变成跨节点调用，而对于K8S设计来讲，Master执行完一段逻辑后触发Minion做相应的逻辑的设计太普遍，都是通过API Server内部接口更新状态的方式来设计，如前面提到的AD Controller执行完attach操作后通过node.Status.VolumesAttached通知Kubelet做mount操作。这里K8S CSI也沿用了这个设计方案。即AD Controller通过向API Server写入一个对象，在Minion侧开启一个进程监听这类对象变化，当检测了有对象Add则执行attach操作，有对象delete则执行detach操作，具体参数则通过这个内部对象属性传递给Minion。于是就引入了两个变化：

&lt;ul&gt;
&lt;li&gt;定义一个用于attach/detach的内部API对象&lt;/li&gt;
&lt;li&gt;增加一个attach/detach的Minion代理，负责监听1中定义的对象变化，再调用本节点上CSI driver执行相应的操作。 于是就有了图2中右边部分的CSI Proxy Container，剩下的就是考虑这些部件间的通信机制打通。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;provision/delete&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store15.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;CSI proxy通过监听API Server有PVC的Add/Delete操作后通过host与container的socket调用CSI接口，CSI Driver接收到调用后，调用存储设备实现卷的增删。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;attach/detach&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store16.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;AD Controller监听API Server的pod，node状态判断是否进行attach/detach操作，如果需要进行，CSI Plugin则通过API Server创建/删除attachvolume(内部API对象). CSI Proxy Container中的attacher监听到API Server中attachvolume的增删后，通过本地socket调用另一个容器中的CSI Driver执行attach/detach操作（注意，CSI接口不是这个名称），CSI Driver再通过调用存储后端完成attach/detach操作。操作完成后，CSI Proxy Container更新attachvolume状态。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;mount/unmount&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store17.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kubelet判断需要做mount操作，通过Host到container的socket调用CSI Driver，CSI Driver在容器内部通过挂载到容器里的Mount Point卷进行bind mount操作。&lt;/p&gt;

&lt;h1 id=&#34;开发csi插件&#34;&gt;开发csi插件&lt;/h1&gt;

&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;

&lt;p&gt;首先我们了解一下插件有两种类型&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Controller Plugin，负责存储对象（Volume）的生命周期管理，在集群中仅需要有一个即可；&lt;/li&gt;
&lt;li&gt;Node Plugin，在必要时与使用 Volume 的容器所在的节点交互，提供诸如节点上的 Volume 挂载/卸载等动作支持，如有需要则在每个服务节点上均部署。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上面的流程，我们知道开发一个csi driver插件，需要实现以下的rpc接口&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;身份服务：Node Plugin和Controller Plugin都必须实现这些RPC集。

&lt;ul&gt;
&lt;li&gt;GetPluginInfo， 获取 Plugin 基本信息&lt;/li&gt;
&lt;li&gt;GetPluginCapabilities，获取 Plugin 支持的能力&lt;/li&gt;
&lt;li&gt;Probe，探测 Plugin 的健康状态&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;控制器服务：Controller Plugin必须实现这些RPC集。

&lt;ul&gt;
&lt;li&gt;Volume CRUD，包括了扩容和容量探测等 Volume 状态检查与操作接口&lt;/li&gt;
&lt;li&gt;Controller Publish/Unpublish Volume ，Node 对 Volume 的访问权限管理&lt;/li&gt;
&lt;li&gt;Snapshot CRD，快照的创建和删除操作，目前 CSI 定义的 Snapshot 仅用于创建 Volume，未提供回滚的语义&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;节点服务：Node Plugin必须实现这些RPC集。

&lt;ul&gt;
&lt;li&gt;Node Stage/Unstage/Publish/Unpublish/GetStats Volume，节点上 Volume 的连接状态管理&lt;/li&gt;
&lt;li&gt;Node Expand Volume, 节点上的 Volume 扩容操作，在 volume 逻辑大小扩容之后，可能还需要同步的扩容 Volume 之上的文件系统并让使用 Volume 的 Container 感知到，所以在 Node Plugin 上需要有对应的接口&lt;/li&gt;
&lt;li&gt;Node Get Capabilities/Info， Plugin 的基础属性与 Node 的属性查询&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CSI 插件的三部分 CSI Identity , CSI Controller , CSI Node 可放在同一个二进制程序中实现。就是我们常见的插件，当然针对不同的存储，可以只实现一种类型就可以。&lt;/p&gt;

&lt;p&gt;对应的rpc定义可以看源码，简单的看一下对应的定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service Identity {
  rpc GetPluginInfo(GetPluginInfoRequest)
    returns (GetPluginInfoResponse) {}

  rpc GetPluginCapabilities(GetPluginCapabilitiesRequest)
    returns (GetPluginCapabilitiesResponse) {}

  rpc Probe (ProbeRequest)
    returns (ProbeResponse) {}
}

service Controller {
  rpc CreateVolume (CreateVolumeRequest)
    returns (CreateVolumeResponse) {}

  rpc DeleteVolume (DeleteVolumeRequest)
    returns (DeleteVolumeResponse) {}

  rpc ControllerPublishVolume (ControllerPublishVolumeRequest)
    returns (ControllerPublishVolumeResponse) {}

  rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest)
    returns (ControllerUnpublishVolumeResponse) {}

  rpc ValidateVolumeCapabilities (ValidateVolumeCapabilitiesRequest)
    returns (ValidateVolumeCapabilitiesResponse) {}

  rpc ListVolumes (ListVolumesRequest)
    returns (ListVolumesResponse) {}

  rpc GetCapacity (GetCapacityRequest)
    returns (GetCapacityResponse) {}

  rpc ControllerGetCapabilities (ControllerGetCapabilitiesRequest)
    returns (ControllerGetCapabilitiesResponse) {}

  rpc CreateSnapshot (CreateSnapshotRequest)
    returns (CreateSnapshotResponse) {}

  rpc DeleteSnapshot (DeleteSnapshotRequest)
    returns (DeleteSnapshotResponse) {}

  rpc ListSnapshots (ListSnapshotsRequest)
    returns (ListSnapshotsResponse) {}

  rpc ControllerExpandVolume (ControllerExpandVolumeRequest)
    returns (ControllerExpandVolumeResponse) {}

  rpc ControllerGetVolume (ControllerGetVolumeRequest)
    returns (ControllerGetVolumeResponse) {
        option (alpha_method) = true;
    }
}

service Node {
  rpc NodeStageVolume (NodeStageVolumeRequest)
    returns (NodeStageVolumeResponse) {}

  rpc NodeUnstageVolume (NodeUnstageVolumeRequest)
    returns (NodeUnstageVolumeResponse) {}

  rpc NodePublishVolume (NodePublishVolumeRequest)
    returns (NodePublishVolumeResponse) {}

  rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest)
    returns (NodeUnpublishVolumeResponse) {}

  rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest)
    returns (NodeGetVolumeStatsResponse) {}


  rpc NodeExpandVolume(NodeExpandVolumeRequest)
    returns (NodeExpandVolumeResponse) {}


  rpc NodeGetCapabilities (NodeGetCapabilitiesRequest)
    returns (NodeGetCapabilitiesResponse) {}

  rpc NodeGetInfo (NodeGetInfoRequest)
    returns (NodeGetInfoResponse) {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些接口都定义在&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/lib/go/csi/csi.pb.go&#34;&gt;csi的spec的lib&lt;/a&gt;中，所以我们开发的时候会引用这个库，我们只要实现这些接口就可以实现csi的基本插件功能。&lt;/p&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;csi-driver-host-path 是社区实现的一个 CSI 插件的示例，它以 hostpath 为后端存储，kubernetes 通过这个 CSI 插件 driver 来对接 hostpath ，管理本地 Node 节点上的存储卷。我们以这个为例子，来看看如何编写一个csi插件。&lt;/p&gt;

&lt;h3 id=&#34;目录结构&#34;&gt;目录结构&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;├── hostpath
│   ├── controllerserver.go
│   ├── nodeserver.go
│   ├── nodeserver_test.go
│   ├── hostpath.go
│   └── utils.go
│   └── identityserver.go
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;controllerserver.go主要是实现controller的rpc接口&lt;/li&gt;
&lt;li&gt;nodeserver.go主要是实现node的rpc接口&lt;/li&gt;
&lt;li&gt;utils.go基本公用函数&lt;/li&gt;
&lt;li&gt;hostpath.go入口，启动rpc服务&lt;/li&gt;
&lt;li&gt;identityserver.go身份验证的rpc接口&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;启动rpc服务&#34;&gt;启动rpc服务&lt;/h3&gt;

&lt;p&gt;首先肯定是定一个结构体包含plugin启动的所需信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type hostPath struct {
    driver *csicommon.CSIDriver

    ids *identityServer
    ns  *nodeServer
    cs  *controllerServer

    cap   []*csi.VolumeCapability_AccessMode
    cscap []*csi.ControllerServiceCapability
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单的介绍一下这个结构体&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;csicommon.CSIDriver：k8s自定义代表插件的结构体, 初始化的时候需要指定插件的RPC功能和支持的读写模式&lt;/li&gt;
&lt;li&gt;endpoint：插件的监听地址,一般的,我们测试的时候可以用tcp方式进行,比如&lt;code&gt;tcp://127.0.0.1:10000&lt;/code&gt;,最后在k8s中部署的时候一般使用unix方式:/csi/csi.sock&lt;/li&gt;
&lt;li&gt;csicommon.DefaultIdentityServer: 认证服务一般不需要特别实现,使用k8s公共部分的即可&lt;/li&gt;
&lt;li&gt;controllerServer: 实现CSI中的controller服务的RPC功能,继承后可以选择性覆盖部分方法&lt;/li&gt;
&lt;li&gt;nodeServer: 实现CSI中的node服务的RPC功能,继承后可以选择性覆盖部分方法&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后就是调用这个结构体的run方法，该方法中调用csicommon的公共方法启动socket监听&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (hp *hostPath) Run(driverName, nodeID, endpoint string) {
    glog.Infof(&amp;quot;Driver: %v &amp;quot;, driverName)
    glog.Infof(&amp;quot;Version: %s&amp;quot;, vendorVersion)

    // Initialize default library driver
    hp.driver = csicommon.NewCSIDriver(driverName, vendorVersion, nodeID)
    if hp.driver == nil {
        glog.Fatalln(&amp;quot;Failed to initialize CSI Driver.&amp;quot;)
    }
    hp.driver.AddControllerServiceCapabilities(
        []csi.ControllerServiceCapability_RPC_Type{
            csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME,
            csi.ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT,
            csi.ControllerServiceCapability_RPC_LIST_SNAPSHOTS,
        })
    hp.driver.AddVolumeCapabilityAccessModes([]csi.VolumeCapability_AccessMode_Mode{csi.VolumeCapability_AccessMode_SINGLE_NODE_WRITER})

    // Create GRPC servers
    hp.ids = NewIdentityServer(hp.driver)
    hp.ns = NewNodeServer(hp.driver)
    hp.cs = NewControllerServer(hp.driver)

    s := csicommon.NewNonBlockingGRPCServer()
    s.Start(endpoint, hp.ids, hp.cs, hp.ns)
    s.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;start来启动grpc服务来给对应的客户端进行调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *nonBlockingGRPCServer) serve(endpoint string, ids csi.IdentityServer, cs csi.ControllerServer, ns csi.NodeServer) {

    proto, addr, err := parseEndpoint(endpoint)
    if err != nil {
        glog.Fatal(err.Error())
    }

    if proto == &amp;quot;unix&amp;quot; {
        addr = &amp;quot;/&amp;quot; + addr
        if err := os.Remove(addr); err != nil &amp;amp;&amp;amp; !os.IsNotExist(err) { //nolint: vetshadow
            glog.Fatalf(&amp;quot;Failed to remove %s, error: %s&amp;quot;, addr, err.Error())
        }
    }

    listener, err := net.Listen(proto, addr)
    if err != nil {
        glog.Fatalf(&amp;quot;Failed to listen: %v&amp;quot;, err)
    }

    opts := []grpc.ServerOption{
        grpc.UnaryInterceptor(logGRPC),
    }
    server := grpc.NewServer(opts...)
    s.server = server

    if ids != nil {
        csi.RegisterIdentityServer(server, ids)
    }
    if cs != nil {
        csi.RegisterControllerServer(server, cs)
    }
    if ns != nil {
        csi.RegisterNodeServer(server, ns)
    }

    glog.Infof(&amp;quot;Listening for connections on address: %#v&amp;quot;, listener.Addr())

    server.Serve(listener)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;实现csi-identity&#34;&gt;实现CSI Identity&lt;/h3&gt;

&lt;p&gt;然后就是对应接口的实现，先看CSI Identity 用于认证driver的身份信息，上面提到的 kubernetes 外部组件调用，返回 CSI driver 的身份信息和健康状态。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewIdentityServer(name, version string) *identityServer {
    return &amp;amp;identityServer{
        name:    name,
        version: version,
    }
}

func (ids *identityServer) GetPluginInfo(ctx context.Context, req *csi.GetPluginInfoRequest) (*csi.GetPluginInfoResponse, error) {
    glog.V(5).Infof(&amp;quot;Using default GetPluginInfo&amp;quot;)

    if ids.name == &amp;quot;&amp;quot; {
        return nil, status.Error(codes.Unavailable, &amp;quot;Driver name not configured&amp;quot;)
    }

    if ids.version == &amp;quot;&amp;quot; {
        return nil, status.Error(codes.Unavailable, &amp;quot;Driver is missing version&amp;quot;)
    }

    return &amp;amp;csi.GetPluginInfoResponse{
        Name:          ids.name,
        VendorVersion: ids.version,
    }, nil
}

func (ids *identityServer) Probe(ctx context.Context, req *csi.ProbeRequest) (*csi.ProbeResponse, error) {
    return &amp;amp;csi.ProbeResponse{}, nil
}

func (ids *identityServer) GetPluginCapabilities(ctx context.Context, req *csi.GetPluginCapabilitiesRequest) (*csi.GetPluginCapabilitiesResponse, error) {
    glog.V(5).Infof(&amp;quot;Using default capabilities&amp;quot;)
    return &amp;amp;csi.GetPluginCapabilitiesResponse{
        Capabilities: []*csi.PluginCapability{
            {
                Type: &amp;amp;csi.PluginCapability_Service_{
                    Service: &amp;amp;csi.PluginCapability_Service{
                        Type: csi.PluginCapability_Service_CONTROLLER_SERVICE,
                    },
                },
            },
        },
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;实现csi-controller&#34;&gt;实现CSI Controller&lt;/h3&gt;

&lt;p&gt;再来看看CSI Controller 主要实现 Volume 管理流程当中的 &amp;ldquo;Provision&amp;rdquo; 和 &amp;ldquo;Attach&amp;rdquo; 阶段。&amp;rdquo;Provision&amp;rdquo; 阶段是指创建和删除 Volume 的流程，而 &amp;ldquo;Attach&amp;rdquo; 阶段是指把存储卷附着在某个 Node 或脱离某个 Node 的流程。只有块存储类型的 CSI 插件才需要 &amp;ldquo;Attach&amp;rdquo; 功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewControllerServer(ephemeral bool) *controllerServer {
    if ephemeral {
        return &amp;amp;controllerServer{caps: getControllerServiceCapabilities(nil)}
    }
    return &amp;amp;controllerServer{
        caps: getControllerServiceCapabilities(
            []csi.ControllerServiceCapability_RPC_Type{
                csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME,
                csi.ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT,
                csi.ControllerServiceCapability_RPC_LIST_SNAPSHOTS,
            }),
    }
}

func (cs *controllerServer) CreateVolume(ctx context.Context, req *csi.CreateVolumeRequest) (*csi.CreateVolumeResponse, error) {
    if err := cs.validateControllerServiceRequest(csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME); err != nil {
        glog.V(3).Infof(&amp;quot;invalid create volume req: %v&amp;quot;, req)
        return nil, err
    }

    // Check arguments
    if len(req.GetName()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Name missing in request&amp;quot;)
    }
    caps := req.GetVolumeCapabilities()
    if caps == nil {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Volume Capabilities missing in request&amp;quot;)
    }

    // Keep a record of the requested access types.
    var accessTypeMount, accessTypeBlock bool

    for _, cap := range caps {
        if cap.GetBlock() != nil {
            accessTypeBlock = true
        }
        if cap.GetMount() != nil {
            accessTypeMount = true
        }
    }
    // A real driver would also need to check that the other
    // fields in VolumeCapabilities are sane. The check above is
    // just enough to pass the &amp;quot;[Testpattern: Dynamic PV (block
    // volmode)] volumeMode should fail in binding dynamic
    // provisioned PV to PVC&amp;quot; storage E2E test.

    if accessTypeBlock &amp;amp;&amp;amp; accessTypeMount {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;cannot have both block and mount access type&amp;quot;)
    }

    var requestedAccessType accessType

    if accessTypeBlock {
        requestedAccessType = blockAccess
    } else {
        // Default to mount.
        requestedAccessType = mountAccess
    }

    // Check for maximum available capacity
    capacity := int64(req.GetCapacityRange().GetRequiredBytes())
    if capacity &amp;gt;= maxStorageCapacity {
        return nil, status.Errorf(codes.OutOfRange, &amp;quot;Requested capacity %d exceeds maximum allowed %d&amp;quot;, capacity, maxStorageCapacity)
    }

    // Need to check for already existing volume name, and if found
    // check for the requested capacity and already allocated capacity
    if exVol, err := getVolumeByName(req.GetName()); err == nil {
        // Since err is nil, it means the volume with the same name already exists
        // need to check if the size of exisiting volume is the same as in new
        // request
        if exVol.VolSize &amp;gt;= int64(req.GetCapacityRange().GetRequiredBytes()) {
            // exisiting volume is compatible with new request and should be reused.
            // TODO (sbezverk) Do I need to make sure that RBD volume still exists?
            return &amp;amp;csi.CreateVolumeResponse{
                Volume: &amp;amp;csi.Volume{
                    VolumeId:      exVol.VolID,
                    CapacityBytes: int64(exVol.VolSize),
                    VolumeContext: req.GetParameters(),
                },
            }, nil
        }
        return nil, status.Error(codes.AlreadyExists, fmt.Sprintf(&amp;quot;Volume with the same name: %s but with different size already exist&amp;quot;, req.GetName()))
    }

    volumeID := uuid.NewUUID().String()
    path := getVolumePath(volumeID)

    if requestedAccessType == blockAccess {
        executor := utilexec.New()
        size := fmt.Sprintf(&amp;quot;%dM&amp;quot;, capacity/mib)
        // Create a block file.
        out, err := executor.Command(&amp;quot;fallocate&amp;quot;, &amp;quot;-l&amp;quot;, size, path).CombinedOutput()
        if err != nil {
            glog.V(3).Infof(&amp;quot;failed to create block device: %v&amp;quot;, string(out))
            return nil, err
        }

        // Associate block file with the loop device.
        volPathHandler := volumepathhandler.VolumePathHandler{}
        _, err = volPathHandler.AttachFileDevice(path)
        if err != nil {
            glog.Errorf(&amp;quot;failed to attach device: %v&amp;quot;, err)
            // Remove the block file because it&#39;ll no longer be used again.
            if err2 := os.Remove(path); err != nil {
                glog.Errorf(&amp;quot;failed to cleanup block file %s: %v&amp;quot;, path, err2)
            }
            return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed to attach device: %v&amp;quot;, err))
        }
    }

    vol, err := createHostpathVolume(volumeID, req.GetName(), capacity, requestedAccessType)
    if err != nil {
        return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed to create volume: %s&amp;quot;, err))
    }
    glog.V(4).Infof(&amp;quot;created volume %s at path %s&amp;quot;, vol.VolID, vol.VolPath)

    if req.GetVolumeContentSource() != nil {
        contentSource := req.GetVolumeContentSource()
        if contentSource.GetSnapshot() != nil {
            snapshotId := contentSource.GetSnapshot().GetSnapshotId()
            snapshot, ok := hostPathVolumeSnapshots[snapshotId]
            if !ok {
                return nil, status.Errorf(codes.NotFound, &amp;quot;cannot find snapshot %v&amp;quot;, snapshotId)
            }
            if snapshot.ReadyToUse != true {
                return nil, status.Errorf(codes.Internal, &amp;quot;Snapshot %v is not yet ready to use.&amp;quot;, snapshotId)
            }
            snapshotPath := snapshot.Path
            args := []string{&amp;quot;zxvf&amp;quot;, snapshotPath, &amp;quot;-C&amp;quot;, path}
            executor := utilexec.New()
            out, err := executor.Command(&amp;quot;tar&amp;quot;, args...).CombinedOutput()
            if err != nil {
                return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed pre-populate data for volume: %v: %s&amp;quot;, err, out))
            }
        }
    }

    return &amp;amp;csi.CreateVolumeResponse{
        Volume: &amp;amp;csi.Volume{
            VolumeId:      volumeID,
            CapacityBytes: req.GetCapacityRange().GetRequiredBytes(),
            VolumeContext: req.GetParameters(),
        },
    }, nil
}

func (cs *controllerServer) DeleteVolume(ctx context.Context, req *csi.DeleteVolumeRequest) (*csi.DeleteVolumeResponse, error) {

    // Check arguments
    if len(req.GetVolumeId()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Volume ID missing in request&amp;quot;)
    }

    if err := cs.validateControllerServiceRequest(csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME); err != nil {
        glog.V(3).Infof(&amp;quot;invalid delete volume req: %v&amp;quot;, req)
        return nil, err
    }

    vol, err := getVolumeByID(req.GetVolumeId())
    if err != nil {
        // Return OK if the volume is not found.
        return &amp;amp;csi.DeleteVolumeResponse{}, nil
    }
    glog.V(4).Infof(&amp;quot;deleting volume %s&amp;quot;, vol.VolID)

    if vol.VolAccessType == blockAccess {

        volPathHandler := volumepathhandler.VolumePathHandler{}
        // Get the associated loop device.
        device, err := volPathHandler.GetLoopDevice(getVolumePath(vol.VolID))
        if err != nil {
            return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed to get the loop device: %v&amp;quot;, err))
        }

        if device != &amp;quot;&amp;quot; {
            // Remove any associated loop device.
            glog.V(4).Infof(&amp;quot;deleting loop device %s&amp;quot;, device)
            if err := volPathHandler.RemoveLoopDevice(device); err != nil {
                return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed to remove loop device: %v&amp;quot;, err))
            }
        }
    }

    if err := deleteHostpathVolume(vol.VolID); err != nil &amp;amp;&amp;amp; !os.IsNotExist(err) {
        return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed to delete volume: %s&amp;quot;, err))
    }

    glog.V(4).Infof(&amp;quot;volume deleted ok: %s&amp;quot;, vol.VolID)

    return &amp;amp;csi.DeleteVolumeResponse{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中，CreateVolume 和 DeleteVolume 是实现 &amp;ldquo;Provision&amp;rdquo; 阶段需要实现的接口，External provisioner 组件会 CSI 插件的这个接口以创建或者删除存储卷。ControllerPublishVolume 和 ControllerUnpublishVolume 是实现 &amp;ldquo;Attach&amp;rdquo; 阶段需要实现的接口，External attach 组件会调用 CSI 插件实现的这个接口以把某个块存储卷附着或脱离某个 Node 。
如果想扩展 CSI 的功能，可以实现更多功能的接口，如快照功能的接口 CreateSnapshot 和 DeleteSnapshot。&lt;/p&gt;

&lt;h3 id=&#34;实现csi-node&#34;&gt;实现CSI Node&lt;/h3&gt;

&lt;p&gt;最后再来看看CSI Node 部分主要负责 Volume 管理流程当中的 &amp;ldquo;Mount&amp;rdquo; 阶段，即把 Volume 挂载至 Pod 容器，或者从 Pod 中卸载 Volume 。在宿主机 Node 上需要执行的操作都包含在这个部分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewNodeServer(nodeId string, ephemeral bool) *nodeServer {
    return &amp;amp;nodeServer{
        nodeID:    nodeId,
        ephemeral: ephemeral,
    }
}

func (ns *nodeServer) NodePublishVolume(ctx context.Context, req *csi.NodePublishVolumeRequest) (*csi.NodePublishVolumeResponse, error) {

    // Check arguments
    if req.GetVolumeCapability() == nil {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Volume capability missing in request&amp;quot;)
    }
    if len(req.GetVolumeId()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Volume ID missing in request&amp;quot;)
    }
    if len(req.GetTargetPath()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Target path missing in request&amp;quot;)
    }

    targetPath := req.GetTargetPath()

    if req.GetVolumeCapability().GetBlock() != nil &amp;amp;&amp;amp;
        req.GetVolumeCapability().GetMount() != nil {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;cannot have both block and mount access type&amp;quot;)
    }

    // if ephemeral is specified, create volume here to avoid errors
    if ns.ephemeral {
        volID := req.GetVolumeId()
        volName := fmt.Sprintf(&amp;quot;ephemeral-%s&amp;quot;, volID)
        vol, err := createHostpathVolume(req.GetVolumeId(), volName, maxStorageCapacity, mountAccess)
        if err != nil &amp;amp;&amp;amp; !os.IsExist(err) {
            glog.Error(&amp;quot;ephemeral mode failed to create volume: &amp;quot;, err)
            return nil, status.Error(codes.Internal, err.Error())
        }
        glog.V(4).Infof(&amp;quot;ephemeral mode: created volume: %s&amp;quot;, vol.VolPath)
    }

    vol, err := getVolumeByID(req.GetVolumeId())
    if err != nil {
        return nil, status.Error(codes.NotFound, err.Error())
    }

    if req.GetVolumeCapability().GetBlock() != nil {
        if vol.VolAccessType != blockAccess {
            return nil, status.Error(codes.InvalidArgument, &amp;quot;cannot publish a non-block volume as block volume&amp;quot;)
        }

        volPathHandler := volumepathhandler.VolumePathHandler{}

        // Get loop device from the volume path.
        loopDevice, err := volPathHandler.GetLoopDevice(vol.VolPath)
        if err != nil {
            return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed to get the loop device: %v&amp;quot;, err))
        }

        mounter := mount.New(&amp;quot;&amp;quot;)

        // Check if the target path exists. Create if not present.
        _, err = os.Lstat(targetPath)
        if os.IsNotExist(err) {
            if err = mounter.MakeFile(targetPath); err != nil {
                return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed to create target path: %s: %v&amp;quot;, targetPath, err))
            }
        }
        if err != nil {
            return nil, status.Errorf(codes.Internal, &amp;quot;failed to check if the target block file exists: %v&amp;quot;, err)
        }

        // Check if the target path is already mounted. Prevent remounting.
        notMount, err := mounter.IsNotMountPoint(targetPath)
        if err != nil {
            if !os.IsNotExist(err) {
                return nil, status.Errorf(codes.Internal, &amp;quot;error checking path %s for mount: %s&amp;quot;, targetPath, err)
            }
            notMount = true
        }
        if !notMount {
            // It&#39;s already mounted.
            glog.V(5).Infof(&amp;quot;Skipping bind-mounting subpath %s: already mounted&amp;quot;, targetPath)
            return &amp;amp;csi.NodePublishVolumeResponse{}, nil
        }

        options := []string{&amp;quot;bind&amp;quot;}
        if err := mount.New(&amp;quot;&amp;quot;).Mount(loopDevice, targetPath, &amp;quot;&amp;quot;, options); err != nil {
            return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed to mount block device: %s at %s: %v&amp;quot;, loopDevice, targetPath, err))
        }
    } else if req.GetVolumeCapability().GetMount() != nil {
        if vol.VolAccessType != mountAccess {
            return nil, status.Error(codes.InvalidArgument, &amp;quot;cannot publish a non-mount volume as mount volume&amp;quot;)
        }

        notMnt, err := mount.New(&amp;quot;&amp;quot;).IsLikelyNotMountPoint(targetPath)
        if err != nil {
            if os.IsNotExist(err) {
                if err = os.MkdirAll(targetPath, 0750); err != nil {
                    return nil, status.Error(codes.Internal, err.Error())
                }
                notMnt = true
            } else {
                return nil, status.Error(codes.Internal, err.Error())
            }
        }

        if !notMnt {
            return &amp;amp;csi.NodePublishVolumeResponse{}, nil
        }

        fsType := req.GetVolumeCapability().GetMount().GetFsType()

        deviceId := &amp;quot;&amp;quot;
        if req.GetPublishContext() != nil {
            deviceId = req.GetPublishContext()[deviceID]
        }

        readOnly := req.GetReadonly()
        volumeId := req.GetVolumeId()
        attrib := req.GetVolumeContext()
        mountFlags := req.GetVolumeCapability().GetMount().GetMountFlags()

        glog.V(4).Infof(&amp;quot;target %v\nfstype %v\ndevice %v\nreadonly %v\nvolumeId %v\nattributes %v\nmountflags %v\n&amp;quot;,
            targetPath, fsType, deviceId, readOnly, volumeId, attrib, mountFlags)

        options := []string{&amp;quot;bind&amp;quot;}
        if readOnly {
            options = append(options, &amp;quot;ro&amp;quot;)
        }
        mounter := mount.New(&amp;quot;&amp;quot;)
        path := getVolumePath(volumeId)

        if err := mounter.Mount(path, targetPath, &amp;quot;&amp;quot;, options); err != nil {
            var errList strings.Builder
            errList.WriteString(err.Error())
            if ns.ephemeral {
                if rmErr := os.RemoveAll(path); rmErr != nil &amp;amp;&amp;amp; !os.IsNotExist(rmErr) {
                    errList.WriteString(fmt.Sprintf(&amp;quot; :%s&amp;quot;, rmErr.Error()))
                }
            }
        }
    }

    return &amp;amp;csi.NodePublishVolumeResponse{}, nil
}

func (ns *nodeServer) NodeUnpublishVolume(ctx context.Context, req *csi.NodeUnpublishVolumeRequest) (*csi.NodeUnpublishVolumeResponse, error) {

    // Check arguments
    if len(req.GetVolumeId()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Volume ID missing in request&amp;quot;)
    }
    if len(req.GetTargetPath()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Target path missing in request&amp;quot;)
    }
    targetPath := req.GetTargetPath()
    volumeID := req.GetVolumeId()

    vol, err := getVolumeByID(volumeID)
    if err != nil {
        return nil, status.Error(codes.NotFound, err.Error())
    }

    switch vol.VolAccessType {
    case blockAccess:
        // Unmount and delete the block file.
        err = mount.New(&amp;quot;&amp;quot;).Unmount(targetPath)
        if err != nil {
            return nil, status.Error(codes.Internal, err.Error())
        }
        if err = os.RemoveAll(targetPath); err != nil {
            return nil, status.Error(codes.Internal, err.Error())
        }
        glog.V(4).Infof(&amp;quot;hostpath: volume %s has been unpublished.&amp;quot;, targetPath)
    case mountAccess:
        // Unmounting the image
        err = mount.New(&amp;quot;&amp;quot;).Unmount(req.GetTargetPath())
        if err != nil {
            return nil, status.Error(codes.Internal, err.Error())
        }
        glog.V(4).Infof(&amp;quot;hostpath: volume %s/%s has been unmounted.&amp;quot;, targetPath, volumeID)
    }

    if ns.ephemeral {
        glog.V(4).Infof(&amp;quot;deleting volume %s&amp;quot;, volumeID)
        if err := deleteHostpathVolume(volumeID); err != nil &amp;amp;&amp;amp; !os.IsNotExist(err) {
            return nil, status.Error(codes.Internal, fmt.Sprintf(&amp;quot;failed to delete volume: %s&amp;quot;, err))
        }
    }

    return &amp;amp;csi.NodeUnpublishVolumeResponse{}, nil
}

func (ns *nodeServer) NodeStageVolume(ctx context.Context, req *csi.NodeStageVolumeRequest) (*csi.NodeStageVolumeResponse, error) {

    // Check arguments
    if len(req.GetVolumeId()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Volume ID missing in request&amp;quot;)
    }
    if len(req.GetStagingTargetPath()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Target path missing in request&amp;quot;)
    }
    if req.GetVolumeCapability() == nil {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Volume Capability missing in request&amp;quot;)
    }

    return &amp;amp;csi.NodeStageVolumeResponse{}, nil
}

func (ns *nodeServer) NodeUnstageVolume(ctx context.Context, req *csi.NodeUnstageVolumeRequest) (*csi.NodeUnstageVolumeResponse, error) {

    // Check arguments
    if len(req.GetVolumeId()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Volume ID missing in request&amp;quot;)
    }
    if len(req.GetStagingTargetPath()) == 0 {
        return nil, status.Error(codes.InvalidArgument, &amp;quot;Target path missing in request&amp;quot;)
    }

    return &amp;amp;csi.NodeUnstageVolumeResponse{}, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubelet 会调用 CSI 插件实现的接口，以实现 volume 的挂载和卸载。&lt;/p&gt;

&lt;p&gt;其中 Volume 的挂载被分成了 NodeStageVolume 和 NodePublishVolume 两个阶段。NodeStageVolume 接口主要是针对块存储类型的 CSI 插件而提供的。 块设备在 &amp;ldquo;Attach&amp;rdquo; 阶段被附着在 Node 上后，需要挂载至 Pod 对应目录上，但因为块设备在 linux 上只能 mount 一次，而在 kubernetes volume 的使用场景中，一个 volume 可能被挂载进同一个 Node 上的多个 Pod 实例中，所以这里提供了 NodeStageVolume 这个接口，使用这个接口把块设备格式化后先挂载至 Node 上的一个临时全局目录，然后再调用 NodePublishVolume 使用 linux 中的 bind mount 技术把这个全局目录挂载进 Pod 中对应的目录上。&lt;/p&gt;

&lt;p&gt;NodeUnstageVolume 和 NodeUnpublishVolume 正是 volume 卸载阶段所分别对应的上述两个流程。从上述代码中可以看到，因为 hostpath 非块存储类型的第三方存储，所以没有实现 NodeStageVolume 和 NodeUnstageVolume 这两个接口。&lt;/p&gt;

&lt;p&gt;当然，如果是非块存储类型的 CSI 插件，也就不必实现 NodeStageVolume 和 NodeUnstageVolume 这两个接口了。&lt;/p&gt;

&lt;p&gt;到这里一个完整的csi插件就开发完成，大体都是这些流程，只要实现对应的rpc接口逻辑就好。&lt;/p&gt;

&lt;h1 id=&#34;源码分析&#34;&gt;源码分析&lt;/h1&gt;

&lt;h2 id=&#34;pv-controller&#34;&gt;pv controller&lt;/h2&gt;

&lt;p&gt;根据存储流程，首先调度后，pv控制器来处理pv和pvc的关系，pv控制器在组件kube-controller-manager中，我们先来看看pv控制器，首先看到pv控制器在kube-controller-manager的注册。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;controllers[&amp;quot;persistentvolume-binder&amp;quot;] = startPersistentVolumeBinderController
controllers[&amp;quot;persistentvolume-expander&amp;quot;] = startVolumeExpandController
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分别支持原生的存储和扩展的存储，我们来看对待的初始化函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func startPersistentVolumeBinderController(ctx ControllerContext) (http.Handler, bool, error) {
    plugins, err := ProbeControllerVolumePlugins(ctx.Cloud, ctx.ComponentConfig.PersistentVolumeBinderController.VolumeConfiguration)
    if err != nil {
        return nil, true, fmt.Errorf(&amp;quot;failed to probe volume plugins when starting persistentvolume controller: %v&amp;quot;, err)
    }
    filteredDialOptions, err := options.ParseVolumeHostFilters(
        ctx.ComponentConfig.PersistentVolumeBinderController.VolumeHostCIDRDenylist,
        ctx.ComponentConfig.PersistentVolumeBinderController.VolumeHostAllowLocalLoopback)
    if err != nil {
        return nil, true, err
    }
    params := persistentvolumecontroller.ControllerParameters{
        KubeClient:                ctx.ClientBuilder.ClientOrDie(&amp;quot;persistent-volume-binder&amp;quot;),
        SyncPeriod:                ctx.ComponentConfig.PersistentVolumeBinderController.PVClaimBinderSyncPeriod.Duration,
        VolumePlugins:             plugins,
        Cloud:                     ctx.Cloud,
        ClusterName:               ctx.ComponentConfig.KubeCloudShared.ClusterName,
        VolumeInformer:            ctx.InformerFactory.Core().V1().PersistentVolumes(),
        ClaimInformer:             ctx.InformerFactory.Core().V1().PersistentVolumeClaims(),
        ClassInformer:             ctx.InformerFactory.Storage().V1().StorageClasses(),
        PodInformer:               ctx.InformerFactory.Core().V1().Pods(),
        NodeInformer:              ctx.InformerFactory.Core().V1().Nodes(),
        EnableDynamicProvisioning: ctx.ComponentConfig.PersistentVolumeBinderController.VolumeConfiguration.EnableDynamicProvisioning,
        FilteredDialOptions:       filteredDialOptions,
    }
    volumeController, volumeControllerErr := persistentvolumecontroller.NewController(params)
    if volumeControllerErr != nil {
        return nil, true, fmt.Errorf(&amp;quot;failed to construct persistentvolume controller: %v&amp;quot;, volumeControllerErr)
    }
    go volumeController.Run(ctx.Stop)
    return nil, true, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建各种params最后创建结构体PersistentVolumeController&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewController creates a new PersistentVolume controller
func NewController(p ControllerParameters) (*PersistentVolumeController, error) {
    eventRecorder := p.EventRecorder
    if eventRecorder == nil {
        broadcaster := record.NewBroadcaster()
        broadcaster.StartStructuredLogging(0)
        broadcaster.StartRecordingToSink(&amp;amp;v1core.EventSinkImpl{Interface: p.KubeClient.CoreV1().Events(&amp;quot;&amp;quot;)})
        eventRecorder = broadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: &amp;quot;persistentvolume-controller&amp;quot;})
    }

    controller := &amp;amp;PersistentVolumeController{
        volumes:                       newPersistentVolumeOrderedIndex(),
        claims:                        cache.NewStore(cache.DeletionHandlingMetaNamespaceKeyFunc),
        kubeClient:                    p.KubeClient,
        eventRecorder:                 eventRecorder,
        runningOperations:             goroutinemap.NewGoRoutineMap(true /* exponentialBackOffOnError */),
        cloud:                         p.Cloud,
        enableDynamicProvisioning:     p.EnableDynamicProvisioning,
        clusterName:                   p.ClusterName,
        createProvisionedPVRetryCount: createProvisionedPVRetryCount,
        createProvisionedPVInterval:   createProvisionedPVInterval,
        claimQueue:                    workqueue.NewNamed(&amp;quot;claims&amp;quot;),
        volumeQueue:                   workqueue.NewNamed(&amp;quot;volumes&amp;quot;),
        resyncPeriod:                  p.SyncPeriod,
        operationTimestamps:           metrics.NewOperationStartTimeCache(),
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后初始化volumes的插件，包括hostpath nfs csi等等&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Prober is nil because PV is not aware of Flexvolume.
if err := controller.volumePluginMgr.InitPlugins(p.VolumePlugins, nil /* prober */, controller); err != nil {
    return nil, fmt.Errorf(&amp;quot;Could not initialize volume plugins for PersistentVolume Controller: %v&amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后添加volume informer机制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.VolumeInformer.Informer().AddEventHandler(
    cache.ResourceEventHandlerFuncs{
        AddFunc:    func(obj interface{}) { controller.enqueueWork(controller.volumeQueue, obj) },
        UpdateFunc: func(oldObj, newObj interface{}) { controller.enqueueWork(controller.volumeQueue, newObj) },
        DeleteFunc: func(obj interface{}) { controller.enqueueWork(controller.volumeQueue, obj) },
    },
)
controller.volumeLister = p.VolumeInformer.Lister()
controller.volumeListerSynced = p.VolumeInformer.Informer().HasSynced
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后添加claim informer机制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.ClaimInformer.Informer().AddEventHandler(
    cache.ResourceEventHandlerFuncs{
        AddFunc:    func(obj interface{}) { controller.enqueueWork(controller.claimQueue, obj) },
        UpdateFunc: func(oldObj, newObj interface{}) { controller.enqueueWork(controller.claimQueue, newObj) },
        DeleteFunc: func(obj interface{}) { controller.enqueueWork(controller.claimQueue, obj) },
    },
)
controller.claimLister = p.ClaimInformer.Lister()
controller.claimListerSynced = p.ClaimInformer.Informer().HasSynced
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后添加 storageclas pod node资源 informer机制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;controller.classLister = p.ClassInformer.Lister()
controller.classListerSynced = p.ClassInformer.Informer().HasSynced
controller.podLister = p.PodInformer.Lister()
controller.podIndexer = p.PodInformer.Informer().GetIndexer()
controller.podListerSynced = p.PodInformer.Informer().HasSynced
controller.NodeLister = p.NodeInformer.Lister()
controller.NodeListerSynced = p.NodeInformer.Informer().HasSynced
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此NewController调用就结束了，下面调用这个PersistentVolumeController的run函数运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Run starts all of this controller&#39;s control loops
func (ctrl *PersistentVolumeController) Run(stopCh &amp;lt;-chan struct{}) {
    defer utilruntime.HandleCrash()
    defer ctrl.claimQueue.ShutDown()
    defer ctrl.volumeQueue.ShutDown()

    klog.Infof(&amp;quot;Starting persistent volume controller&amp;quot;)
    defer klog.Infof(&amp;quot;Shutting down persistent volume controller&amp;quot;)

    if !cache.WaitForNamedCacheSync(&amp;quot;persistent volume&amp;quot;, stopCh, ctrl.volumeListerSynced, ctrl.claimListerSynced, ctrl.classListerSynced, ctrl.podListerSynced, ctrl.NodeListerSynced) {
        return
    }

    ctrl.initializeCaches(ctrl.volumeLister, ctrl.claimLister)

    go wait.Until(ctrl.resync, ctrl.resyncPeriod, stopCh)
    go wait.Until(ctrl.volumeWorker, time.Second, stopCh)
    go wait.Until(ctrl.claimWorker, time.Second, stopCh)

    metrics.Register(ctrl.volumes.store, ctrl.claims, &amp;amp;ctrl.volumePluginMgr)

    &amp;lt;-stopCh
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开启了三个goroutine来定期执行对应的函数&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;resysc 定期list pv pvc并加入到队列&lt;/li&gt;
&lt;li&gt;volumeManager 这个我们在之前说过，主要是管理pv的状态迁移&lt;/li&gt;
&lt;li&gt;claimWorker 这个我们在之前说过，主要处理 PVC 的 add / update / delete 相关事件以及 PVC 的状态迁移&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来看看对应的函数，先看resysc&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// resync supplements short resync period of shared informers - we don&#39;t want
// all consumers of PV/PVC shared informer to have a short resync period,
// therefore we do our own.
func (ctrl *PersistentVolumeController) resync() {
    klog.V(4).Infof(&amp;quot;resyncing PV controller&amp;quot;)

    pvcs, err := ctrl.claimLister.List(labels.NewSelector())
    if err != nil {
        klog.Warningf(&amp;quot;cannot list claims: %s&amp;quot;, err)
        return
    }
    for _, pvc := range pvcs {
        ctrl.enqueueWork(ctrl.claimQueue, pvc)
    }

    pvs, err := ctrl.volumeLister.List(labels.NewSelector())
    if err != nil {
        klog.Warningf(&amp;quot;cannot list persistent volumes: %s&amp;quot;, err)
        return
    }
    for _, pv := range pvs {
        ctrl.enqueueWork(ctrl.volumeQueue, pv)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见就是获取pvcs，pvs，最后放到对应的队列中处理。我们再来看看volumeManager&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// volumeWorker processes items from volumeQueue. It must run only once,
// syncVolume is not assured to be reentrant.
func (ctrl *PersistentVolumeController) volumeWorker() {
    workFunc := func() bool {
        keyObj, quit := ctrl.volumeQueue.Get()
        if quit {
            return true
        }
        defer ctrl.volumeQueue.Done(keyObj)
        key := keyObj.(string)
        klog.V(5).Infof(&amp;quot;volumeWorker[%s]&amp;quot;, key)

        _, name, err := cache.SplitMetaNamespaceKey(key)
        if err != nil {
            klog.V(4).Infof(&amp;quot;error getting name of volume %q to get volume from informer: %v&amp;quot;, key, err)
            return false
        }
        volume, err := ctrl.volumeLister.Get(name)
        if err == nil {
            // The volume still exists in informer cache, the event must have
            // been add/update/sync
            ctrl.updateVolume(volume)
            return false
        }
        if !errors.IsNotFound(err) {
            klog.V(2).Infof(&amp;quot;error getting volume %q from informer: %v&amp;quot;, key, err)
            return false
        }

        // The volume is not in informer cache, the event must have been
        // &amp;quot;delete&amp;quot;
        volumeObj, found, err := ctrl.volumes.store.GetByKey(key)
        if err != nil {
            klog.V(2).Infof(&amp;quot;error getting volume %q from cache: %v&amp;quot;, key, err)
            return false
        }
        if !found {
            // The controller has already processed the delete event and
            // deleted the volume from its cache
            klog.V(2).Infof(&amp;quot;deletion of volume %q was already processed&amp;quot;, key)
            return false
        }
        volume, ok := volumeObj.(*v1.PersistentVolume)
        if !ok {
            klog.Errorf(&amp;quot;expected volume, got %+v&amp;quot;, volumeObj)
            return false
        }
        ctrl.deleteVolume(volume)
        return false
    }
    for {
        if quit := workFunc(); quit {
            klog.Infof(&amp;quot;volume worker queue shutting down&amp;quot;)
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从队列取进行处理，未有就退出等待下一个周期在处理，如果有则 updateVolume 更新操作，可能包括 add update sync 等操作，处理并删除队列，具体处理的逻辑在上面已经说过，我们简单看一下&lt;/p&gt;

&lt;h2 id=&#34;extrenal-provisioner&#34;&gt;extrenal provisioner&lt;/h2&gt;

&lt;h2 id=&#34;csi插件&#34;&gt;csi插件&lt;/h2&gt;

&lt;h2 id=&#34;ad-controller&#34;&gt;AD controller&lt;/h2&gt;

&lt;p&gt;AD Controller中核心部件包括两个缓存，一个populator，一个reconciler，一个status updater。&lt;/p&gt;

&lt;p&gt;两个缓存分别是desired status和actual status，分别代表跟attach/detach相关对象模型的期望状态和实际状态。
reconciler通过定期比较期望状态和实际状态来决定是执行attach，还是detach，还是什么都不做。
populator负责定期从API Server同步相关模型值到期望状态缓存中。
status updater负责node的状态刷新，这里主要是当有卷做完attach/detach操作后，从记录实际状态缓存中获取每个node已经attach的卷信息，向API Server同步node.Status.VolumesAttached，通过这个状态Kubelet才知道AD Controller是否已经完成attach操作。详细代码查看这里，结构见下图：&lt;/p&gt;

&lt;p&gt;核心的逻辑部件有一定了解之后，再梳理下它的逻辑。对于AD Controller来讲，它的核心职责就是当API Server中，有卷声明的pod与node间的关系发生变化时，需要决定是通过调用存储插件将这个pod关联的卷attach到对应node的主机（或者虚拟机）上，还是将卷从node上detach掉，这里的关系变化可能是pod调度某个node上，也可能是某个pod在node上终止掉了，所以它需要做这几件事情：&lt;/p&gt;

&lt;p&gt;监控API Server 中的pod和node。
监控当前各个node上卷的实际attach状态。
通过对比API Server中pod和node的状态变化以及node上的实际attach状态来决定做attach、detach操作。
调用相应的卷插件执行attach，detach操作。
操作完成后，通知其它关联组件（这里主要是kubelet）做相应的业务操作。&lt;/p&gt;

&lt;h2 id=&#34;external-attacher&#34;&gt;external-attacher&lt;/h2&gt;

&lt;h2 id=&#34;kubelet&#34;&gt;kubelet&lt;/h2&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s组件系列（三）---- K8s controller manager 详解</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller-manager/</link>
          <pubDate>Tue, 23 Jun 2020 20:22:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller-manager/</guid>
          <description>&lt;p&gt;Kubernetes 项目的核心原理，就是“控制器模式”。目前，Kubernetes 项目默认已经提供了一套 Controller 组件，例如 Deployment, Statefulset, DaemonSet 等，这些 Controller 提供了比较丰富的应用部署和管理功能。&lt;/p&gt;

&lt;h1 id=&#34;kube-controller-manager&#34;&gt;kube-controller-manager&lt;/h1&gt;

&lt;p&gt;controller-manager是管理器的控制者，使用是集群管理控制中心，内部对应控制器如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/controller&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;他们通过API Server提供的接口实时监控整个集群里的每一个资源对象的当前状态，当发生各种故障导致系统状态发生变化，这些controller会尝试将系统从“现有装态”修正到“期望状态”。&lt;/p&gt;

&lt;p&gt;在controller manager启动的时候会遍历所有的controller，执行初始化函数，那么一共支持多少controller？我们可以重代码注册中看到，代码位置：cmd/kube-controller-manager/app/coontrollermanager.go：387&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc {
    controllers := map[string]InitFunc{}
    controllers[&amp;quot;endpoint&amp;quot;] = startEndpointController
    controllers[&amp;quot;endpointslice&amp;quot;] = startEndpointSliceController
    controllers[&amp;quot;endpointslicemirroring&amp;quot;] = startEndpointSliceMirroringController
    controllers[&amp;quot;replicationcontroller&amp;quot;] = startReplicationController
    controllers[&amp;quot;podgc&amp;quot;] = startPodGCController
    controllers[&amp;quot;resourcequota&amp;quot;] = startResourceQuotaController
    controllers[&amp;quot;namespace&amp;quot;] = startNamespaceController
    controllers[&amp;quot;serviceaccount&amp;quot;] = startServiceAccountController
    controllers[&amp;quot;garbagecollector&amp;quot;] = startGarbageCollectorController
    controllers[&amp;quot;daemonset&amp;quot;] = startDaemonSetController
    controllers[&amp;quot;job&amp;quot;] = startJobController
    controllers[&amp;quot;deployment&amp;quot;] = startDeploymentController
    controllers[&amp;quot;replicaset&amp;quot;] = startReplicaSetController
    controllers[&amp;quot;horizontalpodautoscaling&amp;quot;] = startHPAController
    controllers[&amp;quot;disruption&amp;quot;] = startDisruptionController
    controllers[&amp;quot;statefulset&amp;quot;] = startStatefulSetController
    controllers[&amp;quot;cronjob&amp;quot;] = startCronJobController
    controllers[&amp;quot;csrsigning&amp;quot;] = startCSRSigningController
    controllers[&amp;quot;csrapproving&amp;quot;] = startCSRApprovingController
    controllers[&amp;quot;csrcleaner&amp;quot;] = startCSRCleanerController
    controllers[&amp;quot;ttl&amp;quot;] = startTTLController
    controllers[&amp;quot;bootstrapsigner&amp;quot;] = startBootstrapSignerController
    controllers[&amp;quot;tokencleaner&amp;quot;] = startTokenCleanerController
    controllers[&amp;quot;nodeipam&amp;quot;] = startNodeIpamController
    controllers[&amp;quot;nodelifecycle&amp;quot;] = startNodeLifecycleController
    if loopMode == IncludeCloudLoops {
        controllers[&amp;quot;service&amp;quot;] = startServiceController
        controllers[&amp;quot;route&amp;quot;] = startRouteController
        controllers[&amp;quot;cloud-node-lifecycle&amp;quot;] = startCloudNodeLifecycleController
        // TODO: volume controller into the IncludeCloudLoops only set.
    }
    controllers[&amp;quot;persistentvolume-binder&amp;quot;] = startPersistentVolumeBinderController
    controllers[&amp;quot;attachdetach&amp;quot;] = startAttachDetachController
    controllers[&amp;quot;persistentvolume-expander&amp;quot;] = startVolumeExpandController
    controllers[&amp;quot;clusterrole-aggregation&amp;quot;] = startClusterRoleAggregrationController
    controllers[&amp;quot;pvc-protection&amp;quot;] = startPVCProtectionController
    controllers[&amp;quot;pv-protection&amp;quot;] = startPVProtectionController
    controllers[&amp;quot;ttl-after-finished&amp;quot;] = startTTLAfterFinishedController
    controllers[&amp;quot;root-ca-cert-publisher&amp;quot;] = startRootCACertPublisher
    controllers[&amp;quot;ephemeral-volume&amp;quot;] = startEphemeralVolumeController
    if utilfeature.DefaultFeatureGate.Enabled(genericfeatures.APIServerIdentity) &amp;amp;&amp;amp;
        utilfeature.DefaultFeatureGate.Enabled(genericfeatures.StorageVersionAPI) {
        controllers[&amp;quot;storage-version-gc&amp;quot;] = startStorageVersionGCController
    }

    return controllers
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见支持很多的controller，我们来看看我们常见的控制器。&lt;/p&gt;

&lt;h2 id=&#34;replication-controller&#34;&gt;replication controller&lt;/h2&gt;

&lt;p&gt;replication controller副本控制 ,副本控制器的核心作用是确保任何使用集群中的一个RC所关联的Pod副本数量保持预设的值。当然他是通过rc机制实现的。&lt;/p&gt;

&lt;p&gt;RC中的Pod模板就像一个模具，模具制作出来的东西一旦离开模具，二者将毫无关系，一旦pod创建，无论模板如何变化都不会影响到已经创建的pod，并且删除一个RC 不会影响它所创建出来的Pod，当然如果想在RC控制下，删除所有的Pod，需要将RC中设置的pod的副本数该为0，这样才会自动删除所有的Pod。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 重新调度 就是上面说的能确保规定数量的pod运行

2. 弹性伸缩 可以通过spec.replicas来改变pod的数量

3. 滚动更新 新建一个rc为一个副本，然后减少原来的副本数量一个，一个增加，一个减少，直到原始的为0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;node-controller节点管理&#34;&gt;node controller节点管理。&lt;/h2&gt;

&lt;p&gt;首先我们需要了解kubelet通过apiserver向etcd中存储的节点信息（有节点健康状况，节点资源，节点名称地址，操作系统版本，docker版本，kubelet版本等等），其中一个节点健康状况分为三种True，false，unknown三种状态，也是最直接的节点状态&lt;/p&gt;

&lt;p&gt;然后这个控制器就会重etcd中逐个节点读取这些状态，将来自kubelet状态来改变node controller中nodestatusmap中状态，对于状态不对的node节点加入一个队列，等待确认node是否有问题，有问题就进行信息同步，并且删除节点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/controller1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;具体步骤&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; - 如果controller manager在启动时设置了--cluster-cidr，那么为每一个没有设置spec.PodCIDR的节点生成一个CIDR地址，并用该地址设置节点的spec.PodCIDR属性。
 - 逐个读取节点信息，此时node controller中有一个nodestatusMap，里面存储了信息，与新发送过来的节点信息做比较，并更新nodestatusMap中的节点信息。Kubelet发送过来的节点信息，有三种情况：未发送、发送但节点信息未变化、发送并且节点信息变化。此时node controller根据发送的节点信息，更新nodestatusMap，如果判断出在某段时间内没有接受到某个节点的信息，则设置节点状态为“未知”。
 - 最后，将未就绪状态的节点加入到待删除队列中，待删除后，通过API Server将etcd中该节点的信息删除。如果节点为就绪状态，那么就向etcd中同步该节点信息。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;resourcequota-controller资源配额&#34;&gt;resourcequota controller资源配额&lt;/h2&gt;

&lt;p&gt;这一个功能十分必要，它确保任何对象任何时候都不会超量占用资源，确保来系统的稳定性。目前k8s支持三个层次的资源配额&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 容器级别  可以限制cpu和memory

2. pod级别  对pod内所有容器的可用资源进行限制

3. namespace级别  pod数量，rc数量 service数量，rq数量，secret数量，persistent volume数量
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现机制：准入机制（admission caotrol），admission control当前提供了两种方式的配额约束，分别是limitRanger和resourceQuota。其中limitRanger作用于pod和容器上。ResourceQuota作用于namespace上，用于限定一个namespace里的各类资源的使用总额。&lt;/p&gt;

&lt;p&gt;在etcd中会维护一个资源配额记录，每次用户通过apiserver进行请求时，这个控制器会先进行计算，如果资源不过就会拒绝请求。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/controller2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从上图中，我们可以看出，大概有三条路线，resourceQuota controller在这三条路线中都起着重要的作用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;如果用户在定义pod时同时声明了limitranger，则用户通过API Server请求创建或者修改资源对象，这是admission control会计算当前配额的使用情况，不符合约束的则创建失败。（一、三）
对于定义了resource Quota的namespace，resourceQuota controller会定期统计和生成该namespace下的各类对象资源使用总量，统计结果包括：pod、service、RC、secret和PV等对象的实例个数，以及该namespace下所有的container实例所使用的资源量（CPU，memory），然后会将这些结果写入到etcd中，写入的内容为资源对象名称、配额制、使用值，然后admission control会根据统计结果判断是否超额，以确保相关namespace下的资源配置总量不会超过resource Quota的限定值。（二、三）
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;namespace-controller&#34;&gt;namespace controller&lt;/h2&gt;

&lt;p&gt;namespace controller主要是监控namespace的状态，在其失效的情况下,对其进行处理&lt;/p&gt;

&lt;p&gt;用户通过API Server可以创建新的namespace并保存在etcd中，namespace controller定时通过API Server读取这些namespace信息。如果namespace被API标记为优雅删除（通过设置删除周期），则将该namespace的状态设置为“terminating”并保存到etcd中。同时namespace controller删除该namespace下的serviceAccount,RC,pod，secret，PV,listRange，resourceQuota和event等资源对象。&lt;/p&gt;

&lt;p&gt;当namespace的状态为“terminating”后，由admission controller的namespaceLifecycle插件来阻止为该namespace创建新的资源。同时在namespace controller删除完该namespace中的所有资源对象后，namespace controller对该namespace 执行finalize操作，删除namespace的spec.finallizers域中的信息。&lt;/p&gt;

&lt;p&gt;当然这里有一种特殊情况，当个namespace controller发现namespace设置了删除周期，并且该namespace 的spec.finalizers域值为空，那么namespace controller将通过API Server删除该namespace 的资源。&lt;/p&gt;

&lt;h2 id=&#34;serviceaccount-controller和token-controller&#34;&gt;serviceAccount controller和token controller&lt;/h2&gt;

&lt;p&gt;这是两个安全监控，在apiserver启动的时候使用serviceaccount，就会产生一个key和crt，那么在controller mansge启动的时候通过参数指定这个key就会自动创建一个secret，也会创建一个token controller完成对serviceaccount的监控。&lt;/p&gt;

&lt;h2 id=&#34;service-controller和endpoint-controller&#34;&gt;service controller和endpoint controller&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/controller3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图所示了service和endpoint与pod的关系，endpoints表示一个service对应的所有的pod副本的访问地址，而endpoints controller就是负责生成和维护所有endpoints对象的控制器。&lt;/p&gt;

&lt;p&gt;它负责监听service和对应的pod副本的变化，如果检测到service被删除，则删除和该service同名的endpoints对象。如果检测到新的service被创建或者修改，则根据该service的信息获取到相关的pod列表，然后创建或者更新service对应的endpoints对象。如果检测到pod的事件，则更新它对应service的endpoints对象（增加或者删除或者修改对应的endpoint条目）。&lt;/p&gt;

&lt;p&gt;我们都知道将service和pod通过label关联之后，我们访问service的clusterIP对应的服务，就能通过kube-proxy将路由转发到对应的后端的endpoint（pod IP +port）上，最终访问到容器中的服务，实现了service的负载均衡功能。&lt;/p&gt;

&lt;p&gt;service controller的作用，它其实是属于kubernetes与外部的云平台之间的一个接口控制器。Service controller监听service的变化，如果是一个loadBalancer类型的service，则service controller确保外部的云平台上该service对应的loadbalance实例被相应的创建、删除以及更新路由转发表（根据endpoint的条目）。&lt;/p&gt;

&lt;h2 id=&#34;pv-controller&#34;&gt;pv controller&lt;/h2&gt;

&lt;p&gt;其实就是我们注册函数中的&lt;code&gt;controllers[&amp;quot;persistentvolume-binder&amp;quot;] = startPersistentVolumeBinderController&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;PV 控制器 watch 到该 Pod 使用的 PVC 处于 Pending 状态，于是调用 Volume Plugin（in-tree）创建存储卷，并创建 PV 对象（out-of-tree 由 External Provisioner 来处理）。&lt;/p&gt;

&lt;h1 id=&#34;controller控制器&#34;&gt;controller控制器&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller/&#34;&gt;controller控制器&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Infrastructure监控方案</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/infrastructure-base/</link>
          <pubDate>Sat, 13 Jun 2020 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/infrastructure-base/</guid>
          <description>&lt;p&gt;一个完整的监控体系包括：采集数据、分析存储数据、展示数据、告警以及自动化处理、监控工具自身的安全机制，我们来看看如何使用prometheus进行基础设施监控架构。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;首先可以看一下官方给出的架构方案&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20190315.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这边对架构正常使用做一个补充说明：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;grafana是无状态的，可以多部署几个通过nginx来负载均衡，通过不同的端口去访问不同的thanos-query。&lt;/li&gt;
&lt;li&gt;thanos-query查询对应的prometheus集群，获取数据。这边使用thanos来完成了prometheus集群功能，具体看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos的实现&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;prometheus使用hashmod来实现对数据采集的分片，把数据放到不同的prometheus的节点上，实现集群，这个没有使用federation，因为联合在大规模的情况下，瓶颈比较严重，还有目前只是单采集模式，可以设置双采，使用vip+keepalive来实现主备切换。&lt;/li&gt;
&lt;li&gt;prometheus去采集数据并不是直接使用target连接，使用nginx进行了转发采集，使用prometheus的relabel来设置&lt;strong&gt;address&lt;/strong&gt;,使得所有采集都连接nginx，然后使用target作为参数，最终访问target地址，获取数据，这样可以将nginx的网络打通，就能实现跨机房跨区域采集了，后面有网络问题也是处理nginx所在的机器就好。这两步具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus&#34;&gt;prometheus的实现&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;具体的采集&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/infrastructure-base/#监控内容&#34;&gt;监控内容&lt;/a&gt;可以通过探针的部署使用来区分。&lt;/li&gt;
&lt;li&gt;prometheus的数据可以提供给第三方使用，可以直接将数据通过&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus/#adapter&#34;&gt;adapter&lt;/a&gt;推送的kafka，给其他使用方消费&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/#remote-storage&#34;&gt;远程存储&lt;/a&gt;这一块，直接使用了remote read／write，当原生数据库不支持的时候，需要使用adapter进行转化发送。其实大部分我们使用的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos&#34;&gt;prometheus的扩展&lt;/a&gt;来做存储查询。&lt;/li&gt;
&lt;li&gt;告警直接根据rule文件推送的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/alertmanager/&#34;&gt;alartmanager&lt;/a&gt;，这个alartmanager是一个分布式的，在prometheus的yaml文件中都要配置上，alartmanager也可以将数据推送给mq（需要改造），正常可以使用kafka，给一些告警平台进行消费使用。rule文件直接使用consul的注册信息生成，注册信息是后台管理的。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/middleware/serverdiscovery/sd/&#34;&gt;动态注册，服务发现&lt;/a&gt;这一块，可以看见，使用的是consul+consul-template,使用consul注册，并且保存注册信息，这边使用了consul的k/v模式（为什么使用这个下面有说明），然后使用consul-template这个工具将注册的信息生成json文件给prometheus的file_sd_condig使用。使用crontab 来定时更新文件，实现配置文件的自动加载。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;大体架构如此，就可以实现一套物理环境的基础设施监控。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.08.08&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;NOTE：基于thanos查询慢，VM优秀的写入和查询性能，已经使用远程存储将数据都存储到VM进行查询，所以prometheus将数据都写到VM&lt;/strong&gt;，VM具体查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM的实现&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;设计原则&#34;&gt;设计原则&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;监控的是基础设施，目的是为了解决问题，没有必要朝着大而全的方向去做，对于没有必要采集的指标，不浪费资源。&lt;/li&gt;
&lt;li&gt;需要处理的告警才发出来，发出来的是必须要处理的告警&lt;/li&gt;
&lt;li&gt;业务系统和监控分离，哪怕业务系统挂了，监控也不能挂，监控挂了，不影响业务系统的运行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;监控内容&#34;&gt;监控内容&lt;/h1&gt;

&lt;p&gt;具体见监控内容可以查看每个探针，我们来看一下prometheus的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/&#34;&gt;exporter&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;采集组件&#34;&gt;采集组件&lt;/h2&gt;

&lt;p&gt;prometheus的exporter都是独立的，简单几个使用还是不错，解耦还开箱即用，但是数量多了，运维的压力变大了，例如探针管理升级，运行情况的检查等，有几种方案解决&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;做一个管理平台，类似于我们的后台系统，专门对exporter进行管理&lt;/li&gt;
&lt;li&gt;用一个主进程整合几个探针，每个探针依旧是原来的版本&lt;/li&gt;
&lt;li&gt;用telegraf来支持各种类型的input，all in one&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;prometheus使用总结&#34;&gt;prometheus使用总结&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;consul的service的瓶颈问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之前使用consul的services注册job的服务信息，然后使用consul-template动态生成prometheus的配置文件。然后prometheus通过查询consul中注册的信息正则匹配来完成prometheus的采集操作
但是这样当job量很大的时候，比如有20组job，一组job130的target的的时候，就会出现consul请求api瓶颈&lt;/p&gt;

&lt;p&gt;现在使用consul的k/v格式进行注册，直接通过IP：port作为key，对应的label作为vaule，然后使用consul-template动态生成discovery的json文件，然后prometheus使用file sd来发现这个json文件，相当于将对应的json的内容写到了prometheus的配置文件中去，这个时候五分钟consul-template动态生成一次，不会每次都去请求，这样consul的压力就几乎没有了，经过测试可以达到5000个target，prometheus的shard极限，对consul依旧没有什么压力，现在主要瓶颈在于json文件大小，filesd的压力，可以继续优化成多个文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;自动刷新配置文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于Prometheus是“拉”的方式主动监测，所以需要在server端指定被监控节点的列表。当被监控的节点增多之后，每次增加节点都需要更改配置文件，非常麻烦，我这里用consul-template+consul动态生成配置文件，这种方式同样适用于其他需要频繁更改配置文件的服务。另外一种解决方案是etcd+confd，基本现在主流的动态配置系统分这两大阵营。consul-template的定位和confd差不多，不过它是consul自家推出的模板系统。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 对象存储云存储</title>
          <link>https://kingjcy.github.io/post/distributed/store/oss/</link>
          <pubDate>Thu, 04 Jun 2020 15:52:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/oss/</guid>
          <description>&lt;p&gt;不同的云厂商对它有不同的英文缩写命名。例如阿里云把自家的对象存储服务叫做OSS，华为云叫OBS，腾讯云叫COS，七牛叫Kodo，百度叫BOS，网易叫NOS……五花八门，反正都是一个技术。&lt;/p&gt;

&lt;h1 id=&#34;阿里云oss&#34;&gt;阿里云OSS&lt;/h1&gt;

&lt;p&gt;阿里云存储服务（Open Storage Service，简称OSS），是阿里云对外提供的海量，安全，低成本，高可靠的云存储服务。用户可以通过调用API，在任何应用、任何时间、任何地点上传和下载数据，也可以通过用户Web控制台对数据进行简单的管理。OSS适合存放任意文件类型，适合各种网站、开发企业及开发者使用。&lt;/p&gt;

&lt;h1 id=&#34;华为云obs&#34;&gt;华为云OBS&lt;/h1&gt;

&lt;h1 id=&#34;腾讯云cos&#34;&gt;腾讯云COS&lt;/h1&gt;

&lt;h1 id=&#34;七牛云kodo&#34;&gt;七牛云Kodo&lt;/h1&gt;

&lt;h1 id=&#34;百度云bos&#34;&gt;百度云BOS&lt;/h1&gt;

&lt;h1 id=&#34;技术方案&#34;&gt;技术方案&lt;/h1&gt;

&lt;h2 id=&#34;ceph&#34;&gt;ceph&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/ceph/ceph/&#34;&gt;ceph&lt;/a&gt;我们都知道同时支持对象存储，块存储和文件系统服务。&lt;/p&gt;

&lt;h2 id=&#34;minio&#34;&gt;minio&lt;/h2&gt;

&lt;p&gt;minio是一个基于Apache License V2.0开源协议的对象存储服务，它兼容亚马逊S3云存储服务，非常适合于存储大容量非结构化的数据，如图片，视频，日志文件等。而一个对象文件可以任意大小，从几KB到最大的5T不等。它是一个非常轻量级的服务，可以很简单的和其它的应用结合，类似于NodeJS, Redis或者MySQL。&lt;/p&gt;

&lt;p&gt;minio默认不计算MD5，除非传输给客户端的时候，所以很快，支持windows，有web页进行管理。&lt;/p&gt;

&lt;p&gt;从目前来看，如果想自建对象存储服务的话，有能力，规模比较大的话，采用ceph感觉更好一点。如果只是想要一个对象存储，要求没有那么多的话，可以采用minio。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- Distributed</title>
          <link>https://kingjcy.github.io/post/distributed/distributed/</link>
          <pubDate>Tue, 26 May 2020 20:10:41 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/distributed/</guid>
          <description>&lt;p&gt;分布式和集群相关的东西，已经是未来系统发展的趋势。&lt;/p&gt;

&lt;h1 id=&#34;分布式&#34;&gt;分布式&lt;/h1&gt;

&lt;p&gt;为什么会有分布式？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以很好的减少依赖（每个服务都只要开发测试自己的逻辑数据）&lt;/li&gt;
&lt;li&gt;可以简单的进行扩展，（分布式存储这种）&lt;/li&gt;
&lt;li&gt;能够很简单的支持高可用（负载均衡到多实例）&lt;/li&gt;
&lt;li&gt;容错性（将数据以备份的信息存储多分）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;单系统&#34;&gt;单系统&lt;/h2&gt;

&lt;p&gt;单块系统就是所有的代码都在一个工程里，最多可能就是通过maven等构件工具拆分了一下代码工程模块，不同的模块可以放在不同的工程代码里。&lt;/p&gt;

&lt;p&gt;很多流量很小的企业内部系统，比如OA、CRM、财务等系统，甚至可能就直接在一台机器的tomcat下部署一下。&lt;/p&gt;

&lt;p&gt;然后直接配置一下域名解析，就可以让这个系统的可能几十个，或者几百个用户通过访问域名来使用这个软件了。&lt;/p&gt;

&lt;p&gt;你哪怕就部署一台机器，这个系统也可以运行，只不过为了所谓的“高可用”，可能一般会部署两台机器，前面加一层负载均衡设备，这样其中一个机器挂了，另外一个机器上还有一个系统可以用。&lt;/p&gt;

&lt;h2 id=&#34;分布式系统&#34;&gt;分布式系统&lt;/h2&gt;

&lt;p&gt;分布式的主要特点就是分而治之，实现解耦，高性能的解决问题。比如把一个大的系统拆分为很多小的系统，甚至很多小的服务，然后几个人组成一个小组就专门维护其中一个小系统，或者每个人维护一个小服务。这样就可以分而治之，这样每个人可以专注维护自己的代码。不同的子系统之间，就是通过接口互相来回调用，每个子系统都有自己的数据库。&lt;/p&gt;

&lt;p&gt;下面是一些应用，都是采用了分布式的思想。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;比如在load balance后面的web server，就是一种基本的分布式处理请求的模式，他可以并行处理请求，提高响应效率，也就提高了性能&lt;/li&gt;
&lt;li&gt;比如hadoop中的hdfs它就是在不同节点上存储不同的数据，这就是一种扩大容量的分布式存储。一份数据三副本也实现分布式的数据存储，实现来分布式数据的容错能力&lt;/li&gt;
&lt;li&gt;比如redis在不同的节点上分布不同的slot来完成分布式的存储。但是每个实例的m/s的最终一致性也实现分布式的数据容错能力。&lt;/li&gt;
&lt;li&gt;比如etcd的分布式存储，实现强一致性，在各个节点的数据时刻一致，实现高可用。&lt;/li&gt;
&lt;li&gt;比如ceph一样实现各个节点上数据一致的分布式共享。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些都是分布式的应用，分布式只是一种可以使不同资源共同去完成一个目标的方法。分布式让不同的节点完成着不同的或者相同的任务，来提高能力，可以是计算，可以是存储，可以是容错。其实分布式更多的是一种思想。当然这种思想可以实现不同的事情，比如上面提到的，提高性能，提高存储能力，提高可用能力，提高容错能力，实现数据共享等等。&lt;/p&gt;

&lt;p&gt;通过分布式思想实现的系统，也就是我们常说的分布式系统，分布式系统相对于单系统来说，更加的复杂，很多传统的问题都需要分布式的解决方案。&lt;/p&gt;

&lt;h2 id=&#34;分布式系统所带来的技术问题&#34;&gt;分布式系统所带来的技术问题&lt;/h2&gt;

&lt;p&gt;那么大家这个时候可以思考一下，如果你的公司是采用这种分布式系统的方式来构建公司的一个大规模系统的，那么这个时候会涉及到哪些技术问题？&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式服务框架&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;你如果要让不同的子系统或者服务之间互相通信，首先必须有一套分布式服务框架。主要是做服务发现和服务间通信，也就是各个服务可以互相感知到对方在哪里，可以发送请求过去，可以通过HTTP或者RPC的方式。最常见的技术就是dubbo以及spring cloud，当然大厂一般都是自己有服务框架。基本上在这个领域目前还是阿里的java写框架一家独大，
我所使用的c/c++，golang都是使用标准库自己来实现这些服务发现通信的问题，所以也没有什么比较好的框架。目前在容器领域，随着k8s的模式的趋势发展，下一代的分布式框架开始衍生，出现了k8s的相关解决方案-&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/microservices/&#34;&gt;微服务&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式事务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一旦你的系统拆分为了多个子系统之后，那么一个贯穿全局的分布式事务应该怎么来实现？&lt;/p&gt;

&lt;p&gt;这个你需要了解TCC、最终一致性、2PC等&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-event/&#34;&gt;分布式事务的实现方案和开源技术&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;不同的系统之间如果需要在全局加锁获取某个资源的锁定，此时应该怎么来做？毕竟大家不是在一个JVM里了，不可能用synchronized来在多个子系统之间实现锁吧，是不是？&lt;/p&gt;

&lt;p&gt;所以这个时候就需要使用&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-lock/&#34;&gt;分布式锁&lt;/a&gt;来解决这些问题。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式存储&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果你原来就是个单块系统，那么你其实是可以在单个JVM里进行本地缓存就可以了，比如搞一个HashMap来缓存一些数据。但是现在你有很多个子系统，他们如果要共享一个缓存，你应该怎么办？是不是需要引入&lt;a href=&#34;https://kingjcy.github.io/post/database/redis/redis/&#34;&gt;Redis等缓存系统&lt;/a&gt;，实现数据缓存共享。&lt;/p&gt;

&lt;p&gt;还有很多的数据共享存储的实例，我们都需要使用到&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/store/&#34;&gt;分布式存储&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式消息系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在单块系统内，就一个JVM进程内部，你可以用类似LinkedList之类的数据结构作为一个本地内存里的队列。&lt;/p&gt;

&lt;p&gt;但是多个子系统之间要进行消息队列的传递呢？那是不是要引入类似RabbitMQ之类的分布式&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/mq-compare/&#34;&gt;消息队列中间件&lt;/a&gt;？&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式搜索系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果在单块系统内，你可以比如在本地就基于Lucene（支持全文索引的数据库系统）来开发一个全文检索模块，但是如果是分布式系统下的很多子系统，你还能直接基于Lucene吗？&lt;/p&gt;

&lt;p&gt;明显不行，你需要在系统里引入一个外部的分布式搜索系统，比如&lt;a href=&#34;https://kingjcy.github.io/post/database/elasticsearch/&#34;&gt;Elasticsearch&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他很多的技术&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;比如说&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-config/&#34;&gt;分布式配置中心&lt;/a&gt;、分布式日志中心、分布式监控告警中心、分布式会话，等等，都是分布式系统场景下你需要使用和了解的一些技术。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;一致性问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一致性问题其实也是分布式事务中的，但是这个问题比较突出，单独说一下，一致性又可以分为强一致性与弱一致性。&lt;/p&gt;

&lt;p&gt;强一致性可以理解为在任意时刻，所有节点中的数据是一样的。同一时间点，你在节点A中获取到key1的值与在节点B中获取到key1的值应该都是一样的。&lt;/p&gt;

&lt;p&gt;弱一致性包含很多种不同的实现，目前分布式系统中广泛实现的是最终一致性。所谓最终一致性，就是不保证在任意时刻任意节点上的同一份数据都是相同的，但是随着时间的迁移，不同节点上的同一份数据总是在向趋同的方向变化。也可以简单的理解为在一段时间后，节点间的数据会最终达到一致状态。&lt;/p&gt;

&lt;p&gt;所以对于一致性，一致的程度不同大体可以分为强、弱、最终一致性三类。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;强一致性
对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。比如小明更新V0到V1，那么小华读取的时候也应该是V1。&lt;/li&gt;
&lt;li&gt;弱一致性
如果能容忍后续的部分或者全部访问不到，则是弱一致性。比如小明更新VO到V1，可以容忍那么小华读取的时候是V0。&lt;/li&gt;
&lt;li&gt;最终一致性
如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。比如小明更新VO到V1，可以使得小华在一段时间之后读取的时候是V0。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;分布式的难点是一致性，基本解决方案&lt;/p&gt;

&lt;p&gt;CAP理论&lt;/p&gt;

&lt;p&gt;CAP理论指的是一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。其中分区容错性（Partition tolerance）是分布式的根本，是必须要实现的，所以一般都是实现AP和CP。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Consistency (一致性)：
“all nodes see the same data at the same time”,即更新操作成功并返回客户端后，所有节点在同一时间的数据完全一致，这就是分布式的一致性。一致性的问题在并发系统中不可避免，对于客户端来说，一致性指的是并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。&lt;/li&gt;
&lt;li&gt;Availability (可用性):
可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。&lt;/li&gt;
&lt;li&gt;Partition Tolerance (分区容错性):
即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。这个是必须要保障的，不然都没有分布式的意义了。
分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，对于用户而言并没有什么体验上的影响。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;假设N1和N2之间通信的时候网络突然出现故障，有用户向N1发送数据更新请求，那N1中的数据DB0将被更新为DB1，由于网络是断开的，N2中的数据库仍旧是DB0；

如果这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据DB1，怎么办呢？有二种选择，第一，牺牲数据一致性，响应旧的数据DB0给用户；第二，牺牲可用性，阻塞等待，直到网络连接恢复，数据更新操作完成之后，再给用户响应最新的数据DB1。

上面的过程比较简单，但也说明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。也就是说分布式系统不可能同时满足三个特性。这就需要我们在搭建系统时进行取舍了
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们分析一下既然可以满足两个，那么舍弃哪一个比较好呢？&lt;/p&gt;

&lt;p&gt;（1）满足CA舍弃P，也就是满足一致性和可用性，舍弃容错性。但是这也就意味着你的系统不是分布式的了，因为涉及分布式的想法就是把功能分开，部署到不同的机器上。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。&lt;/p&gt;

&lt;p&gt;（2）满足CP舍弃A，也就是满足一致性和容错性，舍弃可用性。如果你的系统允许有段时间的访问失效等问题，这个是可以满足的。就好比多个人并发买票，后台网络出现故障，你买的时候系统就崩溃了。设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。&lt;/p&gt;

&lt;p&gt;（3）满足AP舍弃C，也就是满足可用性和容错性，舍弃一致性。这也就是意味着你的系统在并发访问的时候可能会出现数据不一致的情况。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。&lt;/p&gt;

&lt;p&gt;实时证明，大多数都是牺牲了一致性。像12306还有淘宝网，就好比是你买火车票，本来你看到的是还有一张票，其实在这个时刻已经被买走了，你填好了信息准备买的时候发现系统提示你没票了。这就是牺牲了一致性。&lt;/p&gt;

&lt;p&gt;所以在牺牲强一致性，使用最终一致性的情况下，eBay 架构师 Dan Pritchett 提出了 BASE 理论，用于解决大规模分布式系统下的数据一致性问题。&lt;/p&gt;

&lt;p&gt;BASE 理论告诉我们：可以通过放弃系统在每个时刻的强一致性来换取系统的可扩展性。&lt;/p&gt;

&lt;p&gt;BASE 是 Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent (最终一致性)三个短语的缩写。是对CAP中AP的一个扩展&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基本可用:分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。&lt;/li&gt;
&lt;li&gt;软状态:允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是CAP中的不一致。&lt;/li&gt;
&lt;li&gt;最终一致:最终一致是指经过一段时间后，所有节点数据都将会达到一致。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是不是说牺牲一致性一定是最好的。就好比mysql中的事务机制，张三给李四转了100块钱，这时候必须保证张三的账户上少了100，李四的账户多了100。因此需要数据的一致性，而且什么时候转钱都可以，也需要可用性。但是可以转钱失败是可以允许的。&lt;/p&gt;

&lt;h2 id=&#34;分布式的发展&#34;&gt;分布式的发展&lt;/h2&gt;

&lt;p&gt;分布式是单机系统发展到一定时候的产物，其实和微服务的发展是一样的，微服务是分布式发展到一定阶段的概念，两者其实是差不多的，所以&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/microservices/&#34;&gt;微服务的演进&lt;/a&gt;也是分布式的发展。&lt;/p&gt;

&lt;h1 id=&#34;集群&#34;&gt;集群&lt;/h1&gt;

&lt;p&gt;集群就是集中一堆机器来一起做事。&lt;/p&gt;

&lt;p&gt;比如上面的webserver可以是一个集群，他们互不干扰，共同的处理着同一种任务。前面的load balance也可以分布式部署，也可以是一个集群，后段的数据库也可以分布式部署，也可以是一个集群，这个几个集群组合在一起可以是一个大的集群。&lt;/p&gt;

&lt;p&gt;比如hadoop中的hdfs也是一个集群，但是它们互相通信，能够知道各个节点的情况，同时由中心统一管理调度&lt;/p&gt;

&lt;p&gt;比如上面的redis也是一个集群，他们也互相通信，却是存在着无中心化的管理&lt;/p&gt;

&lt;p&gt;比如上面的etcd，ceph几个节点也是一个集群，要求节点上数据完全保持一致&lt;/p&gt;

&lt;p&gt;这些都是集群，所以集群通俗点说就是一堆节点，当然都是相关的，不相关的不会组合在一起&lt;/p&gt;

&lt;p&gt;所以说集群和分布式是不同的概念，好比“分头做事”和“一堆相关的人”。他们也是紧密相连的，由分布式一般都是集群，有集群一般都是分布式的。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Goroutine</title>
          <link>https://kingjcy.github.io/post/golang/go-goroutinechannel/</link>
          <pubDate>Sun, 24 May 2020 14:49:00 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-goroutinechannel/</guid>
          <description>&lt;p&gt;goroutine和channel是go语言的两大基石，这边主要来研究一下goroutine，&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-channel/&#34;&gt;channel可以查看这里&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;并发&#34;&gt;并发&lt;/h1&gt;

&lt;p&gt;发展演变&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;多进程&amp;mdash;&amp;mdash;-开销太大，都是基于内核的调用&lt;/li&gt;
&lt;li&gt;多线程&amp;mdash;&amp;mdash;-相对开销小，但是远远达不到需求，最多并发1万这样&lt;/li&gt;
&lt;li&gt;基于回调的非阻塞/异步io&amp;mdash;-共享内存式的同步异步，导致编程相当复杂&lt;/li&gt;
&lt;li&gt;协程&amp;mdash;-轻量级线程，轻松达到100w的并发，使用成本低、消耗资源低、能效高&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;线程和协程的对比&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;内存占用&lt;/p&gt;

&lt;p&gt;创建一个goroutine不需要太多的内存 - 大概2KB左右的栈空间。如果需要更多的栈空间，就从堆里分配额外的空间来使用。新创建的线程会占用1MB的内存空间（这大约是goroutine的500倍）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建和销毁的开销&lt;/p&gt;

&lt;p&gt;线程需要从操作系统里请求资源并在用完之后释放回去，因此创建和销毁线程的开销非常大。为了避免这些开销，我们通常的做法是维护一个线程池。Goroutine的创建和销毁是由运行环境（runtime）完成的。这些操作的开销就比较小。Go语言不支持手工管理goroutine。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;切换开销&lt;/p&gt;

&lt;p&gt;当一个线程阻塞的时候，另外一个线程需要被调度到当前处理器上运行。线程的调度是抢占式的（preemptively）。当切换一个线程的时候，调度器需要保存／恢复所有的寄存器。这包括16个通用寄存器，程序指针（program counter），栈指针（stack pointer），段寄存器（segment registers）和16个XMM寄存器，浮点协处理器状态，16个AVX寄存器，所有的特殊模块寄存器（MSR）等。当在线程间快速切换的时候这些开销就变得非常大了。&lt;/p&gt;

&lt;p&gt;Goroutine的调度是协同合作式的（cooperatively）。不依赖操作系统和其提供的线程，golang自己实现的CSP并发模型实现：M, P, G。当切换goroutine的时候，调度器只需要保存和恢复三个寄存器 - 程序指针，栈指针和DX。切换的开销就小多了。&lt;/p&gt;

&lt;p&gt;前面已经谈到了，goroutine的数目会比线程多很多，但这并不影响切换的时间。有两个原因：第一，只有可以运行的goroutine才会被考虑，正在阻塞的goroutine会被忽略。第二，现代的调度器的复杂度都是O(1)的。这意味着选择的数目（线程或者是goroutine）不会影响切换的时间。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;go协程&#34;&gt;go协程&lt;/h1&gt;

&lt;p&gt;所以在协程的基础上，go支持在语言上实现协程并发goroutine，goroutine类似于进程，各个协程之间互不干涉，通过channel通信控制，这样减少了很多的复杂问题，goroutine的使用非常简单：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;关键字go&amp;mdash;&amp;ndash;只要在执行体前加上关键字go就能实现协程并发&lt;/li&gt;
&lt;li&gt;go func(){}()加了()就不仅是类型定义，而且要实例化（需要绑定参数，就是最后一个括号中的参数）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;goroutine的模型来源&#34;&gt;goroutine的模型来源&lt;/h2&gt;

&lt;p&gt;Goroutine，Go语言基于并发（并行）编程给出的自家的解决方案。goroutine是什么？通常goroutine会被当做coroutine（协程）的 golang实现，从比较粗浅的层面来看，这种认知也算是合理，但实际上，goroutine并非传统意义上的协程，现在主流的线程模型分三种：内核级线程模型、用户级线程模型和两级线程模型（也称混合型线程模型），传统的协程库属于用户级线程模型，而goroutine和它的Go Scheduler在底层实现上其实是属于两级线程模型，因此，有时候为了方便理解可以简单把goroutine类比成协程，但心里一定要有个清晰的认知 — goroutine并不等同于协程。&lt;/p&gt;

&lt;p&gt;并发模型一代代地升级，有IO多路复用、多进程以及多线程，这几种模型都各有长短。多线程，因为其轻量和易用，成为并发编程中使用频率最高的并发模型，而后衍生的协程等其他子产品，也都基于它，而我们今天要分析的 goroutine 也是基于线程，因此，我们先来聊聊线程的三大模型：&lt;/p&gt;

&lt;p&gt;线程的实现模型主要有3种，在上一段说过：内核级线程模型、用户级线程模型和两级线程模型（也称混合型线程模型），它们之间最大的差异就在于用户线程与内核调度实体（KSE，Kernel Scheduling Entity）之间的对应关系上。而所谓的内核调度实体 KSE 就是指可以被操作系统内核调度器调度的对象实体。简单来说 KSE 就是内核级线程，是操作系统内核的最小调度单元，也就是我们写代码的时候通俗理解上的线程了&lt;/p&gt;

&lt;p&gt;1、用户级线程模型（比如python的gevent）&lt;/p&gt;

&lt;p&gt;用户线程与内核线程KSE是多对一（N : 1）的映射模型，多个用户线程的一般从属于单个进程并且多线程的调度是由用户自己的线程库来完成，线程的创建、销毁以及多线程之间的协调等操作都是由用户自己的线程库来负责而无须借助系统调用来实现。&lt;/p&gt;

&lt;p&gt;这种线程模型并不能做到真正意义上的并发，假设在某个用户进程上的某个用户线程因为一个阻塞调用（比如I/O阻塞）而被CPU给中断（抢占式调度）了，那么该进程内的所有线程都被阻塞（因为单个用户进程内的线程自调度是没有CPU时钟中断的，从而没有轮转调度）&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/goroutine/yonghutai.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、内核级线程模型(比如Java的java.lang.Thread、C++11的std::thread等等)&lt;/p&gt;

&lt;p&gt;用户线程与内核线程KSE是一对一（1 : 1）的映射模型，也就是每一个用户线程绑定一个实际的内核线程，而线程的调度则完全交付给操作系统内核去做，应用程序对线程的创建、终止以及同步都基于内核提供的系统调用来完成，大部分编程语言的线程库(比如Java的java.lang.Thread、C++11的std::thread等等)都是对操作系统的线程（内核级线程）的一层封装，&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/goroutine/neiheji.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3、两级线程模型&lt;/p&gt;

&lt;p&gt;用户线程与内核KSE是多对多（N : M）的映射模型：，两级线程模型中的一个进程可以与多个内核线程KSE关联，也就是说一个进程内的多个线程可以分别绑定一个自己的KSE，这点和内核级线程模型相似；它的进程里的线程并不与KSE唯一绑定，而是可以多个用户线程映射到同一个KSE，当某个KSE因为其绑定的线程的阻塞操作被内核调度出CPU时，其关联的进程中其余用户线程可以重新与其他KSE绑定运行。goroutine的优势在于上下文切换在完全用户态进行，无需像线程一样频繁在用户态与内核态之间切换，节约了资源消耗。所以，两级线程模型靠（自身调度与系统调度协同工作），也就是 — 『薛定谔的模型』（误），因为这种模型的高度复杂性，操作系统内核开发者一般不会使用，所以更多时候是作为第三方库的形式出现，而Go语言中的runtime调度器就是采用的这种实现方案，实现了Goroutine与KSE之间的动态关联，不过Go语言的实现更加高级和优雅；&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/goroutine/liangji.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;go的调度模型&#34;&gt;go的调度模型&lt;/h2&gt;

&lt;p&gt;G-P-M 模型概述&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/goroutine/gpm5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;G: 表示Goroutine，每个Goroutine对应一个G结构体，G存储Goroutine的运行堆栈、状态以及任务函数，可重用。G并非执行体，每个G需要绑定到P才能被调度执行。每个Goroutine对象中的sched保存着其上下文信息&lt;/p&gt;

&lt;p&gt;P: Processor，表示逻辑处理器， 对G来说，P相当于CPU核，G只有绑定到P(在P的local runq中)才能被调度。对M来说，P提供了相关的执行环境(Context)，如内存分配状态(mcache)，任务队列(G)等，P的数量决定了系统内最大可并行的G的数量（前提：物理CPU核数 &amp;gt;= P的数量），P的数量由用户设置的GOMAXPROCS决定，但是不论GOMAXPROCS设置为多大，P的数量最大为256。或者通过运行时调用函数runtime.GOMAXPROCS()进行设置。Processor数量固定意味着任意时刻只有固定数量的线程在运行go代码。Goroutine中就是我们要执行并发的代码。图中P正在执行的Goroutine为蓝色的；处于待执行状态的Goroutine为灰色的，灰色的Goroutine形成了一个队列runqueues&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;默认情况下，在任意时刻，只有一个Goroutine可以被调度执行。我们未来可能会将其设计的更加智能，但是目前，你必须通过 设置 GOMAXPROCS 环境变量或者导入 runtime 包并调用 runtime.GOMAXPROCS(NCPU) , 来告诉Go 的运行时系统最大并行执行的Goroutine数目。你可以通过 runtime.NumCPU() 获得当前运行系统的逻 辑核数，作为一个有用的参考。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;M代表内核级线程，一个M就是一个线程，goroutine就是跑在M之上的；M是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息。代表着真正执行计算的资源，在绑定有效的P后，进入schedule循环；而schedule循环的机制大致是从Global队列、P的Local队列以及wait队列中获取G，切换到G的执行栈上并执行G的函数，调用goexit做清理工作并回到M，如此反复。M并不保留G状态，这是G可以跨M调度的基础，M的数量是不定的，由Go Runtime调整，为了防止创建过多OS线程导致系统调度不过来，目前默认最大限制为10000个。对内核级线程的封装，数量对应真实的CPU数（真正干活的对象）&lt;/p&gt;

&lt;p&gt;三者关系的宏观的图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/goroutine/gpm.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;work-stealing 的j均衡调度算法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;每个P维护一个G的本地队列；&lt;/li&gt;
&lt;li&gt;当一个G被创建出来，或者变为可执行状态时，就把他放到P的可执行队列中；&lt;/li&gt;
&lt;li&gt;当一个G在M里执行结束后，P会从队列中把该G取出；如果此时P的队列为空，即没有其他G可以执行， M就随机选择另外一个P，从其可执行的G队列中取走一半。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当通过go关键字创建一个新的goroutine的时候，它会优先被放入P的本地队列。为了运行goroutine，M需要持有（绑定）一个P，接着M会启动一个OS线程，循环从P的本地队列里取出一个goroutine并执行。当然还有上文提及的 work-stealing调度算法：当M执行完了当前P的Local队列里的所有G后，P也不会就这么在那躺尸啥都不干，它会先尝试从Global队列寻找G来执行，如果Global队列为空，它会随机挑选另外一个P，从它的队列里中拿走一半的G到自己的队列中执行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/goroutine/gpm3.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/goroutine/gpm4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;goroutine调度模型的瓶颈&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有基于G-P-M的Go调度器背书，go程序的并发编程中，可以任性地起大规模的goroutine来执行任务，官方也宣称用golang写并发程序的时候随便起个成千上万的goroutine毫无压力。然而，你起1000个goroutine没有问题，10000也没有问题，10w个可能也没问题；那，100w个呢？1000w个呢？&lt;/p&gt;

&lt;p&gt;即便每个goroutine只分配2KB的内存，但如果是恐怖如斯的数量，聚少成多，内存暴涨，也会对&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-gc/&#34;&gt;GC&lt;/a&gt;造成极大的负担（gc在1.8之后去掉了STW（Stop The World）机制）&lt;/p&gt;

&lt;p&gt;其实这个和golang本身并没有太大的关系，任何技术不加以控制，也是会奔溃的，一般我们都是使用池化技术来解决这类问题，具体可以看&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-goroutinechannel/#并发使用&#34;&gt;并发使用&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;相关问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1.go为什么要实现自己的协程调度，而不用系统调度？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;线程较多时，开销较大。&lt;/li&gt;
&lt;li&gt;OS 的调度，程序不可控。而 Go GC 需要停止所有的线程，使内存达到一致状态。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2.GM为啥不行？P有什么作用？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;是让我们可以直接放开其他线程，当遇到内核线程阻塞的时候。否则每个 P 都有一个队列，用来存正在执行的 G。避免 Global Sched Lock。&lt;/li&gt;
&lt;li&gt;每个 M 运行都需要一个 MCache 结构。M Pool 中通常有较多 M，但执行的只有几个，为每个池子中的每个 M 分配一个 MCache 则会形成不必要的浪费，通过把 cache 从 M 移到 P，每个运行的 M 都有关联的 P，这样只有运行的 M 才有自己的 MCache。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3.Goroutine vs OS thread 有什么区别？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;其实 goroutine 用到的就是线程池的技术，当 goroutine 需要执行时，会从 thread pool 中选出一个可用的 M 或者新建一个 M。而 thread pool 中如何选取线程，扩建线程，回收线程，Go Scheduler 进行了封装，对程序透明，只管调用就行，从而简化了 thread pool 的使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;并发使用&#34;&gt;并发使用&lt;/h1&gt;

&lt;h2 id=&#34;简单并发&#34;&gt;简单并发&lt;/h2&gt;

&lt;p&gt;简单的并发，一般用于启动多协程来处理后端数据业务，类似于多线程处理数据，直接使用for循环并发一定数量的goroutine，然后对数据进行处理，如果是处理同一个数据，则需要使用锁，通过传递参数，还可以对业务进行分通道处理&lt;/p&gt;

&lt;p&gt;我们没法控制goroutine产生数量，如果处理程序稍微耗时，在单机万级十万级qps请求下，goroutine大规模爆发，内存暴涨，处理效率会很快下降甚至引发程序崩溃。&lt;/p&gt;

&lt;p&gt;它无法控制创建goroutine的数量。因为我们每分钟收到了一百万个POST请求，很快就奔溃了。&lt;/p&gt;

&lt;p&gt;控制并发是必然的，也是防止大量资源被占用导致崩死的必须手段，当然并发的管理也是必要的，确保并发能够完整的执行，就要使用&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-context/&#34;&gt;go-context&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;工作池&#34;&gt;工作池&lt;/h2&gt;

&lt;p&gt;工作池+job队列 先启动一定数量的goroutine，使用channel，让当前goroutine处于阻塞状态，当有task往通道里传输，然后进行处理。&lt;/p&gt;

&lt;p&gt;将请求放入队列,通过一定数量(例如CPU核心数)goroutine组成一个worker池(pool),workder池中的worker读取队列执行任务，这样可以处理百万级请求并且控制并发量不会崩溃。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;工作者工作协程，挂入调度器，取Job，执行Job，周而复始&lt;/li&gt;
&lt;li&gt;调度器，从Job队列取Job，分配给工作者，周而复始&lt;/li&gt;
&lt;li&gt;web响应里，模拟了客户的请求-Job，并将此Job放入Job队列，只有有客户端请求，就周而复始的工作&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;工作池实现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先，我们定义一个job的接口, 具体内容由具体job实现；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Job interface {
    Do() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后定义一下job队列和work池类型，这里我们work池也用golang的channel实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// define job channel
type JobChan chan Job

// define worker channer
type WorkerChan chan JobChan
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们分别维护一个全局的job队列和工作池。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    JobQueue          JobChan
    WorkerPool        WorkerChan
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;worker的实现。每一个worker都有一个job channel，在启动worker的时候会被注册到work pool中。启动后通过自身的job channel取到job并执行job。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Worker struct {
    JobChannel JobChan
    quit       chan bool
}

func (w *Worker) Start() {
    go func() {
        for {
            // regist current job channel to worker pool
            WorkerPool &amp;lt;- w.JobChannel
            select {
            case job := &amp;lt;-w.JobChannel:
                if err := job.Do(); err != nil {
                    fmt.printf(&amp;quot;excute job failed with err: %v&amp;quot;, err)
                }
            // recieve quit event, stop worker
            case &amp;lt;-w.quit:
                return
            }
        }
    }()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现一个分发器（Dispatcher）。分发器包含一个worker的指针数组，启动时实例化并启动最大数目的worker，然后从job队列中不断取job选择可用的worker来执行job。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Dispatcher struct {
    quit    chan bool
}

func (d *Dispatcher) Run() {
    for i := 0; i &amp;lt; MaxWorkerPoolSize; i++ {
        worker := NewWorker()
        worker.Start()
    }

    for {
        select {
        case job := &amp;lt;-JobQueue:
            go func(job Job) {
                jobChan := &amp;lt;-WorkerPool
                jobChan &amp;lt;- job
            }(job)
        // stop dispatcher
        case &amp;lt;-d.quit:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;net/http&amp;quot;
    &amp;quot;fmt&amp;quot;
)

type Job struct {
    request string
}

func (j *Job)Handle(){
    fmt.Println(&amp;quot;test&amp;quot;)
}

type worker struct {
    work  JobChan
    quit chan bool
}

func (w *worker)start(i int)  {
    fmt.Println(&amp;quot;start worker:&amp;quot;,i)
    go func(i int) {
        for {
            fmt.Println(&amp;quot;add free worklist&amp;quot;)
            workList &amp;lt;- w.work
            select {
            case Task := &amp;lt;- w.work:
                fmt.Println(&amp;quot;worker&amp;quot;,i,&amp;quot;handle job .....&amp;quot;)
                Task.Handle()
                fmt.Println(&amp;quot;worker&amp;quot;,i,&amp;quot;handle over .....&amp;quot;)
            case &amp;lt;- w.quit:
                return
            }
        }
    }(i)

}



type schedule struct {
    quit chan bool
}

func newWorker() *worker {
    workchan := make(chan Job,1)
    return &amp;amp;worker{work:workchan}
}


func (s *schedule)schedule() {
    workList = make(chan JobChan,10)
    fmt.Println(&amp;quot;start pool&amp;quot;)
    for i := 0; i &amp;lt; 10; i++ {
        w := newWorker()
        w.start(i)
    }

    for {
        fmt.Println(&amp;quot;get task and get worker&amp;quot;)
        select {
        case job := &amp;lt;-queue:
            go func(job Job) {
                fmt.Println(&amp;quot;get worker&amp;quot;)
                jobChan := &amp;lt;-workList
                fmt.Println(&amp;quot;insert task into job&amp;quot;)
                jobChan &amp;lt;- job
            }(job)
            // stop dispatcher
        case &amp;lt;-s.quit:
            return
        }
    }


}

//define type queue and work
type JobChan chan Job
type WorkChan chan JobChan

//任务队列
var queue JobChan
//工作池用队列实现
var workList WorkChan

func newschedule() schedule  {
    fmt.Println(&amp;quot;newschedule&amp;quot;)
    return schedule{}
}


func init(){
    s := newschedule()
    go s.schedule()
}

func main()  {
    fmt.Println(&amp;quot;main&amp;quot;)
    queue = make(chan Job,1024)

    http.HandleFunc(&amp;quot;/metrics&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
        job := Job{&amp;quot;test&amp;quot;}
        queue &amp;lt;- job
    })


    fmt.Println(&amp;quot;start sueccess and listen at 9000!!&amp;quot;)
    http.ListenAndServe(&amp;quot;localhost:9000&amp;quot;,nil)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;异步&#34;&gt;异步&lt;/h2&gt;

&lt;p&gt;异步处理并不算一种并发的使用方式，但是却是并发中经常使用的，在工作池的基础上使用goroutine处理，但是不用等返回，留一个channenl返回，使用select读取channel中的数据，完成处理，这样可以加大处理的速度，也就提高了并发能力。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- K8s API</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-api/</link>
          <pubDate>Sat, 16 May 2020 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-api/</guid>
          <description>&lt;p&gt;为了方便查阅 API 接口的详细定义，Kubernetes 使用了 swagger-ui 提供 API 在线查询功能，其官网为 &lt;a href=&#34;http://kubernetes.kansea.com/docs/api-reference/v1/operations/，&#34;&gt;http://kubernetes.kansea.com/docs/api-reference/v1/operations/，&lt;/a&gt; Kubernetes开发团队会定期更新、生成 UI 及文档。Swagger UI 是一款 REST API 文档在线自动生成和功能测试软件，关于 Swagger 的内容请访问官网 &lt;a href=&#34;http://swagger.io。&#34;&gt;http://swagger.io。&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;api设计哲学&#34;&gt;API设计哲学&lt;/h1&gt;

&lt;p&gt;运行在 Master 节点上的 API Server 进程同时提供了 swagger-ui 的访问地址：http://&lt;master-ip&gt;: &lt;master-port&gt;/swagger-ui/。假设我们的 API Server 安装在 192.168.1.128 服务器上，绑定了 8080 端口，则可以通过访问 &lt;a href=&#34;http://192.168.1.128:8080/swagger-ui/&#34;&gt;http://192.168.1.128:8080/swagger-ui/&lt;/a&gt; 来查看 API 信息。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;声明式风格的API设计&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;声明性API中，你声明系统要执行的操作，系统将不断向该状态驱动。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;命令式API的设计&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;命令式API中，直接发出服务器要执行的命令，例如： “运行容器”、“停止容器”等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;api对象&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;k8s的api采用了声明式的api设计，并且在在etcd中都有完整的资源路径。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/api2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;完整的资源路径是由：Group（API组）、Version（API版本）和Resource（API资源类型）三个部分组成的。&lt;/p&gt;

&lt;h1 id=&#34;api&#34;&gt;API&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;版本和资源对象&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;curl localhost:8080/api  #查看kubernetes API的版本信息
curl localhost:8080/api/v1  #查看kubernetes API支持的所有的资源对象
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;具体的资源操作&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、首先要找到具体的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;localhost:8080/api/v1/资源对象（ns，pod，service）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、然后不同的资源需要不同的处理&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;node&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;node是以name来进行资源划分的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;分类          说明                      方法  API
查   list or watch objects of kind Node      GET     /api/v1/nodes
    read the specified Node                 GET     /api/v1/nodes/{name}
增   create a Node                           POST    /api/v1/nodes
删   delete a Node                           DELETE  /api/v1/nodes/{name}
改   replace the specified Node              PUT     /api/v1/nodes/{name}
    partially update the specified Node P   ATCH    /api/v1/nodes/{name}
    replace status of the specified Node    PUT     /api/v1/nodes/{name}/status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;namespace&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;namespace也是以name来进行资源划分的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;分类      说明                                          方法  API
查       list or watch objects of kind Namespace             GET     /api/v1/namespaces
        read the specified Namespace                        GET     /api/v1/namespaces/{name}
增       create a Namespace                                  POST    /api/v1/namespaces
删       delete a Namespace                                  DELETE  /api/v1/namespaces/{name}
改       replace the specified Namespace                     PUT     /api/v1/namespaces/{name}
        partially update the specified Namespace            PATCH   /api/v1/namespaces/{name}
        replace finalize of the specified Namespace         PUT     /api/v1/namespaces/{name}/finalize
        replace status of the specified Namespace           PUT     /api/v1/namespaces/{name}/status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;endpoint&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Endpoints是以Namespace维度划分资源，然后结合name来区分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;分类      说明                                          方法  API
查       list or watch objects of kind Endpoints             GET     /api/v1/endpoints
        list or watch objects of kind Endpoints             GET     /api/v1/namespaces/{namespace}/endpoints
        read the specified Endpoints                        GET     /api/v1/namespaces/{namespace}/endpoints/{name}
增       create a Endpoints                                  POST    /api/v1/namespaces/{namespace}/endpoints
删       delete a Endpoints                                  DELETE  /api/v1/namespaces/{namespace}/endpoints/{name}
改       replace the specified Endpoints                     PUT     /api/v1/namespaces/{namespace}/endpoints/{name}
        partially update the specified Endpoints            PATCH   /api/v1/namespaces/{namespace}/endpoints/{name}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;pod&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;pod是以Namespace维度划分资源，然后结合name来区分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;分类          说明                              方法  API
查           list or watch objects of kind Pod       GET /api/v1/namespaces/{namespace}/pods
            read the specified Pod                  GET /api/v1/namespaces/{namespace}/pods/{name}
增           create a Pod                            POST    /api/v1/namespaces/{namespace}/pods
删           delete a Pod                            DELETE  /api/v1/namespaces/{namespace}/pods/{name}
改           replace the specified Pod               PUT /api/v1/namespaces/{namespace}/pods/{name}
            partially update the specified Pod      PATCH   /api/v1/namespaces/{namespace}/pods/{name}

            connect GET requests to attach of Pod   GET /api/v1/namespaces/{namespace}/pods/{name}/attach
            connect POST requests to attach of Pod  POST    /api/v1/namespaces/{namespace}/pods/{name}/attach
            create binding of a Binding             POST    /api/v1/namespaces/{namespace}/pods/{name}/binding
            connect GET requests to exec of Pod     GET /api/v1/namespaces/{namespace}/pods/{name}/exec
            connect POST requests to exec of Pod    POST    /api/v1/namespaces/{namespace}/pods/{name}/exec
            read log of the specified Pod           GET /api/v1/namespaces/{namespace}/pods/{name}/log
            connect GET requests to portforward of Pod  GET /api/v1/namespaces/{namespace}/pods/{name}/portforward
            connect POST requests to portforward of Pod POST    /api/v1/namespaces/{namespace}/pods/{name}/portforward
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Service&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;分类              说明                                      方法          API
查               list or watch objects of kind Service           GET             /api/v1/namespaces/{namespace}/services
                read the specified Service                      GET             /api/v1/namespaces/{namespace}/services/{name}
增               create a Service                                POST            /api/v1/namespaces/{namespace}/services
删               delete a Service                                DELETE          /api/v1/namespaces/{namespace}/services/{name}
改               replace the specified Service                   PUT             /api/v1/namespaces/{namespace}/services/{name}
                partially update the specified Service          PATCH           /api/v1/namespaces/{namespace}/services/{name}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ReplicationController&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;分类              说明                                                  方法      API
查                   list or watch objects of kind ReplicationController     GET         /api/v1/namespaces/{namespace}/replicationcontrollers
                    read the specified ReplicationController                GET         /api/v1/namespaces/{namespace}/replicationcontrollers/{name}
增                   create a ReplicationController                          POST        /api/v1/namespaces/{namespace}/replicationcontrollers
删                   delete a ReplicationController                          DELETE      /api/v1/namespaces/{namespace}/replicationcontrollers/{name}
改                   replace the specified ReplicationController             PUT         /api/v1/namespaces/{namespace}/replicationcontrollers/{name}
                    partially update the specified ReplicationController    PATCH       /api/v1/namespaces/{namespace}/replicationcontrollers/{name}
                    replace status of the specified ReplicationController   PUT         /api/v1/namespaces/{namespace}/replicationcontrollers/{name}/status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实大多都差不多相似，照着规律就行&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;proxy接口&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;kubernetes API server还提供了一类很特殊的rest接口—proxy接口，这个结构就是代理REST请求，即kubernetes API server把收到的rest请求转发到某个node上的kubelet守护进程的rest端口上，由该kubelet进程负责相应。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;node
    masterIP:8080/api/v1/proxy/nodes/{node_name}/pods  #某个节点下所有pod信息
    masterIP:8080/api/v1/proxy/nodes/{node_name}/stats  #某个节点内物理资源的统计信息
    masterIP:8080/api/v1/proxy/nodes/{node_name}/spec  #某个节点的概要信息
pod
    masterIP:8080/api/v1/proxy/namespaces/{namespace}/pods/{pod_name}/{path:*} #访问pod的某个服务接口
    masterIP:8080/api/v1/proxy/namespaces/{namespace}/pods/{pod_name}  #访问pod
service
    masterIP:8080/api/v1/proxy/namespaces/{namespace}/services/{service_name}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以参考上面的资源操作方式，同样的可以进行操作。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;GET /&amp;lt; 资源名的复数格式 &amp;gt;：获得某一类型的资源列表，例如 GET /pods 返回一个 Pod 资源列表。&lt;/li&gt;
&lt;li&gt;POST /&amp;lt; 资源名的复数格式 &amp;gt;：创建一个资源，该资源来自用户提供的 JSON 对象。&lt;/li&gt;
&lt;li&gt;GET /&amp;lt; 资源名复数格式 &amp;gt;/&amp;lt; 名字 &amp;gt;：通过给出的名称（Name）获得单个资源，例如 GET /pods/first 返回一个名称为“first”的 Pod。&lt;/li&gt;
&lt;li&gt;DELETE /&amp;lt; 资源名复数格式 &amp;gt;/&amp;lt; 名字 &amp;gt;：通过给出的名字删除单个资源，删除选项（DeleteOptions）中可以指定的优雅删除（Grace Deletion）的时间（GracePeriodSeconds），该可选项表明了从服务端接收到删除请求到资源被删除的时间间隔（单位为秒）。不同的类别（Kind）可能为优雅删除时间（Grace Period）申明默认值。用户提交的优雅删除时间将覆盖该默认值，包括值为 0 的优雅删除时间。&lt;/li&gt;
&lt;li&gt;PUT /&amp;lt; 资源名复数格式 &amp;gt;/&amp;lt; 名字 &amp;gt;：通过给出的资源名和客户端提供的 JSON 对象来更新或创建资源。&lt;/li&gt;
&lt;li&gt;PATCH /&amp;lt; 资源名复数格式 &amp;gt;/&amp;lt; 名字 &amp;gt;：选择修改资源详细指定的域。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;patch&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于 PATCH 操作，目前 Kubernetes API 通过相应的 HTTP 首部“Content-Type”对其进行识别。&lt;/p&gt;

&lt;p&gt;目前支持以下三种类型的 PATCH 操作。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;JSON Patch, Content-Type: application/json-patch+json。在 RFC6902 的定义中，JSON Patch 是执行在资源对象上的一系列操作，例如 {＂op＂: ＂add＂, ＂path＂: ＂/a/b/c＂, ＂value＂: [ ＂foo＂, ＂bar＂ ]}。详情请查看 RFC6902 说明，网址为 &lt;a href=&#34;HTTPs://tools.ietf.org/html/rfc6902。&#34;&gt;HTTPs://tools.ietf.org/html/rfc6902。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Merge Patch, Content-Type: application/merge-json-patch+json。在 RFC7386 的定义中，Merge Patch 必须包含对一个资源对象的部分描述，这个资源对象的部分描述就是一个 JSON 对象。该 JSON 对象被提交到服务端，并和服务端的当前对象合并，从而创建一个新的对象。详情请查看 RFC73862 说明，网址为 &lt;a href=&#34;HTTPs://tools.ietf.org/html/rfc7386。&#34;&gt;HTTPs://tools.ietf.org/html/rfc7386。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Strategic Merge Patch, Content-Type: application/strategic-merge-patch+json。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Strategic Merge Patch 是一个定制化的 Merge Patch 实现。接下来将详细讲解 Strategic Merge Patch。&lt;/p&gt;

&lt;p&gt;在标准的 JSON Merge Patch 中，JSON 对象总是被合并（merge）的，但是资源对象中的列表域总是被替换的。通常这不是用户所希望的。例如，我们通过下列定义创建一个 Pod 资源对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:

  containers:

    - name: nginx

      image: nginx-1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着我们希望添加一个容器到这个 Pod 中，代码和上传的 JSON 对象如下所示：&lt;/p&gt;

&lt;p&gt;PATCH /api/v1/namespaces/default/pods/pod-name&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:

  containers:

    - name: log-tailer

      image: log-tailer-1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我们使用标准的 Merge Patch，则其中的整个容器列表将被单个的“log-tailer”容器所替换。然而我们的目的是两个容器列表能够合并。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，Strategic Merge Patch 通过添加元数据到 API 对象中，并通过这些新元数据来决定哪个列表被合并，哪个列表不被合并。当前这些元数据作为结构标签，对于 API 对象自身来说是合法的。对于客户端来说，这些元数据作为 Swagger annotations 也是合法的。在上述例子中，向“containers”中添加“patchStrategy”域，且它的值为“merge”，通过添加“patchMergeKey”，它的值为“name”。也就是说，“containers”中的列表将会被合并而不是替换，合并的依据为“name”域的值。&lt;/p&gt;

&lt;p&gt;其实patch在使用过程中还是比较难以操作的，所以在修改的时候，我们更多的是使用直接修改文件的方式来进行更新&lt;/p&gt;

&lt;p&gt;1、在线编辑&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl edit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、离线编辑&lt;/p&gt;

&lt;p&gt;导出yaml文件，然后修改文件，进行apply&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;watch&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;此外，Kubernetes API 添加了资源变动的“观察者”模式的 API 接口。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GET /watch/&amp;lt; 资源名复数格式 &amp;gt;：随时间变化，不断接收一连串的 JSON 对象，这些 JSON 对象记录了给定资源类别内所有资源对象的变化情况。&lt;/li&gt;
&lt;li&gt;GET /watch/&amp;lt; 资源名复数格式 &amp;gt;/&lt;name&gt;：随时间变化，不断接收一连串的 JSON 对象，这些 JSON 对象记录了某个给定资源对象的变化情况。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上述接口改变了返回数据的基本类别，watch 动词返回的是一连串的 JSON 对象，而不是单个的 JSON 对象。并不是所有的对象类别都支持“观察者”模式的 API 接口，在后续的章节中将会说明哪些资源对象支持这种接口。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;另外，Kubernetes 还增加了 HTTP Redirect 与 HTTP Proxy 这两种特殊的 API 接口，前者实现资源重定向访问，后者则实现 HTTP 请求的代理。&lt;/p&gt;

&lt;h1 id=&#34;顶级api对象&#34;&gt;顶级API对象&lt;/h1&gt;

&lt;p&gt;API对象也就是我们常用的post的body，也就是我们使用的yaml文件或json文件的组成，符合声明式api的设计。&lt;/p&gt;

&lt;p&gt;所以也可以直接是说是我们使用的yaml文件的组成。其实最后yaml文件都是转化为结构体进行操作的，所以最终都是符合api的设计，重这边就能知道对应的yaml组成了。&lt;/p&gt;

&lt;p&gt;在 Kubernetes API 中，一个 API 的顶层（Top Level）元素由 kind、apiVersion、metadata、spec 和 status 等几个部分组成，接下来，我们分别对这几个部分进行说明。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kind&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;kind 表明对象有以下三大类别。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对象（objects）：代表在系统中的一个永久资源（实体），例如 Pod、RC、Service、Namespace 及 Node 等。通过操作这些资源的属性，客户端可以对该对象做创建、修改、删除和获取操作。&lt;/li&gt;
&lt;li&gt;列表（list）：一个或多个资源类别的集合。列表有一个通用元数据的有限集合。所有列表（lists）通过“items”域获得对象数组。例如 PodLists、ServiceLists、NodeLists。大部分定义在系统中的对象都有一个返回所有资源（resource）集合的端点，以及零到多个返回所有资源集合的子集的端点。某些对象有可能是单例对象（singletons），例如当前用户、系统默认用户等，这些对象没有列表。&lt;/li&gt;
&lt;li&gt;简单类别（simple）：该类别包含作用在对象上的特殊行为和非持久实体。该类别限制了使用范围，它有一个通用元数据的有限集合，例如 Binding、 Status。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;apiversion&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;apiVersion 表明 API 的版本号，当前版本默认只支持 v1。&lt;/p&gt;

&lt;p&gt;为了在兼容旧版本的同时不断升级新的 API，Kubernetes 提供了多版本 API 的支持能力，每个版本的 API 通过一个版本号路径前缀进行区分，例如 /api/v1beta3。通常情况下，新旧几个不同的 API 版本都能涵盖所有的 Kubernetes 资源对象，在不同的版本之间这些 API 接口存在一些细微差别。Kubernetes 开发团队基于 API 级别选择版本而不是基于资源和域级别，是为了确保 API 能够描述一个清晰的连续的系统资源和行为的视图，能够控制访问的整个过程和控制实验性 API 的访问。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Metadata&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Metadata 是资源对象的元数据定义，是集合类的元素类型，包含一组由不同名称定义的属性。在 Kubernetes 中每个资源对象都必须包含以下 3 种 Metadata。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;namespace：对象所属的命名空间，如果不指定，系统则会将对象置于名为“default”的系统命名空间中。&lt;/li&gt;
&lt;li&gt;name：对象的名字，在一个命名空间中名字应具备唯一性。&lt;/li&gt;
&lt;li&gt;uid：系统为每个对象生成的唯一 ID，符合 RFC 4122 规范的定义。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此外，每种对象还应该包含以下几个重要元数据。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;labels：用户可定义的“标签”，键和值都为字符串的 map，是对象进行组织和分类的一种手段，通常用于标签选择器（Label Selector），用来匹配目标对象。&lt;/li&gt;
&lt;li&gt;annotations：用户可定义的“注解”，键和值都为字符串的 map，被 Kubernetes 内部进程或者某些外部工具使用，用于存储和获取关于该对象的特定元数据。&lt;/li&gt;
&lt;li&gt;resourceVersion：用于识别该资源内部版本号的字符串，在用于 Watch 操作时，可以避免在 GET 操作和下一次 Watch 操作之间造成的信息不一致，客户端可以用它来判断资源是否改变。该值应该被客户端看作不透明，且不做任何修改就返回给服务端。客户端不应该假定版本信息具有跨命名空间、跨不同资源类别、跨不同服务器的含义。&lt;/li&gt;
&lt;li&gt;creationTimestamp：系统记录创建对象时的时间戳，符合 RFC 3339 规范。&lt;/li&gt;
&lt;li&gt;deletionTimestamp：系统记录删除对象时的时间戳，符合 RFC 3339 规范。&lt;/li&gt;
&lt;li&gt;selfLink：通过 API 访问资源自身的 URL，例如一个 Pod 的 link 可能是 /api/v1/namespaces/ default/pods/frontend-o8bg4。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;spec&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;spec 是集合类的元素类型，用户对需要管理的对象进行详细描述的主体部分都在 spec 里给出，它会被 Kubernetes 持久化到 etcd 中保存，系统通过 spec 的描述来创建或更新对象，以达到用户期望的对象运行状态。spec 的内容既包括用户提供的配置设置、默认值、属性的初始化值，也包括在对象创建过程中由其他相关组件（例如 schedulers、auto-scalers）创建或修改的对象属性，比如 Pod 的 Service IP 地址。如果 spec 被删除，那么该对象将会从系统中被删除。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;status&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Status 用于记录对象在系统中的当前状态信息，它也是集合类元素类型，status 在一个自动处理的进程中被持久化，可以在流转的过程中生成。如果观察到一个资源丢失了它的状态（Status），则该丢失的状态可能被重新构造。以 Pod 为例，Pod 的 status 信息主要包括 conditions、containerStatuses、hostIP、phase、podIP、startTime 等。其中比较重要的两个状态属性如下。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;phase：描述对象所处的生命周期阶段，phase 的典型值是“Pending”（创建中）“Running”“Active”（正在运行中）或“Terminated”（已终结），这几种状态对于不同的对象可能有轻微的差别，此外，关于当前 phase 附加的详细说明可能包含在其他域中。&lt;/li&gt;
&lt;li&gt;condition：表示条件，由条件类型和状态值组成，目前仅有一种条件类型 Ready，对应的状态值可以为 True、False 或 Unknown。一个对象可以具备多种 condition，而 condition 的状态值也可能不断发生变化，condition 可能附带一些信息，例如最后的探测时间或最后的转变时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;crd-自定义资源类型&#34;&gt;CRD(自定义资源类型)&lt;/h1&gt;

&lt;p&gt;Aggregation API和crd都可以在不修改k8s核心代码前提下，并扩展k8s api，是一种实现方式，Aggregation API我们下面讲，先说说crd。&lt;/p&gt;

&lt;p&gt;Kubernetes 1.7 之后增加了对 CRD 自定义资源二次开发能力来扩展 Kubernetes API，通过 CRD 我们可以向 Kubernetes API 中增加新资源类型，而不需要修改 Kubernetes 源码来创建自定义的 API server，该功能大大提高了 Kubernetes 的扩展能力。
当你创建一个新的CustomResourceDefinition (CRD)时，Kubernetes API服务器将为你指定的每个版本创建一个新的RESTful资源路径，我们可以根据该api路径来创建一些我们自己定义的类型资源。CRD可以是命名空间的，也可以是集群范围的，由CRD的作用域(scpoe)字段中所指定的，与现有的内置对象一样，删除名称空间将删除该名称空间中的所有自定义对象。customresourcedefinition本身没有名称空间，所有名称空间都可以使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过crd资源创建自定义资源，即自定义一个Restful API和资源类型contab&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;创建自定义contab资源类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f resourcedefinition.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;resourcedefinition.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  # 名称必须与下面的spec字段匹配，格式为: &amp;lt;plural&amp;gt;.&amp;lt;group&amp;gt;
  name: crontabs.stable.example.com
spec:
  # 用于REST API的组名称: /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;
  group: stable.example.com
  # 此CustomResourceDefinition支持的版本列表
  versions:
    - name: v1
      # 每个版本都可以通过服务标志启用/禁用。
      served: true
      # 必须将一个且只有一个版本标记为存储版本。
      storage: true
  # 指定crd资源作用范围在命名空间或集群
  scope: Namespaced
  names:
    # URL中使用的复数名称: /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;/&amp;lt;plural&amp;gt;
    plural: crontabs
    # 在CLI(shell界面输入的参数)上用作别名并用于显示的单数名称
    singular: crontab
    # kind字段使用驼峰命名规则. 资源清单使用如此
    kind: CronTab
    # 短名称允许短字符串匹配CLI上的资源，意识就是能通过kubectl 在查看资源的时候使用该资源的简名称来获取。
    shortNames:
    - ct
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在以下位置创建一个新的带有名称空间的RESTful API端点:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/apis/stable.example.com/v1/namespaces/*/crontabs/...然后我们可以使用该url来创建和管理自定义对象资源。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看自定义contab资源的信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get contab(ct)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;刚刚自定义了ct类型，我们可以创建这种kind的资源实例了。&lt;/p&gt;

&lt;p&gt;my-crontab.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: &amp;quot;stable.example.com/v1&amp;quot;
kind: CronTab
metadata:
  name: my-new-cron-object
spec:
  cronSpec: &amp;quot;* * * * */5&amp;quot;
  image: my-awesome-cron-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建自定义资源contab资源的对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f my-crontab.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get contab(ct)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;验证&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;修改resourcedefinition.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: crontabs.stable.example.com
spec:
  group: stable.example.com
  versions:
    - name: v1
      served: true
      storage: true
  version: v1
  scope: Namespaced
  names:
    plural: crontabs
    singular: crontab
    kind: CronTab
    shortNames:
    - ct
  validation:
   # openAPIV3Schema is the schema for validating custom objects.
    openAPIV3Schema:
      properties:
        spec:
          properties:
            cronSpec: #--必须是字符串，并且必须是正则表达式所描述的形式
              type: string
              pattern: &#39;^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$&#39;
            replicas: #----必须是整数，最小值必须为1，最大值必须为10
              type: integer
              minimum: 1
              maximum: 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到validation就是验证规则。&lt;/p&gt;

&lt;p&gt;这个适合如果创建下面这个类型的实例就会失败&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: &amp;quot;stable.example.com/v1&amp;quot;
kind: CronTab
metadata:
  name: my-new-cron-object
spec:
  cronSpec: &amp;quot;* * * *&amp;quot;
  image: my-awesome-cron-image
  replicas: 15
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;新增打印信息&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在crd文件中添加“additionalPrinterColumns:”字段声明&lt;/p&gt;

&lt;p&gt;resourcedefinition.yaml：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: crontabs.stable.example.com
  spec:
    group: stable.example.com
    version: v1
    scope: Namespaced
    names:
      plural: crontabs
      singular: crontab
      kind: CronTab
      shortNames:
      - ct
    additionalPrinterColumns:
    - name: Spec
      type: string
      description: The cron spec defining the interval a CronJob is run
      JSONPath: .spec.cronSpec
    - name: Replicas
      type: integer
      description: The number of jobs launched by the CronJob
      JSONPath: .spec.replicas
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看自定义资源对象基本信息，可以发现多了Spec，Replicas，Age。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get crontab my-new-cron-object

NAME                       SPEC        REPLICAS   AGE
my-new-cron-object   * * * * *            1         7s
注意：name列不需要定义默认会有的
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;为自定义的资源添加状态和伸缩配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;subresources: 字段来声明状态和伸缩信息。&lt;/p&gt;

&lt;p&gt;resourcedefinition.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: crontabs.stable.example.com
spec:
  group: stable.example.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Namespaced
  names:
    plural: crontabs
    singular: crontab
    kind: CronTab
    shortNames:
    - ct
  # 自定义资源的子资源的描述
  subresources:
    # 启用状态子资源。
    status: {}
    # 启用scale子资源
    scale:
      specReplicasPath: .spec.replicas
      statusReplicasPath: .status.replicas
      labelSelectorPath: .status.labelSelector
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以通过scale来扩缩副本数量了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale --replicas=5 crontabs/my-new-cron-object
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面就是crd的基本用法，其实crd主要是用于自定义controller的开发的第一步，我们还需要controller来控制，所以还需要了解二次开发的&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller/&#34;&gt;控制器开发&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;kubernetes-aggregated-api-servers&#34;&gt;kubernetes Aggregated API Servers&lt;/h1&gt;

&lt;p&gt;Aggregated（聚合的）API server是为了将原来的API server这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的API server集成进来，而不用直接修改kubernetes官方仓库的代码，这样一来也能将API server解耦，方便用户使用实验特性。这些API server可以跟core API server无缝衔接，使用kubectl也可以管理它们。&lt;/p&gt;

&lt;p&gt;Aggregated API是允许k8s的开发人员编写一个自己的服务，可以把这个服务注册到k8s的api里面，这样，就像k8s自己的api一样，你的服务只要运行在k8s集群里面，k8s 的Aggregate通过service名称就可以转发到你写的service里面去了。&lt;/p&gt;

&lt;p&gt;一句话阐述就是：动态注册、发现汇总、和安全代理。&lt;/p&gt;

&lt;p&gt;这时候，我们需要创建一个新的组件，名为kube-aggregator，它需要负责以下几件事：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;提供用于注册API server的API&lt;/li&gt;
&lt;li&gt;汇总所有的API server信息&lt;/li&gt;
&lt;li&gt;代理所有的客户端到API server的请求&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：这里说的API server是一组“API Server”，而不是说我们安装集群时候的那个APIserver组件，而且这组API server是可以横向扩展的。&lt;/p&gt;

&lt;p&gt;这种设计理念有着很多的好处：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一是增加了api的扩展性，这样k8s的开发人员就可以编写自己的API服务器来公开他们想要的API。集群管理员应该能够使用这些服务，而不需要对核心库存储库进行任何更改。&lt;/li&gt;
&lt;li&gt;第二是丰富了APIs，核心kubernetes团队阻止了很多新的API提案。通过允许开发人员将他们的API作为单独的服务器公开，并使集群管理员能够在不对核心库存储库进行任何更改的情况下使用它们，这样就无须社区繁杂的审查了&lt;/li&gt;
&lt;li&gt;第三是开发分阶段实验性API的地方，新的API可以在单独的聚集服务器中开发，当它稳定之后，那么把它们封装起来安装到其他集群就很容易了。&lt;/li&gt;
&lt;li&gt;第四是确保新API遵循kubernetes约定：如果没有这里提出的机制，社区成员可能会被迫推出自己的东西，这可能会或可能不遵循kubernetes约定。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有两种方式来启用kube-aggregator：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;使用test mode/single-user mode，作为一个独立的进程来运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kube-aggregator二进制文件已经包含在kubernetes release里面了。
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用gateway mode，kube-apiserver集成了kube-aggregator组件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;开启 Aggregator，只要修改apiserver的启动参数就好

    --requestheader-client-ca-file=&amp;lt;path to aggregator CA cert&amp;gt;
    --requestheader-allowed-names=aggregator
    --requestheader-extra-headers-prefix=X-Remote-Extra-
    --requestheader-group-headers=X-Remote-Group
    --requestheader-username-headers=X-Remote-User
    --proxy-client-cert-file=&amp;lt;path to aggregator proxy cert&amp;gt;
    --proxy-client-key-file=&amp;lt;path to aggregator proxy key&amp;gt;
    --enable-aggregator-routing=true

Kubeadm 搭建的集群默认已经开启了，minikube也是。其实就是配置了一些证书，用于自定义的apiservice和kube-apiserver进行通信，因为是机密的需要证书认证。
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来看看基本使用，先看一个资源配置清单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apiregistration.k8s.io/v1beta1
kind: APIService
metadata:
  name: v1alpha1.custom-metrics.metrics.k8s.io
spec:
  insecureSkipTLSVerify: true
  group: custom-metrics.metrics.k8s.io
  groupPriorityMinimum: 1000
  versionPriority: 15
  service:
    name: api
    namespace: custom-metrics
  version: v1alpha1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面就定义了资源类型为APIService，service名称为api，空间为custom-metrics的一个资源聚合接口。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用apiregistration.k8s.io/v1beta1 版本的APIService，在metadata.name中定义该API的名字。&lt;/li&gt;
&lt;li&gt;insecureSkipTLSVerify：当与该服务通信时，禁用TLS证书认证。强加建议不要设置这个参数，默认为 false。应该使用CABundle代替。&lt;/li&gt;
&lt;li&gt;service：与该APIService通信时引用的service，其中要注明service的名字和所属的namespace，如果为空的话，则所有的服务都会该API groupversion将在本地443端口处理所有通信。&lt;/li&gt;
&lt;li&gt;groupPriorityMinimum：该组API的处理优先级，主要排序是基于groupPriorityMinimum，该数字越大表明优先级越高，客户端就会与其通信处理请求。次要排序是基于字母表顺序，例如v1.bar比v1.foo的优先级更高。&lt;/li&gt;
&lt;li&gt;versionPriority：VersionPriority控制其组内的API版本的顺序。必须大于零。主要排序基于VersionPriority，从最高到最低（20大于10）排序。次要排序是基于对象名称的字母比较。 （v1.foo在v1.bar之前）由于它们都是在一个组内，因此数字可能很小，一般都小于10。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以看看官方的模版&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: &amp;lt; 注释对象名称 &amp;gt;
spec:
  group: &amp;lt; 拓展 Apiserver 的 API group 名称 &amp;gt;
  version: &amp;lt; 拓展 Apiserver 的 API version&amp;gt;
  groupPriorityMinimum: &amp;lt; APIService 对对应 group 的优先级, 参考 API 文档 &amp;gt;
  versionPriority: &amp;lt; 优先考虑 version 在 group 中的排序, 参考 API 文档 &amp;gt;
  service:
    namespace: &amp;lt; 拓展 Apiserver 服务的 namespace &amp;gt;
    name: &amp;lt; 拓展 Apiserver 服务的 name &amp;gt;
  caBundle: &amp;lt; PEM 编码的 CA 证书，用于对 Webhook 服务器的证书签名 &amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要注册 API，用户必须添加一个 APIService 对象，用它来申领 Kubernetes API 中的 URL 路径。自此以后，聚合层将会把发给该 API 路径的所有内容（例如 /apis/myextension.mycompany.io/v1/…）代理到已注册的 APIService。&lt;/p&gt;

&lt;p&gt;查看我们使用上面的yaml文件创建的APIService。&lt;/p&gt;

&lt;p&gt;下面从源代码的角度来看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pkg/apiserver/apiservice_controller.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和k8s其它controller一样，watch变化分发到add、update和delete方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiServiceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
    AddFunc:    c.addAPIService,
    UpdateFunc: c.updateAPIService,
    DeleteFunc: c.deleteAPIService,
})

serviceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
    AddFunc:    c.addService,
    UpdateFunc: c.updateService,
    DeleteFunc: c.deleteService,
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要监听两种资源apiService和service，分别看看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *APIAggregator) AddAPIService(apiService *apiregistration.APIService) error {
    // if the proxyHandler already exists, it needs to be updated. The aggregation bits do not
    // since they are wired against listers because they require multiple resources to respond
    if proxyHandler, exists := s.proxyHandlers[apiService.Name]; exists {
        proxyHandler.updateAPIService(apiService)
        if s.openAPIAggregationController != nil {
            s.openAPIAggregationController.UpdateAPIService(proxyHandler, apiService)
        }
        return nil
    }

    proxyPath := &amp;quot;/apis/&amp;quot; + apiService.Spec.Group + &amp;quot;/&amp;quot; + apiService.Spec.Version
    // v1. is a special case for the legacy API.  It proxies to a wider set of endpoints.
    if apiService.Name == legacyAPIServiceName {
        proxyPath = &amp;quot;/api&amp;quot;
    }

    // register the proxy handler
    proxyHandler := &amp;amp;proxyHandler{
        contextMapper:   s.contextMapper,
        localDelegate:   s.delegateHandler,
        proxyClientCert: s.proxyClientCert,
        proxyClientKey:  s.proxyClientKey,
        proxyTransport:  s.proxyTransport,
        serviceResolver: s.serviceResolver,
    }
    proxyHandler.updateAPIService(apiService)
    if s.openAPIAggregationController != nil {
        s.openAPIAggregationController.AddAPIService(proxyHandler, apiService)
    }
    s.proxyHandlers[apiService.Name] = proxyHandler
    s.GenericAPIServer.Handler.NonGoRestfulMux.Handle(proxyPath, proxyHandler)
    s.GenericAPIServer.Handler.NonGoRestfulMux.UnlistedHandlePrefix(proxyPath+&amp;quot;/&amp;quot;, proxyHandler)

    // if we&#39;re dealing with the legacy group, we&#39;re done here
    if apiService.Name == legacyAPIServiceName {
        return nil
    }

    // if we&#39;ve already registered the path with the handler, we don&#39;t want to do it again.
    if s.handledGroups.Has(apiService.Spec.Group) {
        return nil
    }

    // it&#39;s time to register the group aggregation endpoint
    groupPath := &amp;quot;/apis/&amp;quot; + apiService.Spec.Group
    groupDiscoveryHandler := &amp;amp;apiGroupHandler{
        codecs:        Codecs,
        groupName:     apiService.Spec.Group,
        lister:        s.lister,
        delegate:      s.delegateHandler,
        contextMapper: s.contextMapper,
    }
    // aggregation is protected
    s.GenericAPIServer.Handler.NonGoRestfulMux.Handle(groupPath, groupDiscoveryHandler)
    s.GenericAPIServer.Handler.NonGoRestfulMux.UnlistedHandle(groupPath+&amp;quot;/&amp;quot;, groupDiscoveryHandler)
    s.handledGroups.Insert(apiService.Spec.Group)
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面path是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;proxyPath := &amp;quot;/apis/&amp;quot; + apiService.Spec.Group + &amp;quot;/&amp;quot; + apiService.Spec.Version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结合上面的例子就是/apis/custom-metrics.metrics.k8s.io/v1alpha1.&lt;/p&gt;

&lt;p&gt;而处理方法请求的handle就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;proxyHandler := &amp;amp;proxyHandler{
        contextMapper:   s.contextMapper,
        localDelegate:   s.delegateHandler,
        proxyClientCert: s.proxyClientCert,
        proxyClientKey:  s.proxyClientKey,
        proxyTransport:  s.proxyTransport,
        serviceResolver: s.serviceResolver,
    }
proxyHandler.updateAPIService(apiService)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的updateAPIService就是更新这个proxy的后端service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *proxyHandler) updateAPIService(apiService *apiregistrationapi.APIService) {
    if apiService.Spec.Service == nil {
        r.handlingInfo.Store(proxyHandlingInfo{local: true})
        return
    }

    newInfo := proxyHandlingInfo{
        restConfig: &amp;amp;restclient.Config{
            TLSClientConfig: restclient.TLSClientConfig{
                Insecure:   apiService.Spec.InsecureSkipTLSVerify,
                ServerName: apiService.Spec.Service.Name + &amp;quot;.&amp;quot; + apiService.Spec.Service.Namespace + &amp;quot;.svc&amp;quot;,
                CertData:   r.proxyClientCert,
                KeyData:    r.proxyClientKey,
                CAData:     apiService.Spec.CABundle,
            },
        },
        serviceName:      apiService.Spec.Service.Name,
        serviceNamespace: apiService.Spec.Service.Namespace,
    }
    newInfo.proxyRoundTripper, newInfo.transportBuildingError = restclient.TransportFor(newInfo.restConfig)
    if newInfo.transportBuildingError == nil &amp;amp;&amp;amp; r.proxyTransport.Dial != nil {
        switch transport := newInfo.proxyRoundTripper.(type) {
        case *http.Transport:
            transport.Dial = r.proxyTransport.Dial
        default:
            newInfo.transportBuildingError = fmt.Errorf(&amp;quot;unable to set dialer for %s/%s as rest transport is of type %T&amp;quot;, apiService.Spec.Service.Namespace, apiService.Spec.Service.Name, newInfo.proxyRoundTripper)
            glog.Warning(newInfo.transportBuildingError.Error())
        }
    }
    r.handlingInfo.Store(newInfo)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个restConfig就是调用service的客户端参数，其中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ServerName: apiService.Spec.Service.Name + &amp;quot;.&amp;quot; + apiService.Spec.Service.Namespace + &amp;quot;.svc&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是具体的service。而上面watch service的变化就是为了动态更新这个apiservice后端handler所用的service。&lt;/p&gt;

&lt;h1 id=&#34;响应&#34;&gt;响应&lt;/h1&gt;

&lt;p&gt;API Server 响应用户请求时附带一个状态码，该状态码符合 HTTP 规范。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;200

OK

表明请求完全成功

201

Created

表明创建类的请求完全成功

204

NoContent

表明请求完全成功，同时 HTTP 响应不包含响应体。

在响应 OPTIONS 方法的 HTTP 请求时返回

307

TemporaryRedirect

表明请求资源的地址被改变，建议客户端使用 Location 首部给出的临时 URL 来定位资源

400

BadRequest

表明请求是非法的，建议客户不要重试，修改该请求

401

Unauthorized

表明请求能够到达服务端，且服务端能够理解用户请求，但是拒绝做更多的事情，因为客户端必须提供认证信息。如果客户端提供了认证信息，则返回该状态码，表明服务端指出所提供的认证信息不合适或非法

403

Forbidden

表明请求能够到达服务端，且服务端能够理解用户请求，但是拒绝做更多的事情，因为该请求被设置成拒绝访问。建议客户不要重试，修改该请求

404

NotFound

表明所请求的资源不存在。建议客户不要重试，修改该请求

405

MethodNotAllowed

表明请求中带有该资源不支持的方法。建议客户不要重试，修改该请求

409

Conflict

表明客户端尝试创建的资源已经存在，或者由于冲突请求的更新操作不能被完成

422

UnprocessableEntity

表明由于所提供的作为请求部分的数据非法，创建或修改操作不能被完成

429

TooManyRequests

表明超出了客户端访问频率的限制或者服务端接收到多于它能处理的请求。建议客户端读取相应的 Retry-After 首部，然后等待该首部指出的时间后再重试

500

InternalServerError

表明服务端能被请求访问到，但是不能理解用户的请求；或者服务端内产生非预期中的一个错误，而且该错误无法被认知；或者服务端不能在一个合理的时间内完成处理（这可能由于服务器临时负载过重造成或者由于和其他服务器通信时的一个临时通信故障造成）

503

ServiceUnavailable

表明被请求的服务无效。建议客户不要重试，修改该请求

504

ServerTimeout

表明请求在给定的时间内无法完成。客户端仅在为请求指定超时（Timeout）参数时会得到该响应
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在调用 API 接口发生错误时，Kubernetes 将会返回一个状态类别（Status Kind）。&lt;/p&gt;

&lt;p&gt;状态对象被编码成 JSON 格式，同时该 JSON 对象被作为请求的响应体。该状态对象包含人和机器使用的域，这些域中包含来自 API 的关于失败原因的详细信息。状态对象中的信息补充了对 HTTP 状态码的说明。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -H ＂Authorization: Bearer WhCDvq4VPpYhrcfmF6ei7V9qlbqTubUc＂ HTTPs://10.240.122.184:443/api/v1/namespaces/default/pods/grafana

&amp;gt; GET /api/v1/namespaces/default/pods/grafana HTTP/1.1

&amp;gt; User-Agent: curl/7.26.0

&amp;gt; Host: 10.240.122.184

&amp;gt; Accept: */*

&amp;gt; Authorization: Bearer WhCDvq4VPpYhrcfmF6ei7V9qlbqTubUc

&amp;gt;



&amp;lt; HTTP/1.1 404 Not Found

&amp;lt; Content-Type: application/json

&amp;lt; Date: Wed, 20 May 2015 18:10:42 GMT

&amp;lt; Content-Length: 232

&amp;lt;

{

  ＂kind＂: ＂Status＂,

  ＂apiVersion＂: ＂v1＂,

  ＂metadata＂: {},

  ＂status＂: ＂Failure＂,

  ＂message＂: ＂pods \＂grafana\＂ not found＂,

  ＂reason＂: ＂NotFound＂,

  ＂details＂: {

    ＂name＂: ＂grafana＂,

    ＂kind＂: ＂pods＂

  },

  ＂code＂: 404

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;详解&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“status”域包含两个可能的值：Success 和 Failure。&lt;/li&gt;
&lt;li&gt;“message”域包含对错误的可读描述。&lt;/li&gt;
&lt;li&gt;“reason”域包含说明该操作失败原因的可读描述。如果该域的值为空，则表示该域内没有任何说明信息。“reason”域澄清 HTTP 状态码，但没有覆盖该状态码。&lt;/li&gt;
&lt;li&gt;“details”可能包含和“reason”域相关的扩展数据。每个“reason”域可以定义它的扩展的“details”域。该域是可选的，返回数据的格式是不确定的，不同的 reason 类型返回的“details”域的内容不一样。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;client&#34;&gt;client&lt;/h1&gt;

&lt;p&gt;使用各种编程语言的Kubernetes API的客户端库，主要包括两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官方支持的Kubernetes客户端库&lt;/li&gt;
&lt;li&gt;社区维护的客户端库&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实编程主要是走client的库，已经封装好了，不需要具体调用对于的api。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官方

&lt;ul&gt;
&lt;li&gt;Go：github.com/kubernetes/client-go/&lt;/li&gt;
&lt;li&gt;Python：github.com/kubernetes-client/python/&lt;/li&gt;
&lt;li&gt;Java：github.com/kubernetes-client/java&lt;/li&gt;
&lt;li&gt;dotnet：github.com/kubernetes-client/csharp&lt;/li&gt;
&lt;li&gt;JavaScript：github.com/kubernetes-client/javascript&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;社区

&lt;ul&gt;
&lt;li&gt;Go：github.com/ericchiang/k8s&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- K8s监控方案</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s-base/</link>
          <pubDate>Tue, 12 May 2020 17:02:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s-base/</guid>
          <description>&lt;p&gt;一个完整的监控体系包括：采集数据、分析存储数据、展示数据、告警以及自动化处理、监控工具自身的安全机制。我们来看看使用prometheus进行kubernetes的容器监控。&lt;/p&gt;

&lt;h1 id=&#34;物理部署promehteus监控k8s&#34;&gt;物理部署promehteus监控K8s&lt;/h1&gt;

&lt;h2 id=&#34;集群监控&#34;&gt;集群监控&lt;/h2&gt;

&lt;p&gt;k8s的集群的监控主要分为以下三个方面，当然还有一些k8s扩展使用的组件都是由对应的监控的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;k8s的物理机监控&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;架构图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20180512-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;直接使用prometheus的node-exporter就可以来获取数据的。Node-exporter会部署在每一个节点上，获取当前物理机的指标，当k8s的node节点数多的时候需要分组进行采集，并且k8s使用的网络支持固定ip，所以直接采用将ip：port注册到consul中，然后prometheus获取注册信息直接采集数据，物理机监控主要是使用node_exporter探针来获取物理机的cpu，内存，磁盘空间和i/o等指标。&lt;/p&gt;

&lt;p&gt;node_exporter可以直接采用的是k8s的daemonset部署的方式，先将node-exporter打成镜像，部署在k8s pod上&lt;/p&gt;

&lt;p&gt;物理监控我们必须要关心一下我们常用的指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;物理机层面&lt;/p&gt;

&lt;p&gt;cpu的使用率／已经使用／总量／request&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用率：1- avg(irate(node_cpu_seconds_total{mode=&amp;ldquo;idle&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}[2m]))&lt;/li&gt;
&lt;li&gt;已经使用：(count(node_cpu_seconds_total{mode=&amp;ldquo;system&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})-sum(irate(node_cpu_seconds_total{mode=&amp;ldquo;idle&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}[5m])))&lt;/li&gt;
&lt;li&gt;总量：count(node_cpu_seconds_total{mode=&amp;ldquo;system&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;memory的使用率／已经使用／总量／request&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用率：((sum(node_memory_MemTotal_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}) - sum(node_memory_MemFree_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}) - sum(node_memory_Cached_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})) / sum(node_memory_MemTotal_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})) * 100&lt;/li&gt;
&lt;li&gt;已经使用：sum(node_memory_MemTotal_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}) - sum(node_memory_MemFree_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}) - sum(node_memory_Cached_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})&lt;/li&gt;
&lt;li&gt;总量：sum(node_memory_MemTotal_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})&lt;/li&gt;
&lt;li&gt;request：sum(kube_pod_container_resource_requests_memory_bytes{cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,node=~&amp;ldquo;$node_name&amp;rdquo;,instance=&amp;ldquo;$instance&amp;rdquo;})&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;disk和disk io&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用率：(sum(node_filesystem_size_bytes{device!=&amp;ldquo;rootfs&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,fstype !=&amp;ldquo;fuse.ossfs.ossInode&amp;rdquo;}) - sum(node_filesystem_free_bytes{device!=&amp;ldquo;rootfs&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,fstype !=&amp;ldquo;fuse.ossfs.ossInode&amp;rdquo;})) / sum(node_filesystem_size_bytes{device!=&amp;ldquo;rootfs&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,fstype !=&amp;ldquo;fuse.ossfs.ossInode&amp;rdquo;}) * 100&lt;/li&gt;
&lt;li&gt;io-read：sum(rate(node_disk_read_bytes_total{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,device=&amp;ldquo;sdb&amp;rdquo;}[5m]))&lt;/li&gt;
&lt;li&gt;io-write：sum(rate(node_disk_written_bytes_total{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,device=&amp;ldquo;sdb&amp;rdquo;}[5m]))&lt;/li&gt;
&lt;li&gt;io-time：sum(rate(node_disk_io_time_seconds_total{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,device=&amp;ldquo;sdb&amp;rdquo;}[5m]))&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;指标可以直接查看对应的grafana的json文件，这边就不一一列举了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;k8s本身指标的监控&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要是k8s自身使用的组件的指标监控&lt;/p&gt;

&lt;p&gt;架构图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20180512-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见都是通过k8s自身组件来暴露指标给prometheus进行采集的。直接将集群的机器的IP：port注册到consul中去给prometheus拉去探测。&lt;/p&gt;

&lt;p&gt;这边有一个不同的地方，就是每个pod的性能情况都是通过cadvisor统一获取，不需要对每一个pod进行按着探针来监控，pod的注册也是为了业务监控的需要，和自身的监控指标并木有关系。&lt;/p&gt;

&lt;p&gt;在k8s中安装kube-state-metrics组件用来采集kubernetes的各种组件状态信息。&lt;/p&gt;

&lt;p&gt;Kubernetes集群上Pod, DaemonSet, Deployment, Job, CronJob等各种资源对象的状态需要监控数据可以被cadvisor采集。cadvisor集成在kubelet中，不需要单独部署。&lt;/p&gt;

&lt;p&gt;下面是一些监控的组件的端口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Node需要注册target，包括kubelet, cadvisor集成在kubelet中和kubelet同时暴露出来，但是使用不同的url, node-exporter, (node_list包含master节点和node节点)
For node in node_list
http://node_ip:9100/metrics


kube-scheduler监控
For node in masters
http://node_ip:10251/metrics

kube-controller-manager监控
For node in masters
http://node_ip:10252/metrics

kube-apiserver监控：需要权限的prometheus带着证书去访问
https://vip:6443/metrics
kube-state-metrics监控
https://vip:10241/metrics

这个vip会把请求转成master上的apiserver或者kube-state-metrics

etcd：需要权限的prometheus带着证书去访问
https://etcd-ip:2379/metrics
https://etcd-event-ip:2382/metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指标都是重上面的组件中要么采集，要么暴露出来的，主要监控项（表达式可以去json文件中去看）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;pod和container的cpu，memory&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pod的数量和状态&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pod的disk&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;etcd等各个组件的状态&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;应用的监控&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要是对一些中间件的监控，架构设计如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20180512-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;K8s内部部署采用单pod单容器多进程的模式，先把镜像打好，然后在启动应用的时候把探针放进去一起启动。&lt;/p&gt;

&lt;p&gt;直接通过ip:port来访问探针指标，使用外部的prometheus来采集探针提供的指标，最后在grafana上进行展示。&lt;/p&gt;

&lt;p&gt;这边注明一下使用单容器多进程的方式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.09.09&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面的方式会互相影响，所以使用sidecar的控制器，可以自动启停增加删除容器，所以最好使用sidecar的模式进行监控。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;sidecar的模式，单pod双容器&lt;/p&gt;

&lt;p&gt;这种模式，探针和应用分离开来，互不影响，便于更新，还在同一个pod下，可以共享网络
但是这种模式，单独启动了一个容器占用了一部分资源，两个镜像比较麻烦，需要管理。&lt;/p&gt;

&lt;p&gt;个人比较推荐sidecar模式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;单pod单容器多进程模式&lt;/p&gt;

&lt;p&gt;这种模式，不能实现解耦，一个应用挂了，监控也跟着挂了，
但是不占用资源，不用管理。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多pod单容器模式&lt;/p&gt;

&lt;p&gt;这种模式，网络需要打通，还是新建pod浪费资源，目前48C256G的机器没个节点上最多要求不能超过一百个pod。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;容器的设计模式&#34;&gt;容器的设计模式&lt;/h2&gt;

&lt;p&gt;这边讲解一些容器的设计模式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20180512-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;重这个对比图中可以看出&lt;/p&gt;

&lt;p&gt;1)单容器管理模式；&lt;/p&gt;

&lt;p&gt;一个pod一个容器的模式，管理简单清晰。直接使用基本命令就可以运行，当然在一个容器中可以运行多个进程，互相协作。&lt;/p&gt;

&lt;p&gt;2)单节点多容器模式；&lt;/p&gt;

&lt;p&gt;多容器才可以体现k8s的强大，Pod是一个轻量级的节点，同一个Pod中的容器可以共享同一块存储空间和同一个网络地址空间，这使得我们可以实现一些组合多个容器在同一节点工作的模式。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;挎斗模式（Sidecar pattern）&lt;/p&gt;

&lt;p&gt;这种模式主要是利用在同一Pod中的容器可以共享存储空间的能力。比如一个往文件系统中写文件，一个容器重文件系统中读取文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;外交官模式(Ambassador pattern)&lt;/p&gt;

&lt;p&gt;这种模式主要利用同一Pod中的容器可以共享网络地址空间的特性。比如一个容器开启一个proxy，给外部访问，类似于网关（外交官），然后这个容器来对转发请求到内部其他容器中处理，然后proxy容器和内部其他容器共享内部网络，直接使用localhost访问就好了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;适配器模式（Adapter pattern）&lt;/p&gt;

&lt;p&gt;分布地执行和存储，统一的监控和管理。比如业务逻辑容器的pod中运行一个exporter容器，对外统一暴露指标，适配prometheus。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其实这三种只是根据使用不同特性区分了而已，其实就是pod内部共享。&lt;/p&gt;

&lt;p&gt;3)多节点多容器模式；&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;多节点选举模式&lt;/p&gt;

&lt;p&gt;多节点选举在分布式系统中是一种重要的模式，特别是对有状态服务来说。在分布式系统中，一般来说，无状态服务，可以随意的水平伸缩，只要把运行业务逻辑的实例复制出去运行就可以，这也就是K8s里ReplicationController和ReplicaSet所做的事情。&lt;/p&gt;

&lt;p&gt;对于有状态服务，人们也希望能够水平的扩展，但因为每个实例有自己的持久化状态，而这个持久化状态必须要延续它的生命，因此，有状态服务的水平伸缩模式就是状态的分片，其中机制跟数据库的分片是一致的。那么对于一个原生为分布式系统设计的有状态服务，每个实例与分片数据的对应关系，就成为这个有状态服务的全局信息。对于任何服务，多个实例的全局信息都需要一个保存的地方。&lt;/p&gt;

&lt;p&gt;一个简单的办法是保存在外部的一个代理服务器上，这也就是MariaDB的Galera解决方案的做法，也是所以代理服务器为后端服务器所做的事情。但这种方式的问题在于，系统要依赖外部代理服务器，而代理服务器本身的高可用和水平伸缩还是没有解决的问题。&lt;/p&gt;

&lt;p&gt;所以对于要原生自己解决高可用和水平伸缩问题的系统，例如Etcd和ElasticSearch，一定要有原生的主控节点选举机制。这样这个分布式系统就不需要依赖外部的系统来维护自己的状态了。对于一个分布式系统，最主要的系统全局信息，就是集群中有哪些节点，Master节点是哪个，每个节点对应哪个分片。主控节点的任务，就是保存和分发这些信息。&lt;/p&gt;

&lt;p&gt;在K8s集群中，一个微服务实例Pod可以有多个容器。这一特性很好地提高了多节点选举机制的可重用性。它使得我们可以专门开发一个用于选举的容器镜像，在实际部署中，将选举容器和普通应用容器组合起来，应用容器只需要从本地的选举容器读取状态，就可以得到选举结果。这样，使得应用容器可以只关注自身业务逻辑相关的代码。&lt;/p&gt;

&lt;p&gt;这就是多节点多容器的选举模式，是一种使用方式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;工作队列模式&lt;/p&gt;

&lt;p&gt;分布式系统的一个重要作用是能够充分利用多个物理计算资源的能力，特别是在动态按需调动计算资源完成计算任务。设想如果有大量的需要处理的任务随机的到来，对计算资源需要的容量是不确定地；显然，按照最大可能计算量和最小可能计算量设置计算节点都是不合理的。所以可以启动多个节点多个容器来处理队列中的任务，也是一种使用方式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;分散收集模式&lt;/p&gt;

&lt;p&gt;根节点接受到来自客户端的服务请求，将服务请求分配给不同的业务模块分散处理，处理结果收集到根节点，经过一定的汇聚合并运算，产生一个合并的结果交付给客户端。也就是启动多个节点多个容器来协调处理，再通过代理合并返回，也是一种使用方式。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其实可以看见，容器的模式都是来源于物理的使用方式，也是一些常用的架构，只不过环境换成了容器，有了对应的制约和方便管理。&lt;/p&gt;

&lt;h2 id=&#34;探针组件&#34;&gt;探针组件&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Kube-state-metrics&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将Kube-state-metrics使用镜像在k8s中运行，运行在master节点上，可以获取到kube相关的所有指标，也就是具体的各种资源对象的状态指标。&lt;/p&gt;

&lt;p&gt;对应的ymal文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources:
  - configmaps
  - secrets
  - nodes
  - pods
  - services
  - resourcequotas
  - replicationcontrollers
  - limitranges
  - persistentvolumeclaims
  - persistentvolumes
  - namespaces
  - endpoints
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups: [&amp;quot;extensions&amp;quot;]
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups: [&amp;quot;apps&amp;quot;]
  resources:
  - statefulsets
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups: [&amp;quot;batch&amp;quot;]
  resources:
  - cronjobs
  - jobs
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups: [&amp;quot;autoscaling&amp;quot;]
  resources:
  - horizontalpodautoscalers
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: kube-system
  name: kube-state-metrics-resizer
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources:
  - pods
  verbs: [&amp;quot;get&amp;quot;]
- apiGroups: [&amp;quot;extensions&amp;quot;]
  resources:
  - deployments
  resourceNames: [&amp;quot;kube-state-metrics&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;update&amp;quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kube-state-metrics
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kube-state-metrics-resizer
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-state-metrics
  namespace: kube-system
spec:
  template:
    metadata:
      labels:
        k8s-app: kube-state-metrics
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      serviceAccountName: kube-state-metrics
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/master.node: &amp;quot;&amp;quot;
      containers:
      - name: kube-state-metrics
        image: xgharborsit01.sncloud.com/sncloud/kube-state-metrics:1.4.0
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 200m
            memory: 128Mi
        env:
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: KUBERNETES_SERVICE_HOST
          value: 10.243.129.252
        - name: KUBERNETES_SERVICE_PORT
          value: &amp;quot;6443&amp;quot;
        volumeMounts:
        - mountPath: /opt/kube-state-metrics/logs
          subPath: kube-state-metrics
          name: k8slog
        ports:
        - name: http-metrics
          containerPort: 10241
        - name: telemetry
          containerPort: 10242
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10241
          initialDelaySeconds: 5
          timeoutSeconds: 5
      volumes:
      - hostPath:
          path: /var/run/docker.sock
        name: docker-sock
      - hostPath:
          path: /k8s_log
        name: k8slog
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Node-exporter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Node-exporter也是使用镜像运行在k8s的每个节点上，用于获取k8s部署节点的物理机器资源信息，并不能获取对应的k8s集群的信息。&lt;/p&gt;

&lt;p&gt;对应的yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: kube-system
spec:
  template:
    metadata:
      labels:
        app: node-exporter
      name: node-exporter
      namespace: kube-system
    spec:
      containers:
      - image: xgharborsit01.sncloud.com:443/sncloud/node_exporter:0.16.0
        imagePullPolicy: IfNotPresent
        name: node-exporter
        resources:
          limits:
            memory: 256Mi
            cpu: 200m
          requests:
            memory: 128Mi
            cpu: 100m
        env:
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        ports:
        - containerPort: 9100
          hostPort: 9100
          name: scrape
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - mountPath: /opt/node-exporter/logs
          subPath: node-exporter
          name: k8slog
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - hostPath:
            path: /k8s_log
            type: &amp;quot;&amp;quot;
          name: k8slog
      hostNetwork: true
      hostPID: true
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;cadvisor-proxy&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;cadvisor-proxy对cadvisor的指标进行过滤处理。这个组件也是部署在k8s上运行。&lt;/p&gt;

&lt;p&gt;对应的ymal部署文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: cadvisor-proxy
  namespace: kube-system
spec:
  template:
    metadata:
      labels:
        app: cadvisor-proxy
    spec:
      hostNetwork: true
      containers:
      - name: proxy
        image: xgharborsit01.sncloud.com:443/sncloud/cadvisor-proxy:1.0.0
        imagePullPolicy: Always
        args:
        - --log.path=/opt/cadvisor-proxy/logs
        resources:
          limits:
            memory: 100Mi
            cpu: 500m
          requests:
            memory: 100Mi
            cpu: 100m
        volumeMounts:
        - mountPath: /var/run/docker.sock
          name: docker-sock
          readOnly: true
        - mountPath: /opt/cadvisor-proxy/logs
          subPath: cadvisor-proxy
          name: k8slog
        env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
      volumes:
      - hostPath:
          path: /var/run/docker.sock
        name: docker-sock
      - hostPath:
          path: /k8s_log
        name: k8slog
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;各种应用的exporter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边监控应用都是将对应的探针和对应的应用一起打在一个镜像里，也就是一个容器运行了两个程序，直接获取对应的指标。也有使用sidecar的模式在一个pod中运行两个容器，获取指标。下面会具体说明sidecar和这种模式的相关差异使用&lt;/p&gt;

&lt;h3 id=&#34;组件的区别&#34;&gt;组件的区别&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;cAdvisor&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;cAdvisor是Google开源的容器资源监控和性能分析工具，它是专门为容器而生，本身也支持 Docker 容器。&lt;/p&gt;

&lt;p&gt;Cadvisor可以直接部署运行在vm或者docker上，监控当前环境下docker运行的容器的资源情况。&lt;/p&gt;

&lt;p&gt;在 Kubernetes 中，我们不需要单独去安装，cAdvisor 已经集成在kubelet中，自己暴露指标，作为 kubelet 内置的一部分程序可以直接使用。&lt;/p&gt;

&lt;p&gt;cAdvisor主要监控数据是容器的资源性能数据，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Kube-state-metrics&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;kube-state-metrics通过监听 API Server 生成有关资源对象的状态指标，比如 Deployment、Node、Pod，需要注意的是 kube-state-metrics 只是简单提供一个 metrics 数据，并不会存储这些指标数据，所以我们可以使用 Prometheus 来抓取这些数据然后存储。&lt;/p&gt;

&lt;p&gt;kube-state-metrics主要监控数据主要是k8s集群有关资源的状态。比如pod的状态，副本数，重启次数等资源状态进行监控。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;metrics-server&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;metrics-server 也是一个集群范围内的资源数据聚合工具，是 Heapster 的替代品，Heapster现在已经停止维护和使用，同样的，metrics-server 也只是显示数据，并不提供数据存储服务。&lt;/p&gt;

&lt;p&gt;metrics-server定时从Kubelet的Summary API(类似/ap1/v1/nodes/nodename/stats/summary)采集指标信息。&lt;/p&gt;

&lt;p&gt;metrics-server主要监控数据主要是kubelet和集成的cadvisor中暴露的容器和集群节点的资源情况，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况，以及kubelet对于同期的维护情况。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kube-state-metrics 和metrics-server和prometheus&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1.kube-state-metrics中监控的数据，metrics-server包括其他组件都无法提供。比如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;我调度了多少个replicas？现在可用的有几个？&lt;/li&gt;
&lt;li&gt;多少个Pod是running/stopped/terminated状态？&lt;/li&gt;
&lt;li&gt;Pod重启了多少次？&lt;/li&gt;
&lt;li&gt;我有多少job在运行中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;等这一系列资源状态的数据。&lt;/p&gt;

&lt;p&gt;2.两者其实没有太大的可比性，本质不一样。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Metrics-server是官方废弃heapster项目，新开的一个项目，就是为了将核心资源监控作为一等公民对待，即像pod、service那样直接通过api-server或者client直接访问，不再是安装一个hepater来汇聚且由heapster单独管理。从 Kubernetes1.8 开始，Metrics-server就作为一个 Deployment对象默认部署在由kube-up.sh脚本创建的集群中，这样就可以直接暴露相关聚合的数据。如果形象的说的话，Metrics-server实质上是一个监控系统。&lt;/li&gt;
&lt;li&gt;kube-state-metrics关注于获取k8s各种资源的最新状态，类似于监控系统中的一个探针。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见两种着力点的方向不一样。&lt;/p&gt;

&lt;p&gt;3.Prometheus和Metrics-server&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prometheus监控系统不用Metrics-server&lt;/li&gt;
&lt;li&gt;Prometheus监控系统一般不用Metrics-server，因为他们都是自己做指标收集、集成的。可以说Prometheus包含了metric-server的能力，且prometheus更加强大，比如Prometheus可以监控metric-server本身组件的监控状态并适时报警，这里的监控就可以通过kube-state-metrics来实现，如metric-server的pod的运行状态。&lt;/li&gt;
&lt;li&gt;当然也可以使用Metrics-server，Metrics-server从 Kubelet、cAdvisor 等获取核心数据，再由prometheus从 metrics-server 获取核心度量，从其他数据源（如 Node Exporter 等）获取非核心度量，再基于它们构建监控告警系统。但是这边新增了一层，在原来的不新增的情况下也是能实现的。所以正常prometheus不使用Metrics-server。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;问题处理&#34;&gt;问题处理&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;监控支持多k8s集群场景&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在prometheus采集的时候对集群打标签，一个集群统一一个标签&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;应用关联k8s&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;应用监控时，我们需要知道当前应用是跑在哪个pod上的，这样就需要唯一标志，因为我可以有pod_IP并且在pod创建后podip就固定了，所以可以根据pod ip到cavisor中获取对应的pod name，然后根据pod name来获取对应pod的指标，包括到kube-state-metrics匹配对应的状态指标。&lt;/p&gt;

&lt;p&gt;这样就可以实现，用户到应用，应用对应的在哪个pod，pod在哪个node，以及pod的相关信息，这种一层层的监控结构。&lt;/p&gt;

&lt;p&gt;后面对探针进行改造，对于每一个暴露出来的指标，加上pod ip和pod name的label，然后以这两个纬度进行监控。&lt;/p&gt;

&lt;h1 id=&#34;k8s部署promehteus监控k8s&#34;&gt;k8s部署promehteus监控K8s&lt;/h1&gt;

&lt;p&gt;探针这一块后端部署都是上面的物理部署一样的，包括cadvisor-proxy，node-exporter，kube-state-metrics等，因为这些本来就是部署在k8s上面的，使用的也是一样的，这这边要解决的就是上面部署在物理机上的组件，包含prometheus，thanos，grafana，alertmanager等还有服务发现的方式。&lt;/p&gt;

&lt;h2 id=&#34;手动部署&#34;&gt;手动部署&lt;/h2&gt;

&lt;h3 id=&#34;部署prometheus&#34;&gt;部署prometheus&lt;/h3&gt;

&lt;p&gt;在k8s上部署Prometheus十分简单，只需要下面4个文件：prometheus.rbac.yml, prometheus.config.yml, prometheus.deploy.yml, prometheus.svc.yml。&lt;/p&gt;

&lt;p&gt;下面给的例子中将Prometheus部署到kube-system命名空间。&lt;/p&gt;

&lt;p&gt;prometheus.rbac.yml定义了Prometheus容器访问k8s apiserver所需的ServiceAccount和ClusterRole及ClusterRoleBinding&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- nonResourceURLs: [&amp;quot;/metrics&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus.config.yml configmap中的prometheus的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
    scrape_configs:

    - job_name: &#39;kubernetes-apiservers&#39;
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

    - job_name: &#39;kubernetes-nodes&#39;
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics

    - job_name: &#39;kubernetes-cadvisor&#39;
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

    - job_name: &#39;kubernetes-service-endpoints&#39;
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name

    - job_name: &#39;kubernetes-services&#39;
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module: [http_2xx]
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
        action: keep
        regex: true
      - source_labels: [__address__]
        target_label: __param_target
      - target_label: __address__
        replacement: blackbox-exporter.example.com:9115
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        target_label: kubernetes_name

    - job_name: &#39;kubernetes-ingresses&#39;
      kubernetes_sd_configs:
      - role: ingress
      relabel_configs:
      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]
        regex: (.+);(.+);(.+)
        replacement: ${1}://${2}${3}
        target_label: __param_target
      - target_label: __address__
        replacement: blackbox-exporter.example.com:9115
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_ingress_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_ingress_name]
        target_label: kubernetes_name

    - job_name: &#39;kubernetes-pods&#39;
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus.deploy.yml定义Prometheus的部署：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  labels:
    name: prometheus-deployment
  name: prometheus
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - image: harbor.frognew.com/prom/prometheus:2.0.0
        name: prometheus
        command:
        - &amp;quot;/bin/prometheus&amp;quot;
        args:
        - &amp;quot;--config.file=/etc/prometheus/prometheus.yml&amp;quot;
        - &amp;quot;--storage.tsdb.path=/prometheus&amp;quot;
        - &amp;quot;--storage.tsdb.retention=24h&amp;quot;
        ports:
        - containerPort: 9090
          protocol: TCP
        volumeMounts:
        - mountPath: &amp;quot;/prometheus&amp;quot;
          name: data
        - mountPath: &amp;quot;/etc/prometheus&amp;quot;
          name: config-volume
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
          limits:
            cpu: 500m
            memory: 2500Mi
      serviceAccountName: prometheus
      imagePullSecrets:
        - name: regsecret
      volumes:
      - name: data
        emptyDir: {}
      - name: config-volume
        configMap:
          name: prometheus-config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus.svc.yml定义Prometheus的Servic，需要将Prometheus以NodePort, LoadBalancer或使用Ingress暴露到集群外部，这样外部的Prometheus才能访问它：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: prometheus
  name: prometheus
  namespace: kube-system
spec:
  type: NodePort
  ports:
  - port: 9090
    targetPort: 9090
    nodePort: 30003
  selector:
    app: prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面就完成了prometheus在k8s的集群中部署，当然只是部署了prometheus，整体的架构还是可以参考物理架构部署&lt;/p&gt;

&lt;h3 id=&#34;部署kube-state-metrics&#34;&gt;部署kube-state-metrics&lt;/h3&gt;

&lt;p&gt;kube-state-metrics已经给出了在Kubernetes部署的manifest定义文件，直接部署，上面物理部署的时候也已经说明了&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s-base/#探针组件&#34;&gt;部署详情&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;将kube-state-metrics部署到Kubernetes上之后，就会发现Kubernetes集群中的Prometheus会在kubernetes-service-endpoints这个job下自动服务发现kube-state-metrics，并开始拉取metrics，当然集群外部的Prometheus也能从集群中的Prometheus拉取到这些数据了。这是因为上2.2中prometheus.config.yml中Prometheus的配置文件job kubernetes-service-endpoints的配置。而部署kube-state-metrics的manifest定义文件kube-state-metrics-service.yaml对kube-state-metricsService的定义包含annotation prometheus.io/scrape: &amp;lsquo;true&amp;rsquo;，因此kube-state-metrics的endpoint可以被Prometheus自动服务发现。&lt;/p&gt;

&lt;h2 id=&#34;peometheus-operator&#34;&gt;peometheus-operator&lt;/h2&gt;

&lt;p&gt;还有很多组件需要k8s部署，但是现在已经不需要这样一个个去手动写yaml文件部署，k8s推出了&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/&#34;&gt;operator的模式&lt;/a&gt;进行promehteus的部署，可以快速的部署使用。&lt;/p&gt;

&lt;h2 id=&#34;使用k8s的服务发现&#34;&gt;使用k8s的服务发现&lt;/h2&gt;

&lt;p&gt;prometheus自身提供了自动发现kubernetes的监控目标的功能，首先直接上官方的prometheus配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# A scrape configuration for running Prometheus on a Kubernetes cluster.
# This uses separate scrape configs for cluster components (i.e. API server, node)
# and services to allow each to use different authentication configs.
#
# Kubernetes labels will be added as Prometheus labels on metrics via the
# `labelmap` relabeling action.
#
# If you are using Kubernetes 1.7.2 or earlier, please take note of the comments
# for the kubernetes-cadvisor job; you will need to edit or remove this job.

# Scrape config for API servers.
#
# Kubernetes exposes API servers as endpoints to the default/kubernetes
# service so this uses `endpoints` role and uses relabelling to only keep
# the endpoints associated with the default/kubernetes service using the
# default named port `https`. This works for single API server deployments as
# well as HA API server deployments.
scrape_configs:
- job_name: &#39;kubernetes-apiservers&#39;

  kubernetes_sd_configs:
  - role: endpoints

  # Default to scraping over https. If required, just disable this or change to
  # `http`.
  scheme: https

  # This TLS &amp;amp; bearer token file config is used to connect to the actual scrape
  # endpoints for cluster components. This is separate to discovery auth
  # configuration because discovery &amp;amp; scraping are two separate concerns in
  # Prometheus. The discovery auth config is automatic if Prometheus runs inside
  # the cluster. Otherwise, more config options have to be provided within the
  # &amp;lt;kubernetes_sd_config&amp;gt;.
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    # If your node certificates are self-signed or use a different CA to the
    # master CA, then disable certificate verification below. Note that
    # certificate verification is an integral part of a secure infrastructure
    # so this should only be disabled in a controlled environment. You can
    # disable certificate verification by uncommenting the line below.
    #
    # insecure_skip_verify: true
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

  # Keep only the default/kubernetes service endpoints for the https port. This
  # will add targets for each API server which Kubernetes adds an endpoint to
  # the default/kubernetes service.
  relabel_configs:
  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
    action: keep
    regex: default;kubernetes;https

# Scrape config for nodes (kubelet).
#
# Rather than connecting directly to the node, the scrape is proxied though the
# Kubernetes apiserver.  This means it will work if Prometheus is running out of
# cluster, or can&#39;t connect to nodes for some other reason (e.g. because of
# firewalling).
- job_name: &#39;kubernetes-nodes&#39;

  # Default to scraping over https. If required, just disable this or change to
  # `http`.
  scheme: https

  # This TLS &amp;amp; bearer token file config is used to connect to the actual scrape
  # endpoints for cluster components. This is separate to discovery auth
  # configuration because discovery &amp;amp; scraping are two separate concerns in
  # Prometheus. The discovery auth config is automatic if Prometheus runs inside
  # the cluster. Otherwise, more config options have to be provided within the
  # &amp;lt;kubernetes_sd_config&amp;gt;.
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

  kubernetes_sd_configs:
  - role: node

  relabel_configs:
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)
  - target_label: __address__
    replacement: kubernetes.default.svc:443
  - source_labels: [__meta_kubernetes_node_name]
    regex: (.+)
    target_label: __metrics_path__
    replacement: /api/v1/nodes/${1}/proxy/metrics

# Scrape config for Kubelet cAdvisor.
#
# This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
# (those whose names begin with &#39;container_&#39;) have been removed from the
# Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
# retrieve those metrics.
#
# In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
# HTTP endpoint; use &amp;quot;replacement: /api/v1/nodes/${1}:4194/proxy/metrics&amp;quot;
# in that case (and ensure cAdvisor&#39;s HTTP server hasn&#39;t been disabled with
# the --cadvisor-port=0 Kubelet flag).
#
# This job is not necessary and should be removed in Kubernetes 1.6 and
# earlier versions, or it will cause the metrics to be scraped twice.
- job_name: &#39;kubernetes-cadvisor&#39;

  # Default to scraping over https. If required, just disable this or change to
  # `http`.
  scheme: https

  # This TLS &amp;amp; bearer token file config is used to connect to the actual scrape
  # endpoints for cluster components. This is separate to discovery auth
  # configuration because discovery &amp;amp; scraping are two separate concerns in
  # Prometheus. The discovery auth config is automatic if Prometheus runs inside
  # the cluster. Otherwise, more config options have to be provided within the
  # &amp;lt;kubernetes_sd_config&amp;gt;.
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

  kubernetes_sd_configs:
  - role: node

  relabel_configs:
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)
  - target_label: __address__
    replacement: kubernetes.default.svc:443
  - source_labels: [__meta_kubernetes_node_name]
    regex: (.+)
    target_label: __metrics_path__
    replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

# Scrape config for service endpoints.
#
# The relabeling allows the actual service scrape endpoint to be configured
# via the following annotations:
#
# * `prometheus.io/scrape`: Only scrape services that have a value of `true`
# * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
# to set this to `https` &amp;amp; most likely set the `tls_config` of the scrape config.
# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
# * `prometheus.io/port`: If the metrics are exposed on a different port to the
# service then set this appropriately.
- job_name: &#39;kubernetes-service-endpoints&#39;

  kubernetes_sd_configs:
  - role: endpoints

  relabel_configs:
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
    action: replace
    target_label: __scheme__
    regex: (https?)
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
    action: replace
    target_label: __metrics_path__
    regex: (.+)
  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
    action: replace
    target_label: __address__
    regex: ([^:]+)(?::\d+)?;(\d+)
    replacement: $1:$2
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    action: replace
    target_label: kubernetes_name

# Example scrape config for probing services via the Blackbox Exporter.
#
# The relabeling allows the actual service scrape endpoint to be configured
# via the following annotations:
#
# * `prometheus.io/probe`: Only probe services that have a value of `true`
- job_name: &#39;kubernetes-services&#39;

  metrics_path: /probe
  params:
    module: [http_2xx]

  kubernetes_sd_configs:
  - role: service

  relabel_configs:
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
    action: keep
    regex: true
  - source_labels: [__address__]
    target_label: __param_target
  - target_label: __address__
    replacement: blackbox-exporter.example.com:9115
  - source_labels: [__param_target]
    target_label: instance
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    target_label: kubernetes_name

# Example scrape config for probing ingresses via the Blackbox Exporter.
#
# The relabeling allows the actual ingress scrape endpoint to be configured
# via the following annotations:
#
# * `prometheus.io/probe`: Only probe services that have a value of `true`
- job_name: &#39;kubernetes-ingresses&#39;

  metrics_path: /probe
  params:
    module: [http_2xx]

  kubernetes_sd_configs:
    - role: ingress

  relabel_configs:
    - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]
      regex: (.+);(.+);(.+)
      replacement: ${1}://${2}${3}
      target_label: __param_target
    - target_label: __address__
      replacement: blackbox-exporter.example.com:9115
    - source_labels: [__param_target]
      target_label: instance
    - action: labelmap
      regex: __meta_kubernetes_ingress_label_(.+)
    - source_labels: [__meta_kubernetes_namespace]
      target_label: kubernetes_namespace
    - source_labels: [__meta_kubernetes_ingress_name]
      target_label: kubernetes_name

# Example scrape config for pods
#
# The relabeling allows the actual pod scrape endpoint to be configured via the
# following annotations:
#
# * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
# * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
# pod&#39;s declared ports (default is a port-free target if none are declared).
- job_name: &#39;kubernetes-pods&#39;

  kubernetes_sd_configs:
  - role: pod

  relabel_configs:
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
    action: replace
    target_label: __metrics_path__
    regex: (.+)
  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
    action: replace
    regex: ([^:]+)(?::\d+)?;(\d+)
    replacement: $1:$2
    target_label: __address__
  - action: labelmap
    regex: __meta_kubernetes_pod_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_pod_name]
    action: replace
    target_label: kubernetes_pod_name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然该配置文件，是在prometheus部署在k8s中生效的,即in-cluster模式。下面我们详细说明一下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;&amp;lt;scrape_config&amp;gt;&lt;/code&gt;:定义收集规则。 在一般情况下，一个scrape配置指定一个job。 在高级配置中，这可能会改变。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;relabel_configs允许在抓取之前对任何目标及其标签进行修改。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-apiservers&lt;/p&gt;

&lt;p&gt;该项主要是让prometheus程序可以访问kube-apiserver，进而进行服务发现。看一下服务发现的代码可以看出，主要服务发现：node，service，ingress，pod。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;switch d.role {
case &amp;quot;endpoints&amp;quot;:
    var wg sync.WaitGroup

    for _, namespace := range namespaces {
        elw := cache.NewListWatchFromClient(rclient, &amp;quot;endpoints&amp;quot;, namespace, nil)
        slw := cache.NewListWatchFromClient(rclient, &amp;quot;services&amp;quot;, namespace, nil)
        plw := cache.NewListWatchFromClient(rclient, &amp;quot;pods&amp;quot;, namespace, nil)
        eps := NewEndpoints(
            log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;endpoint&amp;quot;),
            cache.NewSharedInformer(slw, &amp;amp;apiv1.Service{}, resyncPeriod),
            cache.NewSharedInformer(elw, &amp;amp;apiv1.Endpoints{}, resyncPeriod),
            cache.NewSharedInformer(plw, &amp;amp;apiv1.Pod{}, resyncPeriod),
        )
        go eps.endpointsInf.Run(ctx.Done())
        go eps.serviceInf.Run(ctx.Done())
        go eps.podInf.Run(ctx.Done())

        for !eps.serviceInf.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        for !eps.endpointsInf.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        for !eps.podInf.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        wg.Add(1)
        go func() {
            defer wg.Done()
            eps.Run(ctx, ch)
        }()
    }
    wg.Wait()
case &amp;quot;pod&amp;quot;:
    var wg sync.WaitGroup
    for _, namespace := range namespaces {
        plw := cache.NewListWatchFromClient(rclient, &amp;quot;pods&amp;quot;, namespace, nil)
        pod := NewPod(
            log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;pod&amp;quot;),
            cache.NewSharedInformer(plw, &amp;amp;apiv1.Pod{}, resyncPeriod),
        )
        go pod.informer.Run(ctx.Done())

        for !pod.informer.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        wg.Add(1)
        go func() {
            defer wg.Done()
            pod.Run(ctx, ch)
        }()
    }
    wg.Wait()
case &amp;quot;service&amp;quot;:
    var wg sync.WaitGroup
    for _, namespace := range namespaces {
        slw := cache.NewListWatchFromClient(rclient, &amp;quot;services&amp;quot;, namespace, nil)
        svc := NewService(
            log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;service&amp;quot;),
            cache.NewSharedInformer(slw, &amp;amp;apiv1.Service{}, resyncPeriod),
        )
        go svc.informer.Run(ctx.Done())

        for !svc.informer.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        wg.Add(1)
        go func() {
            defer wg.Done()
            svc.Run(ctx, ch)
        }()
    }
    wg.Wait()
case &amp;quot;ingress&amp;quot;:
    var wg sync.WaitGroup
    for _, namespace := range namespaces {
        ilw := cache.NewListWatchFromClient(reclient, &amp;quot;ingresses&amp;quot;, namespace, nil)
        ingress := NewIngress(
            log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;ingress&amp;quot;),
            cache.NewSharedInformer(ilw, &amp;amp;extensionsv1beta1.Ingress{}, resyncPeriod),
        )
        go ingress.informer.Run(ctx.Done())

        for !ingress.informer.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        wg.Add(1)
        go func() {
            defer wg.Done()
            ingress.Run(ctx, ch)
        }()
    }
    wg.Wait()
case &amp;quot;node&amp;quot;:
    nlw := cache.NewListWatchFromClient(rclient, &amp;quot;nodes&amp;quot;, api.NamespaceAll, nil)
    node := NewNode(
        log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;node&amp;quot;),
        cache.NewSharedInformer(nlw, &amp;amp;apiv1.Node{}, resyncPeriod),
    )
    go node.informer.Run(ctx.Done())

    for !node.informer.HasSynced() {
        time.Sleep(100 * time.Millisecond)
    }
    node.Run(ctx, ch)

default:
    level.Error(d.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;unknown Kubernetes discovery kind&amp;quot;, &amp;quot;role&amp;quot;, d.role)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-nodes&lt;/p&gt;

&lt;p&gt;发现node以后，通过/api/v1/nodes/${1}/proxy/metrics来获取node的metrics。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-cadvisor&lt;/p&gt;

&lt;p&gt;cadvisor已经被集成在kubelet中，所以发现了node就相当于发现了cadvisor。通过 /api/v1/nodes/${1}/proxy/metrics/cadvisor采集容器指标。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-services和kubernetes-ingresses&lt;/p&gt;

&lt;p&gt;该两种资源监控方式差不多，都是需要安装black-box，然后类似于探针去定时访问，根据返回的http状态码来判定service和ingress的服务可用性。&lt;/p&gt;

&lt;p&gt;PS：不过我自己在这里和官方的稍微有点区别，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- target_label: __address__
  replacement: blackbox-exporter.example.com:9115
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;官方大致是需要我们要创建black-box 的ingress从外部访问，这样从效率和安全性都不是最合适的。所以我一般都是直接内部dns访问。如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- target_label: __address__
  replacement: blackbox-exporter.kube-system:9115
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然看源码可以发现，并不是所有的service和ingress都会健康监测，如果需要将服务进行健康监测，那么你部署应用的yaml文件加一些注解。例如：&lt;/p&gt;

&lt;p&gt;对于service和ingress：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;需要加注解：prometheus.io/scrape: &#39;true&#39;

apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: &#39;true&#39;
  name: prometheus-node-exporter
  namespace: kube-system
  labels:
    app: prometheus
    component: node-exporter
spec:
  clusterIP: None
  ports:
    - name: prometheus-node-exporter
      port: 9100
      protocol: TCP
  selector:
    app: prometheus
    component: node-exporter
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-pods&lt;/p&gt;

&lt;p&gt;对于pod的监测也是需要加注解：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus.io/scrape，为true则会将pod作为监控目标。
prometheus.io/path，默认为/metrics
prometheus.io/port , 端口
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以看到此处可以看出，该job并不是监控pod的指标，pod已经通过前面的cadvisor采集。此处是对pod中应用的监控。写过exporter的人应该对这个概念非常清楚。通俗讲，就是你pod中的应用提供了prometheus的监控功能，加上对应的注解，那么该应用的metrics会定时被采集走。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-service-endpoints&lt;/p&gt;

&lt;p&gt;对于服务的终端节点，也需要加注解：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus.io/scrape，为true则会将pod作为监控目标。
prometheus.io/path，默认为/metrics
prometheus.io/port , 端口
prometheus.io/scheme 默认http，如果为了安全设置了https，此处需要改为https
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个基本上同上的。采集service-endpoints的metrics。&lt;/p&gt;

&lt;p&gt;个人认为：如果某些部署应用只有pod没有service，那么这种情况只能在pod上加注解，通过kubernetes-pods采集metrics。如果有service，那么就无需在pod加注解了，直接在service上加即可。毕竟service-endpoints最终也会落到pod上。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;配置项总结&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kubernetes-service-endpoints和kubernetes-pods采集应用中metrics，当然并不是所有的都提供了metrics接口。&lt;/li&gt;
&lt;li&gt;kubernetes-ingresses 和kubernetes-services 健康监测服务和ingress健康的状态&lt;/li&gt;
&lt;li&gt;kubernetes-cadvisor 和 kubernetes-nodes，通过发现node，监控node 和容器的cpu等指标&lt;/li&gt;
&lt;li&gt;自动发现源码，可以参考client-go和prometheus自动发现k8s，这种监听k8s集群中资源的变化，使用informer实现，不要轮询kube-apiserver接口。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;物理部署和k8s的区别和结合&#34;&gt;物理部署和k8s的区别和结合&lt;/h1&gt;

&lt;p&gt;物理部署下的consul走的文件服务发现和k8s的服务发现完全是两种方式，在小规模的情况下使用k8s确实方便，但是在规模较大的情况，就需要分片，目前thanos也是支持k8s，所以应该也是可以分片的，也可以将数据聚合到vm中去。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 计数系统</title>
          <link>https://kingjcy.github.io/post/architecture/count/</link>
          <pubDate>Mon, 04 May 2020 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/count/</guid>
          <description>&lt;p&gt;很多业务都有“计数”需求，在业务复杂，计数扩展频繁，数据量大，并发量大的情况下，计数系统的架构演进与实践。&lt;/p&gt;

&lt;h1 id=&#34;初始架构&#34;&gt;初始架构&lt;/h1&gt;

&lt;p&gt;我们可以很容易想到，关注服务+粉丝服务+消息服务均提供相应接口，就能拿到相关计数数据。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样将所有的数据记录入表，然后对某个属性进行count就能得到计数的数据。这个方案叫做“count”计数法，在数据量并发量不大的情况下，最容易想到且最经常使用的就是这种方法，但随着数据量的上升，并发量的上升，这个方法的弊端将逐步展现：计算量特别大，访问数据特别多。&lt;/p&gt;

&lt;h1 id=&#34;计数外置的架构设计&#34;&gt;计数外置的架构设计&lt;/h1&gt;

&lt;p&gt;计数是一个通用的需求，有没有可能，这个计数的需求实现在一个通用的系统里，而不是由关注服务、粉丝服务、微博服务来分别来提供相应的功能呢（否则扩展性极差）？&lt;/p&gt;

&lt;p&gt;通过分析，上述微博的业务可以抽象成两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户(uid)维度的计数：用户的关注计数，粉丝计数，发布的微博计数&lt;/li&gt;
&lt;li&gt;微博消息(msg_id)维度的计数：消息转发计数，评论计数，点赞计数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;于是可以抽象出两个表，针对这两个维度来进行计数的存储：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;t_user_count (uid, gz_count, fs_count, wb_count);
t_msg_count (msg_id, forword_count, comment_count, praise_count);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;甚至可以更为抽象，一个表搞定所有计数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;t_count(id, type, c1, c2, c3, …)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过type来判断，id究竟是uid还是msg_id，但并不建议这么做。&lt;/p&gt;

&lt;p&gt;存储抽象完，再抽象出一个计数服务对这些数据进行管理，提供友善的RPC接口：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样，在查询一条微博消息的若干个计数的时候，不用进行多次数据库count操作，而会转变为一条数据的多个属性的查询。但是当有微博被转发、评论、点赞的时候，计数服务如何同步的进行计数的变更呢？如果让业务服务来调用计数服务，势必会导致业务系统与计数系统耦合。&lt;/p&gt;

&lt;p&gt;对于不关心下游结果的业务，可以使用MQ来解耦，在业务发生变化的时候，向MQ发送一条异步消息，通知计数系统计数发生了变化即可&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;计数外置，本质是数据的冗余，架构设计上，数据冗余必将引发数据的一致性问题，需要有机制来保证计数系统里的数据与业务系统里的数据一致，常见的方法有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于一致性要求比较高的业务，要有定期check并fix的机制，例如关注计数，粉丝计数，微博消息计数等&lt;/li&gt;
&lt;li&gt;对于一致性要求比较低的业务，即使有数据不一致，业务可以接受，例如微博浏览数，微博转发数等&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;计数外置缓存优化&#34;&gt;计数外置缓存优化&lt;/h2&gt;

&lt;p&gt;计数外置很大程度上解决了计数存取的性能问题，但是否还有优化空间呢？像关注计数，粉丝计数，微博消息计数，变化的频率很低，查询的频率很高，这类读多些少的业务场景，非常适合使用缓存来进行查询优化，减少数据库的查询次数，降低数据库的压力。&lt;/p&gt;

&lt;p&gt;但是，缓存是kv结构的，无法像数据库一样，设置成t_uid_count(uid, c1, c2, c3)这样的schema，如何来对kv进行设计呢？缓存kv结构的value是计数，看来只能在key上做设计，很容易想到，可以使用uid:type来做key，存储对应type的计数。&lt;/p&gt;

&lt;p&gt;对于uid=123的用户，其关注计数，粉丝计数，微博消息计数的缓存就可以设计为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此时对应的counting-service架构变为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个“计数外置缓存优化”方案，可以总结为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用缓存来保存读多写少的计数（其实写多读少，一致性要求不高的计数，也可以先用缓存保存，然后定期刷到数据库中，以降低数据库的读写压力）&lt;/li&gt;
&lt;li&gt;使用id:type的方式作为缓存的key，使用count来作为缓存的value&lt;/li&gt;
&lt;li&gt;多次读取缓存来查询多个uid的计数&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;缓存批量读取优化&#34;&gt;缓存批量读取优化&lt;/h2&gt;

&lt;p&gt;缓存的使用能够极大降低数据库的压力，但多次缓存交互依旧存在优化空间，有没有办法进一步优化呢？&lt;/p&gt;

&lt;p&gt;不要陷入思维定式，谁说value一定只能是一个计数，难道不能多个计数存储在一个value中么？缓存kv结构的key是uid，value可以是多个计数同时存储。&lt;/p&gt;

&lt;p&gt;对于uid=123的用户，其关注计数，粉丝计数，微博消息计数的缓存就可以设计为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样多个用户，多个计数的查询就可以一次搞定。&lt;/p&gt;

&lt;p&gt;这个“计数外置缓存批量优化”方案，可以总结为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用id作为key，使用同一个id的多个计数的拼接作为value&lt;/li&gt;
&lt;li&gt;多个id的多个计数查询，一次搞定&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;计数扩展性优化&#34;&gt;计数扩展性优化&lt;/h2&gt;

&lt;p&gt;考虑完效率，架构设计上还需要考虑扩展性，如果uid除了关注计数，粉丝计数，微博计数，还要增加一个计数，就需要变更表结构，频繁的变更数据库schema的结构显然是不可取的。&lt;/p&gt;

&lt;p&gt;我们可以这样设计表结构来通过行来扩展属性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;t_user_count(uid, count_key, count_value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如果需要新增一个计数XX_count，只需要增加一行即可，而不需要变更表结构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算安全系列---- 安全</title>
          <link>https://kingjcy.github.io/post/cloud/paas/safe/safe/</link>
          <pubDate>Mon, 04 May 2020 11:26:37 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/safe/safe/</guid>
          <description>&lt;p&gt;在基于k8s做应用开发的时候，都是使用admin来使用k8s，基本不用去关注授权的问题。但是，当我们将k8s作为PaaS平台的容器编排引擎，并引入多租户时，就涉及到权限管理相关的问题了，paas平台的安全都是基于k8s的安全机制来实现的。&lt;/p&gt;

&lt;p&gt;Kubernetes中的隔离主要包括这几种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;网络隔离：需要使用网络插件，比如flannel, calico。&lt;/li&gt;
&lt;li&gt;资源隔离：kubernetes原生支持资源隔离，pod就是资源隔离和调度的最小单位，同时使用namespace限制用户空间和资源限额。&lt;/li&gt;
&lt;li&gt;身份隔离：使用RBAC-基于角色的访问控制，多租户的身份认证和权限控制。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;那么k8s的安全机制有哪些？首先我们先来了解一些概念。&lt;/p&gt;

&lt;h1 id=&#34;基础概念&#34;&gt;基础概念&lt;/h1&gt;

&lt;h2 id=&#34;用户&#34;&gt;用户&lt;/h2&gt;

&lt;p&gt;Kubernetes 中有两种用户:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;一种是内置“用户”：ServiceAccount，用于集群内的资源的一种身份,便于认证授权操作，我们叫做系统服务用户。
一种就是我们这种操作集群的人，实际操作 &amp;quot;kubectl&amp;quot; 命令的人,我们直接叫用户。
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;service-account&#34;&gt;Service Account&lt;/h3&gt;

&lt;p&gt;Service Account概念的引入是基于这样的使用场景：运行在pod里的进程需要调用Kubernetes API以及非Kubernetes API的其它服务。这时候pod就需要一个身份做认证授权。所以Service Account它并不是给kubernetes集群的用户使用的，而是给pod里面的进程使用的，它为pod提供必要的身份认证。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get sa --all-namespaces

NAMESPACE     NAME          SECRETS   AGE
default       build-robot   1         1d
default       default       1         32d
default       kube-dns      1         31d
kube-public   default       1         32d
kube-system   dashboard     1         31d
kube-system   default       1         32d
kube-system   heapster      1         30d
kube-system   kube-dns      1         31d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果kubernetes开启了ServiceAccount（–admission_control=…,ServiceAccount,… ）那么会在每个namespace下面都会创建一个默认的default的sa，可见sa是namespace级别的。其中最重要的就是secrets，它是每个sa下面都会拥有的一个加密的token。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get sa  default  -o yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2017-05-02T06:39:12Z
  name: default
  namespace: default
  resourceVersion: &amp;quot;175&amp;quot;
  selfLink: /api/v1/namespaces/default/serviceaccounts/default
  uid: 0de23575-2f02-11e7-98d0-5254c4628ad9
secrets:
- name: default-token-rsf8r
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当用户再该namespace下创建pod的时候都会默认使用这个sa，kubernetes会把默认的sa挂载到容器内。&lt;/p&gt;

&lt;p&gt;看一下这个secret&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get secret default-token-rsf8r -o yaml
apiVersion: v1
data:
  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR2akNDQXFhZ0F3SUJBZ0lVZlpvZDJtSzNsa3JiMzR3NDhhUmtOc0pVVDJjd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbAphVXBwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEUzTURVd01qQTNNekF3TUZvWERUSXlNRFV3TVRBM016QXdNRm93WlRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbGFVcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByZFdKbGNtNWxkR1Z6Ck1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBc2E5Zk1HVGd2MGl0YnlZcHoycXkKOThKWktXdWdFL0VPbXRYS2ExT0Y3ekUxSFh1cDFOVG8rNkhvUEFuR3hhVzg4Q0s0TENrbWhNSGFLdUxnT3IvVApOMGphdnc5YWlPeVdYR1hXUUxVN3U0aVhoaDV6a2N4bmZxRW9JOW9JV2dMTzVEL3hBL0tnZzRQZDRMeFdqMkFQCk4rcVdxQ2crU3BrdkpIQUZWL3IyTk1BbEIzNHBrK0t5djVQMDJSQmd6Y2xTeSs5OUxDWnlIQ1VocGl0TFFabHoKdUNmeGtBeUNoWFcxMWNKdVFtaDM4aFVKa0dhUW9OVDVSNmtoRTArenJDVjVkWnNVMVZuR0FydWxaWXpJY3kregpkeUZpYWYyaitITyt5blg4RUNySzR1TUF3Nk4zN1pnNjRHZVRtbk5EWmVDTTlPelk5czBOVzc1dHU5bHJPZTVqCnZRSURBUUFCbzJZd1pEQU9CZ05WSFE4QkFmOEVCQU1DQVFZd0VnWURWUjBUQVFIL0JBZ3dCZ0VCL3dJQkFqQWQKQmdOVkhRNEVGZ1FVK2RqMThRUkZyMWhKMVhGb1VyYUVVRnpEeVRBd0h3WURWUjBqQkJnd0ZvQVUrZGoxOFFSRgpyMWhKMVhGb1VyYUVVRnpEeVRBd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFBazQ4ODZBa0Fpa3VBVWRiOWU1CitldkVXVVFFaTIyTmc4REhmVTVSbXppU2ZhVllFQ1FuTlBUREprMmYvTm1Kb3RUVWxRZS9Ec3BkNEk1TFova1IKMGI2b1VoZkdmTkVOOXVObkkvZEgzOFBjUTNDaWtVeHhaeFRYTytaaldxcGNHZTRLNzZtaWd2ZWhQR2Z1VUNzQwp0UmZkZDM2YkhnRjN4MzRCWnc5MStDQ2VKQzBSWmNjVENqcHFHUEZFQlM3akJUVUlRVjNodnZycWJMV0hNeTJuCnFIck94UFI1eFkrRU5SQ0xzVWNSdk9icUhBK1g0c1BTdzBwMWpROXNtK1lWNG1ybW9Gd1RyS09kK2FqTVhzVXkKL3ZRYkRzNld4RWkxZ2ZvR3BxZFN6U1k0MS9IWHovMjZWNlFWazJBajdQd0FYZmszYk1wWHdDamRXRG4xODhNbQpXSHM9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
  namespace: ZGVmYXVsdA==
  token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbVJsWm1GMWJIUXRkRzlyWlc0dGNuTm1PSElpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdWbVlYVnNkQ0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJakJrWlRJek5UYzFMVEptTURJdE1URmxOeTA1T0dRd0xUVXlOVFJqTkRZeU9HRmtPU0lzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT21SbFptRjFiSFFpZlEuSmxuamM0Y0xNYkZrRlJVQjIyWGtFN2t4bTJ1dS1aQm9sUTh4VEdDNmdLOTdSZTVOMzBuY2V0SWJsanVOVWFlaDhtMDk2R19nMHE3cmRvWm5XMTV2OFBVXzNyM1hWWlBNc1lxbGRpZlNJbWtReXFqeEphVlBka3Izam5GWVBkVWNaTmk3MFF3cWtEdm5sMXB4SFRNZTZkTVNPTlExbUVoMHZSbHBhRTdjVWtTVlg5blRzaFVJVTVXWE9wRUxwdTVjVjBHV3ZGeDRDSzR6Umt3clNMdlV5X2d5UGZwLWdYVFZQWU80NkJKSWZtaVhlZGhVaW9nempPN285eGxDbUxQVkhyNkFIZGViNExiTVA1dkJ2MlBSZ2RrMW9keTR0VEdxLVRGU3M2VkNoMTZ4dk5IdTRtRVN5TjZmcXVObzJwYUFOelY4b251aTJuaU4yNTU1TzN4SFdR
kind: Secret
metadata:
  annotations:
    kubernetes.io/service-account.name: default
    kubernetes.io/service-account.uid: 0de23575-2f02-11e7-98d0-5254c4628ad9
  creationTimestamp: 2017-05-02T06:42:07Z
  name: default-token-rsf8r
  namespace: default
  resourceVersion: &amp;quot;12551&amp;quot;
  selfLink: /api/v1/namespaces/default/secrets/default-token-rsf8r
  uid: 75c0a236-2f02-11e7-98d0-5254c4628ad9
type: kubernetes.io/service-account-token
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的内容是经过base64加密过后的，我们直接进入容器内&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~ # ls -l  /var/run/secrets/kubernetes.io/serviceaccount/
total 0
lrwxrwxrwx    1 root     root            13 May  4 23:57 ca.crt -&amp;gt; ..data/ca.crt
lrwxrwxrwx    1 root     root            16 May  4 23:57 namespace -&amp;gt; ..data/namespace
lrwxrwxrwx    1 root     root            12 May  4 23:57 token -&amp;gt; ..data/token
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到已将ca.crt 、namespace和token放到容器内了，那么这个容器就可以通过https的请求访问apiserver了，这就是所有的pod都能走apiserver的原因。&lt;/p&gt;

&lt;p&gt;service account是namespace作用域，而User是全cluster唯一。service account会对应一个虚拟User，User名为system:serviceaccount:${namespace}:${sa_name}，比如在default namespace的test service account，则对应的虚拟User为system:serviceaccount:default:test。&lt;/p&gt;

&lt;h3 id=&#34;用户-1&#34;&gt;用户&lt;/h3&gt;

&lt;p&gt;可以理解成实际操作 &amp;ldquo;kubectl&amp;rdquo; 命令的人，区别于 pod 等资源概念的实际操作的k8s集群的运维人员或者用户，也就是我们的平常的用户。&lt;/p&gt;

&lt;h3 id=&#34;kubernetes没有用户以及用户组&#34;&gt;Kubernetes没有用户以及用户组&lt;/h3&gt;

&lt;p&gt;虽然我们在上面列出了两个用户类型，Kubernetes的RBAC模型授权对象(Subject)是用户(User)或者用户组(Group)，即使ServiceAccount也会当作为一个虚拟User，但是很令人疑惑的是通过Kubernetes找不到真正的用户以及用户组信息，甚至连对应的User Resource以及Group Resource都没有，所以有时候在写rolebingding的时候会觉得很奇怪，Subjects需要填User或者Group，可是Kubernetes却没有办法列出可信任的User列表以及Group列表。&lt;/p&gt;

&lt;p&gt;换句话说Kubernetes并没有提供用户管理和身份认证功能，除了Service Account外，所有的用户信息都依赖外部的用户管理系统来存储，因此通过api-serever根本无法列出User和Group。&lt;/p&gt;

&lt;p&gt;这其实挺符合UNIX设计哲学的，即Do One Thing and Do It Well。Kubernetes只专注于做应用编排，其他的功能则提供接口集成，除了认证和授权，我们发现网络、存储也都如此。&lt;/p&gt;

&lt;p&gt;这样做的好处也显而易见，用户账户信息与Kubernetes集群松耦合，便于集成企业已有的身份认证系统，如AD、LADP、Keycloak等。&lt;/p&gt;

&lt;h2 id=&#34;多租户&#34;&gt;多租户&lt;/h2&gt;

&lt;p&gt;1、租户&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;租户的概念不止局限于集群的用户，它可以包含为一组计算，网络，存储等资源组成的工作负载集合。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、多租户的目的？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;多租户集群由多个用户或工作负载共享，这些用户和工作负载被称为“租户”。多租户集群的运营方必须将租户彼此隔离，以最大限度地减少被盗用的租户或恶意租户可能对集群和其他租户造成的损害。此外，必须在租户之间公平地分配集群资源。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、k8s多租户的实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;虽然 Kubernetes 不能保证租户之间完全安全地隔离，但它为特定使用场景提供了足够的相关功能。您可以将每个租户及其 Kubernetes 资源分隔到各自的命名空间中。然后，您可以使用政策来强制执行租户隔离。政策通常按命名空间划分，可用于限制 API 访问、资源使用以及允许容器执行的操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、多租户使用场景:企业内部共享集群的多租户&lt;/p&gt;

&lt;p&gt;该场景下集群的所有用户均来自企业内部，这也是当前很多 k8s 集群客户的使用模式,我们可以通过命名空间对不同部门或团队进行资源的逻辑隔离，同时定义以下几种角色的业务人员：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;集群管理员(cluster admin)

&lt;ul&gt;
&lt;li&gt;具有集群的管理能力（扩缩容、添加节点等操作）&lt;/li&gt;
&lt;li&gt;负责为租户管理员创建和分配命名空间&lt;/li&gt;
&lt;li&gt;负责各类策略（RAM/RBAC/networkpolicy/quota…）的 CRUD&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;租户管理员(namespace admin)

&lt;ul&gt;
&lt;li&gt;至少具有集群的 RAM 只读权限&lt;/li&gt;
&lt;li&gt;管理租户内相关人员的 RBAC 配置&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;租户内用户(namespace user)

&lt;ul&gt;
&lt;li&gt;在租户对应命名空间内使用权限范围内的 k8s 资源&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/tenant&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;安全机制&#34;&gt;安全机制&lt;/h2&gt;

&lt;p&gt;APIServer提供了3A机制：认证机制（Authentication）、授权机制（Authorization）以及准入控制机制（Admission Controllers）。&lt;/p&gt;

&lt;h3 id=&#34;认证机制-authentication&#34;&gt;认证机制（Authentication）&lt;/h3&gt;

&lt;p&gt;k8s 中的认证机制，是在用户访问 APIServer 的第一步。这一步往往只检测请求头或客户端证书，通俗的讲这一步就是验证用户名密码的。所以也就是我们正常使用的认证方式，比如token，basic等。&lt;/p&gt;

&lt;p&gt;认证却支持好几种方式:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;客户端证书&lt;/li&gt;
&lt;li&gt;Bearer Tokens

&lt;ul&gt;
&lt;li&gt;Service Account Token&lt;/li&gt;
&lt;li&gt;BootStrap Token&lt;/li&gt;
&lt;li&gt;Static Token&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;HTTP Basic Auth&lt;/li&gt;
&lt;li&gt;Authenticating Proxy&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;1-客户端证书&#34;&gt;1、客户端证书&lt;/h4&gt;

&lt;p&gt;x509认证是默认开启的认证方式，api-server启动时会指定ca证书以及ca私钥，只要是通过ca签发的客户端x509证书，则可认为是可信的客户端。所以带着签发的客户端证书是可以通过https的安全验证的。&lt;/p&gt;

&lt;p&gt;我们在来看看证书，签发客户端证书有两种方式，一种是基于CA根证书签发证书，另一个种是发起CSR(Certificate Signing Requests)请求。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;使用CA根证书签发客户端证书&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用CA根证书需要CA的私钥，假设要创建一个int32bit用户，所属的组为int32bit，使用openssl签发证书:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl genrsa -out int32bit.key 2048 # 生成私钥
openssl req -new -key int32bit.key -out int32bit.csr -subj &amp;quot;/CN=int32bit/O=int32bit&amp;quot; # 发起CSR请求
openssl x509 -req -in int32bit.csr -CA $CA_LOCATION/ca.crt -CAkey $CA_LOCATION/ca.key -CAcreateserial -out int32bit.crt -days 365 # 基于CSR文件签发x509证书
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中CA_LOCATION为api server的CA证书路径，使用kubeadm部署一般为/etc/kubernetes/pki/。&lt;/p&gt;

&lt;p&gt;最后生成config文件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-credentials int32bit \
  --client-certificate=int32bit.crt \
  --client-key=int32bit.key \
  --embed-certs=true \
  --kubeconfig=&amp;quot;int32bit@int32bit-kubernetes.config.config&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意使用&amp;ndash;embed-certs参数，这样才会把证书内容填充到kubeconfig文件，否则仅填充证书路径。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过CSR签发证书&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;前面通过CA签发证书需要有CA的私钥，其实Kubernetes可以直接发起CSR请求。&lt;/p&gt;

&lt;p&gt;首先创建一个CSR请求，CN为test-csr，O为int32bit，即User为test-csr，Group为int32bbit。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl req -new -newkey rsa:4096 \
  -nodes -keyout test-csr.key \
  -out test-csr.csr -subj &amp;quot;/CN=test-csr/O=int32bit&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;声明一个CSR Resource:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
  name: test-csr
spec:
  groups:
  - int32bit
  request: ... # 这里填test-csr.csr内容并转化为base64
  usages:
  - client auth
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建该资源:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl apply -f test-csr.yaml
certificatesigningrequest.certificates.k8s.io/test-csr created
# kubectl get csr
NAME       AGE   REQUESTOR          CONDITION
test-csr   4s    kubernetes-admin   Pending
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时CSR的状态为Pending，通过kubectl certificate approve命令签发证书:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl certificate approve test-csr
certificatesigningrequest.certificates.k8s.io/test-csr approved
# kubectl get csr
NAME       AGE   REQUESTOR          CONDITION
test-csr   2m    kubernetes-admin   Approved,Issued
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时CSR显示已经完成签发，可以读取证书内容:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl get csr test-csr -o jsonpath=&#39;{.status.certificate}&#39; | base64 -d
-----BEGIN CERTIFICATE-----
MIIEDTCCAvWgAwIBAgIUB9dVsj34xnQ8m5KUQwpdblWapNcwDQYJKoZIhvcNAQEL
...
yvfz8hcwrhQc6APpmZcBnil7iyzia3tnztQjoyaZ0cjC
-----END CERTIFICATE-----
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看证书部分摘要信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl get csr test-csr -o jsonpath=&#39;{.status.certificate}&#39; \
  | base64 -d \
  | openssl x509  -noout  \
  -subject  -issuer
subject=O = int32bit, CN = test-csr
issuer=CN = kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置kubeconfig使用证书认证的方式和前面的一样，这里不再赘述。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可见，客户端向k8s提交带有用户名、用户组等信息的CSR，然后管理员在k8s上为该客户签发证书。当然也可以基于ca私钥的基础上直接使用openssl签发证书，用户名和密码都是隐藏在证书中的，以后，用户使用该证书去访问k8s，api-server就能够辨识出用户。结合RBAC的配置，如果该用户或者用户组通过rolebinding绑定了role，那么该用户就拥有该role对对应资源的操作权限。&lt;/p&gt;

&lt;p&gt;我们搭建k8s过程中签发的CA证书就是一个客户端证书，是一个拥有集群管理员权限的证书。&lt;/p&gt;

&lt;p&gt;使用x509证书相对静态密码来说显然会更安全，只要证书不泄露，可以认为是无懈可击的。但是虽然颁发证书容易，目前却没有很好的方案注销证书。想想如果某个管理员离职，该如何回收他的权限和证书。有人说，证书轮转不就解决了吗？但这也意味着需要重新颁发其他所有证书，非常麻烦。&lt;/p&gt;

&lt;p&gt;所以使用x509证书认证适用于Kubernetes内部组件之间认证，普通用户认证并不推荐通过证书的形式进行认证。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;身份识别&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面说了，用户信息是隐藏在证书中的，我们来看一下，正常我们可以重kubectl的kubeconfig中获取用户的证书内容，一般是一个base64加密的内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// admin-client-certificate.txt
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQWRxZ0F3SUJBZ0lJZjJkVlJqbThFTFF3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB4T0RBMU1UUXdPREUzTVROYUZ3MHhPVEExTVRRd09ERTNNVGRhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXhCbjNqZHc4MGIxR2ZiNnMKdzJOcnFwTG90TVQ0bnlBZjJIaHFNclhqbk8rd25hSzFBSVRPdy8yMm1EajByd0l1SndkUUlqNS9CYUY2M3BQRQoxcFUwdmhJUFZLNG42Skk0ZG1Nem8vbFIzalpwR2VaVzF6ZFhhQ292dzljN2NsYmlIby9tRkc0eHF5dFZMZlg0Ci9TOG1GcDJBOVFjaWVKR0lvNVMwQlIzRlpsVTFQTTdEUmJMRFZWcTFQZHlOWTJHZnNiR3JIbEdnWHZXQUtDZC8KSDc5Z0FxVm9UWGpTSVdDVll1WWNvTHZkdlZYUVNJaVlscFhGUDFqQlFMdmNVN3ZycXRiMTJSbXJ4bnBrVzRwbApkR0VPWDJzTG1mWVo1VGlGcGtSd3oyR3hzbVd5UmJ0Nk91SVNKRkk2UlowcitSbjR5TURLUHJZbEVuZ0RWYzVLClBaNXptd0lEQVFBQm95Y3dKVEFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFFWk5UdlR6Mk9nekNVZHZNRmJyaFBzcCttRDJ2UGpNUkN4aQozQmtBMTB2SUNPU2ZkeW1NbjhhdzBJYktZejJnUWJYcVVmcXpRbVFmYTNpZitRWUJrQis3N3pmc3Y5YW00RVAvCmU2VGc1MnRxVjJQN3MyZUY3dE5BZTIwR3lWNnlGbFExUVVXNS9NNE0rSk1sVitCVWJsOXlFeVFsRU51Y0tmK3UKVFB5S0tUVXR6dlVZcjVFM0VKa3Q4NEVRSU52dzJuUjJqTnZlWjFYV09saVVyS2ZqSEh0ZnZPL241NlVTdUk0dwp1MkxUbElDUmNqNGcrWldsSWplTUZrR3lQYkp5SkFRNjVQMnNHclptMWtsR0dIM216d081Q1AxeVpXdm9VampQCmp6U2pNQ0lhSy9mUjhlUkFKNnExdFQ2YkcyNkwrbmprS0NRRFdLcGpBV09hcHVST2Niaz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后针对该文件数据做base64解码，得到client certificate文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat admin-client-certificate.txt | base64 -d &amp;gt; admin-client.crt

# cat admin-client.crt

-----BEGIN CERTIFICATE-----
MIIC8jCCAdqgAwIBAgIIf2dVRjm8ELQwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
AxMKa3ViZXJuZXRlczAeFw0xODA1MTQwODE3MTNaFw0xOTA1MTQwODE3MTdaMDQx
FzAVBgNVBAoTDnN5c3RlbTptYXN0ZXJzMRkwFwYDVQQDExBrdWJlcm5ldGVzLWFk
bWluMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxBn3jdw80b1Gfb6s
w2NrqpLotMT4nyAf2HhqMrXjnO+wnaK1AITOw/22mDj0rwIuJwdQIj5/BaF63pPE
1pU0vhIPVK4n6JI4dmMzo/lR3jZpGeZW1zdXaCovw9c7clbiHo/mFG4xqytVLfX4
/S8mFp2A9QcieJGIo5S0BR3FZlU1PM7DRbLDVVq1PdyNY2GfsbGrHlGgXvWAKCd/
H79gAqVoTXjSIWCVYuYcoLvdvVXQSIiYlpXFP1jBQLvcU7vrqtb12RmrxnpkW4pl
dGEOX2sLmfYZ5TiFpkRwz2GxsmWyRbt6OuISJFI6RZ0r+Rn4yMDKPrYlEngDVc5K
PZ5zmwIDAQABoycwJTAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYBBQUH
AwIwDQYJKoZIhvcNAQELBQADggEBAEZNTvTz2OgzCUdvMFbrhPsp+mD2vPjMRCxi
3BkA10vICOSfdymMn8aw0IbKYz2gQbXqUfqzQmQfa3if+QYBkB+77zfsv9am4EP/
e6Tg52tqV2P7s2eF7tNAe20GyV6yFlQ1QUW5/M4M+JMlV+BUbl9yEyQlENucKf+u
TPyKKTUtzvUYr5E3EJkt84EQINvw2nR2jNveZ1XWOliUrKfjHHtfvO/n56USuI4w
u2LTlICRcj4g+ZWlIjeMFkGyPbJyJAQ65P2sGrZm1klGGH3mzwO5CP1yZWvoUjjP
jzSjMCIaK/fR8eRAJ6q1tT6bG26L+njkKCQDWKpjAWOapuROcbk=
-----END CERTIFICATE-----
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看证书内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl x509 -in ./admin-client.crt -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 9180400125522743476 (0x7f67554639bc10b4)
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=kubernetes
        Validity
            Not Before: May 14 08:17:13 2018 GMT
            Not After : May 14 08:17:17 2019 GMT
        Subject: O=system:masters, CN=kubernetes-admin
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)

   ... ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从证书输出的信息中，我们看到了下面这行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Subject: O=system:masters, CN=kubernetes-admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;k8s apiserver对kubectl的请求进行client certificate验证(通过ca证书–client-ca-file=/etc/kubernetes/pki/ca.crt对其进行校验)，验证通过后kube-apiserver会得到：group = system:masters的http上下文信息，并传给后续的authorizers。&lt;/p&gt;

&lt;h4 id=&#34;2-bearer-token&#34;&gt;2、bearer token&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Service Account Token&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;service account是Kubernetes唯一由自己管理的账号实体，意味着service account可以通过Kubernetes创建，不过这里的service account并不是直接和User关联的，service account是namespace作用域，而User是全cluster唯一。service account会对应一个虚拟User，User名为system:serviceaccount:${namespace}:${sa_name}，比如在default namespace的test service account，则对应的虚拟User为system:serviceaccount:default:test。&lt;/p&gt;

&lt;p&gt;和前面的bootstrap一样，service account也是使用Bearer Token认证的，不过和前面的Token不一样的是service account是基于JWT(JSON Web Token)认证机制，JWT原理和x509证书认证其实有点类似，都是通过CA根证书进行签名和校验，只是格式不一样而已，JWT由三个部分组成，每个部分由.分割，三个部分依次如下:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Header（头部）: Token的元数据，如alg表示签名算法，typ表示令牌类型，一般为JWT，kid表示Token ID等。&lt;/li&gt;
&lt;li&gt;Payload（负载): 实际存放的用户凭证数据，如iss表示签发人，sub签发对象，exp过期时间等。&lt;/li&gt;
&lt;li&gt;Signature（签名）：基于alg指定的算法生成的数字签名，为了避免被篡改和伪造。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了便于HTTP传输，JWT Token在传递过程中会转成Base64URL编码，其中Base64URL相对我们常用的Base64编码不同的是=被省略、+替换成-，/替换成_，这么做的原因是因为这些字符在URL里面有特殊含义。&lt;/p&gt;

&lt;p&gt;可以用如下脚本实现Kubernetes Service Account的Token解码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

# base64url解码
decode_base64_url() {
  LEN=$((${#1} % 4))
  RESULT=&amp;quot;$1&amp;quot;
  if [ $LEN -eq 2 ]; then
    RESULT+=&#39;==&#39;
  elif [ $LEN -eq 3 ]; then
    RESULT+=&#39;=&#39;
  fi
  echo &amp;quot;$RESULT&amp;quot; | tr &#39;_-&#39; &#39;/+&#39; | base64 -d
}

# 解码JWT
decode_jwt()
{
  JWT_RAW=$1
  for line in $(echo &amp;quot;$JWT_RAW&amp;quot; | awk -F &#39;.&#39; &#39;{print $1,$2}&#39;); do
    RESULT=$(decode_base64_url &amp;quot;$line&amp;quot;)
    echo &amp;quot;$RESULT&amp;quot; | python -m json.tool
  done
}

# 获取k8s sa token
get_k8s_sa_token()
{
  NAME=$1
  TOKEN_NAME=$(kubectl get sa &amp;quot;$NAME&amp;quot; -o jsonpath=&#39;{.secrets[0].name}&#39;)
  kubectl get secret &amp;quot;${TOKEN_NAME}&amp;quot; -o jsonpath=&#39;{.data.token}&#39; | base64 -d
}

main()
{
  NAME=$1
  if [[ -z $NAME ]]; then
    echo &amp;quot;Usage: $0 &amp;lt;secret_name&amp;gt;&amp;quot;
    exit 1
  fi
  TOKEN=$(get_k8s_sa_token &amp;quot;$NAME&amp;quot;)
  decode_jwt &amp;quot;$TOKEN&amp;quot;
}

main &amp;quot;$@&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从解码的数据可见，JWT Token的颁发机构为kubernetes/serviceaccount,颁发对象为SA对应的虚拟用户system:serviceaccount:default:test，除此之外还存储着其他的SA信息，如SA name、namespace、uuid等。这里需要注意的是我们发现JWT Token中没有exp字段，即意味着只要这个SA存在，这个Token就是永久有效的。&lt;/p&gt;

&lt;p&gt;通过如下方式配置kubeconfig:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TOKEN_NAME=$(kubectl get serviceaccounts ${SA_NAME} -n ${namespace} -o jsonpath={.secrets[0].name})
TOKEN=$(kubectl get secret &amp;quot;${TOKEN_NAME}&amp;quot; -n ${namespace} -o jsonpath={.data.token} | base64 -d)
kubectl config set-credentials &amp;quot;${USERNAME}&amp;quot; --token=&amp;quot;$TOKEN&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了验证test-sa，在刚刚创建的int32bit-rolebinding的subjects增加了ServiceAccount test-sa。和预期一样，test-sa能够读取pod列表但没有删除pod权限。&lt;/p&gt;

&lt;p&gt;service account除了可以用于集群外认证外，其还有一个最大的特点是可以通过Pod.spec.serviceAccountName把token attach到Pod中。在 Pod的Spec 中指明 ServiceAccount ，service account本来是系统服务使用的账户，当我们创建service account时，它会自带一个secret，这个secret就是token。我们通过这个token就能来访问 ApiServer，访问系统资源。&lt;/p&gt;

&lt;p&gt;此时Kubernetes会自动把SA的Token通过volume的形式挂载到/run/secrets/kubernetes.io/serviceaccount目录上，从而Pod可以读取token调用Kubernetes API.&lt;/p&gt;

&lt;p&gt;针对一些需要和Kubernetes API交互的应用非常有用，比如coredns就需要监控endpoints、services的变化，因此关联了coredns SA，coredns又关联了system:coredns clusterrole。flannel需要监控pods以及nodes变化同样关联了flannel SA。&lt;/p&gt;

&lt;p&gt;到这里为止，service account可能是Kubernetes目前最完美的认证方案了，既能支持集群外的客户端认证，又支持集群内的Pod关联授权。&lt;/p&gt;

&lt;p&gt;但事实上，service account并不是设计用来给普通user认证的，默认enabled，通常被 pod 所使用,是给集群内部服务使用的。目前虽然token是永久有效的，但未来会改成使用动态token的方式，参考官方设计设计文档Bound Service Account Tokens，此时如果kubectl客户端认证则需要频繁更新token。&lt;/p&gt;

&lt;p&gt;除此之外，SA虽然能够对应一个虚拟User，但不支持自定义Group，在授权体系中不够灵活。另外也不支持客户端高级认证功能，比如MFA、SSO等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Bootstrap Tokens&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;bootstrap token则是由Kubernetes动态生成的，通过Secret形式存储，并且具有一定的生命周期，一旦过期就会失效。不像静态token，只要启动参数不变，token就不会变化。&lt;/p&gt;

&lt;p&gt;我们使用kubeadm会生成一个token:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubeadm token list
TOKEN                     TTL       EXPIRES                USAGES                   DESCRIPTION   EXTRA GROUPS
bpjp71.6ckt2g3o3hso3gn4   23h       2019-12-15T11:58:13Z   authentication,signing   &amp;lt;none&amp;gt;        system:bootstrappers:kubeadm:default-node-token
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Token有两个部分组成，由.分割，前面部分为Token ID bpjp71，后面部分为Token Secret 6ckt2g3o3hso3gn4。Token默认TTL为一天，对应的group为system:bootstrappers:kubeadm:default-node-token，对应User为system:bootstrap:${Token ID}。&lt;/p&gt;

&lt;p&gt;kubeadm创建一个Token会对应在Kubernetes的kube-system namespace创建一个secret，secret名为bootstrap-token-${TOKEN_ID}，这里为bootstrap-token-bpjp71。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl get secret bootstrap-token-bpjp71 -n kube-system -o yaml --export
apiVersion: v1
data:
  auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=
  expiration: MjAxOS0xMi0xNVQxMTo1ODoxM1o=
  token-id: YnBqcDcx
  token-secret: NmNrdDJnM28zaHNvM2duNA==
  usage-bootstrap-authentication: dHJ1ZQ==
  usage-bootstrap-signing: dHJ1ZQ==
kind: Secret
metadata:
  name: bootstrap-token-bpjp71
type: bootstrap.kubernetes.io/token
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时可以通过如下命令生成config:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-credentials bootstrap \
  --user bootstrap \
  --token bpjp71.6ckt2g3o3hso3gn4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了验证boostrap token，我们把用户添加到int32bit-role中，注意对应的虚拟User名。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl  describe  rolebindings int32bit-rolebinding
Name:         int32bit-rolebinding
Labels:       &amp;lt;none&amp;gt;
Annotations:  &amp;lt;none&amp;gt;
Role:
  Kind:  Role
  Name:  int32bit-role
Subjects:
  Kind   Name                     Namespace
  ----   ----                     ---------
  Group  int32bit
  User   system:bootstrap:bpjp71
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种token主要用于临时授权使用，比如kubeadm初始化集群时会生成一个bootstrap token，这个token具有创建certificatesigningrequests权限，从而新Node能够发起CSR请求，请求客户端证书。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Static Token&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;静态token认证和静态密码原理几乎完全一样，唯一不同的是静态token通过token-auth-file指定token文件，文件内容也是和静态密码一样，认证时头部格式为Authorization: Bearer ${Token}，&lt;/p&gt;

&lt;p&gt;因此其优点和缺点也和静态密码完全一样，这里不再赘述。&lt;/p&gt;

&lt;h4 id=&#34;3-http-basic-auth&#34;&gt;3、HTTP Basic Auth&lt;/h4&gt;

&lt;p&gt;静态密码是最简单的认证方式，只需要在api-server启动时指定使用的密码本路径即可:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--basic-auth-file=/etc/static_secret/passwd

Static Password File: 以参数 --basic-auth-file=&amp;lt;SOMEFILE&amp;gt; 指明 basic auth file 的位置。
这个 basic auth file 以 csv 文件的形式存在，里面至少包含三个信息：password、username、uid（user id）、group（一些k8s常用的系统角色），比如NoMoreSecret,int32bit-1,1000,&amp;quot;int32bit&amp;quot; ，此时定义了一个用户int32bit-1，静态密码为NoMoreSecret，所属Group为intt32bit。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置了静态密码，就可以对什么用户属于什么group，从而对于当前group有哪些权限，做到了对用户的权限管理。&lt;/p&gt;

&lt;p&gt;使用http请求认证的时候，认证头部为Basic base64encode(${username}:${password})。&lt;/p&gt;

&lt;p&gt;我们也可以使用kubectl请求，这时候会使用凭证config文件，就可以这样生成&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-credentials int32bit-1 --username=int32bit-1 --password=NoMoreSecret
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以使用kubectl来带着这个认证来操作了。&lt;/p&gt;

&lt;p&gt;通过静态密码的唯一优势是简单，其缺点也是非常明显:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;静态密码是明文，非常不安全，还有可能被暴力破解。
非常不灵活，增加或者删除用户，必须手动修改静态密码文件并重启所有的api-server服务。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方式在实际场景中很少被使用，不建议生产环境使用。&lt;/p&gt;

&lt;h4 id=&#34;集成外部认证系统&#34;&gt;集成外部认证系统&lt;/h4&gt;

&lt;p&gt;Kubernetes最强大的功能是支持集成第三方Id Provider（IdP），主流的如AD、LADP以及OpenStack Keystone等，毕竟专业的人做专业的事。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过OpenID Connect集成keycloak认证系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当前支持OpenID Connect的产品有很多，如:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Keycloak
UAA
Dex
OpenUnison
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、keycloak配置&lt;/p&gt;

&lt;p&gt;由于Kubernetes要求必须是https，测试环境需要签发自己的CA，参考为Kubernetes 搭建支持 OpenId Connect 的身份认证系统:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
mkdir -p ssl
cat &amp;lt;&amp;lt; EOF &amp;gt; ssl/ca.cnf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name

[req_distinguished_name]

[ v3_req ]
basicConstraints = CA:TRUE
EOF
cat &amp;lt;&amp;lt; EOF &amp;gt; ssl/req.cnf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name

[req_distinguished_name]

[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
IP.1 = 192.168.193.172
EOF

openssl genrsa -out ssl/ca-key.pem 2048
openssl req -x509 -new -nodes -key ssl/ca-key.pem -days 365 -out ssl/ca.pem -subj &amp;quot;/CN=keycloak-ca&amp;quot; -extensions v3_req -config ssl/ca.cnf

openssl genrsa -out ssl/keycloak.pem 2048
openssl req -new -key ssl/keycloak.pem -out ssl/keycloak-csr.pem -subj &amp;quot;/CN=keycloak&amp;quot; -config ssl/req.cnf
openssl x509 -req -in ssl/keycloak-csr.pem -CA ssl/ca.pem -CAkey ssl/ca-key.pem -CAcreateserial -out ssl/keycloak.crt -days 365 -extensions v3_req -extfile ssl/req.cnf

# 生成 keystore 并导入 keypair
openssl pkcs12 -export -out ssl/keycloak.p12 -inkey ssl/keycloak.pem -in ssl/keycloak.crt -certfile ssl/ca.pem
keytool -importkeystore -deststorepass &#39;noMoreSecret&#39; -destkeystore ssl/keycloak.jks -srckeystore ssl/keycloak.p12 -srcstoretype PKCS12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于没有配置固定域名，因此添加了alt_names并指定IP。&lt;/p&gt;

&lt;p&gt;最后复制ssl/keycloak.p12到如下两个路径:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp ssl/keycloak.p12 keycloak-8.0.1/standalone/configuration/keycloak.jks
cp ssl/keycloak.p12 /etc/kubernetes/pki/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、keycloak认证信息配置&lt;/p&gt;

&lt;p&gt;登录keycloak管理页面创建一个realm以及client，名称都为int32bit-kubernetes。其中realm类似namespace概念，实现了多租户模型，client对应一个认证主体，所有使用keycloak认证的都需要创建一个对应的client。&lt;/p&gt;

&lt;p&gt;每个client会对应有一个secret，这二者关系就是access key和access secret关系:&lt;/p&gt;

&lt;p&gt;接下来通过Web管理页面执行如下操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;在Roles中创建两个role分别为int32bit-kubernetes-cluster-admin、int32bit-kubernetes-readonly。
在Users中创建两个用户k8s-admin、k8s-readonlly。
k8s-admin关联int32bit-kubernetes-cluster-admin role，k8s-readonlly关联int32bit-kubernetes-readonly role。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注: 管理员可以在User的Credentials面板中设置用户密码。&lt;/p&gt;

&lt;p&gt;通过curl检查是否可认证获取token:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -sSLk \
  -d &amp;quot;client_id=int32bit-kubernetes&amp;quot; \
  -d &amp;quot;client_secret=700eeab2-2f85-45a1-9904-297a0be4d4fd&amp;quot; \
  -d &amp;quot;response_type=code token&amp;quot; \
  -d &amp;quot;grant_type=password&amp;quot; \
  -d &amp;quot;username=k8s-admin&amp;quot; \
  -d &amp;quot;password=noMoreSecret&amp;quot; \
  -d &amp;quot;scope=openid&amp;quot; \
  https://192.168.193.172:8443/auth/realms/int32bit-kubernetes/protocol/openid-connect/token
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中返回的id_token，在后面Kubernetes对接中非常重要，它也是一个JWT Token，解码后的内容如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;alg&amp;quot;: &amp;quot;RS256&amp;quot;,
  &amp;quot;typ&amp;quot;: &amp;quot;JWT&amp;quot;,
  &amp;quot;kid&amp;quot;: &amp;quot;95WIuxaLj99XHhmytuQm4POZztFxYCaw3Pd-KyBGVVQ&amp;quot;
}
{
  &amp;quot;jti&amp;quot;: &amp;quot;39b94185-045a-4cee-b025-d0e0909d6bfd&amp;quot;,
  &amp;quot;exp&amp;quot;: 1576466962,
  &amp;quot;nbf&amp;quot;: 0,
  &amp;quot;iat&amp;quot;: 1576466662,
  &amp;quot;iss&amp;quot;: &amp;quot;https://192.168.193.172:8443/auth/realms/int32bit-kubernetes&amp;quot;,
  &amp;quot;aud&amp;quot;: &amp;quot;int32bit-kubernetes&amp;quot;,
  &amp;quot;sub&amp;quot;: &amp;quot;95374c54-c47f-42a5-9bb2-2e0e417a9ff2&amp;quot;,
  &amp;quot;typ&amp;quot;: &amp;quot;ID&amp;quot;,
  &amp;quot;azp&amp;quot;: &amp;quot;int32bit-kubernetes&amp;quot;,
  &amp;quot;auth_time&amp;quot;: 0,
  &amp;quot;session_state&amp;quot;: &amp;quot;0eafa8ba-6536-4f6a-989f-177e19e4882a&amp;quot;,
  &amp;quot;acr&amp;quot;: &amp;quot;1&amp;quot;,
  &amp;quot;email_verified&amp;quot;: false,
  &amp;quot;preferred_username&amp;quot;: &amp;quot;k8s-admin&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们发现id_token默认没有groups信息，为了支持Kubernetes的Group认证，需要在client中添加mappers字段groups。&lt;/p&gt;

&lt;p&gt;这里之所以映射User Realm Role，而不是Group MemberShip，是因为Group会在id_token中添加前缀/，如/test-group1,/test-group2，这个暂时没想到怎么处理，或许有更好的办法。&lt;/p&gt;

&lt;p&gt;再次生成token_id就会有groups信息了:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;jti&amp;quot;: &amp;quot;39b94185-045a-4cee-b025-d0e0909d6bfd&amp;quot;,
  &amp;quot;exp&amp;quot;: 1576466962,
  &amp;quot;nbf&amp;quot;: 0,
  &amp;quot;iat&amp;quot;: 1576466662,
  &amp;quot;iss&amp;quot;: &amp;quot;https://192.168.193.172:8443/auth/realms/int32bit-kubernetes&amp;quot;,
  &amp;quot;aud&amp;quot;: &amp;quot;int32bit-kubernetes&amp;quot;,
  &amp;quot;sub&amp;quot;: &amp;quot;95374c54-c47f-42a5-9bb2-2e0e417a9ff2&amp;quot;,
  &amp;quot;typ&amp;quot;: &amp;quot;ID&amp;quot;,
  &amp;quot;azp&amp;quot;: &amp;quot;int32bit-kubernetes&amp;quot;,
  &amp;quot;auth_time&amp;quot;: 0,
  &amp;quot;session_state&amp;quot;: &amp;quot;0eafa8ba-6536-4f6a-989f-177e19e4882a&amp;quot;,
  &amp;quot;acr&amp;quot;: &amp;quot;1&amp;quot;,
  &amp;quot;email_verified&amp;quot;: false,
  &amp;quot;groups&amp;quot;: [
    &amp;quot;int32bit-kubernetes-cluster-admin&amp;quot;
  ],
  &amp;quot;preferred_username&amp;quot;: &amp;quot;k8s-admin&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Kubernetes集成keycloak认证&lt;/p&gt;

&lt;p&gt;在api-server中增加如下命令行启动参数:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- --oidc-issuer-url=https://192.168.193.172:8443/auth/realms/int32bit-kubernetes
- --oidc-client-id=int32bit-kubernetes
- --oidc-username-claim=preferred_username
- --oidc-username-prefix=-
- --oidc-groups-claim=groups
- --oidc-ca-file=/etc/kubernetes/pki/keycloak.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数解析&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--oidc-issuer-url路径需要具体到realm，这里为int32bit-kubernetes；
--oidc-client-id对应client id，前面我们已经创建。
--oidc-username-claim、--oidc-groups-claim告诉Kubernetes如何从id_token中读取username和groups，根据前面解码后的id_token，我们不难选择。
--oidc-username-prefix告诉Kubernetes针对这个odic的用户需要添加什么前缀，如果集群同时有多个认证系统，建议添加个前缀加以区分，如指定前缀为odic:，则Kubernetes对应的User为odic: preferred_username。
--oidc-ca-file指定keycloak的根证书，因为不是权威证书，不指定则不会信任该证书。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前面创建了两个用户，与Role的关联关系如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;k8s-admin: int32bit-kubernetes-cluster-admin
k8s-readonly: int32bit-kubernetes-readonly
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相对应的在Kubernetes创建两个clusterrolebinging:&lt;/p&gt;

&lt;p&gt;cluster-admin:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &amp;quot;true&amp;quot;
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:masters
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: int32bit-kubernetes-cluster-admin
cluster-readonly:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &amp;quot;true&amp;quot;
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-readonly
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: int32bit-kubernetes-readonly
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、使用OpenId Connect认证&lt;/p&gt;

&lt;p&gt;生成config文件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-credentials oidc \
   --auth-provider=oidc \
   --auth-provider-arg=idp-issuer-url=https://192.168.193.172:8443/auth/realms/int32bit-kubernetes \
   --auth-provider-arg=client-id=int32bit-kubernetes \
   --auth-provider-arg=client-secret=700eeab2-2f85-45a1-9904-297a0be4d4fd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了便于登录，下载kube-login插件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl krew install oidc-login
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时可以直接通过如下命令进行登录:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl oidc-login --username username --password password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是图形界面，不指定参数直接使用kubectl oidc-login可自动打开浏览器进行登录校验。&lt;/p&gt;

&lt;p&gt;可见使用k8s-admin具有所有权限，而k8s-readonly只有list的权限。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过webhook集成OpenStack Keystone&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;webhook和odic一样也是集成外部认证系统的一种方式，当client发起api-server请求时会触发webhook服务TokenReview调用，webhook会检查用户的凭证信息，如果是合法则返回authenticated&amp;rdquo;: true等信息。api-server会等待webhook服务返回，如果返回的authenticated结果为true，则表明认证成功，否则拒绝访问。&lt;/p&gt;

&lt;p&gt;1、OpenStack Keystone配置&lt;/p&gt;

&lt;p&gt;为了后续测试，我们在Keystone创建如下资源:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
openstack project create int32bit-kubernetes
USERS_AND_ROLES=(k8s-admin k8s-viewer)
for i in &amp;quot;${USERS_AND_ROLES[@]}&amp;quot;; do
  openstack user create --project int32bit-kubernetes --password noMoreSecret &amp;quot;$i&amp;quot;
  openstack role create &amp;quot;$i&amp;quot;
  openstack role add --user &amp;quot;$i&amp;quot; --project int32bit-kubernetes &amp;quot;$i&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中k8s-admin user关联k8s-admin role，k8s-viewer user关联k8s-viewer role，我们根据不同role角色设置不同的权限：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: k8s-auth-policy
  namespace: kube-system
data:
  policies: |
    [
      {
        &amp;quot;resource&amp;quot;: {
          &amp;quot;verbs&amp;quot;: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;],
          &amp;quot;resources&amp;quot;: [&amp;quot;*&amp;quot;],
          &amp;quot;version&amp;quot;: &amp;quot;*&amp;quot;,
          &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;
        },
        &amp;quot;match&amp;quot;: [
          {
            &amp;quot;type&amp;quot;: &amp;quot;role&amp;quot;,
            &amp;quot;values&amp;quot;: [&amp;quot;k8s-viewer&amp;quot;, &amp;quot;k8s-admin&amp;quot;]
          },
          {
            &amp;quot;type&amp;quot;: &amp;quot;project&amp;quot;,
            &amp;quot;values&amp;quot;: [&amp;quot;int32bit-kubernetes&amp;quot;]
          }
        ]
      },
      {
        &amp;quot;resource&amp;quot;: {
          &amp;quot;verbs&amp;quot;: [&amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot;],
          &amp;quot;resources&amp;quot;: [&amp;quot;*&amp;quot;],
          &amp;quot;version&amp;quot;: &amp;quot;*&amp;quot;,
          &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;
        },
        &amp;quot;match&amp;quot;: [
          {
            &amp;quot;type&amp;quot;: &amp;quot;role&amp;quot;,
            &amp;quot;values&amp;quot;: [&amp;quot;k8s-admin&amp;quot;]
          },
          {
            &amp;quot;type&amp;quot;: &amp;quot;project&amp;quot;,
            &amp;quot;values&amp;quot;: [&amp;quot;int32bit-kubernetes&amp;quot;]
          }
        ]
      }
    ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如上policy配置中，k8s-viewer只允许读namesapce default的资源，而k8s-admin允许create、update以及delete等所有权限。&lt;/p&gt;

&lt;p&gt;创建如上configmap:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f keystone-auth.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、配置k8s-keystone-auth webhook插件&lt;/p&gt;

&lt;p&gt;安装和配置k8s-keystone-auth可参考官方文档k8s-keystone-auth&lt;/p&gt;

&lt;p&gt;安装完后验证webhook认证结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
keystone_auth_service_addr=$(kubectl get svc keystone-auth -o jsonpath={.spec.clusterIP})
token=$(openstack token issue -f shell | awk -F &#39;=&#39; &#39;/^id=.*/{print $2}&#39; | tr -d &#39;&amp;quot;&#39;)
cat &amp;lt;&amp;lt;EOF | curl -ks -XPOST -d @- https://${keystone_auth_service_addr}:8443/webhook | python -mjson.tool
{
  &amp;quot;apiVersion&amp;quot;: &amp;quot;authentication.k8s.io/v1beta1&amp;quot;,
  &amp;quot;kind&amp;quot;: &amp;quot;TokenReview&amp;quot;,
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;creationTimestamp&amp;quot;: null
  },
  &amp;quot;spec&amp;quot;: {
    &amp;quot;token&amp;quot;: &amp;quot;$token&amp;quot;
  }
}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出如果authenticated&amp;rdquo;: true则说明认证成功。&lt;/p&gt;

&lt;p&gt;当然也可以验证webhook的授权，如验证k8s-viewer是否具有list pods权限:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;keystone_auth_service_addr=$(kubectl get svc keystone-auth -o jsonpath={.spec.clusterIP})
cat &amp;lt;&amp;lt;EOF | curl -ks -XPOST -d @- https://${keystone_auth_service_addr}:8443/webhook | python -mjson.tool
{
  &amp;quot;apiVersion&amp;quot;: &amp;quot;authorization.k8s.io/v1beta1&amp;quot;,
  &amp;quot;kind&amp;quot;: &amp;quot;SubjectAccessReview&amp;quot;,
  &amp;quot;spec&amp;quot;: {
    &amp;quot;resourceAttributes&amp;quot;: {
      &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;,
      &amp;quot;verb&amp;quot;: &amp;quot;list&amp;quot;,
      &amp;quot;group&amp;quot;: &amp;quot;&amp;quot;,
      &amp;quot;resource&amp;quot;: &amp;quot;pods&amp;quot;
    },
    &amp;quot;user&amp;quot;: &amp;quot;k8s-viewer&amp;quot;,
    &amp;quot;group&amp;quot;: [&amp;quot;423d41d3a02f4b77b4a9bbfbc3a1b3c6&amp;quot;],
    &amp;quot;extra&amp;quot;: {
        &amp;quot;alpha.kubernetes.io/identity/project/id&amp;quot;: [&amp;quot;7c266ba4f14d4a64bda0b6b562f2cd60&amp;quot;],
        &amp;quot;alpha.kubernetes.io/identity/project/name&amp;quot;: [&amp;quot;int32bit-kubernetes&amp;quot;],
        &amp;quot;alpha.kubernetes.io/identity/roles&amp;quot;: [&amp;quot;k8s-viewer&amp;quot;]
    }
  }
}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果输出&amp;rdquo;allowed&amp;rdquo;: true，则说明具有list pods权限。&lt;/p&gt;

&lt;p&gt;3、配置Kubernetes使用keystone webhook&lt;/p&gt;

&lt;p&gt;按照官方文档，创建webhook conf文件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;keystone_auth_service_addr=$(kubectl get svc keystone-auth -o jsonpath={.spec.clusterIP})
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/kubernetes/pki/webhookconfig.yaml
---
apiVersion: v1
kind: Config
preferences: {}
clusters:
  - cluster:
      insecure-skip-tls-verify: true
      server: https://${keystone_auth_service_addr}:8443/webhook
    name: webhook
users:
  - name: webhook
contexts:
  - context:
      cluster: webhook
      user: webhook
    name: webhook
current-context: webhook
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改api-server配置文件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -i &#39;/image:/ i \ \ \ \ - --authentication-token-webhook-config-file=/etc/kubernetes/pki/webhookconfig.yaml&#39; /etc/kubernetes/manifests/kube-apiserver.yaml
sed -i &#39;/image:/ i \ \ \ \ - --authorization-webhook-config-file=/etc/kubernetes/pki/webhookconfig.yaml&#39; /etc/kubernetes/manifests/kube-apiserver.yaml
sed -i &amp;quot;/authorization-mode/c \ \ \ \ - --authorization-mode=Node,Webhook,RBAC&amp;quot; /etc/kubernetes/manifests/kube-apiserver.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如上开启了基于Webhook授权功能，如果仅使用Keystone认证而不使用Keystone授权，可以不开启。&lt;/p&gt;

&lt;p&gt;4、使用Keystone认证&lt;/p&gt;

&lt;p&gt;下载webhook插件，用于请求认证时获取keystone token：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -sSL https://api.nz-por-1.catalystcloud.io:8443/v1/AUTH_b23a5e41d1af4c20974bf58b4dff8e5a/lingxian-public/client-keystone-auth -o ~/keystone/client-keystone-auth
chmod +x ~/keystone/client-keystone-auth
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过如下脚本生成kubeconfig文件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
kubectl config set-cluster &amp;quot;int32bit-kubernetes&amp;quot; \
  --certificate-authority=&amp;quot;/etc/kubernetes/pki/ca.crt&amp;quot; \
  --server https://192.168.193.172:6443 \
  --embed-certs=true \
  --kubeconfig=keystone@int32bit-kubernetes.config

kubectl config set-credentials keystone \
  --kubeconfig keystone@int32bit-kubernetes.config
sed -i &#39;/user: {}/ d&#39; keystone@int32bit-kubernetes.config
cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; keystone@int32bit-kubernetes.config
  user:
    exec:
      command: &amp;quot;/root/keystone/client-keystone-auth&amp;quot;
      apiVersion: &amp;quot;client.authentication.k8s.io/v1beta1&amp;quot;
EOF
kubectl config set-context \
  --cluster=int32bit-kubernetes \
  --user=keystone keystone@int32bit-kubernetes \
  --namespace=default --kubeconfig keystone@int32bit-kubernetes.config
cp keystone@int32bit-kubernetes.config \
  ~/users-credentials/credentials/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置完后就可以通过Keystone实现认证了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat k8s_viewere_openrc
export OS_DOMAIN_NAME=Default
export OS_USERNAME=k8s-viewer
export OS_PASSWORD=noMoreSecret
export OS_PROJECT_NAME=int32bit-kubernetes
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://192.168.193.77:5000/v3
export OS_IDENTITY_API_VERSION=3
# source k8s_viewere_openrc
# kubectl get pod nginx-7cfc94d94-w8tpl
NAME                    READY   STATUS    RESTARTS   AGE
nginx-7cfc94d94-w8tpl   1/1     Running   0          13d
# kubectl delete pod nginx-7cfc94d94-w8tpl
Error from server (Forbidden): pods &amp;quot;nginx-7cfc94d94-w8tpl&amp;quot; is forbidden: User &amp;quot;k8s-viewer&amp;quot; cannot delete resource &amp;quot;pods&amp;quot; in API group &amp;quot;&amp;quot; in the namespace &amp;quot;default&amp;quot;
可见k8s-viewer用户可以查看pod但没有删除pod的权限。

# cat k8s_admin_openrc
export OS_DOMAIN_NAME=Default
export OS_USERNAME=k8s-admin
export OS_PASSWORD=noMoreSecret
export OS_PROJECT_NAME=int32bit-kubernetes
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://192.168.193.77:5000/v3
export OS_IDENTITY_API_VERSION=3
# source  k8s_admin_openrc
# kubectl get pod nginx-7cfc94d94-w8tpl
NAME                    READY   STATUS    RESTARTS   AGE
nginx-7cfc94d94-w8tpl   1/1     Running   0          13d
# kubectl delete pod nginx-7cfc94d94-w8tpl
pod &amp;quot;nginx-7cfc94d94-w8tpl&amp;quot; deleted
k8s-admin用户既可以查看pod，也可以删除pod。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以通过kubectl-access_matrix插件查看权限矩阵:&lt;/p&gt;

&lt;p&gt;进一步说明k8s-admin用户具有default namespace的所有权限，而k8s-viewer只具有可读权限。&lt;/p&gt;

&lt;p&gt;如果企业已经部署OpenStack，Kubernetes运行在OpenStack平台之上，或者通过Magnum部署，集成Keystone实现Kubernetes认证和授权非常方便，很好地把Kubernetes的认证和授权与OpenStack的认证授权统一管理整合在一块。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;使用外部认证系统的优势&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对比前面的认证方式，使用OpenID Connect认证以及基于Webhook的认证方式优势显而易见:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;安全。基于JWT Token交换认证，JWT具有数字签名，可避免伪造。并且相对Service Account JWT，OpenID Connect认证的JWT具有有效期。
灵活。身份认证和集群本身是松耦合的，通过IDP配置账户信息不需要Kubernetes干预。
认证功能丰富。可使用企业身份系统的MFA、SSO等功能实现更完善更安全的认证策略。
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;总结&#34;&gt;总结&lt;/h4&gt;

&lt;p&gt;1、静态密码和静态token认证策略的优点是非常简单，缺点是非常不安全和不灵活，不推荐使用。&lt;/p&gt;

&lt;p&gt;2、x509证书认证本身的安全性保障没有问题，最大的问题是不支持证书回收，意味着一旦证书颁发出去就很难在回收过来。这种认证策略适合集群内部组件之间的认证通信。&lt;/p&gt;

&lt;p&gt;3、bootstrap token适合需要临时授权的场景，如集群初始化。&lt;/p&gt;

&lt;p&gt;4、service account基于JWT认证，JWT包含的字段比较简单，没有有效期和aud字段，存在安全隐患，不适用于普通用户认证，适用于集群内的Pod向api-server认证，如kube-proxy和flannel需要调用api-server监控service和pod的状态变化。&lt;/p&gt;

&lt;p&gt;5、OpenID Connect(oidc)以及webhook可集成企业已有的身份认证系统，如AD、LDAP，其特点是安全、灵活、功能全面，并且身份认证与Kubernetes集群解耦合，非常适用于普通用户的认证，推荐使用。&lt;/p&gt;

&lt;h3 id=&#34;授权机制-authorization&#34;&gt;授权机制（Authorization）&lt;/h3&gt;

&lt;p&gt;当用户通过认证后，k8s 的授权机制将对用户的行为等进行授权检查。换句话说，就是对这个请求本身，是否对某资源、某 namespace、某操作有权限限制。&lt;/p&gt;

&lt;p&gt;若要开启某种模式，需要在 APIServer 启动时，设置参数 &amp;ndash;authorization-mode=RBAC。授权机制目前有 4 种模式：RBAC、ABAC、Node、Webhook。&lt;/p&gt;

&lt;h4 id=&#34;1-rbac&#34;&gt;1、RBAC&lt;/h4&gt;

&lt;p&gt;基于角色的权限访问控制，就是对某个用户赋予某个角色，而这个角色通常决定了对哪些资源拥有怎样的权限。&lt;/p&gt;

&lt;p&gt;首先需要用户，所以我们正常创建sa，然后给他授权&lt;/p&gt;

&lt;p&gt;1、创建sa&lt;/p&gt;

&lt;p&gt;创建一个 ServiceAccount 很简单，只需要指定其所在 namespace 和 name 即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: hdls
  name: hdls-sa
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、定义角色&lt;/p&gt;

&lt;p&gt;RBAC 中最重要的概念就是 Role 和 RoleBinding。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Role 定义了一组对 Kubernetes API 对象的操作权限。&lt;/li&gt;
&lt;li&gt;RoleBinding 则定义的是具体的 ServiceAccount 和 Role 的对应关系。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: hdls
  name: hdls-role
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;pods&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;namespace： 在这里仅限于逻辑上的“隔离”，并不会提供任何实际的隔离或者多租户能力；&lt;/li&gt;
&lt;li&gt;rules：定义的是权限规则，允许“被作用者”，对 hdls 下面的 Pod 对象，进行 GET 和 LIST 操作；

&lt;ul&gt;
&lt;li&gt;apiGroups：为 &amp;ldquo;&amp;rdquo; 代表 core API Group；&lt;/li&gt;
&lt;li&gt;resources：指的是资源类型&lt;/li&gt;
&lt;li&gt;verbs： 指的是具体的操作，当前 Kubernetes（v1.11）里能够对 API 对象进行的所有操作有 &amp;ldquo;get&amp;rdquo;, &amp;ldquo;list&amp;rdquo;, &amp;ldquo;watch&amp;rdquo;, &amp;ldquo;create&amp;rdquo;, &amp;ldquo;update&amp;rdquo;, &amp;ldquo;patch&amp;rdquo;, &amp;ldquo;delete&amp;rdquo;。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、绑定角色&lt;/p&gt;

&lt;p&gt;RoleBinding 则定义的是具体的 ServiceAccount 和 Role 的对应关系。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hdls-rolebinding
  namespace: hdls
subjects:
- kind: ServiceAccount
  name: hdls-sa
  apiGroup: &amp;quot;&amp;quot;
roleRef:
  kind: Role
  name: hdls-role
  apiGroup: &amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;这个 RoleBinding 对象里定义了一个 subjects 字段，即“被作用者”。它的类型是 ServiceAccount，就是上面创建的 sa。这个 subjects 还可以是 User 和 Group，User 是指 k8s 里的用户，而 Group 是指 ServiceAccounts。&lt;/li&gt;
&lt;li&gt;roleRef 字段是用来直接通过名字，引用我们前面定义的 Role 对象（hdls-role），从而定义了 Subject 和 Role 之间的绑定关系。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、使用&lt;/p&gt;

&lt;p&gt;我们再用 kubectl get sa -n hdls -o yaml 命令查看之前的 ServiceAccount，就可以看到 ServiceAccount.secret，这是因为 k8s 会为一个 ServiceAccount 自动创建并分配一个 Secret 对象，而这个 Secret 就是用来跟 APIServer 进行交互的授权文件： Token。
Token 文件的内容一般是证书或者密码，以一个 Secret 对象的方式保存在 etcd 当中。&lt;/p&gt;

&lt;p&gt;这个时候，我们在这个namespace下创建我们的 Pod 的 YAML 文件中定义字段 .spec.serviceAccountName 为上面的 ServiceAccount name 即可声明使用。&lt;/p&gt;

&lt;p&gt;如果一个 Pod 没有声明 serviceAccountName，Kubernetes 会自动在它的 Namespace 下创建一个名叫 default 的默认 ServiceAccount，然后分配给这个 Pod。然而这个默认 ServiceAccount 并没有关联任何 Role。也就是说，此时它有访问 APIServer 的绝大多数权限。&lt;/p&gt;

&lt;p&gt;4、cluster&lt;/p&gt;

&lt;p&gt;Role 和 RoleBinding 对象都是 Namespaced 对象，它们只对自己的 Namespace 内的资源有效。&lt;/p&gt;

&lt;p&gt;而某个 Role 需要对于非 Namespaced 对象（比如：Node），或者想要作用于所有的 Namespace 的时候，我们需要使用 ClusterRole 和 ClusterRoleBinding 去做授权。&lt;/p&gt;

&lt;p&gt;这两个 API 对象的用法跟 Role 和 RoleBinding 完全一样。只不过，它们的定义里，没有了 Namespace 字段。&lt;/p&gt;

&lt;p&gt;Kubernetes 已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。一般来说，这些系统级别的 ClusterRole，是绑定给 Kubernetes 系统组件对应的 ServiceAccount 使用的。&lt;/p&gt;

&lt;p&gt;Kubernetes 还提供了四个内置的 ClusterRole 来供用户直接使用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cluster-admin：整个集群的最高权限。如果在 ClusterRoleBinding 中使用，意味着在这个集群中的所有 namespace 中的所有资源都拥有最高权限，为所欲为；如果在 RoleBinding 中使用，即在某个 namespace 中为所欲为。
admin：管理员权限。如果在 RoleBinding 中使用，意味着在某个 namespace 中，对大部分资源拥有读写权限，包括创建 Role 和 RoleBinding 的权限，但没有对资源 quota 和 namespace 本身的写权限。
edit：写权限。在某个 namespace 中，拥有对大部分资源的读写权限，但没有对 Role 和 RoleBinding 的读写权限。
view：读权限。在某个 namespace 中，仅拥有对大部分资源的读权限，没有对 Role 和 RoleBinding 的读权限，也没有对 seccrets 的读权限。
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-abac&#34;&gt;2、ABAC&lt;/h4&gt;

&lt;p&gt;基于属性的权限访问控制。若要开启该模式，需要在 APIServer 启动时，开启 &amp;ndash;authorization-policy-file=&lt;code&gt;&amp;lt;SOME_FILENAME&amp;gt;&lt;/code&gt; 和 &amp;ndash;authorization-mode=ABAC 两个参数。&lt;/p&gt;

&lt;p&gt;json 对象的格式来定义权限&lt;/p&gt;

&lt;p&gt;与 Yaml 文件一致，必须描述的属性有 apiVersion、kind、spec，而 spec 里描述了具体的用户、资源和行为。看个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;apiVersion&amp;quot;: &amp;quot;abac.authorization.kubernetes.io/v1beta1&amp;quot;, &amp;quot;kind&amp;quot;: &amp;quot;Policy&amp;quot;, &amp;quot;spec&amp;quot;: {&amp;quot;user&amp;quot;: &amp;quot;bob&amp;quot;, &amp;quot;namespace&amp;quot;: &amp;quot;projectCaribou&amp;quot;, &amp;quot;resource&amp;quot;: &amp;quot;pods&amp;quot;, &amp;quot;readonly&amp;quot;: true}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就描述了用户 bob 只有在 namespace projectCaribou 下对 pod 的读权限。&lt;/p&gt;

&lt;h4 id=&#34;3-node&#34;&gt;3、Node&lt;/h4&gt;

&lt;p&gt;Node 授权机制是一种特殊的模式，是 kubelet 发起的请求授权。开启该模式，需要开启参数 &amp;ndash;authorization-mode=Node。&lt;/p&gt;

&lt;p&gt;通过启动 &amp;ndash;enable-admission-plugins=&amp;hellip;,NodeRestriction,&amp;hellip;，来限制 kubelet 访问 node，endpoint、pod、service以及secret、configmap、PV 和 PVC 等相关的资源。&lt;/p&gt;

&lt;h4 id=&#34;4-webhook&#34;&gt;4、Webhook&lt;/h4&gt;

&lt;p&gt;Webhook 模式是一种 HTTP 回调模式，是一种通过 HTTP POST 方式实现的简单事件通知。该模式需要 APIServer 配置参数 –authorization-webhook-config-file=&lt;SOME_FILENAME&gt;，HTTP 配置文件的格式跟 kubeconfig 的格式类似。&lt;/p&gt;

&lt;h3 id=&#34;准入控制-admission-controllers&#34;&gt;准入控制（Admission Controllers）&lt;/h3&gt;

&lt;p&gt;在一个请求通过了认证机制和授权认证后，需要经过最后一层筛查，即准入控制。这个准入控制模块的代码通常在 APIServer 中，并被编译到二进制文件中被执行。这一层安全检查的意义在于，检查该请求是否达到系统的门槛，即是否满足系统的默认设置，并添加默认参数。&lt;/p&gt;

&lt;p&gt;准入控制以插件的形式存在
开启的方式为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关闭的方式为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用的准入控制插件有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AlwaysAdmit：允许所有请求通过，被官方反对，因为没有实际意义；
AlwaysPullImages：将每个 pod 的 image pull policy 改为 always，在多租户的集群被使用；
AlwaysDeny：禁止所有请求通过，被官方反对，因为没有实际意义；
DefaultStorageClass：为每个 PersistentVolumeClaim 创建默认的 PV；
DefaultTolerationSeconds：如果 pod 对污点 node.kubernetes.io/not-ready:NoExecute 和 node.alpha.kubernetes.io/unreachable:NoExecute 没有容忍，为其创建默认的 5 分钟容忍 notready:NoExecute 和unreachable:NoExecute；
LimitRanger：确保每个请求都没有超过其 namespace 下的 LimitRange，如果在 Deployment 中使用了 LimitRange 对象，该准入控制插件必须开启；
NamespaceAutoProvision：检查请求中对应的 namespace 是否存在，若不存在自动创建；
NamespaceExists：检查请求中对应的 namespace 是否存在，若不存在拒绝该请求；
NamespaceLifecycle：保证被删除的 namespace 中不会创建新的资源；
NodeRestriction：不允许 kubelet 修改 Node 和 Pod 对象；
PodNodeSelector：通过读取 namespace 的注解和全局配置，来控制某 namespace 下哪些 label 选择器可被使用；
PodPreset：满足预先设置的标准的 pod 不允许被创建；
Priority：通过 priorityClassName 来决定优先级；
ResourceQuota：保证 namespace 下的资源配额；
ServiceAccount：保证 ServiceAccount 的自动创建，如果用到 ServiceAccount，建议开启；
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;使用模式&#34;&gt;使用模式&lt;/h1&gt;

&lt;p&gt;目前最常见的使用姿势&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;某一个服务或业务方要接入进来使用&lt;/li&gt;
&lt;li&gt;给他们创建一个 SA，再在 rbac 中创建关联的 role(分配权限)，把这个 SA 丢给他们&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为我们不太可能去做云服务平台，对外提供给个人用户，内部服务和业务团队，通过 SA 接入已经足够&lt;/p&gt;

&lt;p&gt;这个 SA 创建后，只拥有读取 pod 的权限，也可以限制为只拥有某个 namespace 下的 pod 的权限，你要接入其他用户，再建一个 SA。&lt;/p&gt;

&lt;p&gt;kubectl 可以通过 SA 配置访问到 apiserver，我们可以参考kubectl的方式去调用相关的API，这个其实就是云平台做的东西。&lt;/p&gt;

&lt;h2 id=&#34;内部使用sa&#34;&gt;内部使用sa&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;创建一个 SA，只拥有读取 pod 的权限，然后通过 kubectl config 的配置去使用这个 SA 访问 apiserver&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1、创建sa&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ cat sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: jcy
  name: jcy-sa
MacBook-Pro:exercise chunyinjiang$ kubectl get sa -n jcy
NAME      SECRETS   AGE
default   1         39h
jcy-sa    1         39h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、创建role&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ cat role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    namespace: jcy
    name: jcy-role
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;pods&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;]
MacBook-Pro:exercise chunyinjiang$ kubectl get role -n jcy
NAME       CREATED AT
jcy-role   2020-06-10T11:29:37Z
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、创建rolebinding&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ cat rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: jcy-rolebinding
  namespace: jcy
subjects:
- kind: ServiceAccount
  name: jcy-sa
  apiGroup: &amp;quot;&amp;quot;
roleRef:
  kind: Role
  name: jcy-role
  apiGroup: &amp;quot;&amp;quot;

MacBook-Pro:exercise chunyinjiang$ kubectl get rolebinding -n jcy
NAME              ROLE            AGE
jcy-rolebinding   Role/jcy-role   38h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、kubeconfig配置sa&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TOKEN_NAME=$(kubectl get serviceaccounts jcy-sa -o jsonpath={.secrets[0].name})
MacBook-Pro:exercise chunyinjiang$ echo $TOKEN_NAME
jcy-sa-token-f5lsr

TOKEN=$(kubectl get secret &amp;quot;${TOKEN_NAME}&amp;quot; -n jcy -o jsonpath={.data.token} | base64 -d)
MacBook-Pro:exercise chunyinjiang$ echo $TOKEN
eyJhbGciOiJSUzI1NiIsImtpZCI6Ilgwa2JRbE5kbFU1UXhRSkZBa0lQX1l4V0VrVkZZbTJVX3hFRS1CbnlRejQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJqY3kiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiamN5LXNhLXRva2VuLWY1bHNyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImpjeS1zYSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImFkMjg0NzAxLTFiMWUtNGFkOC1hZWE3LTI5YzgxNDljNDJmOCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpqY3k6amN5LXNhIn0.DeHgbaW5nDDakGxdV448_BOwrSI_Ez2mI8Ii-aztcZMVYJvo2_tWafAWAGm8s3VROcRxgPH8YroCKnllQBL_odJw55ZWhow_KBfOdAh3yKYbF9rNy2BLCV_n_T_qJdSm6M1eXD4_49dhPr3WqEhB5oDqZCXVpa7D0WExtorkiSctYmzfTFcRz2O8fnGExnhtz05p3HnqjyqpbiZQnP1qx6jJd1zlu4T3zgmmx4BLsqSHNVBhsW4Zhz-RmpO91vYzWQDYK6Mp9Yo97rtZofRy2J-6PJ8eI3RslsueWI_JBRPj8MAV-BwolOwXCgoH5ewGyPUveBPVF7Hqok-e_WaNpQ


kubectl config set-credentials system:serviceaccount:jcy:jcy-sa --token=&amp;quot;$TOKEN&amp;quot;
User &amp;quot;system:serviceaccount:jcy:jcy-sa&amp;quot; set.

kubectl config set-context jcy-context --cluster=minikube --namespace=jcy --user=system:serviceaccount:jcy:jcy-sa

kubectl config use-context jcy-context
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、验证是否生效，以及是否只有读 pod 的权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ kubectl get pods
No resources found in jcy namespace.
MacBook-Pro:exercise chunyinjiang$ kubectl get pods -n default
Error from server (Forbidden): pods is forbidden: User &amp;quot;system:serviceaccount:jcy:jcy-sa&amp;quot; cannot list resource &amp;quot;pods&amp;quot; in API group &amp;quot;&amp;quot; in the namespace &amp;quot;default&amp;quot;
MacBook-Pro:exercise chunyinjiang$ kubectl create ns jay
Error from server (Forbidden): namespaces is forbidden: User &amp;quot;system:serviceaccount:jcy:jcy-sa&amp;quot; cannot create resource &amp;quot;namespaces&amp;quot; in API group &amp;quot;&amp;quot; at the cluster scope
MacBook-Pro:exercise chunyinjiang$ kubectl get sa -n jcy
Error from server (Forbidden): serviceaccounts is forbidden: User &amp;quot;system:serviceaccount:jcy:jcy-sa&amp;quot; cannot list resource &amp;quot;serviceaccounts&amp;quot; in API group &amp;quot;&amp;quot; in the namespace &amp;quot;jcy&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;多租户的实现&#34;&gt;多租户的实现&lt;/h2&gt;

&lt;p&gt;多租户也是基于sa+rbac的基础上实现的，首先用户信息需要单独的系统进行存储，这个可以接入外部的已经成熟的系统，也可以自己开发，我曾经就开发过对应的系统。然后每一个真实的用户对应一个sa，这个时候sa就是正在的用户，每个sa在rbac的基础对namespace角色操作进行关联，完成对应用户的权限管理。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s组件系列（八）—- 存储</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-store/</link>
          <pubDate>Sun, 19 Apr 2020 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-store/</guid>
          <description>&lt;p&gt;Kubernetes 和 Docker 类似，也是通过 Volume 的方式提供对存储的支持。但是Kubernetes对容器存储做了一层自己的抽象，相比docker的存储来讲，K8S的存储抽象更全面，更面向应用，体现在如下几个方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;提供卷生命周期管理&lt;/li&gt;
&lt;li&gt;提供“声明”式定义，将使用者和提供者分离&lt;/li&gt;
&lt;li&gt;提供存储类型定义&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;存储系统架构&#34;&gt;存储系统架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;K8S里存储相关的组件，从顶层来讲，主要包含4大组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Volume Plugins — 存储提供的扩展接口, 包含了各类存储提供者的plugin实现&lt;/li&gt;
&lt;li&gt;Volume Manager — 运行在kubelet 里让存储Ready的部件，主要提供“声明”式定义，将使用者和提供者分离，存储类型定义。&lt;/li&gt;
&lt;li&gt;PV/PVC Controller — 运行在Master上的部件，主要提供卷生命周期管理&lt;/li&gt;
&lt;li&gt;Attach/Detach — 运行在Master上，顾名思义，主要提供“声明”式定义，将使用者和提供者分离&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中Volume Plugins是一个基础部件，后三个是逻辑部件，依赖于Volume Plugins。&lt;/p&gt;

&lt;p&gt;上诉其实就是K8S内部的基本逻辑架构，扩展出去再加上外部与这些部件有交互关系的部件(调用者和实现者)和内部可靠性保证的部件，就可以得出K8S的存储的架构全景。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对于调用者，在master上主要是通过监听API Server来获取资源变化，从而触发卷的增删改查，在minion上，因为只有pod调度到这个node上才会有卷的相应操作，所以它的触发端是kubelet（严格讲是kubelet里的pod manager），根据Pod Manager里pod spec里申明的存储来触发卷的挂载操作。&lt;/p&gt;

&lt;h1 id=&#34;核心流程&#34;&gt;核心流程&lt;/h1&gt;

&lt;p&gt;无论是docker存储也好，K8S存储也好，亦或者是其它类的容器相关的存储抽象，其本质都是一样的——让存储在容器里ready，核心做三件工作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;provision/delete&lt;/li&gt;
&lt;li&gt;attach/detach(可选).&lt;/li&gt;
&lt;li&gt;mount/unmount.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;管理&#34;&gt;管理&lt;/h2&gt;

&lt;p&gt;PV Controller和K8S其它组件一样监听API Server中的资源更新，对于卷管理主要是监听PV，PVC， SC三类资源，当监听到这些资源的创建、删除、修改时，PV Controller经过判断是需要做创建、删除、绑定、回收等动作（后续会展开介绍内部逻辑），然后根据需要调用Volume Plugins进行业务处理，大致调用逻辑如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;挂载&#34;&gt;挂载&lt;/h2&gt;

&lt;p&gt;卷的挂载，主要分两个阶段，attach/detach卷和mount/umount 卷，其中卷的attach/detach，有两个组件做这个工作，分别是Master上的AttachDetach Controller 和Minion上的VolumeManager。&lt;/p&gt;

&lt;p&gt;先来看看AttachDetach Controller（后简称ADController），ADController的处理流程和上面介绍的PV Controller基本类似，首先监听API Server的资源变化，主要监听的是node和pod资源，通过node和pod变更，触发ADController是否attach/detach操作，然后调用plugin做相应的业务处理，大致流程如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;VolumeManager相比ADController最大的区别是事件触发的来源，VolumeManager不会监听API Server，在Minion端所有的资源监听都是Kubelet完成的，Kubelet会监听到调度到该节点上的pod声明，会把pod缓存到Pod Manager中，VolumeManager通过Pod Manager获取PV/PVC的状态，并进行分析出具体的attach/detach, 操作然后调用plugin进行相应的业务处理，流程如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;volume&#34;&gt;Volume&lt;/h1&gt;

&lt;p&gt;先看一下Kubernetes 中 Volume 的 概念与Docker 中的 Volume 类似，但不完全相同。具体区别如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 中的 Volume 与 Pod 的生命周期相同，但与容器的生命周期不相关。当容器终止或重启时，Volume 中的数据也不会丢失。&lt;/li&gt;
&lt;li&gt;当 Pod 被删除时，Volume 才会被清理。并且数据是否丢失取决于 Volume 的具体类型，比如：emptyDir 类型的 Volume 数据会丢失，而 PV 类型的数据则不会丢失。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;持久化卷Volume，支持两种类型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mounted File Volume，Node 将会把 Volume 以指定的文件格式 Mount 到 Container 上，从 Container 的角度看到的是一个目录；&lt;/li&gt;
&lt;li&gt;Raw Block Volume, 直接将 Volume 以 Block Device（磁盘）的形态暴露给 Container，对于某些可以直接操作磁盘的服务，这个形态可以省去文件系统的开销获得更好的性能。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;生命周期&#34;&gt;生命周期&lt;/h2&gt;

&lt;p&gt;一个典型的 CSI Volume 生命周期如下图（来自 CSI SPEC）所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store20.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Volume 被创建后进入 CREATED 状态，此时 Volume 仅在存储系统中存在，对于所有的 Node 或者 Container 都是不可感知的；&lt;/li&gt;
&lt;li&gt;对 CREATED 状态的 Volume 进行 Controlller Publish 动作后在指定的 Node 上进入 NODE_READY 的状态，此时 Node 可以感知到 Volume，但是 Container 依然不可见；&lt;/li&gt;
&lt;li&gt;在 Node 对 Volume 进行 Stage 操作，进入 VOL_READY 的状态，此时 Node 已经与 Volume 建立连接。Volume 在 Node 上以某种形式存在；&lt;/li&gt;
&lt;li&gt;在 Node 上对 Volume 进行 Publish 操作，此时 Volume 将转化为 Node 上 Container 可见的形态被 Container 利用，进入正式服务的状态；&lt;/li&gt;
&lt;li&gt;当 Container 的生命周期结束或其他不再需要 Volume 情况发生时，Node 执行 Unpublish Volume 动作，将 Volume 与 Container 之间的连接关系解除，回退至 VOL_READY 状态；&lt;/li&gt;
&lt;li&gt;Node Unstage 操作将会把 Volume 与 Node 的连接断开，回退至 NODE_READY 状态；&lt;/li&gt;
&lt;li&gt;Controller Unpublish 操作则会取消 Node 对 Volume 的访问权限；&lt;/li&gt;
&lt;li&gt;Delete 则从存储系统中销毁 Volume。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从这个图我们可以看出一个存储卷的供应分别调用了Controller Plugin的CreateVolume、ControllerPublishVolume及Node Plugin的NodeStageVolume、NodePublishVolume这4个gRPC接口，存储卷的销毁分别调用了Node Plugin的NodeUnpublishVolume、NodeUnstageVolume及Controller的ControllerUnpublishVolume、DeleteVolume这4个gRPC接口。&lt;/p&gt;

&lt;p&gt;提到存储，当然就要提到volume存储卷，volume存储卷是Pod中能够被多个容器访问的共享目录，用于存储数据。我们可以大体根据存储类型分为持久化和非持久化。&lt;/p&gt;

&lt;h2 id=&#34;非持久化存储方式&#34;&gt;非持久化存储方式&lt;/h2&gt;

&lt;h3 id=&#34;emptydir&#34;&gt;emptyDir&lt;/h3&gt;

&lt;p&gt;emptryDir，顾名思义是一个空目录，一个emptyDir 第一次创建是在一个pod被指定到具体node的时候，并且会一直存在在pod的生命周期当中，正如它的名字一样，它初始化是一个空的目录，pod中的容器都可以读写这个目录，这个目录可以被挂在到各个容器相同或者不相同的的路径下。当一个pod因为任何原因被移除的时候，这些数据会被永久删除。注意：一个容器崩溃了不会导致数据的丢失，因为容器的崩溃并不移除pod.&lt;/p&gt;

&lt;p&gt;主要用途：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、临时空间，例如用于某些应用程序运行时所需的临时目录，且无需永久保留&lt;/li&gt;
&lt;li&gt;2、长时间任务的中间过程checkpoint的临时保存目录&lt;/li&gt;
&lt;li&gt;3、一个容器需要从另一个容器中获取数据库的目录（多容器共享目录）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：容器的crashing事件并不会导致emptyDir中的数据被删除。&lt;/p&gt;

&lt;p&gt;结构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/volume&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# cat test.yaml
apiVersion: v1
kind: Service
metadata:
  name: serivce-mynginx
  namespace: default
spec:
  type: NodePort
  selector:
    app: mynginx
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 30080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mynginx
  template:
    metadata:
      labels:
        app: mynginx
    spec:
      containers:
      - name: mynginx
        image: lizhaoqwe/nginx:v1
        volumeMounts:
        - mountPath: /usr/share/nginx/html/
          name: share
        ports:
        - name: nginx
          containerPort: 80
      - name: busybox
        image: busybox
        command:
        - &amp;quot;/bin/sh&amp;quot;
        - &amp;quot;-c&amp;quot;
        - &amp;quot;sleep 4444&amp;quot;
        volumeMounts:
        - mountPath: /data/
          name: share
      volumes:
      - name: share
        emptyDir: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建Pod&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f test.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看Pod&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
deploy-5cd657dd46-sx287   2/2     Running   0          2m1s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get svc
NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes        ClusterIP   10.96.0.1      &amp;lt;none&amp;gt;        443/TCP        6d10h
serivce-mynginx   NodePort    10.99.110.43   &amp;lt;none&amp;gt;        80:30080/TCP   2m27s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们进入到busybox容器当中创建一个index.html&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl exec -it deploy-5cd657dd46-sx287 -c busybox -- /bin/sh

容器内部：
/data # cd /data
/data # echo &amp;quot;fengzi&amp;quot; &amp;gt; index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开浏览器验证一下。&lt;/p&gt;

&lt;h3 id=&#34;hostpath&#34;&gt;hostPath&lt;/h3&gt;

&lt;p&gt;hostPath为在Pod上挂载宿主机上的文件或目录，它通常可以用于以下几方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的告诉文件系统进行存储&lt;/li&gt;
&lt;li&gt;2、需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部应用可以直接访问Docker的文件系统&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在使用这种类型的volume时，需要注意以下几点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、在不同的node上具有相同配置的Pod时，可能会因为宿主机上的目录和文件不同而导致对volume上的目录和文件访问结果不一致&lt;/li&gt;
&lt;li&gt;2、如果使用了资源配置，则kubernetes无法将hostPath在宿主机上使用的资源纳入管理&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/volume1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# cat test.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-deploy
  namespace: default
spec:
  selector:
    app: mynginx
  type: NodePort
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 31111

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynginx
  template:
    metadata:
      name: web
      labels:
        app: mynginx
    spec:
      containers:
      - name: mycontainer
        image: lizhaoqwe/nginx:v1
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: persistent-storage
        ports:
        - containerPort: 80
      volumes:
      - name: persistent-storage
        hostPath:
          type: DirectoryOrCreate
          path: /mydata
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DirectoryOrCreate
Directory
FileOrCreate
File
Socket
CharDevice
BloakDevice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f test.yaml
service/nginx-deploy created
deployment.apps/mydeploy created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在目录里操作，都是能够看到的&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;emptyDir和hostPath在功能上的异同分析&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、二者都是node节点的本地存储卷方式；&lt;/li&gt;
&lt;li&gt;2、emptyDir可以选择把数据存到tmpfs类型的本地文件系统中去，hostPath并不支持这一点；&lt;/li&gt;
&lt;li&gt;3、hostPath除了支持挂载目录外，还支持File、Socket、CharDevice和BlockDevice，既支持把已有的文件和目录挂载到容器中，也提供了“如果文件或目录不存在，就创建一个”的功能；&lt;/li&gt;
&lt;li&gt;4、emptyDir是临时存储空间，完全不提供持久化支持；&lt;/li&gt;
&lt;li&gt;5、hostPath的卷数据是持久化在node节点的文件系统中的，即便pod已经被删除了，volume卷中的数据还会留存在node节点上；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;持久化存储方式&#34;&gt;持久化存储方式&lt;/h2&gt;

&lt;p&gt;Kubernetes 目前可以使用 PersistentVolume、PersistentVolumeClaim、StorageClass 三种 API 资源来进行持久化存储。我们来看一下这几个之间的关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/pv&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实PersistentVolume、StorageClass都是pv，基于vloume基础之上的，只不过是静态和动态的区别，PersistentVolumeClaim是连接前面两个和k8s的桥梁，使得存储和k8s分离，三个联合在一起使用和非持久化vloume是一样的道理，那么为什么还需要pv呢？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、pod 重建销毁，如用 Deployment 管理的 pod，在做镜像升级的过程中，会产生新的 pod 并且删除旧的 pod ，那新旧 pod 之间如何复用数据？&lt;/li&gt;
&lt;li&gt;2、宿主机宕机的时候，要把上面的 pod 迁移，这个时候 StatefulSet 管理的 pod，其实已经实现了带卷迁移的语义。这时通过 Pod Volumes 显然是做不到的；&lt;/li&gt;
&lt;li&gt;3、多个 pod 之间，如果想要共享数据，应该如何去声明呢？我们知道，同一个 pod 中多个容器想共享数据，可以借助 Pod Volumes 来解决；当多个 pod 想共享数据时，Pod Volumes 就很难去表达这种语义；&lt;/li&gt;
&lt;li&gt;4、如果要想对数据卷做一些功能扩展性，如：snapshot、resize 这些功能，又应该如何去做呢？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上场景中，通过 Pod Volumes 很难准确地表达它的复用 / 共享语义，对它的扩展也比较困难。因此 K8s 中又引入了 Persistent Volumes 概念，它可以将存储和计算分离，通过不同的组件来管理存储资源和计算资源，然后解耦 pod 和 Volume 之间生命周期的关联。这样，当把 pod 删除之后，它使用的 PV 仍然存在，还可以被新建的 pod 复用。其实就是一句话，实现隔离，完成持久化的存储。&lt;/p&gt;

&lt;h3 id=&#34;pv&#34;&gt;pv&lt;/h3&gt;

&lt;p&gt;PersistentVolume（持久化卷）可以理解成为kubernetes集群中的某个网络存储对应的一块存储，它与Volume类似，定义出来给pvc进行挂载的，只不过非持久化存储的都是不需要资源声明的，pv需要声明来才能被pvc挂载。&lt;/p&gt;

&lt;p&gt;pv和正常的挂载但有以下区别：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pv只能是网络存储，不属于任何Node，但可以在每个Node上访问。&lt;/li&gt;
&lt;li&gt;pv并不是被定义在Pod上的，而是独立于Pod之外定义的，也是集群资源的一种。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;在nfs server服务器上创建nfs卷的映射并重启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# cat /etc/exports
/share_v1  192.168.254.0/24(insecure,rw,no_root_squash)
/share_v2  192.168.254.0/24(insecure,rw,no_root_squash)
/share_v3  192.168.254.0/24(insecure,rw,no_root_squash)
/share_v4  192.168.254.0/24(insecure,rw,no_root_squash)
/share_v5  192.168.254.0/24(insecure,rw,no_root_squash)

[root@localhost ~]# service nfs restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在nfs server服务器上创建响应目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost /]# mkdir /share_v{1,2,3,4,5}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在kubernetes集群中的master节点上创建pv，我这里创建了5个pv对应nfs server当中映射出来的5个目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# cat createpv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv01
spec:
  nfs:　　　　　　　　　　　　　　 #存储类型
    path: /share_v1　　　　　　 #要挂在的nfs服务器的目录位置
    server: 192.168.254.11　　 #nfs server地址，也可以是域名，前提是能被解析
  accessModes: 　　　　　　　　　#访问模式：
  - ReadWriteMany　　　　　　　　　　ReadWriteMany：读写权限，允许多个Node挂载 | ReadWriteOnce：读写权限，只能被单个Node挂在 | ReadOnlyMany：只读权限，允许被多个Node挂载
  - ReadWriteOnce　　　　　　　　　
  capacity:　　　　　　　　　　　　#存储容量　　　　　　　　　　　　
    storage: 10Gi　　　　　　　　 #pv存储卷为10G
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv02
spec:
  nfs:
    path: /share_v2
    server: 192.168.254.11
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 20Gi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv03
spec:
  nfs:
    path: /share_v3
    server: 192.168.254.11
  accessModes:
  - ReadWriteMany
  - ReadWriteOnce
  capacity:
    storage: 30Gi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv04
spec:
  nfs:
    path: /share_v4
    server: 192.168.254.11
  accessModes:
  - ReadWriteMany
  - ReadWriteOnce
  capacity:
    storage: 40Gi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv05
spec:
  nfs:
    path: /share_v5
    server: 192.168.254.11
  accessModes:
  - ReadWriteMany
  - ReadWriteOnce
  capacity:
    storage: 50Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f createpv.yaml
persistentvolume/pv01 created
persistentvolume/pv02 created
persistentvolume/pv03 created
persistentvolume/pv04 created
persistentvolume/pv05 created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看pv&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv01   10Gi       RWO,RWX        Retain           Available                                   5m10s
pv02   20Gi       RWX            Retain           Available                                   5m10s
pv03   30Gi       RWO,RWX        Retain           Available                                   5m9s
pv04   40Gi       RWO,RWX        Retain           Available                                   5m9s
pv05   50Gi       RWO,RWX        Retain           Available                                   5m9s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解析&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ACCESS MODES（pv访问模式，其实就是读写权限）:
　　RWO:ReadWriteOnly：是最基本的方式，可读可写，但只支持被单个Pod挂载。
　　RWX:ReadWriteMany：这种存储可以以读写的方式被多个Pod共享。
　　ROX:ReadOnlyMany：可以以只读的方式被多个Pod挂载。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不是每一种存储都支持这三种方式&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/store1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RECLAIM POLICY（重声明策略，其实就是pv使用的策略）:
　　Retain：保护pvc释放的pv及其上的数据，将不会被其他pvc绑定
　　recycle：保留pv但清空数据
　　delete：删除pvc释放的pv及后端存储volume
STATUS（阶段状态）:
　　Available:空闲状态
　　Bound：已经绑定到某个pvc上
　　Released：对应的pvc已经被删除，但是资源没有被集群回收
　　Failed：pv自动回收失败
CLAIM:
　　被绑定到了那个pvc上面格式为：NAMESPACE/PVC_NAME
　　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;四个PV选择器&lt;/p&gt;

&lt;p&gt;在PVC中绑定一个PV，可以根据下面几种条件组合选择&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Access Modes， 按照访问模式选择pv
Resources， 按照资源属性选择， 比如说请求存储大小为8个G的pv
Selector， 按照pv的label选择
Class， 根据StorageClass的class名称选择, 通过annotation指定了Storage Class的名字, 来绑定特定类型的后端存储
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前pv支持的插件类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GCEPersistentDisk
AWSElasticBlockStore
AzureFile
AzureDisk
FC (Fibre Channel)
FlexVolume
Flocker
NFS
iSCSI
RBD (Ceph Block Device)
CephFS
Cinder (OpenStack block storage)
Glusterfs
VsphereVolume
Quobyte Volumes
HostPath
VMware Photon
Portworx Volumes
ScaleIO Volumes
StorageOS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中最常用的有NFS，ceph rbd，cephFS，HostPath等。&lt;/p&gt;

&lt;h4 id=&#34;nfs&#34;&gt;NFS&lt;/h4&gt;

&lt;p&gt;NFS是Network File System的缩写，就是网络文件系统，这里不详细解说了，可以重&lt;a href=&#34;https://kingjcy.github.io/post/distributed/fs/nfs/&#34;&gt;这里&lt;/a&gt;了解。&lt;/p&gt;

&lt;p&gt;直接编辑/etc/exports文件添加以下内容来共享/share&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost share]# vim /etc/exports
    /share  192.168.254.0/24(insecure,rw,no_root_squash)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在kubernetes集群的master节点中创建yaml文件并写入&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# cat test.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-deploy
  namespace: default
spec:
  selector:
    app: mynginx
  type: NodePort
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 31111

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynginx
  template:
    metadata:
      name: web
      labels:
        app: mynginx
    spec:
      containers:
      - name: mycontainer
        image: lizhaoqwe/nginx:v1
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: nfs
        ports:
        - containerPort: 80
      volumes:
      - name: nfs
        nfs:
          server: 192.168.254.11       #nfs服务器地址
          path: /share　　　　　　　　　　#nfs服务器共享目录
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f test.yaml
service/nginx-deploy created
deployment.apps/mydeploy created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以nfs创建静态pv没有问题！！！&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/volume2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;glusterfs&#34;&gt;glusterfs&lt;/h4&gt;

&lt;p&gt;GlusterFS (Gluster File System) 是一个开源的分布式文件系统，是 Scale-Out 存储解决方案 Gluster 的核心，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。详细了解可以到&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/glusterfs&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;GlusterFS中的volume的模式有很多中，包括以下几种：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、分布卷（默认模式）：即DHT, 也叫 分布卷: 将文件以hash算法随机分布到 一台服务器节点中存储。
2、复制模式：即AFR, 创建volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。
3、条带模式：即Striped, 创建volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。
4、分布式条带模式：最少需要4台服务器才能创建。 创建volume 时 stripe 2 server = 4 个节点： 是DHT 与 Striped 的组合型。
5、分布式复制模式：最少需要4台服务器才能创建。 创建volume 时 replica 2 server = 4 个节点：是DHT 与 AFR 的组合型。
6、条带复制卷模式：最少需要4台服务器才能创建。 创建volume 时 stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。
7、三种模式混合： 至少需要8台 服务器才能创建。 stripe 2 replica 2 , 每4个节点 组成一个 组。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每一种卷都有不同的使用方式，最后都是以pv和pvc的使用方式。&lt;/p&gt;

&lt;p&gt;创建资源pv&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat glusterfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: gluster-dev-volume
spec:
  capacity:
    storage: 8Gi
  accessModes:
    - ReadWriteMany
  glusterfs:
    endpoints: &amp;quot;glusterfs-cluster&amp;quot;---需要创建endpoints和service来暴露glusterfs
    path: &amp;quot;k8s-volume&amp;quot;
    readOnly: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建资源pvc&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat glusterfs-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: glusterfs-nginx
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 8Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例挂载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vi nginx-deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-dm
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
          volumeMounts:
            - name: gluster-dev-volume
              mountPath: &amp;quot;/usr/share/nginx/html&amp;quot;
      volumes:
      - name: gluster-dev-volume
        persistentVolumeClaim:
          claimName: glusterfs-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以共享glusterfs的存储了。&lt;/p&gt;

&lt;h3 id=&#34;storageclass&#34;&gt;StorageClass&lt;/h3&gt;

&lt;p&gt;由于不同的应用程序对于存储性能的要求也不尽相同，比如：读写速度、并发性能、存储大小等。如果只能通过 PVC 对 PV 进行静态申请，显然这并不能满足任何应用对于存储的各种需求。为了解决这一问题，Kubernetes 引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，集群管理员可以先将存储资源定义为不同类型的资源，比如快速存储、慢速存储等。&lt;/p&gt;

&lt;p&gt;当用户通过 PVC 对存储资源进行申请时，StorageClass 会使用 Provisioner（不同 Volume 对应不同的 Provisioner）来自动创建用户所需 PV。这样应用就可以随时申请到合适的存储资源，而不用担心集群管理员没有事先分配好需要的 PV。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;自动创建的 PV 以 ${namespace}-${pvcName}-${pvName} 这样的命名格式创建在后端存储服务器上的共享数据目录中。
自动创建的 PV 被回收后会以 archieved-${namespace}-${pvcName}-${pvName} 这样的命名格式存在后端存储服务器上。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实StorageClass就是动态创建pv，声明一个StorageClass，只要在pvc中annotations中声明对应的标签就可以自动创建pvc中需要的pv&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;1、创建 StorageClass 对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim nfs-client-class.yaml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: course-nfs-storage
provisioner: fuseim.pri/ifs # or choose another name, must match deployment&#39;s env PROVISIONER_NAME&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、使用 Kubectl 命令建立这个 StorageClass。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f nfs-client-class.yaml
storageclass.storage.k8s.io &amp;quot;course-nfs-storage&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上都创建完成后查看下相关资源的状态。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods|grep nfs-client
NAME                                            READY     STATUS      RESTARTS   AGE
nfs-client-provisioner-9d94b899c-nn4c7          1/1       Running     0          1m

$ kubectl get storageclass
NAME                 PROVISIONER      AGE
course-nfs-storage   fuseim.pri/ifs   1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、手动创建的一个 PVC 对象&lt;/p&gt;

&lt;p&gt;新建一个 PVC 对象&lt;/p&gt;

&lt;p&gt;我们这里就来建立一个能使用 StorageClass 资源对象来动态建立 PV 的 PVC，要创建使用 StorageClass 资源对象的 PVC 有以下两种方法。&lt;/p&gt;

&lt;p&gt;方法一：在这个 PVC 对象中添加一个 Annotations 属性来声明 StorageClass 对象的标识。这里我们声明了一个 PVC 对象，采用 ReadWriteMany 的访问模式并向 PV 请求 100Mi 的空间。
    $ vim test-pvc.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-pvc
  annotations:
    volume.beta.kubernetes.io/storage-class: &amp;quot;course-nfs-storage&amp;quot;
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 100Mi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;方法二：把名为 course-nfs-storage 的 StorageClass 设置为 Kubernetes 的默认后端存储。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl patch storageclass course-nfs-storage -p &#39;{&amp;quot;metadata&amp;quot;: {&amp;quot;annotations&amp;quot;:{&amp;quot;storageclass.kubernetes.io/is-default-class&amp;quot;:&amp;quot;true&amp;quot;}}}&#39;
storageclass.storage.k8s.io &amp;quot;course-nfs-storage&amp;quot; patched
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这两种方法都是可以的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f test-pvc.yaml
persistentvolumeclaim &amp;quot;test-pvc&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建完成后，我们来看看对应的资源是否创建成功。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pvc
NAME       STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE
pvc1-nfs   Bound     pv1-nfs                                    1Gi        RWO                                 4h
test-pvc   Bound     pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79   100Mi      RWX            course-nfs-storage   41s

$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM              STORAGECLASS         REASON    AGE
pv1-nfs                                    1Gi        RWO            Recycle          Bound       default/pvc1-nfs                                  5h
pv2-nfs                                    2Gi        RWO            Recycle          Available                                                     1h
pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79   100Mi      RWX            Delete           Bound       default/test-pvc   course-nfs-storage             2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的结果我们可以看到一个名为 test-pvc 的 PVC 对象创建成功并且状态已经是 Bound 了。对应也自动创建了一个名为 pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79 的 PV 对象，其访问模式是 RWX，回收策略是 Delete。STORAGECLASS 栏中的值也正是我们创建的 StorageClass 对象 course-nfs-storage。&lt;/p&gt;

&lt;p&gt;ceph一把也是使用StorageClass来做pv的，我们来看一下：&lt;/p&gt;

&lt;h4 id=&#34;ceph&#34;&gt;ceph&lt;/h4&gt;

&lt;p&gt;ceph是一个linux PB级分布式文件存储系统，它是一个大容量并且简单扩容，高性能，高可靠等特性，详细了解可以到&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/ceph&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;直接看使用配置&lt;/p&gt;

&lt;p&gt;rbd&lt;/p&gt;

&lt;p&gt;1、配置 StorageClass&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 如果使用kubeadm创建的集群 provisioner 使用如下方式
# provisioner: ceph.com/rbd
cat &amp;gt;storageclass-ceph-rdb.yaml&amp;lt;&amp;lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: dynamic-ceph-rdb
provisioner: ceph.com/rbd
# provisioner: kubernetes.io/rbd
parameters:
  monitors: 11.11.11.111:6789,11.11.11.112:6789,11.11.11.113:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: kube-system
  pool: kube
  userId: kube
  userSecretName: ceph-user-secret
  fsType: ext4
  imageFormat: &amp;quot;2&amp;quot;
  imageFeatures: &amp;quot;layering&amp;quot;
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、创建pvc&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt;ceph-rdb-pvc-test.yaml&amp;lt;&amp;lt;EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-rdb-claim
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: dynamic-ceph-rdb
  resources:
    requests:
      storage: 2Gi
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、创建 nginx pod 挂载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt;nginx-pod.yaml&amp;lt;&amp;lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod1
  labels:
    name: nginx-pod1
spec:
  containers:
  - name: nginx-pod1
    image: nginx:alpine
    ports:
    - name: web
      containerPort: 80
    volumeMounts:
    - name: ceph-rdb
      mountPath: /usr/share/nginx/html
  volumes:
  - name: ceph-rdb
    persistentVolumeClaim:
      claimName: ceph-rdb-claim
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;fs&lt;/p&gt;

&lt;p&gt;1、配置 StorageClass&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt;storageclass-cephfs.yaml&amp;lt;&amp;lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: dynamic-cephfs
provisioner: ceph.com/cephfs
parameters:
    monitors: 11.11.11.111:6789,11.11.11.112:6789,11.11.11.113:6789
    adminId: admin
    adminSecretName: ceph-secret
    adminSecretNamespace: &amp;quot;kube-system&amp;quot;
    claimRoot: /volumes/kubernetes
EOF

2、创建pvc
cat &amp;gt;cephfs-pvc-test.yaml&amp;lt;&amp;lt;EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephfs-claim
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: dynamic-cephfs
  resources:
    requests:
      storage: 2Gi
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、创建 nginx pod 挂载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt;nginx-pod.yaml&amp;lt;&amp;lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod1
  labels:
    name: nginx-pod1
spec:
  containers:
  - name: nginx-pod1
    image: nginx:alpine
    ports:
    - name: web
      containerPort: 80
    volumeMounts:
    - name: cephfs
      mountPath: /usr/share/nginx/html
  volumes:
  - name: cephfs
    persistentVolumeClaim:
      claimName: cephfs-claim
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pvc&#34;&gt;pvc&lt;/h3&gt;

&lt;p&gt;有了pv（包括StorageClass）之后我们就可以创建pvc了，pvc也是一种集群资源。&lt;/p&gt;

&lt;p&gt;PersistentVolumeClaim（持久化卷声明），PVC 是用户对存储资源的一种请求。可以说是连接k8s和pv的桥梁：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、职责分离，pvc中只要声明自己需要的大小，访问方式等业务真心关心的存储需求
2、pvc简化来user对存储的要求，pv才是存储的实际信息的承载体，，通过kube-controller-manager中的pv controller将pvc和合适的pv绑定在一起，完成存储。
3、pvc是抽象的接口，pv是接口的实现
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们接着pv来继续创建pvc。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# cat test.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-deploy
  namespace: default
spec:
  selector:
    app: mynginx
  type: NodePort
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 31111

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynginx
  template:
    metadata:
      name: web
      labels:
        app: mynginx
    spec:
      containers:
      - name: mycontainer
        image: nginx
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: html
        ports:
        - containerPort: 80
      volumes:
      - name: html
        persistentVolumeClaim:
          claimName: mypvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
  namespace: default
spec:
  accessMode:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f test.yaml
service/nginx-deploy created
deployment.apps/mydeploy created
persistentvolumeclaim/mypvc created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次查看pv，已经显示pvc被绑定到了pv02上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM           STORAGECLASS   REASON   AGE
pv01   10Gi       RWO,RWX        Retain           Available                                           22m
pv02   20Gi       RWX            Retain           Bound       default/mypvc                           22m
pv03   30Gi       RWO,RWX        Retain           Available                                           22m
pv04   40Gi       RWO,RWX        Retain           Available                                           22m
pv05   50Gi       RWO,RWX        Retain           Available                                           22m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看pvc&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get pvc
NAME    STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mypvc   Bound    pv02     20Gi       RWX                           113s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证&lt;/p&gt;

&lt;p&gt;在nfs server服务器上找到相应的目录执行以下命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost share_v1]# echo &#39;test pvc&#39; &amp;gt; index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后打开浏览器，OK,没问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/volume3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;发展&#34;&gt;发展&lt;/h1&gt;

&lt;p&gt;在CSI之前，K8S里提供存储服务都是原生集成在组件中的，提供了内嵌原生 Driver 的方式连接外部的常见存储系统例如 NFS、iSCSI、CephFS、RBD 等来满足不同业务的需求。这种方式需要将存储提供者的代码逻辑放到K8S的代码库中运行（如果要求不高，其实也能运行够用），调用引擎与插件间属于强耦合，这种方式会带来一些问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;存储插件需要一同随K8S发布。&lt;/li&gt;
&lt;li&gt;K8S社区需要对存储插件的测试、维护负责。&lt;/li&gt;
&lt;li&gt;存储插件的问题有可能会影响K8S部件正常运行。&lt;/li&gt;
&lt;li&gt;存储插件享有K8S部件同等的特权存在安全隐患。&lt;/li&gt;
&lt;li&gt;存储插件开发者必须遵循K8S社区的规则开发代码。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以和其他服务管理系统一样，K8s 逐渐的将存储系统的具体实现从主项目中分离出来，通过定义接口的方式允许第三方厂商自行接入存储服务。在这个道路上也经历了两个阶段：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Flex Volume, 自 1.2 版本引入。第三方存储服务提供商直接在 K8s Server 上部署符合 Flex Volume 规范的 Driver，利用 K8s 暴露出的 mount/unmount/attach/detach 等关键 API 实现存储系统的接入。

&lt;ul&gt;
&lt;li&gt;这个模式主要的问题是，在这个形态下第三方厂商的 Driver 有权限接入物理节点的根文件系统，这会带来安全上的隐患&lt;/li&gt;
&lt;li&gt;存储插件在执行mount、attach这类操作时，往往需要到host去安装第三方工具或者加载一些依赖库，这样host的OS版本往往需要定制，不再是一个简单的linux发型版本，这样的情况太多，会使部署变得复杂。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-store-csi/&#34;&gt;Container Storage Interface (CSI)&lt;/a&gt;, 自 1.9 版本引入，目前已经进入 GA 阶段（1.13）。CSI 定义了容器场景所需要的存储控制原语和完整的控制流程，并且在 K8s 的 CSI 实现中，所有的第三方 Driver 和 K8s 的其他服务扩展一样，以服务容器的形态的运行，不会直接影响到 K8s 的核心系统稳定性，是目前主要使用的模式。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算容器系列---- Docker Principle</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/docker/docker-principle/</link>
          <pubDate>Tue, 14 Apr 2020 16:29:55 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/docker/docker-principle/</guid>
          <description>&lt;p&gt;Docker实质上是汇集了linux容器（各种namespaces）、cgroups以及“叠加”类文件系统（changeroot）等多种核心技术的一种复合技术。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/docker4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，模块各司其职。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求；而后Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。&lt;/li&gt;
&lt;li&gt;Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储；当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境；当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。&lt;/li&gt;
&lt;li&gt;而libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。当执行完运行容器的命令后，一个实际的Docker容器就处于运行状态，该容器拥有独立的文件系统，独立并且安全的运行环境等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;再来看看另外一个架构，这个个架构就简单清晰指明了server/client交互，容器和镜像、数据之间的一些联系&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/docker5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;docker-client&#34;&gt;docker client&lt;/h2&gt;

&lt;p&gt;docker client 是docker架构中用户用来和docker daemon建立通信的客户端，用户使用的可执行文件为docker，通过docker命令行工具可以发起众多管理container的请求&lt;/p&gt;

&lt;p&gt;docker client可以通过以下方式和docker daemon建立通信：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;tcp://host:port&lt;/li&gt;
&lt;li&gt;unix:path_to_socket&lt;/li&gt;
&lt;li&gt;fd://socketfd&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;docker client可以通过设置命令行flag参数的形式设置安全传输层协议(TLS)的有关参数，保证传输的安全性&lt;/p&gt;

&lt;p&gt;docker client发送容器管理请求后，由docker daemon接受并处理请求，当docker client 接收到返回的请求相应并简单处理后，docker client 一次完整的生命周期就结束了，当需要继续发送容器管理请求时，用户必须再次通过docker可以执行文件创建docker client。&lt;/p&gt;

&lt;h2 id=&#34;docker-daemon&#34;&gt;docker daemon&lt;/h2&gt;

&lt;p&gt;docker daemon 是docker架构中一个常驻在后台的系统进程，功能是：接收处理docker client发送的请求。该守护进程在后台启动一个server，server负载接受docker client发送的请求；接受请求后，server通过路由与分发调度，找到相应的handler来执行请求。&lt;/p&gt;

&lt;p&gt;docker daemon启动所使用的可执行文件也为docker，与docker client启动所使用的可执行文件docker相同，在docker命令执行时，通过传入的参数来判别docker daemon与docker client。&lt;/p&gt;

&lt;p&gt;docker daemon的架构可以分为：docker server、engine、job。daemon&lt;/p&gt;

&lt;h2 id=&#34;docker-server&#34;&gt;docker server&lt;/h2&gt;

&lt;p&gt;docker server在docker架构中时专门服务于docker client的server，该server的功能时：接受并调度分发docker client发送的请求。&lt;/p&gt;

&lt;p&gt;在Docker的启动过程中，通过包gorilla/mux（golang的类库解析），创建了一个mux.Router，提供请求的路由功能。在Golang中，gorilla/mux是一个强大的URL路由器以及调度分发器。该mux.Router中添加了众多的路由项，每一个路由项由HTTP请求方法（PUT、POST、GET或DELETE）、URL、Handler三部分组成。&lt;/p&gt;

&lt;p&gt;若Docker Client通过HTTP的形式访问Docker Daemon，创建完mux.Router之后，Docker将Server的监听地址以及mux.Router作为参数，创建一个httpSrv=http.Server{}，最终执行httpSrv.Serve()为请求服务。&lt;/p&gt;

&lt;p&gt;在Server的服务过程中，Server在listener上接受Docker Client的访问请求，并创建一个全新的goroutine来服务该请求。在goroutine中，首先读取请求内容，然后做解析工作，接着找到相应的路由项，随后调用相应的Handler来处理该请求，最后Handler处理完请求之后回复该请求。&lt;/p&gt;

&lt;p&gt;需要注意的是：Docker Server的运行在Docker的启动过程中，是靠一个名为”serveapi”的job的运行来完成的。原则上，Docker Server的运行是众多job中的一个，但是为了强调Docker Server的重要性以及为后续job服务的重要特性，将该”serveapi”的job单独抽离出来分析，理解为Docker Server。&lt;/p&gt;

&lt;h2 id=&#34;engine&#34;&gt;engine&lt;/h2&gt;

&lt;p&gt;Engine是Docker架构中的运行引擎，同时也Docker运行的核心模块。它扮演Docker container存储仓库的角色，并且通过执行job的方式来操纵管理这些容器。&lt;/p&gt;

&lt;p&gt;在Engine数据结构的设计与实现过程中，有一个handler对象。该handler对象存储的都是关于众多特定job的handler处理访问。举例说明，Engine的handler对象中有一项为：{“create”: daemon.ContainerCreate,}，则说明当名为”create”的job在运行时，执行的是daemon.ContainerCreate的handler。&lt;/p&gt;

&lt;h2 id=&#34;job&#34;&gt;job&lt;/h2&gt;

&lt;p&gt;一个Job可以认为是Docker架构中Engine内部最基本的工作执行单元。Docker可以做的每一项工作，都可以抽象为一个job。例如：在容器内部运行一个进程，这是一个job；创建一个新的容器，这是一个job，从Internet上下载一个文档，这是一个job；包括之前在Docker Server部分说过的，创建Server服务于HTTP的API，这也是一个job，等等。&lt;/p&gt;

&lt;p&gt;Job的设计者，把Job设计得与Unix进程相仿。比如说：Job有一个名称，有参数，有环境变量，有标准的输入输出，有错误处理，有返回状态等。&lt;/p&gt;

&lt;h2 id=&#34;docker-registry&#34;&gt;docker registry&lt;/h2&gt;

&lt;p&gt;Docker Registry是一个存储容器镜像的仓库。而容器镜像是在容器被创建时，被加载用来初始化容器的文件架构与目录。&lt;/p&gt;

&lt;p&gt;在Docker的运行过程中，Docker Daemon会与Docker Registry通信，并实现搜索镜像、下载镜像、上传镜像三个功能，这三个功能对应的job名称分别为”search”，”pull” 与 “push”。&lt;/p&gt;

&lt;p&gt;其中，在Docker架构中，Docker可以使用公有的Docker Registry，即大家熟知的Docker Hub，如此一来，Docker获取容器镜像文件时，必须通过互联网访问Docker Hub；同时Docker也允许用户构建本地私有的Docker Registry，这样可以保证容器镜像的获取在内网完成。&lt;/p&gt;

&lt;h2 id=&#34;graph&#34;&gt;Graph&lt;/h2&gt;

&lt;p&gt;Graph在Docker架构中扮演已下载容器镜像的保管者，以及已下载容器镜像之间关系的记录者。一方面，Graph存储着本地具有版本信息的文件系统镜像，另一方面也通过GraphDB记录着所有文件系统镜像彼此之间的关系。&lt;/p&gt;

&lt;p&gt;其中，GraphDB是一个构建在SQLite之上的小型图数据库，实现了节点的命名以及节点之间关联关系的记录。它仅仅实现了大多数图数据库所拥有的一个小的子集，但是提供了简单的接口表示节点之间的关系。&lt;/p&gt;

&lt;p&gt;同时在Graph的本地目录中，关于每一个的容器镜像，具体存储的信息有：该容器镜像的元数据，容器镜像的大小信息，以及该容器镜像所代表的具体rootfs。&lt;/p&gt;

&lt;h2 id=&#34;driver&#34;&gt;driver&lt;/h2&gt;

&lt;p&gt;Driver是Docker架构中的驱动模块。通过Driver驱动，Docker可以实现对Docker容器执行环境的定制。由于Docker运行的生命周期中，并非用户所有的操作都是针对Docker容器的管理，另外还有关于Docker运行信息的获取，Graph的存储与记录等。因此，为了将Docker容器的管理从Docker Daemon内部业务逻辑中区分开来，设计了Driver层驱动来接管所有这部分请求。&lt;/p&gt;

&lt;p&gt;在Docker Driver的实现中，可以分为以下三类驱动：graphdriver、networkdriver和execdriver。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;graphdriver主要用于完成容器镜像的管理，包括存储与获取。即当用户需要下载指定的容器镜像时，graphdriver将容器镜像存储在本地的指定目录；同时当用户需要使用指定的容器镜像来创建容器的rootfs时，graphdriver从本地镜像存储目录中获取指定的容器镜像。
在graphdriver的初始化过程之前，有4种文件系统或类文件系统在其内部注册，它们分别是aufs、btrfs、vfs和devmapper。而Docker在初始化之时，通过获取系统环境变量”DOCKER_DRIVER”来提取所使用driver的指定类型。而之后所有的graph操作，都使用该driver来执行。&lt;/li&gt;
&lt;li&gt;networkdriver的用途是完成Docker容器网络环境的配置，其中包括Docker启动时为Docker环境创建网桥；Docker容器创建时为其创建专属虚拟网卡设备；以及为Docker容器分配IP、端口并与宿主机做端口映射，设置容器防火墙策略等。networkdriver的架构如下：&lt;/li&gt;
&lt;li&gt;execdriver作为Docker容器的执行驱动，负责创建容器运行命名空间，负责容器资源使用的统计与限制，负责容器内部进程的真正运行等。在execdriver的实现过程中，原先可以使用LXC驱动调用LXC的接口，来操纵容器的配置以及生命周期，而现在execdriver默认使用native驱动，不依赖于LXC。具体体现在Daemon启动过程中加载的ExecDriverflag参数，该参数在配置文件已经被设为”native”。这可以认为是Docker在1.2版本上一个很大的改变，或者说Docker实现跨平台的一个先兆。execdriver架构如下：&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;libcontainer&#34;&gt;libcontainer&lt;/h2&gt;

&lt;p&gt;libcontainer是Docker架构中一个使用Go语言设计实现的库，设计初衷是希望该库可以不依靠任何依赖，直接访问内核中与容器相关的API。&lt;/p&gt;

&lt;p&gt;正是由于libcontainer的存在，Docker可以直接调用libcontainer，而最终操纵容器的namespace、cgroups、apparmor、网络设备以及防火墙规则等。这一系列操作的完成都不需要依赖LXC或者其他包。libcontainer架构如下：&lt;/p&gt;

&lt;p&gt;另外，libcontainer提供了一整套标准的接口来满足上层对容器管理的需求。或者说，libcontainer屏蔽了Docker上层对容器的直接管理。又由于libcontainer使用Go这种跨平台的语言开发实现，且本身又可以被上层多种不同的编程语言访问，因此很难说，未来的Docker就一定会紧紧地和Linux捆绑在一起。而于此同时，Microsoft在其著名云计算平台Azure中，也添加了对Docker的支持，可见Docker的开放程度与业界的火热度。&lt;/p&gt;

&lt;p&gt;暂不谈Docker，由于libcontainer的功能以及其本身与系统的松耦合特性，很有可能会在其他以容器为原型的平台出现，同时也很有可能催生出云计算领域全新的项目。&lt;/p&gt;

&lt;h2 id=&#34;docker-container&#34;&gt;docker container&lt;/h2&gt;

&lt;p&gt;Docker container（Docker容器）是Docker架构中服务交付的最终体现形式，Docker按照用户的需求与指令，订制相应的Docker容器。&lt;/p&gt;

&lt;p&gt;用户通过指定容器镜像，使得Docker容器可以自定义rootfs等文件系统； 用户通过指定计算资源的配额，使得Docker容器使用指定的计算资源； 用户通过配置网络及其安全策略，使得Docker容器拥有独立且安全的网络环境； 用户通过指定运行的命令，使得Docker容器执行指定的工作。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;docker引擎原理&#34;&gt;docker引擎原理&lt;/h2&gt;

&lt;p&gt;docker在版本0.9之前是以lxc（linux container）为基础的，0.9以后又引入了自己开发的libcontainer，试图设计出更加通用的容器化技术。&lt;/p&gt;

&lt;p&gt;docker采用c/s架构，服务端默认在后台运行，进行容器的创建等相关操作，客户端就是我们调用的命令行操作，每次客户端运行完就会退出等待再次调用，客户端和服务端是通过socket进行通信的，默认是unix:///var/run/docker.sock,可以通过-H参数修改端口。&lt;/p&gt;

&lt;p&gt;docker是一种轻量级的操作系统虚拟化方案，基于LXC，相对于传统的vm，它只虚拟来操作系统而没有虚拟内核，基于docker engine共享内核，即虚拟出运行库即可运行app，而传统的vm都是虚拟出完整的内核和运行库文件的操作系统。即虚拟出运行库即可运行app，而传统的vm都是虚拟出完整的内核和运行库文件的操作系统。&lt;/p&gt;

&lt;p&gt;docker都是独立的系统，是对操作系统的一种虚拟化。所以需要对其内核，文件系统，网络，PID，UID，IPC，内存，硬盘，cpu进行限制隔离，我们就是通过命名空间namespace，控制组cgroup，联合文件系统UFS，虚拟网络来完成的隔离独立运行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/docker3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;namespace&#34;&gt;namespace&lt;/h3&gt;

&lt;p&gt;Linux内核实现namespace的主要目的就是为了实现轻量级虚拟化（容器）服务。在同一个namespace下的进程可以感知彼此的变化，而对外界的进程一无所知。这样就可以让容器中的进程产生错觉，仿佛自己置身于一个独立的系统环境中，以此达到独立和隔离的目的。&lt;/p&gt;

&lt;p&gt;命名空间提供对于某个进程的一种“视图”，可以将其它命名空间的东西隐藏，从而为该进程提供它自己独立运行的环境。这使得进程之间互相不可见，也不可相互影响,命名空间包括如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IPC：隔离System V IPC和POSIX消息队列。
Network：隔离网络资源。
Mount：隔离文件系统挂载点。每个容器能看到不同的文件系统层次结构。
PID：隔离进程ID。
UTS：隔离主机名和域名。
User：隔离用户ID和组ID。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Docker 引擎在 Linux 上使用以下 namesspaces：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pid namespace：进程隔离（PID: Process ID）
net namespace：管理网络接口（NET: Networking）
ipc namespace：管理访问 IPC 资源（IPC: InterProcess Communication）
mnt namespace：管理文件系统挂载点（MNT: Mount）
uts namespace：隔离内核和版本标识符（UTS: Unix Timesharing System）
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;cgroup&#34;&gt;cgroup&lt;/h3&gt;

&lt;p&gt;Linux 上的 Docker 引擎依赖的另外一项技术叫 control groups （ cgroups ）&lt;/p&gt;

&lt;p&gt;cgroup和namespace类似，也是将进程进行分组，但它的目的和namespace不一样，namespace是为了隔离进程组之间的资源，而cgroup是为了对一组进程进行统一的资源监控和限制。&lt;/p&gt;

&lt;p&gt;用于限制和隔离一组进程对系统资源的使用，也就是做资源QoS，这些资源主要包括CPU、内存、block I/O和网络带宽。资源限制的意思就是如设定应用运行时使用内存的上限，一旦超过这个配额就发出OOM（Out of Memory）。&lt;/p&gt;

&lt;p&gt;Cgroups提供了以下四大功能:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;资源限制（Resource Limitation）：cgroups可以对进程组使用的资源总额进行限制。&lt;/li&gt;
&lt;li&gt;优先级分配（Prioritization）：通过分配的CPU时间片数量及硬盘IO带宽大小，实际上就相当于控制了进程运行的优先级。&lt;/li&gt;
&lt;li&gt;资源统计（Accounting）： cgroups可以统计系统的资源使用量，如CPU使用时长、内存用量等等，这个功能非常适用于计费。&lt;/li&gt;
&lt;li&gt;进程控制（Control）：cgroups可以对进程组执行挂起、恢复等操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cgroups的子系统可以通过查看文件系统目录/sys/fs/cgroup/来查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ls /sys/fs/cgroup/
blkio/            cpuacct/          cpuset/           freezer/          memory/           net_cls,net_prio/ perf_event/       rdma/
cpu/              cpu,cpuacct/      devices/          hugetlb/          net_cls/          net_prio/         pids/             systemd/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看出移动有如下子系统&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;devices：设备权限控制。
cpuset：分配指定的CPU和内存节点。
cpu：控制CPU占用率。
    CPU资源的控制也有两种策略，一种是完全公平调度 （CFS：Completely Fair Scheduler）策略，提供了限额和按比例分配两种方式进行资源控制；另一种是实时调度（Real-Time Scheduler）策略，针对实时进程按周期分配固定的运行时间。
cpuacct：统计CPU使用情况。
memory：限制内存的使用上限。
    memory.limit_bytes：强制限制最大内存使用量，单位有k、m、g三种，填-1则代表无限制。
    memory.usage_bytes：报​​​告​​​该​​​ cgroup中​​​进​​​程​​​使​​​用​​​的​​​当​​​前​​​总​​​内​​​存​​​用​​​量（以字节为单位）。
    memory.max_usage_bytes：报​​​告​​​该​​​ cgroup 中​​​进​​​程​​​使​​​用​​​的​​​最​​​大​​​内​​​存​​​用​​​量。
freezer：冻结（暂停）Cgroup中的进程。
net_cls：配合tc（traffic controller）限制网络带宽。
net_prio：设置进程的网络流量优先级。
huge_tlb：限制HugeTLB的使用。
perf_event：允许Perf工具基于Cgroup分组做性能监测。
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;rootfs&#34;&gt;rootfs&lt;/h3&gt;

&lt;p&gt;Linux 和 Unix 操作系统可以通过 chroot 系统调用将子目录变成根目录，达到视图级别的隔离，进程在 chroot 的帮助下可以具有独立的文件系统，对于这样的文件系统进行增删改查不会影响到其他进程。&lt;/p&gt;

&lt;h2 id=&#34;images&#34;&gt;images&lt;/h2&gt;

&lt;p&gt;容器镜像，是容器分发的载体，很多时候讲容器镜像比作成集装箱，镜像里面包含了应用以及应用运行所需要的依赖，容器镜像采用分层存储，最底层是bootfs/rootfs, bootfs是最底层，类似于linux的引导文件系统，rootfs是基础镜像，比如这里基础镜像是ubuntu:15.04的系统，之后再基础镜像的基础上去做修改，比如增加应用bin文件，以及对环境变量的设置，每种改动最后在镜像中的表现是依次往上进行叠加的image layer。每一层是只读的，&lt;/p&gt;

&lt;p&gt;镜像文件的存储也是采用分层存储，比如多个镜像中存在某一层的内容是相同，那么只会存储一份，极大的节省了存储的成本。如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/image.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;容器运行的时候会加载镜像中的文件系统，利用联合挂载（union mount）技术在已经有的只读文件系统中再挂载一个读写层,容器运行中的对文件系统的改动都会写到该层中，下面的只读层是保持不变，采用了COPY-ON-Writer。当容器删掉之后，这个可读写的层也会消失。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/container.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;docker网络原理&#34;&gt;docker网络原理&lt;/h2&gt;

&lt;p&gt;详细说明在&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/docker/docker-network/&#34;&gt;docker网络&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s组件系列（九）---- 网络</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network/</link>
          <pubDate>Fri, 10 Apr 2020 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network/</guid>
          <description>&lt;p&gt;Kubernetes中有三种网络和三种IP。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/network&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;docker网络实现&#34;&gt;docker网络实现&lt;/h1&gt;

&lt;p&gt;想要理解k8s的网络，需要先了解docker的网络实现，用过docker基本都知道，启动docker engine后，主机的网络设备里会有一个docker0的网关，而容器默认情况下会被分配在一个以docker0为网关的虚拟子网中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@VM-66-197-ubuntu:/home/ubuntu# ifconfig
...
docker0   Link encap:Ethernet  HWaddr 02:42:ec:43:56:b2
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
...
root@VM-66-197-ubuntu:/home/ubuntu# docker inspect nginx
···
&amp;quot;IPAddress&amp;quot;: &amp;quot;172.17.0.2&amp;quot;,
···
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了实现上述功能，docker主要用到了linux的Bridge、Network Namespace、VETH。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bridge相当于是一个虚拟网桥，工作在第二层网络。也可以为它配置IP，工作在三层网络。docker0网关就是通过Bridge实现的。&lt;/li&gt;
&lt;li&gt;Network Namespace是网络命名空间，通过Network Namespace可以建立一些完全隔离的网络栈。比如通过docker network create xxx就是在建立一个Network Namespace。&lt;/li&gt;
&lt;li&gt;VETH是虚拟网卡的接口对，可以把两端分别接在两个不同的Network Namespace中，实现两个原本隔离的Network Namespace的通信。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以总结起来就是：Network Namespace做了容器和宿主机的网络隔离，Bridge分别在容器和宿主机建立一个网关，然后再用VETH将容器和宿主机两个网络空间连接起来。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/network/bridge&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这就docker默认的bridge实现方式，对此，docker总结提出来CNM（container network model）理论，还在这个理论的基础上实现了libnetwork。具体docker网络的实现可以查看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/docker/docker-network/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;kubernetes网络模型&#34;&gt;Kubernetes网络模型&lt;/h1&gt;

&lt;p&gt;1、每个Pod拥有唯一的IP（perPodperIp模型）。&lt;/p&gt;

&lt;p&gt;2、所有的pod都在一个可以直接连通的、扁平的网络空间中。满足以下条件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;任意两个 pod 之间其实是可以直接通信的，无需经过显式地使用 NAT 来接收数据和地址的转换；&lt;/li&gt;
&lt;li&gt;node 与 pod 之间是可以直接通信的，无需使用明显的地址转换；&lt;/li&gt;
&lt;li&gt;pod 看到自己的 IP 跟别人看见它所用的 IP 是一样的，中间不能经过转换。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、四大目标，其实是在设计一个 K8s 的系统为外部世界提供服务的时候，从网络的角度要想清楚，外部世界如何一步一步连接到容器内部的应用？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;外部世界和 service 之间是怎么通信的？就是有一个互联网或者是公司外部的一个用户，怎么用到 service？service 特指 K8s 里面的服务概念。&lt;/li&gt;
&lt;li&gt;service 如何与它后端的 pod 通讯？&lt;/li&gt;
&lt;li&gt;pod 和 pod 之间调用是怎么做到通信的？&lt;/li&gt;
&lt;li&gt;最后就是 pod 内部容器与容器之间的通信？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最终要达到目标，就是外部世界可以连接到最里面，对容器提供服务。&lt;/p&gt;

&lt;h1 id=&#34;pod-network&#34;&gt;Pod Network&lt;/h1&gt;

&lt;h2 id=&#34;pod内网络&#34;&gt;pod内网络&lt;/h2&gt;

&lt;p&gt;Kubernetes的一个Pod中包含有多个容器，这些容器共享一个Network Namespace，更具体的说，是共享一个Network Namespace中的一个IP。创建Pod时，首先会生成一个pause容器，然后其他容器会共享pause容器的网络。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@kube-2:~# docker ps
CONTAINER ID        IMAGE                                            COMMAND                  CREATED             STATUS              PORTS               NAMES
d2dbb9e288e2        mirrorgooglecontainers/pause-amd64:3.0           &amp;quot;/pause&amp;quot;                 3 weeks ago         Up 3 weeks                              k8s_POD_frontend-647d9fdddf-n4x9w_default_9f275ea8-4853-11e8-8c42-fa163e4a07e5_0
cf1bfff28238        nginx                                            &amp;quot;nginx -g &#39;daemon of…&amp;quot;   3 weeks ago         Up 3 weeks                              k8s_nginx-demo_frontend-647d9fdddf-n4x9w_default_9f275ea8-4853-11e8-8c42-fa163e4a07e5_0

root@kube-2:~# docker inspect cf1bf
...
            &amp;quot;NetworkMode&amp;quot;: &amp;quot;container:d2dbb9e288e26231759e28e8d4816862c6c57d4d2822a259bee7fcc9a2fd0b20&amp;quot;,
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出，在这个Pod中，nginx容器通过&amp;rdquo;NetworkMode&amp;rdquo;: &amp;ldquo;container:d2db&amp;hellip;&amp;ldquo;与pause容器共享了网络，我们可以想到docker我网络的contaienr模式。这时候，相同容器之间的访问只需要用localhost+端口的形式，就像他们是部署在同一台物理机的不同进程一样，可以使用本地IPC进行通信。&lt;/p&gt;

&lt;h2 id=&#34;pod间通信&#34;&gt;pod间通信&lt;/h2&gt;

&lt;h3 id=&#34;节点内通信&#34;&gt;节点内通信&lt;/h3&gt;

&lt;p&gt;在每个Kubernetes节点上，都有一个根（root）命名空间（root是作为基准，而不是超级用户）&amp;ndash;root netns。这个命令空间就是我们node节点的网络，包含这个物理网卡eth0。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/network10&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;类似的，每个Pod都有其自身的netns，通过一个虚拟的以太网对连接到root netns。这基本上就是一个管道对，一端在root netns内，另一端在Pod的nens内。&lt;/p&gt;

&lt;p&gt;我们把Pod端的网络接口叫 eth0，这样Pod就不需要知道底层主机，它认为它拥有自己的根网络设备。另一端命名成比如 vethxxx。你可以用ifconfig 或者 ip a 命令列出你的节点上的所有这些接口。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/network11&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;节点上的所有Pod都会完成这个过程。这些Pod要相互通信，就要用到linux的以太网桥 cbr0 了。Docker使用了类似的网桥，称为docker0。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/network12&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;所以一个网络数据包要由pod1到pod2。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;它由pod1中netns的eth0网口离开，通过vethxxx进入root netns。&lt;/li&gt;
&lt;li&gt;然后被传到cbr0，cbr0使用ARP请求，说“谁拥有这个IP”，从而发现目标地址。&lt;/li&gt;
&lt;li&gt;vethyyy说它有这个IP，因此网桥就知道了往哪里转发这个包。&lt;/li&gt;
&lt;li&gt;数据包到达vethyyy，跨过管道对，到达pod2的netns。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这就是同一节点内容器间通信的流程。当然也可以用其它方式，但是无疑这是最简单的方式，同时也是Docker单机采用的方式，可以看出只是把container换成了pod。&lt;/p&gt;

&lt;h3 id=&#34;节点间通信&#34;&gt;节点间通信&lt;/h3&gt;

&lt;p&gt;正如我前面提到，Pod也需要跨节点可达。Kubernetes不关心如何实现。我们可以使用L2（ARP跨节点），L3（IP路由跨节点，就像云提供商的路由表），Overlay网络，或者甚至信鸽。无所谓，只要流量能到达另一个节点的期望Pod就好。每个节点都为Pod IPs分配了唯一的CIDR块（一段IP地址范围），因此每个Pod都拥有唯一的IP，不会和其它节点上的Pod冲突。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/network13&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一个数据包要从pod1到达pod4（在不同的节点上）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;它由pod1中netns的eth0网口离开，通过vethxxx进入root netns。&lt;/li&gt;
&lt;li&gt;然后被传到cbr0，cbr0通过发送ARP请求来找到目标地址。&lt;/li&gt;
&lt;li&gt;本节点上没有Pod拥有pod4的IP地址，因此数据包由cbr0 传到 主网络接口 eth0.&lt;/li&gt;
&lt;li&gt;数据包的源地址为pod1，目标地址为pod4，它以这种方式离开node1进入电缆。&lt;/li&gt;
&lt;li&gt;路由表有每个节点的CIDR块的路由设定，它把数据包路由到CIDR块包含pod4的IP的节点。&lt;/li&gt;
&lt;li&gt;因此数据包到达了node2的主网络接口eth0。现在即使pod4不是eth0的IP，数据包也仍然能转发到cbr0，因为节点配置了IP forwarding enabled。节点的路由表寻找任意能匹配pod4 IP的路由。它发现了 cbr0 是这个节点的CIDR块的目标地址。你可以用route -n命令列出该节点的路由表，它会显示cbr0的路由。&lt;/li&gt;
&lt;li&gt;网桥接收了数据包，发送ARP请求，发现目标IP属于vethyyy。&lt;/li&gt;
&lt;li&gt;数据包跨过管道对到达pod4。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;node-network&#34;&gt;Node network&lt;/h1&gt;

&lt;p&gt;其实就是整个k8s集群node之间的网络，也就是基本的物理机网络，只要各个node之间能相互通信就好。&lt;/p&gt;

&lt;h1 id=&#34;service-network&#34;&gt;Service Network&lt;/h1&gt;

&lt;p&gt;service网络主要是kube-proxy来实现构建的,主要是Pod 和 Service 间通信，外部和 Service 间通信，这些都可以使用kube-proxy的各种模式来解决，具体可以看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-proxy/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;service network比较特殊，每个新创建的service会被分配一个service IP，比如在集群中设置环境变量export SERVICE_CLUSTER_IP_RANGE=${SERVICE_CLUSTER_IP_RANGE:-192.168.3.0/24}，并在kube-apiserver的启动参数&amp;ndash;service-cluster-ip-range使用这个环境变量，这个IP的分配范围是192.168.3.0/24。不过这个IP并不“真实”，更像一个“占位符”并且只有入口流量。&lt;/p&gt;

&lt;p&gt;在 Service 创建的请求中，可以通过设置 spec.clusterIP 字段来指定自己的集群 IP 地址。 比如，希望替换一个已经已存在的 DNS 条目，或者遗留系统已经配置了一个固定的 IP 且很难重新配置。用户选择的 IP 地址必须合法，并且这个 IP 地址在 service-cluster-ip-range CIDR 范围内，这对 API Server 来说是通过一个标识来指定的。 如果 IP 地址不合法，API Server 会返回 HTTP 状态码 422，表示值不合法。&lt;/p&gt;

&lt;h1 id=&#34;kubernetes网络开源组件&#34;&gt;Kubernetes网络开源组件&lt;/h1&gt;

&lt;p&gt;service network的实现依靠kube-proxy，node network依靠的是物理组网，pod network就需要插件来完成了，目前比较通用的就是CNI规范的插件。其实主要是解决两个问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;为容器分配IP地址&lt;/li&gt;
&lt;li&gt;不同容器之间的互通&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;

&lt;p&gt;第2层网络：OSI（Open Systems Interconnections，开放系统互连）网络模型的“数据链路”层。第2层网络会处理网络上两个相邻节点之间的帧传递。第2层网络的一个值得注意的示例是以太网，其中MAC表示为子层。&lt;/p&gt;

&lt;p&gt;第3层网络：OSI网络模型的“网络”层。第3层网络的主要关注点，是在第2层连接之上的主机之间路由数据包。IPv4、IPv6和ICMP是第3层网络协议的示例。&lt;/p&gt;

&lt;p&gt;IPAM：IP地址管理；这个IP地址管理并不是容器所特有的，传统的网络比如说DHCP其实也是一种IPAM，到了容器时代我们谈IPAM，主流的两种方法： 基于CIDR的IP地址段分配地或者精确为每一个容器分配IP。但总之一旦形成一个容器主机集群之后，上面的容器都要给它分配一个全局唯一的IP地址，这就涉及到IPAM的话题。&lt;/p&gt;

&lt;p&gt;Overlay：在现有二层或三层网络之上再构建起来一个独立的网络，这个网络通常会有自己独立的IP地址空间、交换或者路由的实现。&lt;/p&gt;

&lt;p&gt;IPSesc：一个点对点的一个加密通信协议，一般会用到Overlay网络的数据通道里。&lt;/p&gt;

&lt;p&gt;vxLAN：由VMware、Cisco、RedHat等联合提出的这么一个解决方案，这个解决方案最主要是解决VLAN支持虚拟网络数量（4096）过少的问题。因为在公有云上每一个租户都有不同的VPC，4096明显不够用。就有了vxLAN，它可以支持1600万个虚拟网络，基本上公有云是够用的。&lt;/p&gt;

&lt;p&gt;网桥Bridge： 连接两个对等网络之间的网络设备，但在今天的语境里指的是Linux Bridge，就是大名鼎鼎的Docker0这个网桥。&lt;/p&gt;

&lt;p&gt;BGP： 代表“边界网关协议”，主干网自治网络的路由协议，今天有了互联网，互联网由很多小的自治网络构成的，自治网络之间的三层路由是由BGP实现的。&lt;/p&gt;

&lt;p&gt;SDN、Openflow： 软件定义网络里面的一个术语，比如说我们经常听到的流表、控制平面，或者转发平面都是Openflow里的术语。&lt;/p&gt;

&lt;h2 id=&#34;容器网络模型&#34;&gt;容器网络模型&lt;/h2&gt;

&lt;p&gt;容器网络发展到现在，形成了两大阵营，就是Docker的CNM和Google、CoreOS、Kuberenetes主导的CNI。首先明确一点，CNM和CNI并不是网络实现，他们是网络规范和网络体系，从研发的角度他们就是一堆接口，你底层是用Flannel也好、用Calico也好，他们并不关心，CNM和CNI关心的是网络管理的问题。&lt;/p&gt;

&lt;h3 id=&#34;cnm&#34;&gt;CNM&lt;/h3&gt;

&lt;p&gt;CNM（Docker LibnetworkContainer Network Model）&lt;/p&gt;

&lt;p&gt;Docker Libnetwork的优势就是原生，而且和Docker容器生命周期结合紧密；缺点也可以理解为是原生，被Docker“绑架”。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Docker Swarm overlay
Macvlan &amp;amp; IP networkdrivers
Calico
Contiv
Weave
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前因为docker很少单独使用，使用CMN规范的也比较少，但是很多都是相通的。&lt;/p&gt;

&lt;h3 id=&#34;cni&#34;&gt;CNI&lt;/h3&gt;

&lt;p&gt;CNI的优势是兼容其他容器技术（e.g. rkt）及上层编排系统（Kubernetes &amp;amp; Mesos)，而且社区活跃势头迅猛，Kubernetes加上CoreOS主推，缺点是非Docker原生。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Kubernetes
Weave
Macvlan
Calico
Flannel
Contiv
Mesos CNI
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kubernetes 在处理网络上，没有选择自己再独立创造一个，而是选择了其中的 CNI作为了自己的网络插件。（至于为什么不选择 CNM，可以看看这篇官方的解释：Why Kubernetes doesn’t use libnetwork）。不使用 CNM 最关键的一点，是 k8s 考虑到CNM 在一定程度上和 container runtime 联系相对比较紧密，不好解耦。
有了 k8s 这种巨无霸的选择之后，后来的很多项目都在 CNM 和 CNI 之间选择了 CNI。&lt;/p&gt;

&lt;p&gt;CNI目前已经获得了众多开源项目的青睐，比如 K8S、Memos、Cloud Foundry。同时被Cloud Native Computing Foundation所认可。CNCF 背后有众多的科技大亨，所以可以预见，CNI 将会成为未来容器网络的标准，所以很有必要详细研究了解&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network-cni/&#34;&gt;CNI&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;容器网络方案&#34;&gt;容器网络方案&lt;/h2&gt;

&lt;p&gt;容器网络方案基本就是只有下面几种解决方式，后面的组件实现都是基于这些思路来进行的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;隧道方案（Overlay Networking）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;隧道方案在IaaS层的网络中应用也比较多，大家共识是随着节点规模的增长复杂度会提升，而且出了网络问题跟踪起来比较麻烦，大规模集群情况下这是需要考虑的一个点。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Weave：UDP广播，本机建立新的BR，通过PCAP互通
Open vSwitch（OVS）：基于VxLan和GRE协议，但是性能方面损失比较严重
Flannel：UDP广播，VxLan
Racher：IPsec
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;路由方案&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;路由方案一般是从3层或者2层实现隔离和跨主机容器互通的，出了问题也很容易排查。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Calico：基于BGP协议的路由方案，支持很细致的ACL控制，对混合云亲和度比较高。
Macvlan：从逻辑和Kernel层来看隔离性和性能最优的方案，基于二层隔离，所以需要二层路由器支持，大多数云服务商不支持，所以混合云上比较难以实现。
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;底层网络（Underlay Networking）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;vpc网络，也就是路由表，是指在交换机上直接配置对应的ip转发到对应的node，一般私有云都会使用vpc方案。&lt;/p&gt;

&lt;h2 id=&#34;组件&#34;&gt;组件&lt;/h2&gt;

&lt;p&gt;其实网络插件解决的是pod network的问题，都是基于CMN或者CNI的管理规范来开发的网络插件。&lt;/p&gt;

&lt;h3 id=&#34;kubenet&#34;&gt;kubenet&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/kubenet&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;kubenet 是一个基于 CNI bridge 的网络插件，它为每个容器建立一对 veth pair 并连接到 cbr0 网桥上。kubenet 在 bridge 插件的基础上拓展了很多功能，包括&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用 host-local IPAM 插件为容器分配 IP 地址， 并定期释放已分配但未使用的 IP 地址&lt;/li&gt;
&lt;li&gt;设置 sysctl net.bridge.bridge-nf-call-iptables = 1&lt;/li&gt;
&lt;li&gt;为 Pod IP 创建 SNAT 规则

&lt;ul&gt;
&lt;li&gt;-A POSTROUTING ! -d 10.0.0.0/8 -m comment &amp;ndash;comment &amp;ldquo;kubenet: SNAT for outbound traffic from cluster&amp;rdquo; -m addrtype ! &amp;ndash;dst-type LOCAL -j MASQUERADE&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;开启网桥的 hairpin 和 promisc 模式，允许 Pod 访问它自己所在的 Service IP（即通过 NAT 后再访问 Pod 自己）

&lt;ul&gt;
&lt;li&gt;-A OUTPUT -j KUBE-DEDUP&lt;/li&gt;
&lt;li&gt;-A KUBE-DEDUP -p IPv4 -s a:58:a:f4:2:1 -o veth+ &amp;ndash;ip-src 10.244.2.1 -j ACCEPT&lt;/li&gt;
&lt;li&gt;-A KUBE-DEDUP -p IPv4 -s a:58:a:f4:2:1 -o veth+ &amp;ndash;ip-src 10.244.2.0/24 -j DROP&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;HostPort 管理以及设置端口映射&lt;/li&gt;
&lt;li&gt;Traffic shaping，支持通过 kubernetes.io/ingress-bandwidth 和 kubernetes.io/egress-bandwidth 等 Annotation 设置 Pod 网络带宽限制&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;未来 kubenet 插件会迁移到标准的 CNI 插件。&lt;/p&gt;

&lt;h3 id=&#34;flannel&#34;&gt;flannel&lt;/h3&gt;

&lt;p&gt;Flannel是CoreOS团队针对Kubernetes设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。&lt;/p&gt;

&lt;p&gt;在默认的Docker配置中，每个节点上的Docker服务会分别负责所在节点容器的IP分配。这样导致的一个问题是，不同节点上容器可能获得相同的内外IP地址。并使这些容器之间能够之间通过IP地址相互找到，也就是相互ping通。&lt;/p&gt;

&lt;p&gt;Flannel的设计目的就是为集群中的所有节点重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，并让属于不同节点上的容器能够直接通过内网IP通信。&lt;/p&gt;

&lt;p&gt;Flannel实质上是一种“覆盖网络(overlaynetwork)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持udp、vxlan、host-gw、aws-vpc、gce和alloc路由等数据转发方式，默认的节点间数据通信方式是UDP转发。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;flannel网络模型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、互补冲突的ip&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;flannel利用Kubernetes API或者etcd用于存储整个集群的网络配置，根据配置记录集群使用的网段。&lt;/li&gt;
&lt;li&gt;flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以查看/run/flannel/subnet.env文件来查看对于的IP分配，在不同的主机上进行分配。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/flannel&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在flannel network中，每个pod都会被分配唯一的ip地址，且每个K8s node的subnet各不重叠，没有交集，如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/flannel1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、pod间互相访问&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;flanneld将本主机获取的subnet以及用于主机间通信的Public IP通过etcd存储起来，需要时发送给相应模块。&lt;/li&gt;
&lt;li&gt;flannel利用各种backend mechanism，例如udp，vxlan等等，跨主机转发容器间的网络流量，完成容器间的跨主机通信。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;flannel架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/flannel2&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/flannel3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;详解&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建pod，分配ip，由于k8s更改了docker的DOCKER_OPTS，显式指定了–bip，这个值与分配给该node上的subnet的范围是一致的。这样一来，docker引擎每次创建一个Docker container，该container被分配到的ip都在flannel subnet范围内。&lt;/li&gt;
&lt;li&gt;数据包到达docker0网桥，docker0的内核栈处理程序发现这个数据包的目的地址是172.16.57.15，并不是真的要送给自己，于是开始为该数据包找下一hop。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;flannel.1收到数据包后，由于自己不是目的地，也要尝试将数据包重新发送出去。数据包沿着网络协议栈向下流动，在二层时需要封二层以太包，填写目的mac地址，这时一般应该发出arp：”who is 172.16.57.15″。但vxlan设备的特殊性就在于它并没有真正在二层发出这个arp包，因为下面的这个内核参数设置，而是由linux kernel引发一个”L3 MISS”事件并将arp请求发到用户空间的flanned程序。flanned程序收到”L3 MISS”内核事件以及arp请求(who is 172.16.57.15)后，并不会向外网发送arp request，而是尝试从etcd查找该地址匹配的子网的vtep信息。在前面章节我们曾经展示过etcd中Flannel network的配置信息，flanneld从etcd中找到了，接下来，flanned将查询到的信息放入master node host的arp cache表中，flanneld完成这项工作后，linux kernel就可以在arp table中找到 172.16.57.15对应的mac地址并封装二层以太包了。不过这个封包还不能在物理网络上传输，因为它实际上只是vxlan tunnel上的packet。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  # cat /proc/sys/net/ipv4/neigh/flannel.1/app_solicit
  3
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;我们需要将上述的packet再次封包。这个任务在backend为vxlan的flannel network中由linux kernel来完成。flannel.1为vxlan设备，linux kernel可以自动识别，并将上面的packet进行vxlan封包处理。在这个封包过程中，kernel需要知道该数据包究竟发到哪个node上去。kernel需要查看node上的fdb(forwarding database)以获得上面对端vtep设备（已经从arp table中查到其mac地址：d6:51:2e:80:5c:69）所在的node地址。如果fdb中没有这个信息，那么kernel会向用户空间的flanned程序发起”L2 MISS”事件。flanneld收到该事件后，会查询etcd，获取该vtep设备对应的node的”Public IP“，并将信息注册到fdb中。这样Kernel就可以顺利查询到该信息并封包了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;收到上述vxlan包，kernel将识别出这是一个vxlan包，于是拆包后将flannel.1 packet转给minion node上的vtep（flannel.1）。minion node上的flannel.1再将这个数据包转到minion node上的docker0，继而由docker0传输到Pod3的某个容器里。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;flannel后端支持网络&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Flannel可以指定不同的转发后端网络，常用的有hostgw，udp，vxlan等。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;hostgw&lt;/p&gt;

&lt;p&gt;hostgw是最简单的backend，它的原理非常简单，直接添加路由，将目的主机当做网关，直接路由原始封包。&lt;/p&gt;

&lt;p&gt;例如，我们从etcd中监听到一个EventAdded事件subnet为10.1.15.0/24被分配给主机Public IP 192.168.0.100，hostgw要做的工作就是在本主机上添加一条目的地址为10.1.15.0/24，网关地址为192.168.0.100，输出设备为上文中选择的集群间交互的网卡即可。&lt;/p&gt;

&lt;p&gt;优点：简单，直接，效率高&lt;/p&gt;

&lt;p&gt;缺点：要求所有的pod都在一个子网中，如果跨网段就无法通信。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;udp&lt;/p&gt;

&lt;p&gt;如何应对Pod不在一个子网里的场景呢？将Pod的网络包作为一个应用层的数据包，使用UDP封装之后在集群里传输。即overlay。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;因为该封包的目的地不在本主机subnet内，因此封包会首先通过网桥转发到主机中。&lt;/li&gt;
&lt;li&gt;在主机上经过路由匹配，进入网卡flannel.1。(需要注意的是flannel.1是一个tun设备，它是一种工作在三层的虚拟网络设备，而flanneld是一个proxy，它会监听flannel.1并转发流量。)&lt;/li&gt;
&lt;li&gt;当封包进入flannel.1时，flanneld就可以从flanne.1中将封包读出，由于flanne.1是三层设备，所以读出的封包仅仅包含IP层的报头及其负载。&lt;/li&gt;
&lt;li&gt;最后flanneld会将获取的封包作为负载数据，通过udp socket发往目的主机。&lt;/li&gt;
&lt;li&gt;在目的主机的flanneld会监听Public IP所在的设备，从中读取udp封包的负载，并将其放入flannel.1设备内。&lt;/li&gt;
&lt;li&gt;容器网络封包到达目的主机，之后就可以通过网桥转发到目的容器了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;优点：Pod能够跨网段访问&lt;/p&gt;

&lt;p&gt;缺点：隔离性不够，udp不能隔离两个网段。&lt;/p&gt;

&lt;p&gt;其实也就是我们上面的基本架构原理，因为是默认使用的方式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vxlan&lt;/p&gt;

&lt;p&gt;vxlan和上文提到的udp backend的封包结构是非常类似的，不同之处是多了一个vxlan header，以及原始报文中多了个二层的报头。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;flannel更像是经典的桥接模式的扩展。我们知道，在桥接模式中，每台主机的容器都将使用一个默认的网段，容器与容器之间，主机与容器之间都能互相通信。要是，我们能手动配置每台主机的网段，使它们互不冲突。接着再想点办法，将目的地址为非本机容器的流量送到相应主机：如果集群的主机都在一个子网内，就搞一条路由转发过去；若是不在一个子网内，就搞一条隧道转发过去。这样以来，容器的跨网络通信问题就解决了。而flannel做的，其实就是将这些工作自动化了而已。他也存在这个明显的缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;不支持pod之间的网络隔离。Flannel设计思想是将所有的pod都放在一个大的二层网络中，所以pod之间没有隔离策略。&lt;/li&gt;
&lt;li&gt;设备复杂，效率不高。Flannel模型下有三种设备，数量经过多种设备的封装、解析，势必会造成传输效率的下降。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;weave&#34;&gt;Weave&lt;/h3&gt;

&lt;p&gt;Weave Net是一个多主机容器网络方案，支持去中心化的控制平面，各个host上的wRouter间通过建立Full Mesh的TCP链接，并通过Gossip来同步控制信息。这种方式省去了集中式的K/V Store，能够在一定程度上减低部署的复杂性，Weave将其称为“data centric”，而非RAFT或者Paxos的“algorithm centric”。&lt;/p&gt;

&lt;p&gt;数据平面上，Weave通过UDP封装实现L2 Overlay，封装支持两种模式：&lt;/p&gt;

&lt;p&gt;1、运行在user space的sleeve mode：通过pcap设备在Linux bridge上截获数据包并由wRouter完成UDP封装，支持对L2 traffic进行加密，还支持Partial Connection，但是性能损失明显。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/weave&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、运行在kernal space的 fastpath mode：即通过OVS的odp封装VxLAN并完成转发，wRouter不直接参与转发，而是通过下发odp 流表的方式控制转发，这种方式可以明显地提升吞吐量，但是不支持加密等高级功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/weave1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;容器网络&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/weave2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;所有容器都连接到weave网桥&lt;/li&gt;
&lt;li&gt;weave网桥通过veth pair连到内核的openvswitch模块&lt;/li&gt;
&lt;li&gt;跨主机容器通过openvswitch vxlan通信&lt;/li&gt;
&lt;li&gt;policy controller通过配置iptables规则为容器设置网络策略&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;calico&#34;&gt;calico&lt;/h3&gt;

&lt;p&gt;Calico 是一种容器之间互通的网络方案。在虚拟化平台中，比如 OpenStack、Docker 等都需要实现 workloads 之间互连，但同时也需要对容器做隔离控制，就像在 Internet 中的服务仅开放80端口、公有云的多租户一样，提供隔离和管控机制。而在多数的虚拟化平台实现中，通常都使用二层隔离技术来实现容器的网络，这些二层的技术有一些弊端，比如需要依赖 VLAN、bridge 和隧道等技术，其中 bridge 带来了复杂性，vlan 隔离和 tunnel 隧道则消耗更多的资源并对物理环境有要求，随着网络规模的增大，整体会变得越加复杂。我们尝试把 Host 当作 Internet 中的路由器，同样使用 BGP 同步路由，并使用 iptables 来做安全访问策略，最终设计出了 Calico 方案。&lt;/p&gt;

&lt;p&gt;架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/calico&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Calico网络模型主要工作组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Felix：运行在每一台 Host 的 agent 进程，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。

&lt;ul&gt;
&lt;li&gt;Felix会监听ECTD中心的存储，从它获取事件，比如说用户在这台机器上加了一个IP，或者是创建了一个容器等。用户创建pod后，Felix负责将其网卡、IP、MAC都设置好，然后在内核的路由表里面写一条，注明这个IP应该到这张网卡。同样如果用户制定了隔离策略，Felix同样会将该策略创建到ACL中，以实现隔离。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;etcd：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用；&lt;/li&gt;
&lt;li&gt;BGP Client（BIRD）：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。

&lt;ul&gt;
&lt;li&gt;BIRD是一个标准的路由程序，它会从内核里面获取哪一些IP的路由发生了变化，然后通过标准BGP的路由协议扩散到整个其他的宿主机上，让外界都知道这个IP在这里，你们路由的时候得到这里来。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;BGP Route Reflector：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于Calico是一种纯三层的实现，因此可以避免与二层方案相关的数据包封装的操作，中间没有任何的NAT，没有任何的overlay，所以它的转发效率可能是所有方案中最高的，因为它的包直接走原生TCP/IP的协议栈，它的隔离也因为这个栈而变得好做。因为TCP/IP的协议栈提供了一整套的防火墙的规则，所以它可以通过IPTABLES的规则达到比较复杂的隔离逻辑。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;网络模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、IPIP&lt;/p&gt;

&lt;p&gt;从字面来理解，就是把一个IP数据包又套在一个IP包里，即把 IP 层封装到 IP 层的一个 tunnel。它的作用其实基本上就相当于一个基于IP层的网桥！一般来说，普通的网桥是基于mac层的，根本不需 IP，而这个 ipip 则是通过两端的路由做一个 tunnel，把两个本来不通的网络通过点对点连接起来。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/calico1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实这还是一种Overlay模式，需要使用把 IP 层封装到 IP 层的一个 tunnel。在主机的路由上route -n可以看到这个通道tunX。&lt;/p&gt;

&lt;p&gt;2、BGP&lt;/p&gt;

&lt;p&gt;在安装calico网络时，默认安装是IPIP网络。calico.yaml文件中，将CALICO_IPV4POOL_IPIP的值修改成 &amp;ldquo;off&amp;rdquo;，就能够替换成BGP网络。&lt;/p&gt;

&lt;p&gt;BGP网络相比较IPIP网络，最大的不同之处就是没有了隧道设备 tunl0。 前面介绍过IPIP网络pod之间的流量发送tunl0，然后tunl0发送对端设备。BGP网络中，pod之间的流量直接从网卡发送目的地，减少了tunl0这个环节。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/calico2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;总结对比&lt;/p&gt;

&lt;p&gt;IPIP网络：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;流量：tunlo设备封装数据，形成隧道，承载流量。&lt;/li&gt;
&lt;li&gt;适用网络类型：适用于互相访问的pod不在同一个网段中，跨网段访问的场景。外层封装的ip能够解决跨网段的路由问题。&lt;/li&gt;
&lt;li&gt;效率：流量需要tunl0设备封装，效率略低&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BGP网络：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;流量：使用路由信息导向流量&lt;/li&gt;
&lt;li&gt;适用网络类型：适用于互相访问的pod在同一个网段，适用于大型网络。&lt;/li&gt;
&lt;li&gt;效率：原生hostGW，效率高&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;租户隔离问题：Calico 的三层方案是直接在 host 上进行路由寻址，那么对于多租户如果使用同一个 CIDR 网络就面临着地址冲突的问题。&lt;/li&gt;
&lt;li&gt;路由规模问题：通过路由规则可以看出，路由规模和 pod 分布有关，如果 pod离散分布在 host 集群中，势必会产生较多的路由项。&lt;/li&gt;
&lt;li&gt;iptables 规则规模问题：1台 Host 上可能虚拟化十几或几十个容器实例，过多的 iptables 规则造成复杂性和不可调试性，同时也存在性能损耗。&lt;/li&gt;
&lt;li&gt;跨子网时的网关路由问题：当对端网络不为二层可达时，需要通过三层路由机时，需要网关支持自定义路由配置，即 pod 的目的地址为本网段的网关地址，再由网关进行跨三层转发。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;contiv&#34;&gt;contiv&lt;/h3&gt;

&lt;p&gt;Contiv是思科开源的容器网络方案，是一个用于跨虚拟机、裸机、公有云或私有云的异构容器部署的开源容器网络架构，并与主流容器编排系统集成。Contiv最主要的优势是直接提供了多租户网络，并支持L2(VLAN), L3(BGP), Overlay (VXLAN)以及思科自家的ACI。&lt;/p&gt;

&lt;h3 id=&#34;其他&#34;&gt;其他&lt;/h3&gt;

&lt;p&gt;还有很多的网络插件，基本思路都是基于overlay的隧道，或者基于underlay的路由，基本上插件都能支持这两个功能，只是在那个功能上做的更加好，并且还带有其他的辅助功能。其实个人觉得核心解决的问题就是如何进行跨主机网络路由。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- Event And Distributed Event</title>
          <link>https://kingjcy.github.io/post/distributed/distributed-event/</link>
          <pubDate>Tue, 07 Apr 2020 14:53:56 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/distributed-event/</guid>
          <description>&lt;p&gt;事务提供一种机制将一个活动涉及的所有操作纳入到一个不可分割的执行单元，组成事务的所有操作只有在所有操作均能正常执行的情况下方能提交，只要其中任一操作执行失败，都将导致整个事务的回滚。简单地说，事务提供一种“要么什么都不做，要么做全套（All or Nothing）”机制。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/event&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;数据库事务&#34;&gt;数据库事务&lt;/h1&gt;

&lt;p&gt;数据库事务中的四大特性，ACID:&lt;/p&gt;

&lt;p&gt;A:原子性(Atomicity)&lt;/p&gt;

&lt;p&gt;一个事务(transaction)中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。
就像你买东西要么交钱收货一起都执行，要么要是发不出货，就退钱。&lt;/p&gt;

&lt;p&gt;C:一致性(Consistency)&lt;/p&gt;

&lt;p&gt;事务的一致性指的是在一个事务执行之前和执行之后数据库都必须处于一致性状态。如果事务成功地完成，那么系统中所有变化将正确地应用，系统处于有效状态。如果在事务中出现错误，那么系统中的所有变化将自动地回滚，系统返回到原始状态。&lt;/p&gt;

&lt;p&gt;I:隔离性(Isolation)&lt;/p&gt;

&lt;p&gt;指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看到中间状态的数据。
打个比方，你买东西这个事情，是不影响其他人的。&lt;/p&gt;

&lt;p&gt;D:持久性(Durability)&lt;/p&gt;

&lt;p&gt;指的是只要事务成功结束，它对数据库所做的更新就必须永久保存下来。即使发生系统崩溃，重新启动数据库系统后，数据库还能恢复到事务成功结束时的状态。
打个比方，你买东西的时候需要记录在账本上，即使老板忘记了那也有据可查。&lt;/p&gt;

&lt;p&gt;正常数据库操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 开始会话
2. 开始事务
3. 操作1，2，3，4。。。。
4. 提交／回滚事务
5. 完成会话
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很清晰的看出什么是事务。&lt;/p&gt;

&lt;h1 id=&#34;分布式事务&#34;&gt;分布式事务&lt;/h1&gt;

&lt;p&gt;分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。&lt;/p&gt;

&lt;h2 id=&#34;分布式事务产生的原因&#34;&gt;分布式事务产生的原因&lt;/h2&gt;

&lt;p&gt;从上面本地事务来看，我们可以看为两块，一个是service产生多个节点（微服务，将服务拆分成多个，然后按一定步骤调用，但是属于一个事务），另一个是resource（多个数据中心，分库分表）产生多个节点。&lt;/p&gt;

&lt;p&gt;举个互联网常用的交易业务为例&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/event1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图中包含了库存和订单两个独立的微服务，每个微服务维护了自己的数据库。&lt;/p&gt;

&lt;p&gt;在交易系统的业务逻辑中，一个商品在下单之前需要先调用库存服务，进行扣除库存，再调用订单服务，创建订单记录。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/event2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，如果多个数据库之间的数据更新没有保证事务，将会导致出现子系统数据不一致，业务出现问题。&lt;/p&gt;

&lt;h2 id=&#34;分布式事务的难点&#34;&gt;分布式事务的难点&lt;/h2&gt;

&lt;p&gt;事务的原子性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;事务操作跨不同节点，当多个节点某一节点操作失败时，需要保证多节点操作的要么什么都不做，要么做全套（All or Nothing）的原子性。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;事务的一致性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;当发生网络传输故障或者节点故障，节点间数据复制通道中断，在进行事务操作时需要保证数据一致性，保证事务的任何操作都不会使得数据违反数据库定义的约束、触发器等规则。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;事务的隔离性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;事务隔离性的本质就是如何正确处理多个并发事务的读写冲突和写写冲突，因为在分布式事务控制中，可能会出现提交不同步的现象，这个时候就有可能出现“部分已经提交”的事务。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时并发应用访问数据如果没有加以控制，有可能出现“脏读”问题。&lt;/p&gt;

&lt;p&gt;这些的最终影响是导致数据出现不一致，可见现在基本的ACID四大特性，已经无法满足我们分布式事务，所以引入了CAP定理，又被叫作布鲁尔定理。&lt;/p&gt;

&lt;h2 id=&#34;cap-base&#34;&gt;CAP&amp;amp;BASE&lt;/h2&gt;

&lt;p&gt;在分布式系统中，一致性(Consistency)、可用性(Availability)和分区容忍性(Partition Tolerance)3 个要素最多只能同时满足两个，不可兼得。其中，分区容忍性又是不可或缺的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C (一致性):分布式环境下，多个节点的数据是否强一致。&lt;/li&gt;
&lt;li&gt;A (可用性)：非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的，这里的正确指的是比如应该返回50，而不是返回40。&lt;/li&gt;
&lt;li&gt;P (分区容错性):当出现网络分区后，系统能够继续工作。打个比方，这里个集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正常工作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据时间证明cap不能共存。比舍弃其中一个，看使用场景取舍。比如Cassandra、Dynamo 等，默认优先选择 AP，弱化 C;HBase、MongoDB 等，默认优先选择 CP，弱化 A。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/cap&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;两阶段提交协议(简称2PC)是实现分布式事务较为经典的方案，但 2PC 的可扩展性很差，在分布式架构下应用代价较大,mysql的分布式集群就是使用这个分布式事务的解决方法。&lt;/p&gt;

&lt;p&gt;eBay 架构师 Dan Pritchett 提出了 BASE 理论，用于解决大规模分布式系统下的数据一致性问题。&lt;/p&gt;

&lt;p&gt;BASE 理论告诉我们：可以通过放弃系统在每个时刻的强一致性来换取系统的可扩展性。&lt;/p&gt;

&lt;p&gt;BASE 是 Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent (最终一致性)三个短语的缩写。是对CAP中AP的一个扩展&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基本可用:分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。&lt;/li&gt;
&lt;li&gt;软状态:允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是CAP中的不一致。&lt;/li&gt;
&lt;li&gt;最终一致:最终一致是指经过一段时间后，所有节点数据都将会达到一致。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;它的核心思想是即使无法做到强一致性（CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性。&lt;/p&gt;

&lt;p&gt;这里的关键词是“一定时间” 和 “最终”，“一定时间”和数据的特性是强关联的，不同业务不同数据能够容忍的不一致时间是不同的。&lt;/p&gt;

&lt;p&gt;例如支付类业务是要求秒级别内达到一致，因为用户时时关注；用户发的最新微博，可以容忍 30 分钟内达到一致的状态，因为用户短时间看不到明星发的微博是无感知的。&lt;/p&gt;

&lt;p&gt;而“最终”的含义就是不管多长时间，最终还是要达到一致性的状态。&lt;/p&gt;

&lt;p&gt;BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充：CAP 理论是忽略延时的，而实际应用中延时是无法避免的。&lt;/p&gt;

&lt;p&gt;这一点就意味着完美的 CP 场景是不存在的，即使是几毫秒的数据复制延迟，在这几毫秒时间间隔内，系统是不符合 CP 要求的。&lt;/p&gt;

&lt;p&gt;因此 CAP 中的 CP 方案，实际上也是实现了最终一致性，只是“一定时间”是指几毫秒而已。&lt;/p&gt;

&lt;p&gt;AP 方案中牺牲一致性只是指发生分区故障期间，而不是永远放弃一致性。&lt;/p&gt;

&lt;p&gt;这一点其实就是 BASE 理论延伸的地方，分区期间牺牲一致性，但分区故障恢复后，系统应该达到最终一致性。&lt;/p&gt;

&lt;h2 id=&#34;数据一致性模型&#34;&gt;数据一致性模型&lt;/h2&gt;

&lt;p&gt;前面介绍的 BASE 模型提过“强一致性”和“最终一致性”，下面对这些一致性模型展开介绍。&lt;/p&gt;

&lt;p&gt;分布式系统通过复制数据来提高系统的可靠性和容错性，并且将数据的不同的副本存放在不同的机器上，由于维护数据副本的一致性代价很高，因此许多系统采用弱一致性来提高性能。&lt;/p&gt;

&lt;p&gt;下面介绍常见的一致性模型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;强一致性：要求无论更新操作是在哪个数据副本上执行，之后所有的读操作都要能获得最新的数据。 对于单副本数据来说，读写操作是在同一数据上执行的，容易保证强一致性。对多副本数据来说，则需要使用分布式事务协议。&lt;/li&gt;
&lt;li&gt;弱一致性：在这种一致性下，用户读到某一操作对系统特定数据的更新需要一段时间，我们将这段时间称为&amp;rdquo;不一致性窗口&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;最终一致性：是弱一致性的一种特例，在这种一致性下系统保证用户最终能够读取到某操作对系统特定数据的更新（读取操作之前没有该数据的其他更新操作）。 &amp;ldquo;不一致性窗口&amp;rdquo;的大小依赖于交互延迟、系统的负载，以及数据的副本数等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;系统选择哪种一致性模型取决于应用对一致性的需求，所选取的一致性模型还会影响到系统如何处理用户的请求以及对副本维护技术的选择等。&lt;/p&gt;

&lt;p&gt;后面将基于上面介绍的一致性模型分别介绍分布式事务的解决方案。&lt;/p&gt;

&lt;h1 id=&#34;分布式事务解决方案&#34;&gt;分布式事务解决方案&lt;/h1&gt;

&lt;p&gt;有了上面的理论基础后，这里介绍开始介绍几种常见的分布式事务的解决方案。&lt;/p&gt;

&lt;h2 id=&#34;两阶段提交-2pc-two-phase-commit-方案-强一致性-还有3pc&#34;&gt;两阶段提交（2PC, Two Phase Commit）方案（强一致性），还有3PC&lt;/h2&gt;

&lt;p&gt;mysql使用的这种模式。核心就是通过事务协调器将多个事务合并为一个大的事务。&lt;/p&gt;

&lt;p&gt;基于XA协议的两阶段提交:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一阶段是表决阶段，所有参与者都将本事务能否成功的信息反馈发给协调者；&lt;/li&gt;
&lt;li&gt;第二阶段是执行阶段，协调者根据所有参与者的反馈，通知所有参与者，步调一致地在所有分支上提交或者回滚;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/2pc&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod11&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod12&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;单点问题：事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。&lt;/li&gt;
&lt;li&gt;同步阻塞：在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。&lt;/li&gt;
&lt;li&gt;数据不一致：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能。比如：在第二阶段中，假设协调者发出了事务 Commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 Commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总的来说，XA 协议比较简单，成本较低，但是其单点问题，以及不能支持高并发(由于同步阻塞)依然是其最大的弱点。&lt;/p&gt;

&lt;h2 id=&#34;基于可靠消息的最终一致性方案&#34;&gt;基于可靠消息的最终一致性方案&lt;/h2&gt;

&lt;p&gt;本地消息表的方案最初是由 eBay 提出，核心思路是将分布式事务拆分成本地事务进行处理。通过可靠消息系统进行同步，可以是各种消息中间件，也可以是MQ等。&lt;/p&gt;

&lt;p&gt;业务处理服务在业务事务提交之前，向实时消息服务请求发送消息，实时消息服务只记录消息数据，而不是真正的发送。业务处理服务在业务事务提交之后，向实时消息服务确认发送。只有在得到确认发送指令后，实时消息服务才会真正发送。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;方案通过在事务主动发起方额外新建事务消息表，事务发起方处理业务和记录事务消息在本地事务中完成，轮询事务消息表的数据发送事务消息，事务被动方基于消息中间件消费事务消息表中的事务。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面把分布式事务最先开始处理的事务方称为事务主动方，在事务主动方之后处理的业务内的其他事务称为事务被动方。&lt;/p&gt;

&lt;p&gt;为了方便理解，下面继续以电商下单为例进行方案解析，这里把整个过程简单分为扣减库存，订单创建 2 个步骤。&lt;/p&gt;

&lt;p&gt;库存服务和订单服务分别在不同的服务器节点上，其中库存服务是事务主动方，订单服务是事务被动方。&lt;/p&gt;

&lt;p&gt;事务的主动方需要额外新建事务消息表，用于记录分布式事务的消息的发生、处理状态。&lt;/p&gt;

&lt;p&gt;整个业务处理流程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod10&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;步骤1：事务主动方处理本地事务。&lt;/p&gt;

&lt;p&gt;事务主动方在本地事务中处理业务更新操作和写消息表操作。上面例子中库存服务阶段在本地事务中完成扣减库存和写消息表(图中 1、2)。&lt;/p&gt;

&lt;p&gt;步骤 2：事务主动方通过消息中间件，通知事务被动方处理事务通知事务待消息。
消息中间件可以基于 Kafka、RocketMQ 消息队列，事务主动方主动写消息到消息队列，事务消费方消费并处理消息队列中的消息。&lt;/p&gt;

&lt;p&gt;上面例子中，库存服务把事务待处理消息写到消息中间件，订单服务消费消息中间件的消息，完成新增订单（图中 3 - 5）。&lt;/p&gt;

&lt;p&gt;步骤 3：事务被动方通过消息中间件，通知事务主动方事务已处理的消息。
上面例子中，订单服务把事务已处理消息写到消息中间件，库存服务消费中间件的消息，并将事务消息的状态更新为已完成(图中 6 - 8)。&lt;/p&gt;

&lt;p&gt;为了数据的一致性，当处理错误需要重试，事务发送方和事务接收方相关业务处理需要支持幂等。&lt;/p&gt;

&lt;p&gt;具体保存一致性的容错处理如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当步骤 1 处理出错，事务回滚，相当于什么都没发生。&lt;/li&gt;
&lt;li&gt;当步骤 2、步骤 3 处理出错，由于未处理的事务消息还是保存在事务发送方，事务发送方可以定时轮询为超时消息数据，再次发送到消息中间件进行处理。事务被动方消费事务消息重试处理。&lt;/li&gt;
&lt;li&gt;如果是业务上的失败，事务被动方可以发消息给事务主动方进行回滚。&lt;/li&gt;
&lt;li&gt;如果多个事务被动方已经消费消息，事务主动方需要回滚事务时需要通知事务被动方回滚。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tcc事务补偿型方案-原子性-最终一致性&#34;&gt;TCC事务补偿型方案（原子性，最终一致性）&lt;/h2&gt;

&lt;p&gt;某业务模型如图，由服务 A、服务B、服务C、服务D 共同组成的一个微服务架构系统。服务A 需要依次调用服务B、服务C 和服务D 共同完成一个操作。当服务A 调用服务D 失败时，若要保证整个系统数据的一致性，就要对服务B 和服务C 的invoke 操作进行回滚，执行反向的revert 操作。回滚成功后，整个微服务系统是数据一致的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod22&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实现：一个完整的业务活动由一个主业务服务于若干的从业务服务组成。主业务服务负责发起并完成整个业务活动。从业务服务提供TCC型业务操作。业务活动管理器控制业务活动的一致性，它登记业务活动的操作，并在业务活动提交时确认所有的TCC型操作的Confirm操作，在业务活动取消时调用所有TCC型操作的Cancel操作。&lt;/li&gt;
&lt;li&gt;成本：实现TCC操作的成本较高，业务活动结束的时候Confirm和Cancel操作的执行成本。业务活动的日志成本。TCC 的 Try、Confirm 和 Cancel 操作功能要按具体业务来实现，业务耦合度较高，提高了开发成本。&lt;/li&gt;
&lt;li&gt;使用范围：强隔离性，严格一致性要求的业务活动。适用于执行时间较短的业务，比如处理账户或者收费等等。&lt;/li&gt;
&lt;li&gt;特点：不与具体的服务框架耦合，位于业务服务层，而不是资源层，可以灵活的选择业务资源的锁定粒度。TCC里对每个服务资源操作的是本地事务，数据被锁住的时间短，可扩展性好，可以说是为独立部署的SOA服务而设计的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;TCC方案很少用人使用，因为事务回滚操作实际上是严重依赖于手动编写代码来进行回滚和补偿操作，这样的话就会造成补偿代码过多，使得项目非常难以维护。比较适合的场景就是除非真的一致性要求非常高，是系统中的核心业务场景，例如常见的就是资金类的场景，那可以用TCC方案。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;为了方便理解，下面以电商下单为例进行方案解析，这里把整个过程简单分为扣减库存，订单创建 2 个步骤，库存服务和订单服务分别在不同的服务器节点上。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Try 阶段&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从执行阶段来看，与传统事务机制中业务逻辑相同。但从业务角度来看，却不一样。&lt;/p&gt;

&lt;p&gt;TCC 机制中的 Try 仅是一个初步操作，它和后续的确认一起才能真正构成一个完整的业务逻辑，这个阶段主要完成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;完成所有业务检查( 一致性 ) 。&lt;/li&gt;
&lt;li&gt;预留必须业务资源( 准隔离性 ) 。&lt;/li&gt;
&lt;li&gt;Try 尝试执行业务。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TCC 事务机制以初步操作（Try）为中心的，确认操作（Confirm）和取消操作（Cancel）都是围绕初步操作（Try）而展开。&lt;/p&gt;

&lt;p&gt;因此，Try 阶段中的操作，其保障性是最好的，即使失败，仍然有取消操作（Cancel）可以将其执行结果撤销。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod23&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;假设商品库存为 100，购买数量为 2，这里检查和更新库存的同时，冻结用户购买数量的库存，同时创建订单，订单状态为待确认。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Confirm / Cancel 阶段&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;根据 Try 阶段服务是否全部正常执行，继续执行确认操作（Confirm）或取消操作（Cancel）。&lt;/p&gt;

&lt;p&gt;Confirm 和 Cancel 操作满足幂等性，如果 Confirm 或 Cancel 操作执行失败，将会不断重试直到执行完成。&lt;/p&gt;

&lt;p&gt;Confirm：当 Try 阶段服务全部正常执行， 执行确认业务逻辑操作&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod24&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里使用的资源一定是 Try 阶段预留的业务资源。在 TCC 事务机制中认为，如果在 Try 阶段能正常的预留资源，那 Confirm 一定能完整正确的提交。&lt;/p&gt;

&lt;p&gt;Confirm 阶段也可以看成是对 Try 阶段的一个补充，Try+Confirm 一起组成了一个完整的业务逻辑。&lt;/p&gt;

&lt;p&gt;Cancel：当 Try 阶段存在服务执行失败， 进入 Cancel 阶段&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod25&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cancel 取消执行，释放 Try 阶段预留的业务资源，上面的例子中，Cancel 操作会把冻结的库存释放，并更新订单状态为取消。&lt;/p&gt;

&lt;h2 id=&#34;最大努力通知型-弱一致性&#34;&gt;最大努力通知型（弱一致性）&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;实现：业务活动的主动方在完成处理之后向业务活动的被动方发送消息，允许消息丢失。业务活动的被动方根据定时策略，向业务活动的主动方查询，恢复丢失的业务消息。
约束：被动方的处理结果不影响主动方的处理结果。
成本：业务查询与校对系统的建设成本。
使用范围：对业务最终一致性的时间敏感度低。跨企业的业务活动。
特点：业务活动的主动方在完成业务处理之后，向业务活动的被动方发送通知消息。主动方可以设置时间阶梯通知规则，在通知失败后按规则重复通知，知道通知N次后不再通知。主动方提供校对查询接口给被动方按需校对查询，用户恢复丢失的业务消息。
适用范围：银行通知，商户通知。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最大努力通知方案的主要思路如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;系统A本地事务执行完毕之后，发送个消息到消息服务MQ&lt;/li&gt;
&lt;li&gt;这里会有个专门消费消息服务MQ的最大努力通知服务，该服务会消费消息服务MQ，然后写入数据库中记录下来或者是放入个内存队列，接下来调用系统B的接口&lt;/li&gt;
&lt;li&gt;要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是失败就放弃。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前国内互联网公司大都是这么玩儿的，要不你使用RocketMQ支持的，要不你就基于其他MQ中间件自己封装一套类似的逻辑，总之思路就是这样的。其实就是MQ是上面可靠消息的一种，这边只是强调不一定可靠，还有通知。&lt;/p&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;从应用角度看，分布式事务的现实场景常常无法规避，在有能力给出其他解决方案前，2PC也是一个不错的选择。&lt;/p&gt;

&lt;p&gt;对购物转账等电商和金融业务，中间件层的2PC最大问题在于业务不可见，一旦出现不可抗力或意想不到的一致性破坏，如数据节点永久性宕机，业务难以根据2PC的日志进行补偿。金融场景下，数据一致性是命根，业务需要对数据有百分之百的掌控力，建议使用TCC这类分布式事务模型，或基于消息队列的柔性事务框架，这两种方案都在业务层实现，业务开发者具有足够掌控力，可以结合SOA框架来架构，包括Dubbo、Spring Cloud等&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 架构的演进</title>
          <link>https://kingjcy.github.io/post/architecture/architecture-evolution/</link>
          <pubDate>Thu, 05 Mar 2020 19:11:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/architecture-evolution/</guid>
          <description>&lt;p&gt;随着计算机软件的发展，不同的应用在落地，应用架构随着规模的越来越大，也在一步步的进行演进，从最初的单体架构，到后来的集群，然后分布式架构一步步的发展着。&lt;/p&gt;

&lt;h1 id=&#34;架构演变&#34;&gt;架构演变&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/architecture-change&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;整个过程是重单体架构向微服务发展的过程，下面我们来看看每一代架构的发展。&lt;/p&gt;

&lt;h2 id=&#34;单体架构&#34;&gt;单体架构&lt;/h2&gt;

&lt;p&gt;以前使用 Laravel 做 web 项目时，是根据 MVC 去划分目录结构的，即 Controller 层处理业务逻辑，Model 层处理数据库的 CURD，View 层处理数据渲染与页面交互。以及 MVP、MVVM 都是将整个项目的代码是集中在一个代码库中，进行业务处理。这种单一聚合代码的方式在前期实现业务的速度很快，但在后期会暴露很多问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、开发与维护困难：随着业务复杂度的增加，代码的耦合度往往会变高，多个模块相互耦合后不易横向扩展。
2、效率和可靠性低：过大的代码量将降低响应速度，应用潜在的安全问题也会累积。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;单体架构最终走向了单体地狱，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、代码全量编译成一个实例，因为一个模块的问题可能会导致全量的模块不能使用
2、问题难以地位，比如有一个死循环，会导致cpu占用100%，还会影响机器上其他的应用的使用，同时，很难知道是哪个模块出问题
3、代码量大，可能编译启动就需要很长时间，到最后就不可维护了，而且线上出问题就没办法处理了，因为重启都需要很长的时间
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个时候我们使用的组织架构就是最简单的&lt;strong&gt;前台-后台&lt;/strong&gt;在一个实例进程中。&lt;/p&gt;

&lt;h3 id=&#34;垂直拆分的垂直架构&#34;&gt;垂直拆分的垂直架构&lt;/h3&gt;

&lt;p&gt;单体架构的瓶颈太严重了，变的越来越臃肿，每次开发业务，都要将全部功能进行回归，于是就将前后端进行拆分成不同的进程，通过报文进行相互调用，在一定程度上实现了解耦，但是其实并没有很强的水平扩展的能力，所以还算是单体架构。&lt;/p&gt;

&lt;p&gt;在这个时候我们使用的组织架构还是最简单的架构&lt;strong&gt;前台-后台&lt;/strong&gt;,但是前台后台已经运行在不同的实例中，实现了一定的解耦，由最初的mvc的api的调用，变成了http的调用模式。&lt;/p&gt;

&lt;p&gt;但是随着规模越来越大，每个模块很快各种瓶颈有出现了，需要进行各种水平扩展，这个时候就需要进行服务化，使用分布式系统了。&lt;/p&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;架构经过了垂直拆分，已具有一定的解耦性，但是随着规模的扩展，每个流程都出现了单机处理的瓶颈，需要进行水平扩展，这时候就需要集群来并行共同处理业务来提高处理的能力。&lt;/p&gt;

&lt;p&gt;这个时候我们使用的组织架构还是最简单的架构&lt;strong&gt;前台-后台&lt;/strong&gt;，但是后台已经有了进一步的垂直拆分和水平扩展，随着拆分扩展的越来越多，也就有了&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/microservices/&#34;&gt;微服务的概念，下一代架构的诞生&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;第一代微服务-服务化&#34;&gt;第一代微服务&amp;mdash;服务化&lt;/h2&gt;

&lt;p&gt;现在传统的mvc架构带来的单体问题已经十分严重了，基本上已经不再使用了，提倡微服务的思想。&lt;/p&gt;

&lt;p&gt;第一代微服务架构是建立在服务化的基础之上的，什么是服务化？核心就是不同服务之间的通信，一种以服务为核心的解决方案。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务注册&lt;/li&gt;
&lt;li&gt;服务发布&lt;/li&gt;
&lt;li&gt;服务调用&lt;/li&gt;
&lt;li&gt;服务监控&lt;/li&gt;
&lt;li&gt;负载均衡&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实简单的说就是将一些服务封装起来，对外提供tcp/http/rpc接口，不需要关心内部的细节，内部有专门的人去维护，就是对这个服务的服务化，这个封装可以是新增一个服务层，也可以是在这个服务的基础提供接口等。&lt;/p&gt;

&lt;p&gt;其中最著名的就是SOA的面向服务架构思想。&lt;/p&gt;

&lt;h3 id=&#34;soa&#34;&gt;SOA&lt;/h3&gt;

&lt;p&gt;SOA是什么？SOA全英文是Service-Oriented Architecture，中文意思是面向服务架构，是一种思想，一种方法论，一种分布式的服务架构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/20180305.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实就是每个服务独立治理，相互解耦，服务共享不在重复，通过注册中心来服务发现调用的散装架构。我们最常见的架构就是&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Spring Cloud为开发者提供了快速构建分布式系统的通用模型的工具（包括配置管理、服务发现、熔断器、智能路由、微代理、控制总线、一次性令牌、全局锁、领导选举、分布式会话、集群状态等）&lt;/li&gt;
&lt;li&gt;Dubbo是一个阿里巴巴开源出来的一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案，其实主要解决的服务间通信的问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在这个时候组织架构就发生了很大的变化，基本都是要加上一层网关或者企业总线，也就是&lt;strong&gt;前台-网关（ESB）-后台&lt;/strong&gt;的结构。&lt;/p&gt;

&lt;h2 id=&#34;下一代微服务-去中心化&#34;&gt;下一代微服务&amp;ndash;去中心化&lt;/h2&gt;

&lt;h3 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h3&gt;

&lt;p&gt;随着注册中心的出现，任何调用都走网关，网关的瓶颈有到来，于是出现了下一代的微服务架构service mesh，主要落地的项目就是istio，当然还有其他的一些项目，主要是去中心化的设计。&lt;/p&gt;

&lt;p&gt;目前流行的 Service Mesh 开源软件有 Linkerd、Envoy 和 Istio，而最近 Buoyant（开源 Linkerd 的公司）又发布了基于 Kubernetes 的 Service Mesh 开源项目 Conduit，来看一下Service Mesh 开源项目简介：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linkerd（&lt;a href=&#34;https://github.com/linkerd/linkerd）：第一代&#34;&gt;https://github.com/linkerd/linkerd）：第一代&lt;/a&gt; Service Mesh，2016 年 1 月 15 日首发布，业界第一个 Service Mesh 项目，由 Buoyant 创业小公司开发（前 Twitter 工程师），2017 年 7 月 11 日，宣布和 Istio 集成，成为 Istio 的数据面板。&lt;/li&gt;
&lt;li&gt;Envoy（&lt;a href=&#34;https://github.com/envoyproxy/envoy）：第一代&#34;&gt;https://github.com/envoyproxy/envoy）：第一代&lt;/a&gt; Service Mesh，2016 年 9 月 13 日首发布，由 Matt Klein 个人开发（Lyft 工程师），之后默默发展，版本较稳定。&lt;/li&gt;
&lt;li&gt;Istio（&lt;a href=&#34;https://github.com/istio/istio）：第二代&#34;&gt;https://github.com/istio/istio）：第二代&lt;/a&gt; Service Mesh，2017 年 5 月 24 日首发布，由 Google、IBM 和 Lyft 联合开发，只支持 Kubernetes 平台，2017 年 11 月 30 日发布 0.3 版本，开始支持非 Kubernetes 平台，之后稳定的开发和发布。这个在我们k8s基础平台中&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-istio/&#34;&gt;最常使用&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;Conduit（&lt;a href=&#34;https://github.com/runconduit/conduit）：第二代&#34;&gt;https://github.com/runconduit/conduit）：第二代&lt;/a&gt; Service Mesh，2017 年 12 月 5 日首发布，由 Buoyant 公司开发（借鉴 Istio 整体架构，部分进行了优化），对抗 Istio 压力山大，也期待 Buoyant 公司的毅力。&lt;/li&gt;
&lt;li&gt;nginMesh（&lt;a href=&#34;https://github.com/nginmesh/nginmesh）：2017&#34;&gt;https://github.com/nginmesh/nginmesh）：2017&lt;/a&gt; 年 9 月首发布，由 Nginx 开发，定位是作为 Istio 的服务代理，也就是替代 Envoy，思路跟 Linkerd 之前和 Istio 集成很相似，极度低调，GitHub 上的 star 也只有不到 100。&lt;/li&gt;
&lt;li&gt;Kong（&lt;a href=&#34;https://github.com/Kong/kong）：比&#34;&gt;https://github.com/Kong/kong）：比&lt;/a&gt; nginMesh 更加低调，默默发展中。&lt;/li&gt;
&lt;li&gt;go-micro是基于Go语言实现的插件化RPC微服务框架，与go-kit，kite等微服务框架相比，它具有易上手、部署简单、工具插件化等优点。go-micro框架提供了服务发现、负载均衡、同步传输、异步通信以及事件驱动等机制，它尝试去简化分布式系统间的通信，让我们可以专注于自身业务逻辑的开发。所以对于新手而言，go-micro是个不错的微服务实践的开始。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在云原生模型里，一个应用可以由数百个服务组成，每个服务可能有数千个实例，而每个实例可能会持续地发生变化。这种情况下，服务间通信不仅异常复杂，而且也是运行时行为的基础。管理好服务间通信对于保证端到端的性能和可靠性来说是无疑是非常重要的。种种复杂局面便催生了服务间通信层的出现，这个层既不会与应用程序的代码耦合，又能捕捉到底层环境高度动态的特点，让业务开发者只关注自己的业务代码，并将应用云化后带来的诸多问题以不侵入业务代码的方式提供给开发者。&lt;/p&gt;

&lt;p&gt;这个服务间通信层就是 Service Mesh，它可以提供安全、快速、可靠的服务间通讯（service-to-service）。&lt;/p&gt;

&lt;p&gt;Service Mesh，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给 Service Mesh 就可以了。&lt;/p&gt;

&lt;h1 id=&#34;电商系统的演进&#34;&gt;电商系统的演进&lt;/h1&gt;

&lt;p&gt;我们结合上面的架构演进，通过我们最常见的电商系统来看单体到分布式的变化：&lt;/p&gt;

&lt;h2 id=&#34;单体&#34;&gt;单体&lt;/h2&gt;

&lt;p&gt;当我们的项目比较小时，我们只有一个系统，并且把他们写到一起，放在一个服务器上，但是随着平台越来越大，数据量越来越大，我们不得不通过分库，把多个模块的数据库分别放在对应得服务器上，每个模块调用自己的子系统即可。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/ac1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;集群-1&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;当后台单台没有办法承受的时候，我们需要堆机器来水平扩展集群来处理问题，但是一连串的流程都是在一个实例中进行。&lt;/p&gt;

&lt;h2 id=&#34;esb&#34;&gt;ESB&lt;/h2&gt;

&lt;p&gt;随着我们系统的进一步复杂度的提示，在集群的基础上，我们不得不进一步对系统的性能进行提升，我们将多个模块分成多个子系统，多个子系统直接互相调用（因为SOA一般用于大型项目，比较复杂，所以一般总系统不会再集成，会拆分多个，分别做成服务，相互调用）。当我们的电商UI进行一个下订单的任务时，多个服务直接互相调用，系统通过数据总线(java的ESB，其实也就是我们所说的网关)，分别调用对于的子系统即可。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/ac2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;企业数据总线：企业数据总线不是对多个子模块的集成，他在这里充当数据通道的作用，数据总线不关心业务，数据总线根据给的地址和协议去调服务，上端不关心服务在哪里是什么，只找数据总线。&lt;/p&gt;

&lt;p&gt;数据总线是起到调度服务的作用，数据总线不是集成服务，数据总线更新一个调度框架，每个服务需要根据约定向数据总线注册服务，那么如何注册那？其实数据总线就像一个字典结构，&lt;/p&gt;

&lt;p&gt;数据总线里面一个key对于一个value，key指的是服务名，value则是服务的调度方式，还有一点需要说明的是，数据总线只是指路人，服务是不经过数据总线的，如上图的黄色线的路径。&lt;/p&gt;

&lt;p&gt;数据总线通过域名解析实现:一个域名绑定多台服务器，ajax也可以，dns也可以，解析域名嘛。&lt;/p&gt;

&lt;p&gt;其实数据总线还有一些高级应用，比如心跳检测，实现负载均衡等等，就不细说了，目前应用数据总线的有阿里的dubbo,还有zookeeper。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;这个时候组织架构加上了企业总线&lt;strong&gt;前台&amp;mdash;ESB&amp;mdash;后台&lt;/strong&gt;的结构。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;随着企业发展到一定程度，需要规范化管理的时候，我们上了ERP系统，要通过供应链建设B2C业务的时候，我们上了SRM系统，仓储管理到一定规模我们上WMS，物流管理我们上TMS……整个公司各个系统功能有重叠，有交叉，内部协同成了重大问题。&lt;/p&gt;

&lt;p&gt;这就好像我们一直在打是局部信息化战争，头痛医头脚痛医脚，需要解决什么样的问题，就上什么样的系统，最终就形成了所谓烟囱式的系统架构。&lt;/p&gt;

&lt;p&gt;导致整个架构重复造轮子，跨系统管理也给运营人员造成了不必要的时间精力浪费。整个系统管理冗杂又造成资源浪费，这时候就需要将原有的系统规范化、一体化，通过数据总线进去进行深度的整合，打通各个信息孤岛，形成前后贯通的信息化建设。&lt;/p&gt;

&lt;p&gt;我们把这个时代称之为ESB数据孤岛时代。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/electroniccommerce/esb.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;大部分情况下基本上都是够用，虽然ESB很重，核心很难改变，不够灵活，但是随着发展，出现了更加灵活的方式。&lt;/p&gt;

&lt;h2 id=&#34;大中台&#34;&gt;大中台&lt;/h2&gt;

&lt;p&gt;其实中台严格意义上来说，不是一种架构，也不是一种系统，而是一种战略。即可以使用第一代微服务架构来构建，也可以使用第二代微服务架构来构建，目前各大企业也正在容器化推进的过程中&lt;/p&gt;

&lt;p&gt;中台的核心价值是在于，在对企业业务有了柔性支撑和贯通的前提下，再形成协同与智慧的运营体系。&lt;/p&gt;

&lt;p&gt;一般企业架构分成了三个层次：前台、中台、后台。中台又分成三大块，业务中台、数据中台和技术中台。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/electroniccommerce/middle&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;技术中台支撑企业业务发展，通过打通企业内异构系统，支持业务中台；

业务中台围绕公司业务运营进行服务，将获取的多维度数据传递给数据中台，由数据中台分析反馈给业务中台，以优化业务运营。同时数据中台通过BI智能分析，帮助企业管理者更好的做决策分析。三者是相辅相成，相互协作的。
业务中台其实就是把原有的前端的会员中心、营销中心、商品中心，后端的供应链中心、采配中心等重点模块放在业务中台模块，以后前端不管对接多少个第三方，线上线下增加多少家门店，都能进行统一会员、统一商品编码、统一供应链整合，整个系统一体化。真正做到用技术支持业务，通过业务收集大量数据进行决策，统一高效的进行管理。

数据中台：一般指以数据采集、数据集成、数据治理，指标体系和数据仓库统一建设等数据管理活动为典型特征的中台。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当前最需要建设的中台有两种：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;狭义的业务中台：一般指在线业务为典型特征的中台。在OLDI（Online Data-Intensive）时代，越来越多的企业的核心业务都是在线业务，因此把在线业务中台简称为业务中台。
数据中台：一般指以数据采集、数据集成、数据治理，指标体系和数据仓库统一建设等数据管理活动为典型特征的中台。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对业务中台来说，比较符合的场景主要有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;业务系统研发团队至少大几十人（含外包的），需求多变化快，系统又涉及多个领域（比如做ERP、电商的），业务逻辑比较复杂。
这时业务中台可以把系统和业务领域划分清楚，提高研发效率。做相似行业的外包项目为主，业务规模也做的比较大的（一年有两位数的项目）。
这时业务中台可以提升软件复用，降低定制化成本，提高研发效率。如果每个项目都完全不一样，那中台也救不了你。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;支持业务中台的技术体系，包括微服务、DevOps、云原生和分布式事务等。&lt;/p&gt;

&lt;p&gt;将需求设计成微服务架构，然后每个服务使用各种技术栈来开发业务，比如golang的技术栈的高并发的特性来开发web服务等，然后将一些统一的模块进行统一的接入和输出，使用devops的开发模式，在业务中还是需要解决分布式事务等问题。&lt;/p&gt;

&lt;p&gt;比如在网易，是网易轻舟微服务平台，提供微服务应用全生命周期的完整支持，包括下一代微服务Service Mesh支持、经典微服务框架NSF、包括CI/CD的DevOps、分布式事务框架GXTS、APM、API网关、GoAPI全自动化测试以及容器应用管理服务等。&lt;/p&gt;

&lt;p&gt;对数据中台来说，比较符合的场景：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数据产品比较多，每天要看数据如果没数据就不知道怎么工作的运营人员比较多的业务。
比如电商就是典型。尤其是数据产品和运营人员还在多个团队。
用数据的姿势比较复杂，问题比较多，比如经常出现指标不一致、数据出错、想要的数据不知道哪里有等问题。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;支持数据中台的技术体系，包括指标管理、数据服务、元数据管理、数仓开发与管理、数据安全管理、数据资产管理、大数据计算引擎、数据集成/同步/交换引擎等，&lt;/p&gt;

&lt;p&gt;其实数据中台就是将数据进行处理，不同数据资源，统一的输出标准，中间用到大部分就是数据引擎，比如kafka队列，sprak，flink等流式引擎，hadoop，hbase和hive等大数据引擎。&lt;/p&gt;

&lt;p&gt;比如在网易，是以网易猛犸为核心的网易全链路数据中台解决方案。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算容器系列---- Docker</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/docker/docker/</link>
          <pubDate>Fri, 14 Feb 2020 16:08:19 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/docker/docker/</guid>
          <description>&lt;p&gt;docker是一种虚拟化的技术，相对于虚拟机更加轻量级。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/docker.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;虚拟化技术&#34;&gt;虚拟化技术&lt;/h1&gt;

&lt;p&gt;虚拟化是一种资源管理技术，将计算机的各种实体资源比如：cpu、内存、网络、存储等进行抽象，转换呈现出来，来解决实体结构间不可切割的问题，使用户对资源重新组装来提高资源使用率。它的目标通常是为了在一台主机上运行多个系统或应用，从而提高系统资源利用率，降低成本，方便管理和容灾备份等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/docker1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基于硬件的虚拟化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;传统虚拟化技术是在硬件资源级别的虚拟化，需要有虚拟机管理程序（Hypervisor）和虚拟机操作系统（vm），进行的是操作系统级别的隔离。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基于软件的虚拟化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Docker直接建立在操作系统上虚拟化，直接复用本地操作系统，更加轻量级，通过操作系统的cgroup和namespace进行进程级别的隔离。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/docker2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为什么是docker&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;docker成功之处就在于它推出了镜像打包技术，支持了标准化的交付，搞容器虚拟化技术的很多，而虚拟化技术概念更是在50年前就出现了，但是最后docker出现一统天下，是因为它推出了镜像打包技术，使得容器在部署，扩缩容，删除都变得简单，轻量级，是整个云平台的基础。&lt;/p&gt;

&lt;p&gt;相继推动了devops的发展，devops核心是敏捷开发，标准交付，快速部署。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;docker优势&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1、敏捷开发、持续集成，标准交付
    1、开发、测试和生产的环境一致性。也就是环境标准化和版本控制，通过镜像技术将应用与应用所依赖的运行环境打包，进行版本化管理，一旦故障可以快速回滚，相对于虚拟机镜像，docker压缩和备份速度更快，镜像启动如启动一个进程一样
    2、松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，能够快速迭代
    3、镜像仓库，方便管理。

2、快速部署
    1、云和操作系统分发的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、Google Kubernetes Engine 和其他任何地方运行，也就是跨平台跨环境。
    2、轻量级

3、提高资源利用率
    1、资源隔离：可预测的应用程序性能。
    2、资源利用：高效率和高密度。
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;docker缺点&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;1、安全问题
2、大规模情况下的管理问题：部署，升级回滚，扩缩容，自愈等。
3、网络问题：服务发现和负载均衡
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些正是&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-tutorial/&#34;&gt;k8s&lt;/a&gt;解决的问题。&lt;/p&gt;

&lt;h1 id=&#34;docker安装&#34;&gt;docker安装&lt;/h1&gt;

&lt;p&gt;在安装之前需要确保linux的内核版本在3.10.X以上，没有话需要升级内核，我在我的虚拟机的上升级内核了可以参考&lt;a href=&#34;https://kingjcy.github.io/post/linux/centos/centos-kernel-update/&#34;&gt;centos内核升级&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;docker是以linux容器化技术(LXC)为基础的，所以在linux上体验最好，通常使用root用户安装&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;yum&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;初始&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1、docker放在epel镜像源中，所以需要先添加镜像源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install -y http://mirrors.yun-idc.com/epel/6/x86_64/epel-release-6-8.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;加载完需要修改源文件，将baseurl放开，将mirrorlist注释掉才能链接上。&lt;/p&gt;

&lt;p&gt;2、docker与一个系统自带的程序重名，所以修改为docker-io&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install -y docker-io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service docker start  或者docker -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;加入开机启动项：chkconfig docker on&lt;/p&gt;

&lt;p&gt;过程中可能有一个device-mapper-libs需要升级，这边用yum源直接升级 yum upgrade device-mapper-libs没有用，需要现在最新的安装包进行安装&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2017.2.13&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在centos 7上使用无法链接到对应到镜像，重官网上获取新到docker源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum-config-manager --add-repo https://docs.docker.com/engine/installation/linux/repo_files/centos/docker.repo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用yum makecache加载后重新安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install -y docker-engine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在次过程中，有一个组件需要升级，rpm包需要手动下载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm -e selinux-policy-targeted-3.13.1-60.el7_2.9.noarch
rpm -ivh selinux-policy-targeted-3.13.1-190.fc24.noarch.rpm --nodeps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systmctl start docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此docker就安装好了，启动docker服务就可以对docker进行操作了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2017.3.21&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;docker重3.1之后开始改变命名，分为ce和ee企业版。这个都是在官方网站上的产品，在github上都迁至moby项目下，这边安装更新&lt;/p&gt;

&lt;p&gt;OS requirements&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;To install Docker, you need the 64-bit version of CentOS 7.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Uninstall old versions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum remove docker \
                docker-common \
                container-selinux \
                docker-selinux \
                docker-engine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install yum-utils, which provides the yum-config-manager utility:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo yum install -y yum-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;add repo并且生效加载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo yum-config-manager \
    --add-repo \
        https://download.docker.com/linux/centos/docker-ce.repo

$ sudo yum-config-manager --disable docker-ce-edge

$ sudo yum makecache fast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;install&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install docker-ce

$ sudo systemctl start docker
$ sudo docker run hello-world
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;rpm&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;新版的也有对应的rpm可以安装。&lt;/p&gt;

&lt;p&gt;所以可以重官网下载rpm来安装，我们通过官网提供的yum源来下载rpm包，然后安装&lt;/p&gt;

&lt;p&gt;用于离线安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://download.docker.com/linux/centos/7/x86_64/stable/Packages/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这个地址下载对应的rpm包&lt;/p&gt;

&lt;p&gt;安装需要的依赖包也要下载&lt;/p&gt;

&lt;p&gt;在 &lt;a href=&#34;http://mirrors.163.com/centos/7/os/x86_64/Packages/&#34;&gt;http://mirrors.163.com/centos/7/os/x86_64/Packages/&lt;/a&gt; 下载8个依赖，正常就是centos自身所带的包，如果机器解决了centos的包就不用下载了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;audit-libs-python-2.7.6-3.el7.x86_64.rpm

checkpolicy-2.5-4.el7.x86_64.rpm

libcgroup-0.41-13.el7.x86_64.rpm

libseccomp-2.3.1-3.el7.x86_64.rpm

libsemanage-python-2.5-8.el7.x86_64.rpm

policycoreutils-python-2.5-17.1.el7.x86_64.rpm

python-IPy-0.75-6.el7.noarch.rpm

setools-libs-3.3.8-1.1.el7.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是必然需要下载的&lt;/p&gt;

&lt;p&gt;1、在 &lt;a href=&#34;http://rpm.pbone.net/index.php3?stat=3&amp;amp;limit=1&amp;amp;srodzaj=1&amp;amp;dl=40&amp;amp;search=container-selinux&amp;amp;field[]=1&amp;amp;field[]=2&#34;&gt;http://rpm.pbone.net/index.php3?stat=3&amp;amp;limit=1&amp;amp;srodzaj=1&amp;amp;dl=40&amp;amp;search=container-selinux&amp;amp;field[]=1&amp;amp;field[]=2&lt;/a&gt; 下载container-selinux-2.9-4.el7.noarch.rpm&lt;/p&gt;

&lt;p&gt;包都可以在这个 &lt;a href=&#34;http://rpm.pbone.net/index.php3&#34;&gt;http://rpm.pbone.net/index.php3&lt;/a&gt; 来下载&lt;/p&gt;

&lt;p&gt;2、pigz-2.3.4-1.el7.x86_64.rpm&lt;/p&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl start docker
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;docker使用&#34;&gt;docker使用&lt;/h1&gt;

&lt;p&gt;docker容器三大核心：镜像，容器和仓库&lt;/p&gt;

&lt;p&gt;镜像是docker引擎只读的一块模版，包含文件系统，容器是基于镜像创建的一个实例，在镜像上加了一个可写层，实现了对镜像对起停等各种操作。&lt;/p&gt;

&lt;h2 id=&#34;镜像&#34;&gt;镜像&lt;/h2&gt;

&lt;p&gt;镜像：是docker的基础，包含app所需要的lib库以及app应用。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;下载：docker pull name[:tag]  不指定tag默认取最新版本laster。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建容器：docker run -t -i 镜像版本 /bin/bash（可执行文件）   启动一个bash终端，-t表示一个伪终端并绑定在容器的标准输入中，-i则让容器标准输入保持打开  守护态运行 -d  -p  XX:XXX 为映射端口，将docker容器端口XX映射到宿主机的xxX端口&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看镜像信息：docker images&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;为镜像新增标签：docker tag 镜像名 标签名&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;获取镜像的详细信息：docker inspect images-id   同一个镜像的image-id是一样的 返回一个json格式的信息，具体到那一类信息用-f参数&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;搜索镜像：docker search      -s n n星以上的镜像   关键字&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除镜像：docker rmi 标签/image-id    当该镜像创建容器正在运行，则无法删除，当然可以使用-f 强制删除，但是不建议使用。其实最原始的就是docker images rm，后来简化为docker rmi，原来的依然可用。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建镜像：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;基于已有镜像创建   docker commit    [option] container [repository[:tag]]&lt;/p&gt;

&lt;p&gt;option:-a 作者信息  -m 提交信息  -p 提交时暂停容器运行&lt;/p&gt;

&lt;p&gt;例如：docker commit -a &amp;ldquo;jcy&amp;rdquo; -m &amp;ldquo;redisconfigload&amp;rdquo; 01758f83ddb2 redisconfigload&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;基于本地模板的导出导入 docker export/import&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; docker export 7691a814370e &amp;gt; ubuntu.tar.gz
cat ubuntu.tar.gz | docker import - ubuntu:14.04
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;基于dockerfile&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dockerfile一种文本格式的配置文件，由一行行命名组成,支持#开头的注释，主要组成如下：

基础镜像信息 FROM &amp;lt;image&amp;gt;:&amp;lt;tag&amp;gt;  第一条必须是这个命令，可以基于多个镜像，也可以基于空镜像scratch

维护者信息   MAINTAINER &amp;lt;name&amp;gt;

镜像操作指令  RUN &amp;lt;command&amp;gt; 类似于/bin/sh -c  RUN [&amp;quot;executable&amp;quot;,&amp;quot;param1&amp;quot;,&amp;quot;param2&amp;quot;] 使用exec来执行

容器启动时执行的指令
CMD [&amp;quot;executable&amp;quot;,&amp;quot;param1&amp;quot;,&amp;quot;param2&amp;quot;]使用exec来执行,运行一个可执行的文件并提供参数
CMD command param1 param2   也可以是一个shell脚本，者就是一个带参数执行的命令bin/sh中执行，提供给需要交互的用户
CMD [&amp;quot;param1&amp;quot;,&amp;quot;param2&amp;quot;]  给ENTRYPOINT提供默认参数
CMD命令只执行一次，多条也会被覆盖，只执行最后一条，而且会被docker run指定的运行命令所覆盖。

EXPOSE 让docker容器暴露出端口来

ENV key value 指定环境变量
ARG指令定义了用户可以在编译时或者运行时传递的变量，如使用如下命令：--build-arg &amp;lt;varname&amp;gt;=&amp;lt;value&amp;gt;


ADD COPY都是复制,ADD会对tar包进行解压，COPY不会,COPY可以自动创建不存在的目录。

ENTRYPOINT [&amp;quot;executable&amp;quot;,&amp;quot;param1&amp;quot;,&amp;quot;param2&amp;quot;]使用exec来执行
ENTRYPOINT command param1 param2  shell执行，一样是容器启动后执行的，不会被cmd覆盖，同样只有一条生效。多条的情况下最后一条有效。

VOLUME [&amp;quot;/data&amp;quot;] 创造挂载点

WORKDIR  指定容器的工作目录。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后用docker build 来创建镜像 -t指定镜像的标签信息 -f dockerfile的路径 .&lt;/p&gt;

&lt;p&gt;比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker build -t fabric8-console:2.2.199 -f /root/f8/f8-console/fabric8-console-2.2.199/Dockerfile .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每一步都会生成一个块，对应的有一个唯一哈希值标志，下一次再进行打包的时候可以直接用这个缓存的块，可见镜像是由块组成的，dockerfile步骤越多块越多，镜像就越大，所以最后是基于空镜像，将文件系统的相关环境设置好打成tar包，然后用ADD加入，减少后面run执行的步骤，减小镜像的大小。
上面的哈希是根据大小来生成的，如果大小一样，哈希值会一样，不过这个很少出现，如果出现可以加上参数&amp;ndash;no-cache=true不使用缓存。&lt;/p&gt;

&lt;p&gt;我们一般都是使用dockerfile来制作镜像。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;存储镜像：docker save 将镜像存储为本地的文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker save busybox &amp;gt; busybox.tar

docker save --output busybox.tar busybox

    $ sudo docker save -o fedora-all.tar fedora
    $ sudo docker save -o fedora-latest.tar fedora:latest

docker save 1316871b180b -o /root/dockerfile/loggermanager1.0.tar
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;载入镜像：docker load 将本地镜像文件加载为本地镜像库的镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker load &amp;lt; path/xxx.tar
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;上传镜像：docker push   name[:tag]&lt;/p&gt;

&lt;p&gt;先用docker tag打标签，然后这个标签上传到默认的dockerhub中，当然也可以上传到私有仓库。例如：&lt;/p&gt;

&lt;p&gt;在dockerhub上注册了user&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker tag test:latest user/test:latest
docker push user/test:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;私有库的搭建以及上传管理在下面详细讲解。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;镜像下载
Docker默认拉取镜像是从&lt;a href=&#34;https://hub.docker.com/&#34;&gt;这里&lt;/a&gt;拉取,拉取的速度让人。。。，所以是配置国内镜像源，拉取速度十分惊人,可以自己去阿里云申请一个免费的加速器，我们一般会自己免费申请一下阿里云的镜像加速器，其实就是换了阿里的镜像源。&lt;/p&gt;

&lt;p&gt;针对Docker客户端版本大于 1.10.0 的用户&lt;/p&gt;

&lt;p&gt;您可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
{
  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://qzre5v92.mirror.aliyuncs.com&amp;quot;]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;容器&#34;&gt;容器&lt;/h2&gt;

&lt;p&gt;容器：在镜像上加一个隔离层，相当于一个个运行的实例，可以用docker ps -a 来查看当前所有的启停实例。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;创建: docker create&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;启动：docker start/stop  合二为一 docker run&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建容器：docker run -t -i .. /bin/bash    启动一个bash终端，-t表示一个伪终端并绑定在容器的标准输入中，-i则让容器标准输入保持打开 守护态运行-d,退出用exit或者ctrl+d,-v挂载卷，-e传递环境变量，-p映射端口，是通过iptables实现的，可以用iptables-save查看。当然还可以通过参数对资源进行限制。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;也可以使用docker run centos /bin/echo &amp;lsquo;hello world&amp;rsquo;,这样就类似在本地执行echo &amp;lsquo;hello world&amp;rsquo;&lt;/li&gt;
&lt;li&gt;docker run 主要查看本地是否有镜像，没有则去dockerhub上下载,dcokerhub官方网站不支持直接下载镜像，注册后只能有自己的私有仓库，还是不能下载镜像&lt;/li&gt;
&lt;li&gt;利用镜像创建并启动一个容器&lt;/li&gt;
&lt;li&gt;分配一个文件系统，给只读的镜像外层加一个可写层&lt;/li&gt;
&lt;li&gt;从宿主机的网桥接口中桥接一个虚拟接口到容器&lt;/li&gt;
&lt;li&gt;配置一个ip&lt;/li&gt;
&lt;li&gt;执行用户制定的程序&lt;/li&gt;
&lt;li&gt;执行完终止容器&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;进入容器：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;docker attach  name 这个是docker自带的命令，但是在多个终端一起链接到该容器的时候，他们是同步的，当其中一个出现问题时，其他的也会一样出现该问题&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;docker exec -ti   id /bin/bash 这个也是docker自带的，在docker1.3之后，它在容器中启动来一个bash,但是并不是所有的容器都是在linux系统上建立起来的，并不一定能启动bash&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可以用工具nsenter，是一个需要安装的工具。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;容器资源限制参数 dockerfile的路径&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;-m 1024m &amp;ndash;memory-swap=1024m  # 限制内存最大使用（bug：超过后进程被杀死）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&amp;ndash;cpuset-cpus=&amp;ldquo;0,1&amp;rdquo;           # 限制容器使用CPU&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;docker容器随系统自启参数&lt;/p&gt;

&lt;p&gt;docker run &amp;ndash;restart=always redis&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;no – 默认值，如果容器挂掉不自动重启&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;on-failure – 当容器以非 0 码退出时重启容器 同时可接受一个可选的最大重启次数参数 (e.g. on-failure:5).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;always – 不管退出码是多少都要重启&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除：docker rm&lt;/p&gt;

&lt;p&gt;-f 强制删除  -l 删除链接  -v删除挂载卷&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;导入和导出：docker export/import  将容器导出为tar文件，将文件导入为镜像，docker load差不多&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;docker logs id 记录容器内的操作&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;docker logs id&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;docker容器和主机进行拷贝&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;重docker容器内拷贝到主机上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@oegw1 soft]# docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
8d418a7b6021        postgres            &amp;quot;/docker-entrypoint.   7 hours ago         Up 7 hours                              test1
[root@oegw1 soft]# docker exec -t -i 8d418a7b6021 /bin/bash
root@oegw1:/var/lib/postgresql# pwd
/var/lib/postgresql
root@oegw1:/var/lib/postgresql# ls
data
root@oegw1:/var/lib/postgresql# exit
exit
[root@oegw1 soft]# docker cp 8d418a7b6021:/var/lib/postgresql/data /opt/soft/
完成拷贝
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重主机拷贝到docker容器中&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;挂载，也即是主机和docker容器共享一个目录&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在创建容器到时候用 -v XXX：XXX 来挂载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pdapp18 etc]# docker run -it -v /opt/inkscope/etc:/mnt centos /bin/bash
[root@bd91c6b79e87 mnt]# ll
total 4
-rw-r--r--. 1 root root 1323 May 23  2016 inkscope.conf
[root@bd91c6b79e87 mnt]# exit
[root@pdapp18 etc]# pwd
/opt/inkscope/etc
[root@pdapp18 etc]# l
总用量 4
-rw-r--r--. 1 root root 1323 5月  23 2016 inkscope.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;动态挂载&lt;/p&gt;

&lt;p&gt;后续完成&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;docker还提供来一个数据卷的高级用法&lt;/p&gt;

&lt;p&gt;数据卷：“其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的”。感觉像是由一个容器定义的一个数据挂载信息。其他的容器启动可以直接挂载数据卷容器中定义的挂载信息。&lt;/p&gt;

&lt;p&gt;看示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -v /root:/root  --name test centos /bin/bash
创建一个普通的容器。用--name给他指定了一个名（不指定的话会生成一个随机的名子）。

然后其他容器就可以使用--volumes-from来引用这个数据卷，就可以让当前容器中的/root目录和本机的/root的进行共享
docker run -it --volumes-from test centos /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;首先查看这个容器的id&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker inspect -f &#39;{{.Id}}&#39; container-id返回容器的id
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上本机的/var/lib/docker/container/contianer-id/和docker容器的根目录/是一致的，所以可以直接操作这个目录相当于操作容器的根目录，实现共享。但是我试验没有实现，等有时间再看看什么原因。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;仓库&#34;&gt;仓库&lt;/h2&gt;

&lt;p&gt;仓库：存放镜像文件的&lt;/p&gt;

&lt;p&gt;最大docker官方公共仓库：docker hub，国内：docker pool&lt;/p&gt;

&lt;p&gt;官方的仓库镜像一般就是基础镜像，单个单词命名，而username/镜像名这个一般是某个用户上传的镜像&lt;/p&gt;

&lt;p&gt;默认是重docker hub上下载，如果需要重其他的镜像仓库下载则需要加前缀。&lt;/p&gt;

&lt;p&gt;当然也可以创建私有仓库。主要是通过docker registry这个python开源项目进行创建，registry2之后已经重构用go来开发，目前的registry的版本是2.6，但是有着很多的问题，现在已经有对应的容器镜像了，当然也可以通过源码进行安装。当然也可以使用集成的仓库的harbor等管理工具。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;搭建私有库：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先下载官方提供的的镜像并且创建一个容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -p 5000:5000 --restart=always --name registry registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以docker ps看一下镜像已经启动，可以使用crul &lt;a href=&#34;http://ip:5000&#34;&gt;http://ip:5000&lt;/a&gt; 看看网络是否通，本地机器和容器之间的通信也需要iptables，iptables-services来进行通信。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pdapp20 ~]# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
98de3ec23d75        registry:2          &amp;quot;/entrypoint.sh /e...&amp;quot;   16 hours ago        Up 16 hours         0.0.0.0:5000-&amp;gt;5000/tcp   registry&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过iptables-save来保存现有的规则同时也能查看，不过这个只需要安装好iptables,iptables-services并启动着就可以自动加入通信的规则。然后检查容器与主机之间是否是通的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、/usr/sbin/sestatus -v      ##如果SELinux status参数为enabled即为开启状态
SELinux status:                 enabled
2、getenforce                 ##也可以用这个命令检查
关闭SELinux：
1、临时关闭（不用重启机器）：
setenforce 0                  ##设置SELinux 成为permissive模式
                              ##setenforce 1 设置SELinux 成为enforcing模式
2、修改配置文件需要重启机器：
    修改/etc/selinux/config 文件
    将SELINUX=enforcing改为SELINUX=disabled
    重启机器即可
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候网络应该是通的了，然后我们tag一个自己的镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker pull ubuntu &amp;amp;&amp;amp; docker tag ubuntu localhost:5000/ubuntu
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后上传&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker push localhost:5000/ubuntu
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过&lt;a href=&#34;http://localhost:5000/v2/_catalog&#34;&gt;http://localhost:5000/v2/_catalog&lt;/a&gt; （在v1版本的时候是v1/search,现在在v2版本中已经不使用）来查看私有仓库的镜像,并到每一个镜像中去查看该镜像的具体信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pdapp20 ~]# curl http://localhost:5000/v2/_catalog
{&amp;quot;repositories&amp;quot;:[&amp;quot;hello-world&amp;quot;,&amp;quot;ubuntu&amp;quot;]}
[root@pdapp20 ~]# curl http://localhost:5000/v2/tags/list
404 page not found
[root@pdapp20 ~]# curl http://localhost:5000/v2/ubuntu/tags/list
{&amp;quot;name&amp;quot;:&amp;quot;ubuntu&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;latest&amp;quot;]}
[root@pdapp20 ~]# curl http://localhost:5000/v2/hello-world/tags/list
{&amp;quot;name&amp;quot;:&amp;quot;hello-world&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;latest&amp;quot;]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们删除本地的镜像，重自己的私有库来下载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pdapp20 ~]# docker rmi localhost:5000/hello-world
Untagged: localhost:5000/hello-world:latest
Untagged: localhost:5000/hello-world@sha256:2075ac87b043415d35bb6351b4a59df19b8ad154e578f7048335feeb02d0f759
[root@pdapp20 ~]# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
registry            2                   047218491f8c        2 weeks ago         33.2 MB
ubuntu              latest              0ef2e08ed3fa        3 weeks ago         130 MB
hello-world         latest              48b5124b2768        2 months ago        1.84 kB
centos              latest              67591570dd29        3 months ago        192 MB
[root@pdapp20 ~]# docker pull localhost:5000/hello-world
Using default tag: latest
latest: Pulling from hello-world
Digest: sha256:2075ac87b043415d35bb6351b4a59df19b8ad154e578f7048335feeb02d0f759
Status: Downloaded newer image for localhost:5000/hello-world:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然私有仓库可以进行加密认证。&lt;/p&gt;

&lt;p&gt;私有库的镜像的默认存储路径是/tmp/registry,可以通过启动参数-v进行修改，比如下面的例子将上传的镜像放到 /opt/data/registry 目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -p 5000:5000 -v /opt/data/registry:/tmp/registry registry
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以讲镜像存储到云服务器上去，都有对应的配置&lt;/p&gt;

&lt;p&gt;还可以制定配置文件-v /home/user/registry-conf:/registry-conf&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;错误解决&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Private registry push fail: server gave HTTP response to HTTPS client&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.Create or modify /etc/docker/daemon.json
{ &amp;quot;insecure-registries&amp;quot;:[&amp;quot;myregistry.example.com:5000&amp;quot;] }
2.Restart docker daemon
sudo service docker restart
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;其他&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;目前harbor已经在私有库一统天下了，vmware开源，国人开发，界面友好，使用简单，各大企业都是使用这个私有库组件&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/cdci/harbor/&#34;&gt;Harbor&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;数据和网络&#34;&gt;数据和网络&lt;/h1&gt;

&lt;h2 id=&#34;数据&#34;&gt;数据&lt;/h2&gt;

&lt;p&gt;数据管理实现数据的共享和备份。核心就是-v参数。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;共享&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、数据卷&lt;/p&gt;

&lt;p&gt;数据卷是一个可供一个或多个容器使用的特殊目录，它绕过UFS，可以提供很多有用的特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据卷可以在容器之间共享和重用&lt;/li&gt;
&lt;li&gt;对数据卷的修改会立马生效&lt;/li&gt;
&lt;li&gt;对数据卷的更新，不会影响镜像&lt;/li&gt;
&lt;li&gt;卷会一直存在，直到没有容器使用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单说来，卷就是将宿主机中的某个目录，mount到容器中，这样，在容器中此目录下的修改，即便容器关闭，数据也会保留下来，供宿主机和其他容器访问。&lt;/p&gt;

&lt;p&gt;数据卷的使用，类似于 Linux 下对目录或文件进行 mount。&lt;/p&gt;

&lt;p&gt;主要是下面的使用方式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在运行容器的时候，在Docker中创建一个数据卷&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -dti -v /data centos
＃在docker中会有/data目录，这个目录不归属于层级文件系统
ls /data -d
/data
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;将宿主机的一个目录，挂在到容器里，这种方式，数据可以保存在宿主机中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#例如将宿主机的/var/data挂载到容器中的/data
docker run -tdi -v /var/data:/data centos
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;挂载单个文件到容器中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -tdi ~/dbback.tar.gz:/dbback.tar.gz centos
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、数据卷容器&lt;/p&gt;

&lt;p&gt;数据卷容器的作用是，其中挂载的数据卷，可以被使用它的容器，共同使用。也就是多个容器之间可以同时使用这个数据卷，容器对他的写入内容，在其他容器也能看到。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#创建一个包含数据卷的容器供其他容器使用，这个容器并不需要一直开启
docker run -tdi -v /data --name data_s centos
#创建两个容器，使用这个数据卷容器
docker run -ti --volumes-from data_s  --name web1 centos
docker run -ti --volumes-from data_s  --name web2 centos
#此时这两个容器，都可以共同读写/data目录了
#如果需要将数据同步到宿主机的目录中，则创建数据卷容器的时候，选择挂载宿主机的目录，如：
docker run -tdi -v /data:/data --name data_s centos
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;备份&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -volumes-from data -v $(pwd):/backup  ubuntu
tar -zcvf /backup/buckup.tar.gz /data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样可以将容器数据卷data打包成backup.tar.gz，然后放到容器的backup目录下和本地主机的当前目录共享。实现备份。&lt;/p&gt;

&lt;p&gt;数据恢复就是将压缩包解压到共享目录，然后放到数据卷的容器中去。&lt;/p&gt;

&lt;h2 id=&#34;网络&#34;&gt;网络&lt;/h2&gt;

&lt;p&gt;核心参数-p和-P，这边只是简单的映射使用，需要了解实现就要看核心的&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/docker/docker-network/&#34;&gt;网络原理&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;1、容器到主机：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;端口的映射可以制定主机端口，可以多次制定，可以指定所有的，可以通过docker port来查看当前容器的端口。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、容器到容器：&lt;/p&gt;

&lt;p&gt;通过&amp;ndash;link来在容器之间建立一个安全到通道，避免暴露在外&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --rm -it --name web2 --link db:db training/webapp /bin/bash

root@5845d10a2bf5:/opt/webapp# cat /etc/hosts
127.0.0.1    localhost
::1    localhost ip6-localhost ip6-loopback
fe00::0    ip6-localnet
ff00::0    ip6-mcastprefix
ff02::1    ip6-allnodes
ff02::2    ip6-allrouters
172.17.0.3    db c6cd414f08d4   -------这边在web的容器里面有db的容器的主机配置
172.17.0.5    5845d10a2bf5
root@5845d10a2bf5:/opt/webapp# apt-get install -yqq inetutils-ping
E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/universe/i/inetutils/inetutils-ping_1.9.2-1_amd64.deb  Could not resolve &#39;archive.ubuntu.com&#39;

E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
root@5845d10a2bf5:/opt/webapp# ping db
PING db (172.17.0.3) 56(84) bytes of data.
64 bytes from db (172.17.0.3): icmp_seq=1 ttl=64 time=0.204 ms
64 bytes from db (172.17.0.3): icmp_seq=2 ttl=64 time=0.106 ms
64 bytes from db (172.17.0.3): icmp_seq=3 ttl=64 time=0.097 ms
64 bytes from db (172.17.0.3): icmp_seq=4 ttl=64 time=0.102 ms

----网络是通的，说明两个容器之间建立来一个安全的通道。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Docker集群通信：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自定义网桥&lt;/li&gt;
&lt;li&gt;使用adbasssdor容器—目前已经有实现的容器集群管理方案，例如k8s
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;docker实践&#34;&gt;docker实践&lt;/h1&gt;

&lt;h2 id=&#34;busybox&#34;&gt;busybox&lt;/h2&gt;

&lt;p&gt;busybox是一个集成一百多个常用的linux命令的工具箱，经常使用的echo，cat，mount，grep等等都在这个里面，是linux系统的瑞士军刀。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker pull busybox
docker run -it busybox
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以在这个镜像中使用各个命令了。&lt;/p&gt;

&lt;h2 id=&#34;ubuntu-centos&#34;&gt;ubuntu/centos&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;docker pull centos
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;coreos&#34;&gt;CoreOS&lt;/h2&gt;

&lt;p&gt;CoreOS是基于docker发行的linux的版本&lt;/p&gt;

&lt;h2 id=&#34;支持ssh的镜像&#34;&gt;支持ssh的镜像&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;首先下载镜像并创建容器，在容器中安装openssh-server，并启动该服务，然后在根目录下写一个脚本run.sh，来启动ssh，然后用docker commit来打包一个新的镜像。

然后可以用这个镜像创建容器启动服务就可以通过ssh来链接这个容器了

docker run -p 10022:22 -d sshd:centos /run.sh

ssh 宿主机 -p 10022 就可以登录到容器内
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;编排&#34;&gt;编排&lt;/h1&gt;

&lt;h2 id=&#34;docker-compose&#34;&gt;docker Compose&lt;/h2&gt;

&lt;p&gt;Compose，你可以在一个文件中定义一个多容器应用，然后使用一条命令来启动你的应用，完成一切准备工作。&lt;/p&gt;

&lt;p&gt;使用Docker Compose，不再需要使用shell脚本来启动容器。在配置文件中，所有的容器通过services来定义，然后使用docker-compose脚本来启动，停止和重启应用，和应用中的服务以及所有依赖服务的容器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build 构建或重建服务
help 命令帮助
kill 杀掉容器
logs 显示容器的输出内容
port 打印绑定的开放端口
ps 显示容器
pull 拉取服务镜像
restart 重启服务
rm 删除停止的容器
run 运行一个一次性命令
scale 设置服务的容器数目
start 开启服务
stop 停止服务
up 创建并启动容器
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装&lt;/p&gt;

&lt;p&gt;1、在线下载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -L https://github.com/docker/compose/releases/download/1.4.2/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose
chmod  x /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、离线github上有对应的二进制包。&lt;/p&gt;

&lt;p&gt;3、通过配置yml文件来部署。&lt;/p&gt;

&lt;h2 id=&#34;docker-swarm&#34;&gt;docker swarm&lt;/h2&gt;

&lt;p&gt;和k8s是同一个级别的编程工具，目前已经放弃，k8s已经一家独大。&lt;/p&gt;

&lt;h1 id=&#34;常规使用&#34;&gt;常规使用&lt;/h1&gt;

&lt;p&gt;1、docker删除所有的容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker ps -a | awk &#39;{print $1}&#39; | xargs docker rm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、解决SELinux is not supported with the overlay2 graph driver on this kernel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;屏蔽#OPTIONS=&#39;--selinux-enabled --log-driver=journald --signature-verification=false&#39;
加DOCKER_OPTS=&amp;quot;--storage-driver=devicemapper&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、镜像源&lt;/p&gt;

&lt;p&gt;1、docker有官方的中国区加速器了！&lt;/p&gt;

&lt;p&gt;&amp;ndash;registry-mirror=&lt;a href=&#34;https://registry.docker-cn.com&#34;&gt;https://registry.docker-cn.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2、网易163 docker镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo echo &amp;quot;DOCKER_OPTS=\&amp;quot;--registry-mirror=http://hub-mirror.c.163.com\&amp;quot;&amp;quot; &amp;gt;&amp;gt; /etc/default/docker
$ service docker restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般来说，网易的猪肉比市场上的猪肉好吃。但从我的体验来看，有时会pull失败的情况出现，并且重试不好使。&lt;/p&gt;

&lt;p&gt;3、ustc的镜像&lt;/p&gt;

&lt;p&gt;ustc是老牌的linux镜像服务提供者了，还在遥远的ubuntu 5.04版本的时候就在用。之前在blog里有提到可以用ustc的docker仓库镜像，使用方法参考ustc docker 镜像使用帮助，你要是懒得看，可以两条命令搞定（ubuntu亲测）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo echo &amp;quot;DOCKER_OPTS=\&amp;quot;--registry-mirror=https://docker.mirrors.ustc.edu.cn\&amp;quot;&amp;quot; &amp;gt;&amp;gt; /etc/default/docker
$ sudo service docker restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ustc的docker镜像速度不错，一直用的挺happy。但是今天发现不好使了，可能跟这件事有关系吧，今天尝试去pull ubuntu，非常慢，应该是直接去docker hub上去拉了，基本没有加速效果。&lt;/p&gt;

&lt;p&gt;据说收费了，用网易的吧！&lt;/p&gt;

&lt;p&gt;4、daocloud&lt;/p&gt;

&lt;p&gt;DaoCloud也提供了docker加速器，但是跟ustc不同，需要用户注册后才能使用，并且每月限制流量10GB。linux上使用比较简单，一条命令搞定：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://{your_id}.m.daocloud.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际改的是/usr/lib/systemd/system/docker.service，加了个–registry-mirror参数，：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ExecStart=/usr/bin/docker-current daemon --registry-mirror=http://{your_id}.m.daocloud.io\
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置后，需要重新加载配置&amp;amp;重启：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl enable docker; systemctl daemon-reload ; systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是！今天使用DaoCloud的docker加速器体验非常差，加速效果基本没感觉，果断放弃。&lt;/p&gt;

&lt;p&gt;5、alicloud&lt;/p&gt;

&lt;p&gt;阿里云也提供了docker加速器，不过比ustc更麻烦：不光要注册为阿里云的用户，还得加入开发者平台。不过捏着鼻子做完这些以后，它的服务还真是不错，基本1MB/s的pull速度(部分原因可能是因为我也在杭州吧)。配置方法跟daocloud类似，也是开通加速器以后给一个url。&lt;/p&gt;

&lt;p&gt;我直接去改/usr/lib/systemd/system/docker.service了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ExecStart=/usr/bin/docker-current daemon --registry-mirror=https://{your_id}.mirror.aliyuncs.com\
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新加载配置&amp;amp;重启：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl enable docker; systemctl daemon-reload ; systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pull的时候还是显示docker.io，但速度一点都不docker.io。&lt;/p&gt;

&lt;p&gt;4、分层编译&amp;ndash;docker版本要在17.06之后&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM golang:1.9-alpine3.6 as builder

ENV PILOT_DIR /go/src/github.com/AliyunContainerService/log-pilot
ARG GOOS=linux
ARG GOARCH=amd64
RUN set -ex \
    &amp;amp;&amp;amp; apk add --no-cache make git
WORKDIR $PILOT_DIR
COPY . $PILOT_DIR
RUN go install

FROM alpine:3.6

ENV FILEBEAT_VERSION=6.1.1-3
RUN apk update &amp;amp;&amp;amp; \
    apk add ca-certificates &amp;amp;&amp;amp; \
    apk add wget &amp;amp;&amp;amp; \
    update-ca-certificates &amp;amp;&amp;amp; \
    rm -rf /var/cache/apk/*

COPY --from=builder /go/bin/log-pilot /pilot/pilot
COPY assets/filebeat/filebeat.tpl /pilot/

WORKDIR /pilot/
ENTRYPOINT [&amp;quot;/pilot/pilot&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面一个用于编译，然后把编译好的结果copy到我最终的镜像中去&lt;/p&gt;

&lt;p&gt;5、docker跨平台&lt;/p&gt;

&lt;p&gt;docker跨平台是一个重大价值，但是个人觉得是基于docker引擎之上的，类似于java都是跑在jvm上，而跨平台是不同版本的jvm，docker也是一样，也是镜像跑在不同版本的docker上实现跨平台，但是不同的是，docker不是基于原有的系统上创建虚拟机，然后运行docker，docker都是原生的基于内核的，当然一开始不同版本还是基于虚拟机的，现在实现了，这是跨平台的优势之一。&lt;/p&gt;

&lt;p&gt;6、docker stats -a   查看容器资源消耗情况&lt;/p&gt;

&lt;p&gt;7、CICD&lt;/p&gt;

&lt;p&gt;首先企业中的开发人员开发完成app之后，将代码推送代码管理仓库Github中，，这个时候Github探测到仓库中代码的变更，自动触发Jenkins编译应用，之后通过Dockerfile来打包成一个镜像，镜像可以直接推送到镜像存储中保存，上面的流程实现了持续集成，另外Jenkins可以在测试域上启动应用，进行测试，验证完成之后，触发上线应用的流程，实现CD流程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/docker/docker.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;具体可以参考&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/cdci/jenkins/&#34;&gt;CDCI&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/docker/docker-principle&#34;&gt;docker实现的原理&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;发展&#34;&gt;发展&lt;/h1&gt;

&lt;p&gt;docker目前发展并不是太好，k8s新版本开始支持containerd，大有取代docker的趋势。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 电商购物系统</title>
          <link>https://kingjcy.github.io/post/architecture/shopping/</link>
          <pubDate>Tue, 04 Feb 2020 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/shopping/</guid>
          <description>&lt;p&gt;电商购物系统算是目前软件技术落地的很大的一个发展方向，主要以阿里为主导的电商购物系统占据整个行业的半壁江山，是直接和价值挂钩的重要业务方向。&lt;/p&gt;

&lt;h1 id=&#34;架构演进&#34;&gt;架构演进&lt;/h1&gt;

&lt;p&gt;电商系统也是和其他系统一样一步步演进的，架构演进都是通用的，可以看&lt;a href=&#34;https://kingjcy.github.io/post/architecture/architecture-evolution/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通用架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/ms2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;目前电商的整体架构流程如下，并且在每一层如何实现在高并发，大流量的情况下，最大程度能够保证网站的健康。主要是流量控制，让用户的流量像漏斗模型一样逐层减少，让流量始终处于可控的范围之内。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;客户端可以是各种终端，可以是web，进行各种操作。在web这边，主要是&lt;strong&gt;限流&lt;/strong&gt;，可以使用js技术，比如购买按钮置灰，可以减少大量的重复请求。&lt;/li&gt;
&lt;li&gt;CDN层，CDN存储静态页面缓存技术。在这一层主要是&lt;strong&gt;缓存&lt;/strong&gt;，很多的静态资源存储在cdn中，能够快速对请求进行返回。

&lt;ul&gt;
&lt;li&gt;限制用户维度访问频率：针对同一个用户（ Userid 维度），做页面级别缓存，单元时间内的请求，统一走缓存，返回同一个页面。&lt;/li&gt;
&lt;li&gt;限制商品维度访问频率：大量请求同时间段查询同一个商品时，可以做页面级别缓存，不管下回是谁来访问，只要是这个页面就直接返回。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;接入层，nginx等负载均衡技术。这一层主要是&lt;strong&gt;限流截流&lt;/strong&gt;，比如相同请求，只转发一个，分发到不同的服务实例，限制同一个ip的多次请求。&lt;/li&gt;
&lt;li&gt;然后将请求放到MQ中，排队处理，这已经算是到应用层。这一层主要是&lt;strong&gt;削峰&lt;/strong&gt;，请任务放到队列中，峰值的时候排队处理，不会阻塞&lt;/li&gt;
&lt;li&gt;应用层，处理各种应用。这一层就是&lt;strong&gt;高并发模型和异步处理&lt;/strong&gt;，实例实现并发的多进程，多线程的处理，也就上面购物系统处理的一个流程。

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-context/&#34;&gt;并发控制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;异步处理：定一个channel，以goroutine来运行，用于接受异步返回的结果，是实现高并发的一种方式。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-event/&#34;&gt;分布式事务&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-lock/&#34;&gt;分布式安全&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;最后就是DB层，这一层是瓶颈所在，分层的目的是为了将压力留在上层，一般瓶颈都是在数据库，所以需要把有效得请求到数据库中进行处理，针对数据库的瓶颈，主要就是&lt;strong&gt;索引读写分离，分区分库分表&lt;/strong&gt;等。&lt;/li&gt;
&lt;li&gt;在每一层能有缓存的地方都要用&lt;strong&gt;缓存&lt;/strong&gt;，缓存的效率比数据库要高很多，哪怕是缓存数据库。&lt;/li&gt;
&lt;li&gt;在每一层最好能使用&lt;strong&gt;集群和分布式&lt;/strong&gt;来加强系统的承受能力。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;整体来说，核心思想就是限流、缓存、削峰、高并发、异步处理，集群和分布式，做好这些模块才能保证系统的稳定性和最大的承受能力，在高并发和大流量的情况下。&lt;/p&gt;

&lt;p&gt;当然这些就是属于业务架构了，都是可以使用微服务的架构来实现，每一块都可以做成一个微服务。&lt;/p&gt;

&lt;h2 id=&#34;限流截流算法&#34;&gt;限流截流算法&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;令牌桶算法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;令牌桶算法主要用于限制流量的平均流入速率，允许出现一定的突发流量。比如我们常用的nginx就是一个典型的令牌桶算法的实现：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每秒会有r个令牌被放入桶内，也就是说以1/r的速度向桶中放入令牌&lt;/li&gt;
&lt;li&gt;桶的容量是固定不变的，所以多出来的令牌就会被丢弃&lt;/li&gt;
&lt;li&gt;当一个n字节的请求包到达时，消耗n个令牌，然后才能发送该数据包&lt;/li&gt;
&lt;li&gt;如果桶内令牌小于n，数据包就会被限流，比如缓存或者丢弃&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;nginx限流模块的使用，tengine是在nginx的基础进行二次开发，针对高并发，大流量的场景进行优化，也可以了解一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#统一在http域中进行配置

 #限制请求：表示每个ip每秒的请求数不能超过50个
 limit_req_zone $uri zone=api_read:20m rate=50r/s;

 #按ip配置一个连接 zone
 limit_conn_zone $binary_remote_addr zone=perip_conn:10m;

 #按server配置一个连接 zone
 limit_conn_zone $server_name zone=perserver_conn:100m;

location / {
 if (!-e $request_filename){
  rewrite ^/(.*) /index.php last;
 }

 #请求限流排队通过 burst代表着桶的大小，默认是0，这里代表缓存100个
 limit_req zone=api_read burst=100;

 #连接数限制,每个IP并发请求为50
 limit_conn perip_conn 50;

 #服务所限制的连接数(即限制了该server并发连接数量)
 limit_conn perserver_conn 200;

 #连接限速
 #limit_rate 100k;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;漏斗算法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;可以以任何速度向桶内发送请求&lt;/li&gt;
&lt;li&gt;当然桶的容量还是固定的，如果请求大于桶了，就会被丢弃&lt;/li&gt;
&lt;li&gt;然后按固定过得速度来处理桶内的请求&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;计数器算法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;就是对某个对象在单位时间内允许被操作的次数，一旦超过了所设定的阈值，就会拒绝请求，到时间后重置计数器，重新限制。&lt;/p&gt;

&lt;p&gt;比如某个商品在10s内只能抢购5000次，对抢购进行计数，当达到5000次之后，就会拒绝请求，10s过后，对已经抢购的次数进行重置，重新进行抢购。&lt;/p&gt;

&lt;h2 id=&#34;缓存&#34;&gt;缓存&lt;/h2&gt;

&lt;p&gt;使用缓存将频繁访问的热点数据存储在最近的地方（比如CDN），最快响应的地方（比如本地内存），能够快速响应。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;本地缓存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;优点只有一个，操作本地内存简单，响应快。但是他的缺点很明显，因此适用场景也是很明确的&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据不能共享&lt;/li&gt;
&lt;li&gt;数据会丢失&lt;/li&gt;
&lt;li&gt;内存一般不大，用尽后会OOM&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以分布式缓存是比如发展的趋势，正常我们都是会将本地缓存和分布式缓存结合使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式缓存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;分布式缓存也就是我们常用的缓存数据库，比如redis，memcache等，具体就不多说了，可以自己去&lt;a href=&#34;https://kingjcy.github.io/post/database/redis/redis/&#34;&gt;详细了解&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;同一key的读要求&#34;&gt;同一key的读要求&lt;/h3&gt;

&lt;p&gt;1、使用redis进行多读多写&lt;/p&gt;

&lt;p&gt;就是采用冗余存储的方案，将一个key进行计算加工将其存储到redis集群中的各个节点，这个时候进行查询的时候就可以通过处理key来轮询均衡处理，解决热点商品哪怕使用集群也会出现单点的问题。需要注意数据一致性问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/redis.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、本地缓存+redis&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/cache.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;定时重redis缓存中同步数据到本地缓存，时间随着商品的热点来定，在定时期间可能出现数据不一致的情况，需要在最后的数据库扣除的时候做处理，在redis中使用分布式锁，在mysql中就是使用正常的锁。&lt;/p&gt;

&lt;p&gt;还可以通过推送消息的方式来减少不一致的情况，如果redis有变更就推送到队列中，本地消费更新。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/cache2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3、实时热点系统&lt;/p&gt;

&lt;p&gt;我们不可能准确的把所有的热点数据都推送到缓存中，这就需要我们在交易的过程中发现热点数据同步的缓存中，一般通过收集日志进行分析，最后自动推送到redis中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/sd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;同一key的写要求&#34;&gt;同一key的写要求&lt;/h3&gt;

&lt;p&gt;1、在redis中进行扣减库存&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/redis2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;还有进行优化，就是合并操作批量一次处理&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/redis3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、还有用一些优化的数据库，比如ALISQL&lt;/p&gt;

&lt;h2 id=&#34;削峰&#34;&gt;削峰&lt;/h2&gt;

&lt;p&gt;削峰其实就是对峰值流量进行分散处理，避免在同一时间内产生较大的用户流量冲击系统，从而降低系统的负载压力。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分时削峰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将抢购不要设置为一个时间点，比如0点，而是通过多设置几个时间点，来分散用户流量，降低系统压力&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;验证削峰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过验证来拦截缓存用户的请求，比如现在做的验证码新高度的12306的验证，虽然各种各样的验证码，但是确实成功的阻挡了秒杀器和降低了峰值流量&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;队列缓存削峰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将请求放到Q中，按顺序固定速率的处理，实现流量可控，保证系统的稳定，不会奔溃。&lt;/p&gt;

&lt;h2 id=&#34;异步&#34;&gt;异步&lt;/h2&gt;

&lt;p&gt;异步处理并不算一种并发的使用方式，但是却是并发中经常使用的，在工作池的基础上使用goroutine处理，但是不用等返回，留一个channenl返回，使用select读取channel中的数据，完成处理，这样可以加大处理的速度，也就提高了并发能力。&lt;/p&gt;

&lt;h2 id=&#34;数据库优化&#34;&gt;数据库优化&lt;/h2&gt;

&lt;p&gt;对于非结构化或者可以设计为非结构化的数据，我们可以放在redis这种缓存数据库中，但是重要的业务数据最终还是需要关系型数据库的保证，所以对于关系型数据库的优化使用，重来都没有停止过。&lt;/p&gt;

&lt;p&gt;关系型数据库最常见的瓶颈&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大量读写难以承受&lt;/li&gt;
&lt;li&gt;大量查询索引，效率低下&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解决这些问题，最常见的就是读写分离，分区分库分表。&lt;/p&gt;

&lt;h3 id=&#34;读写分离&#34;&gt;读写分离&lt;/h3&gt;

&lt;p&gt;读写分离很简单，一般数据库都是一主一从，主库用于写，从库用于读，可以很大程度上缓存数据库的压力。&lt;/p&gt;

&lt;p&gt;当然这样也有问题，就是主从之间存在着数据延迟，一般都是重设计上解决这个问题，在数据落到主库的时候落一份到缓存，避免从库查不到数据。&lt;/p&gt;

&lt;h3 id=&#34;分区分库分表&#34;&gt;分区分库分表&lt;/h3&gt;

&lt;p&gt;设计表的时候就应该有分区，这样可以加快查询，就像设计好的索引一样。&lt;/p&gt;

&lt;p&gt;首先就是垂直拆分，也就是分库，将不同类型的数据放到不同的库中，比如不停地方的订单放到不同的库中，每个库的压力就不大了。&lt;/p&gt;

&lt;p&gt;然后就是水平拆分，也就是分表，同一个业务表，数据量达到500W，查询效率就很低下了，优化索引什么都没有用，就需要对业务表进行拆分，将将业务按一定的序号拆分为子表，这个时候就需要一些组件完成路由，我们常见的有mysql sharding，mycat等。&lt;/p&gt;

&lt;p&gt;其实在业务量再大的情况下，可能需要同一个业务进行分库分表的操作。&lt;/p&gt;

&lt;h1 id=&#34;基本系统&#34;&gt;基本系统&lt;/h1&gt;

&lt;p&gt;不管是正常的购物还是抢购还是秒杀都是这么一个基本流程,包含这个一下基本的系统&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;搜索系统---订单系统---物流系统
订单系统：购物车系统---确认订单---支付系统，核心就是出发数据库修改库存
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首页，用户，广告，购物，订单，商品，结算&lt;/p&gt;

&lt;h1 id=&#34;基本业务场景&#34;&gt;基本业务场景&lt;/h1&gt;

&lt;h2 id=&#34;秒杀&#34;&gt;秒杀&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/timg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;秒杀系统会做，基本的购物，抢购，抢红包，大促都会做，秒杀的特性：限时限量，导致的场景就是瞬时并发量大。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;热场&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在活动开始之前，最好设计一个“热场”。&lt;/p&gt;

&lt;p&gt;“热场”的形式多种多样，例如：分享活动领优惠券，领秒杀名额等等。“热场”的形式不重要，重要的是通过它获取一些准备信息。&lt;/p&gt;

&lt;p&gt;例如：有可能参与的用户数，他们的地域分布，他们感兴趣的商品。为后面的技术架构提供数据支持，可以先对相关数据进行缓存，减少缓存的压力。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;千万级秒杀系统架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们再具体看一下包含秒杀的逻辑具体架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/timg2&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/ms3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;秒杀系统设计的核心思想：尽量将请求拦截在上层（限流，削峰），高并发（异步处理），充分使用内存缓存。&lt;/p&gt;

&lt;p&gt;整体流程中优化的点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（限流）            1、web层使用js技术，比如购买按钮置灰，可以减少大量的重复请求
（限流截流）        2、负载均衡nginx（也就是网关），进行截流，比如相同请求，相同的IP，只转发一个，分发到不同的服务实例
（缓存）            3、cdn缓存技术，将很多的静态资源存储在cdn中，能够快速返回
                        1、限制用户维度访问频率
                            针对同一个用户（ Userid 维度），做页面级别缓存，单元时间内的请求，统一走缓存，返回同一个页面。
                        2、限制商品维度访问频率
                            大量请求同时间段查询同一个商品时，可以做页面级别缓存，不管下回是谁来访问，只要是这个页面就直接返回。
（削峰）            4、然后将请求放到MQ中，排队处理，
（高并发，异步处理）  5、到后台实例实现并发的多进程，多线程的处理，也就上面购物系统处理的一个流程
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有一下业务上的优化，其实就是使用了均衡和削峰填谷的思想，比如12306所做的，分时分段售票，原来统一10点卖票，现在8点，8点半，9点，&amp;hellip;每隔半个小时放出一批：将流量摊匀。&lt;/p&gt;

&lt;p&gt;1、&lt;a href=&#34;https://kingjcy.github.io/posts/golang/go-context/&#34;&gt;并发控制&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2、&lt;a href=&#34;https://kingjcy.github.io/posts/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、异步处理：定一个channel，以goroutine来运行，用于接受异步返回的结果&lt;/p&gt;

&lt;p&gt;4、&lt;a href=&#34;https://kingjcy.github.io/posts/distributed/distributed-event/&#34;&gt;分布式事务&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;5、&lt;a href=&#34;https://kingjcy.github.io/posts/distributed/distributed-lock/&#34;&gt;分布式安全&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（高并发）           6、最后到数据库包括缓存数据库的读写分离，分区分库分表。
（缓存）             7、缓存是很快的用在每一层能够使用的地方
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分层的目的是为了将压力留在上层，一般瓶颈都是在数据库，所以需要把有效得请求到数据库中进行处理&lt;/p&gt;

&lt;p&gt;秒杀系统设计的核心思想：限流，削峰，高并发，异步处理，内存缓存&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- loki</title>
          <link>https://kingjcy.github.io/post/monitor/log/loki/loki/</link>
          <pubDate>Sat, 18 Jan 2020 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/loki/loki/</guid>
          <description>&lt;p&gt;Loki是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签，为 Prometheus和 Kubernetes用户做了相关优化。项目受 Prometheus 启发，类似于 Prometheus 的日志系统。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;使用场景&#34;&gt;使用场景&lt;/h2&gt;

&lt;p&gt;当我们的容器云运行的应用或者某个节点出现问题了，解决思路应该如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一般在容器云中使用prometheus生态来做监控告警，在metrics触发告警的时候，我们就需要查看日志来处理问题，这个时候就需要日志系统来收集日志进行搜索查看。&lt;/p&gt;

&lt;p&gt;现有的很多日志采集的方案都是采用全文检索对日志进行索引（如ELK方案），优点是功能丰富，允许复杂的操作。但是，这些方案往往规模复杂，资源占用高，操作苦难。很多功能往往用不上，大多数查询只关注一定时间范围和一些简单的参数（如host、service等），这个时候就需要一个轻量级的日志系统，这个时候loki就比较合适了。&lt;/p&gt;

&lt;h2 id=&#34;基本组件&#34;&gt;基本组件&lt;/h2&gt;

&lt;p&gt;Loki 整个系统需要三个组件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、Loki: 相当于 EFK 中的 ElasticSearch，用于存储和查询日志
2、Promtail: 相当于 EFK 中的 Filebeat/Fluentd，用于采集和发送日志
3、Grafana: 相当于 EFK 中的 Kibana，用于 UI 展示
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些组件以以下的部署在我们的系统中&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们可以看出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、loki: 以 Statefulset 方式部署，可横向扩容
2、promtail: 以 Daemonset 方式部署，采集每个节点上容器日志并发送给 loki
3、grafana: 默认不开启，如果集群中已经有 grafana 就可以不用在部署 grafana，如果没有，部署时可以选择也同时部署 grafana
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不使用容器部署，也大体可以看出对应的部署方式，就是Promtail作为采集组件需要部署在每个一个机器上然后将数据推送到loki中，grafana在loki中拉去数据进行展示。&lt;/p&gt;

&lt;h1 id=&#34;部署使用&#34;&gt;部署使用&lt;/h1&gt;

&lt;h2 id=&#34;k8s部署&#34;&gt;k8s部署&lt;/h2&gt;

&lt;p&gt;新增helm源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm repo add loki https://grafana.github.io/loki/charts
$ helm repo update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用helm3部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install loki loki/loki-stack
# 安装到指定命名空间
# helm install loki loki/loki-stack -n monitoring
# 持久化 loki 的数据，避免 loki 重启后数据丢失
# helm install loki loki/loki-stack --set=&amp;quot;loki.persistence.enabled=ture,loki.persistence.size=100G&amp;quot;
# 部署 grafana
# helm install loki loki/loki-stack --set=&amp;quot;grafana.enabled=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们就可以看到对应启动了如下应用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get all -n monitoring | grep loki
pod/loki-0                                 1/1     Running   1          20h
pod/loki-promtail-8phlp                    1/1     Running   1          20h
service/loki                    NodePort    10.111.208.19    &amp;lt;none&amp;gt;        3100:31278/TCP               20h
service/loki-headless           ClusterIP   None             &amp;lt;none&amp;gt;        3100/TCP                     20h
daemonset.apps/loki-promtail   1         1         1       1            1           &amp;lt;none&amp;gt;                   20h
statefulset.apps/loki                1/1     20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如上使用了以 Daemonset 方式部署了promtail，使用Statefulset 方式部署loki，然后用service暴露给grafana。&lt;/p&gt;

&lt;h2 id=&#34;配置文件&#34;&gt;配置文件&lt;/h2&gt;

&lt;p&gt;上面启动了对应的应用，我们来看一下默认的启动情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti loki-0 -n monitoring -- sh
/ $ ps -ef
PID   USER     TIME  COMMAND
    1 loki      0:01 /usr/bin/loki -config.file=/etc/loki/loki.yaml
   23 loki      0:00 sh
   28 loki      0:00 ps -ef
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到就是使用二进制文件和配置文件进行启动，所以我们关键看一下配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/ $ cat /etc/loki/loki.yaml
auth_enabled: false
chunk_store_config:
  max_look_back_period: 0s
ingester:
  chunk_block_size: 262144
  chunk_idle_period: 3m
  chunk_retain_period: 1m
  lifecycler:
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  max_transfer_retries: 0
limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
schema_config:
  configs:
  - from: &amp;quot;2018-04-15&amp;quot;
    index:
      period: 168h
      prefix: index_
    object_store: filesystem
    schema: v9
    store: boltdb
server:
  http_listen_port: 3100
storage_config:
  boltdb:
    directory: /data/loki/index
  filesystem:
    directory: /data/loki/chunks
table_manager:
  retention_deletes_enabled: false
  retention_period: 0s/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们这边详细说明一下配置文件，配置文件主要有以下几块组成&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、target：[target: &amp;lt;string&amp;gt; | default = &amp;quot;all&amp;quot;]
2、auth_enabled：[auth_enabled: &amp;lt;boolean&amp;gt; | default = true] 启动验证，默认是启动的，如果需要关闭，需要设置为false
3、server：主要是配置loki的http模块，最常见的就是配置http的地址和端口
    # HTTP server listen host
    [http_listen_address: &amp;lt;string&amp;gt;]

    # HTTP server listen port
    [http_listen_port: &amp;lt;int&amp;gt; | default = 80]

    # gRPC server listen host
    [grpc_listen_address: &amp;lt;string&amp;gt;]

    # gRPC server listen port
    [grpc_listen_port: &amp;lt;int&amp;gt; | default = 9095]

    # Register instrumentation handlers (/metrics, etc.)
    [register_instrumentation: &amp;lt;boolean&amp;gt; | default = true]

    # Timeout for graceful shutdowns
    [graceful_shutdown_timeout: &amp;lt;duration&amp;gt; | default = 30s]

    # Read timeout for HTTP server
    [http_server_read_timeout: &amp;lt;duration&amp;gt; | default = 30s]

    # Write timeout for HTTP server
    [http_server_write_timeout: &amp;lt;duration&amp;gt; | default = 30s]

    # Idle timeout for HTTP server
    [http_server_idle_timeout: &amp;lt;duration&amp;gt; | default = 120s]

    # Max gRPC message size that can be received
    [grpc_server_max_recv_msg_size: &amp;lt;int&amp;gt; | default = 4194304]

    # Max gRPC message size that can be sent
    [grpc_server_max_send_msg_size: &amp;lt;int&amp;gt; | default = 4194304]

    # Limit on the number of concurrent streams for gRPC calls (0 = unlimited)
    [grpc_server_max_concurrent_streams: &amp;lt;int&amp;gt; | default = 100]

    # Log only messages with the given severity or above. Supported values [debug,
    # info, warn, error]
    [log_level: &amp;lt;string&amp;gt; | default = &amp;quot;info&amp;quot;]

    # Base path to server all API routes from (e.g., /v1/).
    [http_path_prefix: &amp;lt;string&amp;gt;]
4、distributor主要是配置loki的分发，目前只有ring轮询
    [ring: &amp;lt;ring_config&amp;gt;]
5、ring_config主要是用来发现和连接Ingesters
    kvstore:
      # The backend storage to use for the ring. Supported values are
      # consul, etcd, inmemory
      store: &amp;lt;string&amp;gt;

      # The prefix for the keys in the store. Should end with a /.
      [prefix: &amp;lt;string&amp;gt; | default = &amp;quot;collectors/&amp;quot;]

      # Configuration for a Consul client. Only applies if store
      # is &amp;quot;consul&amp;quot;
      consul:
        # The hostname and port of Consul.
        [host: &amp;lt;string&amp;gt; | duration = &amp;quot;localhost:8500&amp;quot;]

        # The ACL Token used to interact with Consul.
        [acl_token: &amp;lt;string&amp;gt;]

        # The HTTP timeout when communicating with Consul
        [http_client_timeout: &amp;lt;duration&amp;gt; | default = 20s]

        # Whether or not consistent reads to Consul are enabled.
        [consistent_reads: &amp;lt;boolean&amp;gt; | default = true]

      # Configuration for an ETCD v3 client. Only applies if
      # store is &amp;quot;etcd&amp;quot;
      etcd:
        # The ETCD endpoints to connect to.
        endpoints:
          - &amp;lt;string&amp;gt;

        # The Dial timeout for the ETCD connection.
        [dial_timeout: &amp;lt;duration&amp;gt; | default = 10s]

        # The maximum number of retries to do for failed ops to ETCD.
        [max_retries: &amp;lt;int&amp;gt; | default = 10]

    # The heartbeat timeout after which ingesters are skipped for
    # reading and writing.
    [heartbeat_timeout: &amp;lt;duration&amp;gt; | default = 1m]

    # The number of ingesters to write to and read from. Must be at least
    # 1.
    [replication_factor: &amp;lt;int&amp;gt; | default = 3]
6、querier主要是查询配置
    # Timeout when querying ingesters or storage during the execution of a
    # query request.
    [query_timeout: &amp;lt;duration&amp;gt; | default = 1m]

    # Limit of the duration for which live tailing requests should be
    # served.
    [tail_max_duration: &amp;lt;duration&amp;gt; | default = 1h]

    # Time to wait before sending more than the minimum successful query
    # requests.
    [extra_query_delay: &amp;lt;duration&amp;gt; | default = 0s]

    # Maximum lookback beyond which queries are not sent to ingester.
    # 0 means all queries are sent to ingester.
    [query_ingesters_within: &amp;lt;duration&amp;gt; | default = 0s]

    # Configuration options for the LogQL engine.
    engine:
      # Timeout for query execution
      [timeout: &amp;lt;duration&amp;gt; | default = 3m]

      # The maximum amount of time to look back for log lines. Only
      # applicable for instant log queries.
      [max_look_back_period: &amp;lt;duration&amp;gt; | default = 30s]
7、ingester_client配置ingester的客户端，其实就是distributor连接ingester的配置
    # Configures how connections are pooled
    pool_config:
      # Whether or not to do health checks.
      [health_check_ingesters: &amp;lt;boolean&amp;gt; | default = false]

      # How frequently to clean up clients for servers that have gone away after
      # a health check.
      [client_cleanup_period: &amp;lt;duration&amp;gt; | default = 15s]

      # How quickly a dead client will be removed after it has been detected
      # to disappear. Set this to a value to allow time for a secondary
      # health check to recover the missing client.
      [remotetimeout: &amp;lt;duration&amp;gt;]

    # The remote request timeout on the client side.
    [remote_timeout: &amp;lt;duration&amp;gt; | default = 5s]

    # Configures how the gRPC connection to ingesters work as a
    # client.
    [grpc_client_config: &amp;lt;grpc_client_config&amp;gt;]
8、grpc_client_config上面的client可以使用grpc，这个时候就要对grpc进行配置
    # The maximum size in bytes the client can receive
    [max_recv_msg_size: &amp;lt;int&amp;gt; | default = 104857600]

    # The maximum size in bytes the client can send
    [max_send_msg_size: &amp;lt;int&amp;gt; | default = 16777216]

    # Whether or not messages should be compressed
    [use_gzip_compression: &amp;lt;bool&amp;gt; | default = false]

    # Rate limit for gRPC client. 0 is disabled
    [rate_limit: &amp;lt;float&amp;gt; | default = 0]

    # Rate limit burst for gRPC client.
    [rate_limit_burst: &amp;lt;int&amp;gt; | default = 0]

    # Enable backoff and retry when a rate limit is hit.
    [backoff_on_ratelimits: &amp;lt;bool&amp;gt; | default = false]

    # Configures backoff when enabled.
    backoff_config:
      # Minimum delay when backing off.
      [min_period: &amp;lt;duration&amp;gt; | default = 100ms]

      # The maximum delay when backing off.
      [max_period: &amp;lt;duration&amp;gt; | default = 10s]

      # Number of times to backoff and retry before failing.
      [max_retries: &amp;lt;int&amp;gt; | default = 10]
9、ingester_config配置Ingesters，主要是配置Ingesters的范围
    # Configures how the lifecycle of the ingester will operate
    # and where it will register for discovery.
    [lifecycler: &amp;lt;lifecycler_config&amp;gt;]

    # Number of times to try and transfer chunks when leaving before
    # falling back to flushing to the store. Zero = no transfers are done.
    [max_transfer_retries: &amp;lt;int&amp;gt; | default = 10]

    # How many flushes can happen concurrently from each stream.
    [concurrent_flushes: &amp;lt;int&amp;gt; | default = 16]

    # How often should the ingester see if there are any blocks
    # to flush
    [flush_check_period: &amp;lt;duration&amp;gt; | default = 30s]

    # The timeout before a flush is cancelled
    [flush_op_timeout: &amp;lt;duration&amp;gt; | default = 10s]

    # How long chunks should be retained in-memory after they&#39;ve
    # been flushed.
    [chunk_retain_period: &amp;lt;duration&amp;gt; | default = 15m]

    # How long chunks should sit in-memory with no updates before
    # being flushed if they don&#39;t hit the max block size. This means
    # that half-empty chunks will still be flushed after a certain
    # period as long as they receive no further activity.
    [chunk_idle_period: &amp;lt;duration&amp;gt; | default = 30m]

    # The targeted _uncompressed_ size in bytes of a chunk block
    # When this threshold is exceeded the head block will be cut and compressed inside the chunk
    [chunk_block_size: &amp;lt;int&amp;gt; | default = 262144]

    # A target _compressed_ size in bytes for chunks.
    # This is a desired size not an exact size, chunks may be slightly bigger
    # or significantly smaller if they get flushed for other reasons (e.g. chunk_idle_period)
    # The default value of 0 for this will create chunks with a fixed 10 blocks,
    # A non zero value will create chunks with a variable number of blocks to meet the target size.
    [chunk_target_size: &amp;lt;int&amp;gt; | default = 0]

    # The compression algorithm to use for chunks. (supported: gzip, lz4, snappy)
    # You should choose your algorithm depending on your need:
    # - `gzip` highest compression ratio but also slowest decompression speed. (144 kB per chunk)
    # - `lz4` fastest compression speed (188 kB per chunk)
    # - `snappy` fast and popular compression algorithm (272 kB per chunk)
    [chunk_encoding: &amp;lt;string&amp;gt; | default = gzip]

    # Parameters used to synchronize ingesters to cut chunks at the same moment.
    # Sync period is used to roll over incoming entry to a new chunk. If chunk&#39;s utilization
    # isn&#39;t high enough (eg. less than 50% when sync_min_utilization is set to 0.5), then
    # this chunk rollover doesn&#39;t happen.
    [sync_period: &amp;lt;duration&amp;gt; | default = 0]
    [sync_min_utilization: &amp;lt;float&amp;gt; | Default = 0]

    # The maximum number of errors a stream will report to the user
    # when a push fails. 0 to make unlimited.
    [max_returned_stream_errors: &amp;lt;int&amp;gt; | default = 10]

    # The maximum duration of a timeseries chunk in memory. If a timeseries runs for longer than this the current chunk will be flushed to the store and a new chunk created.
    [max_chunk_age: &amp;lt;duration&amp;gt; | default = 1h]

    # How far in the past an ingester is allowed to query the store for data.
    # This is only useful for running multiple loki binaries with a shared ring with a `filesystem` store which is NOT shared between the binaries
    # When using any &amp;quot;shared&amp;quot; object store like S3 or GCS this value must always be left as 0
    # It is an error to configure this to a non-zero value when using any object store other than `filesystem`
    # Use a value of -1 to allow the ingester to query the store infinitely far back in time.
    [query_store_max_look_back_period: &amp;lt;duration&amp;gt; | default = 0]
10、lifecycler_config主要就是控制
    # Configures the ring the lifecycler connects to
    [ring: &amp;lt;ring_config&amp;gt;]

    # The number of tokens the lifecycler will generate and put into the ring if
    # it joined without transferring tokens from another lifecycler.
    [num_tokens: &amp;lt;int&amp;gt; | default = 128]

    # Period at which to heartbeat to the underlying ring.
    [heartbeat_period: &amp;lt;duration&amp;gt; | default = 5s]

    # How long to wait to claim tokens and chunks from another member when
    # that member is leaving. Will join automatically after the duration expires.
    [join_after: &amp;lt;duration&amp;gt; | default = 0s]

    # Minimum duration to wait before becoming ready. This is to work around race
    # conditions with ingesters exiting and updating the ring.
    [min_ready_duration: &amp;lt;duration&amp;gt; | default = 1m]

    # Name of network interfaces to read addresses from.
    interface_names:
      - [&amp;lt;string&amp;gt; ... | default = [&amp;quot;eth0&amp;quot;, &amp;quot;en0&amp;quot;]]

    # Duration to sleep before exiting to ensure metrics are scraped.
    [final_sleep: &amp;lt;duration&amp;gt; | default = 30s]
11、storage_config主要是存储的配置，可以是本地file，可以是s3等远程存储。这边有很多配置就不一一看了。
12、cache_config就是将数据放到缓存中，比如memche，redis等
13、chunk_store_config是对chunk存储的设置包括多长时间进行存储等
    # The cache configuration for storing chunks
    [chunk_cache_config: &amp;lt;cache_config&amp;gt;]

    # The cache configuration for deduplicating writes
    [write_dedupe_cache_config: &amp;lt;cache_config&amp;gt;]

    # The minimum time between a chunk update and being saved
    # to the store.
    [min_chunk_age: &amp;lt;duration&amp;gt;]

    # Cache index entries older than this period. Default is
    # disabled.
    [cache_lookups_older_than: &amp;lt;duration&amp;gt;]

    # Limit how long back data can be queried. Default is disabled.
    # This should always be set to a value less than or equal to
    # what is set in `table_manager.retention_period`.
    [max_look_back_period: &amp;lt;duration&amp;gt;]
14、schema_config主要是对时间进行设置，格式是period_config
    # The configuration for chunk index schemas.
    configs:
      - [&amp;lt;period_config&amp;gt;]
    # The date of the first day that index buckets should be created. Use
    # a date in the past if this is your only period_config, otherwise
    # use a date when you want the schema to switch over.
    [from: &amp;lt;daytime&amp;gt;]

    # store and object_store below affect which &amp;lt;storage_config&amp;gt; key is
    # used.

    # Which store to use for the index. Either aws, gcp, bigtable, bigtable-hashed,
    # cassandra, or boltdb.
    store: &amp;lt;string&amp;gt;

    # Which store to use for the chunks. Either aws, aws-dynamo, azure, gcp,
    # bigtable, gcs, cassandra, swift or filesystem. If omitted, defaults to the same
    # value as store.
    [object_store: &amp;lt;string&amp;gt;]

    # The schema version to use, current recommended schema is v11.
    schema: &amp;lt;string&amp;gt;

    # Configures how the index is updated and stored.
    index:
      # Table prefix for all period tables.
      prefix: &amp;lt;string&amp;gt;
      # Table period.
      [period: &amp;lt;duration&amp;gt; | default = 168h]
      # A map to be added to all managed tables.
      tags:
        [&amp;lt;string&amp;gt;: &amp;lt;string&amp;gt; ...]

    # Configured how the chunks are updated and stored.
    chunks:
      # Table prefix for all period tables.
      prefix: &amp;lt;string&amp;gt;
      # Table period.
      [period: &amp;lt;duration&amp;gt; | default = 168h]
      # A map to be added to all managed tables.
      tags:
        [&amp;lt;string&amp;gt;: &amp;lt;string&amp;gt; ...]

    # How many shards will be created. Only used if schema is v10 or greater.
    [row_shards: &amp;lt;int&amp;gt; | default = 16]
15、limits_config
    # Whether the ingestion rate limit should be applied individually to each
    # distributor instance (local), or evenly shared across the cluster (global).
    # The ingestion rate strategy cannot be overridden on a per-tenant basis.
    #
    # - local: enforces the limit on a per distributor basis. The actual effective
    #   rate limit will be N times higher, where N is the number of distributor
    #   replicas.
    # - global: enforces the limit globally, configuring a per-distributor local
    #   rate limiter as &amp;quot;ingestion_rate / N&amp;quot;, where N is the number of distributor
    #   replicas (it&#39;s automatically adjusted if the number of replicas change).
    #   The global strategy requires the distributors to form their own ring, which
    #   is used to keep track of the current number of healthy distributor replicas.
    [ingestion_rate_strategy: &amp;lt;string&amp;gt; | default = &amp;quot;local&amp;quot;]

    # Per-user ingestion rate limit in sample size per second. Units in MB.
    [ingestion_rate_mb: &amp;lt;float&amp;gt; | default = 4]

    # Per-user allowed ingestion burst size (in sample size). Units in MB.
    # The burst size refers to the per-distributor local rate limiter even in the
    # case of the &amp;quot;global&amp;quot; strategy, and should be set at least to the maximum logs
    # size expected in a single push request.
    [ingestion_burst_size_mb: &amp;lt;int&amp;gt; | default = 6]

    # Maximum length of a label name.
    [max_label_name_length: &amp;lt;int&amp;gt; | default = 1024]

    # Maximum length of a label value.
    [max_label_value_length: &amp;lt;int&amp;gt; | default = 2048]

    # Maximum number of label names per series.
    [max_label_names_per_series: &amp;lt;int&amp;gt; | default = 30]

    # Whether or not old samples will be rejected.
    [reject_old_samples: &amp;lt;bool&amp;gt; | default = false]

    # Maximum accepted sample age before rejecting.
    [reject_old_samples_max_age: &amp;lt;duration&amp;gt; | default = 336h]

    # Duration for a table to be created/deleted before/after it&#39;s
    # needed. Samples won&#39;t be accepted before this time.
    [creation_grace_period: &amp;lt;duration&amp;gt; | default = 10m]

    # Enforce every sample has a metric name.
    [enforce_metric_name: &amp;lt;boolean&amp;gt; | default = true]

    # Maximum number of active streams per user, per ingester. 0 to disable.
    [max_streams_per_user: &amp;lt;int&amp;gt; | default = 10000]

    # Maximum line size on ingestion path. Example: 256kb.
    # There is no limit when unset.
    [max_line_size: &amp;lt;string&amp;gt; | default = none ]

    # Maximum number of log entries that will be returned for a query. 0 to disable.
    [max_entries_limit: &amp;lt;int&amp;gt; | default = 5000 ]

    # Maximum number of active streams per user, across the cluster. 0 to disable.
    # When the global limit is enabled, each ingester is configured with a dynamic
    # local limit based on the replication factor and the current number of healthy
    # ingesters, and is kept updated whenever the number of ingesters change.
    [max_global_streams_per_user: &amp;lt;int&amp;gt; | default = 0]

    # Maximum number of chunks that can be fetched by a single query.
    [max_chunks_per_query: &amp;lt;int&amp;gt; | default = 2000000]

    # The limit to length of chunk store queries. 0 to disable.
    [max_query_length: &amp;lt;duration&amp;gt; | default = 0]

    # Maximum number of queries that will be scheduled in parallel by the
    # frontend.
    [max_query_parallelism: &amp;lt;int&amp;gt; | default = 14]

    # Cardinality limit for index queries
    [cardinality_limit: &amp;lt;int&amp;gt; | default = 100000]

    # Maximum number of stream matchers per query.
    [max_streams_matchers_per_query: &amp;lt;int&amp;gt; | default = 1000]

    # Feature renamed to &#39;runtime configuration&#39;, flag deprecated in favor of -runtime-config.file (runtime_config.file in YAML)
    [per_tenant_override_config: &amp;lt;string&amp;gt;]

    # Feature renamed to &#39;runtime configuration&#39;, flag deprecated in favor of -runtime-config.reload-period (runtime_config.period in YAML)
    [per_tenant_override_period: &amp;lt;duration&amp;gt; | default = 10s]
16、frontend_worker_config
    # Address of query frontend service, in host:port format.
    # CLI flag: -querier.frontend-address
    [frontend_address: &amp;lt;string&amp;gt; | default = &amp;quot;&amp;quot;]

    # Number of simultaneous queries to process.
    # CLI flag: -querier.worker-parallelism
    [parallelism: &amp;lt;int&amp;gt; | default = 10]

    # How often to query DNS.
    # CLI flag: -querier.dns-lookup-period
    [dns_lookup_duration: &amp;lt;duration&amp;gt; | default = 10s]

    grpc_client_config:
      # gRPC client max receive message size (bytes).
      # CLI flag: -querier.frontend-client.grpc-max-recv-msg-size
      [max_recv_msg_size: &amp;lt;int&amp;gt; | default = 104857600]

      # gRPC client max send message size (bytes).
      # CLI flag: -querier.frontend-client.grpc-max-send-msg-size
      [max_send_msg_size: &amp;lt;int&amp;gt; | default = 16777216]

      # Use compression when sending messages.
      # CLI flag: -querier.frontend-client.grpc-use-gzip-compression
      [use_gzip_compression: &amp;lt;boolean&amp;gt; | default = false]

      # Rate limit for gRPC client; 0 means disabled.
      # CLI flag: -querier.frontend-client.grpc-client-rate-limit
      [rate_limit: &amp;lt;float&amp;gt; | default = 0]

      # Rate limit burst for gRPC client.
      # CLI flag: -querier.frontend-client.grpc-client-rate-limit-burst
      [rate_limit_burst: &amp;lt;int&amp;gt; | default = 0]

      # Enable backoff and retry when we hit ratelimits.
      # CLI flag: -querier.frontend-client.backoff-on-ratelimits
      [backoff_on_ratelimits: &amp;lt;boolean&amp;gt; | default = false]

      backoff_config:
        # Minimum delay when backing off.
        # CLI flag: -querier.frontend-client.backoff-min-period
        [min_period: &amp;lt;duration&amp;gt; | default = 100ms]

        # Maximum delay when backing off.
        # CLI flag: -querier.frontend-client.backoff-max-period
        [max_period: &amp;lt;duration&amp;gt; | default = 10s]

        # Number of times to backoff and retry before failing.
        # CLI flag: -querier.frontend-client.backoff-retries
        [max_retries: &amp;lt;int&amp;gt; | default = 10]
17、table_manager_config，provision_config都是用于DynamoDB。
18、auto_scaling_config用于DynamoDB的自动伸缩
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体可以参考&lt;a href=&#34;https://github.com/grafana/loki/tree/v1.5.0/docs/configuration&#34;&gt;官网&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;loki的配置还是比较复杂的，下面我们再来看一下promtail的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat promtail.yaml
client:
  backoff_config:
    max_period: 5s
    max_retries: 20
    min_period: 100ms
  batchsize: 102400
  batchwait: 1s
  external_labels: {}
  timeout: 10s
positions:
  filename: /run/promtail/positions.yaml
server:
  http_listen_port: 3101
target_config:
  sync_period: 10s
scrape_configs:
- job_name: kubernetes-pods-name
  pipeline_stages:
    - docker: {}
  kubernetes_sd_configs:
  - role: pod
  relabel_configs:
  - source_labels:
    - __meta_kubernetes_pod_label_name
    target_label: __service__
  - source_labels:
    - __meta_kubernetes_pod_node_name
    target_label: __host__
  - action: drop
    regex: &#39;&#39;
    source_labels:
    - __service__
  - action: labelmap
    regex: __meta_kubernetes_pod_label_(.+)
  - action: replace
    replacement: $1
    separator: /
    source_labels:
    - __meta_kubernetes_namespace
    - __service__
    target_label: job
  - action: replace
    source_labels:
    - __meta_kubernetes_namespace
    target_label: namespace
  - action: replace
    source_labels:
    - __meta_kubernetes_pod_name
    target_label: pod
  - action: replace
    source_labels:
    - __meta_kubernetes_pod_container_name
    target_label: container
  - replacement: /var/log/pods/*$1/*.log
    separator: /
    source_labels:
    - __meta_kubernetes_pod_uid
    - __meta_kubernetes_pod_container_name
    target_label: __path__
- job_name: kubernetes-pods-app
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;promtail的配置和prometheus很像，我们也简单说明一下，promtail的复杂配置分为四个部分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server_config 配置promtail作为一个服务器。开启一个http端口
client_config 配置promtail怎么连接loki，它作为loki的客户端
position_config 指明promtail的配置文件在什么地方生成，重启的时候会读取一些信息
scrape_config 配置一些常用的抓取策略
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们主要配置的地方，就是scrape_config 。它又分为几种常见的抓取方式，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journal_config
syslog_config
relabel_config
static_config
file_sd_config
kubernetes_sd_config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于我们来说，最常使用的就是static_config，比如指定业务的某个日志文件。这部分的描述很长，具体可以参考github文档。&lt;/p&gt;

&lt;p&gt;一个配置文件中，是可以针对不同类型的日志文件同时进行监控的。比如下面的长长的配置文件，就加入了三个抓取策略。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://localhost:3100/loki/api/v1/push

scrape_configs:
  - job_name: journal
    journal:
      max_age: 12h
      labels:
        job: systemd-journal
    relabel_configs:
      - source_labels: [&#39;__journal__systemd_unit&#39;]
        target_label: &#39;unit&#39;
  - job_name: system
    pipeline_stages:
    static_configs:
    - labels:
       job: varlogs
       host: yourhost
       __path__: /var/log/*.log
  - job_name: biz001
    pipeline_stages:
    - match:
       selector: &#39;{app=&amp;quot;test&amp;quot;}&#39;
       stages:
       - regex:
          expression: &#39;.*level=(?P&amp;lt;level&amp;gt;[a-zA-Z]+).*ts=(?P&amp;lt;timestamp&amp;gt;[T\d-:.Z]*).*component=(?P&amp;lt;component&amp;gt;[a-zA-Z]+)&#39;
       - labels:
          level:
          component:
          ts:
          timestrap:
    static_configs:
    - labels:
       job: biz001
       app: test
       node: 001
       host: localhost
       __path__: /alertmgr/dingtalk/nohup.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们配置了三个job（概念见普罗米修斯），journal，system和biz001。尤其注意biz001的配置，这代表了我们对一些日志的通用配置方式。&lt;/p&gt;

&lt;p&gt;首先，看一下biz001的日志格式。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level=info ts=2020-04-30T01:20:38.631Z caller=entry.go:22 component=web http_scheme=http http_proto=HTTP/1.1 http_method=POST remote_addr=[::1]:57710 user_agent=Alertmanager/0.20.0 uri=http://localhost:8060/dingtalk/webhook1/send resp_status=200 resp_bytes_length=2 resp_elapsed_ms=5207.398549 msg=&amp;quot;request complete&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在将日志传送到Loki之前，promtail可以对其进行一系列的操作。比如过滤一些日志，提取一些label，替换一些日志的内容等。&lt;/p&gt;

&lt;p&gt;对于这部分的操作，现有的日志收集工具都搞了一套自己的，而且都很难用。&lt;/p&gt;

&lt;p&gt;比如我们用来解析我们固定格式的nginx日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ps -ef | grep promtail
root     14449 14356  0 21:06 pts/0    00:00:00 grep promtail
root     28509     1  0 Jul21 ?        00:23:12 /opt/promes/loki/promtail-linux-amd64 --config.file=/opt/promes/loki/nginx.yaml
[root@promessitweb19 ~]# cat /opt/promes/loki/nginx.yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /opt/promes/loki/positions.yaml

clients:
  - url: http://10.243.51.50:3100/loki/api/v1/push

scrape_configs:
- job_name: nginx
  static_configs:
  - targets:
      - localhost
    labels:
      job: nginxAccess
      __path__: /opt/rsync_log/access_http.log
      ip: &amp;quot;10.243.58.14&amp;quot;
      appId: PROMES
      softType: blackbox
  pipeline_stages:
  - match:
      selector: &#39;{app=&amp;quot;nginx&amp;quot;}&#39;
      stages:
      - regex:
          expression: &#39;^(?P&amp;lt;remote_addr&amp;gt;\\S+)   (?P&amp;lt;http_x_forwarded_for&amp;gt;\\S+)  (?P&amp;lt;http_x_forwarded_for2&amp;gt;\\S+) (?P&amp;lt;http_x_forwarded_for3&amp;gt;\\S+) (?P&amp;lt;time_iso8601&amp;gt;\\S+)  (?P&amp;lt;request_method&amp;gt;\\S+)    &amp;quot;(?P&amp;lt;document_uri&amp;gt;\\S+)&amp;quot;    &amp;quot;(?P&amp;lt;query_string&amp;gt;\\S+)&amp;quot;    (?P&amp;lt;request_http_protocol&amp;gt;\\S+) (?P&amp;lt;status&amp;gt;\\d{3}|-)    (?P&amp;lt;body_bytes_sent&amp;gt;\\d{3}|-)   (?P&amp;lt;request_time&amp;gt;\\S+)  &amp;quot;(?P&amp;lt;http_referer&amp;gt;\\S+)&amp;quot;    &amp;quot;(?P&amp;lt;user_agent&amp;gt;\\S+)&amp;quot;  traceId:(?P&amp;lt;traceId&amp;gt;\\S+),spanId:(?P&amp;lt;spanId&amp;gt;\\S+)   (?P&amp;lt;server_addr&amp;gt;\\S+)   (?P&amp;lt;hostname&amp;gt;\\S+)  (?P&amp;lt;host&amp;gt;\\S+)  (?P&amp;lt;remote_port&amp;gt;\\S+)   (?P&amp;lt;server_port&amp;gt;\\S+)   &amp;quot;(?P&amp;lt;upstream_addr&amp;gt;\\S+)&amp;quot;   &amp;quot;(?P&amp;lt;upstream_status&amp;gt;\\S+)&amp;quot; &amp;quot;(?P&amp;lt;upstream_response_time&amp;gt;\\S+)&amp;quot;  (?P&amp;lt;version&amp;gt;\\S+)?$&#39;
      - labels:
          remote_addr:
          http_x_forwarded_for:
          http_x_forwarded_for2:
          http_x_forwarded_for3:
          timestamp:
          request_method:
          document_uri:
          query_string:
          request_http_protocol:
          status:
          body_bytes_sent:
          request_time:
          http_referer:
          user_agent:
          traceId:
          spanId:
          server_addr:
          hostname:
          host:
          remote_port:
          server_port:
          upstream_addr:
          upstream_status:
          upstream_response_time:
          version:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;物理部署&#34;&gt;物理部署&lt;/h2&gt;

&lt;p&gt;物理部署很简单，可以直接下载二进制文件，官方还提供来repo，我们还可以编译&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/grafana/loki $GOPATH/src/github.com/grafana/loki
$ cd $GOPATH/src/github.com/grafana/loki
$ make loki
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后直接用二进制文件加配置文件进行启动就可以了，配置文件在/etc/loki/promtail.yaml and /etc/loki/loki.yaml。&lt;/p&gt;

&lt;h2 id=&#34;基本使用&#34;&gt;基本使用&lt;/h2&gt;

&lt;p&gt;下面我们就可以到grafana界面进行操作了，进入 grafana 界面，添加 loki 作为数据源，grafana原生就是支持loki的，所以直接添加loki 在集群中的地址，比如: &lt;a href=&#34;http://loki.monitoring.svc.cluster.local:3100&#34;&gt;http://loki.monitoring.svc.cluster.local:3100&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据源添加好了，我们就可以开始查询分析日志了，点击 Explore，下拉选择 loki 作为数据源，切到 Logs 模式(不用 Metrics 模式)，在 Log labels 按钮那里就能通过 label 筛选日志了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;选择器&#34;&gt;选择器&lt;/h2&gt;

&lt;p&gt;对于查询表达式的标签部分，将其包装在花括号中{}，然后使用键值对的语法来选择标签，多个标签表达式用逗号分隔，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{app=&amp;quot;mysql&amp;quot;,name=&amp;quot;mysql-backup&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前支持以下标签匹配运算符：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;=等于
!=不相等
=~正则表达式匹配
!~不匹配正则表达式
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{name=~&amp;quot;mysql.+&amp;quot;}
{name!~&amp;quot;mysql.+&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;适用于Prometheus标签选择器规则同样也适用于Loki日志流选择器,可以查看官网的&lt;a href=&#34;https://github.com/grafana/loki/blob/v1.5.0/docs/logql.md&#34;&gt;logQL&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;h2 id=&#34;promtail&#34;&gt;Promtail&lt;/h2&gt;

&lt;p&gt;promtail 可以理解为采集日志的 “Prometheus”. 它最巧妙的设计是完全复用了 Prometheus 的服务发现机制与 label 机制.&lt;/p&gt;

&lt;p&gt;以 Kubernetes 服务发现为例, Prometheus 可以通过 Pod 的 Annotations 与 Labels 等信息来确定 Pod 是否需要抓取指标, 假如要的话 Pod 的指标暴露在哪个端口上, 以及这个 Pod 本身有哪些 label, 即 target label.&lt;/p&gt;

&lt;p&gt;确定了这些信息之后, Prometheus 就可以去拉应用的指标了. 同时, 这些指标都会被打上 target label, 用于标注指标的来源. 等到在查询的时候, 我们就可以通过 target label, 比方说 pod_name=foo-123512 或 service=user-service 来获取特定的一个或一组 Pod 上的指标信息.&lt;/p&gt;

&lt;p&gt;promtail 是一样的道理. 它也是通过 Pod 的一些元信息来确定该 Pod 的日志文件位置, 同时为日志打上特定的 target label. 但要注意, 这个 label 不是标注在每一行日志事件上的, 而是被标注在”整个日志”上的. 这里”整个日志”在 loki 中抽象为 stream(日志流). 这就是 loki 文档中所说的”不索引日志, 只索引日志流”. 最终在查询端, 我们通过这些 label 就可以快速查询一个或一组特定的 stream.&lt;/p&gt;

&lt;p&gt;服务发现部分的代码非常直白, 可以去 pkg/promtail/targetmanager.go 中自己看一下, 提两个实现细节:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;promtail 要求所有 target 都跟自己属于同一个 node, 处于其它 node 上的 target 会被忽略;
promtail 使用 target 的 __path__ label 来确定日志路径;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过服务发现确定要收集的应用以及应用的日志路径后, promtail 就开始了真正的日志收集过程. 这里分三步:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、用 fsnotify 监听对应目录下的文件创建与删除(处理 log rolling)
2、对每个活跃的日志文件起一个 goroutine 进行类似 tail -f 的读取, 读取到的内容发送给 channel
3、一个单独的 goroutine 会解析 channel 中的日志行, 分批发送给 loki 的 backend
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;监听&#34;&gt;监听&lt;/h3&gt;

&lt;p&gt;fsnotify负责监听&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    select {
    case event := &amp;lt;-t.watcher.Events:
        switch event.Op {
        case fsnotify.Create:
            // protect against double Creates.
            if _, ok := t.tails[event.Name]; ok {
                level.Info(t.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;got &#39;create&#39; for existing file&amp;quot;, &amp;quot;filename&amp;quot;, event.Name)
                continue
            }

            // newTailer 中会启动一个 goroutine 来读目标文件
            tailer := newTailer(t.logger, t.handler, t.positions, t.path, event.Name)
            t.tails[event.Name] = tailer

        case fsnotify.Remove:
            tailer, ok := t.tails[event.Name]
            if ok {
                // 关闭 tailer
                helpers.LogError(&amp;quot;stopping tailer&amp;quot;, tailer.stop)
                delete(t.tails, event.Name)
            }
        }
    case err := &amp;lt;-t.watcher.Errors:
        level.Error(t.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error from fswatch&amp;quot;, &amp;quot;error&amp;quot;, err)
    case &amp;lt;-t.quit:
        return
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个for循环，一直来处理对应目录下的文件创建与删除的事件。&lt;/p&gt;

&lt;h3 id=&#34;tail日志&#34;&gt;tail日志&lt;/h3&gt;

&lt;p&gt;newTailer() 这个方法中启动的日志文件读取逻辑&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unc newTailer() {
    tail := tail.TailFile(path, tail.Config{
        Follow: true,
        Location: &amp;amp;tail.SeekInfo{
            Offset: positions.Get(path),
            Whence: 0,
        },
    })

    tailer := ...
    go tailer.run()
}

func (t *tailer) run() {
    for {
        select {
        case &amp;lt;-positionWait.C:
            // 定时同步当前读取位置
            pos := t.tail.Tell()
            t.positions.Put(t.path, pos)

        case line, ok := &amp;lt;-t.tail.Lines:
            // handler.Handle() 中是一些日志行的预处理逻辑, 最后将日志行转化为 `Entry` 对象扔进 channel
            if err := t.handler.Handle(model.LabelSet{}, line.Time, line.Text); err != nil {
                level.Error(t.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error handling line&amp;quot;, &amp;quot;error&amp;quot;, err)
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里直接调用了 hpcloud/tail 这个包来完成文件的 tail 操作. hpcloud/tail 的内部实现中, 在读到 EOF 之后, 同样调用了 fsnotify 来获取新内容写入的通知. fsnotify 这个包内部则是依赖了 inotify_init 和 inotify_add_watch 这两个系统调用。&lt;/p&gt;

&lt;h3 id=&#34;日志channel&#34;&gt;日志channel&lt;/h3&gt;

&lt;p&gt;这里有一个单独的 goroutine 会读取所有 tailer 通过 channel 传过来的日志(Entry对象), 然后按批发送给 loki&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    // 每次发送之后要重置计时器
    maxWait.Reset(c.cfg.BatchWait)
    select {
    case &amp;lt;-c.quit:
        return
    case e := &amp;lt;-c.entries:
        // Batch 足够大之后, 执行发送逻辑
        if batchSize+len(e.Line) &amp;gt; c.cfg.BatchSize {
            c.send(batch)
            // 重置 Batch
            batchSize = 0
            batch = map[model.Fingerprint]*logproto.Stream{}
        }

        // 收到 Entry, 先写进 Batch 当中
        batchSize += len(e.Line)

        // 每个 entry 要根据 label 放进对应的日志流(Stream)中
        fp := e.labels.FastFingerprint()
        stream, ok := batch[fp]
        if !ok {
            stream = &amp;amp;logproto.Stream{
                Labels: e.labels.String(),
            }
            batch[fp] = stream
        }
        stream.Entries = append(stream.Entries, e.Entry)

    case &amp;lt;-maxWait.C:
        // 到达每个批次的最大等待时间, 同样执行发送
        if len(batch) &amp;gt; 0 {
            c.send(batch);
            batchSize = 0
            batch = map[model.Fingerprint]*logproto.Stream{}
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用 channel + select 写 batch 逻辑真的挺优雅, 简单易读.&lt;/p&gt;

&lt;h2 id=&#34;loki&#34;&gt;loki&lt;/h2&gt;

&lt;p&gt;loki的基本架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;distributor&#34;&gt;Distributor&lt;/h3&gt;

&lt;p&gt;我们都知道promtail封装后label后的log数据发生到loki，Distributor就是第一个接收日志的组件。由于日志的写入量可能很大，所以不能在它们传入时将它们写入数据库。这会毁掉数据库。我们需要批处理和压缩数据。&lt;/p&gt;

&lt;p&gt;Loki通过构建压缩数据块来实现这一点，方法是在日志进入时对其进行gzip操作，组件ingester是一个有状态的组件，负责构建和刷新chunck，当chunk达到一定的数量或者时间后，刷新到存储中去。每个流的日志对应一个ingester,当日志到达Distributor后，根据元数据和hash算法计算出应该到哪个ingester上面。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们具体看一下promtail 的日志写入请求, 请求体由 protobuf 编码, 格式如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 一次写入请求, 包含多段日志流
type PushRequest struct {
    Streams []*Stream `protobuf:&amp;quot;bytes,1,rep,name=streams&amp;quot; json:&amp;quot;streams,omitempty&amp;quot;`
}
// 一段日志流, 包含它的 label, 以及这段日志流当中的每个日志事件: Entry
type Stream struct {
    Labels  string  `protobuf:&amp;quot;bytes,1,opt,name=labels,proto3&amp;quot; json:&amp;quot;labels,omitempty&amp;quot;`
    Entries []Entry `protobuf:&amp;quot;bytes,2,rep,name=entries&amp;quot; json:&amp;quot;entries&amp;quot;`
}
// 一个日志事件, 包含时间戳与内容
type Entry struct {
    Timestamp time.Time `protobuf:&amp;quot;bytes,1,opt,name=timestamp,stdtime&amp;quot; json:&amp;quot;timestamp&amp;quot;`
    Line      string    `protobuf:&amp;quot;bytes,2,opt,name=line,proto3&amp;quot; json:&amp;quot;line,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;distributor 收到请求后, 会将一个 PushRequest 中的 Stream 根据 labels 拆分成多个 PushRequest, 这个过程使用一致性哈希:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;streams := make([]streamTracker, len(req.Streams))
keys := make([]uint32, 0, len(req.Streams))
for i, stream := range req.Streams {
    // 获取每个 stream 的 label hash
    keys = append(keys, tokenFor(userID, stream.Labels))
    streams[i].stream = stream
}

// 根据 label hash 到 hash ring 上获取对应的 ingester 节点
// 这里的节点指 hash ring 上的节点, 一个节点可能有多个对等的 ingester 副本来做 HA
replicationSets := d.ring.BatchGet(keys, ring.Write)

// 将 Stream 按对应的 ingester 节点进行分组
samplesByIngester := map[string][]*streamTracker{}
ingesterDescs := map[string]ring.IngesterDesc{}
for i, replicationSet := range replicationSets {
    for _, ingester := range replicationSet.Ingesters {
        samplesByIngester[ingester.Addr] = append(samplesByIngester[ingester.Addr], &amp;amp;streams[i])
        ingesterDescs[ingester.Addr] = ingester
    }
}

for ingester, samples := range samplesByIngester {
    // 每组 Stream[] 又作为一个 PushRequest, 下发给对应的 ingester 节点
    d.sendSamples(localCtx, ingester, samples, &amp;amp;tracker)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 All in One 的运行模式中, hash ring 直接存储在内存中. 在生产环境, 由于要起多个 distributor 节点做高可用, 这个 hash ring 会存储到外部的 Consul 集群中.&lt;/p&gt;

&lt;h3 id=&#34;ingester&#34;&gt;Ingester&lt;/h3&gt;

&lt;p&gt;ingester接收到日志并开始构建chunk:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;基本上就是将日志进行压缩并附加到chunk上面。一旦chunk“填满”（数据达到一定数量或者过了一定期限），ingester将其刷新到数据库。我们对块和索引使用单独的数据库，因为它们存储的数据类型不同。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;刷新一个chunk之后，ingester然后创建一个新的空chunk并将新条目添加到该chunk中。&lt;/p&gt;

&lt;p&gt;我们再重代码层来分析一下，ingester 接收 distributor 下发的 PushRequest, 也就是多段日志流([]Entry). 在 ingester 内部会先将收到的 []Entry Append 到内存中的 Chunk 流([]Chunk). 同时会有一组 goroutine 异步将 Chunk 流存储到对象存储当中:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki9&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第一个 Append 过程很关键&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (i *instance) Push(ctx context.Context, req *logproto.PushRequest) error {
    for _, s := range req.Streams {
        // 将收到的日志流 Append 到内存中的日志流上, 同样地, 日志流按 label hash 索引
        fp := client.FastFingerprint(req.labels)
        stream, ok := i.streams[fp]
        if !ok {
            stream = newStream(fp, req.labels)
            // 这个过程中, 还会维护日志流的倒排索引(label -&amp;gt; stream)
            i.index.Add(labels, fp)
            i.streams[fp] = stream
        }
        stream.Push(ctx, s.Entries)
    }
    return nil
}

func (s *stream) Push(_ context.Context, entries []logproto.Entry) error {
    for i := range entries {
        // 假如当前 Chunk 已经关闭或者已经到达设定的最大 Chunk 大小, 则再创建一个新的 Chunk
        if s.chunks[0].closed || !s.chunks[0].chunk.SpaceFor(&amp;amp;entries[i]) {
            s.chunks = append(s.chunks, chunkDesc{
                chunk: chunkenc.NewMemChunk(chunkenc.EncGZIP),
            })
        }
        s.chunks[len(s.chunks)-1].chunk.Append(&amp;amp;entries[i])
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chunk 其实就是多条日志构成的压缩包. 将日志压成 Chunk 的意义是可以直接存入对象存储, 而对象存储是最便宜的(便宜是 loki 的核心目标之一). 在 一个 Chunk 到达指定大小之前它就是 open 的, 会不断 Append 新的日志(Entry) 到里面. 而在达到大小之后, Chunk 就会关闭等待持久化(强制持久化也会关闭 Chunk, 比如关闭 ingester 实例时就会关闭所有的 Chunk并持久化).&lt;/p&gt;

&lt;p&gt;对 Chunk 的大小控制是一个调优要点:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;假如 Chunk 容量过小: 首先是导致压缩效率不高. 同时也会增加整体的 Chunk 数量, 导致倒排索引过大. 最后, 对象存储的操作次数也会变多, 带来额外的性能开销;
假如 Chunk 过大: 一个 Chunk 的 open 时间会更长, 占用额外的内存空间, 同时, 也增加了丢数据的风险. 最后, Chunk 过大也会导致查询读放大, 比方说查一小时的数据却要下载整天的 Chunk;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;丢数据问题: 所有 Chunk 要在 close 之后才会进行存储. 因此假如 ingester 异常宕机, 处于 open 状态的 Chunk, 以及 close 了但还没有来得及持久化的 Chunk 数据都会丢失. 从这个角度来说, ingester 其实也是 stateful 的, 在生产中可以通过给 ingester 跑多个副本来解决这个问题. 另外, ingester 里似乎还没有写 WAL, 这感觉是一个 PR 机会, 可以练习一下写存储的基本功.&lt;/p&gt;

&lt;p&gt;异步存储过程就很简单了, 是一个一对多的生产者消费者模型:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 一个 goroutine 将所有的待存储的 chunks enqueue
func (i *Ingester) sweepStream(instance *instance, stream *stream, immediate bool) {

    // 有一组待存储的队列(默认16个), 取模找一个队列把要存储的 chunk 的引用塞进去
    flushQueueIndex := int(uint64(stream.fp) % uint64(i.cfg.ConcurrentFlushes))
    firstTime, _ := stream.chunks[0].chunk.Bounds()
    i.flushQueues[flushQueueIndex].Enqueue(&amp;amp;flushOp{
        model.TimeFromUnixNano(firstTime.UnixNano()), instance.instanceID,
        stream.fp, immediate,
    })
}

// 每个队列都有一个 goroutine 作为消费者在 dequeue
func (i *Ingester) flushLoop(j int) {
    for {
        op := i.flushQueues[j].Dequeue()
        // 实际的存储操作在这个方法中, 存储完成后, Chunk 会被清理掉
        i.flushUserSeries(op.userID, op.fp, op.immediate)

        // 存储失败的 chunk 会重新塞回队列中
        if op.immediate &amp;amp;&amp;amp; err != nil {
            op.from = op.from.Add(flushBackoff)
            i.flushQueues[j].Enqueue(op)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后是清理过程, 同样是一个单独的 goroutine 定时在跑. ingester 里的所有 Chunk 会在持久化之后隔一小段时间才被清理掉. 这个”一小段时间”由 chunk-retain-time 参数进行控制(默认 15 分钟). 这么做是为了加速热点数据的读取(真正被人看的日志中, 有99%都是生成后的一小段时间内被查看的).&lt;/p&gt;

&lt;h3 id=&#34;querier&#34;&gt;Querier&lt;/h3&gt;

&lt;p&gt;读取就非常简单了，由Querier负责给定一个时间范围和标签选择器，Querier查看索引以确定哪些块匹配，并通过greps将结果显示出来。它还从Ingester获取尚未刷新的最新数据，合并后返回。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;合并返回日志的时候，loki 里用了堆, 时间正序就用最小堆, 时间逆序就用最大堆:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这部分代码实现了一个简单的二叉堆, MinHeap 和 MaxHeap 实现了相反的 `Less()` 方法
type iteratorHeap []EntryIterator
func (h iteratorHeap) Len() int            { return len(h) }
func (h iteratorHeap) Swap(i, j int)       { h[i], h[j] = h[j], h[i] }
func (h iteratorHeap) Peek() EntryIterator { return h[0] }
func (h *iteratorHeap) Push(x interface{}) {
    *h = append(*h, x.(EntryIterator))
}
func (h *iteratorHeap) Pop() interface{} {
    old := *h
    n := len(old)
    x := old[n-1]
    *h = old[0 : n-1]
    return x
}
type iteratorMinHeap struct {
    iteratorHeap
}
func (h iteratorMinHeap) Less(i, j int) bool {
    return h.iteratorHeap[i].Entry().Timestamp.Before(h.iteratorHeap[j].Entry().Timestamp)
}
type iteratorMaxHeap struct {
    iteratorHeap
}
func (h iteratorMaxHeap) Less(i, j int) bool {
    return h.iteratorHeap[i].Entry().Timestamp.After(h.iteratorHeap[j].Entry().Timestamp)
}

// 将一组 Stream 的 iterator 合并成一个 HeapIterator
func NewHeapIterator(is []EntryIterator, direction logproto.Direction) EntryIterator {
    result := &amp;amp;heapIterator{}
    switch direction {
    case logproto.BACKWARD:
        result.heap = &amp;amp;iteratorMaxHeap{}
    case logproto.FORWARD:
        result.heap = &amp;amp;iteratorMinHeap{}
    default:
        panic(&amp;quot;bad direction&amp;quot;)
    }
    // pre-next each iterator, drop empty.
    for _, i := range is {
        result.requeue(i)
    }
    return result
}

func (i *heapIterator) requeue(ei EntryIterator) {
    if ei.Next() {
        heap.Push(i.heap, ei)
        return
    }
    if err := ei.Error(); err != nil {
        i.errs = append(i.errs, err)
    }
    helpers.LogError(&amp;quot;closing iterator&amp;quot;, ei.Close)
}

func (i *heapIterator) Next() bool {
    if i.curr != nil {
        i.requeue(i.curr)
    }
    if i.heap.Len() == 0 {
        return false
    }
    i.curr = heap.Pop(i.heap).(EntryIterator)
    currEntry := i.curr.Entry()
    // keep popping entries off if they match, to dedupe
    for i.heap.Len() &amp;gt; 0 {
        next := i.heap.Peek()
        nextEntry := next.Entry()
        if !currEntry.Equal(nextEntry) {
            break
        }

        next = heap.Pop(i.heap).(EntryIterator)
        i.requeue(next)
    }
    return true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;1、Loki的索引存储可以是cassandra/bigtable/dynamodb来进行扩展，chuncks可以是各种对象存储，放入对象存储中进行扩展。&lt;/p&gt;

&lt;p&gt;2、Querier和Distributor都是无状态的组件，可以水平扩展，可以使用负载均衡。&lt;/p&gt;

&lt;p&gt;3、对于ingester他虽然是有状态的但是，当新的节点加入或者减少，整节点间的chunk会重新分配，已适应新的散列环。这些信息需要存储到etcd或者consul等第三方工具中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.5.20&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;loki今天发布了1.5.0版本！引入了名为boltdb-shipper的新索引选项，这个新索引允许您仅使用对象存储（S3，GCS，文件系统等）来运行Loki。您不再需要单独的专用索引存储（DynamoDB，Bigtable，Cassandra等）！&lt;/p&gt;

&lt;p&gt;该boltdb-shipper索引使用内存中的boltdb索引，但会定期将快照发送到对象存储。这允许通过对象存储共享索引信息。&lt;/p&gt;

&lt;p&gt;将来可扩展可以通过boltdb-shipper索引和memberlist的gossip来完成集群功能。&lt;/p&gt;

&lt;p&gt;在云存储上，ring的信息可以通过gossip协议来进行同步。可以看一下下面的这个配置，基于s3和memberlist的可扩展模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;auth_enabled: false

server:
  http_listen_port: 3100

distributor:
  ring:
    store: memberlist

ingester:
  lifecycler:
    ring:
      kvstore:
        store: memberlist
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 5m
  chunk_retain_period: 30s

memberlist:
  abort_if_cluster_join_fails: false

  # Expose this port on all distributor, ingester
  # and querier replicas.
  bind_port: 7946

  # You can use a headless k8s service for all distributor,
  # ingester and querier components.
  join_members:
  - loki-gossip-ring.loki.svc.cluster.local:7946

  max_join_backoff: 1m
  max_join_retries: 10
  min_join_backoff: 1s

schema_config:
  configs:
  - from: 2020-05-15
    store: boltdb-shipper
    object_store: s3
    schema: v11
    index:
      prefix: index_
      period: 168h

storage_config:
 boltdb_shipper:
   active_index_directory: /loki/index
   cache_location: /loki/index_cache
   resync_interval: 5s
   shared_store: s3

 aws:
   s3: s3://access_key:secret_access_key@custom_endpoint/bucket_name
   s3forcepathstyle: true

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Grok_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/grok_exporter/</link>
          <pubDate>Fri, 10 Jan 2020 17:53:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/grok_exporter/</guid>
          <description>&lt;p&gt;grok_exporter是基于logstash的grok的插件开发的日志分析工具，可以分析非结构化日志根据正则表达式进行匹配，然后生成适合prometheus的规则规范的metrics。&lt;/p&gt;

&lt;h1 id=&#34;编译安装&#34;&gt;编译安装&lt;/h1&gt;

&lt;p&gt;下载代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://github.com/fstab/grok_exporter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译环境&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. go
2. gcc
3. Oniguruma
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前面两个就不多说了，最后一个安装说明一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. Installing the Oniguruma library on OS X

    brew install oniguruma

2.Installing the Oniguruma library on Ubuntu Linux

    sudo apt-get install libonig-dev

3.Installing the Oniguruma library from source

    curl -sLO https://github.com/kkos/oniguruma/releases/download/v6.9.4/onig-6.9.4.tar.gz
    tar xfz onig-6.9.4.tar.gz
    cd /tmp/onig-6.9.4
    ./configure
    make
    make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/fstab/grok_exporter
cd grok_exporter
git submodule update --init --recursive
go install .
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;p&gt;基本启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./grok_exporter -config ./example/config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以在 &lt;a href=&#34;http://localhost:9144/metrics&#34;&gt;http://localhost:9144/metrics&lt;/a&gt; 来访问指标&lt;/p&gt;

&lt;p&gt;其他启动参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage of ./grok_exporter:
  -config string
        Path to the config file. Try &#39;-config ./example/config.yml&#39; to get started.
  -showconfig
        Print the current configuration to the console. Example: &#39;grok_exporter -showconfig -config ./example/config.yml&#39;
  -version
        Print the grok_exporter version.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The grok_exporter configuration file consists of five main sections:

global:
    # Config version
input:
    # How to read log lines (file or stdin).
grok:
    # Available Grok patterns.
metrics:
    # How to map Grok fields to Prometheus metrics.
server:
    # How to expose the metrics via HTTP(S).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;global 主要是配置config的版本，目前最新版都是V2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
    config_version: 2
    retention_check_interval: 53s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grok_exporter   config_version
≤ 0.1.4 1 (see CONFIG_v1.md)
0.2.X, 1.0.X    2 (current version)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、retention_check_interval&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The retention_check_interval is the interval at which grok_exporter checks for expired metrics.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;input 主要是重哪边采集日志，可以是文件，标准输入等,我们使用的是文件输入的方式，在这边只要将文件路径配置好就行。&lt;/p&gt;

&lt;p&gt;1、file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input:
    type: file
    paths:
    - /var/logdir1/*.log
    - /var/logdir2/*.log
    readall: false
    fail_on_missing_logfile: true
    poll_interval_seconds: 5 # should not be needed in most cases, see below
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;type就是类型&lt;/li&gt;
&lt;li&gt;path就是获取文件配置&lt;/li&gt;
&lt;li&gt;readall表示是否重文件开头开始读取，true表示重文件开头读取，false表示重结尾读取&lt;/li&gt;
&lt;li&gt;fail_on_missing_logfile表示不存在采集的文件是否启动成功，如果是true代表文件不存在就启动失败，反之亦然。&lt;/li&gt;
&lt;li&gt;poll_interval_seconds不重要&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、stdin&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input:
    type: stdin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比如monitor the output of journalctl&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journalctl -f | grok_exporter -config config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Webhook&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input:

    type: webhook

    # HTTP Path to POST the webhook
    # Default is `/webhook`
    webhook_path: /webhook

    # HTTP Body POST Format
    # text_single: Webhook POST body is a single plain text log entry
    # text_bulk: Webhook POST body contains multiple plain text log entries
    #   separated by webhook_text_bulk_separator (default: \n\n)
    # json_single: Webhook POST body is a single json log entry.  Log entry
    #   text is selected from the value of a json key determined by
    #   webhook_json_selector.
    # json_bulk: Webhook POST body contains multiple json log entries.  The
    #   POST body envelope must be a json array &amp;quot;[ &amp;lt;entry&amp;gt;, &amp;lt;entry&amp;gt; ]&amp;quot;.  Log
    #   entry text is selected from the value of a json key determined by
    #   webhook_json_selector.
    # Default is `text_single`
    webhook_format: json_bulk

    # JSON Path Selector
    # Within an json log entry, text is selected from the value of this json selector
    #   Example &amp;quot;.path.to.element&amp;quot;
    # Default is `.message`
    webhook_json_selector: .message

    # Bulk Text Separator
    # Separator for text_bulk log entries
    # Default is `\n\n`
    webhook_text_bulk_separator: &amp;quot;\n\n&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;grok 主要是匹配规则的相关正则表达式的定义，我们可以自定义的我们url相关的路径，然后根据路径进行匹配&lt;/p&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grok:
    patterns_dir: ./logstash-patterns-core/patterns
    additional_patterns:
    - &#39;EXIM_MESSAGE [a-zA-Z ]*&#39;
    - &#39;EXIM_SENDER_ADDRESS F=&amp;lt;%{EMAILADDRESS}&amp;gt;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;patterns_dir是指定我们写好的正则表达式文件的目录，我们可以自己去这个目录下编写，，正常logstash-patterns-core是以前用于logstash的，包含了大部分的正则，可以进去查看使用，当然如果自定义的话，也可以自己去写这个文件在这个目录下。&lt;/li&gt;
&lt;li&gt;additional_patterns也是我们给正则表达式起个名字，不用写在文件里，写在这里直接用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如我做的nginx的url的匹配获取参数为标签的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grok:
    additional_patterns:
      - &#39;URL /springRed/getRewards.do!?&#39;            //获取url
      - &#39;ID (?&amp;lt;=promotionId=).*?(?=[&amp;quot;|&amp;amp;|  ])&#39;       //获取参数id
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;metrics 主要定义我们采集的指标&lt;/p&gt;

&lt;p&gt;支持四种类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Counter
Gauge
Histogram
Summary
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.counter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
    - type: counter
      name: alice_occurrences_total
      help: number of log lines containing alice
      match: &#39;alice&#39;
      labels:
          logfile: &#39;{{base .logfile}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;match就是我们匹配的字段，可以正则表达式，可以是中间的任何一段。&lt;/li&gt;
&lt;li&gt;labels就是我们指标中使用的标签，可以使用match中定义的变量&lt;/li&gt;
&lt;li&gt;counter不需要指定value，是自己累加的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
    - type: counter
      name: count_total
      help: Total Number of RedPackage Request.
      match: &#39;%{URL:url}.*%{ID:id}&#39;
      labels:
        url: &#39;{{.url}}&#39;
        id: &#39;{{.id}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.gauge&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
    - type: gauge
      name: grok_example_values
      help: Example gauge metric with labels.
      match: &#39;%{DATE} %{TIME} %{USER:user} %{NUMBER:val}&#39;
      value: &#39;{{.val}}&#39;
      cumulative: false
      labels:
          user: &#39;{{.user}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.Histogram&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
    - type: histogram
      name: grok_example_values
      help: Example histogram metric with labels.
      match: &#39;%{DATE} %{TIME} %{USER:user} %{NUMBER:val}&#39;
      value: &#39;{{.val}}&#39;
      buckets: [1, 2, 3]
      labels:
          user: &#39;{{.user}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.Summary&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
   - type: summary
      name: grok_example_values
      help: Summary metric with labels.
      match: &#39;%{DATE} %{TIME} %{USER:user} %{NUMBER:val}&#39;
      value: &#39;{{.val}}&#39;
      quantiles: {0.5: 0.05, 0.9: 0.01, 0.99: 0.001}
      labels:
          user: &#39;{{.user}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;server服务器监听配置&lt;/p&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server:
    protocol: https
    host: localhost
    port: 9144
    path: /metrics
    cert: /path/to/cert
    key: /path/to/key
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;实例&#34;&gt;实例&lt;/h1&gt;

&lt;p&gt;采集nginx日志中 &lt;a href=&#34;http://test.com:8088/spring/getRewards?promotionId=21&#34;&gt;http://test.com:8088/spring/getRewards?promotionId=21&lt;/a&gt;    类似于这个url的 的数量的统计&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test grok_exporter]# cat config.yml
global:
    config_version: 2
input:
    type: file
    path: /usr/local/nginx/logs/access_http.log
    readall: true # Read from the beginning of the file? False means we start at the end of the file and read only new lines.
grok:
    additional_patterns:
      - &#39;URL /spring/getRewards!?&#39;
      - &#39;ID (?&amp;lt;=promotionId=).*?(?=[&amp;quot;|&amp;amp;|  ])&#39;
metrics:
    - type: counter
      name: count_total
      help: Total Number of RedPackage Request.
      match: &#39;%{URL:url}.*%{ID:id}&#39;
      labels:
        url: &#39;{{.url}}&#39;
        id: &#39;{{.id}}&#39;

server:
    host: 0.0.0.0
    port: 9144
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./grok_exporter -config ./example/config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;采集数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# HELP count_total Total Number of RedPackage Request.
# TYPE count_total counter
count_total{id=&amp;quot;21&amp;quot;,url=&amp;quot;/springRed/getRewards.do&amp;quot;} 387184
count_total{id=&amp;quot;22&amp;quot;,url=&amp;quot;/springRed/getRewards.do&amp;quot;} 384322
count_total{id=&amp;quot;23&amp;quot;,url=&amp;quot;/springRed/getRewards.do&amp;quot;} 381606
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;进程会一直对文件进行读取计算，所以就算不采集数据，程序在这一块也是有消耗的。读取的方式是可以选择的，重文件开始还是文件结束。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus mtail</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/mtail/</link>
          <pubDate>Fri, 10 Jan 2020 17:53:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/mtail/</guid>
          <description>&lt;p&gt;mtail是一个可以从应用程序日志中提取指标，并将其导出到时间序列数据库或时间序列计算器中，以便配置警报和仪表盘的工具。&lt;/p&gt;

&lt;p&gt;提取由定义了模式和动作的mtail程序来控制。&lt;/p&gt;

&lt;h1 id=&#34;安装和使用&#34;&gt;安装和使用&lt;/h1&gt;

&lt;h2 id=&#34;源码编译&#34;&gt;源码编译&lt;/h2&gt;

&lt;p&gt;需要安装运行版本不低于1.9的Go环境。如果不想在本地环境中安装Go，也可以选择Dockerfile。&lt;/p&gt;

&lt;h2 id=&#34;下载编译好的二进制文件后运行即可&#34;&gt;下载编译好的二进制文件后运行即可&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# nohup ./mtail -progs /path/to/mtailprograms -logs /path/to/logfile &amp;amp;
# cat /path/to/mtailprograms
counter nginx_line_count
#正则统计访问日志的行数，可以在grafana中配置视图观察访问量趋势
/$/ {
  nginx_line_count++
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;./mtail -h
mtail version v3.0.0-rc12 git revision 936050df9856da258f13e5df71a5a0c7f8f6acdc go version go1.10.1

Usage:
  -address string
        Host or IP address on which to bind HTTP listener
  -alsologtostderr
        log to standard error as well as files
  -block_profile_rate int
        Nanoseconds of block time before goroutine blocking events reported. 0 turns off.  See https://golang.org/pkg/runtime/#SetBlockProfileRate
  -collectd_prefix string
        Prefix to use for collectd metrics.
  -collectd_socketpath string
        Path to collectd unixsock to write metrics to.
  -compile_only
        Compile programs only, do not load the virtual machine.
  -dump_ast
        Dump AST of programs after parse (to INFO log).
  -dump_ast_types
        Dump AST of programs with type annotation after typecheck (to INFO log).
  -dump_bytecode
        Dump bytecode of programs (to INFO log).
  -emit_prog_label
        Emit the &#39;prog&#39; label in variable exports. (default true)
  -graphite_host_port string
        Host:port to graphite carbon server to write metrics to.
  -graphite_prefix string
        Prefix to use for graphite metrics.
  -log_backtrace_at value
        when logging hits line file:N, emit a stack trace
  -log_dir string
        If non-empty, write log files in this directory
  -logfds value
        List of file descriptor numbers to monitor, separated by commas.  This flag may be specified multiple times.
  -logs value
        List of log files to monitor, separated by commas.  This flag may be specified multiple times.
  -logtostderr
        log to standard error instead of files
  -metric_push_interval_seconds int
        Interval between metric pushes, in seconds. (default 60)
  -metric_push_write_deadline duration
        Time to wait for a push to succeed before exiting with an error. (default 10s)
  -mtailDebug int
        Set parser debug level.
  -mutex_profile_fraction int
        Fraction of mutex contention events reported.  0 turns off.  See http://golang.org/pkg/runtime/#SetMutexProfileFraction
  -one_shot
        Compile the programs, then read the contents of the provided logs from start until EOF, print the values of the metrics store and exit. This is a debugging flag only, not for production use.
  -one_shot_metrics
        DEPRECATED: Dump metrics (to stdout) after one shot mode.
  -override_timezone string
        If set, use the provided timezone in timestamp conversion, instead of UTC.
  -port string
        HTTP port to listen on. (default &amp;quot;3903&amp;quot;)
  -progs string
        Name of the directory containing mtail programs
  -statsd_hostport string
        Host:port to statsd server to write metrics to.
  -statsd_prefix string
        Prefix to use for statsd metrics.
  -stderrthreshold value
        logs at or above this threshold go to stderr
  -syslog_use_current_year
        Patch yearless timestamps with the present year. (default true)
  -v value
        log level for V logs
  -version
        Print mtail version information.
  -vmodule value
        comma-separated list of pattern=N settings for file-filtered logging
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 架构解耦</title>
          <link>https://kingjcy.github.io/post/architecture/coupling/</link>
          <pubDate>Sun, 05 Jan 2020 10:04:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/coupling/</guid>
          <description>&lt;p&gt;自身服务的变动，需要其他依赖服务跟着升级变更，这就叫服务耦合，比如数据库换了一个ip，此时往往连接此数据库的上游需要修改配置重启，明明换ip的是你，凭什么配合重启的却是我？这就是一种典型的架构设计上“反向依赖”的问题。&lt;/p&gt;

&lt;h1 id=&#34;常规方案&#34;&gt;常规方案&lt;/h1&gt;

&lt;p&gt;我们在架构设计的时候就要将服务进行解耦，解决反向依赖带来的各种问题。&lt;/p&gt;

&lt;h2 id=&#34;公共库导致耦合&#34;&gt;公共库导致耦合&lt;/h2&gt;

&lt;p&gt;三个服务s1/s2/s3，通过一个公共的库biz.jar来实现一段业务逻辑，s1/s2/s3其实间接通过biz.jar耦合在了一起，一个业务s1修改一块公共的代码，导致影响其他业务s2/s3，架构上是不合理的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;业务垂直拆分&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果公共的库biz.jar中实现的逻辑“业务特性”很强，可以拆分为biz1.jar/biz2.jar/biz3.jar，来对s1/s2/s3进行解耦。这样的话，任何业务的改动，影响范围只是自己，不会影响其他人。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;服务化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果biz.jar中实现的逻辑“业务共性”很强，可以将biz.jar优化为biz.service服务，来对s1/s2/s3进行解耦。服务化之后，兼容性能更好的通过接口自动化回归测试来保证。&lt;/p&gt;

&lt;p&gt;基础服务的抽象，本身是一种共性聚焦，是系统解耦常见的方案。&lt;/p&gt;

&lt;h1 id=&#34;服务化不彻底导致耦合&#34;&gt;服务化不彻底导致耦合&lt;/h1&gt;

&lt;p&gt;服务化是解决“业务共性”组件库导致系统耦合的常见方案之一，但如果服务化不彻底，service本身也容易成为业务耦合点。典型的服务化不彻底导致的业务耦合的特征是，共性服务中，包含大量“根据不同业务，执行不同个性分支”的代码。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;switch (biz-type)
case biz-1 : exec1
case biz-2 : exec2
case biz-3 : exec3
…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/coupling/coupling.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在这种架构下，biz-1/biz-2/biz-3有个性的业务需求，可能导致修改代码的是共性的biz-service，使其成为研发瓶颈，架构上也是不合理的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;业务特性代码上浮，业务共性代码下沉，彻底解耦&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/coupling/coupling2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;把swithc case中业务特性代码放到业务层实现，这样biz-1/biz-2/biz-3有个性的业务需求，升级的是自己的业务系统。&lt;/p&gt;

&lt;h2 id=&#34;notify的不合理实现导致的耦合&#34;&gt;notify的不合理实现导致的耦合&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/coupling/coupling3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;消息发送方不关注消息接收方的执行结果，如果采用调用的方式来实现通知，会导消息发送方和消息接收方耦合。比如如何新增消息接收方biz-4，会发现修改代码的是消息发送方，新增一个对biz-4的调用，极不合理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过MQ实现解耦&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/coupling/coupling4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;消息发送方upper将消息发布给MQ，消息接收方从MQ去订阅，任何新增对消息的消费，upper都不需要修改代码。&lt;/p&gt;

&lt;h2 id=&#34;配置中的ip导致上下游耦合&#34;&gt;配置中的ip导致上下游耦合&lt;/h2&gt;

&lt;p&gt;下游服务换ip，可能导致多个服务调用方修改配置重启。上下游间接的通过ip这个配置耦合在了一起，架构不合理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过内网域名而不是ip来进行下游连接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果在配置中使用内网域名来进行下游连接，当下游服务或者数据库更换ip时，只需要运维层面将内网域名指向新的ip，然后统一切断原有旧的连接，连接就能够自动切换到新的ip上来。这个过程不需要所有上游配合，非常帅气，强烈推荐！这也是我们常用的vip的架构设计，也可以新增服务发现服务来解决这个问题，用于解决下游集群扩容等问题。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 搜索系统</title>
          <link>https://kingjcy.github.io/post/architecture/search/</link>
          <pubDate>Sat, 04 Jan 2020 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/search/</guid>
          <description>&lt;p&gt;搜索系统在我们日常生活中经常使用，比如baidu，google等，我们来看看其架构和原理。&lt;/p&gt;

&lt;h1 id=&#34;全网搜索引擎架构&#34;&gt;全网搜索引擎架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;核心系统主要分为三部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spider爬虫系统&lt;/li&gt;
&lt;li&gt;search&amp;amp;index建立索引与查询索引系统

&lt;ul&gt;
&lt;li&gt;一部分用于生成索引数据build_index&lt;/li&gt;
&lt;li&gt;一部分用于查询索引数据search_index&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;rank打分排序系统&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心数据主要分为两部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;web网页库&lt;/li&gt;
&lt;li&gt;index索引数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心流程：&lt;/p&gt;

&lt;p&gt;写入&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spider把互联网网页抓过来&lt;/li&gt;
&lt;li&gt;spider把互联网网页存储到网页库中（这个对存储的要求很高，要存储几乎整个“万维网”的镜像）&lt;/li&gt;
&lt;li&gt;build_index从网页库中读取数据，完成分词&lt;/li&gt;
&lt;li&gt;build_index生成倒排索引&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;检索&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;search_index获得用户的搜索词，完成分词&lt;/li&gt;
&lt;li&gt;search_index查询倒排索引，获得“字符匹配”网页，这是初筛的结果&lt;/li&gt;
&lt;li&gt;rank对初筛的结果进行打分排序&lt;/li&gt;
&lt;li&gt;rank对排序后的第一页结果返回&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如baidu，google就是全网搜索引擎架构，我们来看一下百度如何实现实时搜索十五分钟内的新闻的？&lt;/p&gt;

&lt;p&gt;首先任何对数据的更新，并不会实时修改索引，一旦产生碎片，会大大降低检索效率。既然索引数据不能实时修改，如何保证最新的网页能够被索引到呢？&lt;/p&gt;

&lt;p&gt;就是采用了将索引分为全量库、日增量库、小时增量库。比如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;300亿数据在全量索引库中&lt;/li&gt;
&lt;li&gt;1000万1天内修改过的数据在天库中&lt;/li&gt;
&lt;li&gt;50万1小时内修改过的数据在小时库中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当有查询请求发生时，会同时查询各个级别的索引，将结果合并，得到最新的数据：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;全量库是紧密存储的索引，无碎片，速度快&lt;/li&gt;
&lt;li&gt;天库是紧密存储，速度快&lt;/li&gt;
&lt;li&gt;小时库数据量小，速度也快&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当有修改请求发生时，只会操作最低级别的索引，例如小时库。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据库的同步&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dumper：将在线的数据导出&lt;/li&gt;
&lt;li&gt;merger：将离线的数据合并到高一级别的索引中去&lt;/li&gt;
&lt;li&gt;小时库，一小时一次，合并到天库中去；&lt;/li&gt;
&lt;li&gt;天库，一天一次，合并到全量库中去；&lt;/li&gt;
&lt;li&gt;这样就保证了小时库和天库的数据量都不会特别大；&lt;/li&gt;
&lt;li&gt;如果数据量和并发量更大，还能增加星期库，月库来缓冲。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据的写入和读取都是实时的，所以58同城能够检索到1秒钟之前发布的帖子，即使全量库有300亿的数据。百度也能搜索到十五分钟内的数据。&lt;/p&gt;

&lt;h1 id=&#34;站内搜索引擎架构&#34;&gt;站内搜索引擎架构&lt;/h1&gt;

&lt;p&gt;全网搜索需要spider要被动去抓取数据，站内搜索是内部系统生成的数据，例如“发布系统”会将生成的帖子主动推给build_data系统，看似“很小”的差异，架构实现上难度却差很多：全网搜索如何“实时”发现“全量”的网页是非常困难的，而站内搜索容易实时得到全部数据。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;spider和search&amp;amp;index是相对工程的系统，rank是和业务、策略紧密、算法相关的系统，搜索体验的差异主要在此，而业务、策略的优化是需要时间积累的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google的体验比Baidu好，根本在于前者rank牛逼&lt;/li&gt;
&lt;li&gt;国内互联网公司短时间要搞一个体验超越Baidu的搜索引擎，是很难的，真心需要时间的积累&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;搜索原理&#34;&gt;搜索原理&lt;/h1&gt;

&lt;h2 id=&#34;核心数据结构&#34;&gt;核心数据结构&lt;/h2&gt;

&lt;h3 id=&#34;正排索引-forward-index&#34;&gt;正排索引（forward index）&lt;/h3&gt;

&lt;p&gt;由key查询实体的过程，是正排索引。比如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户表：t_user(uid, name, passwd, age, sex)，由uid查询整行的过程，就是正排索引查询。&lt;/li&gt;
&lt;li&gt;网页库：t_web_page(url, page_content)，由url查询整个网页的过程，也是正排索引查询。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举个例子，假设有3个网页：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;url1 -&amp;gt; “我爱北京”
url2 -&amp;gt; “我爱到家”
url3 -&amp;gt; “到家美好”
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个正排索引Map&lt;url, page_content&gt;。&lt;/p&gt;

&lt;h3 id=&#34;分词&#34;&gt;分词&lt;/h3&gt;

&lt;p&gt;分词就是将基本的词进行拆分，比如将上面的内容进行分词：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;url1 -&amp;gt; {我，爱，北京}
url2 -&amp;gt; {我，爱，到家}
url3 -&amp;gt; {到家，美好}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个分词后的正排索引Map&lt;url, list&lt;item&gt;&amp;gt;。&lt;/p&gt;

&lt;h3 id=&#34;倒排索引-inverted-index&#34;&gt;倒排索引（inverted index）&lt;/h3&gt;

&lt;p&gt;由item查询key的过程，是倒排索引。比如上面的网页进行倒排&lt;/p&gt;

&lt;p&gt;分词后倒排索引：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;我 -&amp;gt; {url1, url2}
爱 -&amp;gt; {url1, url2}
北京 -&amp;gt; {url1}
到家 -&amp;gt; {url2, url3}
美好 -&amp;gt; {url3}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由检索词item快速找到包含这个查询词的网页Map&lt;item, list&lt;url&gt;&amp;gt;就是倒排索引。&lt;/p&gt;

&lt;p&gt;正排索引和倒排索引是spider和build_index系统提前建立好的数据结构，为什么要使用这两种数据结构，是因为它能够快速的实现“用户网页检索”需求（业务需求决定架构实现）&lt;/p&gt;

&lt;h2 id=&#34;基本搜索原理&#34;&gt;基本搜索原理&lt;/h2&gt;

&lt;p&gt;假设搜索词是“我爱”，原理如下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;分词，“我爱”会分词为{我，爱}，时间复杂度为O(1)&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个分词后的item，从倒排索引查询包含这个item的网页list&lt;url&gt;，时间复杂度也是O(1)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;我 -&amp;gt; {url1, url2}
爱 -&amp;gt; {url1, url2}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;求list&lt;url&gt;的交集，就是符合所有查询词的结果网页，对于这个例子，{url1, url2}就是最终的查询结果&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心就在于如何求两个子集的交集&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;循环遍历，土办法，时间复杂度O(n*n)&lt;/li&gt;
&lt;li&gt;有序list求交集，拉链法，时间复杂度O(n)&lt;/li&gt;
&lt;li&gt;分桶并行优化，多线程并行&lt;/li&gt;
&lt;li&gt;bitmap再次优化，大大提高运算并行度，时间复杂度O(n)&lt;/li&gt;
&lt;li&gt;跳表skiplist，时间复杂度为O(log(n))&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些都是搜索引擎中常用的求交集的方法，可见搜索引擎中算法使用还是比较多的，其实引擎基本都是算法的天下。&lt;/p&gt;

&lt;h1 id=&#34;搜索架构演进&#34;&gt;搜索架构演进&lt;/h1&gt;

&lt;h2 id=&#34;原始阶段-like&#34;&gt;原始阶段-LIKE&lt;/h2&gt;

&lt;p&gt;直接通过数据看的like关键词进行模糊匹配，能够快速满足业务需求，显然效率低，每次需要全表扫描，计算量大，并发高时cpu容易100%，而且不支持分词，很多查询不了。&lt;/p&gt;

&lt;h2 id=&#34;初级阶段-全文索引&#34;&gt;初级阶段-全文索引&lt;/h2&gt;

&lt;p&gt;正常数据库查询的时候，我们一般都会想到给数据库建索引，然后用于查询，但是这情况情况也存在一些问题。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;只适用于MyISAM&lt;/li&gt;
&lt;li&gt;由于全文索引利用的是数据库特性，搜索需求和普通CURD需求耦合在数据库中：检索需求并发大时，可能影响CURD的请求；CURD并发大时，检索会非常的慢；&lt;/li&gt;
&lt;li&gt;数据量达到百万级别，性能还是会显著降低，查询返回时间很长，业务难以接受&lt;/li&gt;
&lt;li&gt;比较难水平扩展&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;中级阶段-开源外置索引&#34;&gt;中级阶段-开源外置索引&lt;/h2&gt;

&lt;p&gt;外置索引的核心思路是：索引数据与原始数据分离，前者满足搜索需求，后者满足CURD需求，通过一定的机制（双写，通知，定期重建）来保证数据的一致性。&lt;/p&gt;

&lt;p&gt;Solr，Lucene，ES都是常见的开源方案。比如ES“封装一个接口友好的服务，屏蔽底层复杂性”&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ES是一个以Lucene为内核来实现搜索功能，提供REStful接口的服务&lt;/li&gt;
&lt;li&gt;ES能够支持很大数据量的信息存储，支持很高并发的搜索请求&lt;/li&gt;
&lt;li&gt;ES支持集群，向使用者屏蔽高可用/可扩展/负载均衡等复杂特性&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ES完全能满足10亿数据量，5k吞吐量的常见搜索业务需求，强烈推荐。&lt;/p&gt;

&lt;h2 id=&#34;高级阶段-自研搜索引擎&#34;&gt;高级阶段-自研搜索引擎&lt;/h2&gt;

&lt;p&gt;当数据量进一步增加，达到10亿、100亿数据量；并发量也进一步增加，达到每秒10万吞吐；业务个性也逐步增加的时候，就需要自研搜索引擎了，定制化实现搜索内核了。比如58同城的E-search，等等。&lt;/p&gt;

&lt;p&gt;到了定制化自研搜索引擎的阶段，超大数据量、超高并发量为设计重点，为了达到“无限容量、无限并发”的需求，架构设计需要重点考虑“扩展性”，力争做到：增加机器就能扩容（数据量+并发量）。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算系列---- PaaS</title>
          <link>https://kingjcy.github.io/post/cloud/paas/paas/</link>
          <pubDate>Thu, 02 Jan 2020 19:51:38 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/paas/</guid>
          <description>&lt;p&gt;PaaS就是一个为应用提供自动化研发，部署，调度，运维的管理平台。&lt;/p&gt;

&lt;h1 id=&#34;paas&#34;&gt;PaaS&lt;/h1&gt;

&lt;h2 id=&#34;paas的定位&#34;&gt;paas的定位&lt;/h2&gt;

&lt;p&gt;平台即服务（PaaS）与基础设施即服务（IaaS）是不同的，PaaS并不是IaaS的一个扩展特性，对于基础设施即服务（IaaS）来说，基础单元就是资源，这里的资源是指服务器，磁盘，网络等，IaaS所做的一切就是按照需要提供这些资源。例如，亚马逊（amazon）的EC2服务，所有的工具都以资源为中心，所有的文档都是关于资源的，所有的开发都是专注于资源，同时人们也因为需要这些资源而使用它。&lt;/p&gt;

&lt;p&gt;对于平台即服务（PaaS）来说，基础单元就是应用。那么什么是应用？就是一个系统，就是代码以及所有那些在任何时候都与这些代码通信的服务。这不仅仅是资源，事实上，一个应用是由很多单独的资源绑定在一起组成的。将所有这些资源连接在一起所需要付出的工作量通常被低估了。从一个单一的运行Apache和Mysql的服务器转移到一个拥有单独的负载均衡服务器，缓存服务器，应用服务器，数据库服务器以及冗余的失效恢复的系统架构需要大量的工作，包括前期投入以及后期维护。一个成熟的PaaS平台可以为用户提供上述的所有功能，在减少用户大量工作的前提下，大幅度提升用户应用的开发速度，运行稳定性，可靠性，极大的降低了用户的开发，测试及运维的成本。&lt;/p&gt;

&lt;p&gt;利用PaaS可以做的另外一件事，就是从应用的角度来管理IaaS。通常情况下，使用者对于IaaS的资源需求实质上是来源于运行在IaaS之上的应用，如何根据应用的需求动态的使用IaaS资源又成为摆在云使用者面前的一个难题，PaaS作为SaaS与IaaS的沟通者，可以根据SaaS的需求动态的协调IaaS资源，使IaaS按需分配资源的理念变得更智能，更有实际意义。&lt;/p&gt;

&lt;p&gt;IaaS为云使用者提供了按需分配的能力，用户可以按照自己的需求定制计算资源，存储资源，网络资源，并且利用云端的海量资源随时快速的开启资源，并在工作完成时，随时释放资源，在享受云带来的高可靠性的同时，也最大化的降低了使用成本，提升了资源利用率。&lt;/p&gt;

&lt;p&gt;但是IaaS为云使用者带来的便利只局限在资源这个层面上，云使用者可以快速，稳定，海量的使用资源，但是一旦获取到资源后，云使用者依然要为运行在资源之上的应用搭建各种适配环境（部署），解决应用的各种依赖，安装应用要使用的各种服务，维护应用的运行生命周期。这些问题IaaS都没有解决，或者说，这些问题本质上也不是IaaS需要解决的问题，而是PaaS需要解决的问题。&lt;/p&gt;

&lt;p&gt;综上所述&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;IaaS关注于硬件资源的自动化管理，目标是人与机器的解耦合，提升了效率和性能。&lt;/li&gt;
&lt;li&gt;PaaS关注于应用的自动化管理，目标是应用与操作系统的解耦合，提升了弹性和控制。在应用的角度，paas可以管理iaas的资源。&lt;/li&gt;
&lt;li&gt;Paas可以是kvm的虚拟化技术，只不过kvm太重，目前容器技术才是Paas平台的首选。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;paas的分类&#34;&gt;PaaS的分类&lt;/h2&gt;

&lt;p&gt;我们需要了解一下PaaS平台自身的分类。Gartner把它们分为两类&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一类是应用部署和运行平台APaaS（Application Platform As a Service）&lt;/li&gt;
&lt;li&gt;一类是集成平台IPaaS（Integration Platform As a Service）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;APaaS是一种面向IT企业和机构的云计算应用开发与部署平台。APaaS主要为应用提供运行环境和数据存储，能够将本地部署的传统应用直接部署到APaaS上。&lt;/p&gt;

&lt;p&gt;IPaaS是用于集成和协同的PaaS平台，不仅可以支持与现有云服务间的连接性，而且可以以安全的方式提供企业应用的访问能力。IPaaS主要用于集成和构建复合应用。&lt;/p&gt;

&lt;p&gt;大数据厂商的PaaS实际上是属于IPaaS，而容器厂商和IaaS厂商的PaaS大致为APaaS。我们经常见到的云平台建设也是APaaS，APaaS的一般特性：&lt;/p&gt;

&lt;p&gt;1、大规模分布式系统：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;完全模块化的分布式系统，保证云平台可靠性；&lt;/li&gt;
&lt;li&gt;每个模块单独存在和运行，通过消息总线进行通讯；&lt;/li&gt;
&lt;li&gt;系统耦合度低，便于弹性动态扩展；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、弹性伸缩框架：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;平台自身组件支持实时横向扩展;&lt;/li&gt;
&lt;li&gt;根据应用的负载情况，动态加载应用实例；&lt;/li&gt;
&lt;li&gt;应用实例支持实时水平扩展；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、运维自动化：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日常运维操作简化;&lt;/li&gt;
&lt;li&gt;故障自动恢复;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、应用部署简单化：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一键式应用快速部署;&lt;/li&gt;
&lt;li&gt;支持多种应用开发框架，包括Spring、.NET、Ruby on Rails，Node.js等;&lt;/li&gt;
&lt;li&gt;通过buildpack扩展运行不同语言应用的能力;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5、支持多种服务：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持多种数据服务，包括MySQL、mongodb、PostgreSQL等;&lt;/li&gt;
&lt;li&gt;通过service broker组件扩展多种应用服务能力，包括数据库、中间件、缓存、云存储等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;主流paas平台架构及对比&#34;&gt;主流PaaS平台架构及对比&lt;/h2&gt;

&lt;p&gt;了解了PaaS的分类，我们再来看看PaaS的具体技术对比。由于IPaaS具有很强的业务属性，因此这里我们主要来看一下更通用的APaaS，也是目前被大家最多提起的。说到PaaS，相信很多人都会把他和容器、Docker关联起来，下面来看一下这张图：&lt;/p&gt;

&lt;!--![](/media/cloud/compute/cloud1)--&gt;

&lt;!--![](/media/cloud/compute/cloud2)--&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/compute/cloud3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这张图很清晰的划分出了XaaS以及各种概念对应的平台。大家所熟知的基于docker的编排工具搭建的基础平台在这里实际上是CaaS的一种。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;CaaS&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CaaS其实就是容器云平台，提供直接管理操作基础设施的性能，带来基础设施层面的灵活性。用户通过直接操作容器可以更灵活的实现应用迁移、部署。但这个更加轻量的平台带来了用户学习成本和使用复杂度的增加。&lt;/p&gt;

&lt;p&gt;容器云平台的搭建只依托 Swarm/Mesos/K8s 等容器编排调度系统就可以实现，同时还需要引入大量的第三方解决方案，例如日志、监控、网络等。这就意味着一定的试错成本，另外第三方系统的成熟度发展不一，组成一套统一的云平台后进入生产环境的应用需要经过一定周期的论证和验证。&lt;/p&gt;

&lt;p&gt;但是这个算是基础平台，下面的平台都是基于这个平台之上进行封装整合其他第三方组件组成一个完成的生态平台，所以这些基础编排工具很重要。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Cloud Foundry 平台&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Cloud Foundry 隐藏了基础设施层面的复杂度，提供应用层的管理操作，简化基础设施和应用的构建管理，平台也使用容器技术，但仅仅是平台架构中的实现细节。通过 CF 可以更加敏捷的实现应用开发、部署、业务实现等。&lt;/p&gt;

&lt;p&gt;Cloud Foundry 的架构是一个相当完整的 PaaS 架构，模块丰富，平台自身可以提供对于平台节点、应用的监控、管理，日志，自动化运维等完整的解决方案。每个模块都部署在一个或多个虚拟机上，在经过长时间的应用，Cloud Foundry 已有很多在生产环境的案例。CloudFoundry是基于容器技术打造。相比于虚拟机，容器带来的系统开销非常低，所以，从经济性来说，容器的技术远远好于虚拟机。另外一个比较的标准是性能，容器的性能相对而言更好一些，但是，从安全性和隔离型来说，虚拟机是远远好于容器的。&lt;/p&gt;

&lt;p&gt;CloudFoundry的架构设计如下图所示。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先，CF也提供了一个路由模块(Router)，该模块基本是基于ngnix打造，只是在ngnix技术上提供了动态注册的功能。在部署时，由于CF会同时部署非常多的应用实例，所以需要一个router集群来满足应用的需要；&lt;/li&gt;
&lt;li&gt;其次，CF的应用容器基于自己开发的warden技术，warden也是基于LXC技术，但是使用c和ruby作了一层简单的封装。Docker的大热让CloudFoundry很纠结；&lt;/li&gt;
&lt;li&gt;第三，CF使用service broker来集成各种资源服务，如mongo、mysql、rabbitmq和redis等。最后，CF使用消息总线NATS/GNATS来完成应用之间的通讯。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/compute/cloud4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们简单看一下Cloud Foundry的特性&lt;/p&gt;

&lt;p&gt;优势&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持各种各样的语言、web框架、数据&lt;/li&gt;
&lt;li&gt;为开发者和云供应商提供简易和快速的自服务部署&lt;/li&gt;
&lt;li&gt;应用程序容器、服务和节点都被监视，如果在预期状态之外会自动重启&lt;/li&gt;
&lt;li&gt;PaaS支持大数据和移动服务&lt;/li&gt;
&lt;li&gt;可以使用命令行、Eclipse、Spring Tools Suite、Maven、Gradle 进行部署&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;虽然自动横向扩展只存在测试版，但是已计划在第三季度的Pivotal CF中释放；另一个Cloud Foundry发行版ActiveState的Stackato已经支持这一特性&lt;/li&gt;
&lt;li&gt;Cloud Foundry v2版本尚未提供可下载“微型”VM，但是你可以下载Stackato Micro，或者使用其中一个Cloud Foundry安装程序在本地VM中进行安装&lt;/li&gt;
&lt;li&gt;只支持 Ubuntu Linux上的有限应用程序，除下你使用Cloud Foundry的Uhuru Windows版本，这个并未评测&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;平台&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pivotal CF：VMware vSphere，OpenStac，Amazon Web Services，Google Cloud Platform&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们简单的对比一下传统的编排工具打造的容器平台和容器平台项目的差距&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;特性          Cloudfoundry             Docker／CaaS
部署          直接部署程序Release包  需要制作Docker镜像
监控          完整解决方案          需要集成第三方工具
日志          完整解决方案          需要集成第三方工具
网络          完整解决方案          需要集成第三方工具
自动化运维       可以和IaaS联动       不支持IaaS联动
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是CF底层并没有使用docker，相对于想docker一统天下的局面就有点显得落后和不友好了。所以目前使用的已经不多了，所以我们再来看看最新兴起的以docker为基础的paas平台项目：openshift和rancher。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;openshift&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Cloud Foundry 是一个常被用于和 Openshift 对比的产品，前者由 Pivotal 公司开发(大名鼎鼎的 Springboot，SpringCloud就是他们家的)，后者由 Redhat 开发。很难用一两句话说清两个 PaaS 孰优孰劣，但就我个人的使用体验而言，Cloud Foundry 自动化程度更高一些，而 Openshift 可定制化程度也更高。但是由于cf底层基础并不是k8s+docker的生态，所以现在并不具有可比性。目前主要的对比对象是国内的rancher项目。&lt;/p&gt;

&lt;p&gt;Openshift 实际上由三部分组成&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;核心部分实现容器的调度是封装的 Kubernetes&lt;/li&gt;
&lt;li&gt;除此之外还有一个内置的镜像仓库（Image Registry），这个仓库是可选的，Openshift 也可以配置使用 Dockerhub 或者企业自己的镜像仓库&lt;/li&gt;
&lt;li&gt;最外层部分是一个友好的 Web 界面，用于展示和操作 Openshift 的资源。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如下图所示，Openshift 要成为一个完整的数字化平台需要依赖于两个外部系统，一个代码库，一个是持续集成服务，事实上这两个外部服务也是可以跑在 Openshift 里面的。右边的灰色矩形就是 Openshift 的主要架构了，它的上层是一个路由（Router），用于 DNS 解析和转发，确保用户能够调用到 Openshift 集群中的服务。红色的部分是跑在 RHEL 操作系统上的 Kubernetes 集群，侧面是外部存储服务，因为集群里的计算单元是漂浮的，所以通常 Kubernetes 集群只提供计算能力，数据持久外需要依赖外部的比如说 S3，EBS 等云服务商提供的存储服务。最下层同样也是由云服务商提供的基础设施服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/compute/cloud6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们简单看一下OpenShift的特性&lt;/p&gt;

&lt;p&gt;优势&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持各种各样的语言、web框架、数据、应用程序堆栈&lt;/li&gt;
&lt;li&gt;为开发者和云供应商提供简易和快速的自服务部署&lt;/li&gt;
&lt;li&gt;自动应用程序扩展&lt;/li&gt;
&lt;li&gt;在开源代码等级整合Git，通过git push出发自动部署&lt;/li&gt;
&lt;li&gt;闲置gear终止允许更高的应用程序密度&lt;/li&gt;
&lt;li&gt;只要支持Red Hat Enterprise Linux就可以运行在任何硬件、云或者是虚拟机上&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;很大程度受限于只支持 Red Hat Linux上运行的应用程序，除非你使用Uhuru OpenShift.Net产品，这一点并未评测&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;平台&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OpenShift Enterprise：Red Hat Enterprise Linux。&lt;/li&gt;
&lt;li&gt;OpenShift Origin：KVM、VirtualBox、VMware Fusion/Player&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;rancher&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;虽然 Kubernete 在容器调度方面表现出众，但他的功能还元不构成一个 PaaS 平台，于是很多厂商开始基于 Kubernetes 开发自己的 PaaS 平台，其中就有目前在国内创业公司被大量使用的 Rancher，值得注意的是 Rancher 是在2016年才开始发布第一个版本，而 Openshift 则早在13年就开始基于 Kubernetes 的开发，但目前看来国内市场对 Rancher 的接受度是远高于 Openshift 的，就使用体验上看，Rancher 也确实更简单易用。&lt;/p&gt;

&lt;p&gt;这边需要详解的了解一下&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/platform/racher/&#34;&gt;rancher&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;它们俩各有各的特点。我们来简单的做一下对比&lt;/p&gt;

&lt;p&gt;OpenShift:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OpenShift有开源版本，是免费的。但是真的在企业级的使用情况下，还是要花钱的，比较贵。这个应该是相比于rancher比较不受欢迎的地方。&lt;/li&gt;
&lt;li&gt;一般版本会落后K8S一个大版本，对原生K8S改进较大，这个也是影响paas平台选择的一个重要因素。&lt;/li&gt;
&lt;li&gt;一般为只管理单个OpenShift集群&lt;/li&gt;
&lt;li&gt;设计了ImageStream，BuildConfig与DeploymentConfig等资源对象，及s2i构建方法，方便了开发者实施Devops。&lt;/li&gt;
&lt;li&gt;添加了一个内部镜像仓库。&lt;/li&gt;
&lt;li&gt;使用Route资源，为应用提供了一个公共统一的访问入口。类似于Ingress，使用起来比Ingress方便。&lt;/li&gt;
&lt;li&gt;提供了一个友好的可视化界面。&lt;/li&gt;
&lt;li&gt;对容器有更多的安全策略，更安全&lt;/li&gt;
&lt;li&gt;有更高的可靠性。 作为RedHat的企业级容器平台，红帽会对集群做详细的测试，修复bug。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rancher:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;具有良好的界面，rancher 的中文化做的比较好，对k8s的一些组件重新做了打包，在国内安装比较方便&lt;/li&gt;
&lt;li&gt;方便管理多个K8S集群&lt;/li&gt;
&lt;li&gt;对网络插件的选择会比OpenShift更加灵活&lt;/li&gt;
&lt;li&gt;与K8S版本同步，及时拥有K8S最新的特性，对K8S自身改动较少&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;个人认为，单集群管理使用OpenShift，更稳定，更简单，也更安全，而如果是要管理多集群，选择Rancher。不过OpenShift 4起红帽也支持多集群管理，但还不能私有化部署。&lt;/p&gt;

&lt;p&gt;两种方案都有不少的企业客户选择，因为都是基于K8S， 功能上都差不多 。不管是构建DevOps流水线，还是生产部署原生应用上。&lt;/p&gt;

&lt;h1 id=&#34;容器云平台&#34;&gt;容器云平台&lt;/h1&gt;

&lt;p&gt;现在各大企业搭建的容器云平台都是基于k8s+docker的paas平台。基本上都是基于rancher，openshift搭建，或者使用k8s自己管理，容器云平台自下而上分别覆盖了云计算的 IaaS 层和 PaaS 层涉及的各类问题，包括资源调度、服务编排、应用部署、监控日志、配置管理、存储网络管理、安全等。&lt;/p&gt;

&lt;h2 id=&#34;paas平台架构&#34;&gt;PaaS平台架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/compute/cloud9&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图是我认为比较完全的一个paas平台的架构，我觉得主要分为四大块建设：DevOps，基础集成，监控日志，安全。&lt;/p&gt;

&lt;h3 id=&#34;devops&#34;&gt;DevOps&lt;/h3&gt;

&lt;p&gt;DevOps主要就是用于研发的持续集成和标准交付，实现敏捷开发迭代部署，就是基于gitlab，jenkins，harbor，helm搭建的开发部署平台。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;CI/CD&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CI/CD 要求每次的集成都是通过自动化的构建来验证，包括自动编译、发布和测试，从而尽快地发现集成错误，让团队能够更快的开发内聚的软件，减轻了软件发布时的压力。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;持续集成：持续集成是一种实现产品和应用快速迭代的一种实践方式，这种实践要求开发人员所提交的每一次代码都能够快速被合并到生产线，并进行自动构建和上线。能够容易定位 Bug 并提前发现和解决，降低开发成本。&lt;/li&gt;
&lt;li&gt;持续交付：持续交付的缩短了需求完成周期，满足小粒度交付需求，打造自持敏捷开发，精益迭代和持续交付的研发基础设施。此外，提高了产品迭代速度，提升了应用软件质量。并且支持容器扩容、收缩、升级和回滚，轻松实现应用灰度发布，还拥有更快的应用交付和Go-to-Market能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CI/CD流水线式实现流程：应用从代码编译、测试、打包和部署的过程，流水线管理一般使用常用的 Jenkinsfile 来表述一组 CI/CD 流程。从代码仓库 、代码编译、镜像制作、镜像安全、推送到仓库、应用版本、到定时构建的端到端流水线设置。&lt;/p&gt;

&lt;p&gt;持续集成、持续交付、持续部署提供了一个优秀的 DevOps 环境。对于整个开发团队来说，能很大地提升开发效率，好处与挑战并行。无论如何，频繁部署、快速交付以及开发测试流程自动化都将成为未来软件工程的重要组成部分。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;持续集成（Continuous integration，简称CI：频繁地（一天多次或者N次）将代码集成到主干。将软件个人研发的部分向软件整体部分交付，频繁进行集成以便更快地发现其中的错误。&lt;/li&gt;
&lt;li&gt;持续交付（Continuous delivery）：指的是，频繁地将软件的新版本，交付给质量团队或者用户，以供评审。如果评审通过，代码就进入生产阶段。持续交付在持续集成的基础上，将集成后的代码部署到更贴近真实运行环境的「类生产环境」(production-like environments)中。持续交付优先于整个产品生命周期的软件部署，建立在高水平自动化持续集成之上。&lt;/li&gt;
&lt;li&gt;持续部署（continuous deployment,简称CD）：是持续交付的下一步，指的是代码通过评审以后，自动部署到生产环境。持续部署的目标是，代码在任何时刻都是可部署的，可以进入生产阶段。持续部署的前提是能自动化完成测试、构建、部署等步骤。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/cdci/jenkins/&#34;&gt;Jenkins&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;交付中心&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;容器镜像实现容器运行时标准化，应用模板（helm）实现编排文件的标准化。镜像和应用模板是开发（Dev）和运维（Ops）的媒介，完成测试的镜像、应用模板可以发布到生产环境；然后在容器云平台上部署和管理应用，持续监控应用服务的运行情况，并保持持续的反馈运行情况，以便及时的改进，形成一个良性循环。交付中心实现镜像、应用模板的集中安全统一管理，实现企业软件资源积累和沉淀。让所有用户都可以自由地下载为服务组件，这为开发者提供了巨大便利。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;镜像仓库：提供了多种构建镜像的方式，支持对接 CI/CD 工具，能够简化企业应用容器化的难度，轻松实现应用的容器化。并提供日常镜像维护的功能，比如镜像扫描、镜像同步和镜像清理等。其实就是镜像仓库，目前最常用的Harbor。&lt;/li&gt;
&lt;li&gt;应用模板：基于 Helm 标准的应用模板提供统一的资源管理与调度，高效地实现了模板的快速部署与后期管理，大幅简化了Kubernetes资源的安装管理过程。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/cdci/harbor/&#34;&gt;harbor&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/cdci/helm/&#34;&gt;helm&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;基础集成&#34;&gt;基础集成&lt;/h3&gt;

&lt;p&gt;基础集成就是基于docker+k8s的搭建的资源调度平台，是paas平台的核心和基础。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-tutorial/&#34;&gt;k8s&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/docker/docker/&#34;&gt;docker&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基础设施管理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;容器云平台的基础设施资源主要包括主机、网络、存储资源等。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;主机：容器云平台的计算能力由节点 (Node) 提供，节点类型分为控制节点和容器节点，业务应用的容器组都运行在容器节点。节点可以是物理机或虚拟机，平台可根据业务需求组建一定规模的节点集群。容器云平台的节点管理需满足企业对集群运维监控的需求，支持实时查看集群资源使用情况和节点状态，查看任意节点上 CPU 和内存的消耗、容器组数及状态。并且提供对节点的全方位细粒度的资源监控如 IOPS、磁盘吞吐、网卡流量，让用户一目了然所有节点资源状态。其实这个就是IaaS层面的管理了。&lt;/li&gt;
&lt;li&gt;网络：容器云平台的容器需要进行网络隔离或者网络连通，网络的管理功能需要支持标准的容器网络模型，还需要支持扩展的网络技术。以此来提供丰富的网络功能，更好的为业务服务提供最佳网络服务方案。不同的选择在网络性能、网络节点规模等方面各不相同。网络方案主要有 Bridge 网络模式、Host网络模式、Overlay网络模式等方案。容器调度管理应支持图形化管理网络，减轻网络配置技术难度，弱化对指定硬件厂商的依赖。此外，网络通信安全方面应支持创建安全加密网络，保障网络通信信息安全。&lt;/li&gt;
&lt;li&gt;存储：容器是无状态的，当容器崩溃重启时，容器中临时存放在磁盘的文件将会丢失。 其次，当在一个容器组中同时运行多个容器时，需要在这些容器之间共享文件。但是生产业务应用大部分都是有状态，容器云平台通过存储卷解决存储问题，并支持对接多种存储方案，比如采用 Ceph、NFS、glusterfs 等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这块一般是云服务厂商需要建设，比如阿里云，腾讯云等，如果直接使用云服务，一般直接使用其能力就可以，重点就在调度发布的能力建设了。&lt;/p&gt;

&lt;p&gt;核心&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network/&#34;&gt;k8s网络&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-store/&#34;&gt;分布式存储&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;应用管理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;容器云平台其实主要是针对应用（serverless），最核心的功能就是应用全生命周期管理。应用通常是一个独立完整的业务功能，一个应用可能由多个服务组件组成，对于微服务而言每个组件都可以独立于其他组件部署、启动、运行和治理。容器云平台需要提高对应用的一键式部署、健康检查、弹性扩缩、升级发布、资源管理、访问管理、监控管理等功能，从而保证应用的整体服务能力。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一键式部署：一般支持通过镜像、应用模板和 YAML 等方式部署应用，让用户可通过多种应用交付物快速便捷的部署。&lt;/li&gt;
&lt;li&gt;健康检查：由于容器启动后应用还需要较长时间才能接受请求，容器正常运行但容器中的应用服务异常等原因。需要支持应用健康状态的检查，健康检查结果会指导负载均衡、滚动发布和弹性伸缩，实现更平滑的过程 。&lt;/li&gt;
&lt;li&gt;弹性扩缩：支持通过手动、自动和定时等方式对容器的实例进行弹性的扩缩，保证应用运行的稳定性。&lt;/li&gt;
&lt;li&gt;升级发布：支持通过单一服务按照指定实例数滚动升级实现滚动发布；通过建立跨服务的 4 层负载均衡实现灰度发布／蓝绿发布；并支持一键回滚；基于预定义镜像规则自动发布，支持 DevOps 自动化流水线场景。&lt;/li&gt;
&lt;li&gt;资源管理：对容器部署或运行过程中的的 CPU、内存等计算资源、网络和存储资源的分配进行集中管理。&lt;/li&gt;
&lt;li&gt;访问管理：容器的外部接入访问入口，和平台内部访问容器的控制台入口的管理。&lt;/li&gt;
&lt;li&gt;监控管理：对容器产生的日志进行统一查看和导出，并且配置容器的日志管理服务器；对容器的资源包括CPU、内存、网络、存储等的使用情况进行可视化监控。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller-manager/&#34;&gt;k8s控制器&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-scheduler/&#34;&gt;k8s调度&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;服务治理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;服务治理是主要针对分布式服务框架的微服务，处理服务调用之间的关系、服务发布和发现、故障监控与处理，服务的参数配置、服务降级和熔断、服务监控等。&lt;/p&gt;

&lt;p&gt;为什么需要服务治理？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;过多的服务配置发现困难，需要服务注册和发现机制&lt;/li&gt;
&lt;li&gt;服务规模大，要求高，需要服务发布策略&lt;/li&gt;
&lt;li&gt;过多服务，导致性能指标分析难度较大，需要监控定位&lt;/li&gt;
&lt;li&gt;过多流量需要管控&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;服务治理通过提供完整的非侵入式的微服务治理解决方案，支持完整的服务生命周期管理（发布，发现，监控）和流量治理。能够很好的解决云原生服务的管理、网络连接以及安全管理等服务网络治理问题。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;灰度发布：灰度发布是迭代软件产品在生产环境安全上线的重要手段。允许用户按照标准制定一套流量分发规则，平滑稳定的实现灰度发布功能。主要有金丝雀、蓝绿、A/B Testing 等典型灰度发布功能。&lt;/li&gt;
&lt;li&gt;流量治理：应用流量治理提供可视化云原生应用的网络状态监控，并实现在线的网络连接和安全策略的管理配置。提供策略化、场景化的网络连接、安全策略管理能力。支持基于应用拓扑对服务配置负载均衡、熔断容错等治理规则，并提供实时的、可视化的服务流量管理。&lt;/li&gt;
&lt;li&gt;流量监控：通过流量监控可以监控流量概况、组件运行状态、调用链等信息，并在系统业务异常时快速定位到问题点，这块在监控日志告警那边实现。&lt;/li&gt;
&lt;li&gt;服务发现：解决服务调用的问题，不在需要配置大量的调用地址，直接使用DNS解析。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/ops/ops&#34;&gt;服务发布&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-principle/#服务发现&#34;&gt;服务发现&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-istio/&#34;&gt;流量治理控制&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;监控&#34;&gt;监控&lt;/h3&gt;

&lt;p&gt;云原生的监控主要重三种方式采集监控，分别是：metrics、log、train，在容器平台中我们通常使用的就是基于prometheus+EFK+调用链生态搭建的监控日志告警平台。其实所有的监控都是重这三个方面进行采集处理的，三种相互辅助的完整监控体系。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日志：日志模块需收集平台组件、业务应用、业务服务以及云上中间件的日志，可采用平台统一日志管理系统EFK生态，进行采集，存储，展示。&lt;/li&gt;
&lt;li&gt;监控告警：监控模块需提供对集群和应用的资源状态等监控metrics，支持大规模系统监控、多指标监控、多维度监控，为每一个层级资源的运行状态都提供实时的多种指标监控，并且收集资源实时监控数据和历史监控数据，帮助用户观察和建立资源和集群性能的正常标准，通过不同时间、不同负载条件下监测集群各项基础指标，并以图表或列表的形式展现，监控模块可采用平台的自有监控体系，或对接外部监控系统，比如 Prometheus。&lt;/li&gt;
&lt;li&gt;调用链：调用链模块就是对请求调用的链路的各种情况进行采集，存储，并用图表进行展示。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/monitor/&#34;&gt;监控体系（metrics,log,trace）&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;安全&#34;&gt;安全&lt;/h3&gt;

&lt;p&gt;安全就是基于RBAC的权限管理平台，主要是针对用户，权限，多租户等机制。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;多租户管理：不同租户中的资源彼此隔离。使得它们既可以共享同一个平套资源，也能够互不干扰。多租户管理的核心是分配好人员 (组织) 和资源之间的权限关系。对于容器云平台来说，需要将平台的计算资源、存储资源和网络资源，分配给各租户，让租户根据自身使用场景管理应用、用户、角色和资源。&lt;/li&gt;
&lt;li&gt;用户管理：用户管理模块，可对企业用户进行增删改查等操作，以及配置用户密码安全策略，保证用户和平台安全；同时可对接企业用户目录，同步企业现存用户管理系统，避免重复操作。&lt;/li&gt;
&lt;li&gt;权限管理：权限管理是在 Kubernetes 的角色访问控制（RBAC）的能力基础上，打造的细粒度权限管理功能。支持集群级别、租户级别的权限控制，能够从集群和租户层面对用户组或用户进行细粒度授权。&lt;/li&gt;
&lt;li&gt;第三方登录认证：当容器云平台中具有多系统，或者对接企业现有用户中心时，需要实现第三方登录认证。用户只需要登录一次，就可以访问所有相互信任的应用系统。解决企业不同业务应用之间的身份认证问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/safe/safe/&#34;&gt;rbac&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;paas组件&#34;&gt;PaaS组件&lt;/h3&gt;

&lt;p&gt;以上就是对PaaS平台基本架构进行了一个整体的划分，在每个部分都有其实现的基本组件，我们可以查看下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/compute/cloud7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;容器化推广发展过程&#34;&gt;容器化推广发展过程&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;搭建强稳定性的容器云平台，这是企业内部应用上容器云平台的基石；&lt;/li&gt;
&lt;li&gt;完成单体应用的容器化，开展企业应用容器化之路；&lt;/li&gt;
&lt;li&gt;开展企业内部应用容器化推广，培养企业内部对于容器标准化意识，明晰容器化应用与平台运维的边界；&lt;/li&gt;
&lt;li&gt;开始 DevOps 企业组织架构建设，完善应用自动化构建、测试、部署、发布全流程；&lt;/li&gt;
&lt;li&gt;开展中间件层面支持，完成企业数据中台；&lt;/li&gt;
&lt;li&gt;引入微服务流程，完成微服务相关治理能力。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;容器云平台建设意义&#34;&gt;容器云平台建设意义&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;为什么企业建设容器云平台？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;传统企业在数字化转型进程中，已完成物理机虚拟化的进程，虚拟化技术一定程度上降低了运维复杂性，提升资源的使用率。但这仅解决了 IaaS 层面基础设施的问题。业务应用研发还面临很多挑战：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;业务应用不够稳定，响应效率低下，业务流程复杂，导致用户体验较差；  &lt;/li&gt;
&lt;li&gt;业务应用规模复杂，组件间耦合度高；庞大的部署架构使得应用的开发、测试、发版和升级也比较复杂，使得业务升级停机时间和部署成本增加；&lt;/li&gt;
&lt;li&gt;在面临互联网企业的激烈市场竞争时，业务部门的需求变化越发频繁，同时希望研发部门的软件交付周期越来越短。但研发工作量大、周期长等问题，难以支持需求的快速响应和敏捷开发。&lt;/li&gt;
&lt;li&gt;基础设施薄弱，缺乏支撑互联网快速迭代的云环境，资源分配效率低。生产环境缺乏互联网监控手段等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总结就是&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;趋势：比如业务复杂不稳定耦合度高，维护迭代交付困难，资源使用效率低。&lt;/li&gt;
&lt;li&gt;降本：机器成本和人力成本&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;容器云平台可以解决以上问题，并且实现以下的作用：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;标准化交付&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;容器镜像实现应用运行环境的一致性和标准化，屏蔽了应用部署过程中遇到的不同环境需要的环境配置、安装步骤等复杂问题。把原先部署、配置的运维工作提前到开发交付阶段，在制作镜像的阶段解决运维上线中出现的问题。提供了企业开发、测试和生产环境的一致性，对于自主研发能力和效率的提升会有极大的帮助。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;应用微服务化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;微服务架构可轻量级构建冗余，可扩展性强。容器云平台提供应用微服务化的能力，将现有的大型应用程序通过微服务架构拆成多个独立模块，每个模块使用一个应用镜像进行微服务部署；支持镜像级别的升级发布；支持容器粒度的隔离，且容器被平均分布在底层宿主机上，保证应用每个微服务的安全和稳定。助力企业一步实现微服务架构，实现应用云原生转型。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;弹性扩缩&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;应用的访问流量是不确定的，需要避免因流量激增导致应用挂掉；以及避免因为流量减少导致大量资源浪费。容器云平台支持对主机、应用服务级别的双重扩缩，可根据用户的业务需求和预设策略，自动调整计算资源，使主机或服务数量自动随业务负载增长而增加，随业务负载降低而减少。实现业务应用的快速弹性扩缩，提升资源使用效率，保证应用运行稳定性。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;敏捷开发快速上线（DevOps）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;DevOps 将开发团队与运维团队通过一套流程或方法建立更具协作性、更高效的的关系，使得开发、测试、发布应用能够更加敏捷、高效、可靠。容器的 build、ship、run 的理念及其技术特点，更够更好的与 CI/CD 技术进行融合，从技术手段上保证项目管理方式和管理理念的真正有效落地。同时容器云平台提供代码构建、镜像打包、服务快速部署、灰度发布、自动伸缩、负载均衡等持续交付工具链，大大简化了持续集成、测试和发布的过程。使开发者专注业务的开发和测试无需关注运行环境和运维，加速应用的快速迭代和上线。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;跨平台&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;容器可运行在多种云平台环境中，目前支持容器的 IaaS 平台包括但不限于亚马逊平台（AWS)、Google云平台（GCP）、微软云平台（Azure），企业无需担心应用和第三方云平台绑定。并且实现对企业已有异构基础资源的统一化管理，屏蔽环境差异性。实现应用多云混合部署，降低系统运维难度。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;提高资源利用率&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;容器是基于操作系统的轻量级虚拟化技术，多个容器可以共享操作系统的内核进程和内核资源，从而有效节省操作系统级资源开销。容器具有资源隔离与限制的能力，可以精确地对应用分配 CPU 和内存等资源，保证了应用间不会相互影响。并且容器云平台将资源进行池化管理，按需分配、快速调度、环境隔离和及时回收，改变企业 IT 资源使用方式，提高整理利用率。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;应用资源积累&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;镜像仓库可集中式存放、管理企业的业务应用镜像，并且很好的分发到不同的环境进行部署。业务应用镜像经过安全扫描、部署测试等流程化审核后，可将业务应用镜像进行售卖和运营。并且容器云平台内置应用商店，可将社区容器化应用、常用中间件等一键部署，减少对非业务组件的研发和维护。&lt;/p&gt;

&lt;p&gt;总体来说，就是可以解决环境平台问题，降低耦合度，快速迭代交付，还可以提供资源利用率，实现自动化运维，所以建设容器云平台很有意义。&lt;/p&gt;

&lt;h2 id=&#34;paas平台和saas应用市场的关系&#34;&gt;PaaS平台和SaaS应用市场的关系&lt;/h2&gt;

&lt;p&gt;大家有没有发现一个现象？10年以前，SaaS应用市场是非常少的。现在各大平台都会有自己的SaaS应用市场。出现这种现象的原因无外乎技术的进步，搭建SaaS应用市场的成本降低了。更确切的说：SaaS应用市场是PaaS平台的一种外延。在底层PaaS技术的支撑下，SaaS应用的开发、交付、运营的门槛大幅度的降低了。&lt;/p&gt;

&lt;p&gt;基于PaaS平台构建的SaaS应用市场会逐步加快SaaS应用生态的发展，有了PaaS平台帮我们解决底层平台的问题，那么我们的应用开发者要怎么做才能开发出云原生应用呢？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用标准化流程自动配置，从而使新的开发者花费最少的学习成本加入这个项目。&lt;/li&gt;
&lt;li&gt;和操作系统之间尽可能的划清界限，在各个系统中提供最大的可移植性。&lt;/li&gt;
&lt;li&gt;适合部署在现代的云计算平台，从而在服务器和系统管理方面节省资源。&lt;/li&gt;
&lt;li&gt;将开发环境和生产环境的差异降至最低，并使用持续交付实施敏捷开发。&lt;/li&gt;
&lt;li&gt;可以在工具、架构和开发流程不发生明显变化的前提下实现扩展。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这套理论适用于任意语言和后端服务（数据库、消息队列、缓存等）开发的应用程序。&lt;/p&gt;

&lt;p&gt;下面我们一起来看看具体包含哪些要素：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;基准代码：一份基准代码，多份部署
依赖：显式声明依赖关系
配置：在环境中存储配置
后端服务：把后端服务当作附加资源
构建，发布，运行：严格分离构建和运行
进程：以一个或多个无状态进程运行应用
端口绑定：通过端口绑定提供服务
并发：通过进程模型进行扩展
易处理：快速启动和优雅终止可最大化健壮性
开发环境与线上环境等价：尽可能的保持开发，预发布，线上环境相同
日志：把日志当作事件流
管理进程：后台管理任务当作一次性进程运行
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;paas的未来发展&#34;&gt;PaaS的未来发展&lt;/h2&gt;

&lt;p&gt;先抛一个结论：PaaS是云的未来。&lt;/p&gt;

&lt;p&gt;IaaS是一个资源转售的生意，PaaS代表了云计算的未来，PaaS解决的是平台架构的问题，同时也是真正做到按需付费。对于业务客户而言，底层技术不会给他们带来直接的业务价值，这也就决定了软件开发商更应该聚焦在业务层。高并发、高可靠、存储服务、应用自身维护等和业务不直接关联的平台服务都可以借助PaaS技术来完成。现在的创业团队可以借助各类PaaS技术快速的创建高并发、高可靠的应用，未来这种模式也会进一步普及。&lt;/p&gt;

&lt;p&gt;更融合的调度 物理机、虚拟机和容器各有优势，一个复杂的应用场景会用到各计算平台的优势，融合调度未来必然会成为主流框架。&lt;/p&gt;

&lt;p&gt;更融合的编排 说到调度，就离不开编排。当前阶段上云是大趋势，私有云加公有云的模式会长期持续。那么融合的编排框架也必然会成为解决成本问题的一个重要选择。&lt;/p&gt;

&lt;p&gt;更细粒度的弹性 IaaS解决了基础设施的弹性，但是还不够。技术的发展会进一步细化弹性的粒度，往更节约的方向发展。&lt;/p&gt;

&lt;p&gt;更高的资源利用率 当前的技术及架构下，计算资源很大一部分时间都是闲置的。以有限的资源来应对更高的数据处理要求，资源利用率会随着云技术的发展进一步提高。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>golang使用系列---- net/http/pprof</title>
          <link>https://kingjcy.github.io/post/golang/go-net-http-pporf/</link>
          <pubDate>Thu, 26 Dec 2019 17:06:13 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-net-http-pporf/</guid>
          <description>&lt;p&gt;golang 开发过程进行性能调优，pprof 一定是一个大杀器般的工具。&lt;/p&gt;

&lt;h1 id=&#34;pprof&#34;&gt;PProf&lt;/h1&gt;

&lt;p&gt;想要进行性能优化，首先瞩目在 Go 自身提供的工具链来作为分析依据，本文将带你学习、使用 Go 后花园，涉及如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;runtime/pprof：采集程序（非 Server）的运行数据进行分析
net/http/pprof：采集 HTTP Server 的运行时数据进行分析
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实net/http/pprof中只是使用runtime/pprof包来进行封装了一下，并在http端口上暴露出来&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;pprof 是用于可视化和分析性能分析数据的工具,pprof 以 profile.proto 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告）&lt;/p&gt;

&lt;p&gt;profile.proto 是一个 Protocol Buffer v3 的描述文件，它描述了一组 callstack 和 symbolization 信息， 作用是表示统计分析的一组采样的调用栈，是很常见的 stacktrace 配置文件格式&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;p&gt;1、使用方式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基准测试文件：例如使用命令go test . -bench . -cpuprofile prof.cpu 生成采样文件后，再通过命令 go tool pprof [binary] prof.cpu 来进行分析。&lt;/li&gt;
&lt;li&gt;import _ net/http/pprof：如果我们的应用是一个web服务，我们可以在http服务启动的代码文件(eg: main.go)添加 import _ net/http/pprof，这样我们的服务 便能自动开启profile功能，有助于我们直接分析采样结果。&lt;/li&gt;
&lt;li&gt;通过在代码里面调用 runtime.StartCPUProfile或者runtime.WriteHeapProfile。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、支持什么使用模式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Report generation：报告生成&lt;/li&gt;
&lt;li&gt;Interactive terminal use：交互式终端使用&lt;/li&gt;
&lt;li&gt;Web interface：Web 界面&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、可以做什么&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CPU Profiling：CPU 分析，按照一定的频率采集所监听的应用程序 CPU（含寄存器）的使用情况，可确定应用程序在主动消耗 CPU 周期时花费时间的位置&lt;/li&gt;
&lt;li&gt;Memory Profiling：内存分析，在应用程序进行堆分配时记录堆栈跟踪，用于监视当前和历史内存使用情况，以及检查内存泄漏&lt;/li&gt;
&lt;li&gt;Block Profiling：阻塞分析，记录 goroutine 阻塞等待同步（包括定时器通道）的位置&lt;/li&gt;
&lt;li&gt;Mutex Profiling：互斥锁分析，报告互斥锁的竞争情况&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面只是常用的一部分，还有如下都可以分析，只不过使用的不多。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Profile Descriptions:
allocs: A sampling of all past memory allocations
block: Stack traces that led to blocking on synchronization primitives
cmdline: The command line invocation of the current program
goroutine: Stack traces of all current goroutines
heap: A sampling of memory allocations of live objects. You can specify the gc GET parameter to run GC before taking the heap sample.
mutex: Stack traces of holders of contended mutexes
profile: CPU profile. You can specify the duration in the seconds GET parameter. After you get the profile file, use the go tool pprof command to investigate the profile.
threadcreate: Stack traces that led to the creation of new OS threads
trace: A trace of execution of the current program. You can specify the duration in the seconds GET parameter. After you get the trace file, use the go tool trace command to investigate the trace.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;第一种使用方式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;编写测试用例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package data

import &amp;quot;testing&amp;quot;

const url = &amp;quot;https://github.com/EDDYCJY&amp;quot;

func TestAdd(t *testing.T) {
    s := Add(url)
    if s == &amp;quot;&amp;quot; {
        t.Errorf(&amp;quot;Test.Add error!&amp;quot;)
    }
}

func BenchmarkAdd(b *testing.B) {
    for i := 0; i &amp;lt; b.N; i++ {
        Add(url)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行测试用例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go test -bench=. -cpuprofile=cpu.prof
pkg: github.com/EDDYCJY/go-pprof-example/data
BenchmarkAdd-4       10000000           187 ns/op
PASS
ok      github.com/EDDYCJY/go-pprof-example/data    2.300s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动 PProf 可视化界面进行分析&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;方法一：
$ go tool pprof -http=:8080 cpu.prof
方法二：
$ go tool pprof cpu.prof
$ (pprof) web
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;第二种使用方式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们最常用的就是第二种方式，import _ net/http/pprof，我们将编写一个简单且有点问题的例子，用于基本的程序初步分析&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;log&amp;quot;
    &amp;quot;net/http&amp;quot;
    _ &amp;quot;net/http/pprof&amp;quot;
    &amp;quot;github.com/EDDYCJY/go-pprof-example/data&amp;quot;
)

func main() {
    go func() {
        for {
            log.Println(data.Add(&amp;quot;https://github.com/EDDYCJY&amp;quot;))
        }
    }()

    http.ListenAndServe(&amp;quot;0.0.0.0:6060&amp;quot;, nil)
}



package data

var datas []string

func Add(str string) string {
    data := []byte(str)
    sData := string(data)
    datas = append(datas, sData)

    return sData
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行这个文件，你的 HTTP 服务会多出 /debug/pprof 的 endpoint 可用于观察应用程序的情况&lt;/p&gt;

&lt;h2 id=&#34;交互分析&#34;&gt;交互分析&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;通过 Web 界面&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;查看当前总览：访问 &lt;a href=&#34;http://127.0.0.1:6060/debug/pprof/&#34;&gt;http://127.0.0.1:6060/debug/pprof/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/debug/pprof/

profiles:
0    block
5    goroutine
3    heap
0    mutex
9    threadcreate

full goroutine stack dump
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个页面中有许多子页面，咱们继续深究下去，看看可以得到什么？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cpu（CPU Profiling）: $HOST/debug/pprof/profile，默认进行 30s 的 CPU Profiling，得到一个分析用的 profile 文件
block（Block Profiling）：$HOST/debug/pprof/block，查看导致阻塞同步的堆栈跟踪
goroutine：$HOST/debug/pprof/goroutine，查看当前所有运行的 goroutines 堆栈跟踪
heap（Memory Profiling）: $HOST/debug/pprof/heap，查看活动对象的内存分配情况
mutex（Mutex Profiling）：$HOST/debug/pprof/mutex，查看导致互斥锁的竞争持有者的堆栈跟踪
threadcreate：$HOST/debug/pprof/threadcreate，查看创建新OS线程的堆栈跟踪
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;通过交互式终端使用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;（1）获取cpu相关：go tool pprof &lt;a href=&#34;http://localhost:6060/debug/pprof/profile?seconds=60&#34;&gt;http://localhost:6060/debug/pprof/profile?seconds=60&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go tool pprof http://localhost:6060/debug/pprof/profile\?seconds\=60

Fetching profile over HTTP from http://localhost:6060/debug/pprof/profile?seconds=60
Saved profile in /Users/eddycjy/pprof/pprof.samples.cpu.007.pb.gz
Type: cpu
Duration: 1mins, Total samples = 26.55s (44.15%)
Entering interactive mode (type &amp;quot;help&amp;quot; for commands, &amp;quot;o&amp;quot; for options)
(pprof)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行该命令后，需等待 60 秒（可调整 seconds 的值），pprof 会进行 CPU Profiling。结束后将默认进入 pprof 的交互式命令模式，可以对分析的结果进行查看或导出。具体可执行 pprof help 查看命令说明&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(pprof) top10
Showing nodes accounting for 25.92s, 97.63% of 26.55s total
Dropped 85 nodes (cum &amp;lt;= 0.13s)
Showing top 10 nodes out of 21
      flat  flat%   sum%        cum   cum%
    23.28s 87.68% 87.68%     23.29s 87.72%  syscall.Syscall
     0.77s  2.90% 90.58%      0.77s  2.90%  runtime.memmove
     0.58s  2.18% 92.77%      0.58s  2.18%  runtime.freedefer
     0.53s  2.00% 94.76%      1.42s  5.35%  runtime.scanobject
     0.36s  1.36% 96.12%      0.39s  1.47%  runtime.heapBitsForObject
     0.35s  1.32% 97.44%      0.45s  1.69%  runtime.greyobject
     0.02s 0.075% 97.51%     24.96s 94.01%  main.main.func1
     0.01s 0.038% 97.55%     23.91s 90.06%  os.(*File).Write
     0.01s 0.038% 97.59%      0.19s  0.72%  runtime.mallocgc
     0.01s 0.038% 97.63%     23.30s 87.76%  syscall.Write
flat：给定函数上运行耗时
flat%：同上的 CPU 运行耗时总比例
sum%：给定函数累积使用 CPU 总比例
cum：当前函数加上它之上的调用运行总耗时
cum%：同上的 CPU 运行耗时总比例
最后一列为函数名称，在大多数的情况下，我们可以通过这五列得出一个应用程序的运行情况，加以优化 🤔
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;（2）获取内存相关：go tool pprof &lt;a href=&#34;http://localhost:6060/debug/pprof/heap&#34;&gt;http://localhost:6060/debug/pprof/heap&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go tool pprof http://localhost:6060/debug/pprof/heap
Fetching profile over HTTP from http://localhost:6060/debug/pprof/heap
Saved profile in /Users/eddycjy/pprof/pprof.alloc_objects.alloc_space.inuse_objects.inuse_space.008.pb.gz
Type: inuse_space
Entering interactive mode (type &amp;quot;help&amp;quot; for commands, &amp;quot;o&amp;quot; for options)
(pprof) top
Showing nodes accounting for 837.48MB, 100% of 837.48MB total
      flat  flat%   sum%        cum   cum%
  837.48MB   100%   100%   837.48MB   100%  main.main.func1
-inuse_space：分析应用程序的常驻内存占用情况
-alloc_objects：分析应用程序的内存临时分配情况
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;（3）获取阻塞相关：go tool pprof &lt;a href=&#34;http://localhost:6060/debug/pprof/block&#34;&gt;http://localhost:6060/debug/pprof/block&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;（4）获取锁相关：go tool pprof &lt;a href=&#34;http://localhost:6060/debug/pprof/mutex&#34;&gt;http://localhost:6060/debug/pprof/mutex&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;PProf 可视化界面&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、web命令&lt;/p&gt;

&lt;p&gt;启动 PProf 可视化界面正常有两种方式，一种就是对cpu文件进行分析，比如上面bench的cpu.prof，或者交互生成的pprof.samples.cpu.007.pb.gz，在交互的命令中输入web，就可以通过web进行访问。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ (pprof) web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果出现 Could not execute dot; may need to install graphviz.，就是提示你要安装 graphviz 了 ，我们简单说明一下安装（详细请右拐谷歌）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install graphviz # for macos
apt install graphviz # for ubuntu
yum install graphviz # for centos
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 PProf 可视化界面，下面是url&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（1）Top
（2）Graph
框越大，线越粗代表它占用的时间越大哦
（3）Peek
（4）Source
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 PProf 的可视化界面，我们能够更方便、更直观的看到 Go 应用程序的调用链、使用情况等，并且在 View 菜单栏中，还支持如上多种方式的切换&lt;/p&gt;

&lt;p&gt;我们也可以直接在上面的命令行中直接操作，安装完成后，我们继续在上文的交互式终端里输入 web，注意，虽然这个命令的名字叫“web”，但它的实际行为是产生一个 .svg 文件，并调用你的系统里设置的默认打开 .svg 的程序打开它。如果你的系统里打开 .svg 的默认程序并不是浏览器（比如可能是你的代码编辑器），这时候你需要设置一下默认使用浏览器打开 .svg 文件，相信这难不倒你。&lt;/p&gt;

&lt;p&gt;2、PProf 火焰图&lt;/p&gt;

&lt;p&gt;另一种可视化数据的方法是火焰图，原先是 uber 开源的一个工具：go-torch，可以直接读取 golang profiling 数据，并生成一个火焰图的 svg 文件。火焰图 svg 文件可以通过浏览器打开，它对于调用图的最优点是它是动态的：可以通过点击每个方块来 zoom in 分析它上面的内容。&lt;/p&gt;

&lt;p&gt;现在这个项目已经合并到工具pprof中去了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go-torch is deprecated, use pprof instead
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要安装FlameGraph的脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/brendangregg/FlameGraph.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把flamegraph.pl拷到我们机器环境变量$PATH的路径中去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp flamegraph.pl /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在终端输入 flamegraph.pl -h 是否安装FlameGraph成功&lt;/p&gt;

&lt;p&gt;1.获取cpuprofile&lt;/p&gt;

&lt;p&gt;获取最近10秒程序运行的cpuprofile,-seconds参数不填默认为30。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go tool pprof http://127.0.0.1:8080/debug/pprof/profile -seconds 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等10s后会生成一个: pprof.samples.cpu.001.pb.gz文件&lt;/p&gt;

&lt;p&gt;2.生成火焰图&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go tool pprof -http=:8081 ~/pprof/pprof.samples.cpu.001.pb.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中-http=:8081会启动一个http服务,端口为8081,然后浏览器会弹出此文件的图解&lt;/p&gt;

&lt;p&gt;还是需要基于上面安装的graphviz，如果没有安装需要先安装一下。然后可以在界面选择&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/pprof/pprof.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在使用火焰图的时候，也是需要使用交互的命令的，比如在list函数的时候，也可以看出每个函数的调用时间。&lt;/p&gt;

&lt;h1 id=&#34;实战&#34;&gt;实战&lt;/h1&gt;

&lt;h2 id=&#34;获取问题程序&#34;&gt;获取问题程序&lt;/h2&gt;

&lt;p&gt;直接到github上下载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get -d github.com/wolfogre/go-pprof-practice
cd $GOPATH/src/github.com/wolfogre/go-pprof-practice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    // 略
    _ &amp;quot;net/http/pprof&amp;quot; // 会自动注册 handler 到 http server，方便通过 http 接口获取程序运行采样报告
    // 略
)

func main() {
    // 略

    runtime.GOMAXPROCS(1) // 限制 CPU 使用数，避免过载
    runtime.SetMutexProfileFraction(1) // 开启对锁调用的跟踪
    runtime.SetBlockProfileRate(1) // 开启对阻塞操作的跟踪

    go func() {
        // 启动一个 http server，注意 pprof 相关的 handler 已经自动注册过了
        if err := http.ListenAndServe(&amp;quot;:6060&amp;quot;, nil); err != nil {
            log.Fatal(err)
        }
        os.Exit(0)
    }()

    // 略
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go build
./go-pprof-practice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用pprof查看资源使用情况，这个在上面都详细说明过了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://localhost:6060/debug/pprof/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;排查cpu占比过高&#34;&gt;排查cpu占比过高&lt;/h2&gt;

&lt;p&gt;1、使用命令：go tool pprof &lt;a href=&#34;http://localhost:6060/debug/pprof/profile&#34;&gt;http://localhost:6060/debug/pprof/profile&lt;/a&gt; ，等待一会儿后，进入一个交互式终端：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/pprof/pprof1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、输入 top 命令，查看 CPU 占用较高的调用：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/pprof/pprof2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;很明显，CPU 占用过高是 github.com/wolfogre/go-pprof-practice/animal/felidae/tiger.(*Tiger).Eat 造成的。&lt;/p&gt;

&lt;p&gt;3、输入 list Eat，查看问题具体在代码的哪一个位置&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/pprof/pprof3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，是第 24 行那个一百亿次空循环占用了大量 CPU 时间，至此，问题定位成功！&lt;/p&gt;

&lt;p&gt;4、我们还可以使用web来形象的查看&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/pprof/pprof4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图中，tiger.(*Tiger).Eat 函数的框特别大，箭头特别粗，pprof 生怕你不知道这个函数的 CPU 占用很高，这张图还包含了很多有趣且有价值的信息。&lt;/p&gt;

&lt;h2 id=&#34;排查内存占用过高&#34;&gt;排查内存占用过高&lt;/h2&gt;

&lt;p&gt;同样命令：go tool pprof &lt;a href=&#34;http://localhost:6060/debug/pprof/heap，&#34;&gt;http://localhost:6060/debug/pprof/heap，&lt;/a&gt; 然后再一次使用 top、list 来定问问题代码，就不多说了&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/pprof/pprof5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到这次出问题的地方在 github.com/wolfogre/go-pprof-practice/animal/muridae/mouse.(*Mouse).Steal，可以看到，这里有个循环会一直向 m.buffer 里追加长度为 1 MiB 的数组，直到总容量到达 1 GiB 为止，且一直不释放这些内存，这就难怪会有这么高的内存占用了。&lt;/p&gt;

&lt;h2 id=&#34;其他&#34;&gt;其他&lt;/h2&gt;

&lt;p&gt;其实排查其他的问题都是通过相同的方式进行排查，top，list，web来看出问题所在，当然使用火焰图就更加的形象了。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 文件存储系统hdfs</title>
          <link>https://kingjcy.github.io/post/distributed/store/fs/hfds/</link>
          <pubDate>Sun, 15 Dec 2019 20:21:18 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/fs/hfds/</guid>
          <description>&lt;p&gt;Hadoop：一个分布式系统基础架构，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。&lt;/p&gt;

&lt;p&gt;Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;特点&#34;&gt;特点&lt;/h2&gt;

&lt;p&gt;优势&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;专为存储超大文件而设计：hdfs应该能够支持GB级别大小的文件；它应该能够提供很大的数据带宽并且能够在集群中拓展到成百上千个节点；它的一个实例应该能够支持千万数量级别的文件。&lt;/li&gt;
&lt;li&gt;适用于流式的数据访问：hdfs适用于批处理的情况而不是交互式处理；它的重点是保证高吞吐量而不是低延迟的用户响应&lt;/li&gt;
&lt;li&gt;容错性：完善的冗余备份机制&lt;/li&gt;
&lt;li&gt;支持简单的一致性模型：HDFS需要支持一次写入多次读取的模型，而且写入过程文件不会经常变化&lt;/li&gt;
&lt;li&gt;移动计算优于移动数据：HDFS提供了使应用计算移动到离它最近数据位置的接口&lt;/li&gt;
&lt;li&gt;兼容各种硬件和软件平台&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大量小文件：文件的元数据都存储在NameNode内存中，大量小文件会占用大量内存。&lt;/li&gt;
&lt;li&gt;低延迟数据访问：hdfs是专门针对高数据吞吐量而设计的&lt;/li&gt;
&lt;li&gt;多用户写入，任意修改文件&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基本架构&#34;&gt;基本架构&lt;/h1&gt;

&lt;p&gt;HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode，HDFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/hdfs2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;NameNode&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当一个客户端请求一个文件或者存储一个文件时，它需要先知道具体到哪个DataNode上存取，获得这些信息后，客户端再直接和这个DataNode进行交互，而这些信息的维护者就是NameNode。&lt;/p&gt;

&lt;p&gt;NameNode管理着文件系统命名空间，它维护这文件系统树及树中的所有文件和目录。NameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。对于实际文件数据的保存与操作，都是由DataNode负责。当一个客户端请求数据时，它仅仅是从NameNode中获取文件的元信息，而具体的数据传输不需要经过NameNode，是由客户端直接与相应的DataNode进行交互。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;SecondaryNameNode&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;SecondaryNameNode并不是NameNode的备份。我们从前面的介绍已经知道，所有HDFS文件的元信息都保存在NameNode的内存中。在NameNode启动时，它首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在了内存中，同时为了防止数据丢失，这些操作又会不断被持久化到本地edits文件中。&lt;/p&gt;

&lt;p&gt;Edits文件存在的目的是为了提高系统的操作效率，NameNode在更新内存中的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题而诞生的。&lt;/p&gt;

&lt;p&gt;SecondaryNameNode的角色就是定期的合并edits和fsimage文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;DataNode&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;DataNode是hdfs中的worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除、和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。当对hdfs文件系统进行读写时，NameNode告知客户端每个数据驻留在哪个DataNode，客户端直接与DataNode进行通信，DataNode还会与其它DataNode通信，复制这些块以实现冗余。&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;数据备份&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;HDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其它所有数据块大小都是一样的。数据块的大小和备份因子都是可以配置的。NameNode负责各个数据块的备份，DataNode会通过心跳的方式定期的向NameNode发送自己节点上的Block 报告，这个报告中包含了DataNode节点上的所有数据块的列表。&lt;/p&gt;

&lt;p&gt;文件副本的分布位置直接影响着HDFS的可靠性和性能。一个大型的HDFS文件系统一般都是需要跨很多机架的，不同机架之间的数据传输需要经过网关，并且，同一个机架中机器之间的带宽要大于不同机架机器之间的带宽。如果把所有的副本都放在不同的机架中，这样既可以防止机架失败导致数据块不可用，又可以在读数据时利用到多个机架的带宽，并且也可以很容易的实现负载均衡。但是，如果是写数据，各个数据块需要同步到不同的机架，会影响到写数据的效率。&lt;/p&gt;

&lt;p&gt;而在Hadoop中，如果副本数量是3的情况下，Hadoop默认是这么存放的，把第一个副本放到机架的一个节点上，另一个副本放到同一个机架的另一个节点上，把最后一个节点放到不同的机架上。这种策略减少了跨机架副本的个数提高了写的性能，也能够允许一个机架失败的情况，算是一个很好的权衡。&lt;/p&gt;

&lt;p&gt;关于副本的选择，在读的过程中，HDFS会选择最近的一个副本给请求者。&lt;/p&gt;

&lt;p&gt;关于安全模式，当 Hadoop的NameNode节点启动时，会进入安全模式阶段。在此阶段，DataNode会向NameNode上传它们数据块的列表，让 NameNode得到块的位置信息，并对每个文件对应的数据块副本进行统计。当最小副本条件满足时，即一定比例的数据块都达到最小副本数，系统就会退出安全模式，而这需要一定的延迟时间。当最小副本条件未达到要求时，就会对副本数不足的数据块安排DataNode进行复制，直至达到最小副本数。而在安全模式下，系统会处于只读状态，NameNode不会处理任何块的复制和删除命令。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;HDFS中的沟通协议&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有的HDFS中的沟通协议都是基于tcp/ip协议，一个客户端通过指定的tcp端口与NameNode机器建立连接，并通过ClientProtocol协议与NameNode交互。而DataNode则通过DataNode Protocol协议与NameNode进行沟通。HDFS的RCP(远程过程调用)对ClientProtocol和DataNode Protocol做了封装。按照HDFS的设计，NameNode不会主动发起任何请求，只会被动接受来自客户端或DataNode的请求。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;可靠性保证&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以允许DataNode失败。DataNode会定期（默认3秒）的向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时，NameNode把失败节点的数据（从另外的副本节点获取）备份到另外一个健康的节点。这保证了集群始终维持指定的副本数。&lt;/p&gt;

&lt;p&gt;可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果发现不匹配，NameNode同样会重新备份损坏的数据块。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;hdfs文件读取过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;hdfs有一个FileSystem实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。hdfs通过rpc调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回含有该块副本的DataNode的节点地址，另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就存储在客户端所在的节点上。&lt;/p&gt;

&lt;p&gt;hdfs会返回一个FSDataInputStream对象，FSDataInputStream类转而封装成DFSDataInputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;客户端发起读请求&lt;/li&gt;
&lt;li&gt;客户端与NameNode得到文件的块及位置信息列表&lt;/li&gt;
&lt;li&gt;客户端直接和DataNode交互读取数据&lt;/li&gt;
&lt;li&gt;读取完成关闭连接&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/hdfs4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当FSDataInputStream与DataNode通信时遇到错误，它会选取另一个较近的DataNode，并为出故障的DataNode做标记以免重复向其读取数据。FSDataInputStream还会对读取的数据块进行校验和确认，发现块损坏时也会重新读取并通知NameNode。&lt;/p&gt;

&lt;p&gt;这样设计的巧妙之处：&lt;/p&gt;

&lt;p&gt;1、让客户端直接联系DataNode检索数据，可以使hdfs扩展到大量的并发客户端，因为数据流就是分散在集群的每个节点上的，在运行MapReduce任务时，每个客户端就是一个DataNode节点。&lt;/p&gt;

&lt;p&gt;2、NameNode仅需相应块的位置信息请求（位置信息在内存中，速度极快），否则随着客户端的增加，NameNode会很快成为瓶颈。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;hdfs文件写入过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;hdfs有一个DistributedFileSystem实例，客户端通过调用这个实例的create()方法就可以创建文件。DistributedFileSystem会发送给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件，在创建文件前NameNode会做一些检查，如文件是否存在，客户端是否有创建权限等，若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog，若不通过会向客户端抛出IOException。创建成功之后DistributedFileSystem会返回一个FSDataOutputStream对象，客户端由此开始写入数据。&lt;/p&gt;

&lt;p&gt;同读文件过程一样，FSDataOutputStream类转而封装成DFSDataOutputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;客户端在向NameNode请求之前先写入文件数据到本地文件系统的一个临时文件&lt;/li&gt;
&lt;li&gt;待临时文件达到块大小时开始向NameNode请求DataNode信息&lt;/li&gt;
&lt;li&gt;NameNode在文件系统中创建文件并返回给客户端一个数据块及其对应DataNode的地址列表（列表中包含副本存放的地址）&lt;/li&gt;
&lt;li&gt;客户端通过上一步得到的信息把创建临时文件块flush到列表中的第一个DataNode&lt;/li&gt;
&lt;li&gt;当文件关闭，NameNode会提交这次文件创建，此时，文件在文件系统中可见&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面第四步描述的flush过程实际处理过程比较负杂，现在单独描述一下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;首先，第一个DataNode是以数据包(数据包一般4KB)的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。&lt;/li&gt;
&lt;li&gt;在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包&lt;/li&gt;
&lt;li&gt;第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点&lt;/li&gt;
&lt;li&gt;传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK,最终，第一个DataNode会向客户端发回一个ACK&lt;/li&gt;
&lt;li&gt;当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点。然后，客户端会向NameNode发送一个确认&lt;/li&gt;
&lt;li&gt;如果管道中的任何一个DataNode失败，管道会被关闭。数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点&lt;/li&gt;
&lt;li&gt;数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在hdfs中，供读取时进行完整性校验&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;hdfs文件删除过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;hdfs文件删除过程一般需要如下几步：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;一开始删除文件，NameNode只是重命名被删除的文件到/trash目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在/trash中文件会被保留一定间隔的时间（可配置，默认是6小时），在这期间，文件可以很容易的恢复，恢复只需要将文件从/trash移出即可。&lt;/li&gt;
&lt;li&gt;当指定的时间到达，NameNode将会把文件从命名空间中删除&lt;/li&gt;
&lt;li&gt;标记删除的文件块释放空间，HDFS文件系统显示空间增加&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- K8s Principle</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-principle/</link>
          <pubDate>Wed, 20 Nov 2019 20:22:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-principle/</guid>
          <description>&lt;p&gt;kubernetes是一种以容器为核心的，自动化部署应用程序的分布式的容器管理系统。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/k8s1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;核心组件&#34;&gt;核心组件&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;master&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;apiserver: 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；可以说是k8s的内部交互的网关总线。&lt;/li&gt;
&lt;li&gt;controller manager: 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；是控制器的管理者，负责很多的控制器。&lt;/li&gt;
&lt;li&gt;scheduler: 负责资源的调度，按照（预选优选）调度策略将Pod调度到相应的机器上；&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;node&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;kubelet: 负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；主要干活的对象，负责和docker进行交互，创建容器，维护容器。&lt;/li&gt;
&lt;li&gt;kube-proxy: 负责为Service提供cluster内部的服务发现和负载均衡；提供了一种网络模式。&lt;/li&gt;
&lt;li&gt;docker engine:  docker引擎，负责docker的创建和管理，containerd已经快速崛起了。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;数据库&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;etcd: 保存了整个集群的状态；&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;网络&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;fannel: 实现pod网络的互通&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了核心组件，还有一些推荐的插件Add-ons：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-dns负责为整个集群提供DNS服务，主要用于解决igress的负载均衡策略，如何找到相关的容器。从 Kubernetes v1.12 开始，CoreDNS 是推荐的 DNS 服务器，取代了kube-dns。&lt;/li&gt;
&lt;li&gt;Ingress Controller为服务提供外网入口，负载均衡。&lt;/li&gt;
&lt;li&gt;kube-state-metrics提供资源监控，主要是状态。&lt;/li&gt;
&lt;li&gt;Dashboard提供GUI，友好的界面。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;分层架构&#34;&gt;分层架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/k8s2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;根据作用可以将对应的功能进行分层&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基础设施层：container runtime、网络、存储等&lt;/li&gt;
&lt;li&gt;核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境。&lt;/li&gt;
&lt;li&gt;应用层：部署（无状态、有状态应用、Job等）和路由（服务发现、负载均衡等）&lt;/li&gt;
&lt;li&gt;管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等）&lt;/li&gt;
&lt;li&gt;接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦&lt;/li&gt;
&lt;li&gt;生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴。

&lt;ul&gt;
&lt;li&gt;Kubernetes 外部：日志、监控、配置管理、CI、CD、Workflow等&lt;/li&gt;
&lt;li&gt;Kubernetes 内部：CRI、CNI、CSI、镜像仓库、Cloud Provider、集群自身的配置和管理等。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;详细说明&#34;&gt;详细说明&lt;/h1&gt;

&lt;p&gt;kubernetes架构是master/node，下面我们对每个节点上的组件进行了解.&lt;/p&gt;

&lt;h2 id=&#34;master&#34;&gt;master&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-apiserver/&#34;&gt;k8s组件系列（一）&amp;mdash;- apiserver详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-scheduler/&#34;&gt;k8s组件系列（二）&amp;mdash;- scheduler详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller-manager/&#34;&gt;k8s组件系列（三）&amp;mdash;- controller-manager详解&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;node组件&#34;&gt;node组件&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-kubelet/&#34;&gt;k8s组件系列（四）&amp;mdash;- kubelet详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-proxy/&#34;&gt;k8s组件系列（五）&amp;mdash;- proxy详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/posts/cloud/paas/base/docker/docker&#34;&gt;k8s组件系列（六）&amp;mdash;- docker详解&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;数据库&#34;&gt;数据库&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/db/etcd&#34;&gt;k8s组件系列（七）&amp;mdash;- etcd详解&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;存储&#34;&gt;存储&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-store/&#34;&gt;k8s组件系列（八）&amp;mdash;- 存储&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;网络&#34;&gt;网络&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network/&#34;&gt;k8s组件系列（九）&amp;mdash;- 网络&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;核心插件&#34;&gt;核心插件&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-addons/&#34;&gt;k8s组件系列（十）&amp;mdash;- 核心插件&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;pod&#34;&gt;pod&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;pod&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、为什么需要pod？pause父容器是做什么的？&lt;/p&gt;

&lt;p&gt;Pod的概念，主要是在多个容器中，隐藏了docker中复杂的标志位以及管理docker容器、共享卷及其他docker资源的复杂性。同时也隐藏了不同容器运行环境的差异。&lt;/p&gt;

&lt;p&gt;原则上，任何人只需要创建一个父容器就可以配置docker来管理容器组之间的共享问题。这个父容器需要能够准确的知道如何去创建共享运行环境的容器，还能管理这些容器的生命周期。为了实现这个父容器的构想，kubernetes中，用pause容器来作为一个pod中所有容器的父容器。这个pause容器有两个核心的功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;它提供整个pod的Linux命名空间的基础。业务容器共享pause的ip（pod_ip）和挂载的volume（存储空间）&lt;/li&gt;
&lt;li&gt;启用PID命名空间，它在每个pod中都作为PID为1进程，并回收僵尸进程。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/pod.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/pod2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、pod是什么？&lt;/p&gt;

&lt;p&gt;pod是kubernetes定义的一种操作单位。连接了容器和管理。&lt;/p&gt;

&lt;p&gt;pod可以用一个或者多个容器组成。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;多个容器（sidecar模式）：在一个Pod中同时运行多个容器。一个Pod中也可以同时封装几个需要紧密耦合互相协作的容器，它们之间共享资源。这些在同一个Pod中的容器可以互相协作成为一个service单位——一个容器共享文件，另一个“sidecar”容器来更新这些文件。Pod将这些容器的存储资源作为一个实体来管理。比如：一种应用的实例，例如一个web站点，包含前端，后端，数据库这三个容器，放在一个pod中对外就是一个web服务。&lt;/li&gt;
&lt;li&gt;单个容器：一个Pod中运行一个容器。“每个Pod中一个容器”的模式是最常见的用法；在这种使用方式中，你可以把Pod想象成是单个容器的封装，kuberentes管理的是Pod而不是直接管理容器。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pod内部共享资源&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Pod中可以共享两种资源：网络和存储（挂载的volume）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pod内部的容器可以使用localhost互相通信。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可以Pod指定多个共享的Volume。Pod中的所有容器都可以访问共享的volume。Volume也可以用来持久化Pod中的存储资源，以防容器重启后文件丢失。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3、pod的特性&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;你很少会直接在kubernetes中创建单个Pod。因为Pod的生命周期是短暂的，用后即焚的实体。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Controller可以创建和管理多个Pod，提供副本管理、滚动升级和集群级别的自愈能力。例如，如果一个Node故障，Controller就能自动将该节点上的Pod调度到其他健康的Node上。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;状态&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;pod的五种状态&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pending pod已经创建，但是内部镜像还没有完全创建&lt;/li&gt;
&lt;li&gt;running 容器已经创建，至少有一个容器处于运行状态&lt;/li&gt;
&lt;li&gt;succeeded  pod内容器都成功终止，且不会重启&lt;/li&gt;
&lt;li&gt;failed  所有容器已经退出，至少有一个是因为发生错误而退出&lt;/li&gt;
&lt;li&gt;Unkown：由于某中原因apiserver无法获取到Pod的状态。通常是由于Master与pod所在的主机失去连接了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有一下其他的状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CrashLoopBackOff： 容器退出，kubelet正在将它重启
InvalidImageName： 无法解析镜像名称
ImageInspectError： 无法校验镜像
ErrImageNeverPull： 策略禁止拉取镜像
ImagePullBackOff： 正在重试拉取
RegistryUnavailable： 连接不到镜像中心
ErrImagePull： 通用的拉取镜像出错
CreateContainerConfigError： 不能创建kubelet使用的容器配置
CreateContainerError： 创建容器失败
m.internalLifecycle.PreStartContainer  执行hook报错
RunContainerError： 启动容器失败
PostStartHookError： 执行hook报错
ContainersNotInitialized： 容器没有初始化完毕
ContainersNotReady： 容器没有准备完毕
ContainerCreating：容器创建中
PodInitializing：pod 初始化中
DockerDaemonNotReady：docker还没有完全启动
NetworkPluginNotReady： 网络插件还没有完全启动
Terminating： 退出中
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;重启策略&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;pod的重启策略：kubelet将根据RestartPolicy的设置来进行相应的操作&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Always: 当容器失效时, 由kubelet自动重启该容器&lt;/li&gt;
&lt;li&gt;OnFailure: 当容器终止运行且退出码不为0时, 由kubelet自动重启该容器&lt;/li&gt;
&lt;li&gt;Never: 不论容器运行状态如何, kubelet都不会重启该容器&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;镜像拉取策略&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;支持三种ImagePullPolicy&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Always：不管镜像是否存在都会进行一次拉取。&lt;/li&gt;
&lt;li&gt;Never：不管镜像是否存在都不会进行拉取&lt;/li&gt;
&lt;li&gt;IfNotPresent：只有镜像不存在时，才会进行镜像拉取。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;默认为IfNotPresent，但:latest标签的镜像默认为Always。&lt;/li&gt;
&lt;li&gt;拉取镜像时docker会进行校验，如果镜像中的MD5码没有变，则不会拉取镜像数据。&lt;/li&gt;
&lt;li&gt;生产环境中应该尽量避免使用:latest标签，而开发环境中可以借助:latest标签自动拉取最新的镜像。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;资源限制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;spec.containers[].resources.limits.cpu：CPU上限，可以短暂超过，容器也不会被停止
spec.containers[].resources.limits.memory：内存上限，不可以超过；如果超过，容器可能会被停止或调度到其他资源充足的机器上
spec.containers[].resources.requests.cpu：CPU请求，可以超过
spec.containers[].resources.requests.memory：内存请求，可以超过；但如果超过，容器可能会在Node内存不足时清理
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cpu是以千分之一c为最小单位m，一般设置0.3核就是300m&lt;/p&gt;

&lt;p&gt;limit是上限，如果应用超过limit，会kill掉&lt;/p&gt;

&lt;p&gt;request给调度用的  调度在选择pod调度到那个node上，会看node上已经调度的pod所声明的request。你如果设置最小需求，那么你node上可能会被调度很多pod, 但是业务大的时候，会使得node的压力比较大，所以设置为正常的时候的需求，request不应该是一个下线值，而是一个运行参考值。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;钩子&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;容器生命周期钩子（Container Lifecycle Hooks）监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数。支持两种钩子：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;postStart： 容器启动后执行，注意由于是异步执行，它无法保证一定在ENTRYPOINT之后运行。如果失败，容器会被杀死，并根据RestartPolicy决定是否重启&lt;/li&gt;
&lt;li&gt;preStop：容器停止前执行，常用于资源清理。如果失败，容器同样也会被杀死&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而钩子的回调函数支持两种方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;exec：在容器内执行命令&lt;/li&gt;
&lt;li&gt;httpGet：向指定URL发起GET请求&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;postStart和preStop钩子示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: [&amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;echo Hello from the postStart handler &amp;gt; /usr/share/message&amp;quot;]
      preStop:
        exec:
          command: [&amp;quot;/usr/sbin/nginx&amp;quot;,&amp;quot;-s&amp;quot;,&amp;quot;quit&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;控制器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;pod只是运行的最小单元，我们一般不会直接使用pod。大部分情况下我们都是使用deployment（RS），deamonset，statefulset，job等控制器来完成部署调度使用，具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller/&#34;&gt;k8s控制器&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;扩缩容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pod 水平自动伸缩（Horizontal Pod Autoscaler）和垂直扩展（Vertical Pod Autoscaler）以及CA（ cluster-autoscaler）特性，可以说是很实用的特性，完全自动化实现了资源的充分利用，所以单独拿出来说说，具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-autoscaler/&#34;&gt;k8s autoscaler&lt;/a&gt;。。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;限制带宽&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以通过给Pod增加kubernetes.io/ingress-bandwidth和kubernetes.io/egress-bandwidth这两个annotation来限制Pod的网络带宽&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: qos
  annotations:
    kubernetes.io/ingress-bandwidth: 3M
    kubernetes.io/egress-bandwidth: 4M
spec:
  containers:
  - name: iperf3
    image: networkstatic/iperf3
    command:
    - iperf3
    - -s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;yaml模版&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实就是我们声明式操作的资源配置清单，具体参数详解可以查看k8s权威指南的第二章第四节。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;pod的配置管理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;configmap，下面有详解的使用说明&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;健康检查&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kubelet使用liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去（谁的程序还没几个bug呢）。&lt;/p&gt;

&lt;p&gt;Kubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态。该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。&lt;/p&gt;

&lt;p&gt;一、探测&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;liveness probe&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;两种探测都有三种方式&lt;/p&gt;

&lt;p&gt;1、基于命令的探测&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    image: gcr.io/google_containers/busybox
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该配置文件给Pod配置了一个容器。periodSeconds 规定kubelet要每隔5秒执行一次liveness probe。 initialDelaySeconds 告诉kubelet在第一次执行probe之前要的等待5秒钟。探针检测命令是在容器中执行 cat /tmp/healthy 命令。如果命令执行成功，将返回0，kubelet就会认为该容器是活着的并且很健康。如果返回非0值，kubelet就会杀掉这个容器并重启它。&lt;/p&gt;

&lt;p&gt;容器启动时，执行该命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/bin/sh -c &amp;quot;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在容器生命的最初30秒内有一个 /tmp/healthy 文件，在这30秒内 cat /tmp/healthy命令会返回一个成功的返回码。30秒后， cat /tmp/healthy 将返回失败的返回码。&lt;/p&gt;

&lt;p&gt;创建Pod：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/exec-liveness.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在30秒内，查看Pod的event：结果显示没有失败的liveness probe：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe pod liveness-exec
FirstSeen    LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
24s       24s     1   {default-scheduler }                    Normal      Scheduled   Successfully assigned liveness-exec to worker0
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulling     pulling image &amp;quot;gcr.io/google_containers/busybox&amp;quot;
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulled      Successfully pulled image &amp;quot;gcr.io/google_containers/busybox&amp;quot;
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Created     Created container with docker id 86849c15382e; Security:[seccomp=unconfined]
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Started     Started container with docker id 86849c15382e
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动35秒后，再次查看pod的event：在最下面有一条信息显示liveness probe失败，容器被删掉并重新创建。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe pod liveness-exec
FirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
37s       37s     1   {default-scheduler }                    Normal      Scheduled   Successfully assigned liveness-exec to worker0
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulling     pulling image &amp;quot;gcr.io/google_containers/busybox&amp;quot;
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulled      Successfully pulled image &amp;quot;gcr.io/google_containers/busybox&amp;quot;
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Created     Created container with docker id 86849c15382e; Security:[seccomp=unconfined]
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Started     Started container with docker id 86849c15382e
2s        2s      1   {kubelet worker0}   spec.containers{liveness}   Warning     Unhealthy   Liveness probe failed: cat: can&#39;t open &#39;/tmp/healthy&#39;: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再等30秒，确认容器已经重启：从输出结果来RESTARTS值加1了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pod liveness-exec
NAME            READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、基于HTTP请求&lt;/p&gt;

&lt;p&gt;我们还可以使用HTTP GET请求作为liveness probe。下面是一个基于gcr.io/google_containers/liveness镜像运行了一个容器的Pod的例子http-liveness.yaml：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    args:
    - /server
    image: gcr.io/google_containers/liveness
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
          - name: X-Custom-Header
            value: Awesome
      initialDelaySeconds: 3
      periodSeconds: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该配置文件只定义了一个容器，livenessProbe 指定kubelet需要每隔3秒执行一次liveness probe。initialDelaySeconds 指定kubelet在该执行第一次探测之前需要等待3秒钟。该探针将向容器中的server的8080端口发送一个HTTP GET请求。如果server的/healthz路径的handler返回一个成功的返回码，kubelet就会认定该容器是活着的并且很健康。如果返回失败的返回码，kubelet将杀掉该容器并重启它。&lt;/p&gt;

&lt;p&gt;任何大于200小于400的返回码都会认定是成功的返回码。其他返回码都会被认为是失败的返回码。&lt;/p&gt;

&lt;p&gt;最开始的10秒该容器是活着的， /healthz handler返回200的状态码。这之后将返回500的返回码。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/healthz&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
    duration := time.Now().Sub(started)
    if duration.Seconds() &amp;gt; 10 {
        w.WriteHeader(500)
        w.Write([]byte(fmt.Sprintf(&amp;quot;error: %v&amp;quot;, duration.Seconds())))
    } else {
        w.WriteHeader(200)
        w.Write([]byte(&amp;quot;ok&amp;quot;))
    }
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;容器启动3秒后，kubelet开始执行健康检查。第一次健康监测会成功，但是10秒后，健康检查将失败，kubelet将杀掉和重启容器。&lt;/p&gt;

&lt;p&gt;创建一个Pod来测试一下HTTP liveness检测：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/http-liveness.yaml

After 10 seconds, view Pod events to verify that liveness probes have failed and the Container has been restarted:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10秒后，查看Pod的event，确认liveness probe失败并重启了容器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe pod liveness-http
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、基于TCP liveness探针&lt;/p&gt;

&lt;p&gt;第三种liveness probe使用TCP Socket。 使用此配置，kubelet将尝试在指定端口上打开容器的套接字。 如果可以建立连接，容器被认为是健康的，如果不能就认为是失败的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: gcr.io/google_containers/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如您所见，TCP检查的配置与HTTP检查非常相似。 此示例同时使用了readiness和liveness probe。 容器启动后5秒钟，kubelet将发送第一个readiness probe。 这将尝试连接到端口8080上的goproxy容器。如果探测成功，则该pod将被标记为就绪。Kubelet将每隔10秒钟执行一次该检查。&lt;/p&gt;

&lt;p&gt;除了readiness probe之外，该配置还包括liveness probe。 容器启动15秒后，kubelet将运行第一个liveness probe。 就像readiness probe一样，这将尝试连接到goproxy容器上的8080端口。如果liveness probe失败，容器将重新启动。&lt;/p&gt;

&lt;p&gt;4、使用命名的端口&lt;/p&gt;

&lt;p&gt;可以使用命名的ContainerPort作为HTTP或TCP liveness检查：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ports:
- name: liveness-port
  containerPort: 8080
  hostPort: 8080

livenessProbe:
  httpGet:
  path: /healthz
  port: liveness-port
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;readiness probe&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1、定义readiness探针&lt;/p&gt;

&lt;p&gt;有时，应用程序暂时无法对外部流量提供服务。 例如，应用程序可能需要在启动期间加载大量数据或配置文件。 在这种情况下，你不想杀死应用程序，但你也不想发送请求。 Kubernetes提供了readiness probe来检测和减轻这些情况。 Pod中的容器可以报告自己还没有准备，不能处理Kubernetes服务发送过来的流量。&lt;/p&gt;

&lt;p&gt;Readiness probe的配置跟liveness probe很像。唯一的不同是使用 readinessProbe而不是livenessProbe。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;readinessProbe:
  exec:
    command:
    - cat
    - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readiness probe的HTTP和TCP的探测器配置跟liveness probe一样。&lt;/p&gt;

&lt;p&gt;Readiness和livenss probe可以并行用于同一容器。 使用两者可以确保流量无法到达未准备好的容器，并且容器在失败时重新启动。&lt;/p&gt;

&lt;p&gt;二、配置Probe&lt;/p&gt;

&lt;p&gt;Probe 中有很多精确和详细的配置，通过它们你能准确的控制liveness和readiness检查：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;initialDelaySeconds：容器启动后第一次执行探测是需要等待多少秒。
periodSeconds：执行探测的频率。默认是10秒，最小1秒。
timeoutSeconds：探测超时时间。默认1秒，最小1秒。
successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1。
failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HTTP probe 中可以给 httpGet设置其他配置项：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;host：连接的主机名，默认连接到pod的IP。你可能想在http header中设置&amp;quot;Host&amp;quot;而不是使用IP。
scheme：连接使用的schema，默认HTTP。
path: 访问的HTTP server的path。
httpHeaders：自定义请求的header。HTTP运行重复的header。
port：访问的容器的端口名字或者端口号。端口号必须介于1和65535之间。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于HTTP探测器，kubelet向指定的路径和端口发送HTTP请求以执行检查。 Kubelet将probe发送到容器的IP地址，除非地址被httpGet中的可选host字段覆盖。 在大多数情况下，你不想设置主机字段。 有一种情况下你可以设置它。 假设容器在127.0.0.1上侦听，并且Pod的hostNetwork字段为true。 然后，在httpGet下的host应该设置为127.0.0.1。 如果你的pod依赖于虚拟主机，这可能是更常见的情况，你不应该是用host，而是应该在httpHeaders中设置Host头。&lt;/p&gt;

&lt;h2 id=&#34;label&#34;&gt;label&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;label&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Label keys的语法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一个可选前缀+名称，通过/来区分&lt;/li&gt;
&lt;li&gt;名称部分是必须的，并且最多63个字符，开始和结束的字符必须是字母或者数字，中间是字母数字和_、-、.。&lt;/li&gt;
&lt;li&gt;前缀可选，如指定必须是个DNS子域，一系列的DNS label通过.来划分，长度不超过253个字符，“/”来结尾。如前缀被省略了，这个Label的key被假定为对用户私有的。系统组成部分（比如scheduler,controller-manager,apiserver,kubectl）,必须要指定一个前缀，Kuberentes.io前缀是为K8S内核部分保留的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;label value语法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;长度不超过63个字符。&lt;/li&gt;
&lt;li&gt;可以为空&lt;/li&gt;
&lt;li&gt;首位字符必须为字母数字字符&lt;/li&gt;
&lt;li&gt;中间必须是横线、_、.、数字、字母。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;主要用于label selector对其他的没有任何意义。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Label选择器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;基于相等性或者不相等性的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;environment = production
tier != frontend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个选择所有键等于 environment 值为 production 的资源。后一种选择所有键为 tier 值不等于 frontend 的资源，和那些没有键为 tier 的label的资源。&lt;/p&gt;

&lt;p&gt;要过滤所有处于 production 但不是 frontend 的资源，可以使用逗号操作符&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;environment=production,tier!=frontend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基于set的条件&lt;/p&gt;

&lt;p&gt;基于集合的label条件允许用一组值来过滤键。支持三种操作符: in ， notin ,和 exists(仅针对于key符号) 。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;environment in (production, qa)
tier notin (frontend, backend)
partition
!partitio
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;主要使用场景&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、kube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的pod副本的数量，实现pod数量的自动控制。&lt;/p&gt;

&lt;p&gt;2、kube-proxy进程通过service的Label Selector来选择对应的Pod，建立出对应的Pod的转发路由表。&lt;/p&gt;

&lt;p&gt;3、通过Node定义的Label，使用NodeSelector实现定向调度。&lt;/p&gt;

&lt;h2 id=&#34;service&#34;&gt;service&lt;/h2&gt;

&lt;p&gt;Kubernete Service 是一个定义了一组Pod的策略的抽象，我们也有时候叫做宏观服务，也可以是我们常说的微服务，之前说的pod，rs等就是服务。这些被服务标记的Pod都是（一般）通过label Selector决定的。可见service主要提供了负载均衡和服务发现的功能。&lt;/p&gt;

&lt;p&gt;我们已经能够通过ReplicaSet来创建一组Pod来提供具有高可用性的服务。虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pod IP仅仅是集群内可见的虚拟IP，外部无法访问。&lt;/li&gt;
&lt;li&gt;Pod IP会随着Pod的销毁而消失，当ReplicaSet对Pod进行动态伸缩时，Pod IP可能随时随地都会变化，这样对于我们访问这个服务带来了难度。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此，Kubernetes中的Service对象就是解决以上问题的实现服务发现核心关键。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;yaml模版&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;参考k8s权威指南第二章第五节。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Service同其他Kubernetes对象一样，也是通过yaml或json文件进行定义。此外，它和其他Controller对象一样，通过Label Selector来确定一个Service将要使用哪些Pod。一个简单的Service定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  labels:
    run: nginx
  name: nginx-service
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 81
  selector:
    app: nginx
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解析&lt;/p&gt;

&lt;p&gt;1、通过spec.selector字段确定这个Service将要使用哪些Label。在本例中，这个名为nginx的Service，将会管理所有具有app: nginxLabel的Pod。&lt;/p&gt;

&lt;p&gt;2、spec.ports.port: 80表明此Service将会监听80端口，并将所有监听到的请求转发给其管理的Pod。spec.ports.targetPort: 81表明此Service监听到的80端口的请求都会被转发给其管理的Pod的81端口，此字段可以省略，省略后其值会被设置为spec.ports.port的值。&lt;/p&gt;

&lt;p&gt;3、type: ClusterIP表面此Service的type，有如下几种&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ClusterIP。默认值。给这个Service分配一个Cluster IP，它是Kubernetes系统自动分配的虚拟IP，因此只能在集群内部访问。&lt;/li&gt;
&lt;li&gt;NodePort。将Service通过指定的Node上的端口暴露给外部。通过此方法，访问任意一个NodeIP:nodePort都将路由到ClusterIP，从而成功获得该服务。&lt;/li&gt;
&lt;li&gt;LoadBalancer。在 NodePort 的基础上，借助 cloud provider 创建一个外部的负载均衡器，并将请求转发到 &lt;NodeIP&gt;:NodePort。此模式只能在云服务器（AWS等）上使用。&lt;/li&gt;
&lt;li&gt;ExternalName。将服务通过 DNS CNAME 记录方式转发到指定的域名（通过 spec.externlName 设定）。需要 kube-dns 版本在 1.7 以上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;1、假如有3个app: nginx Pod运行在3个不同的Node中，那么此时客户端访问任意一个Node的30001端口都能访问到这个nginx服务。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    run: nginx
spec:
  selector:
    app: nginx
  ports:
  - port: 80
    nodePort: 30001
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、如果云服务商支持外接负载均衡器，则可以通过spec.type=LoadBalancer来定义Service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
  clusterIP: 10.0.171.239
  loadBalancerIP: 78.11.24.19
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 146.148.47.155
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;查看service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;kubectl discribe service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/service1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个 IP 地址就是 service 的 IP 地址（clusterIP），这个 IP 地址在集群里面可以被其它 pod 所访问，相当于通过这个 IP 地址提供了统一的一个 pod 的访问入口，以及服务发现。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/service2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;实际的架构如上图所示。在 service 创建之后，它会在集群里面创建一个虚拟的 IP 地址以及端口，在集群里，所有的 pod 和 node 都可以通过这样一个 IP 地址和端口去访问到这个 service。这个 service 会把它选择的 pod 及其 IP 地址都挂载到后端。这样通过 service 的 IP 地址访问时，就可以负载均衡到后端这些 pod 上面去。&lt;/p&gt;

&lt;p&gt;当 pod 的生命周期有变化时，比如说其中一个 pod 销毁，service 就会自动从后端摘除这个 pod。这样实现了：就算 pod 的生命周期有变化，它访问的端点是不会发生变化的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;访问service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;集群内&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1、首先我们可以通过 service 的虚拟 IP 去访问，比如说刚创建的 my-service 这个服务，通过 kubectl get svc 或者 kubectl discribe service 都可以看到它的虚拟 IP 地址是 172.29.3.27，端口是 80，然后就可以通过这个虚拟 IP 及端口在 pod 里面直接访问到这个 service 的地址。&lt;/p&gt;

&lt;p&gt;2、第二种方式直接访问服务名，依靠 DNS 解析，就是同一个 namespace 里 pod 可以直接通过 service 的名字去访问到刚才所声明的这个 service。不同的 namespace 里面，我们可以通过 service 名字加“.”，然后加 service 所在的哪个 namespace 去访问这个 service，例如我们直接用 curl 去访问，就是 my-service:80 就可以访问到这个 service。&lt;/p&gt;

&lt;p&gt;3、第三种是通过环境变量访问，在同一个 namespace 里的 pod 启动时，K8s 会把 service 的一些 IP 地址、端口，以及一些简单的配置，通过环境变量的方式放到 K8s 的 pod 里面。在 K8s pod 的容器启动之后，通过读取系统的环境变量比读取到 namespace 里面其他 service 配置的一个地址，或者是它的端口号等等。比如在集群的某一个 pod 里面，可以直接通过 curl $ 取到一个环境变量的值，比如取到 MY_SERVICE_SERVICE_HOST 就是它的一个 IP 地址，MY_SERVICE 就是刚才我们声明的 MY_SERVICE，SERVICE_PORT 就是它的端口号，这样也可以请求到集群里面的 MY_SERVICE 这个 service。&lt;/p&gt;

&lt;p&gt;Headless Service&lt;/p&gt;

&lt;p&gt;service 有一个特别的形态就是 Headless Service。service 创建的时候可以指定 clusterIP:None，告诉 K8s 说我不需要 clusterIP（就是刚才所说的集群里面的一个虚拟 IP），然后 K8s 就不会分配给这个 service 一个虚拟 IP 地址，它没有虚拟 IP 地址怎么做到负载均衡以及统一的访问入口呢？&lt;/p&gt;

&lt;p&gt;它是这样来操作的：pod 可以直接通过 service_name 用 DNS 的方式解析到所有后端 pod 的 IP 地址，通过 DNS 的 A 记录的方式会解析到所有后端的 Pod 的地址，由客户端选择一个后端的 IP 地址，这个 A 记录会随着 pod 的生命周期变化，返回的 A 记录列表也发生变化，这样就要求客户端应用要从 A 记录把所有 DNS 返回到 A 记录的列表里面 IP 地址中，客户端自己去选择一个合适的地址去访问 pod。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/service3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以从上图看一下跟刚才我们声明的模板的区别，就是在中间加了一个 clusterIP:None，即表明不需要虚拟 IP。实际效果就是集群的 pod 访问 my-service 时，会直接解析到所有的 service 对应 pod 的 IP 地址，返回给 pod，然后 pod 里面自己去选择一个 IP 地址去直接访问。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;集群外&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1、NodePort 的方式就是在集群的 node 上面（即集群的节点的宿主机上面）去暴露节点上的一个端口，这样相当于在节点的一个端口上面访问到之后就会再去做一层转发，转发到虚拟的 IP 地址上面，就是刚刚宿主机上面 service 虚拟 IP 地址。&lt;/p&gt;

&lt;p&gt;2、也可以直接把容器的port直接映射到node上，hostNetWork=true&lt;/p&gt;

&lt;p&gt;3、LoadBalancer 类型就是在 NodePort 上面又做了一层转换，刚才所说的 NodePort 其实是集群里面每个节点上面一个端口，LoadBalancer 是在所有的节点前又挂一个负载均衡。比如在阿里云上挂一个 SLB，这个负载均衡会提供一个统一的入口，并把所有它接触到的流量负载均衡到每一个集群节点的 node pod 上面去。然后 node pod 再转化成 ClusterIP，去访问到实际的 pod 上面。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;没有 selector 的 Service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;服务最常见的是抽象化对 Kubernetes Pod 的访问，但是它们也可以抽象化其他种类的后端。 实例:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;希望在生产环境中使用外部的数据库集群，但测试环境使用自己的数据库。&lt;/li&gt;
&lt;li&gt;希望服务指向另一个 命名空间 中或其它集群中的服务。&lt;/li&gt;
&lt;li&gt;您正在将工作负载迁移到 Kubernetes。 在评估该方法时，您仅在 Kubernetes 中运行一部分后端。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在任何这些场景中，都能够定义没有 selector 的 Service。 实例:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于此服务没有选择器，因此 不会 自动创建相应的 Endpoint 对象。 您可以通过手动添加 Endpoint 对象，将服务手动映射到运行该服务的网络地址和端口：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Endpoints
metadata:
  name: my-service
subsets:
  - addresses:
      - ip: 192.0.2.42
    ports:
      - port: 9376
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问没有 selector 的 Service，与有 selector 的 Service 的原理相同。 请求将被路由到用户定义的 Endpoint， YAML中为: 192.0.2.42:9376 (TCP)。&lt;/p&gt;

&lt;p&gt;ExternalName Service 是 Service 的特例，它没有 selector，也没有使用 DNS 名称代替。&lt;/p&gt;

&lt;h2 id=&#34;volume-pv-pvc-storageclass&#34;&gt;Volume,pv,pvc,StorageClass&lt;/h2&gt;

&lt;p&gt;Kubernetes中存储中有四个重要的概念：Volume、PersistentVolume PV、PersistentVolumeClaim PVC、StorageClass。掌握了这四个概念，就掌握了Kubernetes中存储系统的核心。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Volumes是最基础的存储抽象，其支持多种类型，包括本地存储、NFS、FC以及众多的云存储，我们也可以编写自己的存储插件来支持特定的存储系统。Volume可以被Pod直接使用，也可以被PV使用。普通的Volume和Pod之间是一种静态的绑定关系，在定义Pod的同时，通过volume属性来定义存储的类型，通过volumeMount来定义容器内的挂载点。&lt;/li&gt;
&lt;li&gt;PersistentVolume。与普通的Volume不同，PV是Kubernetes中的一个资源对象，创建一个PV相当于创建了一个存储资源对象，这个资源的使用要通过PVC来请求。&lt;/li&gt;
&lt;li&gt;PersistentVolumeClaim。PVC是用户对存储资源PV的请求，根据PVC中指定的条件Kubernetes动态的寻找系统中的PV资源并进行绑定。目前PVC与PV匹配可以通过StorageClassName、matchLabels或者matchExpressions三种方式。&lt;/li&gt;
&lt;li&gt;StorageClass就是动态创建pv。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体使用可以看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-store/&#34;&gt;k8s存储&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;configmap&#34;&gt;configmap&lt;/h2&gt;

&lt;p&gt;k8s提供了两种配置模式，一种就是正常的配置configmap，另外一种就是加密的secret。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;configmap的作用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;应用部署的一个最佳实战是将应用所需的配置信息与程序进行分离，这样可以使应用程序被更好的复用，通过不同的配置也能实现更灵活的功能，将应用打包为容器镜像后，可以通过环境变量或者外挂文件的方式在创建容器时进行配置注入，但在大规模容器集群的环境中，对多个容器进行不同的配置讲变得非常复杂，Kubernetes 1.2开始提供了一种统一的应用配置管理方案-configMap&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;configmap的用法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ConfigMap供容器使用的典型用法如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;生成为容器内的环境变量&lt;/li&gt;
&lt;li&gt;设置容器启动命令的启动参数（需设置为环境变量）&lt;/li&gt;
&lt;li&gt;以volume的形式挂载为容器内部的文件或者目录&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1、configMap编写变量注入pod中&lt;/p&gt;

&lt;p&gt;比如我们用configmap创建两个变量，一个是nginx_port=80，一个是nginx_server=192.168.254.13&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create configmap nginx-var --from-literal=nginx_port=80 --from-literal=nginx_server=192.168.254.13
configmap/nginx-var created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看configmap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get cm
NAME        DATA   AGE
nginx-var   2      5s


[root@master ~]# kubectl describe cm nginx-var
Name:         nginx-var
Namespace:    default
Labels:       &amp;lt;none&amp;gt;
Annotations:  &amp;lt;none&amp;gt;

Data
====
nginx_port:
----
80
nginx_server:
----
192.168.254.13
Events:  &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们创建pod，把这2个变量注入到环境变量当中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# cat test2.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-nginx
  namespace: default
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 30080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: web
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - name: nginx
          containerPort: 80
        volumeMounts:
          - name: html
            mountPath: /user/share/nginx/html/
        env:
        - name: TEST_PORT
          valueFrom:
            configMapKeyRef:
              name: nginx-var
              key: nginx_port
        - name: TEST_HOST
          valueFrom:
            configMapKeyRef:
              name: nginx-var
              key: nginx_server
      volumes:
      - name: html
        emptyDir: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行pod文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f test2.yaml
service/service-nginx created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看pod&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
mydeploy-d975ff774-fzv7g   1/1     Running   0          19s
mydeploy-d975ff774-nmmqt   1/1     Running   0          19s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入到容器中查看环境变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl exec -it mydeploy-d975ff774-fzv7g -- /bin/sh


# printenv
SERVICE_NGINX_PORT_80_TCP_PORT=80
KUBERNETES_PORT=tcp://10.96.0.1:443
SERVICE_NGINX_PORT_80_TCP_PROTO=tcp
KUBERNETES_SERVICE_PORT=443
HOSTNAME=mydeploy-d975ff774-fzv7g
SERVICE_NGINX_SERVICE_PORT_NGINX=80
HOME=/root
PKG_RELEASE=1~buster
SERVICE_NGINX_PORT_80_TCP=tcp://10.99.184.186:80
TEST_HOST=192.168.254.13
TEST_PORT=80
TERM=xterm
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
NGINX_VERSION=1.17.3
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
KUBERNETES_PORT_443_TCP_PORT=443
NJS_VERSION=0.3.5
KUBERNETES_PORT_443_TCP_PROTO=tcp
SERVICE_NGINX_SERVICE_HOST=10.99.184.186
SERVICE_NGINX_PORT=tcp://10.99.184.186:80
SERVICE_NGINX_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_SERVICE_HOST=10.96.0.1
PWD=/
SERVICE_NGINX_PORT_80_TCP_ADDR=10.99.184.186
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以发现configMap当中的环境变量已经注入到了pod容器当中&lt;/p&gt;

&lt;p&gt;这里要注意的是，如果是用这种环境变量的注入方式，pod启动后，如果在去修改configMap当中的变量，对于pod是无效的，如果是以卷的方式挂载，是可的实时更新的，这一点要清楚&lt;/p&gt;

&lt;p&gt;2、用configMap以存储卷的形式挂载到pod中&lt;/p&gt;

&lt;p&gt;上面说到了configMap以变量的形式虽然可以注入到pod当中，但是如果在修改变量的话pod是不会更新的，如果想让configMap中的配置跟pod内部的实时更新，就需要以存储卷的形式挂载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# cat test2.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-nginx
  namespace: default
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 30080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: web
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - name: nginx
          containerPort: 80
        volumeMounts:
          - name: html-config
            mountPath: /nginx/vars/
            readOnly: true
      volumes:
      - name: html-config
        configMap:
          name: nginx-var
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f test2.yaml
service/service-nginx created
deployment.apps/mydeploy created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看pod&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
mydeploy-6f6b6c8d9d-pfzjs   1/1     Running   0          90s
mydeploy-6f6b6c8d9d-r9rz4   1/1     Running   0          90s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入到容器中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl exec -it mydeploy-6f6b6c8d9d-pfzjs -- /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在容器中查看configMap对应的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@mydeploy-6f6b6c8d9d-pfzjs:/# cd /nginx/vars
root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars# ls
nginx_port  nginx_server
root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars# cat nginx_port
80
root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改configMap中的配置，把端口号从80修改成8080&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl edit cm nginx-var
# Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  nginx_port: &amp;quot;8080&amp;quot;
  nginx_server: 192.168.254.13
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2019-09-13T14:22:20Z&amp;quot;
  name: nginx-var
  namespace: default
  resourceVersion: &amp;quot;248779&amp;quot;
  selfLink: /api/v1/namespaces/default/configmaps/nginx-var
  uid: dfce8730-f028-4c57-b497-89b8f1854630
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改完稍等片刻查看文件档中的值，已然更新成8080&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars# cat nginx_port
8080
root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、configMap创建配置文件注入到pod当中&lt;/p&gt;

&lt;p&gt;这里以nginx配置文件为例子，我们在宿主机上配置好nginx的配置文件，创建configmap，最后通过configmap注入到容器中&lt;/p&gt;

&lt;p&gt;创建nginx配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# vim www.conf
server {
    server_name: 192.168.254.13;
    listen: 80;
    root /data/web/html/;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建configMap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create configmap nginx-config --from-file=/root/www.conf
configmap/nginx-config created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看configMap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get cm
NAME           DATA   AGE
nginx-config   1      3m3s
nginx-var      2      63m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建pod并挂载configMap存储卷&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# cat test2.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-nginx
  namespace: default
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 30080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: web
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - name: nginx
          containerPort: 80
        volumeMounts:
          - name: html-config
            mountPath: /etc/nginx/conf.d/
            readOnly: true
      volumes:
      - name: html-config
        configMap:
          name: nginx-config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动容器，并让容器启动的时候就加载configMap当中的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl create -f test2.yaml
service/service-nginx created
deployment.apps/mydeploy created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看容器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl get pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
mydeploy-fd46f76d6-jkq52   1/1     Running   0          22s   10.244.1.46   node1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问容器当中的网页，80端口是没问题的，8888端口访问不同&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# curl 10.244.1.46
this is test web


[root@master ~]# curl 10.244.1.46:8888
curl: (7) Failed connect to 10.244.1.46:8888; 拒绝连接
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来我们去修改configMap当中的内容，吧80端口修改成8888&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# kubectl edit cm nginx-config
# Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  www.conf: |
    server {
        server_name 192.168.254.13;
        listen 8888;
        root /data/web/html/;
    }
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2019-09-13T15:22:22Z&amp;quot;
  name: nginx-config
  namespace: default
  resourceVersion: &amp;quot;252615&amp;quot;
  selfLink: /api/v1/namespaces/default/configmaps/nginx-config
  uid: f1881f87-5a91-4b8e-ab39-11a2f45733c2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入到容器查看配置文件，可以发现配置文件已经修改过来了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@mydeploy-fd46f76d6-jkq52:/usr/bin# cat /etc/nginx/conf.d/www.conf
server {
    server_name 192.168.254.13;
    listen 8888;
    root /data/web/html/;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在去测试访问，发现还是报错，这是因为配置文件虽然已经修改了，但是nginx服务并没有加载配置文件，我们手动加载一下，以后可以用脚本形式自动完成加载文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# curl 10.244.1.46
this is test web
[root@master ~]# curl 10.244.1.46:8888
curl: (7) Failed connect to 10.244.1.46:8888; 拒绝连接
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在容器内部手动加载配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@mydeploy-fd46f76d6-jkq52:/usr/bin# nginx -s reload
2019/09/13 16:04:12 [notice] 34#34: signal process started
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再去测试访问，可以看到80端口已经访问不通，反而是我们修改的8888端口可以访问通&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ~]# curl 10.244.1.46
curl: (7) Failed connect to 10.244.1.46:80; 拒绝连接
[root@master ~]# curl 10.244.1.46:8888
this is test web
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;configmap的实际应用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、我们经常使用的就是设置pod的环境变量，比如一些IP和端口的设置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@001 ~]# kubectl get cm agent-config -n kube-system -o yaml
apiVersion: v1
data:
  voyage_agent_exporter_port: &amp;quot;969&amp;quot;
  voyage_agent_grpc_port: &amp;quot;966&amp;quot;
  voyage_agent_http_port: &amp;quot;968&amp;quot;
  voyage_agent_mulit_uplinks: &#39;{&amp;quot;ovs&amp;quot;:[&amp;quot;service0&amp;quot;, &amp;quot;service1&amp;quot;]}&#39;
  voyage_agent_netlink_timeout: &amp;quot;10000&amp;quot;
  voyage_agent_single_uplinks: service0,service1
  voyage_cni_config: |-
    {
      &amp;quot;cniVersion&amp;quot;: &amp;quot;0.3.1&amp;quot;,
      &amp;quot;name&amp;quot;: &amp;quot;voyage-net&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;voyage-cni&amp;quot;
    }
  voyage_server_grpc_port: &amp;quot;961&amp;quot;
  voyage_server_ip_list: 10.243.40.1,10.243.40.2,10.243.40.3
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,&amp;quot;data&amp;quot;:{&amp;quot;voyage_agent_exporter_port&amp;quot;:&amp;quot;969&amp;quot;,&amp;quot;voyage_agent_grpc_port&amp;quot;:&amp;quot;966&amp;quot;,&amp;quot;voyage_agent_http_port&amp;quot;:&amp;quot;968&amp;quot;,&amp;quot;voyage_agent_mulit_uplinks&amp;quot;:&amp;quot;{\&amp;quot;ovs\&amp;quot;:[\&amp;quot;service0\&amp;quot;, \&amp;quot;service1\&amp;quot;]}&amp;quot;,&amp;quot;voyage_agent_netlink_timeout&amp;quot;:&amp;quot;10000&amp;quot;,&amp;quot;voyage_agent_single_uplinks&amp;quot;:&amp;quot;service0,service1&amp;quot;,&amp;quot;voyage_cni_config&amp;quot;:&amp;quot;{\n  \&amp;quot;cniVersion\&amp;quot;: \&amp;quot;0.3.1\&amp;quot;,\n  \&amp;quot;name\&amp;quot;: \&amp;quot;voyage-net\&amp;quot;,\n  \&amp;quot;type\&amp;quot;: \&amp;quot;voyage-cni\&amp;quot;\n}&amp;quot;,&amp;quot;voyage_server_grpc_port&amp;quot;:&amp;quot;961&amp;quot;,&amp;quot;voyage_server_ip_list&amp;quot;:&amp;quot;10.243.40.1,10.243.40.2,10.243.40.3&amp;quot;},&amp;quot;kind&amp;quot;:&amp;quot;ConfigMap&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;voyage-agent-config&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;kube-system&amp;quot;}}
  creationTimestamp: &amp;quot;2020-01-19T18:03:55Z&amp;quot;
  name: voyage-agent-config
  namespace: kube-system
  resourceVersion: &amp;quot;692680451&amp;quot;
  selfLink: /api/v1/namespaces/kube-system/configmaps/voyage-agent-config
  uid: 0ebf9112-3ae6-11ea-ad4e-6c92bf8d8058
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在pod启动的时候可以设置这些探针，但是由于这种方式并不是实时的，如果修改还是需要重新发布，所以并不常用，对于这些配置我们设置一个配置中心，然后在每次发布的时候，拉去配置来设置环境变量，我们更多的是使用env，比如日志采集的环境变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:
  containers:
  - args:
    - --log.file=/opt/logs/app/test1.log
    - --log.interval=60s
    - --log.lineSize=500
    - --log.maxLines=10000000
    env:
    - name: test_log_app
      value: /opt/logs/app/*.log
    - name: test_log_app_prefix
      value: V1,ldcId,hostgroup,appId,ip,path,lid
    - name: appId
      value: loggen
    - name: test_log_app_brokerlist
      value: kafkasit02broker01.cnsuning.com:9092,kafkasit02broker02.cnsuning.com:9092,kafkasit02broker03.cnsuning.com:9092
    - name: test_log_app_topic
      value: ctdsa_nodejs_sit_njxz
    - name: KUBERNETES_POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    image: xgharborsit01.sncloud.com/sncloud/loggen:v0.0.1
    imagePullPolicy: IfNotPresent
    name: loggen
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /opt/logs
      name: log
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-gj6mr
      readOnly: true
  dnsPolicy: ClusterFirst
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、将应用的配置文件挂载，然后给应用程序启动使用，是我们最常用的，比如filebeat的yaml文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@xgpcc01m010243040001 ~]# kubectl get cm filebeat-config -n kube-system -o yaml
apiVersion: v1
data:
  filebeat-k8slog.yml: &amp;quot;filebeat.inputs:\n- type: log\n  enabled: true\n  close_eof:
    false\n  close_inactive: 5m\n  close_removed: false\n  close_renamed: false\n
    \ ignore_older: 48h\n  clean_inactive: 72h\n  clean_removed: true\n  paths:\n
    \ - \&amp;quot;\&amp;quot;\n  fields_under_root: true\n  fields:\n    brokerlist:\n    split: \&amp;quot;
    \       \&amp;quot;\noutput.kafka:\n  topic: \n  version: \&amp;quot;0.8.2.2\&amp;quot;\n  codec.format:\n
    \   ignoreNotFound: true\n    string: &#39;V1%{[split]}%{[ldc]}%{[split]}%{[hostgroup]}%{[split]}%{[appid]}%{[split]}%{[ip]}%{[split]}%{[path]}%{[split]}%{[lid]}%{[split]}%{[host.name]}%{[split]}%{[host.ip]}%{[split]}%{[@timestamp]}%{[split]}%{[message]}&#39;\n&amp;quot;
  filebeat.yml: |
    max_procs: 2
    queue:
      mem:
        events: 512
        flush.min_events: 256
    filebeat.inputs:
    - type: log
      enabled: false
      paths:
      - /var/log/filebeat-pause.log
    filebeat.config:
      inputs:
        enabled: true
        path: ${path.home}/inputs.d/*.yml
        reload.enabled: true
        reload.period: 10s
    output.kafka:
      topic: &amp;quot;%{[topic]}&amp;quot;
      version: &amp;quot;0.8.2.2&amp;quot;
      codec.format:
        ignoreNotFound: true
        string: &#39;%{[message]}&#39;
      metadata:
        retry.max: 2
        full: true
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2020-04-23T13:58:00Z&amp;quot;
  name: filebeat-config
  namespace: kube-system
  resourceVersion: &amp;quot;654788331&amp;quot;
  selfLink: /api/v1/namespaces/kube-system/configmaps/filebeat-config
  uid: 7153cf8e-856a-11ea-8bc6-6c92bf977c52
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看filebeat的资源配置清单filebeat.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@xgpcc01m010243040001 ~]# kubectl get ds filebeat -n kube-system -o yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  creationTimestamp: &amp;quot;2020-04-09T17:43:00Z&amp;quot;
  generation: 2
  labels:
    addon: filebeat
    app: filebeat
    namespace: kube-system
  name: filebeat
  namespace: kube-system
  resourceVersion: &amp;quot;741573898&amp;quot;
  selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/filebeat
  uid: 8e6ee5ef-7a89-11ea-a446-6c92bf8d8058
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      addon: filebeat
      app: filebeat
      namespace: kube-system
  template:
    metadata:
      creationTimestamp: null
      labels:
        addon: filebeat
        app: filebeat
        namespace: kube-system
    spec:
      containers:
      - args:
        - --path.home=/opt/filebeats/filebeat
        - --path.config=/etc/filebeat
        - --httpprof=:6060
        command:
        - /opt/filebeats/bin/filebeat
        env:
        - name: ldc
          value: NJXZ
        - name: appid
          value: PCC
        - name: envType
          value: prd
        - name: system_MwType_serviceID
          value: PCC_filebeat_01
        - name: cloudHostGroupId
          value: &amp;quot;01&amp;quot;
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        image: xgharbor01.sncloud.com/sncloud/filebeat:7.5.2-4
        imagePullPolicy: Always
        name: filebeat
        resources:
          limits:
            cpu: &amp;quot;3&amp;quot;
            memory: 2Gi
          requests:
            cpu: &amp;quot;1&amp;quot;
            memory: 800Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /host/var/lib/kubelet/pods
          mountPropagation: HostToContainer
          name: kubeletpods
          readOnly: true
        - mountPath: /opt/filebeats/filebeat
          name: k8slog
          subPath: filebeats/filebeat
        - mountPath: /etc/filebeat
          name: config
      - args:
        - --path.home=/opt/filebeats/filebeat-k8slog
        - --path.config=/etc/filebeat
        - -c
        - filebeat-k8slog.yml
        command:
        - /opt/filebeats/bin/filebeat
        env:
        - name: ldc
          value: NJXZ
        - name: appid
          value: PCC
        - name: envType
          value: prd
        - name: system_MwType_serviceID
          value: PCC_filebeat_01
        - name: cloudHostGroupId
          value: &amp;quot;01&amp;quot;
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        image: xgharbor01.sncloud.com/sncloud/filebeat:7.5.2-4
        imagePullPolicy: Always
        name: filebeat-k8slog
        resources:
          limits:
            cpu: 250m
            memory: 512Mi
          requests:
            cpu: 125m
            memory: 125Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /opt/filebeats/filebeat-k8slog
          name: k8slog
          subPath: filebeats/filebeat-k8slog
        - mountPath: /k8s_log
          name: k8slog
        - mountPath: /etc/filebeat
          name: config
      - args:
        - --path.base=/host
        - --path.template=filebeat.tpl
        - --path.filebeat-home=/opt/filebeats/filebeat
        - --path.logs=/opt/log-pilot/logs
        - --logLevel=debug
        - --logPrefix=sn
        command:
        - /opt/log-pilot/bin/log-pilot
        env:
        - name: ldc
          value: NJXZ
        - name: appid
          value: PCC
        - name: envType
          value: prd
        - name: system_MwType_serviceID
          value: PCC_filebeat_01
        - name: cloudHostGroupId
          value: &amp;quot;01&amp;quot;
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: xgharbor01.sncloud.com/sncloud/log-pilot:1.0.2
        imagePullPolicy: Always
        name: log-pilot
        resources:
          limits:
            cpu: 250m
            memory: 512Mi
          requests:
            cpu: 125m
            memory: 125Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /opt/filebeats/filebeat
          name: k8slog
          subPath: filebeats/filebeat
        - mountPath: /opt/log-pilot/logs
          name: k8slog
          subPath: log-pilot
        - mountPath: /host/var/lib/kubelet/pods
          mountPropagation: HostToContainer
          name: kubeletpods
          readOnly: true
        - mountPath: /var/run/docker.sock
          mountPropagation: HostToContainer
          name: docker-sock
          readOnly: true
      dnsPolicy: ClusterFirst
      hostNetwork: true
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      volumes:
      - configMap:
          defaultMode: 420
          name: filebeat-config
        name: config
      - hostPath:
          path: /var/lib/kubelet/pods
          type: &amp;quot;&amp;quot;
        name: kubeletpods
      - hostPath:
          path: /var/run/docker.sock
          type: &amp;quot;&amp;quot;
        name: docker-sock
      - hostPath:
          path: /k8s_log
          type: &amp;quot;&amp;quot;
        name: k8slog
  templateGeneration: 6
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 25%
    type: RollingUpdate
status:
  currentNumberScheduled: 128
  desiredNumberScheduled: 128
  numberAvailable: 128
  numberMisscheduled: 0
  numberReady: 128
  observedGeneration: 2
  updatedNumberScheduled: 128
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create  -f  config.yaml
configmap &amp;quot;special-config&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;用作环境变量&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;env&amp;quot;]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
      envFrom:
        - configMapRef:
            name: env-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当 Pod 结束后会输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SPECIAL_LEVEL_KEY=very
SPECIAL_TYPE_KEY=charm
log_level=INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;用作命令行参数&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将 ConfigMap 用作命令行参数时，需要先把 ConfigMap 的数据保存在环境变量中，然后通过 $(VAR_NAME) 的方式引用环境变量.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&amp;quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当 Pod 结束后会输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;very charm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;使用 volume 将 ConfigMap 作为文件或目录直接挂载&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将创建的 ConfigMap 直接挂载至 Pod 的 / etc/config 目录下，其中每一个 key-value 键值对都会生成一个文件，key 为文件名，value 为内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: vol-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;cat /etc/config/special.how&amp;quot;]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当 Pod 结束后会输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;very
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将创建的 ConfigMap 中 special.how 这个 key 挂载到 / etc/config 目录下的一个相对路径 / keys/special.level。如果存在同名文件，直接覆盖。其他的 key 不挂载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&amp;quot;/bin/sh&amp;quot;,&amp;quot;-c&amp;quot;,&amp;quot;cat /etc/config/keys/special.level&amp;quot;]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: special.how
          path: keys/special.level
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当 Pod 结束后会输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;very
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ConfigMap 支持同一个目录下挂载多个 key 和多个目录。例如下面将 special.how 和 special.type 通过挂载到 / etc/config 下。并且还将 special.how 同时挂载到 / etc/config2 下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&amp;quot;/bin/sh&amp;quot;,&amp;quot;-c&amp;quot;,&amp;quot;sleep 36000&amp;quot;]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
      - name: config-volume2
        mountPath: /etc/config2
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: special.how
          path: keys/special.level
        - key: special.type
          path: keys/special.type
    - name: config-volume2
      configMap:
        name: special-config
        items:
        - key: special.how
          path: keys/special.level
  restartPolicy: Never

# ls  /etc/config/keys/
special.level  special.type
# ls  /etc/config2/keys/
special.level
# cat  /etc/config/keys/special.level
very
# cat  /etc/config/keys/special.type
charm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 subpath 将 ConfigMap 作为单独的文件挂载到目录&lt;/p&gt;

&lt;p&gt;在一般情况下 configmap 挂载文件时，会先覆盖掉挂载目录，然后再将 congfigmap 中的内容作为文件挂载进行。如果想不对原来的文件夹下的文件造成覆盖，只是将 configmap 中的每个 key，按照文件的方式挂载到目录下，可以使用 subpath 参数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: nginx
      command: [&amp;quot;/bin/sh&amp;quot;,&amp;quot;-c&amp;quot;,&amp;quot;sleep 36000&amp;quot;]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/nginx/special.how
        subPath: special.how
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: special.how
          path: special.how
  restartPolicy: Never


root@dapi-test-pod:/# ls /etc/nginx/
conf.d    fastcgi_params    koi-utf  koi-win  mime.types  modules  nginx.conf  scgi_params    special.how  uwsgi_params  win-utf
root@dapi-test-pod:/# cat /etc/nginx/special.how
very
root@dapi-test-pod:/#
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;secret&#34;&gt;secret&lt;/h2&gt;

&lt;p&gt;Secret解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者Pod Spec中。Secret可以以Volume或者环境变量的方式使用。&lt;/p&gt;

&lt;p&gt;Secret有三种类型：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Service Account：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中；
Opaque：base64编码格式的Secret，用来存储密码、密钥等；
kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息。
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Opaque Secret&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Opaque类型的数据是一个map类型，要求value是base64编码格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo -n &amp;quot;admin&amp;quot; | base64
YWRtaW4=
$ echo -n &amp;quot;1f2d1e2e67df&amp;quot; | base64
MWYyZDFlMmU2N2Rm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;secrets.yml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: MWYyZDFlMmU2N2Rm
  username: YWRtaW4=
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着，就可以创建secret了：kubectl create -f secrets.yml。&lt;/p&gt;

&lt;p&gt;创建好secret之后，有两种方式来使用它：&lt;/p&gt;

&lt;p&gt;1、以Volume方式&lt;/p&gt;

&lt;p&gt;将Secret挂载到Volume中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  labels:
    name: db
  name: db
spec:
  volumes:
  - name: secrets
    secret:
      secretName: mysecret
  containers:
  - image: gcr.io/my_project_id/pg:v1
    name: db
    volumeMounts:
    - name: secrets
      mountPath: &amp;quot;/etc/secrets&amp;quot;
      readOnly: true
    ports:
    - name: cp
      containerPort: 5432
      hostPort: 5432
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、以环境变量方式&lt;/p&gt;

&lt;p&gt;将Secret导出到环境变量中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: wordpress-deployment
spec:
  replicas: 2
  strategy:
      type: RollingUpdate
  template:
    metadata:
      labels:
        app: wordpress
        visualize: &amp;quot;true&amp;quot;
    spec:
      containers:
      - name: &amp;quot;wordpress&amp;quot;
        image: &amp;quot;wordpress&amp;quot;
        ports:
        - containerPort: 80
        env:
        - name: WORDPRESS_DB_USER
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: username
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: password
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubernetes.io/dockerconfigjson&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以直接用kubectl命令来创建用于docker registry认证的secret：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
secret &amp;quot;myregistrykey&amp;quot; created.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以直接读取~/.docker/config.json的内容来创建：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat ~/.docker/config.json | base64
$ cat &amp;gt; myregistrykey.yaml &amp;lt;&amp;lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: myregistrykey
data:
  .dockerconfigjson: UmVhbGx5IHJlYWxseSByZWVlZWVlZWVlZWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGx5eXl5eXl5eXl5eXl5eXl5eXl5eSBsbGxsbGxsbGxsbGxsbG9vb29vb29vb29vb29vb29vb29vb29vb29vb25ubm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg==
type: kubernetes.io/dockerconfigjson
EOF

$ kubectl create -f myregistrykey.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在创建Pod的时候，通过imagePullSecrets来引用刚创建的myregistrykey:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers:
    - name: foo
      image: janedoe/awesomeapp:v1
  imagePullSecrets:
    - name: myregistrykey
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Service Account&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Service Account用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run nginx --image nginx
deployment &amp;quot;nginx&amp;quot; created
$ kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
nginx-3137573019-md1u2   1/1       Running   0          13s
$ kubectl exec nginx-3137573019-md1u2 ls /run/secrets/kubernetes.io/serviceaccount
ca.crt
namespace
token
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;namespace&#34;&gt;namespace&lt;/h2&gt;

&lt;p&gt;namespace主要是实现资源的隔离，Namespace是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。比如我们将namespace命名为系统名，一个系统一个namespace。&lt;/p&gt;

&lt;p&gt;初始化的namespace&lt;/p&gt;

&lt;p&gt;在默认情况下，新的集群上有三个命名空间：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;default：向集群中添加对象而不提供命名空间，这样它会被放入默认的命名空间中。在创建替代的命名空间之前，该命名空间会充当用户新添加资源的主要目的地，无法删除。&lt;/li&gt;
&lt;li&gt;kube-public：kube-public命名空间的目的是让所有具有或不具有身份验证的用户都能全局可读。这对于公开bootstrap组件所需的集群信息非常有用。它主要是由Kubernetes自己管理。&lt;/li&gt;
&lt;li&gt;kube-system：kube-system命名空间用于Kubernetes管理的Kubernetes组件，一般规则是，避免向该命名空间添加普通的工作负载。它一般由系统直接管理，因此具有相对宽松的策略。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;创建命名空间&lt;/p&gt;

&lt;p&gt;方式一&lt;/p&gt;

&lt;p&gt;vi ns.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Namespace
metadata:
  name: custom-namespace     #这是命名空间的名称
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubectl create -f ns.yaml&lt;/p&gt;

&lt;p&gt;方式二&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create namespace custom-namespace
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;resource-quotas&#34;&gt;Resource Quotas&lt;/h2&gt;

&lt;p&gt;资源配额（Resource Quotas）是用来限制用户资源用量的一种机制。&lt;/p&gt;

&lt;p&gt;它的工作原理为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;资源配额应用在Namespace上，并且每个Namespace最多只能有一个ResourceQuota对象
开启计算资源配额后，创建容器时必须配置计算资源请求或限制（也可以用LimitRange设置默认值）
用户超额后禁止创建新的资源
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;资源配额的启用&lt;/p&gt;

&lt;p&gt;首先，在API Server启动时配置ResourceQuota adminssion control；然后在namespace中创建ResourceQuota对象即可。&lt;/p&gt;

&lt;p&gt;资源配额的类型&lt;/p&gt;

&lt;p&gt;1、计算资源，包括cpu和memory
    cpu, limits.cpu, requests.cpu
    memory, limits.memory, requests.memory&lt;/p&gt;

&lt;p&gt;2、存储资源，包括存储资源的总量以及指定storage class的总量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;requests.storage：存储资源总量，如500Gi
persistentvolumeclaims：pvc的个数
.storageclass.storage.k8s.io/requests.storage
.storageclass.storage.k8s.io/persistentvolumeclaims
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、对象数，即可创建的对象的个数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pods, replicationcontrollers, configmaps, secrets
resourcequotas, persistentvolumeclaims
services, services.loadbalancers, services.nodeports
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;计算资源示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    pods: &amp;quot;4&amp;quot;
    requests.cpu: &amp;quot;1&amp;quot;
    requests.memory: 1Gi
    limits.cpu: &amp;quot;2&amp;quot;
    limits.memory: 2Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对象个数示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
spec:
  hard:
    configmaps: &amp;quot;10&amp;quot;
    persistentvolumeclaims: &amp;quot;4&amp;quot;
    replicationcontrollers: &amp;quot;20&amp;quot;
    secrets: &amp;quot;10&amp;quot;
    services: &amp;quot;10&amp;quot;
    services.loadbalancers: &amp;quot;2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;LimitRange&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;默认情况下，Kubernetes中所有容器都没有任何CPU和内存限制。LimitRange用来给Namespace增加一个资源限制，包括最小、最大和默认资源。比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: LimitRange
metadata:
  name: mylimits
spec:
  limits:
  - max:
      cpu: &amp;quot;2&amp;quot;
      memory: 1Gi
    min:
      cpu: 200m
      memory: 6Mi
    type: Pod
  - default:
      cpu: 300m
      memory: 200Mi
    defaultRequest:
      cpu: 200m
      memory: 100Mi
    max:
      cpu: &amp;quot;2&amp;quot;
      memory: 1Gi
    min:
      cpu: 100m
      memory: 3Mi
    type: Container
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建limitrange&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/limits.yaml --namespace=limit-example
limitrange &amp;quot;mylimits&amp;quot; created
$ kubectl describe limits mylimits --namespace=limit-example
Name:   mylimits
Namespace:  limit-example
Type        Resource      Min      Max      Default Request      Default Limit      Max Limit/Request Ratio
----        --------      ---      ---      ---------------      -------------      -----------------------
Pod         cpu           200m     2        -                    -                  -
Pod         memory        6Mi      1Gi      -                    -                  -
Container   cpu           100m     2        200m                 300m               -
Container   memory        3Mi      1Gi      100Mi                200Mi              -
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;配额范围&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每个配额在创建时可以指定一系列的范围&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;范围  说明
Terminating podSpec.ActiveDeadlineSeconds&amp;gt;=0的Pod
NotTerminating  podSpec.activeDeadlineSeconds=nil的Pod
BestEffort  所有容器的requests和limits都没有设置的Pod（Best-Effort）
NotBestEffort   与BestEffort相反
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ingress&#34;&gt;Ingress&lt;/h2&gt;

&lt;p&gt;service和pod的IP仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到service在Node上暴露的NodePort上，然后再由kube-proxy将其转发给相关的Pod。&lt;/p&gt;

&lt;p&gt;而Ingress就是为进入集群的请求提供路由规则的集合，如下图所示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ingress可以给service提供集群外部访问的URL、负载均衡、SSL终止、HTTP路由等。为了配置这些Ingress规则，集群管理员需要部署一个Ingress controller，它监听Ingress和service的变化，并根据规则配置负载均衡并提供访问入口。&lt;/p&gt;

&lt;p&gt;Ingress格式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        backend:
          serviceName: test
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个Ingress都需要配置rules，目前Kubernetes仅支持http规则。上面的示例表示请求/testpath时转发到服务test的80端口。&lt;/p&gt;

&lt;p&gt;根据Ingress Spec配置的不同，Ingress可以分为以下几种类型：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;单服务Ingress&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;单服务Ingress即该Ingress仅指定一个没有任何规则的后端服务。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  backend:
    serviceName: testsvc
    servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注：单个服务还可以通过设置Service.Type=NodePort或者Service.Type=LoadBalancer来对外暴露。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;路由到多服务的Ingress&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;路由到多服务的Ingress即根据请求路径的不同转发到不同的后端服务上，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foo.bar.com -&amp;gt; 178.91.123.132 -&amp;gt; / foo    s1:80
                                 / bar    s2:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过下面的Ingress来定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        backend:
          serviceName: s1
          servicePort: 80
      - path: /bar
        backend:
          serviceName: s2
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用kubectl create -f创建完ingress后：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -
          foo.bar.com
          /foo          s1:80
          /bar          s2:80
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;虚拟主机Ingress&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;虚拟主机Ingress即根据名字的不同转发到不同的后端服务上，而他们共用同一个的IP地址，如下所示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foo.bar.com --|                 |-&amp;gt; foo.bar.com s1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-&amp;gt; bar.foo.com s2:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是一个基于Host header路由请求的Ingress：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: s1
          servicePort: 80
  - host: bar.foo.com
    http:
      paths:
      - backend:
          serviceName: s2
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注：没有定义规则的后端服务称为默认后端服务，可以用来方便的处理404页面。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;TLS Ingress&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;TLS Ingress通过Secret获取TLS私钥和证书(名为tls.crt和tls.key)，来执行TLS终止。如果Ingress中的TLS配置部分指定了不同的主机，则它们将根据通过SNI TLS扩展指定的主机名（假如Ingress controller支持SNI）在多个相同端口上进行复用。&lt;/p&gt;

&lt;p&gt;定义一个包含tls.crt和tls.key的secret：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
kind: Secret
metadata:
  name: testsecret
  namespace: default
type: Opaque
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ingress中引用secret：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: no-rules-map
spec:
  tls:
    - secretName: testsecret
  backend:
    serviceName: s1
    servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，不同Ingress controller支持的TLS功能不尽相同。 请参阅有关nginx，GCE或任何其他Ingress controller的文档，以了解TLS的支持情况。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;更新Ingress&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以通过kubectl edit ing name的方法来更新ingress：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -                       178.91.123.132
          foo.bar.com
          /foo          s1:80

$ kubectl edit ing test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这会弹出一个包含已有IngressSpec yaml文件的编辑器，修改并保存就会将其更新到kubernetes API server，进而触发Ingress Controller重新配置负载均衡：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: s1
          servicePort: 80
        path: /foo
  - host: bar.baz.com
    http:
      paths:
      - backend:
          serviceName: s2
          servicePort: 80
        path: /foo
..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新后：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -                       178.91.123.132
          foo.bar.com
          /foo          s1:80
          bar.baz.com
          /foo          s2:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，也可以通过kubectl replace -f new-ingress.yaml命令来更新，其中new-ingress.yaml是修改过的Ingress yaml。&lt;/p&gt;

&lt;h2 id=&#34;dns&#34;&gt;DNS&lt;/h2&gt;

&lt;p&gt;kubernetes 提供了 service 的概念可以通过 VIP 访问 pod 提供的服务，但是在使用的时候还有一个问题：怎么知道某个应用的 VIP？&lt;/p&gt;

&lt;p&gt;比如我们有两个应用，一个 app，一个 是 db，每个应用使用 rc 进行管理，并通过 service 暴露出端口提供服务。app 需要连接到 db 应用，我们只知道 db 应用的名称，但是并不知道它的 VIP 地址。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;最简单的办法是从 kubernetes 提供的 API 查询。但这是一个糟糕的做法，首先每个应用都要在启动的时候编写查询依赖服务的逻辑，这本身就是重复和增加应用的复杂度；其次这也导致应用需要依赖 kubernetes，不能够单独部署和运行。&lt;/li&gt;
&lt;li&gt;当然如果通过增加配置选项也是可以做到的，但这又是增加复杂度同时，在配置规模变大后难以维护。&lt;/li&gt;
&lt;li&gt;开始的时候，kubernetes 采用了 docker 使用过的方法——环境变量。每个 pod 启动时候，会把通过环境变量设置所有服务的 IP 和 port 信息，这样 pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息。这种方式服务和环境变量的匹配关系有一定的规范，使用起来也相对简单，但是有个很大的问题：依赖的服务必须在 pod 启动之前就存在，不然是不会出现在环境变量中的。&lt;/li&gt;
&lt;li&gt;更理想的方案是：应用能够直接使用服务的名字，不需要关心它实际的 ip 地址，中间的转换能够自动完成。名字和 ip 之间的转换就是 DNS 系统的功能，因此 kubernetes 也提供了 DNS 方法来解决这个问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DNS具体的实现和原理可以看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-addons/#dns&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;服务发现&#34;&gt;服务发现&lt;/h2&gt;

&lt;p&gt;服务发现是分布式架构里服务治理的重要组成部分，服务发现的的基本原理，可以参考&lt;a href=&#34;https://kingjcy.github.io/post/middleware/serverdiscovery/sd/&#34;&gt;这里&lt;/a&gt;。我们这里主要看服务发现的实现之一：k8s的服务发现&lt;/p&gt;

&lt;p&gt;在 K8s 里面，服务发现与负载均衡就是通过Service实现。通过下图我们可以看出，K8s Service 向上提供了外部网络以及 pod 网络的访问，即外部网络可以通过 service 去访问，pod 网络也可以通过 K8s Service 去访问。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/sd&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关于service实现的负载均衡和服务发现，主要是以下几个部分&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;k8s服务注册&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、环境变量： 当你创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。需要注意的是，要想一个Pod中注入某个Service的环境变量，则必须Service要先比该Pod创建。这一点，几乎使得这种方式进行服务发现不可用。&lt;/p&gt;

&lt;p&gt;比如，一个ServiceName为redis-master的Service，对应的ClusterIP:Port为10.0.0.11:6379，则其对应的环境变量为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;REDIS_MASTER_SERVICE_HOST=10.0.0.11
REDIS_MASTER_SERVICE_PORT=6379
REDIS_MASTER_PORT=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP_PROTO=tcp
REDIS_MASTER_PORT_6379_TCP_PORT=6379
REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后pod可以根据对应服务的环境变量来进行调用，可见这种方式需要前置条件，目前已经基本上不使用。&lt;/p&gt;

&lt;p&gt;2、直接使用配置来调用，已经不算服务注册与发现了，就是我们正常的配置调用，这个也是有很大的问题的，主要是在k8s集群的规模中配置太过复杂，根本难以配置和维护。&lt;/p&gt;

&lt;p&gt;3、k8s提供api查询对应的服务，这也可以解决，但这是一个糟糕的做法，首先每个应用都要在启动的时候编写查询依赖服务的逻辑，这本身就是重复和增加应用的复杂度；其次这也导致应用需要依赖 kubernetes，不能够单独部署和运行。&lt;/p&gt;

&lt;p&gt;4、DNS：这也是k8s官方强烈推荐的方式。可以通过cluster add-on的方式轻松的创建KubeDNS来对集群内的Service进行服务发现。核心就是DNS监控服务进行注册，服务调用通过DNS服务器进行解析ip调用，更多关于&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-principle/#dns&#34;&gt;DNS&lt;/a&gt;的内容在上面有说明。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;k8s域名解析&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kubernetes 中，域名的全称，必须是 service-name.namespace.svc.cluster.local 这种模式，服务名，就是Kubernetes中 Service 的名称，namespace就是namespace的名称。&lt;/p&gt;

&lt;p&gt;然后通过查询DNS服务器来获取对应的ip进行调用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;负载均衡&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以把 Kubernetes Service 理解为前端和后端两部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;前端：名称、IP 和端口等不变的部分。也就是我们前面主要的服务发现的注册和发现&lt;/li&gt;
&lt;li&gt;后端：符合特定标签选择条件的 Pod 集合。主要是用于后端的负载均衡。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;前端是稳定可靠的，它的名称、IP 和端口在 Service 的整个生命周期中都不会改变。前端的稳定性意味着无需担心客户端 DNS 缓存超时等问题。&lt;/p&gt;

&lt;p&gt;后端是高度动态的，其中包括一组符合标签选择条件的 Pod，会通过负载均衡的方式进行访问。这里的负载均衡是一个简单的 4 层轮询。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;服务发现流程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/service&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，K8s 服务发现以及 K8s Service 是这样整体的一个架构。&lt;/p&gt;

&lt;p&gt;在 K8s master 节点里面有 APIServer，就是统一管理 K8s 所有对象的地方，所有的组件都会注册到 APIServer 上面去监听这个对象的变化，比如说我们刚才的组件 pod 生命周期发生变化等这些事件。这里面最关键的有三个组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一个是 Cloud Controller Manager，负责去配置 LoadBalancer 的一个负载均衡器给外部去访问；&lt;/li&gt;
&lt;li&gt;另外一个就是 Coredns，就是通过 Coredns 去观测 APIServer 里面的 service 后端 pod 的一个变化，去配置 service 的 DNS 解析，实现可以通过 service 的名字直接访问到 service 的虚拟 IP，或者是 Headless 类型的 Service 中的 IP 列表的解析；&lt;/li&gt;
&lt;li&gt;然后在每个 node 里面会有 kube-proxy 这个组件，它通过监听 service 以及 pod 变化，然后实际去配置集群里面的 node pod 或者是虚拟 IP 地址的一个访问。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实际访问链路是什么样的呢？比如说从集群内部的一个 Client Pod3 去访问 Service，就类似于刚才所演示的一个效果。Client Pod3 首先通过 Coredns 这里去解析出 ServiceIP，Coredns 会返回给它 ServiceName 所对应的 service IP 是什么，这个 Client Pod3 就会拿这个 Service IP 去做请求，它的请求到宿主机的网络之后，就会被 kube-proxy 所配置的 iptables 或者 IPVS 去做一层拦截处理，之后去负载均衡到每一个实际的后端 pod 上面去，这样就实现了一个负载均衡以及服务发现。&lt;/p&gt;

&lt;p&gt;对于外部的流量，比如说刚才通过公网访问的一个请求。它是通过外部的一个负载均衡器 Cloud Controller Manager 去监听 service 的变化之后，去配置的一个负载均衡器，然后转发到节点上的一个 NodePort 上面去，NodePort 也会经过 kube-proxy 的一个配置的一个 iptables，把 NodePort 的流量转换成 ClusterIP，紧接着转换成后端的一个 pod 的 IP 地址，去做负载均衡以及服务发现。这就是整个 K8s 服务发现以及 K8s Service 整体的结构。&lt;/p&gt;

&lt;h2 id=&#34;安全机制&#34;&gt;安全机制&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/safe&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes 官方文档给出了上面这张图。描述了用户在访问或变更资源的之前，需要经过 APIServer 的认证机制、授权机制以及准入控制机制。这三个机制(简称3A)可以这样理解，先检查是否合法用户，再检查该请求的行为是否有权限，最后做进一步的验证或添加默认参数。&lt;/p&gt;

&lt;p&gt;这一块属于paas的中的安全云的基础，具体我们在&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/safe/safe/&#34;&gt;安全云&lt;/a&gt;中做说明。&lt;/p&gt;

&lt;h2 id=&#34;k8s的pod创建流程&#34;&gt;k8s的pod创建流程&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/create&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;具体的创建步骤包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;客户端提交创建请求，可以通过API Server的Restful API，也可以使用kubectl命令行工具。支持的数据类型包括JSON和YAML。&lt;/li&gt;
&lt;li&gt;API Server处理用户请求，存储Pod数据到etcd。&lt;/li&gt;
&lt;li&gt;控制器根据资源情况确定创建的pod&lt;/li&gt;
&lt;li&gt;调度器通过API Server查看未绑定的Pod。尝试为Pod分配主机。&lt;/li&gt;
&lt;li&gt;过滤主机 (调度预选)：调度器用一组规则过滤掉不符合要求的主机。比如Pod指定了所需要的资源量，那么可用资源比Pod需要的资源量少的主机会被过滤掉。&lt;/li&gt;
&lt;li&gt;主机打分(调度优选)：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把容一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等。&lt;/li&gt;
&lt;li&gt;选择主机：选择打分最高的主机，进行binding操作，结果存储到etcd中。&lt;/li&gt;
&lt;li&gt;kubelet根据调度结果执行Pod创建操作： 绑定成功后，scheduler会调用APIServer的API在etcd中创建一个boundpod对象，描述在一个工作节点上绑定运行的所有pod信息。运行在每个工作节点上的kubelet也会定期与etcd同步boundpod信息，一旦发现应该在该工作节点上运行的boundpod对象没有更新，则调用Docker API创建并启动pod内的容器。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;kubelet创建pod&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/create1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、获取Pod进行准入检查&lt;/p&gt;

&lt;p&gt;kubelet的事件源主要包含两个部分：静态Pod和Apiserver，我们这里只考虑普通的Pod，则会直接将Pod加入到PodManager来进行管理，并且进行准入检查&lt;/p&gt;

&lt;p&gt;准入检查主要包含两个关键的控制器：驱逐管理与预选检查驱逐管理主要是根据当前的资源压力，检测对应的Pod是否容忍当前的资源压力；预选检查则是根据当前活跃的容器和当前节点的信息来检查是否满足当前Pod的基础运行环境，例如亲和性检查，同时如果当前的Pod的优先级特别高或者是静态Pod，则会尝试为其进行资源抢占，会按照QOS等级逐级来进行抢占从而满足其运行环境&lt;/p&gt;

&lt;p&gt;2、创建事件管道与容器管理主线程&lt;/p&gt;

&lt;p&gt;kubelet接收到一个新创建的Pod首先会为其创建一个事件管道，并且启动一个容器管理的主线程消费管道里面的事件，并且会基于最后同步时间来等待当前kubelet中最新发生的事件(从本地的podCache中获取)，如果是一个新建的Pod，则主要是通过PLEG中更新时间操作，广播的默认空状态来作为最新的状态&lt;/p&gt;

&lt;p&gt;3、同步最新状态&lt;/p&gt;

&lt;p&gt;当从本地的podCache中获取到最新的状态信息和从事件源获取的Pod信息后，会结合当前当前statusManager和probeManager里面的Pod里面的容器状态来更新，从而获取当前感知到的最新的Pod状态&lt;/p&gt;

&lt;p&gt;4、准入控制检查&lt;/p&gt;

&lt;p&gt;之前的准入检查是Pod运行的资源硬性限制的检查，而这里的准入检查则是软状态即容器运行时和版本的一些软件运行环境检查，如果这里检查失败，则会讲对应的容器状态设置为Blocked&lt;/p&gt;

&lt;p&gt;5、更新容器状态&lt;/p&gt;

&lt;p&gt;在通过准入检查之后，会调用statusManager来进行POd最新状态的同步，此处可能会同步给apiserver&lt;/p&gt;

&lt;p&gt;6、Cgroup配置&lt;/p&gt;

&lt;p&gt;在更新完成状态之后会启动一个PodCOntainerManager主要作用则是为对应的Pod根据其QOS等级来进行Cgroup配置的更新&lt;/p&gt;

&lt;p&gt;7、Pod基础运行环境准备&lt;/p&gt;

&lt;p&gt;接下来kubelet会为Pod的创建准备基础的环境，包括Pod数据目录的创建、镜像秘钥的获取、等待volume挂载完成等操作创建Pod的数据目录主要是创建 Pod运行所需要的Pod、插件、Volume目录，并且会通过Pod配置的镜像拉取秘钥生成秘钥信息，到此kubelet创建容器的工作就已经基本完成&lt;/p&gt;

&lt;p&gt;8、container创建&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/create2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算Pod容器变更
计算容器变更主要包括：Pod的sandbox是否变更、短声明周期容器、初始化容器是否完成、业务容器是否已经完成，相应的我们会得到一个几个对应的容器列表：需要被kill掉的容器列表、需要启动的容器列表，注意如果我们的初始化容器未完成，则不会进行将要运行的业务容器加入到需要启动的容器列表，可以看到这个地方是两个阶段&lt;/li&gt;
&lt;li&gt;初始化失败尝试终止
如果之前检测到之前的初始化容器失败，则会检查当前Pod的所有容器和sandbox关联的容器如果有在运行的容器，会全部进行Kill操作，并且等待操作完成&lt;/li&gt;
&lt;li&gt;未知状态容器补偿
当一些Pod的容器已经运行，但是其状态仍然是Unknow的时候，在这个地方会进行统一的处理，全部kill掉，从而为接下来的重新启动做清理操作，此处和3.2只会进行一个分支，但核心的目标都是清理那些运行失败或者无法获取状态的容器&lt;/li&gt;
&lt;li&gt;创建容器沙箱
在启动Pod的容器之前，首先会为其创建一个sandbox容器，当前Pod的所有容器都和Pod对应的sandbox共享同一个namespace从而共享一个namespace里面的资源，创建Sandbox比较复杂，后续会继续介绍&lt;/li&gt;

&lt;li&gt;&lt;p&gt;启动Pod相关容器
Pod的容器目前分为三大类：短生命周期容器、初始化容器、业务容器，启动顺序也是从左到右依次进行,如果对于的容器创建失败，则会通过backoff机制来延缓容器的创建，这里我们顺便介绍下containerRuntime启动容器的流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;检查容器镜像是否拉取
镜像的拉取首先会进行对应容器镜像的拼接，然后将之前获取的拉取的秘钥信息和镜像信息，一起交给CRI运行时来进行底层容器镜像的拉取，当然这里也会各种backoff机制，从而避免频繁拉取失败影响kubelet的性能&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建容器配置
创建容器配置主要是为了容器的运行创建对应的配置数据，主要包括：Pod的主机名、域名、挂载的volume、configMap、secret、环境变量、挂载的设备信息、要挂载的目录信息、端口映射信息、根据环境生成执行的命令、日志目录等信息&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;调用runtimeService完成容器的创建
调用runtimeService传递容器的配置信息，调用CRI，并且最终调用容器的创建接口完成容器的状态&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;调用runtimeService启动容器
通过之前创建容器返回的容器ID，来进行对应的容器的启动，并且会为容器创建对应的日志目录&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行容器的回调钩子
如果容器配置了PostStart钩子，则会在此处进行对应钩子的执行，如果钩子的类型是Exec类则会调用CNI的EXec接口完成在容器内的执行&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 常用ID生成的方法</title>
          <link>https://kingjcy.github.io/post/architecture/id/</link>
          <pubDate>Wed, 16 Oct 2019 10:04:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/id/</guid>
          <description>&lt;p&gt;几乎所有的业务系统，都有生成一个记录标识的需求，全局唯一，趋势有序是记录标识生成的两大核心需求。&lt;/p&gt;

&lt;p&gt;如何高效生成趋势有序的全局唯一ID？&lt;/p&gt;

&lt;h1 id=&#34;auto-increment&#34;&gt;auto_increment&lt;/h1&gt;

&lt;p&gt;使用数据库的 auto_increment 来生成全局唯一递增ID&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;简单，使用数据库已有的功能&lt;/li&gt;
&lt;li&gt;能够保证唯一性&lt;/li&gt;
&lt;li&gt;能够保证递增性&lt;/li&gt;
&lt;li&gt;步长固定&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可用性难以保证：数据库常见架构是一主多从+读写分离，生成自增ID是写请求，主库挂了就玩不转了&lt;/li&gt;
&lt;li&gt;扩展性差，性能有上限：因为写入是单点，数据库主库的写性能决定ID的生成性能上限，并且难以扩展&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;改进方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;增加主库，避免写入单点&lt;/li&gt;
&lt;li&gt;数据水平切分，保证各主库生成的ID不重复&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/id/id.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所述，由1个写库变成3个写库，每个写库设置不同的auto_increment初始值，以及相同的增长步长，以保证每个数据库生成的ID是不同的（上图中库0生成0,3,6,9…，库1生成1,4,7,10，库2生成2,5,8,11…）&lt;/p&gt;

&lt;p&gt;改进后的架构保证了可用性，但缺点是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;丧失了ID生成的“绝对递增性”：先访问库0生成0,3，再访问库1生成1，可能导致在非常短的时间内，ID生成不是绝对递增的（这个问题不大，我们的目标是趋势递增，不是绝对递增）&lt;/li&gt;
&lt;li&gt;数据库的写压力依然很大，每次生成ID都要访问数据库&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决上述两个问题，引出了第二个常见的方案&lt;/p&gt;

&lt;h1 id=&#34;单点批量id生成服务&#34;&gt;单点批量ID生成服务&lt;/h1&gt;

&lt;p&gt;分布式系统之所以难，很重要的原因之一是“没有一个全局时钟，难以保证绝对的时序”，要想保证绝对的时序，还是只能使用单点服务，用本地时钟保证“绝对时序”。数据库写压力大，是因为每次生成ID都访问了数据库，可以使用批量的方式降低数据库写压力。&lt;/p&gt;

&lt;p&gt;数据库使用双master保证可用性，数据库中只存储当前ID的最大值，例如0。ID生成服务假设每次批量拉取6个ID，服务访问数据库，将当前ID的最大值修改为5，这样应用访问ID生成服务索要ID，ID生成服务不需要每次访问数据库，就能依次派发0,1,2,3,4,5这些ID了，当ID发完后，再将ID的最大值修改为11，就能再次派发6,7,8,9,10,11这些ID了，于是数据库的压力就降低到原来的1/6了。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;保证了ID生成的绝对递增有序&lt;/li&gt;
&lt;li&gt;大大的降低了数据库的压力，ID生成可以做到每秒生成几万几十万个&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务仍然是单点&lt;/li&gt;
&lt;li&gt;如果服务挂了，服务重启起来之后，继续生成ID可能会不连续，中间出现空洞（服务内存是保存着0,1,2,3,4,5，数据库中max-id是5，分配到3时，服务重启了，下次会从6开始分配，4和5就成了空洞，不过这个问题也不大）&lt;/li&gt;
&lt;li&gt;虽然每秒可以生成几万几十万个ID，但毕竟还是有性能上限，无法进行水平扩展&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;改进方法：&lt;/p&gt;

&lt;p&gt;单点服务的常用高可用优化方案是“备用服务”，也叫“影子服务”，有一个影子服务时刻处于备用状态，当主服务挂了的时候影子服务顶上。这个切换的过程对调用方是透明的，可以自动完成，常用的技术是vip+keepalived，具体就不在这里展开。&lt;/p&gt;

&lt;h1 id=&#34;uuid&#34;&gt;uuid&lt;/h1&gt;

&lt;p&gt;上述方案来生成ID，虽然性能大增，但由于是单点系统，总还是存在性能上限的。同时，上述两种方案，不管是数据库还是服务来生成ID，业务方Application都需要进行一次远程调用，比较耗时。有没有一种本地生成ID的方法，即高性能，又时延低呢？&lt;/p&gt;

&lt;p&gt;uuid是一种常见的方案：string ID =GenUUID();&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;本地生成ID，不需要进行远程调用，时延低&lt;/li&gt;
&lt;li&gt;扩展性好，基本可以认为没有性能上限&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无法保证趋势递增&lt;/li&gt;
&lt;li&gt;uuid过长，往往用字符串表示，作为主键建立索引查询效率低，常见优化方案为“转化为两个uint64整数存储”或者“折半存储”（折半后不能保证唯一性）&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;取当前毫秒数&#34;&gt;取当前毫秒数&lt;/h1&gt;

&lt;p&gt;uuid是一个本地算法，生成性能高，但无法保证趋势递增，且作为字符串ID检索效率低，有没有一种能保证递增的本地算法呢？&lt;/p&gt;

&lt;p&gt;取当前毫秒数是一种常见方案：uint64 ID = GenTimeMS();&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;本地生成ID，不需要进行远程调用，时延低&lt;/li&gt;
&lt;li&gt;生成的ID趋势递增&lt;/li&gt;
&lt;li&gt;生成的ID是整数，建立索引后查询效率高&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果并发量超过1000，会生成重复的ID&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我去，这个缺点要了命了，不能保证ID的唯一性。当然，使用微秒可以降低冲突概率，但每秒最多只能生成1000000个ID，再多的话就一定会冲突了，所以使用微秒并不从根本上解决问题。&lt;/p&gt;

&lt;h1 id=&#34;类snowflake算法&#34;&gt;类snowflake算法&lt;/h1&gt;

&lt;p&gt;snowflake是twitter开源的分布式ID生成算法，其核心思想是：一个long型的ID，使用其中41bit作为毫秒数，10bit作为机器编号，12bit作为毫秒内序列号。这个算法单机每秒内理论上最多可以生成1000*(2^12)，也就是400W的ID，完全能满足业务的需求。&lt;/p&gt;

&lt;p&gt;借鉴snowflake的思想，结合各公司的业务逻辑和并发量，可以实现自己的分布式ID生成算法。&lt;/p&gt;

&lt;p&gt;举例，假设某公司ID生成器服务的需求如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（1）单机高峰并发量小于1W，预计未来5年单机高峰并发量小于10W
（2）有2个机房，预计未来5年机房数量小于4个
（3）每个机房机器数小于100台
（4）目前有5个业务线有ID生成需求，预计未来业务线数量小于10个
（5）…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分析过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高位取从2016年1月1日到现在的毫秒数（假设系统ID生成器服务在这个时间之后上线），假设系统至少运行10年，那至少需要10年*365天*24小时*3600秒*1000毫秒=320*10^9，差不多预留39bit给毫秒数&lt;/li&gt;
&lt;li&gt;每秒的单机高峰并发量小于10W，即平均每毫秒的单机高峰并发量小于100，差不多预留7bit给每毫秒内序列号&lt;/li&gt;
&lt;li&gt;5年内机房数小于4个，预留2bit给机房标识&lt;/li&gt;
&lt;li&gt;每个机房小于100台机器，预留7bit给每个机房内的服务器标识
0 业务线小于10个，预留4bit给业务线标识&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样设计的64bit标识，可以保证：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个业务线、每个机房、每个机器生成的ID都是不同的&lt;/li&gt;
&lt;li&gt;同一个机器，每个毫秒内生成的ID都是不同的&lt;/li&gt;
&lt;li&gt;同一个机器，同一个毫秒内，以序列号区区分保证生成的ID是不同的&lt;/li&gt;
&lt;li&gt;将毫秒数放在最高位，保证生成的ID是趋势递增的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;由于“没有一个全局时钟”，每台服务器分配的ID是绝对递增的，但从全局看，生成的ID只是趋势递增的（有些服务器的时间早，有些服务器的时间晚）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后一个容易忽略的问题：&lt;/p&gt;

&lt;p&gt;生成的ID，例如message-id/ order-id/ tiezi-id，在数据量大时往往需要分库分表，这些ID经常作为取模分库分表的依据，为了分库分表后数据均匀，ID生成往往有“取模随机性”的需求，所以我们通常把每秒内的序列号放在ID的最末位，保证生成的ID是随机的。&lt;/p&gt;

&lt;p&gt;又如果，我们在跨毫秒时，序列号总是归0，会使得序列号为0的ID比较多，导致生成的ID取模后不均匀。解决方法是，序列号不是每次都归0，而是归一个0到9的随机数，这个地方。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 调用优化</title>
          <link>https://kingjcy.github.io/post/architecture/call/</link>
          <pubDate>Sat, 05 Oct 2019 10:04:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/call/</guid>
          <description>&lt;p&gt;第三方接口挂掉，最好是不影响我们自身服务的运行，但是我们没有办法控制第三方接口稳定，所以我们需要优化我们的调用架构。&lt;/p&gt;

&lt;h1 id=&#34;基本情况&#34;&gt;基本情况&lt;/h1&gt;

&lt;p&gt;我们以跨公网调用其他平台接口为例，很多时候，业务需要跨公网调用一个第三方服务提供的接口，为了避免每个调用方都依赖于第三方服务，往往会抽象一个服务，所以基本调用流程都是&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/call/call.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见基本流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;业务调用方调用内部service&lt;/li&gt;
&lt;li&gt;内部service跨公网调用第三方接口&lt;/li&gt;
&lt;li&gt;第三方接口返回结果给内部service&lt;/li&gt;
&lt;li&gt;内部service返回结果给业务调用方&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;内部服务可能对上游业务提供了很多服务接口，当有一个接口跨公网第三方调用超时时，可能导致所有接口都不可用，即使大部分接口不依赖于跨公网第三方调用。&lt;/p&gt;

&lt;p&gt;而且我们经常会遇到这种情况，内部服务对业务方提供的N个接口，会共用服务容器内的工作线程（假设有100个工作线程）。假设这N个接口的某个接口跨公网依赖于第三方的接口，发生了网络抖动，或者接口超时（不妨设超时时间为5秒）。&lt;/p&gt;

&lt;h1 id=&#34;解决方案&#34;&gt;解决方案&lt;/h1&gt;

&lt;p&gt;最常想到的就是，加大调用次数，降低超时时间，或者拆分业务，但是都不能重根本上解决问题。&lt;/p&gt;

&lt;h2 id=&#34;异步代理法&#34;&gt;异步代理法&lt;/h2&gt;

&lt;p&gt;增加一个代理，向服务屏蔽究竟是“本地实时”还是“异步远程”去获取返回结果&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/call/call1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;本地实时流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;业务调用方调用内部service&lt;/li&gt;
&lt;li&gt;内部service调用异步代理service&lt;/li&gt;
&lt;li&gt;异步代理service通过OpenID在本地拿取数据&lt;/li&gt;
&lt;li&gt;异步代理service将数据返回内部service&lt;/li&gt;
&lt;li&gt;内部service返回结果给业务调用方&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;异步远程流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;异步代理service定期跨公网调用微信服务&lt;/li&gt;
&lt;li&gt;微信服务返回数据&lt;/li&gt;
&lt;li&gt;刷新本地数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样公网抖动，第三方接口超时，不影响内部接口调用，只会返回的不是最新的数据，很多业务场景都是适用允许的。&lt;/p&gt;

&lt;h2 id=&#34;第三方接口备份与切换法&#34;&gt;第三方接口备份与切换法&lt;/h2&gt;

&lt;p&gt;同时使用（或者备份）多个第三方服务。这样就可以调用第一个三方接口超时后，调用第二个备份服务，未来都直接调用备份服务，直到超时的服务恢复，这样公网抖动，第三方接口超时，不影响内部接口调用（初期少数几个请求会超时），但是不是所有公网调用都能够像短息网关，电子合同服务一样有备份接口的，像微信、支付宝等就只此一家&lt;/p&gt;

&lt;h2 id=&#34;异步调用法&#34;&gt;异步调用法&lt;/h2&gt;

&lt;p&gt;本地调用成功就返回成功，异步调用第三方接口同步数据（和异步代理有微小差别），其实就是将数据拉到本地。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Net/Http 应用层</title>
          <link>https://kingjcy.github.io/post/golang/go-net-http/</link>
          <pubDate>Thu, 26 Sep 2019 17:05:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-net-http/</guid>
          <description>&lt;p&gt;http包提供了HTTP协议的客户端和服务端的实现。&lt;/p&gt;

&lt;h1 id=&#34;http客户端&#34;&gt;HTTP客户端&lt;/h1&gt;

&lt;h2 id=&#34;直接使用http方法&#34;&gt;直接使用http方法&lt;/h2&gt;

&lt;p&gt;直接使用http方法，其实就是使用标准库默认的结构体client，transport等来实现请求。&lt;/p&gt;

&lt;p&gt;http包中封装了Get、Head、Post和PostForm函数可以直接发出HTTP/ HTTPS请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resp, err := http.Get(&amp;quot;http://example.com/&amp;quot;)
...
resp, err := http.Post(&amp;quot;http://example.com/upload&amp;quot;, &amp;quot;image/jpeg&amp;quot;, &amp;amp;buf)
...
resp, err := http.PostForm(&amp;quot;http://example.com/form&amp;quot;,
    url.Values{&amp;quot;key&amp;quot;: {&amp;quot;Value&amp;quot;}, &amp;quot;id&amp;quot;: {&amp;quot;123&amp;quot;}})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序在使用完回复后必须关闭回复的主体。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resp, err := http.Get(&amp;quot;http://example.com/&amp;quot;)
if err != nil {
    // handle error
}
defer resp.Body.Close()
body, err := ioutil.ReadAll(resp.Body)
// ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;原理解析&#34;&gt;原理解析&lt;/h3&gt;

&lt;p&gt;http直接提供的Post等方法实现在client.go文件中，以Post为例，其他都是一样&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Post(url, contentType string, body io.Reader) (resp *Response, err error) {
    return DefaultClient.Post(url, contentType, body)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际是调用了默认结构体client的Post方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var DefaultClient = &amp;amp;Client{}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看Post方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) Post(url, contentType string, body io.Reader) (resp *Response, err error) {
    req, err := NewRequest(&amp;quot;POST&amp;quot;, url, body)
    if err != nil {
        return nil, err
    }
    req.Header.Set(&amp;quot;Content-Type&amp;quot;, contentType)
    return c.Do(req)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据url和请求体body新建一个reqest，然后调用DefaultClient的Do方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) Do(req *Request) (*Response, error) {
    return c.do(req)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用内部的do方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) do(req *Request) (retres *Response, reterr error) {
    if testHookClientDoResult != nil {
        defer func() { testHookClientDoResult(retres, reterr) }()
    }
    if req.URL == nil {
        req.closeBody()
        return nil, &amp;amp;url.Error{
            Op:  urlErrorOp(req.Method),
            Err: errors.New(&amp;quot;http: nil Request.URL&amp;quot;),
        }
    }

    var (
        deadline      = c.deadline()
        reqs          []*Request
        resp          *Response
        copyHeaders   = c.makeHeadersCopier(req)
        reqBodyClosed = false // have we closed the current req.Body?

        // Redirect behavior:
        redirectMethod string
        includeBody    bool
    )
    uerr := func(err error) error {
        // the body may have been closed already by c.send()
        if !reqBodyClosed {
            req.closeBody()
        }
        var urlStr string
        if resp != nil &amp;amp;&amp;amp; resp.Request != nil {
            urlStr = stripPassword(resp.Request.URL)
        } else {
            urlStr = stripPassword(req.URL)
        }
        return &amp;amp;url.Error{
            Op:  urlErrorOp(reqs[0].Method),
            URL: urlStr,
            Err: err,
        }
    }
    for {
        // For all but the first request, create the next
        // request hop and replace req.
        if len(reqs) &amp;gt; 0 {
            loc := resp.Header.Get(&amp;quot;Location&amp;quot;)
            if loc == &amp;quot;&amp;quot; {
                resp.closeBody()
                return nil, uerr(fmt.Errorf(&amp;quot;%d response missing Location header&amp;quot;, resp.StatusCode))
            }
            u, err := req.URL.Parse(loc)
            if err != nil {
                resp.closeBody()
                return nil, uerr(fmt.Errorf(&amp;quot;failed to parse Location header %q: %v&amp;quot;, loc, err))
            }
            host := &amp;quot;&amp;quot;
            if req.Host != &amp;quot;&amp;quot; &amp;amp;&amp;amp; req.Host != req.URL.Host {
                // If the caller specified a custom Host header and the
                // redirect location is relative, preserve the Host header
                // through the redirect. See issue #22233.
                if u, _ := url.Parse(loc); u != nil &amp;amp;&amp;amp; !u.IsAbs() {
                    host = req.Host
                }
            }
            ireq := reqs[0]
            req = &amp;amp;Request{
                Method:   redirectMethod,
                Response: resp,
                URL:      u,
                Header:   make(Header),
                Host:     host,
                Cancel:   ireq.Cancel,
                ctx:      ireq.ctx,
            }
            if includeBody &amp;amp;&amp;amp; ireq.GetBody != nil {
                req.Body, err = ireq.GetBody()
                if err != nil {
                    resp.closeBody()
                    return nil, uerr(err)
                }
                req.ContentLength = ireq.ContentLength
            }

            // Copy original headers before setting the Referer,
            // in case the user set Referer on their first request.
            // If they really want to override, they can do it in
            // their CheckRedirect func.
            copyHeaders(req)

            // Add the Referer header from the most recent
            // request URL to the new one, if it&#39;s not https-&amp;gt;http:
            if ref := refererForURL(reqs[len(reqs)-1].URL, req.URL); ref != &amp;quot;&amp;quot; {
                req.Header.Set(&amp;quot;Referer&amp;quot;, ref)
            }
            err = c.checkRedirect(req, reqs)

            // Sentinel error to let users select the
            // previous response, without closing its
            // body. See Issue 10069.
            if err == ErrUseLastResponse {
                return resp, nil
            }

            // Close the previous response&#39;s body. But
            // read at least some of the body so if it&#39;s
            // small the underlying TCP connection will be
            // re-used. No need to check for errors: if it
            // fails, the Transport won&#39;t reuse it anyway.
            const maxBodySlurpSize = 2 &amp;lt;&amp;lt; 10
            if resp.ContentLength == -1 || resp.ContentLength &amp;lt;= maxBodySlurpSize {
                io.CopyN(ioutil.Discard, resp.Body, maxBodySlurpSize)
            }
            resp.Body.Close()

            if err != nil {
                // Special case for Go 1 compatibility: return both the response
                // and an error if the CheckRedirect function failed.
                // See https://golang.org/issue/3795
                // The resp.Body has already been closed.
                ue := uerr(err)
                ue.(*url.Error).URL = loc
                return resp, ue
            }
        }

        reqs = append(reqs, req)
        var err error
        var didTimeout func() bool
        //调用 send
        if resp, didTimeout, err = c.send(req, deadline); err != nil {
            // c.send() always closes req.Body
            reqBodyClosed = true
            if !deadline.IsZero() &amp;amp;&amp;amp; didTimeout() {
                err = &amp;amp;httpError{
                    // TODO: early in cycle: s/Client.Timeout exceeded/timeout or context cancelation/
                    err:     err.Error() + &amp;quot; (Client.Timeout exceeded while awaiting headers)&amp;quot;,
                    timeout: true,
                }
            }
            return nil, uerr(err)
        }

        var shouldRedirect bool
        redirectMethod, shouldRedirect, includeBody = redirectBehavior(req.Method, resp, reqs[0])
        if !shouldRedirect {
            return resp, nil
        }

        req.closeBody()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用send方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) send(req *Request, deadline time.Time) (resp *Response, didTimeout func() bool, err error) {
    if c.Jar != nil {
        for _, cookie := range c.Jar.Cookies(req.URL) {
            req.AddCookie(cookie)
        }
    }
    resp, didTimeout, err = send(req, c.transport(), deadline)
    if err != nil {
        return nil, didTimeout, err
    }
    if c.Jar != nil {
        if rc := resp.Cookies(); len(rc) &amp;gt; 0 {
            c.Jar.SetCookies(req.URL, rc)
        }
    }
    return resp, nil, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边需要确定实现transport的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) transport() RoundTripper {
    if c.Transport != nil {
        return c.Transport
    }
    return DefaultTransport
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用默认的DefaultTransport（如果transport自定义了，就使用自定义的，否则使用默认的），这边这个接口调用就是DefaultTransport，也就是Transport.go中的Transport结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var DefaultTransport RoundTripper = &amp;amp;Transport{
    Proxy: ProxyFromEnvironment,
    DialContext: (&amp;amp;net.Dialer{
        Timeout:   30 * time.Second,
        KeepAlive: 30 * time.Second,
        DualStack: true,
    }).DialContext,
    MaxIdleConns:          100,
    IdleConnTimeout:       90 * time.Second,
    TLSHandshakeTimeout:   10 * time.Second,
    ExpectContinueTimeout: 1 * time.Second,
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看Transport结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Transport struct {
    idleMu     sync.Mutex
    wantIdle   bool                                // user has requested to close all idle conns
    idleConn   map[connectMethodKey][]*persistConn // most recently used at end
    idleConnCh map[connectMethodKey]chan *persistConn
    idleLRU    connLRU

    reqMu       sync.Mutex
    reqCanceler map[*Request]func(error)

    altMu    sync.Mutex   // guards changing altProto only
    altProto atomic.Value // of nil or map[string]RoundTripper, key is URI scheme

    connCountMu          sync.Mutex
    connPerHostCount     map[connectMethodKey]int
    connPerHostAvailable map[connectMethodKey]chan struct{}

    // Proxy specifies a function to return a proxy for a given
    // Request. If the function returns a non-nil error, the
    // request is aborted with the provided error.
    //
    // The proxy type is determined by the URL scheme. &amp;quot;http&amp;quot;,
    // &amp;quot;https&amp;quot;, and &amp;quot;socks5&amp;quot; are supported. If the scheme is empty,
    // &amp;quot;http&amp;quot; is assumed.
    //
    // If Proxy is nil or returns a nil *URL, no proxy is used.
    Proxy func(*Request) (*url.URL, error)

    // DialContext specifies the dial function for creating unencrypted TCP connections.
    // If DialContext is nil (and the deprecated Dial below is also nil),
    // then the transport dials using package net.
    //
    // DialContext runs concurrently with calls to RoundTrip.
    // A RoundTrip call that initiates a dial may end up using
    // a connection dialed previously when the earlier connection
    // becomes idle before the later DialContext completes.
    DialContext func(ctx context.Context, network, addr string) (net.Conn, error)

    // Dial specifies the dial function for creating unencrypted TCP connections.
    //
    // Dial runs concurrently with calls to RoundTrip.
    // A RoundTrip call that initiates a dial may end up using
    // a connection dialed previously when the earlier connection
    // becomes idle before the later Dial completes.
    //
    // Deprecated: Use DialContext instead, which allows the transport
    // to cancel dials as soon as they are no longer needed.
    // If both are set, DialContext takes priority.
    Dial func(network, addr string) (net.Conn, error)

    // DialTLS specifies an optional dial function for creating
    // TLS connections for non-proxied HTTPS requests.
    //
    // If DialTLS is nil, Dial and TLSClientConfig are used.
    //
    // If DialTLS is set, the Dial hook is not used for HTTPS
    // requests and the TLSClientConfig and TLSHandshakeTimeout
    // are ignored. The returned net.Conn is assumed to already be
    // past the TLS handshake.
    DialTLS func(network, addr string) (net.Conn, error)

    // TLSClientConfig specifies the TLS configuration to use with
    // tls.Client.
    // If nil, the default configuration is used.
    // If non-nil, HTTP/2 support may not be enabled by default.
    TLSClientConfig *tls.Config

    // TLSHandshakeTimeout specifies the maximum amount of time waiting to
    // wait for a TLS handshake. Zero means no timeout.
    TLSHandshakeTimeout time.Duration

    // DisableKeepAlives, if true, disables HTTP keep-alives and
    // will only use the connection to the server for a single
    // HTTP request.
    //
    // This is unrelated to the similarly named TCP keep-alives.
    DisableKeepAlives bool

    // DisableCompression, if true, prevents the Transport from
    // requesting compression with an &amp;quot;Accept-Encoding: gzip&amp;quot;
    // request header when the Request contains no existing
    // Accept-Encoding value. If the Transport requests gzip on
    // its own and gets a gzipped response, it&#39;s transparently
    // decoded in the Response.Body. However, if the user
    // explicitly requested gzip it is not automatically
    // uncompressed.
    DisableCompression bool

    // MaxIdleConns controls the maximum number of idle (keep-alive)
    // connections across all hosts. Zero means no limit.
    MaxIdleConns int

    // MaxIdleConnsPerHost, if non-zero, controls the maximum idle
    // (keep-alive) connections to keep per-host. If zero,
    // DefaultMaxIdleConnsPerHost is used.
    MaxIdleConnsPerHost int

    // MaxConnsPerHost optionally limits the total number of
    // connections per host, including connections in the dialing,
    // active, and idle states. On limit violation, dials will block.
    //
    // Zero means no limit.
    //
    // For HTTP/2, this currently only controls the number of new
    // connections being created at a time, instead of the total
    // number. In practice, hosts using HTTP/2 only have about one
    // idle connection, though.
    MaxConnsPerHost int

    // IdleConnTimeout is the maximum amount of time an idle
    // (keep-alive) connection will remain idle before closing
    // itself.
    // Zero means no limit.
    IdleConnTimeout time.Duration

    // ResponseHeaderTimeout, if non-zero, specifies the amount of
    // time to wait for a server&#39;s response headers after fully
    // writing the request (including its body, if any). This
    // time does not include the time to read the response body.
    ResponseHeaderTimeout time.Duration

    // ExpectContinueTimeout, if non-zero, specifies the amount of
    // time to wait for a server&#39;s first response headers after fully
    // writing the request headers if the request has an
    // &amp;quot;Expect: 100-continue&amp;quot; header. Zero means no timeout and
    // causes the body to be sent immediately, without
    // waiting for the server to approve.
    // This time does not include the time to send the request header.
    ExpectContinueTimeout time.Duration

    // TLSNextProto specifies how the Transport switches to an
    // alternate protocol (such as HTTP/2) after a TLS NPN/ALPN
    // protocol negotiation. If Transport dials an TLS connection
    // with a non-empty protocol name and TLSNextProto contains a
    // map entry for that key (such as &amp;quot;h2&amp;quot;), then the func is
    // called with the request&#39;s authority (such as &amp;quot;example.com&amp;quot;
    // or &amp;quot;example.com:1234&amp;quot;) and the TLS connection. The function
    // must return a RoundTripper that then handles the request.
    // If TLSNextProto is not nil, HTTP/2 support is not enabled
    // automatically.
    TLSNextProto map[string]func(authority string, c *tls.Conn) RoundTripper

    // ProxyConnectHeader optionally specifies headers to send to
    // proxies during CONNECT requests.
    ProxyConnectHeader Header

    // MaxResponseHeaderBytes specifies a limit on how many
    // response bytes are allowed in the server&#39;s response
    // header.
    //
    // Zero means to use a default limit.
    MaxResponseHeaderBytes int64

    // nextProtoOnce guards initialization of TLSNextProto and
    // h2transport (via onceSetNextProtoDefaults)
    nextProtoOnce sync.Once
    h2transport   h2Transport // non-nil if http2 wired up
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;中文讲解&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Transport struct {
    // Proxy指定一个对给定请求返回代理的函数。
    // 如果该函数返回了非nil的错误值，请求的执行就会中断并返回该错误。
    // 如果Proxy为nil或返回nil的*URL置，将不使用代理。
    Proxy func(*Request) (*url.URL, error)

    // Dial指定创建TCP连接的拨号函数。如果Dial为nil，会使用net.Dial。
    //Dial获取一个tcp 连接，也就是net.Conn结构，你就记住可以往里面写request
    //然后从里面搞到response就行了
    Dial func(network, addr string) (net.Conn, error)

    // TLSClientConfig指定用于tls.Client的TLS配置信息。
    // 如果该字段为nil，会使用默认的配置信息。
    TLSClientConfig *tls.Config

    // TLSHandshakeTimeout指定等待TLS握手完成的最长时间。零值表示不设置超时。
    TLSHandshakeTimeout time.Duration

    // 如果DisableKeepAlives为真，会禁止不同HTTP请求之间TCP连接的重用。
    DisableKeepAlives bool

    // 如果DisableCompression为真，会禁止Transport在请求中没有Accept-Encoding头时，
    // 主动添加&amp;quot;Accept-Encoding: gzip&amp;quot;头，以获取压缩数据。
    // 如果Transport自己请求gzip并得到了压缩后的回复，它会主动解压缩回复的主体。
    // 但如果用户显式的请求gzip压缩数据，Transport是不会主动解压缩的。
    DisableCompression bool

    // 如果MaxIdleConnsPerHost!=0，会控制每个主机下的最大闲置连接。
    // 如果MaxIdleConnsPerHost==0，会使用DefaultMaxIdleConnsPerHost。
    MaxIdleConnsPerHost int

    // ResponseHeaderTimeout指定在发送完请求（包括其可能的主体）之后，
    // 等待接收服务端的回复的头域的最大时间。零值表示不设置超时。
    // 该时间不包括获取回复主体的时间。
    ResponseHeaderTimeout time.Duration

    // 内含隐藏或非导出字段



    //保存从 connectMethodKey （代表着不同的协议 不同的host，也就是不同的请求）到 persistConn 的映射
    idleConn   map[connectMethodKey][]*persistConn // most recently used at end
    //用来在并发http请求的时候在多个 goroutine 里面相互发送持久连接，也就是说， 这些持久连接是可以重复利用的， 你的http请求用某个persistConn用完了，通过这个channel发送给其他http请求使用这个persistConn，然后我们找到transport的RoundTrip方法
    idleConnCh map[connectMethodKey]chan *persistConn
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用内部send方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func send(ireq *Request, rt RoundTripper, deadline time.Time) (resp *Response, didTimeout func() bool, err error) {
    req := ireq // req is either the original request, or a modified fork

    if rt == nil {
        req.closeBody()
        return nil, alwaysFalse, errors.New(&amp;quot;http: no Client.Transport or DefaultTransport&amp;quot;)
    }

    if req.URL == nil {
        req.closeBody()
        return nil, alwaysFalse, errors.New(&amp;quot;http: nil Request.URL&amp;quot;)
    }

    if req.RequestURI != &amp;quot;&amp;quot; {
        req.closeBody()
        return nil, alwaysFalse, errors.New(&amp;quot;http: Request.RequestURI can&#39;t be set in client requests.&amp;quot;)
    }

    // forkReq forks req into a shallow clone of ireq the first
    // time it&#39;s called.
    forkReq := func() {
        if ireq == req {
            req = new(Request)
            *req = *ireq // shallow clone
        }
    }

    // Most the callers of send (Get, Post, et al) don&#39;t need
    // Headers, leaving it uninitialized. We guarantee to the
    // Transport that this has been initialized, though.
    if req.Header == nil {
        forkReq()
        req.Header = make(Header)
    }

    if u := req.URL.User; u != nil &amp;amp;&amp;amp; req.Header.Get(&amp;quot;Authorization&amp;quot;) == &amp;quot;&amp;quot; {
        username := u.Username()
        password, _ := u.Password()
        forkReq()
        req.Header = ireq.Header.clone()
        req.Header.Set(&amp;quot;Authorization&amp;quot;, &amp;quot;Basic &amp;quot;+basicAuth(username, password))
    }

    if !deadline.IsZero() {
        forkReq()
    }
    stopTimer, didTimeout := setRequestCancel(req, rt, deadline)

    resp, err = rt.RoundTrip(req)
    if err != nil {
        stopTimer()
        if resp != nil {
            log.Printf(&amp;quot;RoundTripper returned a response &amp;amp; error; ignoring response&amp;quot;)
        }
        if tlsErr, ok := err.(tls.RecordHeaderError); ok {
            // If we get a bad TLS record header, check to see if the
            // response looks like HTTP and give a more helpful error.
            // See golang.org/issue/11111.
            if string(tlsErr.RecordHeader[:]) == &amp;quot;HTTP/&amp;quot; {
                err = errors.New(&amp;quot;http: server gave HTTP response to HTTPS client&amp;quot;)
            }
        }
        return nil, didTimeout, err
    }
    if !deadline.IsZero() {
        resp.Body = &amp;amp;cancelTimerBody{
            stop:          stopTimer,
            rc:            resp.Body,
            reqDidTimeout: didTimeout,
        }
    }
    return resp, nil, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用DefaultTransport也就是Transport.go中的Transport结构体的RoundTrip方法（当出现自定义的时候，就调用对应的Transport的RoundTrip方法，这边直接使用这个借口就是DefaultTransport），可见使用golang net/http库发送http请求，最后都是调用 http transport的 RoundTrip方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// roundTrip implements a RoundTripper over HTTP.
func (t *Transport) roundTrip(req *Request) (*Response, error) {
    t.nextProtoOnce.Do(t.onceSetNextProtoDefaults)
    ctx := req.Context()
    trace := httptrace.ContextClientTrace(ctx)

    if req.URL == nil {
        req.closeBody()
        return nil, errors.New(&amp;quot;http: nil Request.URL&amp;quot;)
    }
    if req.Header == nil {
        req.closeBody()
        return nil, errors.New(&amp;quot;http: nil Request.Header&amp;quot;)
    }
    scheme := req.URL.Scheme
    isHTTP := scheme == &amp;quot;http&amp;quot; || scheme == &amp;quot;https&amp;quot;
    if isHTTP {
        for k, vv := range req.Header {
            if !httpguts.ValidHeaderFieldName(k) {
                return nil, fmt.Errorf(&amp;quot;net/http: invalid header field name %q&amp;quot;, k)
            }
            for _, v := range vv {
                if !httpguts.ValidHeaderFieldValue(v) {
                    return nil, fmt.Errorf(&amp;quot;net/http: invalid header field value %q for key %v&amp;quot;, v, k)
                }
            }
        }
    }

    if t.useRegisteredProtocol(req) {
        altProto, _ := t.altProto.Load().(map[string]RoundTripper)
        if altRT := altProto[scheme]; altRT != nil {
            if resp, err := altRT.RoundTrip(req); err != ErrSkipAltProtocol {
                return resp, err
            }
        }
    }
    if !isHTTP {
        req.closeBody()
        return nil, &amp;amp;badStringError{&amp;quot;unsupported protocol scheme&amp;quot;, scheme}
    }
    if req.Method != &amp;quot;&amp;quot; &amp;amp;&amp;amp; !validMethod(req.Method) {
        return nil, fmt.Errorf(&amp;quot;net/http: invalid method %q&amp;quot;, req.Method)
    }
    if req.URL.Host == &amp;quot;&amp;quot; {
        req.closeBody()
        return nil, errors.New(&amp;quot;http: no Host in request URL&amp;quot;)
    }

    for {
        select {
        case &amp;lt;-ctx.Done():
            req.closeBody()
            return nil, ctx.Err()
        default:
        }

        // treq gets modified by roundTrip, so we need to recreate for each retry.
        treq := &amp;amp;transportRequest{Request: req, trace: trace}
        cm, err := t.connectMethodForRequest(treq)
        if err != nil {
            req.closeBody()
            return nil, err
        }

        // Get the cached or newly-created connection to either the
        // host (for http or https), the http proxy, or the http proxy
        // pre-CONNECTed to https server. In any case, we&#39;ll be ready
        // to send it requests.
        pconn, err := t.getConn(treq, cm)
        if err != nil {
            t.setReqCanceler(req, nil)
            req.closeBody()
            return nil, err
        }

        var resp *Response
        if pconn.alt != nil {
            // HTTP/2 path.
            t.decHostConnCount(cm.key()) // don&#39;t count cached http2 conns toward conns per host
            t.setReqCanceler(req, nil)   // not cancelable with CancelRequest
            resp, err = pconn.alt.RoundTrip(req)
        } else {
            resp, err = pconn.roundTrip(treq)
        }
        if err == nil {
            return resp, nil
        }
        if !pconn.shouldRetryRequest(req, err) {
            // Issue 16465: return underlying net.Conn.Read error from peek,
            // as we&#39;ve historically done.
            if e, ok := err.(transportReadFromServerError); ok {
                err = e.err
            }
            return nil, err
        }
        testHookRoundTripRetried()

        // Rewind the body if we&#39;re able to.
        if req.GetBody != nil {
            newReq := *req
            var err error
            newReq.Body, err = req.GetBody()
            if err != nil {
                return nil, err
            }
            req = &amp;amp;newReq
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前面对输入的错误处理部分我们忽略， 其实就2步，先获取一个TCP长连接，所谓TCP长连接就是三次握手建立连接后不close而是一直保持重复使用（节约环保） 然后调用这个持久连接persistConn 这个struct的roundTrip方法。我们先看获取连接&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (t *Transport) getConn(req *Request, cm connectMethod) (*persistConn, error) {
    if pc := t.getIdleConn(cm); pc != nil {
        // set request canceler to some non-nil function so we
        // can detect whether it was cleared between now and when
        // we enter roundTrip
        t.setReqCanceler(req, func() {})
        return pc, nil
    }

    type dialRes struct {
        pc  *persistConn
        err error
    }
    dialc := make(chan dialRes)
    //定义了一个发送 persistConn的channel

    prePendingDial := prePendingDial
    postPendingDial := postPendingDial

    handlePendingDial := func() {
        if prePendingDial != nil {
            prePendingDial()
        }
        go func() {
            if v := &amp;lt;-dialc; v.err == nil {
                t.putIdleConn(v.pc)
            }
            if postPendingDial != nil {
                postPendingDial()
            }
        }()
    }

    cancelc := make(chan struct{})
    t.setReqCanceler(req, func() { close(cancelc) })

    // 启动了一个goroutine, 这个goroutine 获取里面调用dialConn搞到
    // persistConn, 然后发送到上面建立的channel  dialc里面，
    go func() {
        pc, err := t.dialConn(cm)
        dialc &amp;lt;- dialRes{pc, err}
    }()

    idleConnCh := t.getIdleConnCh(cm)
    select {
    case v := &amp;lt;-dialc:
        // dialc 我们的 dial 方法先搞到通过 dialc通道发过来了
        return v.pc, v.err
    case pc := &amp;lt;-idleConnCh:
        // 这里代表其他的http请求用完了归还的persistConn通过idleConnCh这个
        // channel发送来的
        handlePendingDial()
        return pc, nil
    case &amp;lt;-req.Cancel:
        handlePendingDial()
        return nil, errors.New(&amp;quot;net/http: request canceled while waiting for connection&amp;quot;)
    case &amp;lt;-cancelc:
        handlePendingDial()
        return nil, errors.New(&amp;quot;net/http: request canceled while waiting for connection&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里面的代码写的很有讲究 , 上面代码里面我也注释了， 定义了一个发送 persistConn的channel dialc， 启动了一个goroutine, 这个goroutine 获取里面调用dialConn搞到persistConn, 然后发送到dialc里面，主协程goroutine在 select里面监听多个channel,看看哪个通道里面先发过来 persistConn，就用哪个，然后return。&lt;/p&gt;

&lt;p&gt;这里要注意的是 idleConnCh 这个通道里面发送来的是其他的http请求用完了归还的persistConn， 如果从这个通道里面搞到了，dialc这个通道也等着发呢，不能浪费，就通过handlePendingDial这个方法把dialc通道里面的persistConn也发到idleConnCh，等待后续给其他http请求使用。&lt;/p&gt;

&lt;p&gt;每个新建的persistConn的时候都把tcp连接里地输入流，和输出流用br（br *bufio.Reader）,和bw(bw *bufio.Writer)包装了一下，往bw写就写到tcp输入流里面了，读输出流也是通过br读，并启动了读循环和写循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pconn.br = bufio.NewReader(noteEOFReader{pconn.conn, &amp;amp;pconn.sawEOF})
pconn.bw = bufio.NewWriter(pconn.conn)
go pconn.readLoop()
go pconn.writeLoop()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再看pconn.roundTrip 调用这个持久连接persistConn 这个struct的roundTrip方法。先瞄一下 persistConn 这个struct&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type persistConn struct {
    t        *Transport
    cacheKey connectMethodKey
    conn     net.Conn
    tlsState *tls.ConnectionState
    br       *bufio.Reader       // 从tcp输出流里面读
    sawEOF   bool                // whether we&#39;ve seen EOF from conn; owned by readLoop
    bw       *bufio.Writer       // 写到tcp输入流
     reqch    chan requestAndChan // 主goroutine 往channnel里面写，读循环从     
                                 // channnel里面接受
    writech  chan writeRequest   // 主goroutine 往channnel里面写                                      
                                 // 写循环从channel里面接受
    closech  chan struct{}       // 通知关闭tcp连接的channel 

    writeErrCh chan error

    lk                   sync.Mutex // guards following fields
    numExpectedResponses int
    closed               bool // whether conn has been closed
    broken               bool // an error has happened on this connection; marked broken so it&#39;s not reused.
    canceled             bool // whether this conn was broken due a CancelRequest
    // mutateHeaderFunc is an optional func to modify extra
    // headers on each outbound request before it&#39;s written. (the
    // original Request given to RoundTrip is not modified)
    mutateHeaderFunc func(Header)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;里面是各种channel, 用的是出神入化， 各位要好好理解一下，这里有三个goroutine，有两个channel writeRequest 和 requestAndChan&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type writeRequest struct {
    req *transportRequest
    ch  chan&amp;lt;- error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主goroutine 往writeRequest里面写，写循环从writeRequest里面接受&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type responseAndError struct {
    res *Response
    err error
}

type requestAndChan struct {
    req *Request
    ch  chan responseAndError
    addedGzip bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主goroutine 往requestAndChan里面写，读循环从requestAndChan里面接受。&lt;/p&gt;

&lt;p&gt;注意这里的channel都是双向channel，也就是channel 的struct里面有一个chan类型的字段， 比如 reqch chan requestAndChan 这里的 requestAndChan 里面的 ch chan responseAndError。&lt;/p&gt;

&lt;p&gt;这个是很牛叉，主 goroutine 通过 reqch 发送requestAndChan 给读循环，然后读循环搞到response后通过 requestAndChan 里面的通道responseAndError把response返给主goroutine，所以我画了一个双向箭头。&lt;/p&gt;

&lt;p&gt;我们研究一下代码，我理解下来其实就是三个goroutine通过channel互相协作的过程。&lt;/p&gt;

&lt;p&gt;主循环：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pc *persistConn) roundTrip(req *transportRequest) (resp *Response, err error) {
    ... 忽略
    // Write the request concurrently with waiting for a response,
    // in case the server decides to reply before reading our full
    // request body.
    writeErrCh := make(chan error, 1)
    pc.writech &amp;lt;- writeRequest{req, writeErrCh}
    //把request发送给写循环
    resc := make(chan responseAndError, 1)
    pc.reqch &amp;lt;- requestAndChan{req.Request, resc, requestedGzip}
    //发送给读循环
    var re responseAndError
    var respHeaderTimer &amp;lt;-chan time.Time
    cancelChan := req.Request.Cancel
WaitResponse:
    for {
        select {
        case err := &amp;lt;-writeErrCh:
            if isNetWriteError(err) {
                //写循环通过这个channel报告错误
                select {
                case re = &amp;lt;-resc:
                    pc.close()
                    break WaitResponse
                case &amp;lt;-time.After(50 * time.Millisecond):
                    // Fall through.
                }
            }
            if err != nil {
                re = responseAndError{nil, err}
                pc.close()
                break WaitResponse
            }
            if d := pc.t.ResponseHeaderTimeout; d &amp;gt; 0 {
                timer := time.NewTimer(d)
                defer timer.Stop() // prevent leaks
                respHeaderTimer = timer.C
            }
        case &amp;lt;-pc.closech:
            // 如果长连接挂了， 这里的channel有数据， 进入这个case, 进行处理

            select {
            case re = &amp;lt;-resc:
                if fn := testHookPersistConnClosedGotRes; fn != nil {
                    fn()
                }
            default:
                re = responseAndError{err: errClosed}
                if pc.isCanceled() {
                    re = responseAndError{err: errRequestCanceled}
                }
            }
            break WaitResponse
        case &amp;lt;-respHeaderTimer:
            pc.close()
            re = responseAndError{err: errTimeout}
            break WaitResponse
            // 如果timeout，这里的channel有数据， break掉for循环
        case re = &amp;lt;-resc:
            break WaitResponse
           // 获取到读循环的response, break掉 for循环
        case &amp;lt;-cancelChan:
            pc.t.CancelRequest(req.Request)
            cancelChan = nil
        }
    }

    if re.err != nil {
        pc.t.setReqCanceler(req.Request, nil)
    }
    return re.res, re.err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段代码主要就干了三件事&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;主goroutine -&amp;gt;requestAndChan -&amp;gt; 读循环goroutine

主goroutine -&amp;gt;writeRequest-&amp;gt; 写循环goroutine

主goroutine 通过select 监听各个channel上的数据， 比如请求取消， timeout，长连接挂了，写流出错，读流出错， 都是其他goroutine 发送过来的， 跟中断一样，然后相应处理，上面也提到了，有些channel是主goroutine通过channel发送给其他goroutine的struct里面包含的channel, 比如 case err := &amp;lt;-writeErrCh: case re = &amp;lt;-resc:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读循环代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pc *persistConn) readLoop() {

    ... 忽略
    alive := true
    for alive {

        ... 忽略
        rc := &amp;lt;-pc.reqch

        var resp *Response
        if err == nil {
            resp, err = ReadResponse(pc.br, rc.req)
            if err == nil &amp;amp;&amp;amp; resp.StatusCode == 100 {
                //100  Continue  初始的请求已经接受，客户应当继续发送请求的其 
                // 余部分
                resp, err = ReadResponse(pc.br, rc.req)
                // 读pc.br（tcp输出流）中的数据，这里的代码在response里面
                //解析statusCode，头字段， 转成标准的内存中的response 类型
                //  http在tcp数据流里面，head和body以 /r/n/r/n分开， 各个头
                // 字段 以/r/n分开
            }
        }

        if resp != nil {
            resp.TLS = pc.tlsState
        }

        ...忽略
        //上面处理一些http协议的一些逻辑行为，
        rc.ch &amp;lt;- responseAndError{resp, err} //把读到的response返回给    
                                             //主goroutine

        .. 忽略
        //忽略部分， 处理cancel req中断， 发送idleConnCh归还pc（持久连接）到持久连接池中（map）    
    pc.close()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;无关代码忽略，这段代码主要干了一件事情&lt;/p&gt;

&lt;p&gt;读循环goroutine 通过channel requestAndChan 接受主goroutine发送的request(rc := &amp;lt;-pc.reqch), 并从tcp输出流中读取response， 然后反序列化到结构体中， 最后通过channel 返给主goroutine (rc.ch &amp;lt;- responseAndError{resp, err} )&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pc *persistConn) writeLoop() {
    for {
        select {
        case wr := &amp;lt;-pc.writech:   //接受主goroutine的 request
            if pc.isBroken() {
                wr.ch &amp;lt;- errors.New(&amp;quot;http: can&#39;t write HTTP request on broken connection&amp;quot;)
                continue
            }
            err := wr.req.Request.write(pc.bw, pc.isProxy, wr.req.extra)   //写入tcp输入流
            if err == nil {
                err = pc.bw.Flush()
            }
            if err != nil {
                pc.markBroken()
                wr.req.Request.closeBody()
            }
            pc.writeErrCh &amp;lt;- err 
            wr.ch &amp;lt;- err         //  出错的时候返给主goroutineto 
        case &amp;lt;-pc.closech:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写循环就更简单了，select channel中主gouroutine的request，然后写入tcp输入流，如果出错了，channel 通知调用者。&lt;/p&gt;

&lt;p&gt;整体看下来，过程都很简单，但是代码中有很多值得我们学习的地方，比如高并发请求如何复用tcp连接，这里是连接池的做法，如果使用多个 goroutine相互协作完成一个http请求，出现错误的时候如何通知调用者中断错误，代码风格也有很多可以借鉴的地方。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;http.Client 表示一个http client端，用来处理HTTP相关的工作，例如cookies, redirect, timeout等工作，其内部包含一个Transport，tranport用来建立一个连接，其中维护了一个空闲连接池idleConn map[connectMethodKey][]*persistConn，其中的每个成员都是一个persistConn对象，persistConn是个具体的连接实例，包含了连接的上下文，会启动两个groutine分别执行readLoop和writeLoop, 每当transport调用roundTrip的时候，就会从连接池中选择一个空闲的persistConn，然后调用其roundTrip方法，将读写请求通过channel分别发送到readLoop和writeLoop中，然后会进行select各个channel的信息，包括连接关闭，请求超时，writeLoop出错， readLoop返回读取结果等。在writeLoop中发送请求，在readLoop中获取response并通过channe返回给roundTrip函数中，并再次将自己加入到idleConn中，等待下次请求到来。&lt;/p&gt;

&lt;h2 id=&#34;自定义client&#34;&gt;自定义client&lt;/h2&gt;

&lt;p&gt;在上面我们说到调用结构体的成员函数都是默认的结构体的成员函数，但是如果我们有一些特殊的需求，我们就需要重新定义这些结构体，然后实现自己的逻辑，整个http请求也就会按着我们的逻辑进行处理，这也是我们实现一些功能的必要手段。最基本的就是自定义client，也是我们编程常用的，深入一些就需要了解一些传输transport等。&lt;/p&gt;

&lt;p&gt;1、要管理HTTP客户端的头域、重定向策略和其他设置，创建一个Client：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;client := &amp;amp;http.Client{
    CheckRedirect: redirectPolicyFunc,
}
resp, err := client.Get(&amp;quot;http://example.com&amp;quot;)
// ...
req, err := http.NewRequest(&amp;quot;GET&amp;quot;, &amp;quot;http://example.com&amp;quot;, nil)
// ...
req.Header.Add(&amp;quot;If-None-Match&amp;quot;, `W/&amp;quot;wyzzy&amp;quot;`)
resp, err := client.Do(req)
// ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是在上面的基础上增加了对client结构体的设置，而不是使用DefaultClient，我们来看一下client的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Client struct {
    // Transport指定执行独立、单次HTTP请求的机制。
    // 如果Transport为nil，则使用DefaultTransport。
    Transport RoundTripper
    // CheckRedirect指定处理重定向的策略。
    // 如果CheckRedirect不为nil，客户端会在执行重定向之前调用本函数字段。
    // 参数req和via是将要执行的请求和已经执行的请求（切片，越新的请求越靠后）。
    // 如果CheckRedirect返回一个错误，本类型的Get方法不会发送请求req，
    // 而是返回之前得到的最后一个回复和该错误。（包装进url.Error类型里）
    //
    // 如果CheckRedirect为nil，会采用默认策略：连续10此请求后停止。
    CheckRedirect func(req *Request, via []*Request) error
    // Jar指定cookie管理器。
    // 如果Jar为nil，请求中不会发送cookie，回复中的cookie会被忽略。
    Jar CookieJar
    // Timeout指定本类型的值执行请求的时间限制。
    // 该超时限制包括连接时间、重定向和读取回复主体的时间。
    // 计时器会在Head、Get、Post或Do方法返回后继续运作并在超时后中断回复主体的读取。
    //
    // Timeout为零值表示不设置超时。
    //
    // Client实例的Transport字段必须支持CancelRequest方法，
    // 否则Client会在试图用Head、Get、Post或Do方法执行请求时返回错误。
    // 本类型的Transport字段默认值（DefaultTransport）支持CancelRequest方法。
    Timeout time.Duration
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要是对这些结构体中的成员的如何运用才是重点，然后就调用client的Get，Do等方法就是上面的执行逻辑，这边只是简单的client的处理，后面的逻辑依然使用的是默认的Transport。&lt;/p&gt;

&lt;p&gt;2、要管理代理、TLS配置、keep-alive、压缩和其他设置，创建一个Transport：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tr := &amp;amp;http.Transport{
    TLSClientConfig:    &amp;amp;tls.Config{RootCAs: pool},
    DisableCompression: true,
}
client := &amp;amp;http.Client{Transport: tr}
resp, err := client.Get(&amp;quot;https://example.com&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Client和Transport类型都可以安全的被多个go程同时使用。出于效率考虑，应该一次建立、尽量重用。&lt;/p&gt;

&lt;p&gt;这边在client的基础上对client的transport的管理代理、TLS配置、keep-alive、压缩和其他设置，然后后面的逻辑中主要是切换到自定义的transport的逻辑运行。&lt;/p&gt;

&lt;h1 id=&#34;http服务端&#34;&gt;http服务端&lt;/h1&gt;

&lt;h2 id=&#34;http-status&#34;&gt;http status&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;const (
    StatusContinue           = 100
    StatusSwitchingProtocols = 101
    StatusOK                   = 200
    StatusCreated              = 201
    StatusAccepted             = 202
    StatusNonAuthoritativeInfo = 203
    StatusNoContent            = 204
    StatusResetContent         = 205
    StatusPartialContent       = 206
    StatusMultipleChoices   = 300
    StatusMovedPermanently  = 301
    StatusFound             = 302
    StatusSeeOther          = 303
    StatusNotModified       = 304
    StatusUseProxy          = 305
    StatusTemporaryRedirect = 307
    StatusBadRequest                   = 400
    StatusUnauthorized                 = 401
    StatusPaymentRequired              = 402
    StatusForbidden                    = 403
    StatusNotFound                     = 404
    StatusMethodNotAllowed             = 405
    StatusNotAcceptable                = 406
    StatusProxyAuthRequired            = 407
    StatusRequestTimeout               = 408
    StatusConflict                     = 409
    StatusGone                         = 410
    StatusLengthRequired               = 411
    StatusPreconditionFailed           = 412
    StatusRequestEntityTooLarge        = 413
    StatusRequestURITooLong            = 414
    StatusUnsupportedMediaType         = 415
    StatusRequestedRangeNotSatisfiable = 416
    StatusExpectationFailed            = 417
    StatusTeapot                       = 418
    StatusInternalServerError     = 500
    StatusNotImplemented          = 501
    StatusBadGateway              = 502
    StatusServiceUnavailable      = 503
    StatusGatewayTimeout          = 504
    StatusHTTPVersionNotSupported = 505
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们比较常用的就是404（服务未发现），503（服务不可用）等。&lt;/p&gt;

&lt;h2 id=&#34;http-header&#34;&gt;http header&lt;/h2&gt;

&lt;p&gt;Header代表HTTP头域的键值对。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Header map[string][]string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基本操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Get(key string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get返回键对应的第一个值，如果键不存在会返回&amp;rdquo;&amp;ldquo;。如要获取该键对应的值切片，请直接用规范格式的键访问map。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Set(key, value string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set添加键值对到h，如键已存在则会用只有新值一个元素的切片取代旧值切片。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Add(key, value string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add添加键值对到h，如键已存在则会将新的值附加到旧值切片后面。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Del(key string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Del删除键值对。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Write(w io.Writer) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write以有线格式将头域写入w。&lt;/p&gt;

&lt;h2 id=&#34;用于http客户端和服务端的结构体&#34;&gt;用于http客户端和服务端的结构体&lt;/h2&gt;

&lt;p&gt;type Request&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Request struct {
    // Method指定HTTP方法（GET、POST、PUT等）。对客户端，&amp;quot;&amp;quot;代表GET。
    Method string
    // URL在服务端表示被请求的URI，在客户端表示要访问的URL。
    //
    // 在服务端，URL字段是解析请求行的URI（保存在RequestURI字段）得到的，
    // 对大多数请求来说，除了Path和RawQuery之外的字段都是空字符串。
    // （参见RFC 2616, Section 5.1.2）
    //
    // 在客户端，URL的Host字段指定了要连接的服务器，
    // 而Request的Host字段（可选地）指定要发送的HTTP请求的Host头的值。
    URL *url.URL
    // 接收到的请求的协议版本。本包生产的Request总是使用HTTP/1.1
    Proto      string // &amp;quot;HTTP/1.0&amp;quot;
    ProtoMajor int    // 1
    ProtoMinor int    // 0
    // Header字段用来表示HTTP请求的头域。如果头域（多行键值对格式）为：
    //  accept-encoding: gzip, deflate
    //  Accept-Language: en-us
    //  Connection: keep-alive
    // 则：
    //  Header = map[string][]string{
    //      &amp;quot;Accept-Encoding&amp;quot;: {&amp;quot;gzip, deflate&amp;quot;},
    //      &amp;quot;Accept-Language&amp;quot;: {&amp;quot;en-us&amp;quot;},
    //      &amp;quot;Connection&amp;quot;: {&amp;quot;keep-alive&amp;quot;},
    //  }
    // HTTP规定头域的键名（头名）是大小写敏感的，请求的解析器通过规范化头域的键名来实现这点。
    // 在客户端的请求，可能会被自动添加或重写Header中的特定的头，参见Request.Write方法。
    Header Header
    // Body是请求的主体。
    //
    // 在客户端，如果Body是nil表示该请求没有主体买入GET请求。
    // Client的Transport字段会负责调用Body的Close方法。
    //
    // 在服务端，Body字段总是非nil的；但在没有主体时，读取Body会立刻返回EOF。
    // Server会关闭请求的主体，ServeHTTP处理器不需要关闭Body字段。
    Body io.ReadCloser
    // ContentLength记录相关内容的长度。
    // 如果为-1，表示长度未知，如果&amp;gt;=0，表示可以从Body字段读取ContentLength字节数据。
    // 在客户端，如果Body非nil而该字段为0，表示不知道Body的长度。
    ContentLength int64
    // TransferEncoding按从最外到最里的顺序列出传输编码，空切片表示&amp;quot;identity&amp;quot;编码。
    // 本字段一般会被忽略。当发送或接受请求时，会自动添加或移除&amp;quot;chunked&amp;quot;传输编码。
    TransferEncoding []string
    // Close在服务端指定是否在回复请求后关闭连接，在客户端指定是否在发送请求后关闭连接。
    Close bool
    // 在服务端，Host指定URL会在其上寻找资源的主机。
    // 根据RFC 2616，该值可以是Host头的值，或者URL自身提供的主机名。
    // Host的格式可以是&amp;quot;host:port&amp;quot;。
    //
    // 在客户端，请求的Host字段（可选地）用来重写请求的Host头。
    // 如过该字段为&amp;quot;&amp;quot;，Request.Write方法会使用URL字段的Host。
    Host string
    // Form是解析好的表单数据，包括URL字段的query参数和POST或PUT的表单数据。
    // 本字段只有在调用ParseForm后才有效。在客户端，会忽略请求中的本字段而使用Body替代。
    Form url.Values
    // PostForm是解析好的POST或PUT的表单数据。
    // 本字段只有在调用ParseForm后才有效。在客户端，会忽略请求中的本字段而使用Body替代。
    PostForm url.Values
    // MultipartForm是解析好的多部件表单，包括上传的文件。
    // 本字段只有在调用ParseMultipartForm后才有效。
    // 在客户端，会忽略请求中的本字段而使用Body替代。
    MultipartForm *multipart.Form
    // Trailer指定了会在请求主体之后发送的额外的头域。
    //
    // 在服务端，Trailer字段必须初始化为只有trailer键，所有键都对应nil值。
    // （客户端会声明哪些trailer会发送）
    // 在处理器从Body读取时，不能使用本字段。
    // 在从Body的读取返回EOF后，Trailer字段会被更新完毕并包含非nil的值。
    // （如果客户端发送了这些键值对），此时才可以访问本字段。
    //
    // 在客户端，Trail必须初始化为一个包含将要发送的键值对的映射。（值可以是nil或其终值）
    // ContentLength字段必须是0或-1，以启用&amp;quot;chunked&amp;quot;传输编码发送请求。
    // 在开始发送请求后，Trailer可以在读取请求主体期间被修改，
    // 一旦请求主体返回EOF，调用者就不可再修改Trailer。
    //
    // 很少有HTTP客户端、服务端或代理支持HTTP trailer。
    Trailer Header
    // RemoteAddr允许HTTP服务器和其他软件记录该请求的来源地址，一般用于日志。
    // 本字段不是ReadRequest函数填写的，也没有定义格式。
    // 本包的HTTP服务器会在调用处理器之前设置RemoteAddr为&amp;quot;IP:port&amp;quot;格式的地址。
    // 客户端会忽略请求中的RemoteAddr字段。
    RemoteAddr string
    // RequestURI是被客户端发送到服务端的请求的请求行中未修改的请求URI
    // （参见RFC 2616, Section 5.1）
    // 一般应使用URI字段，在客户端设置请求的本字段会导致错误。
    RequestURI string
    // TLS字段允许HTTP服务器和其他软件记录接收到该请求的TLS连接的信息
    // 本字段不是ReadRequest函数填写的。
    // 对启用了TLS的连接，本包的HTTP服务器会在调用处理器之前设置TLS字段，否则将设TLS为nil。
    // 客户端会忽略请求中的TLS字段。
    TLS *tls.ConnectionState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Request类型代表一个服务端接受到的或者客户端发送出去的HTTP请求。&lt;/p&gt;

&lt;p&gt;Request各字段的意义和用途在服务端和客户端是不同的。除了字段本身上方文档，还可参见Request.Write方法和RoundTripper接口的文档。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Response struct {
    Status     string // 例如&amp;quot;200 OK&amp;quot;
    StatusCode int    // 例如200
    Proto      string // 例如&amp;quot;HTTP/1.0&amp;quot;
    ProtoMajor int    // 例如1
    ProtoMinor int    // 例如0
    // Header保管头域的键值对。
    // 如果回复中有多个头的键相同，Header中保存为该键对应用逗号分隔串联起来的这些头的值
    // （参见RFC 2616 Section 4.2）
    // 被本结构体中的其他字段复制保管的头（如ContentLength）会从Header中删掉。
    //
    // Header中的键都是规范化的，参见CanonicalHeaderKey函数
    Header Header
    // Body代表回复的主体。
    // Client类型和Transport类型会保证Body字段总是非nil的，即使回复没有主体或主体长度为0。
    // 关闭主体是调用者的责任。
    // 如果服务端采用&amp;quot;chunked&amp;quot;传输编码发送的回复，Body字段会自动进行解码。
    Body io.ReadCloser
    // ContentLength记录相关内容的长度。
    // 其值为-1表示长度未知（采用chunked传输编码）
    // 除非对应的Request.Method是&amp;quot;HEAD&amp;quot;，其值&amp;gt;=0表示可以从Body读取的字节数
    ContentLength int64
    // TransferEncoding按从最外到最里的顺序列出传输编码，空切片表示&amp;quot;identity&amp;quot;编码。
    TransferEncoding []string
    // Close记录头域是否指定应在读取完主体后关闭连接。（即Connection头）
    // 该值是给客户端的建议，Response.Write方法的ReadResponse函数都不会关闭连接。
    Close bool
    // Trailer字段保存和头域相同格式的trailer键值对，和Header字段相同类型
    Trailer Header
    // Request是用来获取此回复的请求
    // Request的Body字段是nil（因为已经被用掉了）
    // 这个字段是被Client类型发出请求并获得回复后填充的
    Request *Request
    // TLS包含接收到该回复的TLS连接的信息。 对未加密的回复，本字段为nil。
    // 返回的指针是被（同一TLS连接接收到的）回复共享的，不应被修改。
    TLS *tls.ConnectionState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Response代表一个HTTP请求的回复&lt;/p&gt;

&lt;p&gt;type ResponseWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ResponseWriter interface {
    // Header返回一个Header类型值，该值会被WriteHeader方法发送。
    // 在调用WriteHeader或Write方法后再改变该对象是没有意义的。
    Header() Header
    // WriteHeader该方法发送HTTP回复的头域和状态码。
    // 如果没有被显式调用，第一次调用Write时会触发隐式调用WriteHeader(http.StatusOK)
    // WriterHeader的显式调用主要用于发送错误码。
    WriteHeader(int)
    // Write向连接中写入作为HTTP的一部分回复的数据。
    // 如果被调用时还未调用WriteHeader，本方法会先调用WriteHeader(http.StatusOK)
    // 如果Header中没有&amp;quot;Content-Type&amp;quot;键，
    // 本方法会使用包函数DetectContentType检查数据的前512字节，将返回值作为该键的值。
    Write([]byte) (int, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResponseWriter接口被HTTP处理器用于构造HTTP回复。这个一般用于服务端处理请求&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;正常我们使用的返回方式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
 &amp;quot;net/http&amp;quot;
)

func main() {

 http.HandleFunc(&amp;quot;/&amp;quot;, func (w http.ResponseWriter, r *http.Request){


   w.Header().Set(&amp;quot;name&amp;quot;, &amp;quot;my name is smallsoup&amp;quot;)
   w.WriteHeader(500)
   w.Write([]byte(&amp;quot;hello world\n&amp;quot;))

 })

 http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;type CloseNotifier&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type CloseNotifier interface {
    // CloseNotify返回一个通道，该通道会在客户端连接丢失时接收到唯一的值
    CloseNotify() &amp;lt;-chan bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HTTP处理器ResponseWriter接口参数的下层如果实现了CloseNotifier接口，可以让用户检测下层的连接是否停止。如果客户端在回复准备好之前关闭了连接，该机制可以用于取消服务端耗时较长的操作。&lt;/p&gt;

&lt;h2 id=&#34;http-服务端使用和原理解析&#34;&gt;http 服务端使用和原理解析&lt;/h2&gt;

&lt;p&gt;ListenAndServe使用指定的监听地址和处理器启动一个HTTP服务端。处理器参数通常是nil，这表示采用包变量DefaultServeMux作为处理器。Handle和HandleFunc函数可以向DefaultServeMux添加处理器。如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.Handle(&amp;quot;/foo&amp;quot;, fooHandler)
http.HandleFunc(&amp;quot;/bar&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintf(w, &amp;quot;Hello, %q&amp;quot;, html.EscapeString(r.URL.Path))
})
log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenAndServe该方法用于在指定的TCP网络地址addr进行监听，然后调用服务端处理程序来处理传入的连接请求。该方法有两个参数：第一个参数addr 即监听地址；第二个参数表示服务端处理程序，通常为空，这意味着服务端调用 http.DefaultServeMux 进行处理，而服务端编写的业务逻辑处理程序 http.Handle() 或 http.HandleFunc() 默认注入 http.DefaultServeMux 中。&lt;/p&gt;

&lt;p&gt;理解HTTP相关的网络应用，主要关注两个地方-客户端(client)和服务端(server)，两者的交互主要是client的request以及server的response,主要就在于如何接受client的request并向client返回response。&lt;/p&gt;

&lt;p&gt;接收request的过程中，最重要的莫过于路由（router），即实现一个Multiplexer器。Go http中既可以使用内置的mutilplexer &amp;mdash; DefautServeMux，也可以自定义。Multiplexer路由的目的就是为了找到处理器函数（handler），后者将对request进行处理，同时构建response&lt;/p&gt;

&lt;p&gt;流程为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Clinet -&amp;gt; Requests -&amp;gt;  Multiplexer(router) -&amp;gt; handler  -&amp;gt; Response -&amp;gt; Clinet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于一个http服务，大致需要理解这两个封装的过程就可以理解上面的实现了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.首先需要注册路由，即提供url模式和handler函数的映射.
2.其次就是实例化一个server对象，并开启对客户端的监听。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看go http服务的代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/&amp;quot;, indexHandler) -----即是注册路由。
http.ListenAndServe(&amp;quot;127.0.0.1:8000&amp;quot;, nil)---启动server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server := &amp;amp;Server{Addr: addr, Handler: handler}
server.ListenAndServe()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;注册路由&#34;&gt;注册路由&lt;/h3&gt;

&lt;p&gt;net/http包暴露的注册路由的api很简单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/&amp;quot;, indexHandler) -----即是注册路由。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HandlerFunc是一个函数类型，如下定义，同时实现了Handler接口的ServeHTTP方法。使用HandlerFunc类型包装一下路由定义的indexHandler函数，其目的就是为了让这个函数也实现ServeHTTP方法，即转变成一个handler处理器(函数)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type HandlerFunc func(ResponseWriter, *Request)

// ServeHTTP calls f(w, r).
func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) {
    f(w, r)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们最开始写的例子中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/&amp;quot;,Indexhandler)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样 IndexHandler 函数也有了ServeHTTP方法。&lt;/p&gt;

&lt;p&gt;ServeMux和handler处理器函数的连接桥梁就是Handler接口。ServeMux的ServeHTTP方法实现了寻找注册路由的handler的函数（可以看下面的监控服务流程），并调用该handler的ServeHTTP方法。ServeHTTP方法就是真正处理请求和构造响应的地方。&lt;/p&gt;

&lt;p&gt;Go其实支持外部实现的路由器 ListenAndServe的第二个参数就是 用以配置外部路由器的，它是一个Handler接口，即外部路由器只要实现了Handler接口就可以,我们可以在自己实现 的路由器的ServHTTP里面实现自定义路由功能。&lt;/p&gt;

&lt;p&gt;如下代码所示，我们自己实现了一个简易的路由器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import ( 
    &amp;quot;fmt&amp;quot;
    &amp;quot;net/http&amp;quot; 
    )
type MyMux struct { }

func (p *MyMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path == &amp;quot;/&amp;quot; {
    sayhelloName(w, r)
    return 
}
    http.NotFound(w, r)
    return 
}

func sayhelloName(w http.ResponseWriter, r *http.Request) { f
    mt.Fprintf(w, &amp;quot;Hello myroute!&amp;quot;)
}

func main() {
    mux := &amp;amp;MyMux{}
    http.ListenAndServe(&amp;quot;:9090&amp;quot;, mux) 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;multiplexer&#34;&gt;multiplexer&lt;/h3&gt;

&lt;p&gt;http.HandleFunc选取了DefaultServeMux作为multiplexer：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) {
    DefaultServeMux.HandleFunc(pattern, handler)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DefaultServeMux是ServeMux的一个实例。当然http包也提供了NewServeMux方法创建一个ServeMux实例，默认则创建一个DefaultServeMux：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewServeMux allocates and returns a new ServeMux.
func NewServeMux() *ServeMux { return new(ServeMux) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DefaultServeMux的代码定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// DefaultServeMux is the default ServeMux used by Serve.
var DefaultServeMux = &amp;amp;defaultServeMux
var defaultServeMux ServeMux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然也可以是其他可以实现的实例 ，比如上面实现的mux。&lt;/p&gt;

&lt;p&gt;路由结构体ServeMux&lt;/p&gt;

&lt;p&gt;ServeMux的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ServeMux struct {
    mu    sync.RWMutex                      //锁，由于请求涉及到并发处理，因此这里需要一个锁机制
    m     map[string]muxEntry               // 路由规则，一个string对应一个mux实体，这里的string就是注册的路由
    hosts bool 
}

type muxEntry struct {
    explicit bool                // 是否精确匹配
    h        Handler              // 这个路由表达式对应哪个handler
    pattern  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ServeMux结构中最重要的字段为m，这是一个map，key是一些url模式，value是一个muxEntry结构，后者里定义存储了具体的url模式和handler。&lt;/p&gt;

&lt;p&gt;当然，所谓的ServeMux也实现了ServeHTTP接口，也算是一个handler，不过ServeMux的ServeHTTP方法不是用来处理request和respone，而是用来找到路由注册的handler，可以看服务监听时候的调用过程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// HandleFunc registers the handler function for the given pattern.
func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) {
    if handler == nil {
        panic(&amp;quot;http: nil handler&amp;quot;)
    }
    mux.Handle(pattern, HandlerFunc(handler))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ServeMux的Handle方法，将会对pattern和handler函数做一个map映射：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Handle registers the handler for the given pattern.
// If a handler already exists for pattern, Handle panics.
func (mux *ServeMux) Handle(pattern string, handler Handler) {
    mux.mu.Lock()
    defer mux.mu.Unlock()

    if pattern == &amp;quot;&amp;quot; {
        panic(&amp;quot;http: invalid pattern &amp;quot; + pattern)
    }
    if handler == nil {
        panic(&amp;quot;http: nil handler&amp;quot;)
    }
    if mux.m[pattern].explicit {
        panic(&amp;quot;http: multiple registrations for &amp;quot; + pattern)
    }

    if mux.m == nil {
        mux.m = make(map[string]muxEntry)
    }
    mux.m[pattern] = muxEntry{explicit: true, h: handler, pattern: pattern}

    if pattern[0] != &#39;/&#39; {
        mux.hosts = true
    }

    // Helpful behavior:
    // If pattern is /tree/, insert an implicit permanent redirect for /tree.
    // It can be overridden by an explicit registration.
    n := len(pattern)
    if n &amp;gt; 0 &amp;amp;&amp;amp; pattern[n-1] == &#39;/&#39; &amp;amp;&amp;amp; !mux.m[pattern[0:n-1]].explicit {
        // If pattern contains a host name, strip it and use remaining
        // path for redirect.
        path := pattern
        if pattern[0] != &#39;/&#39; {
            // In pattern, at least the last character is a &#39;/&#39;, so
            // strings.Index can&#39;t be -1.
            path = pattern[strings.Index(pattern, &amp;quot;/&amp;quot;):]
        }
        url := &amp;amp;url.URL{Path: path}
        mux.m[pattern[0:n-1]] = muxEntry{h: RedirectHandler(url.String(), StatusMovedPermanently), pattern: pattern}
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Handle函数的主要目的在于把handler和pattern模式绑定到map[string]muxEntry的map上，其中muxEntry保存了更多pattern和handler的信息，还记得前面讨论的Server结构吗？Server的m字段就是map[string]muxEntry这样一个map。&lt;/p&gt;

&lt;p&gt;此时，pattern和handler的路由注册完成。接下来就是如何开始server的监听，以接收客户端的请求。&lt;/p&gt;

&lt;h3 id=&#34;启动服务&#34;&gt;启动服务&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;http.ListenAndServe(&amp;quot;127.0.0.1:8000&amp;quot;, nil)---启动server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server := &amp;amp;Server{Addr: addr, Handler: handler}

server.ListenAndServe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注册好路由之后，启动web服务还需要开启服务器监听。http的ListenAndServer方法中可以看到创建了一个Server对象，并调用了Server对象的同名方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenAndServe(addr string, handler Handler) error {
    server := &amp;amp;Server{Addr: addr, Handler: handler}
    return server.ListenAndServe()
}
// ListenAndServe listens on the TCP network address srv.Addr and then
// calls Serve to handle requests on incoming connections.
// Accepted connections are configured to enable TCP keep-alives.
// If srv.Addr is blank, &amp;quot;:http&amp;quot; is used.
// ListenAndServe always returns a non-nil error.
func (srv *Server) ListenAndServe() error {
    addr := srv.Addr
    if addr == &amp;quot;&amp;quot; {
        addr = &amp;quot;:http&amp;quot;
    }
    ln, err := net.Listen(&amp;quot;tcp&amp;quot;, addr)
    if err != nil {
        return err
    }
    return srv.Serve(tcpKeepAliveListener{ln.(*net.TCPListener)})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Server的ListenAndServe方法中，会初始化监听地址Addr，同时调用Listen方法设置监听。最后将监听的TCP对象传入Serve方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Serve accepts incoming connections on the Listener l, creating a
// new service goroutine for each. The service goroutines read requests and
// then call srv.Handler to reply to them.
//
// For HTTP/2 support, srv.TLSConfig should be initialized to the
// provided listener&#39;s TLS Config before calling Serve. If
// srv.TLSConfig is non-nil and doesn&#39;t include the string &amp;quot;h2&amp;quot; in
// Config.NextProtos, HTTP/2 support is not enabled.
//
// Serve always returns a non-nil error. After Shutdown or Close, the
// returned error is ErrServerClosed.
func (srv *Server) Serve(l net.Listener) error {
    defer l.Close()
    if fn := testHookServerServe; fn != nil {
        fn(srv, l)
    }
    var tempDelay time.Duration // how long to sleep on accept failure

    if err := srv.setupHTTP2_Serve(); err != nil {
        return err
    }

    srv.trackListener(l, true)
    defer srv.trackListener(l, false)

    baseCtx := context.Background() // base is always background, per Issue 16220
    ctx := context.WithValue(baseCtx, ServerContextKey, srv)
    for {
        rw, e := l.Accept()
        if e != nil {
            select {
            case &amp;lt;-srv.getDoneChan():
                return ErrServerClosed
            default:
            }
            if ne, ok := e.(net.Error); ok &amp;amp;&amp;amp; ne.Temporary() {
                if tempDelay == 0 {
                    tempDelay = 5 * time.Millisecond
                } else {
                    tempDelay *= 2
                }
                if max := 1 * time.Second; tempDelay &amp;gt; max {
                    tempDelay = max
                }
                srv.logf(&amp;quot;http: Accept error: %v; retrying in %v&amp;quot;, e, tempDelay)
                time.Sleep(tempDelay)
                continue
            }
            return e
        }
        tempDelay = 0
        c := srv.newConn(rw)
        c.setState(c.rwc, StateNew) // before Serve can return
        go c.serve(ctx)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监听开启之后，一旦客户端请求到达，创建一个conn结构体，这个conn中保留了这次请求的信息，go就开启一个协程serve处理请求，主要逻辑都在serve方法之中。&lt;/p&gt;

&lt;p&gt;serve方法比较长，其主要职能就是，创建一个上下文对象，然后调用Listener的Accept方法用来　获取连接数据并使用newConn方法创建连接对象。最后使用goroutein协程的方式处理连接请求。因为每一个连接都开起了一个协程，请求的上下文都不同，同时又保证了go的高并发。serve也是一个长长的方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Serve a new connection.
func (c *conn) serve(ctx context.Context) {
    c.remoteAddr = c.rwc.RemoteAddr().String()
    ctx = context.WithValue(ctx, LocalAddrContextKey, c.rwc.LocalAddr())
    defer func() {
        if err := recover(); err != nil &amp;amp;&amp;amp; err != ErrAbortHandler {
            const size = 64 &amp;lt;&amp;lt; 10
            buf := make([]byte, size)
            buf = buf[:runtime.Stack(buf, false)]
            c.server.logf(&amp;quot;http: panic serving %v: %v\n%s&amp;quot;, c.remoteAddr, err, buf)
        }
        if !c.hijacked() {
            c.close()
            c.setState(c.rwc, StateClosed)
        }
    }()

    if tlsConn, ok := c.rwc.(*tls.Conn); ok {
        if d := c.server.ReadTimeout; d != 0 {
            c.rwc.SetReadDeadline(time.Now().Add(d))
        }
        if d := c.server.WriteTimeout; d != 0 {
            c.rwc.SetWriteDeadline(time.Now().Add(d))
        }
        if err := tlsConn.Handshake(); err != nil {
            c.server.logf(&amp;quot;http: TLS handshake error from %s: %v&amp;quot;, c.rwc.RemoteAddr(), err)
            return
        }
        c.tlsState = new(tls.ConnectionState)
        *c.tlsState = tlsConn.ConnectionState()
        if proto := c.tlsState.NegotiatedProtocol; validNPN(proto) {
            if fn := c.server.TLSNextProto[proto]; fn != nil {
                h := initNPNRequest{tlsConn, serverHandler{c.server}}
                fn(c.server, tlsConn, h)
            }
            return
        }
    }

    // HTTP/1.x from here on.

    ctx, cancelCtx := context.WithCancel(ctx)
    c.cancelCtx = cancelCtx
    defer cancelCtx()

    c.r = &amp;amp;connReader{conn: c}
    c.bufr = newBufioReader(c.r)
    c.bufw = newBufioWriterSize(checkConnErrorWriter{c}, 4&amp;lt;&amp;lt;10)

    for {
        w, err := c.readRequest(ctx)
        if c.r.remain != c.server.initialReadLimitSize() {
            // If we read any bytes off the wire, we&#39;re active.
            c.setState(c.rwc, StateActive)
        }
        if err != nil {
            const errorHeaders = &amp;quot;\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n&amp;quot;

            if err == errTooLarge {
                // Their HTTP client may or may not be
                // able to read this if we&#39;re
                // responding to them and hanging up
                // while they&#39;re still writing their
                // request. Undefined behavior.
                const publicErr = &amp;quot;431 Request Header Fields Too Large&amp;quot;
                fmt.Fprintf(c.rwc, &amp;quot;HTTP/1.1 &amp;quot;+publicErr+errorHeaders+publicErr)
                c.closeWriteAndWait()
                return
            }
            if isCommonNetReadError(err) {
                return // don&#39;t reply
            }

            publicErr := &amp;quot;400 Bad Request&amp;quot;
            if v, ok := err.(badRequestError); ok {
                publicErr = publicErr + &amp;quot;: &amp;quot; + string(v)
            }

            fmt.Fprintf(c.rwc, &amp;quot;HTTP/1.1 &amp;quot;+publicErr+errorHeaders+publicErr)
            return
        }

        // Expect 100 Continue support
        req := w.req
        if req.expectsContinue() {
            if req.ProtoAtLeast(1, 1) &amp;amp;&amp;amp; req.ContentLength != 0 {
                // Wrap the Body reader with one that replies on the connection
                req.Body = &amp;amp;expectContinueReader{readCloser: req.Body, resp: w}
            }
        } else if req.Header.get(&amp;quot;Expect&amp;quot;) != &amp;quot;&amp;quot; {
            w.sendExpectationFailed()
            return
        }

        c.curReq.Store(w)

        if requestBodyRemains(req.Body) {
            registerOnHitEOF(req.Body, w.conn.r.startBackgroundRead)
        } else {
            if w.conn.bufr.Buffered() &amp;gt; 0 {
                w.conn.r.closeNotifyFromPipelinedRequest()
            }
            w.conn.r.startBackgroundRead()
        }

        // HTTP cannot have multiple simultaneous active requests.[*]
        // Until the server replies to this request, it can&#39;t read another,
        // so we might as well run the handler in this goroutine.
        // [*] Not strictly true: HTTP pipelining. We could let them all process
        // in parallel even if their responses need to be serialized.
        // But we&#39;re not going to implement HTTP pipelining because it
        // was never deployed in the wild and the answer is HTTP/2.
        serverHandler{c.server}.ServeHTTP(w, w.req)
        w.cancelCtx()
        if c.hijacked() {
            return
        }
        w.finishRequest()
        if !w.shouldReuseConnection() {
            if w.requestBodyLimitHit || w.closedRequestBodyEarly() {
                c.closeWriteAndWait()
            }
            return
        }
        c.setState(c.rwc, StateIdle)
        c.curReq.Store((*response)(nil))

        if !w.conn.server.doKeepAlives() {
            // We&#39;re in shutdown mode. We might&#39;ve replied
            // to the user without &amp;quot;Connection: close&amp;quot; and
            // they might think they can send another
            // request, but such is life with HTTP/1.1.
            return
        }

        if d := c.server.idleTimeout(); d != 0 {
            c.rwc.SetReadDeadline(time.Now().Add(d))
            if _, err := c.bufr.Peek(4); err != nil {
                return
            }
        }
        c.rwc.SetReadDeadline(time.Time{})
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用defer定义了函数退出时，连接关闭相关的处理。然后就是读取连接的网络数据，并处理读取完毕时候的状态。接下来就是调用serverHandler{c.server}.ServeHTTP(w, w.req)方法处理请求了。最后就是请求处理完毕的逻辑。serverHandler是一个重要的结构，它近有一个字段，即Server结构，同时它也实现了Handler接口方法ServeHTTP，并在该接口方法中做了一个重要的事情，初始化multiplexer路由多路复用器。如果server对象没有指定Handler，则使用默认的DefaultServeMux作为路由Multiplexer。并调用初始化Handler的ServeHTTP方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// serverHandler delegates to either the server&#39;s Handler or
// DefaultServeMux and also handles &amp;quot;OPTIONS *&amp;quot; requests.
type serverHandler struct {
    srv *Server
}

func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) {
    handler := sh.srv.Handler
    if handler == nil {
        handler = DefaultServeMux
    }
    if req.RequestURI == &amp;quot;*&amp;quot; &amp;amp;&amp;amp; req.Method == &amp;quot;OPTIONS&amp;quot; {
        handler = globalOptionsHandler{}
    }
    handler.ServeHTTP(rw, req)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里DefaultServeMux的ServeHTTP方法其实也是定义在ServeMux结构中的，相关代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Find a handler on a handler map given a path string.
// Most-specific (longest) pattern wins.
func (mux *ServeMux) match(path string) (h Handler, pattern string) {
    // Check for exact match first.
    v, ok := mux.m[path]
    if ok {
        return v.h, v.pattern
    }

    // Check for longest valid match.
    var n = 0
    for k, v := range mux.m {
        if !pathMatch(k, path) {
            continue
        }
        if h == nil || len(k) &amp;gt; n {
            n = len(k)
            h = v.h
            pattern = v.pattern
        }
    }
    return
}
func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) {

    // CONNECT requests are not canonicalized.
    if r.Method == &amp;quot;CONNECT&amp;quot; {
        return mux.handler(r.Host, r.URL.Path)
    }

    // All other requests have any port stripped and path cleaned
    // before passing to mux.handler.
    host := stripHostPort(r.Host)
    path := cleanPath(r.URL.Path)
    if path != r.URL.Path {
        _, pattern = mux.handler(host, path)
        url := *r.URL
        url.Path = path
        return RedirectHandler(url.String(), StatusMovedPermanently), pattern
    }

    return mux.handler(host, r.URL.Path)
}

// handler is the main implementation of Handler.
// The path is known to be in canonical form, except for CONNECT methods.
func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) {
    mux.mu.RLock()
    defer mux.mu.RUnlock()

    // Host-specific pattern takes precedence over generic ones
    if mux.hosts {
        h, pattern = mux.match(host + path)
    }
    if h == nil {
        h, pattern = mux.match(path)
    }
    if h == nil {
        h, pattern = NotFoundHandler(), &amp;quot;&amp;quot;
    }
    return
}

// ServeHTTP dispatches the request to the handler whose
// pattern most closely matches the request URL.
func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) {
    if r.RequestURI == &amp;quot;*&amp;quot; {
        if r.ProtoAtLeast(1, 1) {
            w.Header().Set(&amp;quot;Connection&amp;quot;, &amp;quot;close&amp;quot;)
        }
        w.WriteHeader(StatusBadRequest)
        return
    }
    h, _ := mux.Handler(r)
    h.ServeHTTP(w, r)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mux的ServeHTTP方法通过调用其Handler方法寻找注册到路由上的handler函数，并调用该函数的ServeHTTP方法，本例则是IndexHandler函数。&lt;/p&gt;

&lt;p&gt;mux的Handler方法对URL简单的处理，然后调用handler方法，后者会创建一个锁，同时调用match方法返回一个handler和pattern。&lt;/p&gt;

&lt;p&gt;在match方法中，mux的m字段是map[string]muxEntry图，后者存储了pattern和handler处理器函数，因此通过迭代m寻找出注册路由的patten模式与实际url匹配的handler函数并返回。&lt;/p&gt;

&lt;p&gt;返回的结构一直传递到mux的ServeHTTP方法，接下来调用handler函数的ServeHTTP方法，即IndexHandler函数，然后把response写到http.RequestWirter对象返回给客户端。&lt;/p&gt;

&lt;p&gt;上述函数运行结束即serverHandler{c.server}.ServeHTTP(w, w.req)运行结束。接下来就是对请求处理完毕之后上希望和连接断开的相关逻辑。&lt;/p&gt;

&lt;p&gt;至此，Golang中一个完整的http服务介绍完毕，包括注册路由，开启监听，处理连接，路由处理函数。
多数的web应用基于HTTP协议，客户端和服务器通过request-response的方式交互。一个server并不可少的两部分莫过于路由注册和连接处理。Golang通过一个ServeMux实现了的multiplexer路由多路复用器来管理路由。同时提供一个Handler接口提供ServeHTTP用来实现handler处理其函数，后者可以处理实际request并构造response。&lt;/p&gt;

&lt;h3 id=&#34;总结-1&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;理解go中的http服务，最重要就是要理解Multiplexer和handler，Golang中的Multiplexer基于ServeMux结构，同时也实现了Handler接口。下面对几个重要概念说明，两个重要的结构体和一个接口&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Handler类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Golang没有继承，类多态的方式可以通过接口实现。所谓接口则是定义声明了函数签名，任何结构只要实现了与接口函数签名相同的方法，就等同于实现了接口。go的http服务都是基于handler进行处理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Handler interface {
    ServeHTTP(ResponseWriter, *Request)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;任何结构体，只要实现了ServeHTTP方法，这个结构就可以称之为handler对象。ServeMux会使用handler并调用其ServeHTTP方法处理请求并返回响应。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;handler处理器(函数)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;handler处理器(函数)-就是HandleFunc的第二个参数，是一个函数： 具有func(w http.ResponseWriter, r *http.Requests)签名的函数，经过HandlerFunc结构包装的handler函数，它实现了ServeHTTP接口方法的函数。调用handler处理器的ServeHTTP方法时，即调用handler函数本身。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;handler对象：实现了Handler接口ServeHTTP方法的结构。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ServeMux和handler处理器函数的连接桥梁就是Handler接口。ServeMux的ServeHTTP方法实现了寻找注册路由的handler的函数，并调用该handler的ServeHTTP方法。ServeHTTP方法就是真正处理请求和构造响应的地方。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Server结构体&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从http.ListenAndServe的源码可以看出，它还是创建了一个server对象，并调用server对象的ListenAndServe方法来实现监听路由：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenAndServe(addr string, handler Handler) error {
    server := &amp;amp;Server{Addr: addr, Handler: handler}
    return server.ListenAndServe()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看server的结构如下，其实上面已经解释过：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server struct {
    Addr         string        
    Handler      Handler       
    ReadTimeout  time.Duration 
    WriteTimeout time.Duration 
    TLSConfig    *tls.Config   

    MaxHeaderBytes int

    TLSNextProto map[string]func(*Server, *tls.Conn, Handler)

    ConnState func(net.Conn, ConnState)
    ErrorLog *log.Logger
    disableKeepAlives int32     nextProtoOnce     sync.Once 
    nextProtoErr      error     
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;server结构存储了服务器处理请求常见的字段。其中Handler字段也保留Handler接口。如果Server接口没有提供Handler结构对象，那么会使用DefautServeMux做multiplexer，后面再做分析。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;路由结构体ServeMux&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ServeMux的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ServeMux struct {
    mu    sync.RWMutex                      //锁，由于请求涉及到并发处理，因此这里需要一个锁机制
    m     map[string]muxEntry               // 路由规则，一个string对应一个mux实体，这里的string就是注册的路由
    hosts bool 
}

type muxEntry struct {
    explicit bool                // 是否精确匹配
    h        Handler              // 这个路由表达式对应哪个handler
    pattern  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ServeMux结构中最重要的字段为m，这是一个map，key是一些url模式，value是一个muxEntry结构，后者里定义存储了具体的url模式和handler。&lt;/p&gt;

&lt;p&gt;当然，所谓的ServeMux也实现了ServeHTTP接口，也算是一个handler，不过ServeMux的ServeHTTP方法不是用来处理request和respone，而是用来找到路由注册的handler&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Go代码的执行流程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;过对http包的分析之后，现在让我们来梳理一下整个的代码执行过程。&lt;/p&gt;

&lt;p&gt;1、首先调用Http.HandleFunc&lt;/p&gt;

&lt;p&gt;按顺序做了几件事:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1 调用了DefaultServerMux的HandleFunc
2 调用了DefaultServerMux的Handle
3 往DefaultServeMux的map[string]muxEntry中增加对应的handler和路由规则
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、其次调用http.ListenAndServe(&amp;rdquo;:9090&amp;rdquo;, nil)&lt;/p&gt;

&lt;p&gt;按顺序做了几件事情:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1 实例化Server
2 调用Server的ListenAndServe()
3 调用net.Listen(&amp;quot;tcp&amp;quot;, addr)监听端口
4 启动一个for循环，在循环体中Accept请求
5 对每个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve()
6 读取每个请求的内容w, err := c.readRequest()
7 判断handler是否为空，如果没有设置handler(这个例子就没有设置handler)，handler就设置为 DefaultServeMux
8 调用handler的ServeHttp
9 在这个例子中，下面就进入到DefaultServerMux.ServeHttp
10 根据request选择handler，并且进入到这个handler的ServeHTTP mux.handler(r).ServeHTTP(w, r)
11 选择handler:
    A 判断是否有路由能满足这个request(循环遍历ServerMux的muxEntry)
    B 如果有路由满足，调用这个路由handler的ServeHttp
    C 如果没有路由满足，调用NotFoundHandler的ServeHttp
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自定义server&#34;&gt;自定义server&lt;/h2&gt;

&lt;p&gt;要管理服务端的行为，可以创建一个自定义的Server：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s := &amp;amp;http.Server{
    Addr:           &amp;quot;:8080&amp;quot;,
    Handler:        myHandler,
    ReadTimeout:    10 * time.Second,
    WriteTimeout:   10 * time.Second,
    MaxHeaderBytes: 1 &amp;lt;&amp;lt; 20,
}
log.Fatal(s.ListenAndServe())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也是上面的流程，就是新增了一个server结构体的，做对应的操作，来看一下server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server struct {
    Addr           string        // TCP address to listen on, &amp;quot;:http&amp;quot; if empty
    Handler        Handler       // handler to invoke, http.DefaultServeMux if nil
    ReadTimeout    time.Duration // maximum duration before timing out read of the request
    WriteTimeout   time.Duration // maximum duration before timing out write of the response
    MaxHeaderBytes int           // maximum size of request headers, DefaultMaxHeaderBytes if 0
    TLSConfig      *tls.Config   // optional TLS config, used by ListenAndServeTLS

    // TLSNextProto optionally specifies a function to take over
    // ownership of the provided TLS connection when an NPN
    // protocol upgrade has occurred.  The map key is the protocol
    // name negotiated. The Handler argument should be used to
    // handle HTTP requests and will initialize the Request&#39;s TLS
    // and RemoteAddr if not already set.  The connection is
    // automatically closed when the function returns.
    TLSNextProto map[string]func(*Server, *tls.Conn, Handler)

    // ConnState specifies an optional callback function that is
    // called when a client connection changes state. See the
    // ConnState type and associated constants for details.
    ConnState func(net.Conn, ConnState)

    // ErrorLog specifies an optional logger for errors accepting
    // connections and unexpected behavior from handlers.
    // If nil, logging goes to os.Stderr via the log package&#39;s
    // standard logger.
    ErrorLog *log.Logger
    // contains filtered or unexported fields
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;都是什么作用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server struct {
    Addr           string        // 监听的TCP地址，如果为空字符串会使用&amp;quot;:http&amp;quot;
    Handler        Handler       // 调用的处理器，如为nil会调用http.DefaultServeMux
    ReadTimeout    time.Duration // 请求的读取操作在超时前的最大持续时间
    WriteTimeout   time.Duration // 回复的写入操作在超时前的最大持续时间
    MaxHeaderBytes int           // 请求的头域最大长度，如为0则用DefaultMaxHeaderBytes
    TLSConfig      *tls.Config   // 可选的TLS配置，用于ListenAndServeTLS方法
    // TLSNextProto（可选地）指定一个函数来在一个NPN型协议升级出现时接管TLS连接的所有权。
    // 映射的键为商谈的协议名；映射的值为函数，该函数的Handler参数应处理HTTP请求，
    // 并且初始化Handler.ServeHTTP的*Request参数的TLS和RemoteAddr字段（如果未设置）。
    // 连接在函数返回时会自动关闭。
    TLSNextProto map[string]func(*Server, *tls.Conn, Handler)
    // ConnState字段指定一个可选的回调函数，该函数会在一个与客户端的连接改变状态时被调用。
    // 参见ConnState类型和相关常数获取细节。
    ConnState func(net.Conn, ConnState)
    // ErrorLog指定一个可选的日志记录器，用于记录接收连接时的错误和处理器不正常的行为。
    // 如果本字段为nil，日志会通过log包的标准日志记录器写入os.Stderr。
    ErrorLog *log.Logger
    // 内含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server
func (s *Server) SetKeepAlivesEnabled(v bool)
func (srv *Server) Serve(l net.Listener) error
func (srv *Server) ListenAndServe() error
func (srv *Server) ListenAndServeTLS(certFile, keyFile string) error

func (*Server) SetKeepAlivesEnabled
func (s *Server) SetKeepAlivesEnabled(v bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SetKeepAlivesEnabled控制是否允许HTTP闲置连接重用（keep-alive）功能。默认该功能总是被启用的。只有资源非常紧张的环境或者服务端在关闭进程中时，才应该关闭该功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Server) Serve
func (srv *Server) Serve(l net.Listener) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Serve会接手监听器l收到的每一个连接，并为每一个连接创建一个新的服务go程。该go程会读取请求，然后调用srv.Handler回复请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Server) ListenAndServe
func (srv *Server) ListenAndServe() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenAndServe监听srv.Addr指定的TCP地址，并且会调用Serve方法接收到的连接。如果srv.Addr为空字符串，会使用&amp;rdquo;:http&amp;rdquo;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Server) ListenAndServeTLS
func (srv *Server) ListenAndServeTLS(certFile, keyFile string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenAndServeTLS监听srv.Addr确定的TCP地址，并且会调用Serve方法处理接收到的连接。必须提供证书文件和对应的私钥文件。如果证书是由权威机构签发的，certFile参数必须是顺序串联的服务端证书和CA证书。如果srv.Addr为空字符串，会使用&amp;rdquo;:https&amp;rdquo;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算CDCI系列---- jenkins</title>
          <link>https://kingjcy.github.io/post/cloud/paas/cdci/jenkins/</link>
          <pubDate>Fri, 13 Sep 2019 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/cdci/jenkins/</guid>
          <description>&lt;p&gt;Jenkins是一款开源 CI&amp;amp;CD 软件，用于自动化各种任务，包括构建、测试和部署软件,Jenkins 支持各种运行方式，可通过系统包、Docker 或者通过一个独立的 Java 程序。&lt;/p&gt;

&lt;h1 id=&#34;jenkins&#34;&gt;Jenkins&lt;/h1&gt;

&lt;p&gt;为什么选择jenkins？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;jenkins无论是使用基数还是使用趋势,jenkins的学习性价比对比travis（老牌的托管工具），gitlab ci,bamboo（存在关键词意义重叠还没有解决方案）要高许多.&lt;/li&gt;
&lt;li&gt;从二次开发的角度来看,jenkins开源,而且使用的语言是java,使用的框架为spring,两者分别为国内语言社区和框架社区中的顶级社区,发展的特别的好.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;jenkins的中文官方文档还是比较好的，可以作为很有用的&lt;a href=&#34;https://www.jenkins.io/zh/doc/&#34;&gt;参考文档&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;Jenkins通常作为一个独立的应用程序在其自己的流程中运行， 内置Java servlet 容器/应用程序服务器（Jetty）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;系统要求&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;最低推荐配置:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;256MB可用内存&lt;/li&gt;
&lt;li&gt;1GB可用磁盘空间(作为一个Docker容器运行jenkins的话推荐10GB)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为小团队推荐的硬件配置:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1GB+可用内存&lt;/li&gt;
&lt;li&gt;50 GB+ 可用磁盘空间&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;软件配置:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Java 8—​无论是Java运行时环境（JRE）还是Java开发工具包（JDK）都可以。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;docker&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;直接使用官方镜像运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run \
    -u root \
    --rm \
    -d \
    -p 8080:8080 \
    -p 50000:50000 \
    -v jenkins-data:/var/jenkins_home \
    -v /var/run/docker.sock:/var/run/docker.sock \
    jenkinsci/blueocean
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&amp;ndash;rm：（可选） jenkinsci/blueocean 关闭时自动删除Docker容器（下图为实例）。如果您需要退出Jenkins，这可以保持整洁。&lt;/li&gt;
&lt;li&gt;-d：（可选）jenkinsci/blueocean 在后台运行容器（即“分离”模式）并输出容器ID。如果您不指定此选项， 则在终端窗口中输出正在运行的此容器的Docker日志。&lt;/li&gt;
&lt;li&gt;-p 8080:8080：映射（例如“发布”）jenkinsci/blueocean 容器的端口8080到主机上的端口8080。 第一个数字代表主机上的端口，而最后一个代表容器的端口。因此，如果您为此选项指定 -p 49000:8080 ，您将通过端口49000访问主机上的Jenkins。&lt;/li&gt;
&lt;li&gt;-p 50000:50000：（可选）将 jenkinsci/blueocean 容器的端口50000 映射到主机上的端口50000。 如果您在其他机器上设置了一个或多个基于JNLP的Jenkins代理程序，而这些代理程序又与 jenkinsci/blueocean 容器交互（充当“主”Jenkins服务器，或者简称为“Jenkins主”）， 则这是必需的。默认情况下，基于JNLP的Jenkins代理通过TCP端口50000与Jenkins主站进行通信。 您可以通过“ 配置全局安全性” 页面更改Jenkins主服务器上的端口号。如果您要将您的Jenkins主机的JNLP代理端口的TCP端口 值更改为51000（例如），那么您需要重新运行Jenkins（通过此 docker run …​命令）并指定此“发布”选项 -p 52000:51000，其中最后一个值与Jenkins master上的这个更改值相匹配，第一个值是Jenkins主机的主机上的端口号， 通过它，基于JNLP的Jenkins代理与Jenkins主机进行通信 - 例如52000。&lt;/li&gt;
&lt;li&gt;-v jenkins-data:/var/jenkins_home：（可选，但强烈建议）映射在容器中的&lt;code&gt;/var/jenkins_home&lt;/code&gt; 目录到具有名字 jenkins-data 的volume。 如果这个卷不存在，那么这个 docker run 命令会自动为你创建卷。 如果您希望每次重新启动Jenkins（通过此 docker run &amp;hellip; 命令）时保持Jenkins状态，则此选项是必需的 。 如果你没有指定这个选项，那么在每次重新启动后，Jenkins将有效地重置为新的实例。
注意: 所述的 jenkins-data 卷也可以 docker volume create命令创建： docker volume create jenkins-data 代替映射 /var/jenkins_home 目录转换为Docker卷，还 可以将此目录映射到计算机本地文件系统上的目录。 例如，指定该选项 -v $HOME/jenkins:/var/jenkins_home 会将容器的 /var/jenkins_home 目录映射 到 本地计算机上目录中的 jenkins 子目录， 该$HOME目录通常是 /Users/&lt;your-username&gt;/jenkins 或&lt;code&gt;/home/&amp;lt;your-username&amp;gt;/jenkins&lt;/code&gt; 。&lt;/li&gt;
&lt;li&gt;-v /var/run/docker.sock:/var/run/docker.sock（可选 /var/run/docker.sock 表示Docker守护程序通过其监听的基于Unix的套接字。 该映射允许 jenkinsci/blueocean 容器与Docker守护进程通信， 如果 jenkinsci/blueocean 容器需要实例化其他Docker容器，则该守护进程是必需的。 如果运行声明式管道，其语法包含agent部分用 docker
例如， agent { docker { &amp;hellip; } } 此选项是必需的。 在Pipeline Syntax 页面上阅读更多关于这个的信息 。&lt;/li&gt;
&lt;li&gt;jenkinsci/blueocean Docker镜像本身。如果此镜像尚未下载，则此 docker run 命令 将自动为您下载镜像。此外，如果自上次运行此命令后发布了此镜像的任何更新， 则再次运行此命令将自动为您下载这些已发布的镜像更新。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Linux&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以直接用yum源或者不同操作系统的包管理工具进行安装，比如我们在CentOS 7上用yum安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Java 8
yum install java

# Jenkins stable version
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key
yum install jenkins

# start jenkins
service jenkins start

# 初始化配置向导
http://192.168.56.103:8080/

cat /var/lib/jenkins/secrets/initialAdminPassword
5224fc83b6d84cc2be69a18c53309ea4

Install suggested plugins
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;war&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;直接去官网下载 war 文件，并 cd 到 jenkins.war 所在目录，执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -jar jenkins.war --httpPort=8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;war 包自带 jetty 服务器，以上命令会自动启服务器，并完成部署。&lt;/p&gt;

&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Job 类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Freestyle project
自由风格项目，Jenkins 最主要的项目类型&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Maven Project
Maven 项目专用，类似 Freestyle，更简单&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multi-configuration project
多配置项目，适合需要大量不同配置 (环境，平台等) 构建&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pipeline
流水线项目，适合使用 pipeline(workflow)插件功能构建流水线任务，或者使用 Freestyle project 不容易实现的复杂任务&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multibranch Pipeline
多分支流水线项目，根据 SCM 仓库中的分支创建多个 Pipeline 项目&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Freestyle 项目&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;General
项目基本配置
项目名字，描述，参数，禁用项目，并发构建，限制构建默认 node 等等&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Source code Management
代码库信息，支持 Git，Subversion 等&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Build Triggers
构建触发方式
周期性构建，Poll SCM，远程脚本触发构建，其他项目构建结束后触发等&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Build Environment
构建环境相关设置
构建前删除 workspace，向 Console 输出添加时间戳，设置构建名称，插入环境变量等&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Build
项目构建任务
添加 1 个或者多个构建步骤&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Post-build Actions
构建后行为
Artifact 归档，邮件通知，发布单元测试报告，触发下游项目等等&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Pipeline项目&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pipeline，简而言之，就是一套运行于 Jenkins 上的工作流框架，将原本独立 运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排与可视化。&lt;/p&gt;

&lt;p&gt;Pipeline 是 Jenkins2.X 最核心的特性，帮助 Jenkins 实现从 CI 到 CD 与 DevOps 的转变&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Stage
阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，例如: “Build”, “Test”, “Deploy” 。
注意，Stage 是一个逻辑分组的概念，可以跨多个 Node。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Node
节点，一个 Node 就是一个 Jenkins 节点，或者是 Master，或者是 Agent，是执行 Step 的具体 运行环境。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Step
步骤，Step 是最基本的操作单元，小到创建一个目录，大到构建一个 Docker 镜像，由各类 Jenkins Plugin 提供，例如: sh ‘make’&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Pipeline 和 Freestyle 的区别&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Freestyle:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;上游 / 下游 Job 调度，如 BuildJob -&amp;gt;TestJob -&amp;gt; DeployJob&lt;/li&gt;
&lt;li&gt;在 DSL Job 里面调度多个子 Job(利用 Build Flow plugin)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pipeline:
- 单个 Job 中完成所有的任务编排
- 全局视图&lt;/p&gt;

&lt;p&gt;Pipeline 会取代 Freestyle 么?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Pipeline 一定会取代 Build Flow 插件
会，当你希望做到 Pipeline as code 的时候
会，当你独立运行一组 Job 没有特殊价值或者意义的时候
会，当你可以从 Multibranch Pipeline 受益的时候
会，当你希望获取类似于 TravisCI 风格的工作流的时候
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;创建第一个Freestyle Job&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、安装 Timestamper 插件&lt;/p&gt;

&lt;p&gt;系统管理 - 插件管理 - 可用插件，搜索到 timestamper 点击 Install without restart&lt;/p&gt;

&lt;p&gt;2、新建一个 Freestyle 类型的 Job&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;General 项目名称: My-first-freestyle-demo&lt;/li&gt;
&lt;li&gt;Build Environment 构建环境: 勾选 Add timestamps to the Console Output&lt;/li&gt;
&lt;li&gt;Build 构建: 屏幕打印出 “这是我的第一个 Jenkins Job, oops”&lt;/li&gt;
&lt;li&gt;Post-build Actions 构建后操作: 无&lt;/li&gt;
&lt;li&gt;点击立刻构建&lt;/li&gt;

&lt;li&gt;&lt;p&gt;找到控制台输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Console Output
14:40:59 Started by user admin
14:40:59 Building in workspace /var/lib/jenkins/workspace/My-first-freestyle-demo
14:41:00 [My-first-freestyle-demo] $ /bin/sh -xe /tmp/jenkins3737737887278720679.sh
14:41:00 + echo &#39;这是我的第一个 Jenkins Job, oops&#39;
14:41:00 这是我的第一个 Jenkins Job, oops
14:41:00 Finished: SUCCESS
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Pipeline&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pipeline 脚本是由 Groovy 语言实现&lt;/p&gt;

&lt;p&gt;Pipeline 支持两种语法
- Declarative 声明式(在 Pipeline plugin 2.5 中引入)
- Scripted Pipeline 脚本式&lt;/p&gt;

&lt;p&gt;如何创建基本的 Pipeline
- 直接在 Jenkins Web UI 网页界面中输入脚本
- 通过创建一个 Jenkinsfile 可以检入项目的源代码管理库&lt;/p&gt;

&lt;p&gt;最佳实践
- 通常推荐在 Jenkins 中直接从源代码控制 (SCM) 中载入 Jenkinsfile Pipeline&lt;/p&gt;

&lt;p&gt;快速创建一个简单的 Pipeline&lt;/p&gt;

&lt;p&gt;1、新建 Job: Jenkins -&amp;gt; 新建 -&amp;gt; 输入 Job 名称: “My-first-pipeline-demo” -&amp;gt; 选择 Pipeline -&amp;gt; 点击 “OK”&lt;/p&gt;

&lt;p&gt;2、配置: 在 Pipeline -&amp;gt; Script 文本输入框中输入下列语句，点击 ”保存”&lt;/p&gt;

&lt;p&gt;3、立即构建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pipeline {
    agent any
    stages {
        stage(&#39;Build&#39;) {
            steps {
                echo &#39;Build&#39;
            }
        }
        stage(&#39;Test&#39;) {
            steps {
                echo &#39;Test&#39;
            }
        }
        stage(&#39;Deploy&#39;) {
            steps {
                echo &#39;Deploy&#39;
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;kubernetes集成jenkins实现cicd&#34;&gt;Kubernetes集成Jenkins实现CICD&lt;/h1&gt;

&lt;h2 id=&#34;架构&#34;&gt;架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/jenkins/jenkins&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;github/gitlab/svn，存储代码的仓库，研发人员code代码提交到代码仓库中，触发jenkins的job任务。&lt;/li&gt;
&lt;li&gt;jenkins，CICD组件，负责编译，打包，安装。jenkins读取项目配置文件，进行测试，编译，构建镜像，推送到的harbor镜像仓库，然后生成helm部署的yaml文件。&lt;/li&gt;
&lt;li&gt;harbor，docker镜像仓库。存储jenkins推送的镜像，给helm在k8s上部署的时候拉去。&lt;/li&gt;
&lt;li&gt;helm，批量部署镜像，使用jenkins生成的yaml文件批量部署应用。&lt;/li&gt;
&lt;li&gt;k8s，容器编排调度平台。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们再来看看基于Jenkins的CI/CD流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/jenkins/jenkins1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;应用构建和发布流程说明。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户向Gitlab提交代码，代码中必须包含Dockerfile&lt;/li&gt;
&lt;li&gt;用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数，确定后触发Jenkins自动构建&lt;/li&gt;
&lt;li&gt;Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库&lt;/li&gt;
&lt;li&gt;Jenkins的CI流水线中包括了自定义脚本，根据我们已准备好的kubernetes的YAML模板，将其中的变量替换成用户输入的选项&lt;/li&gt;
&lt;li&gt;生成应用的kubernetes YAML配置文件&lt;/li&gt;
&lt;li&gt;更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息&lt;/li&gt;
&lt;li&gt;更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。关于边缘节点，请查看边缘节点配置&lt;/li&gt;
&lt;li&gt;Jenkins调用kubernetes的API，部署应用&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;k8s中安装jenkins&#34;&gt;k8s中安装jenkins&lt;/h2&gt;

&lt;p&gt;通过service暴露服务，通过deployment来部署jenkins，使用jenkins镜像，同样设置启动参数，还有以下就是用户和权限的问题。具体可以参考[这里]()。&lt;/p&gt;

&lt;h2 id=&#34;插件&#34;&gt;插件&lt;/h2&gt;

&lt;p&gt;为了方便集成 Maven、Kubernetes、配置文件等等，这里需要安装几个插件,这里插件可以在 系统管理—&amp;gt;插件管理—&amp;gt;可选插件 里面安装下面列出的插件。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Git 插件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Jenkins 安装中默认安装 Git 插件，所以不需要单独安装。利用 git 工具可以将 github、gitlab 等等的地址下载源码。&lt;/li&gt;
&lt;li&gt;如果是私有项目 Git 一般需要配置一个凭据用于验证（凭据-&amp;gt;系统-&amp;gt;全局凭据-&amp;gt;添加凭据），如果是公开项目，则无需任何配置。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;利用 Git 插件拉取源码,分别可以在jenkinsfile中设置拉取的“分支”、“显示拉取日志”、“拉取的凭据”、“拉取的地址”。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git branch: &amp;quot;master&amp;quot; ,changelog: true , credentialsId: &amp;quot;xxxx-xxxx-xxxx&amp;quot;, url: &amp;quot;https://github.com/xxxxxx&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Docker 插件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Jenkins 安装中默认安装 Docker 插件，所以不需要单独安装。利用 Docker 插件可以设置 Docker 环境，运行 Docker 命令，配置远程 Docker 仓库凭据等。比如写一个简单的执行例子来描述 Docker 镜像的构建过程在jenkinsfile中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 此方法是设置docker仓库地址，然后选择存了用户名、密码的凭据ID进行验证。注意，只有在此方法之中才生效。
docker.withRegistry(&amp;quot;https://hub.docker.com/&amp;quot;, &amp;quot;xxxxx-xxxx-xxxx-xxxx&amp;quot;) {
    echo &amp;quot;构建镜像&amp;quot;
    def customImage = docker.build(&amp;quot;hub.mydlq.club/myproject/springboot-helloworld:0.0.1&amp;quot;)
    echo &amp;quot;推送镜像&amp;quot;
    customImage.push()
    echo &amp;quot;删除镜像&amp;quot;
    sh &amp;quot;docker rmi hub.mydlq.club/myproject/springboot-helloworld:0.0.1&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kubernetes&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 插件的目的是能够使用 Kubernetes 集群动态配置 Jenkins 代理（使用Kubernetes调度机制来优化负载），运行单个构建，等构建完成后删除该代理。这里我们需要用到这个插件来启动 Jenkins Slave 代理镜像，让代理执行 Jenkins 要执行的 Job。&lt;/li&gt;
&lt;li&gt;需要配置连接 kubernetes 集群的凭据（Kubernetes ServiceAccount token），此凭据的账户权限最好设置较大点，避免出现未知问题。配置完成后，需要在后面的 Cloud 云配置中设置这个凭据。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;需要配置连接Kubernetes集群，启动 Jenkins Slave 代理的相关配置（系统管理—&amp;gt;系统设置—&amp;gt;云）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;名称： kubernetesKubernetes
地址： kubernetes.default.svc.cluster.local (默认集群内调用 k8s api 地址)
禁用 HTTPS 证书检查： 勾选 (不验证https)
凭据： 新增凭据—&amp;gt;Secret text—&amp;gt;Secret 设置 kubernetes 的 Token (进入 k8s dashboard 的 token 等都行)
Jenkins地址： jenkins.mydlqcloud:8080/jenkins (用于代理与 Jenkins 连接的地址，用的是 k8s 集群中 jenkins 服务的地址为“http://jenkins服务名.jenkins所在namespace:jenkins端口号/jenkins后缀”)
其他： 默认即可
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Template 模板配置，也就是我们启动应用的yaml文件&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kubernetes Cli&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kubernetes Cli 插件作用是在执行 Jenkins Job 时候提供 kubectl 与 Kubernetes 集群交互环境。可以在 Pipeline 或自由式项目中允许执行 kubectl 相关命令。它的主要作用是提供 kubectl 运行环境，当然也可以提供 helm 运行环境。比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 提供 kubectl 执行的环境，其中得设置存储了 token 的凭据ID和 kubernetes api 地址
withKubeConfig([credentialsId: &amp;quot;xxxx-xxxx-xxxx-xxxx&amp;quot;,serverUrl: &amp;quot;https://kubernetes.default.svc.cluster.local&amp;quot;]) {
    sh &amp;quot;kubectl get nodes&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Config File Provider&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Config File Provider 插件作用就是提供在 Jenkins 中存储 properties、xml、json、settings.xml 等信息，可以在执行 Pipeline 过程中可以写入存储的配置。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pipeline Utility Steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这是一个操作文件的插件，例如读写 json、yaml、pom.xml、Properties 等等。在这里主要用这个插件读取 pom.xml 文件的参数设置，获取变量，方便构建 Docker 镜像。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;实践&#34;&gt;实践&lt;/h2&gt;

&lt;p&gt;1、创建一个名为 “k8s-test” 的任务，类型选择“流水线”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/jenkins/jenkins2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、配置流水线&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;常规配置

&lt;ul&gt;
&lt;li&gt;为了安全，禁止并发构建。&lt;/li&gt;
&lt;li&gt;为了提升效率，这里设置流水线效率，持久保存设置覆盖。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/jenkins/jenkins3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pipeline脚本也就是jenkinsfile&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 代理名称，填写系统设置中设置的 Cloud 中 Template 模板的 label
def label = &amp;quot;jnlp-agent&amp;quot;

// 调用Kubernetes提供的方法
podTemplate(label: label,cloud: &#39;kubernetes&#39; ){
    // 在代理节点上运行脚本
    node (label) {
        echo &amp;quot;测试 kubernetes 中 jenkins slave 代理！~&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心就是在这边的流水线脚本，他是一个步骤一个步骤的执行，没步骤执行的命令如果没有就需要安装对应的插件，整个脚本是使用Groovy语言。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、运行构建&lt;/p&gt;

&lt;p&gt;回到任务界面，点击立即构造来执行任务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/jenkins/jenkins4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4、查看流水线日志&lt;/p&gt;

&lt;p&gt;点击执行历史栏中点击，查看控制台输出的日志信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/jenkins/jenkins5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;jenkinsfile&#34;&gt;jenkinsfile&lt;/h3&gt;

&lt;p&gt;我们这边重点来看看jenkinsfile。首先是 Jenkinsfile 脚本存放在哪比较方便？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、新建 Git 项目，专门存放不同的 jenkinsfile 脚本，Jenkins 创建任务时候指定脚本存放的 Git 地址；方便统一管理，一改动git上的配置，jenkins 任务的流水线脚本都会跟着变化；也是我最推荐的方式。&lt;/li&gt;
&lt;li&gt;2、放到各个项目中，当在执行 Jenkins 任务时候读取 Git项目，从中检测 jenkinsfile 脚本从而执行；可以针对每个项目单独设置，更灵活，就是不方便统一管理，维护需要各个项目组；&lt;/li&gt;
&lt;li&gt;3、每个脚本都放置到 Jenkins 每个任务的配置中，每次都执行配置中设置的脚本；每次都新建项目时候在配置中设置脚本，比较费力不方便维护，不太推荐；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后我们先写一个简单的脚本，用于一般项目的构建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def label = &amp;quot;jnlp-agent&amp;quot;

podTemplate(label: label,cloud: &#39;kubernetes&#39; ){
    node (label) {
        stage(&#39;Git阶段&#39;){
            echo &amp;quot;1、开始拉取代码&amp;quot;
            sh &amp;quot;git version&amp;quot;
        }
        stage(&#39;Maven阶段&#39;){
            container(&#39;maven&#39;) {
                echo &amp;quot;2、开始Maven编译、推送到本地库&amp;quot;
                sh &amp;quot;mvn -version&amp;quot;
            }
        }
        stage(&#39;Docker阶段&#39;){
            container(&#39;docker&#39;) {
                echo &amp;quot;3、开始读取Maven pom变量，并执行Docker编译、推送、删除&amp;quot;
                sh &amp;quot;docker version&amp;quot;
            }
        }
         stage(&#39;Helm阶段&#39;){
            container(&#39;helm-kubectl&#39;) {
                echo &amp;quot;4、开始检测Kubectl环境，测试执行Helm部署，与执行部署&amp;quot;
                sh &amp;quot;helm version&amp;quot;
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个里面并没有真正的操作，只是一个流程，我们运行一下看看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Running on jnlp-agent-g7qk5 in /home/jenkins/workspace/k8s-test
[Pipeline] {
[Pipeline] stage
[Pipeline] { (Git阶段)
[Pipeline] echo
1、开始拉取代码
[Pipeline] sh
+ git version
git version 2.11.0
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Maven阶段)
[Pipeline] container
[Pipeline] {
[Pipeline] echo
2、开始Maven编译、推送到本地库
[Pipeline] sh
+ mvn -version
Apache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-24T18:41:47Z)
Maven home: /usr/share/maven
Java version: 1.8.0_201, vendor: Oracle Corporation, runtime: /usr/lib/jvm/java-1.8-openjdk/jre
Default locale: en_US, platform encoding: UTF-8
OS name: &amp;quot;linux&amp;quot;, version: &amp;quot;3.10.0-957.1.3.el7.x86_64&amp;quot;, arch: &amp;quot;amd64&amp;quot;, family: &amp;quot;unix&amp;quot;
[Pipeline] }
[Pipeline] // container
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Docker阶段)
[Pipeline] container
[Pipeline] {
[Pipeline] echo
3、开始读取Maven pom变量，并执行Docker编译、推送、删除
[Pipeline] sh
+ docker version
Client:
 Version:           18.06.2-ce
 API version:       1.38
 Go version:        go1.10.4
 Git commit:        6d37f41
 Built:             Sun Feb 10 03:43:40 2019
 OS/Arch:           linux/amd64
 Experimental:      false
Server: Docker Engine - Community
 Engine:
  Version:          18.09.3
  API version:      1.39 (minimum version 1.12)
  Go version:       go1.10.8
  Git commit:       774a1f4
  Built:            Thu Feb 28 06:02:24 2019
  OS/Arch:          linux/amd64
  Experimental:     false
[Pipeline] }
[Pipeline] // container
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Helm阶段)
[Pipeline] container
[Pipeline] {
[Pipeline] echo
4、开始检测Kubectl环境，测试执行Helm部署，与执行部署
[Pipeline] sh
+ helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.13.1&amp;quot;, GitCommit:&amp;quot;618447cbf203d147601b4b9bd7f8c37a5d39fbb4&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.13.1&amp;quot;, GitCommit:&amp;quot;79d07943b03aea2b76c12644b4b54733bc5958d6&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
[Pipeline] // podTemplate
[Pipeline] End of Pipeline
Finished: SUCCESS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到清晰的流程，我们来丰富每个流程的操作&lt;/p&gt;

&lt;p&gt;1、Git 拉取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def label = &amp;quot;jnlp-agent&amp;quot;

podTemplate(label: label,cloud: &#39;kubernetes&#39; ){
    node (label) {
        stage(&#39;Git阶段&#39;){
            echo &amp;quot;Git 阶段&amp;quot;
            git branch: &amp;quot;master&amp;quot; ,changelog: true , url: &amp;quot;https://github.com/my-dlq/springboot-helloworld.git&amp;quot;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、Maven 编译&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def label = &amp;quot;jnlp-agent&amp;quot;

podTemplate(label: label,cloud: &#39;kubernetes&#39; ){
    node (label) {
        stage(&#39;Git阶段&#39;){
            echo &amp;quot;Git 阶段&amp;quot;
            git branch: &amp;quot;master&amp;quot; ,changelog: true , url: &amp;quot;https://github.com/my-dlq/springboot-helloworld.git&amp;quot;
        }
        stage(&#39;Maven阶段&#39;){
            container(&#39;maven&#39;) {
                //这里引用上面设置的全局的 settings.xml 文件，根据其ID将其引入并创建该文件
                configFileProvider([configFile(fileId: &amp;quot;75884c5a-4ec2-4dc0-8d87-58b6b1636f8a&amp;quot;, targetLocation: &amp;quot;settings.xml&amp;quot;)]){
                    sh &amp;quot;mvn clean install -Dmaven.test.skip=true --settings settings.xml&amp;quot;
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Docker 编译&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def label = &amp;quot;jnlp-agent&amp;quot;

podTemplate(label: label,cloud: &#39;kubernetes&#39; ){
    node (label) {
        stage(&#39;Git阶段&#39;){
            echo &amp;quot;Git 阶段&amp;quot;
            git branch: &amp;quot;master&amp;quot; ,changelog: true , url: &amp;quot;https://github.com/my-dlq/springboot-helloworld.git&amp;quot;
        }
        stage(&#39;Maven阶段&#39;){
            echo &amp;quot;Maven 阶段&amp;quot;
            container(&#39;maven&#39;) {
                //这里引用上面设置的全局的 settings.xml 文件，根据其ID将其引入并创建该文件
                configFileProvider([configFile(fileId: &amp;quot;75884c5a-4ec2-4dc0-8d87-58b6b1636f8a&amp;quot;, targetLocation: &amp;quot;settings.xml&amp;quot;)]){
                    sh &amp;quot;mvn clean install -Dmaven.test.skip=true --settings settings.xml&amp;quot;
                }
            }
        }
        stage(&#39;Docker阶段&#39;){
            echo &amp;quot;Docker 阶段&amp;quot;
            container(&#39;docker&#39;) {
                // 读取pom参数
                echo &amp;quot;读取 pom.xml 参数&amp;quot;
                pom = readMavenPom file: &#39;./pom.xml&#39;
                // 设置镜像仓库地址
                hub = &amp;quot;registry.cn-shanghai.aliyuncs.com&amp;quot;
                // 设置仓库项目名
                project_name = &amp;quot;mydlq&amp;quot;
                echo &amp;quot;编译 Docker 镜像&amp;quot;
                docker.withRegistry(&amp;quot;http://${hub}&amp;quot;, &amp;quot;ffb3b544-108e-4851-b747-b8a00bfe7ee0&amp;quot;) {
                    echo &amp;quot;构建镜像&amp;quot;
                    // 设置推送到aliyun仓库的mydlq项目下，并用pom里面设置的项目名与版本号打标签
                    def customImage = docker.build(&amp;quot;${hub}/${project_name}/${pom.artifactId}:${pom.version}&amp;quot;)
                    echo &amp;quot;推送镜像&amp;quot;
                    customImage.push()
                    echo &amp;quot;删除镜像&amp;quot;
                    sh &amp;quot;docker rmi ${hub}/${project_name}/${pom.artifactId}:${pom.version}&amp;quot;
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、Helm 启动应用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 执行Helm的方法
def helmDeploy(Map args) {
    // Helm 初始化
    if(args.init){
        sh &amp;quot;helm init --client-only --stable-repo-url ${args.url}&amp;quot;
    }
    // Helm 尝试部署
    else if (args.dry_run) {
        println &amp;quot;尝试 Helm 部署，验证是否能正常部署&amp;quot;
        sh &amp;quot;helm upgrade --install ${args.name} --namespace ${args.namespace} -f values.yaml --set ${args.repository},${args.tag} stable/${args.template} --dry-run --debug&amp;quot;
    }
    // Helm 正式部署
    else {
        println &amp;quot;正式 Helm 部署&amp;quot;
        sh &amp;quot;helm upgrade --install ${args.name} --namespace ${args.namespace} -f values.yaml --set ${args.repository},${args.tag} stable/${args.template}&amp;quot;
    }
}

// 方法调用
stage() {
    echo &amp;quot;Helm 初始化 http://chart.mydlq.club&amp;quot;
    helmDeploy(init: true ,url: &amp;quot;Helm 仓库地址&amp;quot;);
    echo &amp;quot;Helm 尝试执行部署&amp;quot;
    helmDeploy(init: false ,dry: true ,name: &amp;quot;应用名&amp;quot; ,namespace: &amp;quot;应用启动的Namespace&amp;quot; ,image: &amp;quot;镜像名&amp;quot;,tag: &amp;quot;镜像标签&amp;quot; ,template: &amp;quot;选用的chart模板&amp;quot;)
    echo &amp;quot;Helm 正式执行部署&amp;quot;
    helmDeploy(init: false ,dry: false ,name: &amp;quot;应用名&amp;quot; ,namespace: &amp;quot;应用启动的Namespace&amp;quot; ,image: &amp;quot;镜像名&amp;quot;,tag: &amp;quot;镜像标签&amp;quot; ,template: &amp;quot;选用的chart模板&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;运行结果和后续&#34;&gt;运行结果和后续&lt;/h3&gt;

&lt;p&gt;上面的 Helm步骤执行完成后，就可以进行简单测试了，其中此项目引用的chart是一个简单的 SpringBoot 项目，其中用 NodePort 方式暴露了两个端口，30080 &amp;amp; 30081，分别对应8080、8081俩个端口，切提供了一个 Hello World 接口为“/hello”，所以我们这里访问一下这个接口地址：&lt;a href=&#34;http://192.168.2.11:30080/hello&#34;&gt;http://192.168.2.11:30080/hello&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在实现基本功能的同时也会新增和完善整个流程，比如在脚本中设置每个阶段超时时间，设置相关的邮件通知等，完整的jenkinsfile如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 执行Helm的方法
def helmDeploy(Map args) {
    if(args.init){
        println &amp;quot;Helm 初始化&amp;quot;
        sh &amp;quot;helm init --client-only --stable-repo-url ${args.url}&amp;quot;
    } else if (args.dry_run) {
        println &amp;quot;尝试 Helm 部署，验证是否能正常部署&amp;quot;
        sh &amp;quot;helm upgrade --install ${args.name} --namespace ${args.namespace} ${args.values} --set ${args.image},${args.tag} stable/${args.template} --dry-run --debug&amp;quot;
    } else {
        println &amp;quot;正式 Helm 部署&amp;quot;
        sh &amp;quot;helm upgrade --install ${args.name} --namespace ${args.namespace} ${args.values} --set ${args.image},${args.tag} stable/${args.template}&amp;quot;
    }
}

// jenkins slave 执行流水线任务
timeout(time: 600, unit: &#39;SECONDS&#39;) {
    try{
        def label = &amp;quot;jnlp-agent&amp;quot;
        podTemplate(label: label,cloud: &#39;kubernetes&#39; ){
            node (label) {
                stage(&#39;Git阶段&#39;){
                    echo &amp;quot;Git 阶段&amp;quot;
                    git branch: &amp;quot;master&amp;quot; ,changelog: true , url: &amp;quot;https://github.com/my-dlq/springboot-helloworld.git&amp;quot;
                }
                stage(&#39;Maven阶段&#39;){
                    echo &amp;quot;Maven 阶段&amp;quot;
                    container(&#39;maven&#39;) {
                        //这里引用上面设置的全局的 settings.xml 文件，根据其ID将其引入并创建该文件
                        configFileProvider([configFile(fileId: &amp;quot;75884c5a-4ec2-4dc0-8d87-58b6b1636f8a&amp;quot;, targetLocation: &amp;quot;settings.xml&amp;quot;)]){
                            sh &amp;quot;mvn clean install -Dmaven.test.skip=true --settings settings.xml&amp;quot;
                        }
                    }
                }
                stage(&#39;Docker阶段&#39;){
                    echo &amp;quot;Docker 阶段&amp;quot;
                    container(&#39;docker&#39;) {
                        // 读取pom参数
                        echo &amp;quot;读取 pom.xml 参数&amp;quot;
                        pom = readMavenPom file: &#39;./pom.xml&#39;
                        // 设置镜像仓库地址
                        hub = &amp;quot;registry.cn-shanghai.aliyuncs.com&amp;quot;
                        // 设置仓库项目名
                        project_name = &amp;quot;mydlq&amp;quot;
                        echo &amp;quot;编译 Docker 镜像&amp;quot;
                        docker.withRegistry(&amp;quot;http://${hub}&amp;quot;, &amp;quot;ffb3b544-108e-4851-b747-b8a00bfe7ee0&amp;quot;) {
                            echo &amp;quot;构建镜像&amp;quot;
                            // 设置推送到aliyun仓库的mydlq项目下，并用pom里面设置的项目名与版本号打标签
                            def customImage = docker.build(&amp;quot;${hub}/${project_name}/${pom.artifactId}:${pom.version}&amp;quot;)
                            echo &amp;quot;推送镜像&amp;quot;
                            customImage.push()
                            echo &amp;quot;删除镜像&amp;quot;
                            sh &amp;quot;docker rmi ${hub}/${project_name}/${pom.artifactId}:${pom.version}&amp;quot;
                        }
                    }
                }
                stage(&#39;Helm阶段&#39;){
                    container(&#39;helm-kubectl&#39;) {
                        withKubeConfig([credentialsId: &amp;quot;8510eda6-e1c7-4535-81af-17626b9575f7&amp;quot;,serverUrl: &amp;quot;https://kubernetes.default.svc.cluster.local&amp;quot;]) {
                            // 设置参数
                            image = &amp;quot;image.repository=${hub}/${project_name}/${pom.artifactId}&amp;quot;
                            tag = &amp;quot;image.tag=${pom.version}&amp;quot;
                            template = &amp;quot;spring-boot&amp;quot;
                            repo_url = &amp;quot;http://chart.mydlq.club&amp;quot;
                            app_name = &amp;quot;${pom.artifactId}&amp;quot;
                            // 检测是否存在yaml文件
                            def values = &amp;quot;&amp;quot;
                            if (fileExists(&#39;values.yaml&#39;)) {
                                values = &amp;quot;-f values.yaml&amp;quot;
                            }
                            // 执行 Helm 方法
                            echo &amp;quot;Helm 初始化&amp;quot;
                            helmDeploy(init: true ,url: &amp;quot;${repo_url}&amp;quot;);
                            echo &amp;quot;Helm 执行部署测试&amp;quot;
                            helmDeploy(init: false ,dry_run: true ,name: &amp;quot;${app_name}&amp;quot; ,namespace: &amp;quot;mydlqcloud&amp;quot; ,image: &amp;quot;${image}&amp;quot; ,tag: &amp;quot;${tag}&amp;quot; , values: &amp;quot;${values}&amp;quot; ,template: &amp;quot;${template}&amp;quot;)
                            echo &amp;quot;Helm 执行正式部署&amp;quot;
                            helmDeploy(init: false ,dry_run: false ,name: &amp;quot;${app_name}&amp;quot; ,namespace: &amp;quot;mydlqcloud&amp;quot;,image: &amp;quot;${image}&amp;quot; ,tag: &amp;quot;${tag}&amp;quot; , values: &amp;quot;${values}&amp;quot; ,template: &amp;quot;${template}&amp;quot;)
                        }
                    }
                }
            }
        }
    }catch(Exception e) {
        currentBuild.result = &amp;quot;FAILURE&amp;quot;
    }finally {
        // 获取执行状态
        def currResult = currentBuild.result ?: &#39;SUCCESS&#39;
        // 判断执行任务状态，根据不同状态发送邮件
        stage(&#39;email&#39;){
            if (currResult == &#39;SUCCESS&#39;) {
                echo &amp;quot;发送成功邮件&amp;quot;
                emailext(subject: &#39;任务执行成功&#39;,to: &#39;32******7@qq.com&#39;,body: &#39;&#39;&#39;任务已经成功构建完成...&#39;&#39;&#39;)
            }else {
                echo &amp;quot;发送失败邮件&amp;quot;
                emailext(subject: &#39;任务执行失败&#39;,to: &#39;32******7@qq.com&#39;,body: &#39;&#39;&#39;任务执行失败构建失败...&#39;&#39;&#39;)
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Gc</title>
          <link>https://kingjcy.github.io/post/golang/go-gc/</link>
          <pubDate>Tue, 27 Aug 2019 16:26:15 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-gc/</guid>
          <description>&lt;p&gt;以下是Golang GC算法的里程碑：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;v1.1 STW（停止所有运行时）
v1.3 Mark（标记） STW（停止所有运行时）, Sweep（清除） 并行
v1.5 三色标记法
v1.8 三色标记法 + hybrid write barrier（混合屏障）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经典的GC算法有三种：引用计数(reference counting)、标记-清扫(mark &amp;amp; sweep)、复制收集(Copy and Collection)。
golang时基于标记-清扫(mark &amp;amp; sweep)的基础上进行改进的gc方法。&lt;/p&gt;

&lt;h1 id=&#34;引用计数法&#34;&gt;引用计数法&lt;/h1&gt;

&lt;p&gt;原理是在每个对象内部维护一个整数值，叫做这个对象的引用计数，当对象被引用时引用计数加一，当对象不被引用时引用计数减一。当引用计数为 0 时，自动销毁对象。&lt;/p&gt;

&lt;p&gt;目前引用计数法主要用在 c++ 标准库的 std::shared_ptr 、微软的 COM 、Objective-C 和 PHP 中。&lt;/p&gt;

&lt;p&gt;但是引用计数法有个缺陷就是不能解决循环引用的问题。循环引用是指对象 A 和对象 B 互相持有对方的引用。这样两个对象的引用计数都不是 0 ，因此永远不能被收集。&lt;/p&gt;

&lt;p&gt;另外的缺陷是，每次对象的赋值都要将引用计数加一，增加了消耗。&lt;/p&gt;

&lt;h1 id=&#34;标记-清扫-mark-sweep&#34;&gt;标记-清扫(mark &amp;amp; sweep)&lt;/h1&gt;

&lt;p&gt;此算法主要有两个主要的步骤：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;第一步，找出不可达的对象，然后做上标记。
第二步，回收标记好的对象。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;操作非常简单，但是有一点需要额外注意： mark and sweep 算法在执行的时候，需要程序暂停即 stop the world 。&lt;/p&gt;

&lt;p&gt;也就是说，这段时间程序会阻塞在gc的时候。&lt;/p&gt;

&lt;p&gt;我们来看一下图解：&lt;/p&gt;

&lt;p&gt;开始标记，程序暂停。程序和对象的此时关系是这样的：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后开始标记，process找出它所有可达的对象，并做上标记。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;标记完了之后，然后开始清除未标记的对象：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后 垃圾 清除了，变成了下图这样。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最后，停止暂停，让程序继续跑。然后循环重复这个过程，直到 process 生命周期结束。&lt;/p&gt;

&lt;p&gt;标记-清扫(Mark And Sweep)算法存在什么问题？&lt;/p&gt;

&lt;p&gt;标记-清扫(Mark And Sweep)算法 这种算法虽然非常的简单，但是还存在一些问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;STW，stop the world；让程序暂停，程序出现卡顿。&lt;/li&gt;
&lt;li&gt;标记需要扫描整个heap&lt;/li&gt;
&lt;li&gt;清除数据会产生heap碎片&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里面最重要的问题就是：mark-and-sweep 算法会暂停整个整个程序。&lt;/p&gt;

&lt;p&gt;Go是如何面对并这个问题的呢？三色并发标记法+混合屏障&lt;/p&gt;

&lt;h1 id=&#34;三色并发标记法&#34;&gt;三色并发标记法&lt;/h1&gt;

&lt;p&gt;这个算法可以实现 &amp;ldquo;on-the-fly&amp;rdquo;，也就是在程序执行的同时进行收集，并不需要暂停整个程序。&lt;/p&gt;

&lt;p&gt;我们先来看看Golang的三色标记法的大体流程。&lt;/p&gt;

&lt;p&gt;首先：程序创建的对象都标记为白色。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;gc开始：扫描所有可到达的对象，标记为灰色&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从灰色对象中找到其引用对象标记为灰色，把灰色对象本身标记为黑色&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;监视对象中的内存修改，并持续上一步的操作，直到灰色标记的对象不存在&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc8.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此时，gc回收白色对象。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最后，将所有黑色对象变为白色，并重复以上所有过程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;好了，大体的流程就是这样的。&lt;/p&gt;

&lt;p&gt;但是三色标记法是一定要依赖STW的. 因为如果不暂停程序, 程序的逻辑改变对象引用关系, 这种动作如果在标记阶段做了修改，会影响标记结果的正确性。&lt;/p&gt;

&lt;p&gt;比如&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc20.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc21.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc22.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc23.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出，有两个问题, 在三色标记法中,是不希望被发生的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;条件1: 一个白色对象被黑色对象引用(白色被挂在黑色下)
条件2: 灰色对象与它之间的可达关系的白色对象遭到破坏(灰色同时丢了该白色)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当以上两个条件同时满足时, 就会出现对象丢失现象!&lt;/p&gt;

&lt;p&gt;为了防止这种现象的发生，最简单的方式就是STW，直接禁止掉其他用户程序对对象引用关系的干扰，但是STW的过程有明显的资源浪费，对所有的用户程序都有很大影响，如何能在保证对象不丢失的情况下合理的尽可能的提高GC效率，减少STW时间呢？&lt;/p&gt;

&lt;p&gt;答案就是, 那么我们只要使用一个机制,来破坏上面的两个条件就可以了.也就是引入来屏障机制&lt;/p&gt;

&lt;h1 id=&#34;屏障机制&#34;&gt;屏障机制&lt;/h1&gt;

&lt;p&gt;我们让GC回收器,满足下面两种情况之一时,可保对象不丢失. 所以引出两种方式.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“强-弱” 三色不变式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;不存在黑色对象引用到白色对象的指针。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc25.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;所有被黑色对象引用的白色对象都处于灰色保护状态.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc26.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;为了遵循上述的两个方式,Golang团队初步得到了如下具体的两种屏障方式“插入屏障”, “删除屏障”.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;插入屏障&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;具体操作: 在A对象引用B对象的时候，B对象被标记为灰色。(将B挂在A下游，B必须被标记为灰色)&lt;/p&gt;

&lt;p&gt;满足: 强三色不变式. (不存在黑色对象引用白色对象的情况了， 因为白色会强制变成灰色)&lt;/p&gt;

&lt;p&gt;伪码如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;添加下游对象(当前下游对象slot, 新下游对象ptr) {
  //1
  标记灰色(新下游对象ptr)

  //2
  当前下游对象slot = 新下游对象ptr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A.添加下游对象(nil, B)   //A 之前没有下游， 新添加一个下游对象B， B被标记为灰色
A.添加下游对象(C, B)     //A 将下游对象C 更换为B，  B被标记为灰色
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​
这段伪码逻辑就是写屏障,. 我们知道,黑色对象的内存槽有两种位置, 栈和堆. 栈空间的特点是容量小,但是要求相应速度快,因为函数调用弹出频繁使用, 所以“插入屏障”机制,在栈空间的对象操作中不使用. 而仅仅使用在堆空间对象的操作中.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;删除屏障&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;具体操作: 被删除的对象，如果自身为灰色或者白色，那么被标记为灰色。&lt;/p&gt;

&lt;p&gt;满足: 弱三色不变式. (保护灰色对象到白色对象的路径不会断)&lt;/p&gt;

&lt;p&gt;伪代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;添加下游对象(当前下游对象slot， 新下游对象ptr) {
  //1
  if (当前下游对象slot是灰色 || 当前下游对象slot是白色) {
          标记灰色(当前下游对象slot)     //slot为被删除对象， 标记为灰色
  }

  //2
  当前下游对象slot = 新下游对象ptr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A.添加下游对象(B, nil)   //A对象，删除B对象的引用。  B被A删除，被标记为灰(如果B之前为白)
A.添加下游对象(B, C)         //A对象，更换下游B变成C。   B被A删除，被标记为灰(如果B之前为白)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我们回到刚才的问题：Go是如何解决 标记-清除(mark and sweep) 算法中的卡顿(stw，stop the world)问题的呢？就是gc和用户逻辑如何并行操作，减少stw的操作&lt;/p&gt;

&lt;p&gt;标记-清除(mark and sweep)算法的STW(stop the world)操作，就是runtime把所有的线程全部冻结掉，所有的线程全部冻结意味着用户逻辑是暂停的。这样所有的对象都不会被修改了，这时候去扫描是绝对安全的。&lt;/p&gt;

&lt;p&gt;Go如何减短这个过程呢？标记-清除(mark and sweep)算法包含两部分逻辑：标记和清除。&lt;/p&gt;

&lt;p&gt;我们知道Golang三色标记法中最后只剩下的黑白两种对象，黑色对象是程序恢复后接着使用的对象，如果不碰触黑色对象，只清除白色的对象，还有删除保障，肯定不会影响程序逻辑。所以： 清除操作和用户逻辑可以并发。&lt;/p&gt;

&lt;p&gt;标记操作和用户逻辑也是并发的，用户逻辑会时常生成对象或者改变对象的引用，就是使用写入保障，标记和用户逻辑就可以并发。&lt;/p&gt;

&lt;p&gt;比如&lt;/p&gt;

&lt;p&gt;process新生成对象的时候，GC该如何操作呢？不会乱吗？&lt;/p&gt;

&lt;p&gt;我们看如下图，在此状态下：process程序又新生成了一个对象，我们设想会变成这样：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc11.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但是这样显然是不对的，因为按照三色标记法的步骤，这样新生成的对象A最后会被清除掉，这样会影响程序逻辑。&lt;/p&gt;

&lt;p&gt;Golang为了解决这个问题，引入了 写屏障 这个机制。&lt;/p&gt;

&lt;p&gt;写屏障：该屏障之前的写操作和之后的写操作相比，先被系统其它组件感知。&lt;/p&gt;

&lt;p&gt;通俗的讲：就是在gc跑的过程中，可以监控对象的内存修改，并对对象进行重新标记。(实际上也是超短暂的stw，然后对对象进行标记)&lt;/p&gt;

&lt;p&gt;在上述情况中， 新生成的对象，一律都标位灰色！&lt;/p&gt;

&lt;p&gt;即下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;那么，灰色或者黑色对象的引用改为白色对象的时候，Golang是该如何操作的？&lt;/p&gt;

&lt;p&gt;看如下图，一个黑色对象引用了曾经标记的白色对象。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc13.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这时候，写屏障机制被触发，向GC发送信号，GC重新扫描对象并标位灰色。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/gc/gc14.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;因此，gc一旦开始，无论是创建对象还是对象的引用改变，都会先变为灰色。&lt;/p&gt;

&lt;h1 id=&#34;触发时机&#34;&gt;触发时机&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;gcTriggerAlways: 强制触发GC，没找到什么情况下使用这个&lt;/li&gt;
&lt;li&gt;gcTriggerHeap: 当前分配的内存达到一定值（动态计算）就触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerTime: 当一定时间（2分钟）没有执行过GC就触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerCycle: 要求启动新一轮的GC, 已启动则跳过, 手动触发GC的runtime.GC()会使用这个条件,我们经常在代码中会使用这个来触发强制gc。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;调试&#34;&gt;调试&lt;/h1&gt;

&lt;p&gt;GODEBUG=gctrace=1 在运行二进制文件的时候加上这个环境变量可以直接在终端查看相关gc信息。如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gc 45 @37.801s 11%: 0.19+627+0.29 ms clock, 0.38+424/621/0+0.59 ms cpu, 356-&amp;gt;415-&amp;gt;225 MB, 453 MB goal, 4 P
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;gc 45：表示第45次GC，共有4个P (线程)参与GC。&lt;/li&gt;
&lt;li&gt;@37.801s：表示程序执行的总时间&lt;/li&gt;
&lt;li&gt;11%: 表示gc 占时间比。&lt;/li&gt;
&lt;li&gt;0.19+627+0.29 us clock：STW（stop-the-world）0.19ms, 并发标记和扫描的时间627ms, STW标记的时间0.29ms。表示第一次STW + 标记(Marking) + 第二次STW的时钟时间，单位是ms。比如0.006+39+0.004 ms clock，表示第一次STW持续的时间时钟时间是0.006ms，第二次STW持续的时钟时间是0.004ms，标记Marking处理持续的时钟时间是39ms；&lt;/li&gt;
&lt;li&gt;0.38+424/621/0+0.59 ms cpu, 表示垃圾回收占用cpu时间。表示第一次STW + Mark assist/Mark(Dedicated + Fractional)/Mark(Idle) + 第二次STW的CPU时间。与时钟时间的统计不同，CPU时间会对各个核上对应的处理时间进行累加。比如0.006+&lt;sup&gt;36&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;.&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;0&lt;/sub&gt;+0.004 ms cpu，0.006ms表示第一次STW过程中，被STW的多个核的时钟时间之和，其值大于等于对应时钟时间。36ms表示在整个Mark过程中，进行assist Mark的CPU累计时间。2.2ms表示在整个Mark过程中，在gcMarkWorkerDedicatedMode和gcMarkWorkerFractionalMode两种工作模式下进行Mark处理的CPU累计时间之和。0.8ms表示在gcMarkWorkerIdleMode模式下进行Mark处理的CPU累计时间之和。0.004ms表示第二次被STW的多个核的时钟时间之和；&lt;/li&gt;
&lt;li&gt;356-&amp;gt;415-&amp;gt;225 MB, 453 MB goal,表示堆的大小，gc后堆的大小，存活堆的大小。表示GC开始前申请的内存大小 -&amp;gt; GC标记(Mark)结束后申请的内存大小 -&amp;gt; 被标记存活的内存大小。比如420-&amp;gt;435-&amp;gt;210 MB，表示GC开始前一共申请了420MB的内存，GC标记(Mark)处理完后一共申请了435MB的内存，也就说在整个标记阶段，又新申请了15MB的内存，标记阶段一共标记了210MB的内存，就是说有435MB-210MB=225MB的内存可以被回收；&lt;/li&gt;
&lt;li&gt;453 MB goal 表示整体堆的大小为435M。目标堆大小，也就是下一次GC的大小&lt;/li&gt;
&lt;li&gt;4 P：占用核个数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据官方描述，golang1.0 的gc 可以降到100ms 以内，但是这里gc 都超过1s了，这明显是不可以接受的，说明gc 是有很大异常的。&lt;/p&gt;

&lt;p&gt;检查思路，首先利用pprof 打出整个调用过程累计的堆分配图，查出到底是哪些模块堆分配异常。通过代码内嵌pprof 暴露端口的方式，终端输出svg。&lt;/p&gt;

&lt;p&gt;然后终端输入：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go tool pprof -alloc_space -cum -svg http://127.0.0.1:8080/debug/pprof/heap &amp;gt; heap.svg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以找到对应的资源消耗所在了。&lt;/p&gt;

&lt;p&gt;Golang 是 runtime 时定期或满足条件时并行执行回收器，也可以在代码中显示调用 runtime.GC()，显示调用会阻塞整个程序直到垃圾回收完成；&lt;/p&gt;

&lt;h1 id=&#34;gc调优&#34;&gt;gc调优&lt;/h1&gt;

&lt;p&gt;Go 的 GC 被设计为极致简洁，与较为成熟的 Java GC 的数十个可控参数相比，严格意义上来讲，Go 可供用户调整的参数只有 GOGC 环境变量，所以我们优化的方向只有&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;减少用户代码对 GC 产生的压力，这一方面包含了减少用户代码分配内存的数量（即对程序的代码行为进行调优）&lt;/li&gt;
&lt;li&gt;最小化 Go 的 GC 对 CPU 的使用率（即调整 GOGC）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从这两点来看，所谓 GC 调优的核心思想也就是充分的围绕上面的两点来展开：优化内存的申请速度，尽可能的少申请内存，复用已申请的内存。或者简单来说，不外乎这三个关键字：&lt;strong&gt;控制、减少、复用&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;合理化内存分配-提高cpu-利用率&#34;&gt;合理化内存分配、提高CPU 利用率&lt;/h2&gt;

&lt;h3 id=&#34;合理化内存分配的速度&#34;&gt;合理化内存分配的速度&lt;/h3&gt;

&lt;p&gt;我们来看这样一个例子。在这个例子中，concat 函数负责拼接一些长度不确定的字符串。并且为了快速完成任务，出于某种原因，在两个嵌套的 for 循环中一口气创建了 800 个 goroutine。在 main 函数中，启动了一个 goroutine 并在程序结束前不断的触发 GC，并尝试输出 GC 的平均执行时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;runtime&amp;quot;
    &amp;quot;runtime/trace&amp;quot;
    &amp;quot;sync/atomic&amp;quot;
    &amp;quot;time&amp;quot;
)
var (
    stop  int32
    count int64
    sum   time.Duration
)
func concat() {
    for n := 0; n &amp;lt; 100; n++ {
        for i := 0; i &amp;lt; 8; i++ {
            go func() {
                s := &amp;quot;Go GC&amp;quot;
                s += &amp;quot; &amp;quot; + &amp;quot;Hello&amp;quot;
                s += &amp;quot; &amp;quot; + &amp;quot;World&amp;quot;
                _ = s
            }()
        }
    }
}
func main() {
    f, _ := os.Create(&amp;quot;trace.out&amp;quot;)
    defer f.Close()
    trace.Start(f)
    defer trace.Stop()
    go func() {
        var t time.Time
        for atomic.LoadInt32(&amp;amp;stop) == 0 {
            t = time.Now()
            runtime.GC()
            sum += time.Since(t)
            count++
        }
        fmt.Printf(&amp;quot;GC spend avg: %v\n&amp;quot;, time.Duration(int64(sum)/count))
    }()
    concat()
    atomic.StoreInt32(&amp;amp;stop, 1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个程序的执行结果是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build -o main
$ ./main
GC spend avg: 2.583421ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GC 平均执行一次需要长达 2ms 的时间，goroutine 的执行时间占其生命周期总时间非常短的一部分，但大部分时间都花费在调度器的等待上了，说明同时创建大量 goroutine 对调度器产生的压力确实不小，我们不妨将这一产生速率减慢，一批一批地创建 goroutine：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func concat() {
    wg := sync.WaitGroup{}
    for n := 0; n &amp;lt; 100; n++ {
        wg.Add(8)
        for i := 0; i &amp;lt; 8; i++ {
            go func() {
                s := &amp;quot;Go GC&amp;quot;
                s += &amp;quot; &amp;quot; + &amp;quot;Hello&amp;quot;
                s += &amp;quot; &amp;quot; + &amp;quot;World&amp;quot;
                _ = s
                wg.Done()
            }()
        }
        wg.Wait()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候我们再来看：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build -o main
$ ./main
GC spend avg: 328.54µs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GC 的平均时间就降到 300 微秒了。这时的赋值器 CPU 使用率也提高到了 60%，相对来说就很可观了。&lt;/p&gt;

&lt;h3 id=&#34;降低并复用已经申请的内存&#34;&gt;降低并复用已经申请的内存&lt;/h3&gt;

&lt;p&gt;我们通过一个非常简单的 Web 程序来说明复用内存的重要性。在这个程序中，每当产生一个 /example2的请求时，都会创建一段内存，并用于进行一些后续的工作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net/http&amp;quot;
    _ &amp;quot;net/http/pprof&amp;quot;
)
func newBuf() []byte {
    return make([]byte, 10&amp;lt;&amp;lt;20)
}
func main() {
    go func() {
        http.ListenAndServe(&amp;quot;localhost:6060&amp;quot;, nil)
    }()
    http.HandleFunc(&amp;quot;/example2&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
        b := newBuf()
        // 模拟执行一些工作
        for idx := range b {
            b[idx] = 1
        }
        fmt.Fprintf(w, &amp;quot;done, %v&amp;quot;, r.URL.Path[1:])
    })
    http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候我们使用一个压测工具 ab，来同时产生 500 个请求（-n 一共 500 个请求，-c 一个时刻执行请求的数量，每次 100 个并发请求），GC 反复被触发，一个显而易见的原因就是内存分配过多。我们可以通过 go tool pprof 来查看究竟是谁分配了大量内存（使用 web 指令来使用浏览器打开统计信息的可视化图形），可见 newBuf 产生的申请的内存过多，现在我们使用 sync.Pool 来复用 newBuf 所产生的对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net/http&amp;quot;
    _ &amp;quot;net/http/pprof&amp;quot;
    &amp;quot;sync&amp;quot;
)
// 使用 sync.Pool 复用需要的 buf
var bufPool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 10&amp;lt;&amp;lt;20)
    },
}
func main() {
    go func() {
        http.ListenAndServe(&amp;quot;localhost:6060&amp;quot;, nil)
    }()
    http.HandleFunc(&amp;quot;/example2&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
        b := bufPool.Get().([]byte)
        for idx := range b {
            b[idx] = 0
        }
        fmt.Fprintf(w, &amp;quot;done, %v&amp;quot;, r.URL.Path[1:])
        bufPool.Put(b)
    })
    http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但从 Requests per second 每秒请求数来看，从原来的 506.63 变为 1171.32 得到了近乎一倍的提升。从 trace 的结果来看，GC 也没有频繁的被触发从而长期消耗 CPU 使用率。&lt;/p&gt;

&lt;h2 id=&#34;调整-gogc&#34;&gt;调整 GOGC&lt;/h2&gt;

&lt;p&gt;我们已经知道了 GC 的触发原则是由步调算法来控制的，其关键在于估计下一次需要触发 GC 时，堆的大小。可想而知，如果我们在遇到海量请求的时，为了避免 GC 频繁触发，是否可以通过将 GOGC 的值设置得更大，让 GC 触发的时间变得更晚，从而减少其触发频率，进而增加用户代码对机器的使用率。&lt;/p&gt;

&lt;p&gt;GOGC代表了占用中的内存增长比率，达到该比率时应当触发1次GC，该参数可以通过环境变量设置。GOGC参数取值为整数，默认值是100，单位是百分比。假如当前heap占用内存为4MB，GOGC = 75，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4 * (1+75%) = 7MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等heap占用内存大小达到7MB时会触发1轮GC。GOGC还有2个特殊值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“off” : 代表关闭GC&lt;/li&gt;
&lt;li&gt;0 : 代表持续进行垃圾回收，只用于调试&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以非常简单粗暴的将 GOGC 调整为 1000，来执行上一个例子中未复用对象之前的程序：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOGC=1000 ./main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然我们也可以直接设置环境变量为1000，可以测试，压测的结果得到了一定幅度的改善（Requests per second 从原来的 506.63 提高为了 541.61），并且 GC 的执行频率明显降低&lt;/p&gt;

&lt;p&gt;在实际实践中可表现为需要紧急处理一些由 GC 带来的瓶颈时，人为将 GOGC 调大，加钱加内存，扛过这一段峰值流量时期。&lt;/p&gt;

&lt;p&gt;当然，这种做法其实是治标不治本，并没有从根本上解决内存分配过于频繁的问题，极端情况下，反而会由于 GOGC 太大而导致回收不及时而耗费更多的时间来清理产生的垃圾，这对时间不算敏感的应用还好，但对实时性要求较高的程序来说就是致命的打击了。&lt;/p&gt;

&lt;p&gt;如果程序一段时间内驻留内存飙升，并且GC没法立马把这些内存给回收，那么会导致下一次GC的阈值上去，压根没法触发GC，程序肯定会OOM的。这个很显而易见的问题，我都能想到，golang应该不会考虑不到啊，果然在golang 的blog里面&lt;a href=&#34;https://blog.golang.org/ismmkeynote&#34;&gt;https://blog.golang.org/ismmkeynote&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;目前官方针对这一个问题还没有明确的解决方案，因此这时更妥当的做法仍然是，定位问题的所在，并从代码层面上进行优化。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控trace系列---- jaeger</title>
          <link>https://kingjcy.github.io/post/monitor/trace/jaeger/</link>
          <pubDate>Tue, 13 Aug 2019 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/trace/jaeger/</guid>
          <description>&lt;p&gt;Jaeger 是 Uber 开源的分布式追踪系统，兼容 OpenTracing 标准，于 2017 年 9 月加入 CNCF 基金会。&lt;/p&gt;

&lt;h1 id=&#34;jaeger&#34;&gt;jaeger&lt;/h1&gt;

&lt;p&gt;受Dapper和OpenZipkin启发的Jaeger是由Uber Technologies作为开源发布的分布式跟踪系统。它用于监视和诊断基于微服务的分布式系统，包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;分布式上下文传播&lt;/li&gt;
&lt;li&gt;分布式交易监控&lt;/li&gt;
&lt;li&gt;根本原因分析&lt;/li&gt;
&lt;li&gt;服务依赖性分析性能/延迟优化&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;特性&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;高扩展性&lt;/p&gt;

&lt;p&gt;Jaeger后端的设计没有单点故障，可以根据业务需求进行扩展。例如，Uber上任何给定的Jaeger安装通常每天要处理数十亿个跨度。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;原生支持OpenTracing&lt;/p&gt;

&lt;p&gt;Jaeger后端，Web UI和工具库已完全设计为支持OpenTracing标准。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过跨度引用将迹线表示为有向无环图（不仅是树）&lt;/li&gt;
&lt;li&gt;支持强类型的跨度标签和结构化日志通过行李&lt;/li&gt;
&lt;li&gt;支持通用的分布式上下文传播机制&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多存储后端&lt;/p&gt;

&lt;p&gt;Jaeger支持两个流行的开源NoSQL数据库作为跟踪存储后端：Cassandra 3.4+和Elasticsearch 5.x / 6.x / 7.x。正在进行使用其他数据库的社区实验，例如ScyllaDB，InfluxDB，Amazon DynamoDB。Jaeger还附带了一个简单的内存存储区，用于测试设置。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;现代化的UI&lt;/p&gt;

&lt;p&gt;Jaeger Web UI是使用流行的开源框架（如React）以Javascript实现的。v1.0中发布了几项性能改进，以允许UI有效处理大量数据，并显示具有成千上万个跨度的跟踪（例如，我们尝试了具有80,000个跨度的跟踪）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;云原生部署&lt;/p&gt;

&lt;p&gt;Jaeger后端作为Docker映像的集合进行分发。这些二进制文件支持各种配置方法，包括命令行选项，环境变量和多种格式（yaml，toml等）的配置文件。Kubernetes模板和Helm图表有助于将其部署到Kubernetes集群。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可观察性&lt;/p&gt;

&lt;p&gt;默认情况下，所有Jaeger后端组件都公开Prometheus指标（也支持其他指标后端）。使用结构化日志库zap将日志写到标准输出。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安全&lt;/p&gt;

&lt;p&gt;Jaeger的第三方安全审核可在&lt;a href=&#34;https://github.com/jaegertracing/security-audits&#34;&gt;https://github.com/jaegertracing/security-audits&lt;/a&gt; 中获得。有关Jaeger中可用安全机制的摘要，请参见问题＃1718。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;与Zipkin的向后兼容性&lt;/p&gt;

&lt;p&gt;尽管我们建议使用OpenTracing API来对应用程序进行检测并绑定到Jaeger客户端库，以从其他地方无法获得的高级功能中受益，但是如果您的组织已经使用Zipkin库对检测进行了投资，则不必重写所有代码。Jaeger通过在HTTP上接受Zipkin格式（Thrift或JSON v1 / v2）的跨度来提供与Zipkin的向后兼容性。从Zipkin后端切换只是将流量从Zipkin库路由到Jaeger后端的问题。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装部署&#34;&gt;安装部署&lt;/h2&gt;

&lt;p&gt;开始多合一的最简单方法是使用发布到DockerHub的预构建映像（单个命令行）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.14
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or run the jaeger-all-in-one(.exe) executable from the binary distribution archives:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jaeger-all-in-one --collector.zipkin.http-port=9411
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then navigate to &lt;a href=&#34;http://localhost:16686&#34;&gt;http://localhost:16686&lt;/a&gt; to access the Jaeger UI.&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/jaeger&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;整体可以分为四个部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;jaeger-client：Jaeger 的客户端，实现了 OpenTracing 的 API，支持主流编程语言。客户端直接集成在目标 Application 中，其作用是记录和发送 Span 到 Jaeger Agent。在 Application 中调用 Jaeger Client Library 记录 Span 的过程通常被称为埋点。&lt;/li&gt;
&lt;li&gt;jaeger-agent：暂存 Jaeger Client 发来的 Span，并批量向 Jaeger Collector 发送 Span，一般每台机器上都会部署一个 Jaeger Agent。官方的介绍中还强调了 Jaeger Agent 可以将服务发现的功能从 Client 中抽离出来，不过从架构角度讲，如果是部署在 Kubernetes 或者是 Nomad 中，Jaeger Agent 存在的意义并不大。&lt;/li&gt;
&lt;li&gt;jaeger-collector：接受 Jaeger Agent 发来的数据，并将其写入存储后端，目前支持采用 Cassandra 和 Elasticsearch 作为存储后端。个人还是比较推荐用 Elasticsearch，既可以和日志服务共用同一个 ES，又可以使用 Kibana 对 Trace 数据进行额外的分析。架构图中的存储后端是 Cassandra，旁边还有一个 Spark，讲的就是可以用 Spark 等其他工具对存储后端中的 Span 进行直接分析。&lt;/li&gt;
&lt;li&gt;jaeger-query &amp;amp; jaeger-ui：读取存储后端中的数据，以直观的形式呈现。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控trace系列---- zipkin</title>
          <link>https://kingjcy.github.io/post/monitor/trace/zipkin/</link>
          <pubDate>Tue, 13 Aug 2019 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/trace/zipkin/</guid>
          <description>&lt;p&gt;zipkin是分布式链路调用监控系统，聚合各业务系统调用延迟数据，达到链路调用监控跟踪。&lt;/p&gt;

&lt;h1 id=&#34;zipkin&#34;&gt;zipkin&lt;/h1&gt;

&lt;p&gt;一个独立的分布式追踪系统，客户端存在于应用中（即各服务中），应具备追踪信息生成、采集发送等功能，而服务端应该包含以下基本的三个功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;信息收集：用来收集各服务端采集的信息，并对这些信息进行梳理存储、建立索引。&lt;/li&gt;
&lt;li&gt;数据存储：存储追踪数据。&lt;/li&gt;
&lt;li&gt;查询服务：提供查询请求链路信息的接口。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装使用&#34;&gt;安装使用&lt;/h2&gt;

&lt;p&gt;下载jar包，直接运行。简单粗暴，但要注意必须jdk1.8及以上。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget -O zipkin.jar &#39;https://search.maven.org/remote_content?g=io.zipkin.java&amp;amp;a=zipkin-server&amp;amp;v=LATEST&amp;amp;c=exec&#39;
java -jar zipkin.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动成功后，打开浏览器访问zipkin的webUI，输入&lt;a href=&#34;http://ip:9411/&#34;&gt;http://ip:9411/&lt;/a&gt; ,显示web界面。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/zipkin&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;zipkin(服务端)包含四个组件，分别是collector、storage、search、web UI。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;collector 就是信息收集器,作为一个守护进程，它会时刻等待客户端传递过来的追踪数据，对这些数据进行验证、存储以及创建查询需要的索引。&lt;/li&gt;
&lt;li&gt;storage  是存储组件。zipkin 默认直接将数据存在内存中，此外支持使用Cassandra、ElasticSearch 和 Mysql。&lt;/li&gt;
&lt;li&gt;search 是一个查询进程，它提供了简单的JSON API来供外部调用查询。&lt;/li&gt;
&lt;li&gt;web UI 是zipkin的服务端展示平台，主要调用search提供的接口，用图表将链路信息清晰地展示给开发人员。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;数据发送&#34;&gt;数据发送&lt;/h2&gt;

&lt;p&gt;一个 span 表示一次服务调用，那么追踪器必定是被服务调用发起的动作触发，生成基本信息，同时为了追踪服务提供方对其他服务的调用情况，便需要传递本次追踪链路的traceId和本次调用的span-id。服务提供方完成服务将结果响应给调用方时，需要根据调用发起时记录的时间戳与当前时间戳计算本次服务的持续时间进行记录，至此这次调用的追踪span完成，就可以发送给zipkin服务端了。但是需要注意的是，发送span给zipkin collector不得影响此次业务结果，其发送成功与否跟业务无关，因此这里需要采用异步的方式发送，防止追踪系统发送延迟与发送失败导致用户系统的延迟与中断。下图就表示了一次http请求调用的追踪流程（基于zipkin官网提供的流程图）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/zipkin1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出服务A请求服务B时先被追踪器拦截，记录tag信息、时间戳，同时将追踪标识添加进http header中传递给服务B，在服务B响应后，记录持续时间，最终采取异步的方式发送给zipkin收集器。span从被追踪的服务传送到Zipkin收集器有三种主要的传送方式：http、Kafka以及Scribe（Facebook开源的日志收集系统）。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Kafka Client</title>
          <link>https://kingjcy.github.io/post/middleware/mq/kafka-client/</link>
          <pubDate>Thu, 08 Aug 2019 15:49:11 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/kafka-client/</guid>
          <description>&lt;p&gt;Go Kafka客户端简单示例&lt;/p&gt;

&lt;h1 id=&#34;客户端&#34;&gt;客户端&lt;/h1&gt;

&lt;h2 id=&#34;生产者&#34;&gt;生产者&lt;/h2&gt;

&lt;h3 id=&#34;库&#34;&gt;库&lt;/h3&gt;

&lt;p&gt;1.sarama目前使用最多的golang的client&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/Shopify/sarama
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该库要求kafka版本在0.8及以上，支持kafka定义的high-level API和low-level API，但不支持常用的consumer自动rebalance和offset追踪，所以一般得结合cluster版本使用。&lt;/p&gt;

&lt;p&gt;2.sarama-cluster依赖库，弥补了上面了不足&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/bsm/sarama-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要kafka 0.9及以上版本&lt;/p&gt;

&lt;h3 id=&#34;生产模式&#34;&gt;生产模式&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;同步消息模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;github.com/Shopify/sarama&amp;quot;
    &amp;quot;time&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;os/signal&amp;quot;
    &amp;quot;sync&amp;quot;
)

var Address = []string{&amp;quot;10.130.138.164:9092&amp;quot;,&amp;quot;10.130.138.164:9093&amp;quot;,&amp;quot;10.130.138.164:9094&amp;quot;}

func main()  {
    syncProducer(Address)
    //asyncProducer1(Address)
}

//同步消息模式
func syncProducer(address []string)  {
    config := sarama.NewConfig()
    config.Producer.Return.Successes = true
    config.Producer.Timeout = 5 * time.Second
    p, err := sarama.NewSyncProducer(address, config)
    if err != nil {
        log.Printf(&amp;quot;sarama.NewSyncProducer err, message=%s \n&amp;quot;, err)
        return
    }
    defer p.Close()
    topic := &amp;quot;test&amp;quot;
    srcValue := &amp;quot;sync: this is a message. index=%d&amp;quot;
    for i:=0; i&amp;lt;10; i++ {
        value := fmt.Sprintf(srcValue, i)
        msg := &amp;amp;sarama.ProducerMessage{
            Topic:topic,
            Value:sarama.ByteEncoder(value),
        }
        part, offset, err := p.SendMessage(msg)
        if err != nil {
            log.Printf(&amp;quot;send message(%s) err=%s \n&amp;quot;, value, err)
        }else {
            fmt.Fprintf(os.Stdout, value + &amp;quot;发送成功，partition=%d, offset=%d \n&amp;quot;, part, offset)
        }
        time.Sleep(2*time.Second)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同步生产者发送消息，使用的不是channel，并且SendMessage方法有三个返回的值，分别为这条消息的被发送到了哪个partition，处于哪个offset，是否有error。&lt;/p&gt;

&lt;p&gt;也就是说，只有在消息成功的发送并写入了broker，才会有返回值。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;异步消息之Goroutines&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;异步消费者(Goroutines)：用不同的goroutine异步读取Successes和Errors channel&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func asyncProducer1(address []string)  {
    config := sarama.NewConfig()
    config.Producer.Return.Successes = true
    //config.Producer.Partitioner = 默认为message的hash
    p, err := sarama.NewAsyncProducer(address, config)
    if err != nil {
        log.Printf(&amp;quot;sarama.NewSyncProducer err, message=%s \n&amp;quot;, err)
        return
    }

    //Trap SIGINT to trigger a graceful shutdown.
    signals := make(chan os.Signal, 1)
    signal.Notify(signals, os.Interrupt)

    var wg sync.WaitGroup
    var enqueued, successes, errors int
    wg.Add(2) //2 goroutine

    // 发送成功message计数
    go func() {
        defer wg.Done()
        for range p.Successes() {
            successes++
        }
    }()

    // 发送失败计数
    go func() {
        defer wg.Done()
        for err := range p.Errors() {
            log.Printf(&amp;quot;%s 发送失败，err：%s\n&amp;quot;, err.Msg, err.Err)
            errors++
        }
    }()

    // 循环发送信息
    asrcValue := &amp;quot;async-goroutine: this is a message. index=%d&amp;quot;
    var i int
    Loop:
    for {
        i++
        value := fmt.Sprintf(asrcValue, i)
        msg := &amp;amp;sarama.ProducerMessage{
            Topic:&amp;quot;test&amp;quot;,
            Value:sarama.ByteEncoder(value),
        }
        select {
        case p.Input() &amp;lt;- msg: // 发送消息
            enqueued++
            fmt.Fprintln(os.Stdout, value)
        case &amp;lt;-signals: // 中断信号
            p.AsyncClose()
            break Loop
        }
        time.Sleep(2 * time.Second)
    }
    wg.Wait()

    fmt.Fprintf(os.Stdout, &amp;quot;发送数=%d，发送成功数=%d，发送失败数=%d \n&amp;quot;, enqueued, successes, errors)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;异步生产者使用channel接收（生产成功或失败）的消息，并且也通过channel来发送消息，这样做通常是性能最高的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;异步消息之Select&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;异步消费者(Select)：同一线程内，通过select同时发送消息 和 处理errors计数。该方式效率较低，如果有大量消息发送， 很容易导致success和errors的case无法执行，从而阻塞一定时间。&lt;/p&gt;

&lt;p&gt;当然可以通过设置config.Producer.Return.Successes=false;config.Producer.Return.Errors=false来解决&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func asyncProducer2(address []string)  {
    config := sarama.NewConfig()
    config.Producer.Return.Errors = true
    p, err := sarama.NewAsyncProducer(address, config)
    if err != nil {
        log.Printf(&amp;quot;sarama.NewSyncProducer err, message=%s \n&amp;quot;, err)
        return
    }

    //Trap SIGINT to trigger a graceful shutdown.
    signals := make(chan os.Signal, 1)
    signal.Notify(signals, os.Interrupt)

    var enqueued, successes, errors int
    asrcValue := &amp;quot;async-select: this is a message. index=%d&amp;quot;
    var i int
    Loop:
    for {
        i++
        value := fmt.Sprintf(asrcValue, i)
        msg := &amp;amp;sarama.ProducerMessage{
            Topic:&amp;quot;test&amp;quot;,
            Value:sarama.ByteEncoder(value),
        }
        select {
        case p.Input() &amp;lt;- msg:
            fmt.Fprintln(os.Stdout, value)
            enqueued++
        case &amp;lt;-p.Successes():
            successes++
        case err := &amp;lt;-p.Errors():
            log.Printf(&amp;quot;%s 发送失败，err：%s\n&amp;quot;, err.Msg, err.Err)
            errors++
        case &amp;lt;-signals:
            p.AsyncClose()
            break Loop
        }
        time.Sleep(2 * time.Second)
    }

    fmt.Fprintf(os.Stdout, &amp;quot;发送数=%d，发送失败数=%d \n&amp;quot;, enqueued, errors)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;注意事项&#34;&gt;注意事项&lt;/h3&gt;

&lt;p&gt;我们在来看看Shopify/sarama的producer有两种运行模式的一些注意的的地方&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;同步模式:producer把消息发给kafka之后会等待结果返回。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;config := sarama.NewConfig()
config.Producer.Return.Successes = true
client, err := sarama.NewClient([]{&amp;quot;localhost:9092&amp;quot;}, config)
if err != nil {
    log.Fatalf(&amp;quot;unable to create kafka client: %q&amp;quot;, err)
}

producer, err := sarama.NewSyncProducerFromClient(client)
if err != nil {
    log.Fatalf(&amp;quot;unable to create kafka producer: %q&amp;quot;, err)
}
defer producer.Close()

text := fmt.Sprintf(&amp;quot;message %08d&amp;quot;, i)
partition, offset , err := producer.SendMessage(&amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)})
if err != nil {
    log.Fatalf(&amp;quot;unable to produce message: %q&amp;quot;, err)
}
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意同步模式下，下面配置必须置上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config.Producer.Return.Successes = true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;否则运行报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2018/12/25 08:08:30 unable to create kafka producer: &amp;quot;kafka:
invalid configuration (Producer.Return.Successes must be true to be used in a SyncProducer)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;异步模式:producer把消息发给kafka之后不会等待结果返回。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;异步模式，顾名思义就是produce一个message之后不等待发送完成返回；这样调用者可以继续做其他的工作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config := sarama.NewConfig()
// config.Producer.Return.Successes = true
client, err := sarama.NewClient([]{&amp;quot;localhost:9092&amp;quot;}, config)
if err != nil {
    log.Fatalf(&amp;quot;unable to create kafka client: %q&amp;quot;, err)
}

producer, err := sarama.NewAsyncProducerFromClient
if err != nil {
    log.Fatalf(&amp;quot;unable to create kafka producer: %q&amp;quot;, err)
}
defer producer.Close()

text := fmt.Sprintf(&amp;quot;message %08d&amp;quot;, i)
producer.Input() &amp;lt;- &amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
// wait response
select {
        //case msg := &amp;lt;-producer.Successes():
        //    log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
        case err := &amp;lt;-producer.Errors():
            log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
        default:
            log.Println(&amp;quot;Produced message default&amp;quot;,)
}
...
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;异步模式produce一个消息后，缺省并不会报告成功状态，需要打开返回配置。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;config.Producer.Return.Successes = false
...
producer.Input() &amp;lt;- &amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
log.Printf(&amp;quot;Produced message: [%s]\n&amp;quot;,text)

// wait response
select {
    case msg := &amp;lt;-producer.Successes():
        log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
    case err := &amp;lt;-producer.Errors():
        log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则这段代码会挂住，因为设置没有要求返回成功config.Producer.Return.Successes = false，那么在select等待的时候producer.Successes()不会返回，producer.Errors()也不会返回(假设没有错误发生)，就挂在这儿。当然可以加一个default分支绕过去，就不会挂住了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select {
    case msg := &amp;lt;-producer.Successes():
        log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
    case err := &amp;lt;-producer.Errors():
        log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
    default:
        log.Println(&amp;quot;Produced message default&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果打开了Return.Successes配置，则上述代码段等同于同步方式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config.Producer.Return.Successes = true
...
producer.Input() &amp;lt;- &amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
log.Printf(&amp;quot;Produced message: [%s]\n&amp;quot;,text)

// wait response
select {
    case msg := &amp;lt;-producer.Successes():
        log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
    case err := &amp;lt;-producer.Errors():
        log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从log可以看到，每发送一条消息，收到一条Return.Successes，类似于：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2018/12/25 08:51:51 Produced message: [message 00002537]
2018/12/25 08:51:51 Produced message successes: [message 00002537]
2018/12/25 08:51:51 Produced message: [message 00002538]
2018/12/25 08:51:51 Produced message successes: [message 00002538]
2018/12/25 08:51:51 Produced message: [message 00002539]
2018/12/25 08:51:51 Produced message successes: [message 00002539]
2018/12/25 08:51:51 Produced message: [message 00002540]
2018/12/25 08:51:51 Produced message successes: [message 00002540]
2018/12/25 08:51:51 Produced message: [message 00002541]
2018/12/25 08:51:51 Produced message successes: [message 00002541]
2018/12/25 08:51:51 Produced message: [message 00002542]
2018/12/25 08:51:51 Produced message successes: [message 00002542]
2018/12/25 08:51:51 Produced message: [message 00002543]
2018/12/25 08:51:51 Produced message successes: [message 00002543]
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就像是同步produce一样的行为了。&lt;/p&gt;

&lt;p&gt;如果打开了Return.Successes配置，而又没有producer.Successes()提取，那么Successes()这个chan消息会被写满。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config.Producer.Return.Successes = true
...
log.Printf(&amp;quot;Reade to Produced message: [%s]\n&amp;quot;,text)
producer.Input() &amp;lt;- &amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
log.Printf(&amp;quot;Produced message: [%s]\n&amp;quot;,text)

// wait response
select {
    //case msg := &amp;lt;-producer.Successes():
    //    log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
    case err := &amp;lt;-producer.Errors():
        log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
    default:
        log.Println(&amp;quot;Produced message default&amp;quot;,)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写满的结果就是不能再写入了，导致后面的Return.Successes消息丢失, 而且producer也会挂住，因为共享的buffer被占满了，大量的Return.Successes没有被消耗掉。&lt;/p&gt;

&lt;p&gt;运行一段时间后：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2018/12/25 08:58:24 Reade to Produced message: [message 00000603]
2018/12/25 08:58:24 Produced message: [message 00000603]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000604]
2018/12/25 08:58:24 Produced message: [message 00000604]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000605]
2018/12/25 08:58:24 Produced message: [message 00000605]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000606]
2018/12/25 08:58:24 Produced message: [message 00000606]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000607]
2018/12/25 08:58:24 Produced message: [message 00000607]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000608]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在produce第00000608个message的时候被挂住了，因为消息缓冲满了；这个缓冲的大小是可配的(可能是这个MaxRequestSize?)，但是不管大小是多少，如果没有去提取Success消息最终都会被占满的。&lt;/p&gt;

&lt;p&gt;结论就是说配置config.Producer.Return.Successes = true和操作&amp;lt;-producer.Successes()必须配套使用；配置成true，那么就要去读取Successes，如果配置成false，则不能去读取Successes。&lt;/p&gt;

&lt;h3 id=&#34;重要配置参数&#34;&gt;重要配置参数&lt;/h3&gt;

&lt;p&gt;1、MaxMessageBytes int&lt;/p&gt;

&lt;p&gt;这个参数影响了一条消息的最大字节数，默认是1000000。但是注意，这个参数必须要小于broker中的 message.max.bytes。&lt;/p&gt;

&lt;p&gt;2、RequiredAcks RequiredAcks&lt;/p&gt;

&lt;p&gt;这个参数影响了消息需要被多少broker写入之后才返回。取值可以是0、1、-1&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1代表了不需要等待broker确认才返回、这样最容易丢失消息但同时性能却是最好的&lt;/li&gt;
&lt;li&gt;0代表需要分区的leader确认后才返回，这是一种折中的方案，它会等待副本 Leader 响应，但不会等到 follower 的响应。一旦 Leader 挂掉消息就会丢失。但性能和消息安全性都得到了一定的保证。&lt;/li&gt;
&lt;li&gt;-1代表需要分区的所有副本确认后返回这样可以保证消息不会丢失，但同时性能和吞吐量却是最低的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、Partitioner PartitionerConstructor&lt;/p&gt;

&lt;p&gt;这个是分区器。Sarama默认提供了几种分区器，如果不指定默认使用Hash分区器。&lt;/p&gt;

&lt;p&gt;4、Retry&lt;/p&gt;

&lt;p&gt;这个参数代表了重试的次数，以及重试的时间，主要发生在一些可重试的错误中。&lt;/p&gt;

&lt;p&gt;在重试过程中需要考虑幂等操作了，比如当同时发送了2个请求，如果第一个请求发送到broker中，broker写入失败了，但是第二个请求写入成功了，那么客户端将重新发送第一个消息的请求，这个时候会造成乱序。又比如当第一个请求返回acks的时候，因为网络原因，客户端没有收到，所以客户端进行了重发，这个时候就会造成消息的重复。&lt;/p&gt;

&lt;p&gt;所以，幂等生产者就是为了保证消息发送到broker中是有序且不重复的。一共有两个参数&lt;/p&gt;

&lt;p&gt;5、MaxOpenRequests int&lt;/p&gt;

&lt;p&gt;这个参数代表了允许没有收到acks而可以同时发送的最大batch数。&lt;/p&gt;

&lt;p&gt;6、Idempotent bool&lt;/p&gt;

&lt;p&gt;用于幂等生产者，当这一项设置为true的时候，生产者将保证生产的消息一定是有序且精确一次的。&lt;/p&gt;

&lt;p&gt;7、Flush&lt;/p&gt;

&lt;p&gt;用于设置将消息打包发送，简单来讲就是每次发送消息到broker的时候，不是生产一条消息就发送一条消息，而是等消息累积到一定的程度了，再打包发送。所以里面含有两个参数。一个是多少条消息触发打包发送，一个是累计的消息大小到了多少，然后发送。&lt;/p&gt;

&lt;p&gt;8、Compression&lt;/p&gt;

&lt;p&gt;压缩数据进行发送，选择不同支持的压缩方式，也可以不压缩，压缩比较消耗资源，不压缩可以提高速度。&lt;/p&gt;

&lt;h2 id=&#34;源码解析&#34;&gt;源码解析&lt;/h2&gt;

&lt;h3 id=&#34;创建过程&#34;&gt;创建过程&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;producer, err := sarama.NewAsyncProducer([]string{&amp;quot;localhost:9092&amp;quot;}, config)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一切都从这么一行开始讲起。在这里其实就只有两个部分，先是通过地址和配置，构建一个 client 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewAsyncProducer(addrs []string, conf *Config) (AsyncProducer, error) {

  // 构建client
    client, err := NewClient(addrs, conf)
    if err != nil {
        return nil, err
    }

  // 构建AsyncProducer
    return newAsyncProducer(client)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Client的创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;先构建一个 client 结构体。然后创建完之后，刷新元数据，并且启动一个协程，在后台进行刷新。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewClient(addrs []string, conf *Config) (Client, error) {
    ...
  // 构建一个client
  client := &amp;amp;client{
        conf:                    conf,
        closer:                  make(chan none),
        closed:                  make(chan none),
        brokers:                 make(map[int32]*Broker),
        metadata:                make(map[string]map[int32]*PartitionMetadata),
        metadataTopics:          make(map[string]none),
        cachedPartitionsResults: make(map[string][maxPartitionIndex][]int32),
        coordinators:            make(map[string]int32),
    }
  // 把用户输入的broker地址作为“种子broker”增加到seedBrokers中
  // 随后客户端会根据已有的broker地址，自动刷新元数据，以获取更多的broker地址
  // 所以称之为种子
  random := rand.New(rand.NewSource(time.Now().UnixNano()))
    for _, index := range random.Perm(len(addrs)) {
        client.seedBrokers = append(client.seedBrokers, NewBroker(addrs[index]))
    }
    ...
  // 启动协程在后台刷新元数据
  go withRecover(client.backgroundMetadataUpdater)
  return client, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;元数据的更新&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;后台更新元数据的设计其实很简单，利用一个 ticker ，按时对元数据进行更新，直到 client 关闭。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) backgroundMetadataUpdater() {

  // 按照配置的时间更新元数据
  ticker := time.NewTicker(client.conf.Metadata.RefreshFrequency)
    defer ticker.Stop()

  // 循环获取channel，判断是执行更新操作还是终止
  for {
        select {
        case &amp;lt;-ticker.C:
            if err := client.refreshMetadata(); err != nil {
                Logger.Println(&amp;quot;Client background metadata update:&amp;quot;, err)
            }
        case &amp;lt;-client.closer:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们继续来看看 client.refreshMetadata() 这个方法，在这里我们设置了需要刷新元数据的主题，重试的次数，超时的时间。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) RefreshMetadata(topics ...string) error {
  deadline := time.Time{}
    if client.conf.Metadata.Timeout &amp;gt; 0 {
        deadline = time.Now().Add(client.conf.Metadata.Timeout)
    }
  // 设置参数
    return client.tryRefreshMetadata(topics, client.conf.Metadata.Retry.Max, deadline)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看tryRefreshMetadata这个方法。在这个方法中，会选取已经存在的broker，构造获取元数据的请求。在收到回应后，如果不存在任何的错误，就将这些元数据用于更新客户端。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) tryRefreshMetadata(topics []string, attemptsRemaining int, deadline time.Time) error {
  ...

  broker := client.any()
    for ; broker != nil &amp;amp;&amp;amp; !pastDeadline(0); broker = client.any() {
    ...
            req := &amp;amp;MetadataRequest{
          Topics: topics,
          // 是否允许创建不存在的主题
          AllowAutoTopicCreation: allowAutoTopicCreation
        }
    response, err := broker.GetMetadata(req)
    switch err.(type) {
        case nil:
            allKnownMetaData := len(topics) == 0
      // 对元数据进行更新
            shouldRetry, err := client.updateMetadata(response, allKnownMetaData)
            if shouldRetry {
                Logger.Println(&amp;quot;client/metadata found some partitions to be leaderless&amp;quot;)
                return retry(err)
            }
            return err
        case ...
      ...
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当客户端拿到了 response 之后，首先，先对本地保存 broker 进行更新。然后，对 topic 进行更新，以及这个 topic 下面的那些 partition 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) updateMetadata(data *MetadataResponse, allKnownMetaData bool) (retry bool, err error) {
  ...
  // 假设返回了新的broker id，那么保存这些新的broker，这意味着增加了broker、或者下线的broker重新上线了
  // 如果返回的id我们已经保存了，但是地址变化了，那么更新地址
  // 如果本地保存的一些id没有返回，说明这些broker下线了，那么删除他们
  client.updateBroker(data.Brokers)

  // 然后对topic也进行元数据的更新
  // 主要是更新topic以及topic对应的partition
  for _, topic := range data.Topics {
    ...
    // 更新每个topic以及对应的partition
    client.metadata[topic.Name] = make(map[int32]*PartitionMetadata, len(topic.Partitions))
        for _, partition := range topic.Partitions {
            client.metadata[topic.Name][partition.ID] = partition
            ...
        }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;与Broker建立连接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要是通过broker := client.any()来实现的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) any() *Broker {
    ...
    if len(client.seedBrokers) &amp;gt; 0 {
        _ = client.seedBrokers[0].Open(client.conf)
        return client.seedBrokers[0]
    }

    // 不保证一定是按顺序的
    for _, broker := range client.brokers {
        _ = broker.Open(client.conf)
        return broker
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open方法异步的建立了一个tcp连接，然后创建了一个缓冲大小为MaxOpenRequests的channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) Open(conf *Config) error {
  if conf == nil {
        conf = NewConfig()
    }
  ...
  go withRecover(func() {
    ...
    dialer := conf.getDialer()
        b.conn, b.connErr = dialer.Dial(&amp;quot;tcp&amp;quot;, b.addr)

    ...
    b.responses = make(chan responsePromise, b.conf.Net.MaxOpenRequests-1)
    ...
    go withRecover(b.responseReceiver)
  })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个名为 responses 的 channel ，用于接收从 broker发送回来的消息。然后，又启动了一个协程，用于接收消息。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;从Broker接收响应&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当 broker 收到一个 response 的时候，先解析消息的头部，然后再解析消息的内容。并把这些内容写进 response 的 packets 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) responseReceiver() {
  for response := range b.responses {

    ...
    // 先根据Header的版本读取对应长度的Header
    var headerLength = getHeaderLength(response.headerVersion)
        header := make([]byte, headerLength)
        bytesReadHeader, err := b.readFull(header)
    decodedHeader := responseHeader{}
        err = versionedDecode(header, &amp;amp;decodedHeader, response.headerVersion)

    ...
    // 解析具体的内容
    buf := make([]byte, decodedHeader.length-int32(headerLength)+4)
        bytesReadBody, err := b.readFull(buf)

    // 省略了一些错误处理，总之，如果发生了错误，就把错误信息写进 response.errors 中
    response.packets &amp;lt;- buf
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;发送与接受消息&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们回到这一行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;response, err := broker.GetMetadata(req)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们直接进去，之前看的是元数据的处理，其实也可以用于发送接收消息。发现在这里构造了一个接受返回信息的结构体，然后调用了sendAndReceive方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) GetMetadata(request *MetadataRequest) (*MetadataResponse, error) {
    response := new(MetadataResponse)

    err := b.sendAndReceive(request, response)

    if err != nil {
        return nil, err
    }

    return response, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这里我们可以看到，先是调用了send方法，然后返回了一个promise。并且当有消息写入这个promise的时候，就得到了结果。&lt;/p&gt;

&lt;p&gt;而且回想一下我们在receiver中，是不是把获取到的 response 写进了 packets ，把错误结果写进了 errors 呢，跟这里是一致的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) sendAndReceive(req protocolBody, res protocolBody) error {
    responseHeaderVersion := int16(-1)
    if res != nil {
        responseHeaderVersion = res.headerVersion()
    }

    promise, err := b.send(req, res != nil, responseHeaderVersion)
    if err != nil {
        return err
    }

    if promise == nil {
        return nil
    }

  // 这里的promise，是上面send方法返回的
    select {
    case buf := &amp;lt;-promise.packets:
        return versionedDecode(buf, res, req.version())
    case err = &amp;lt;-promise.errors:
        return err
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在send方法中，把需要发送的消息通过与broker的tcp连接，同步发送到broker中。&lt;/p&gt;

&lt;p&gt;然后构建了一个responsePromise类型的channel，然后直接将这个结构体丢进这个channel中。然后回想一下，我们在responseReceiver这个方法中，不断消费接收到的response。&lt;/p&gt;

&lt;p&gt;此时在responseReceiver中，收到了send方法传递的responsePromise，他就会通过conn来读取数据，然后将数据写入这个responsePromise的packets中，或者将错误信息写入errors中。&lt;/p&gt;

&lt;p&gt;而此时，再看看send方法，他返回了这个responsePromise的指针。所以，sendAndReceive方法就在等待这个responsePromise内的packets或者errors的channel被写入数据。当responseReceiver接收到了响应并且写入数据的时候，packets或者errors就会被写入消息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) send(rb protocolBody, promiseResponse bool, responseHeaderVersion int16) (*responsePromise, error) {

  ...
  // 将请求的内容封装进 request ，然后发送到Broker中
  // 注意一下这里的 b.write(buf)
  // 里面做了 b.conn.Write(buf) 这件事情
  req := &amp;amp;request{correlationID: b.correlationID, clientID: b.conf.ClientID, body: rb}
    buf, err := encode(req, b.conf.MetricRegistry)
  bytes, err := b.write(buf)

  ...
  // 如果我们的response为nil，也就是说当不需要response的时候，是不会放进inflight发送队列的
  if !promiseResponse {
        // Record request latency without the response
        b.updateRequestLatencyAndInFlightMetrics(time.Since(requestTime))
        return nil, nil
    }

  // 构建一个接收响应的 channel ，返回这个channel的指针
  // 这个 channel 内部包含了两个 channel，一个用来接收响应，一个用来接收错误
  promise := responsePromise{requestTime, req.correlationID, responseHeaderVersion, make(chan []byte), make(chan error)}
    b.responses &amp;lt;- promise

  // 这里返回指针特别的关键，是把消息的发送跟消息的接收联系在一起了
    return &amp;amp;promise, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我们来用一张图说明一下上面这个发送跟接收的过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/kafka/sarama.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;asyncprocuder&#34;&gt;AsyncProcuder&lt;/h3&gt;

&lt;p&gt;AsyncProcuder是如何发送消息的。我们从newAsyncProducer(client)这一行开始讲起。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newAsyncProducer(client Client) (AsyncProducer, error) {
  ...
  p := &amp;amp;asyncProducer{
        client:     client,
        conf:       client.Config(),
        errors:     make(chan *ProducerError),
        input:      make(chan *ProducerMessage),
        successes:  make(chan *ProducerMessage),
        retries:    make(chan *ProducerMessage),
        brokers:    make(map[*Broker]*brokerProducer),
        brokerRefs: make(map[*brokerProducer]int),
        txnmgr:     txnmgr,
    }

  go withRecover(p.dispatcher)
    go withRecover(p.retryHandler)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先是构建了asyncProducer结构体，然后协程启动的go withRecover(p.dispatcher)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) dispatcher() {
  handlers := make(map[string]chan&amp;lt;- *ProducerMessage)
  ...
  for msg := range p.input {
    ...
    // 拦截器
    for _, interceptor := range p.conf.Producer.Interceptors {
            msg.safelyApplyInterceptor(interceptor)
        }

    ...
    // 找到这个Topic对应的Handler
    handler := handlers[msg.Topic]
        if handler == nil {
      // 如果此时还不存在这个Topic对应的Handler，那么创建一个
      // 虽然说他叫Handler，但他其实是一个无缓冲的
            handler = p.newTopicProducer(msg.Topic)
            handlers[msg.Topic] = handler
        }
        // 然后把这条消息写进这个Handler中
        handler &amp;lt;- msg
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个方法中，首先创建了一个以Topic为key的map，这个map的value是无缓冲的channel。&lt;/p&gt;

&lt;p&gt;到这里我们很容易可以推测得出，当通过input发送一条消息的时候，消息会到dispatcher这里，被分配到各个Topic中。&lt;/p&gt;

&lt;p&gt;然后让我们来handler = p.newTopicProducer(msg.Topic)这一行的代码。&lt;/p&gt;

&lt;p&gt;在这里创建了一个缓冲大小为ChannelBufferSize的channel，用于存放发送到这个主题的消息。&lt;/p&gt;

&lt;p&gt;然后创建了一个topicProducer，在这个时候你可以认为消息已经交付给各个topic的topicProducer了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) newTopicProducer(topic string) chan&amp;lt;- *ProducerMessage {
    input := make(chan *ProducerMessage, p.conf.ChannelBufferSize)
    tp := &amp;amp;topicProducer{
        parent:      p,
        topic:       topic,
        input:       input,
        breaker:     breaker.New(3, 1, 10*time.Second),
        handlers:    make(map[int32]chan&amp;lt;- *ProducerMessage),
        partitioner: p.conf.Producer.Partitioner(topic),
    }
    go withRecover(tp.dispatch)
    return input
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们来看看go withRecover(tp.dispatch)这一行代码。同样是启动了一个协程，来处理消息。&lt;/p&gt;

&lt;p&gt;也就是说，到了这一步，对于每一个Topic，都有一个协程来处理消息。&lt;/p&gt;

&lt;p&gt;在这个dispatch()方法中，也同样的接收到一条消息，就会去找这条消息所在的分区的channel，然后把消息写进去。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (tp *topicProducer) dispatch() {
  for msg := range tp.input {
    ...

    // 同样是找到这条消息所在的分区对应的channel，然后把消息丢进去
    handler := tp.handlers[msg.Partition]
        if handler == nil {
            handler = tp.parent.newPartitionProducer(msg.Topic, msg.Partition)
            tp.handlers[msg.Partition] = handler
        }

        handler &amp;lt;- msg
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们进tp.parent.newPartitionProducer(msg.Topic, msg.Partition)这里看看。&lt;/p&gt;

&lt;p&gt;你可以发现partitionProducer跟topicProducer是很像的。&lt;/p&gt;

&lt;p&gt;其实他们就是代表了一条消息的分发，从producer到topic到partition。&lt;/p&gt;

&lt;p&gt;注意，这里面的channel缓冲大小，也是ChannelBufferSize。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) newPartitionProducer(topic string, partition int32) chan&amp;lt;- *ProducerMessage {
    input := make(chan *ProducerMessage, p.conf.ChannelBufferSize)
    pp := &amp;amp;partitionProducer{
        parent:    p,
        topic:     topic,
        partition: partition,
        input:     input,

        breaker:    breaker.New(3, 1, 10*time.Second),
        retryState: make([]partitionRetryState, p.conf.Producer.Retry.Max+1),
    }
    go withRecover(pp.dispatch)
    return input
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到了这一步，我们再来看看消息到了每个partition所在的channel，是如何处理的。&lt;/p&gt;

&lt;p&gt;其实在这一步中，主要是做一些错误处理之类的，然后把消息丢进brokerProducer。&lt;/p&gt;

&lt;p&gt;可以理解为这一步是业务逻辑层到网络IO层的转变，在这之前我们只关心消息去到了哪个分区，而在这之后，我们需要找到这个分区所在的broker的地址，并使用之前已经建立好的TCP连接，发送这条消息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pp *partitionProducer) dispatch() {

  // 找到这个主题和分区的leader所在的broker
  pp.leader, _ = pp.parent.client.Leader(pp.topic, pp.partition)
  // 如果此时找到了这个leader
  if pp.leader != nil {
        pp.brokerProducer = pp.parent.getBrokerProducer(pp.leader)
        pp.parent.inFlight.Add(1)
    // 发送一条消息来表示同步
        pp.brokerProducer.input &amp;lt;- &amp;amp;ProducerMessage{Topic: pp.topic, Partition: pp.partition, flags: syn}
    }
  ...// 各种异常情况

  // 然后把消息丢进brokerProducer中
  pp.brokerProducer.input &amp;lt;- msg
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到了这里，大概算是整个发送流程最后的一个步骤了。&lt;/p&gt;

&lt;p&gt;我们来看看pp.parent.getBrokerProducer(pp.leader)这行代码里面的内容。&lt;/p&gt;

&lt;p&gt;其实就是找到asyncProducer中的brokerProducer，如果不存在，则创建一个。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) getBrokerProducer(broker *Broker) *brokerProducer {
    p.brokerLock.Lock()
    defer p.brokerLock.Unlock()

    bp := p.brokers[broker]

    if bp == nil {
        bp = p.newBrokerProducer(broker)
        p.brokers[broker] = bp
        p.brokerRefs[bp] = 0
    }

    p.brokerRefs[bp]++

    return bp
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那我们就来看看brokerProducer是怎么创建出来的。&lt;/p&gt;

&lt;p&gt;看这个方法中启动的第二个协程，我们可以推测bridge这个channel收到消息后，会把收到的消息打包成一个request，然后调用Produce方法。&lt;/p&gt;

&lt;p&gt;并且，将返回的结果的指针地址，写进response中。&lt;/p&gt;

&lt;p&gt;然后构造好brokerProducerResponse，并且写入responses中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) newBrokerProducer(broker *Broker) *brokerProducer {
    var (
        input     = make(chan *ProducerMessage)
        bridge    = make(chan *produceSet)
        responses = make(chan *brokerProducerResponse)
    )

    bp := &amp;amp;brokerProducer{
        parent:         p,
        broker:         broker,
        input:          input,
        output:         bridge,
        responses:      responses,
        stopchan:       make(chan struct{}),
        buffer:         newProduceSet(p),
        currentRetries: make(map[string]map[int32]error),
    }
    go withRecover(bp.run)

    // minimal bridge to make the network response `select`able
    go withRecover(func() {
        for set := range bridge {
            request := set.buildRequest()

            response, err := broker.Produce(request)

            responses &amp;lt;- &amp;amp;brokerProducerResponse{
                set: set,
                err: err,
                res: response,
            }
        }
        close(responses)
    })

    if p.conf.Producer.Retry.Max &amp;lt;= 0 {
        bp.abandoned = make(chan struct{})
    }

    return bp
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我们再来看看broker.Produce(request)这一行代码。&lt;/p&gt;

&lt;p&gt;是不是很熟悉呢，我们在client部分讲到的sendAndReceive方法。&lt;/p&gt;

&lt;p&gt;而且我们可以发现，如果我们设置了需要Acks，就会返回一个response；如果没设置，那么消息发出去之后，就不管了。&lt;/p&gt;

&lt;p&gt;此时在获取了response，并且填入了response的内容后，返回这个response的内容。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) Produce(request *ProduceRequest) (*ProduceResponse, error) {
    var (
        response *ProduceResponse
        err      error
    )

    if request.RequiredAcks == NoResponse {
        err = b.sendAndReceive(request, nil)
    } else {
        response = new(ProduceResponse)
        err = b.sendAndReceive(request, response)
    }

    if err != nil {
        return nil, err
    }

    return response, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，Sarama生产者相关的内容就介绍完毕了。&lt;/p&gt;

&lt;h3 id=&#34;syncproducer-和asyncproducer的关系&#34;&gt;syncProducer 和asyncProducer的关系&lt;/h3&gt;

&lt;p&gt;syncProducer 是所有功能都是由asyncProducer实现的，而syncProducer 之所以可以同步发送消息，答案就在SendMessage 函数中，源码如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; func(sp *syncProducer)SendMessage(msg *ProducerMessage) (partitionint32,offsetint64,errerror) {

   expectation :=make(chan*ProducerError,1)

   msg.expectation = expectation

   sp.producer.Input() &amp;lt;- msg

   if err := &amp;lt;-expectation;err != nil {    // 阻塞等待返回结果

        return-1,-1,err.Err

    }

   return msg.Partition,msg.Offset,nil

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而使用asyncProducer 时，只需要 直接将信息producer.Input()&amp;lt;-&amp;amp;ProducerMessage{} 放入进producer.Input(), 然后异步读取返回结果 chan*ProducerError&lt;/p&gt;

&lt;p&gt;消息传递过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; // one per topic

// partitions messages, then dispatches them by partition

type topicProducer struct{

    parent *asyncProducer

    topic string

    input &amp;lt;-chan*ProducerMessage

    breaker *breaker.Breaker

    handlers map[int32] chan&amp;lt;- *ProducerMessage

    partitioner Partitioner

}



type brokerProducer struct{

    parent *asyncProducer

    broker *Broker

    input  &amp;lt;-chan*ProducerMessage

    output chan&amp;lt;- *produceSet

    responses  &amp;lt;-chan*brokerProducerResponse

    buffer *produceSet

    timer  &amp;lt;-chantime.Time

    timerFired bool

    closing error

    currentRetries map[string]map[int32]error

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由代码可以看出topicProducer，partitionProducer，brokerProducer的parent都是asyncProducer&lt;/p&gt;

&lt;p&gt;消息传递过程：&lt;/p&gt;

&lt;p&gt;asyncProducer.dispatcher -&amp;gt;topicProducer.dispath -&amp;gt; partitionProducer.dispatch -&amp;gt; brokerProducer -&amp;gt;produceSet&lt;/p&gt;

&lt;p&gt;其中produceSet 对消息进行聚集，若配置了压缩的参数，则会压缩一个set中的所有的msg, 即批量压缩， 然后构建一个ProduceRequest ,然后由 broker.Produce 将请求发送出去，其中 broker 结构体代表一个kafka broker 的连接&lt;/p&gt;

&lt;p&gt;partitionProducer 会选择leader broker地址 ,若选择失败，则会重新选择leader broker ，然后由这个连接发送消息根据kafka版本不同，消息会放入到不同的结构体中若版本大于V0.11，set.recordsToSend.RecordBatch.addRecord(rec) 将一个rec添加进去，否则将set.recordsToSend.MsgSet.addMessage(msgToSend)
 &lt;/p&gt;

&lt;p&gt;在生成一个newBrokerProducer时，broker会开启消费output, 而output就是一个存放produceSet的channel,阻塞等待刷新ProduceRequest  并将其发送出去&lt;/p&gt;

&lt;h2 id=&#34;消费者&#34;&gt;消费者&lt;/h2&gt;

&lt;p&gt;消费者集群模实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main()  {
    topic := []string{&amp;quot;test&amp;quot;}
    var wg = &amp;amp;sync.WaitGroup{}
    wg.Add(2)
    //广播式消费：消费者1
    go clusterConsumer(wg, Address, topic, &amp;quot;group-1&amp;quot;)
    //广播式消费：消费者2
    go clusterConsumer(wg, Address, topic, &amp;quot;group-2&amp;quot;)

    wg.Wait()
}

// 支持brokers cluster的消费者
func clusterConsumer(wg *sync.WaitGroup,brokers, topics []string, groupId string)  {
    defer wg.Done()
    config := cluster.NewConfig()
    config.Consumer.Return.Errors = true
    config.Group.Return.Notifications = true
    config.Consumer.Offsets.Initial = sarama.OffsetNewest

    // init consumer
    consumer, err := cluster.NewConsumer(brokers, groupId, topics, config)
    if err != nil {
        log.Printf(&amp;quot;%s: sarama.NewSyncProducer err, message=%s \n&amp;quot;, groupId, err)
        return
    }
    defer consumer.Close()

    // trap SIGINT to trigger a shutdown
    signals := make(chan os.Signal, 1)
    signal.Notify(signals, os.Interrupt)

    // consume errors
    go func() {
        for err := range consumer.Errors() {
            log.Printf(&amp;quot;%s:Error: %s\n&amp;quot;, groupId, err.Error())
        }
    }()

    // consume notifications
    go func() {
        for ntf := range consumer.Notifications() {
            log.Printf(&amp;quot;%s:Rebalanced: %+v \n&amp;quot;, groupId, ntf)
        }
    }()

    // consume messages, watch signals
    var successes int
    Loop:
    for {
        select {
        case msg, ok := &amp;lt;-consumer.Messages():
            if ok {
                fmt.Fprintf(os.Stdout, &amp;quot;%s:%s/%d/%d\t%s\t%s\n&amp;quot;, groupId, msg.Topic, msg.Partition, msg.Offset, msg.Key, msg.Value)
                consumer.MarkOffset(msg, &amp;quot;&amp;quot;)  // mark message as processed
                successes++
            }
        case &amp;lt;-signals:
            break Loop
        }
    }
    fmt.Fprintf(os.Stdout, &amp;quot;%s consume %d messages \n&amp;quot;, groupId, successes)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;如何优雅的使用-kafka-生产者&#34;&gt;如何优雅的使用 Kafka 生产者&lt;/h1&gt;

&lt;h2 id=&#34;发送流程&#34;&gt;发送流程&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;初始化以及真正发送消息的 kafka-producer-network-thread IO 线程。&lt;/li&gt;
&lt;li&gt;将消息序列化。&lt;/li&gt;
&lt;li&gt;得到需要发送的分区。&lt;/li&gt;
&lt;li&gt;写入内部的一个缓存区中。&lt;/li&gt;
&lt;li&gt;初始化的 IO 线程不断的消费这个缓存来发送消息。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;分区策略&#34;&gt;分区策略&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;指定分区&lt;/li&gt;
&lt;li&gt;自定义路由&lt;/li&gt;
&lt;li&gt;默认策略，通常都是使用这种策略，其实就是一种对分区的轮询简单的来说分为以下几步：

&lt;ul&gt;
&lt;li&gt;获取 Topic 分区数。&lt;/li&gt;
&lt;li&gt;将内部维护的一个线程安全计数器 +1。&lt;/li&gt;
&lt;li&gt;与分区数取模得到分区编号。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实这就是很典型的轮询算法，所以只要分区数不频繁变动这种方式也会比较均匀。&lt;/p&gt;

&lt;h1 id=&#34;性能&#34;&gt;性能&lt;/h1&gt;

&lt;p&gt;生产速度&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;12c32G1000M--3台

单个生产者线程，单副本--821557个记录/秒（78.3 MB /秒）
单个生产者线程，三个副本，异步方式--786980 record / sec（75.1 MB / sec）
单个生产者线程，3个副本，同步复制----428823条记录/秒（40.2 MB /秒）
三个生产者，3个副本，异步复制----2024032个记录/秒（193.0 MB /秒）
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;问题&#34;&gt;问题&lt;/h1&gt;

&lt;h2 id=&#34;golang连接kafka-sarama-内存暴涨问题记录&#34;&gt;golang连接kafka(sarama)内存暴涨问题记录&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;问题背景&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用kafka客户端类库(sarama）步发布消息， qps为100+， 上线后内存，cpu爆炸。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;排查过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;首先排查代码层面写的有逻辑bug， 比如连接未close， 排查无问题&lt;/li&gt;
&lt;li&gt;排查发布的消息较大， 导致golang频繁gc, 和同事确认，无频繁gc&lt;/li&gt;
&lt;li&gt;通过查看源码，发现每次http请求，操作kafka都是短链接， 即频繁的会新建短链接， 排查到这里，还是不能特别确认是因为短链接导致， 因为之前接入rabbitmq类库，也是使用的短链接。&lt;/li&gt;
&lt;li&gt;使用pprof打印出火焰图, profile, 和block的， 也没发现特别大的bug点。&lt;/li&gt;
&lt;li&gt;使用sarama meory搜索官方issue, 和谷歌查询。 得到出具体结论&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;分析具体问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从官方issue 得知， sarama类库自动依赖第三方的统计类库go-mertic, 主要是为了方便给prometheus统计数据。 sarama类库默认打开。导致该统计站的内存，迟迟未释放&lt;/p&gt;

&lt;p&gt;因此使用sarama前， 将该统计关闭即可。&lt;/p&gt;

&lt;p&gt;对应代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics.UseNilMetrics = true
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Kafka</title>
          <link>https://kingjcy.github.io/post/middleware/mq/kafka/</link>
          <pubDate>Fri, 19 Jul 2019 20:21:50 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/kafka/</guid>
          <description>&lt;p&gt;Apache Kafka由著名职业社交公司LinkedIn开发，最初是被设计用来解决LinkedIn公司内部海量日志传输等问题。Kafka使用Scala语言编写，于2011年开源并进入Apache孵化器，2012年10月正式毕业，现在为Apache顶级项目。Kafka是一个分布式数据流平台，具有高吞吐、低延迟、高容错等特点。&lt;/p&gt;

&lt;h1 id=&#34;kafka简介&#34;&gt;Kafka简介&lt;/h1&gt;

&lt;h2 id=&#34;基本术语&#34;&gt;基本术语&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;消息message&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;先不管其他的，我们使用Kafka这个消息系统肯定是先关注消息这个概念，在Kafka中，每一个消息由键、值和一个时间戳组成（key、value和timestamp）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;主题topic&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kafka集群存储同一类别的消息流称为主题&lt;/p&gt;

&lt;p&gt;主题会有多个订阅者（0个1个或多个），当主题发布消息时，会向订阅者推送记录&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;日志 offset&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;针对每一个主题，Kafka集群维护了一个像下面这样的分区日志offset：&lt;/p&gt;

&lt;p&gt;这些分区位offset于不同的服务器上，每一个分区可以看做是一个结构化的提交日志offset，每写入一条记录都会记录到其中一个分区并且分配一个唯一地标识其位置的数字称为偏移量offset&lt;/p&gt;

&lt;p&gt;Kafka集群会将发布的消息保存一段时间，不管是否被消费。例如，如果设置保存天数为2天，那么从消息发布起的两天之内，该消息一直可以被消费，但是超过两天后就会被丢弃以节省空间。其次，Kafka的数据持久化性能很好，所以长时间存储数据不是问题&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分区 Partition&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每个topic可以有一个或多个partition（分区）。分区是在物理层面上的，不同的分区对应着不同的数据文件。Kafka使用分区支持物理上的并发写入和读取，从而大大提高了吞吐量&lt;/p&gt;

&lt;p&gt;Kafka中采用分区的设计有几个目的。一是可以处理更多的消息，不受单台服务器的限制。Topic拥有多个分区意味着它可以不受限的处理更多的数据。第二，分区可以作为并行处理的单元，稍后会谈到这一点&lt;/p&gt;

&lt;p&gt;Log的分区被分布到集群中的多个服务器上。每个服务器处理它分到的分区。根据配置每个分区还可以复制到其它服务器作为备份容错。 每个分区有一个leader，零或多个follower。Leader处理此分区的所有的读写请求，而follower被动的复制数据。如果leader宕机，其它的一个follower会被推举为新的leader。 一台服务器可能同时是一个分区的leader，另一个分区的follower。 这样可以平衡负载，避免所有的请求都只让一台或者某几台服务器处理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;生产者 producer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;生产者往某个Topic上发布消息。生产者还可以选择将消息分配到Topic的哪个节点上。最简单的方式是轮询分配到各个分区以平衡负载，也可以根据某种算法依照权重选择分区&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;消费者 consumer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kafka有一个消费者组的概念，生产者把消息发到的是消费者组，在消费者组里面可以有很多个消费者实例。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;消费者组 Consumer Group&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一个消费者组可以包含一个或多个消费者。使用多分区+多消费者方式可以极大提高数据下游的处理速度。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;节点 Broker&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;消息队列中常用的概念，在Kafka中指部署了Kafka实例的服务器节点。&lt;/p&gt;

&lt;h1 id=&#34;安装部署&#34;&gt;安装部署&lt;/h1&gt;

&lt;h2 id=&#34;单机&#34;&gt;单机&lt;/h2&gt;

&lt;p&gt;1、安装jdk&lt;/p&gt;

&lt;p&gt;以oracle jdk为例，下载地址&lt;a href=&#34;http://java.sun.com/javase/downloads/index.jsp&#34;&gt;http://java.sun.com/javase/downloads/index.jsp&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum -y install jdk-8u141-linux-x64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、下载解压&lt;/p&gt;

&lt;p&gt;下载地址：&lt;a href=&#34;http://kafka.apache.org/downloads，如0.10.1.0版本的Kafka下载&#34;&gt;http://kafka.apache.org/downloads，如0.10.1.0版本的Kafka下载&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://apache.fayea.com/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz
tar -xvf kafka_2.11-0.10.1.0.tgz
cd kafka_2.11-0.10.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# grep -Ev &amp;quot;^#|^$&amp;quot; /data/kafka/config/server.properties
broker.id=0
delete.topic.enable=true
listeners=PLAINTEXT://192.168.15.131:9092
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/data/kafka/data
num.partitions=1
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.flush.interval.messages=10000
log.flush.interval.ms=1000
log.retention.hours=168
log.retention.bytes=1073741824
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
zookeeper.connect=192.168.15.131:2181,192.168.15.132:2181,192.168.15.133:2181
zookeeper.connection.timeout.ms=6000
group.initial.rebalance.delay.ms=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;提示：其他主机将该机器的kafka目录拷贝即可，然后需要修改broker.id、listeners地址。有关kafka配置文件参数，参考：&lt;a href=&#34;http://orchome.com/12；&#34;&gt;http://orchome.com/12；&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、安装Zookeeper&lt;/p&gt;

&lt;p&gt;Kafka需要Zookeeper的监控，所以先要安装Zookeeper。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://apache.forsale.plus/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz
tar zxf zookeeper-3.4.9.tar.gz
mv zookeeper-3.4.9 /data/zk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置文件内容如下所示:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# cat /data/zk/conf/zoo.cfg
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/data/zk/data/zookeeper
dataLogDir=/data/zk/data/logs
clientPort=2181
maxClientCnxns=60
autopurge.snapRetainCount=3
autopurge.purgeInterval=1

server.1=zk01:2888:3888
server.2=zk02:2888:3888
server.3=zk03:2888:3888
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;server.id=host:port:port:表示了不同的zookeeper服务器的自身标识，作为集群的一部分，每一台服务器应该知道其他服务器的信息。用户可以从“server.id=host:port:port” 中读取到相关信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在服务器的data(dataDir参数所指定的目录)下创建一个文件名为myid的文件，这个文件的内容只有一行，指定的是自身的id值。比如，服务器“1”应该在myid文件中写入“1”。这个id必须在集群环境中服务器标识中是唯一的，且大小在1～255之间。这一样配置中，zoo1代表第一台服务器的IP地址。第一个端口号（port）是从follower连接到leader机器的
端口，第二个端口是用来进行leader选举时所用的端口。所以，在集群配置过程中有三个非常重要的端口：clientPort：2181、port:2888、port:3888。&lt;/p&gt;

&lt;p&gt;如果想更换日志输出位置，除了在zoo.cfg加入&amp;rdquo;dataLogDir=/data/zk/data/logs&amp;rdquo;外，还需要修改zkServer.sh文件，大概修改方式地方在125行左右，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;125 ZOO_LOG_DIR=&amp;quot;$($GREP &amp;quot;^[[:space:]]*dataLogDir&amp;quot; &amp;quot;$ZOOCFG&amp;quot; | sed -e &#39;s/.*=//&#39;)&amp;quot;
126 if [ ! -w &amp;quot;$ZOO_LOG_DIR&amp;quot; ] ; then
127 mkdir -p &amp;quot;$ZOO_LOG_DIR&amp;quot;
128 fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在启动服务之前，还需要分别在zookeeper创建myid，方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo 1 &amp;gt;  /data/zk/data/zookeeper/myid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/data/zk/bin/zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证服务&lt;/p&gt;

&lt;p&gt;查看相关端口号&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# ss -lnpt|grep java
LISTEN     0      50          :::34442                   :::*                   users:((&amp;quot;java&amp;quot;,pid=2984,fd=18))
LISTEN     0      50       ::ffff:192.168.15.133:3888                    :::*                   users:((&amp;quot;java&amp;quot;,pid=2984,fd=26))
LISTEN     0      50          :::2181                    :::*                   users:((&amp;quot;java&amp;quot;,pid=2984,fd=25))


###查看zookeeper服务状态
[root@localhost ~]# /data/zk/bin/zkServer.sh status

ZooKeeper JMX enabled by default

Using config: /data/zk/bin/../conf/zoo.cfgMode: follower
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、Kafka目录介绍&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/bin 操作kafka的可执行脚本，还包含windows下脚本&lt;/li&gt;
&lt;li&gt;/config 配置文件所在目录&lt;/li&gt;
&lt;li&gt;/libs 依赖库目录&lt;/li&gt;
&lt;li&gt;/logs 日志数据目录，目录kafka把server端日志分为5种类型，分为:server,request,state，log-cleaner，controller&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5、启动Kafka&lt;/p&gt;

&lt;p&gt;在每台服务器上进入Kafka目录，分别执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-server-start.sh config/server.properties &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检测2181与9092端口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netstat -tunlp|egrep &amp;quot;(2181|9092)&amp;quot;
tcp        0      0 :::2181                     :::*                        LISTEN      19787/java          
tcp        0      0 :::9092                     :::*                        LISTEN      28094/java 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;p&gt;Kafka的进程ID为28094，占用端口为9092&lt;/p&gt;

&lt;p&gt;QuorumPeerMain为对应的zookeeper实例，进程ID为19787，在2181端口监听&lt;/p&gt;

&lt;p&gt;6、单机连通性测试&lt;/p&gt;

&lt;p&gt;启动2个XSHELL客户端，一个用于生产者发送消息，一个用于消费者接受消息。&lt;/p&gt;

&lt;p&gt;运行producer，随机敲入几个字符，相当于把这个敲入的字符消息发送给队列。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --broker-list 192.168.153.118:9092 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：早版本的Kafka，–broker-list 192.168.1.181:9092需改为–zookeeper 192.168.1.181:2181&lt;/p&gt;

&lt;p&gt;运行consumer，可以看到刚才发送的消息列表。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --zookeeper 192.168.153.118:2181 --topic test --from-beginning  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;p&gt;producer，指定的Socket(192.168.1.181+9092),说明生产者的消息要发往kafka，也即是broker&lt;/p&gt;

&lt;p&gt;consumer, 指定的Socket(192.168.1.181+2181),说明消费者的消息来自zookeeper（协调转发）&lt;/p&gt;

&lt;p&gt;上面的只是一个单个的broker，下面我们来实验一个多broker的集群。&lt;/p&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;搭建一个多个broker的伪集群，刚才只是启动了单个broker，现在启动有3个broker组成的集群，这些broker节点也都是在本机上。&lt;/p&gt;

&lt;p&gt;1、为每一个broker提供配置文件&lt;/p&gt;

&lt;p&gt;然后修改各个服务器的配置文件：进入Kafka的config目录，修改server.properties&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# brokerid就是指各台服务器对应的id，所以各台服务器值不同
broker.id=0
# 端口号，无需改变
port=9092
# 当前服务器的IP，各台服务器值不同
host.name=192.168.0.10
# Zookeeper集群的ip和端口号
zookeeper.connect=192.168.0.10:2181,192.168.0.11:2181,192.168.0.12:2181
# 日志目录
log.dirs=/home/www/kafka-logs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们先看看config/server0.properties配置信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;broker.id=0
listeners=PLAINTEXT://:9092
port=9092
host.name=192.168.1.181
num.network.threads=4
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/tmp/kafka-logs
num.partitions=5
num.recovery.threads.per.data.dir=1
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
log.cleaner.enable=false
zookeeper.connect=192.168.1.181:2181
zookeeper.connection.timeout.ms=6000
queued.max.requests =500
log.cleanup.policy = delete
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;p&gt;broker.id为集群中唯一的标注一个节点，因为在同一个机器上，所以必须指定不同的端口和日志文件，避免数据被覆盖。&lt;/p&gt;

&lt;p&gt;在上面单个broker的实验中，为什么kafka的端口为9092，这里可以看得很清楚。&lt;/p&gt;

&lt;p&gt;kafka cluster怎么同zookeeper交互的，配置信息中也有体现。&lt;/p&gt;

&lt;p&gt;那么下面，我们仿照上面的配置文件，提供2个broker的配置文件：&lt;/p&gt;

&lt;p&gt;server2.properties:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;broker.id=2
listeners=PLAINTEXT://:9094
port=9094
host.name=192.168.1.181
num.network.threads=4
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/tmp/kafka-logs2
num.partitions=5
num.recovery.threads.per.data.dir=1
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
log.cleaner.enable=false
zookeeper.connect=192.168.1.181:2181
zookeeper.connection.timeout.ms=6000
queued.max.requests =500
log.cleanup.policy = delete
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、启动所有的broker&lt;/p&gt;

&lt;p&gt;命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-server-start.sh config/server0.properties &amp;amp;   #启动broker0
bin/kafka-server-start.sh config/server1.properties &amp;amp; #启动broker1
bin/kafka-server-start.sh config/server2.properties &amp;amp; #启动broker2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看2181、9092、9093、9094端口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netstat -tunlp|egrep &amp;quot;(2181|9092|9093|9094)&amp;quot;
tcp        0      0 :::9093                     :::*                        LISTEN      29725/java          
tcp        0      0 :::2181                     :::*                        LISTEN      19787/java          
tcp        0      0 :::9094                     :::*                        LISTEN      29800/java          
tcp        0      0 :::9092                     :::*                        LISTEN      29572/java  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个zookeeper在2181端口上监听，3个kafka cluster(broker)分别在端口9092,9093,9094监听。&lt;/p&gt;

&lt;p&gt;3、创建topic&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --create --topic topic_1 --partitions 1 --replication-factor 3  \--zookeeper localhost:2181
bin/kafka-topics.sh --create --topic topic_2 --partitions 1 --replication-factor 3  \--zookeeper localhost:2181
bin/kafka-topics.sh --create --topic topic_3 --partitions 1 --replication-factor 3  \--zookeeper localhost:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看topic创建情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --list --zookeeper localhost:2181
test
topic_1
topic_2
topic_3
[root@atman081 kafka_2.10-0.9.0.0]# bin/kafka-topics.sh --describe --zookeeper localhost:2181
Topic:test  PartitionCount:1  ReplicationFactor:1 Configs:
  Topic: test Partition: 0  Leader: 0 Replicas: 0 Isr: 0
Topic:topic_1 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_1  Partition: 0  Leader: 2 Replicas: 2,1,0 Isr: 2,1,0
Topic:topic_2 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_2  Partition: 0  Leader: 1 Replicas: 1,2,0 Isr: 1,2,0
Topic:topic_3 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_3  Partition: 0  Leader: 0 Replicas: 0,2,1 Isr: 0,2,1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个行显示所有partitions的一个总结，以下每一行给出一个partition中的信息，如果我们只有一个partition，则只显示一行。&lt;/p&gt;

&lt;p&gt;leader 是在给出的所有partitons中负责读写的节点，每个节点都有可能成为leader&lt;/p&gt;

&lt;p&gt;replicas 显示给定partiton所有副本所存储节点的节点列表，不管该节点是否是leader或者是否存活。&lt;/p&gt;

&lt;p&gt;isr 副本都已同步的的节点集合，这个集合中的所有节点都是存活状态，并且跟leader同步&lt;/p&gt;

&lt;p&gt;4、模拟客户端发送，接受消息&lt;/p&gt;

&lt;p&gt;发送消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --topic topic_1 --broker-list 192.168.1.181:9092,192.168.1.181:9093,192.168.1.181:9094
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接收消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --topic topic_1 --zookeeper 192.168.1.181:2181 --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要注意，此时producer将topic发布到了3个broker中，现在就有点分布式的概念了。&lt;/p&gt;

&lt;p&gt;5、kill some broker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kill broker(id=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先，我们根据前面的配置，得到broker(id=0)应该在9092监听,这样就能确定它的PID了。&lt;/p&gt;

&lt;p&gt;broker0没kill之前topic在kafka cluster中的情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --describe --zookeeper localhost:2181
Topic:test  PartitionCount:1  ReplicationFactor:1 Configs:
  Topic: test Partition: 0  Leader: 0 Replicas: 0 Isr: 0
Topic:topic_1 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_1  Partition: 0  Leader: 2 Replicas: 2,1,0 Isr: 2,1,0
Topic:topic_2 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_2  Partition: 0  Leader: 1 Replicas: 1,2,0 Isr: 1,2,0
Topic:topic_3 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_3  Partition: 0  Leader: 2 Replicas: 0,2,1 Isr: 2,1,0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kill之后，再观察，做下对比。很明显，主要变化在于Isr，以后再分析&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --describe --zookeeper localhost:2181
Topic:test  PartitionCount:1  ReplicationFactor:1 Configs:
  Topic: test Partition: 0  Leader: -1  Replicas: 0 Isr: 
Topic:topic_1 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_1  Partition: 0  Leader: 2 Replicas: 2,1,0 Isr: 2,1
Topic:topic_2 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_2  Partition: 0  Leader: 1 Replicas: 1,2,0 Isr: 1,2
Topic:topic_3 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_3  Partition: 0  Leader: 2 Replicas: 0,2,1 Isr: 2,1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试下，发送消息，接受消息，是否收到影响。&lt;/p&gt;

&lt;p&gt;发送消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --topic topic_1 --broker-list 192.168.1.181:9092,192.168.1.181:9093,192.168.1.181:9094
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接收消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --topic topic_1 --zookeeper 192.168.1.181:2181 --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，kafka的分布式机制，容错能力还是挺好的~&lt;/p&gt;

&lt;h1 id=&#34;kafka常用命令&#34;&gt;Kafka常用命令&lt;/h1&gt;

&lt;p&gt;1、新建一个主题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --create --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --replication-factor 2 --partitions 2 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test有两个复制因子和两个分区&lt;/p&gt;

&lt;p&gt;2、查看新建的主题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --describe --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.8.8版本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./kafka-create-topic.sh --partition 1 --replica 1 --zookeeper localhost:2181 --topic test

./kafka-list-topic.sh --zookeeper localhost:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下查询的结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@dx3 bin]# ./kafka-topics.sh --describe --zookeeper localhost:2183 --topic test
Topic:test    PartitionCount:3    ReplicationFactor:3    Configs:
    Topic: test    Partition: 0    Leader: 0    Replicas: 0,1,2    Isr: 0,2,1
    Topic: test    Partition: 1    Leader: 1    Replicas: 1,2,0    Isr: 1,2,0
    Topic: test    Partition: 2    Leader: 2    Replicas: 2,0,1    Isr: 2,0,1

其中第一行是所有分区的信息，下面的每一行对应一个分区
Leader：负责某个分区所有读写操作的节点
Replicas：复制因子节点
Isr：存活节点
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以这个Kafka集群一共三个节点，test这个Topic, 编号为0的Partition,Leader在broker.id=0这个节点上，副本在broker.id为0 1 2这个三个几点，并且所有副本都存活，并跟broker.id=0这个节点同步&lt;/p&gt;

&lt;p&gt;3、查看Kafka所有的主题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --list --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、在终端发送消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、在终端接收（消费）消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --bootstrap-server localhost:9092 --topic test --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们正常情况下不会使用自带的命令行进行数据的发送和消费，我们都是使用第三方库进行&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/kafka-client/&#34;&gt;客户端包括生产者和消费者&lt;/a&gt;的编码，实现业务的正常使用。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;先举一个例子&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/kafka/20170719.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kafka集群有两台服务器，四个分区，此外有两个消费者组A和B，消费者组A具有2个消费者实例C1-2，消费者B具有4个消费者实例C3-6&lt;/p&gt;

&lt;p&gt;例如此时我们创建了一个主题test，有两个分区，分别是Server1的P0和Server2的P1，假设此时我们通过test发布了一条消息，那么这条消息只会发到P0或P1其中之一，也就是消息只会发给其中的一个分区&lt;/p&gt;

&lt;p&gt;分区接收到消息后会记录在分区日志中，记录的方式我们讲过了，就是通过offset，分区中的消息都是以k-v形式存在。k表示offset，称之为偏移量，一个64位整型的唯一标识，offset代表了Topic分区中所有消息流中该消息的起始字节位置。 v就是实际的消息内容,正因为有这个偏移量的存在，所以一个分区内的消息是有先后顺序的，即offset大的消息比offset小的消息后到。但是注意，由于消息随机发往主题的任意一个分区，因此虽然同一个分区的消息有先后顺序，但是不同分区之间的消息就没有先后顺序了，那么如果我们要求消费者顺序消费主题发的消息那该怎么办呢，此时只要在创建主题的时候只提供一个分区即可&lt;/p&gt;

&lt;p&gt;讲完了主题发消息，接下来就该消费者消费消息了，假设上面test的消息发给了分区P0，此时从图中可以看到，有两个消费者组，那么P0将会把消息发到哪个消费者组呢？从图中可以看到，P0把消息既发给了消费者组A也发给了B，但是A中消息仅被C1消费，B中消息仅被C3消费。这就是我们要讲的，主题发出的消息会发往所有的消费者组，而每一个消费者组下面可以有很多消费者实例，这条消息只会被他们中的一个消费掉。&lt;/p&gt;

&lt;p&gt;在多分区的情况下如何保证有序性呢？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、kafka分布式的单位是partition，同一个partition用一个write ahead log组织，记录的offset，所以可以保证FIFO的顺序。&lt;/li&gt;
&lt;li&gt;2、不同partition之间不能保证顺序。但是绝大多数用户都可以通过message key来定义，因为同一个key的message可以保证只发送到同一个partition，比如说key是user id，table row id等等，所以同一个user或者同一个record的消息永远只会发送到同一个partition上，保证了同一个user或record的顺序。&lt;/li&gt;
&lt;li&gt;3、消费出来自己根据一些数据进行排序，比如时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;核心API&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kafka具有4个核心API：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Producer API：用于向Kafka主题发送消息。&lt;/li&gt;
&lt;li&gt;Consumer API：用于从订阅主题接收消息并且处理这些消息。&lt;/li&gt;
&lt;li&gt;Streams API：作为一个流处理器，用于从一个或者多个主题中消费消息流然后为其他主题生产消息流，高效地将输入流转换为输出流。&lt;/li&gt;
&lt;li&gt;Connector API：用于构建和运行将Kafka主题和已有应用或者数据系统连接起来的可复用的生产者或消费者。例如一个主题到一个关系型数据库的连接能够捕获表的任意变化。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;KAFKA吞吐量大的原因&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、消息顺序写入磁盘，并且批量处理。&lt;/p&gt;

&lt;p&gt;KAFKA维护一个Topic中的分区log，以顺序追加的方式向各个分区中写入消息，每个分区都是不可变的消息队列。分区中的消息都是以k-v形式存在。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;k表示offset，称之为偏移量，一个64位整型的唯一标识，offset代表了Topic分区中所有消息流中该消息的起始字节位置。&lt;/li&gt;
&lt;li&gt;v就是实际的消息内容，每个分区中的每个offset都是唯一存在的，所有分区的消息都是一次写入，在消息未过期之前都可以调整offset来实现多次读取。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们知道现在的磁盘大多数都还是机械结构（SSD不在讨论的范围内），如果将消息以随机写的方式存入磁盘，就会按柱面、磁头、扇区的方式进行（寻址过程），缓慢的机械运动（相对内存）会消耗大量时间，导致磁盘的写入速度只能达到内存写入速度的几百万分之一，为了规避随机写带来的时间消耗，KAFKA采取顺序写的方式存储数据。&lt;/p&gt;

&lt;p&gt;新来的消息只能追加到已有消息的末尾，并且已经生产的消息不支持随机删除以及随机访问，但是消费者可以通过重置offset的方式来访问已经消费过的数据。&lt;/p&gt;

&lt;p&gt;即使顺序读写，过于频繁的大量小I/O操作一样会造成磁盘的瓶颈，所以KAFKA在此处的处理是把这些消息集合在一起批量发送，这样减少对磁盘IO的过度读写，而不是一次发送单个消息。&lt;/p&gt;

&lt;p&gt;2、标准化二进制消息格式&lt;/p&gt;

&lt;p&gt;尤其是在负载比较高的情况下无效率的字节复制影响是显着的。为了避免这种情况，KAFKA采用由Producer，broker和consumer共享的标准化二进制消息格式，这样数据块就可以在它们之间自由传输，无需转换，降低了字节复制的成本开销。&lt;/p&gt;

&lt;p&gt;同时，KAFKA采用了MMAP(Memory Mapped Files，内存映射文件)技术。很多现代操作系统都大量使用主存做磁盘缓存，一个现代操作系统可以将内存中的所有剩余空间用作磁盘缓存，而当内存回收的时候几乎没有性能损失。&lt;/p&gt;

&lt;p&gt;由于KAFKA是基于JVM的，并且任何与Java内存使用打过交道的人都知道两件事：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对象的内存开销非常高，通常是实际要存储数据大小的两倍；&lt;/li&gt;
&lt;li&gt;随着数据的增加，java的垃圾收集也会越来越频繁并且缓慢。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于此，使用文件系统，同时依赖页面缓存就比使用其他数据结构和维护内存缓存更有吸引力：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;不使用进程内缓存，就腾出了内存空间，可以用来存放页面缓存的空间几乎可以翻倍。&lt;/li&gt;
&lt;li&gt;如果KAFKA重启，进行内缓存就会丢失，但是使用操作系统的页面缓存依然可以继续使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可能有人会问KAFKA如此频繁利用页面缓存，如果内存大小不够了怎么办？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KAFKA会将数据写入到持久化日志中而不是刷新到磁盘。实际上它只是转移到了内核的页面缓存。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;利用文件系统并且依靠页缓存比维护一个内存缓存或者其他结构要好，它可以直接利用操作系统的页缓存来实现文件到物理内存的直接映射。完成映射之后对物理内存的操作在适当时候会被同步到硬盘上。&lt;/p&gt;

&lt;p&gt;3、页缓存技术&lt;/p&gt;

&lt;p&gt;为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。这样做的好处有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;避免Object消耗：如果是使用 Java 堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。&lt;/li&gt;
&lt;li&gt;避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相比于使用JVM或in-memory cache等数据结构，利用操作系统的Page Cache更加简单可靠。首先，操作系统层面的缓存利用率会更高，因为存储的都是紧凑的字节结构而不是独立的对象。其次，操作系统本身也对于Page Cache做了大量优化，提供了 write-behind、read-ahead以及flush等多种机制。再者，即使服务进程重启，系统缓存依然不会消失，避免了in-process cache重建缓存的过程。&lt;/p&gt;

&lt;p&gt;通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。&lt;/p&gt;

&lt;p&gt;4、零拷贝&lt;/p&gt;

&lt;p&gt;如下所示，数据从磁盘传输到socket要经过以下几个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;操作系统将数据从磁盘读入到内核空间的页缓存&lt;/li&gt;
&lt;li&gt;应用程序将数据从内核空间读入到用户空间缓存中&lt;/li&gt;
&lt;li&gt;应用程序将数据写回到内核空间到socket缓存中&lt;/li&gt;
&lt;li&gt;操作系统将数据从socket缓冲区复制到网卡缓冲区，以便将数据经网络发出&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里有四次拷贝，两次系统调用，这是非常低效的做法。如果使用sendfile，只需要一次拷贝就行&lt;/p&gt;

&lt;p&gt;linux操作系统 “零拷贝” 机制使用了sendfile方法， 允许操作系统将数据从Page Cache 直接发送到网络，只需要最后一步的copy操作将数据复制到 NIC 缓冲区， 这样避免重新复制数据 。&lt;/p&gt;

&lt;p&gt;通过这种 “零拷贝” 的机制，Page Cache 结合 sendfile 方法，Kafka消费端的性能也大幅提升。这也是为什么有时候消费端在不断消费数据时，我们并没有看到磁盘io比较高，此刻正是操作系统缓存在提供数据。&lt;/p&gt;

&lt;p&gt;5、批量压缩&lt;/p&gt;

&lt;p&gt;在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络带宽，对于需要在广域网上的数据中心之间发送消息的数据流水线尤其如此。所以数据压缩就很重要。可以每个消息都压缩，但是压缩率相对很低。所以KAFKA使用了批量压缩，即将多个消息一起压缩而不是单个消息压缩。&lt;/p&gt;

&lt;p&gt;KAFKA允许使用递归的消息集合，批量的消息可以通过压缩的形式传输并且在日志中也可以保持压缩格式，直到被消费者解压缩。&lt;/p&gt;

&lt;p&gt;KAFKA支持Gzip和Snappy压缩协议。&lt;/p&gt;

&lt;p&gt;6、批量读写&lt;/p&gt;

&lt;p&gt;Kafka数据读写也是批量的而不是单条的。&lt;/p&gt;

&lt;p&gt;除了利用底层的技术外，Kafka还在应用程序层面提供了一些手段来提升性能。最明显的就是使用批次。在向Kafka写入数据时，可以启用批次写入，这样可以避免在网络上频繁传输单个消息带来的延迟和带宽开销。假设网络带宽为10MB/S，一次性传输10MB的消息比传输1KB的消息10000万次显然要快得多。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;KAFKA数据可靠性深度解读&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KAFKA的消息保存在Topic中，Topic可分为多个分区，为保证数据的安全性，每个分区又有多个Replia。&lt;/p&gt;

&lt;p&gt;多分区的设计的特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;为了并发读写，加快读写速度；&lt;/li&gt;
&lt;li&gt;是利用多分区的存储，利于数据的均衡；&lt;/li&gt;
&lt;li&gt;是为了加快数据的恢复速率，一但某台机器挂了，整个集群只需要恢复一部分数据，可加快故障恢复的时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个Partition分为多个Segment，每个Segment有.log和.index 两个文件，每个log文件承载具体的数据，每条消息都有一个递增的offset，Index文件是对log文件的索引，Consumer查找offset时使用的是二分法根据文件名去定位到哪个Segment，然后解析msg，匹配到对应的offset的msg。&lt;/p&gt;

&lt;p&gt;Partition recovery过程&lt;/p&gt;

&lt;p&gt;每个Partition会在磁盘记录一个RecoveryPoint,，记录已经flush到磁盘的最大offset。当broker 失败重启时，会进行loadLogs。首先会读取该Partition的RecoveryPoint，找到包含RecoveryPoint的segment及以后的segment， 这些segment就是可能没有完全flush到磁盘segments。然后调用segment的recover，重新读取各个segment的msg，并重建索引。每次重启KAFKA的broker时，都可以在输出的日志看到重建各个索引的过程。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据同步&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Producer和Consumer都只与Leader交互，每个Follower从Leader拉取数据进行同步。&lt;/p&gt;

&lt;p&gt;如上图所示，ISR是所有不落后的replica集合，不落后有两层含义：距离上次FetchRequest的时间不大于某一个值或落后的消息数不大于某一个值，Leader失败后会从ISR中随机选取一个Follower做Leader，该过程对用户是透明的。&lt;/p&gt;

&lt;p&gt;当Producer向Broker发送数据时,可以通过request.required.acks参数设置数据可靠性的级别。&lt;/p&gt;

&lt;p&gt;此配置是表明当一次Producer请求被认为完成时的确认值。特别是，多少个其他brokers必须已经提交了数据到它们的log并且向它们的Leader确认了这些信息。&lt;/p&gt;

&lt;p&gt;典型的值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;0： 表示Producer从来不等待来自broker的确认信息。这个选择提供了最小的时延但同时风险最大（因为当server宕机时，数据将会丢失）。&lt;/li&gt;
&lt;li&gt;1：表示获得Leader replica已经接收了数据的确认信息。这个选择时延较小同时确保了server确认接收成功。&lt;/li&gt;
&lt;li&gt;-1：Producer会获得所有同步replicas都收到数据的确认。同时时延最大，然而，这种方式并没有完全消除丢失消息的风险，因为同步replicas的数量可能是1。如果你想确保某些replicas接收到数据，那么你应该在Topic-level设置中选项min.insync.replicas设置一下。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;仅设置 acks= -1 也不能保证数据不丢失,当ISR列表中只有Leader时,同样有可能造成数据丢失。要保证数据不丢除了设置acks=-1，还要保证ISR的大小大于等于2。&lt;/p&gt;

&lt;p&gt;具体参数设置：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;request.required.acks:设置为-1 等待所有ISR列表中的Replica接收到消息后采算写成功。&lt;/li&gt;
&lt;li&gt;min.insync.replicas: 设置为&amp;gt;=2,保证ISR中至少两个Replica。&lt;/li&gt;
&lt;li&gt;Producer：要在吞吐率和数据可靠性之间做一个权衡。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;KAFKA作为现代消息中间件中的佼佼者，以其速度和高可靠性赢得了广大市场和用户青睐，其中的很多设计理念都是非常值得我们学习的，本文所介绍的也只是冰山一角，希望能够对大家了解KAFKA有一定的作用。&lt;/p&gt;

&lt;h1 id=&#34;kafka的应用场景&#34;&gt;Kafka的应用场景&lt;/h1&gt;

&lt;h2 id=&#34;kafka用作消息系统&#34;&gt;&lt;strong&gt;Kafka用作消息系统&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Kafka流的概念与传统企业消息系统有什么异同？&lt;/p&gt;

&lt;p&gt;传统消息系统有两个模型：队列和发布-订阅系统。在队列模式中，每条服务器的消息会被消费者池中的一个所读取；而发布-订阅系统中消息会广播给所有的消费者。这两种模式各有优劣。队列模式的优势是可以将消息数据让多个消费者处理以实现程序的可扩展，然而这就导致其没有多个订阅者，只能用于一个进程。发布-订阅模式的好处在于数据可以被多个进程消费使用，但是却无法使单一程序扩展性能&lt;/p&gt;

&lt;p&gt;Kafka中消费者组的概念同时涵盖了这两方面。对应于队列的概念，Kafka中每个消费者组中有多个消费者实例可以接收消息；对应于发布-订阅模式，Kafka中可以指定多个消费者组来订阅消息&lt;/p&gt;

&lt;p&gt;相对传统消息系统，Kafka可以提供更强的顺序保证&lt;/p&gt;

&lt;h2 id=&#34;kafka用作存储系统&#34;&gt;&lt;strong&gt;Kafka用作存储系统&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;任何发布消息与消费消息解耦的消息队列其实都可以看做是用来存放发布的消息的存储系统，而Kafka是一个非常高效的存储系统&lt;/p&gt;

&lt;p&gt;写入Kafka的数据会被存入磁盘并且复制到集群中以容错。Kafka允许生产者等待数据完全复制并且确保持久化到磁盘的确认应答&lt;/p&gt;

&lt;p&gt;Kafka使用的磁盘结构扩容性能很好——不管服务器上有50KB还是50TB，Kafka的表现都是一样的&lt;/p&gt;

&lt;p&gt;由于能够精致的存储并且供客户端程序进行读操作，你可以把Kafka看做是一个用于高性能、低延迟的存储提交日志、复制及传播的分布式文件系统&lt;/p&gt;

&lt;h2 id=&#34;kafka的流处理&#34;&gt;&lt;strong&gt;Kafka的流处理&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;仅仅读、写、存储流数据是不够的，Kafka的目的是实现实时流处理。&lt;/p&gt;

&lt;p&gt;在Kafka中一个流处理器的处理流程是首先持续性的从输入主题中获取数据流，然后对其进行一些处理，再持续性地向输出主题中生产数据流。例如一个销售商应用，接收销售和发货量的输入流，输出新订单和调整后价格的输出流&lt;/p&gt;

&lt;p&gt;可以直接使用producer和consumer API进行简单的处理。对于复杂的转换，Kafka提供了更强大的Streams API。可构建聚合计算或连接流到一起的复杂应用程序&lt;/p&gt;

&lt;p&gt;流处理有助于解决这类应用面临的硬性问题：处理无序数据、代码更改的再处理、执行状态计算等&lt;/p&gt;

&lt;p&gt;Streams API所依托的都是Kafka的核心内容：使用producer和consumer API作为输入，使用Kafka作为状态存储，在流处理实例上使用相同的组机制来实现容错&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 推荐系统</title>
          <link>https://kingjcy.github.io/post/architecture/recommend/</link>
          <pubDate>Thu, 04 Jul 2019 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/recommend/</guid>
          <description>&lt;p&gt;推荐系统主要依赖于算法，是将大数据进行分析后得到一个想要结果，进行评分推荐，其实和搜索系统有异曲同工之妙。&lt;/p&gt;

&lt;h1 id=&#34;offline&#34;&gt;offline&lt;/h1&gt;

&lt;p&gt;线下推荐子系统又主要分为线下挖掘模块、数据管理工具两大部分。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;挖掘模块&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/recommend/reconmend.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;线下挖掘模块，是各类线下挖掘算法实施的核心，它读取各种数据源，运用各种算法实施线下数据挖掘，产出初步的挖掘结果，并将挖掘结果以一定格式保存下来。典型的，实施这些挖掘策略的是一些跑在hadoop平台上的job，并行实施策略，并将挖掘结果保存到hadoop上。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据管理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数据管理工具，即DataMgrTools，它是一个工具（或者服务），它能够接受一些管理命令，读取某些特定格式的线下数据，将这些数据实时或者周期性的打到线上的redis或者内存中，供线上服务读取。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/recommend/recommend1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;定义好线下数据格式，线上数据格式，通过上下游API做数据的迁移和转换。&lt;/p&gt;

&lt;p&gt;数据管理工具是一个与业务无关的通用工具，它需要支持多种特定格式数据的上传，因为线下挖掘模块产出的数据可能存储在文件里，HDFS上，数据库里，甚至是特定二进制数据。&lt;/p&gt;

&lt;h1 id=&#34;online&#34;&gt;online&lt;/h1&gt;

&lt;p&gt;线上推荐子系统主要分为展示服务、分流服务、推荐内核、策略module服务等几个部分。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;展示服务，或者说是接入服务，它是整个推荐系统线上部分的入口，即整个推荐系统的接入层，它向上游提供接口，供上游业务方调用。&lt;/li&gt;
&lt;li&gt;分流服务，它是推荐系统中一个非常有特色也非常重要的一个服务，它的作用是将上游过来的请求，按照不同的策略，以不同的比例，分流到不同的推荐算法实验平台（也就是下游的推荐内核）中去。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;推荐内核，是各类线上推荐算法实施的核心，它其实只是一个通用的实验平台容器，每个推荐服务内部可能跑的是不同类型的推荐算法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;虽然推荐服务中跑着不同的推荐算法，但每个算法的实施步骤都是相同的，都需要经过：

（1）预处理；
（2）预分析；
（3）去重过滤；
（4）排序；
（5）推荐解释；

等五个步骤，每个步骤都可能存在多种不同的算法，不同的模型，各个步骤中的一种算法组合起来，完成一个完整的流程，构成一个“推荐算法实验平台”。
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;策略服务，又叫策略module服务，它实现了一个个推荐内核下游的推荐module。在推荐内核执行各个推荐步骤时，每个步骤中都可能存在不同的算法/策略，这些算法/和策略可能需要调用一些和策略绑定比较紧密的module服务，它们并不是通用服务，而是相对专有的服务。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/recommend/recommend2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实就是收集数据，数据处理，过滤，打分，推荐。&lt;/p&gt;

&lt;h1 id=&#34;推荐系统&#34;&gt;推荐系统&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;首页推荐：提取用户画像，根据线下提取出的用户年龄、性别、品类偏好等在首页综合推荐宝贝&lt;/li&gt;
&lt;li&gt;宝贝详情页推荐：买了还买，看了还看类的关联宝贝推荐&lt;/li&gt;
&lt;li&gt;附近推荐：和首页推荐的差异在于，提高了地理位置的权重，地理位置不仅要包含当前地理位置，还需要包含常见活跃区域，例如家里、公司等&lt;/li&gt;
&lt;li&gt;搜索推荐：除了关键词全匹配，要考虑同义词、近义词、易错词、拼音等推荐，产品层面，提示“你是不是想找xxoo宝贝”&lt;/li&gt;
&lt;li&gt;召回推荐：在用户退出系统后，通过RFM模型做优惠券推送或者消息推送做客户挽留与召回&lt;/li&gt;
&lt;li&gt;列表页推荐：用户既然进入到了美甲，成交意愿是非常强烈的，首页的推荐至关重要&lt;/li&gt;
&lt;li&gt;宝贝详情页推荐：买了还买，看了还看类的关联宝贝推荐&lt;/li&gt;
&lt;li&gt;下单成功页推荐：既然下单了某个甲样，可能会喜欢相近的甲样哟&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus blackbox_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/blackbox_exporter/</link>
          <pubDate>Wed, 03 Jul 2019 10:10:09 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/blackbox_exporter/</guid>
          <description>&lt;p&gt;blackbox主要是用这个探针去探测其他机器的网络情况，比如可以使用icmp协议来完成ping其他机器监控的任务，可以使用http协议来完成url探测的功能。&lt;/p&gt;

&lt;h1 id=&#34;实现原理&#34;&gt;实现原理&lt;/h1&gt;

&lt;p&gt;1、实现probe接口，然后根据不同的协议，走不通的实现&lt;/p&gt;

&lt;p&gt;2、icmp实现过程，首先就是使用socket请求，等待返回，icmp不存在几次握手连接的，只是发送请求等待返回（改造，实现ping丢包率：使用for循环发送十次请求，根据返回情况和时间来计算丢包率和延时）&lt;/p&gt;

&lt;p&gt;3、http协议，就是发送http请求，解析返回码&lt;/p&gt;

&lt;h1 id=&#34;使用注意&#34;&gt;使用注意&lt;/h1&gt;

&lt;p&gt;1、有的url返回容易超时，默认是1S，可以设置在配置文件中超时时间&lt;/p&gt;

&lt;p&gt;2、有的URL需要验证，则使用指定秘要文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http_2xx:
prober: http
timeout: 20000000000
http:
  tls_config:
    ca_file: /opt/promes/exporter/blackbox_exporter/blackbox_exporter_v0.12.1_linux-amd64/ssl/cargo.crt
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算CDCI系列---- harbor</title>
          <link>https://kingjcy.github.io/post/cloud/paas/cdci/harbor/</link>
          <pubDate>Sun, 23 Jun 2019 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/cdci/harbor/</guid>
          <description>&lt;p&gt;Harbor是由VMware公司开源的企业级的Docker Registry管理项目，它包括权限管理(RBAC)、LDAP、日志审核、管理界面、自我注册、镜像复制和中文支持等功能。harbor项目是由VMware中国研发的团队负责开发，所以对国内非常友好。&lt;/p&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;p&gt;安装Harbor，需要提前安装docker，和docker-compose，这边就不详细叙述了。当然这是单独部署harbor，也可以部署在k8s中，也可以使用&lt;a href=&#34;https://github.com/goharbor/harbor-helm&#34;&gt;helm部署&lt;/a&gt;到k8s中，都是差不多的。&lt;/p&gt;

&lt;p&gt;1、下载installer&lt;/p&gt;

&lt;p&gt;其实就是去harbor的github的release上下载一个版本：&lt;a href=&#34;https://github.com/goharbor/harbor/releases&#34;&gt;https://github.com/goharbor/harbor/releases&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget &#39;https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.4-rc1.tgz&#39; .
tar -zxvf harbor-offline-installer-v1.8.4-rc1.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、修改配置&lt;/p&gt;

&lt;p&gt;下载下来之后解压缩，目录下会有harbor.conf，就是Harbor的配置文件了，修改hostname,harbor_admin_password。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#harbor.yml
cat harbor.yml |grep -v &#39;#&#39; |grep -v &#39;^$&#39;
hostname: registry.test.myop.com
http:
  port: 80
harbor_admin_password: Harbor12345
database:
  password: root123
data_volume: /data1/harbor
clair:
  updaters_interval: 12
  http_proxy:
  https_proxy:
  no_proxy: 127.0.0.1,localhost,core,registry
jobservice:
  max_job_workers: 10
chart:
  absolute_url: disabled
log:
  level: info
  rotate_count: 50
  rotate_size: 200M
  location: /var/log/harbor
_version: 1.8.0

#修改docker-compose.yml, 把 ports改为5000.
vim docker-compose.yml,
   dns_search:
   ports:
      - 5000:5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置文件详解&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;主要参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、hostname：目标主机的主机名，用于访问UI和注册表服务。它应该是目标计算机的IP地址或完全限定域名（FQDN），例如192.168.1.10或reg.yourdomain.com。不要使用localhost或127.0.0.1用于主机名 - 注册表服务需要由外部客户端访问！
2、ui_url_protocol：（http或https。默认为http）用于访问UI和令牌/通知服务的协议。默认情况下，这是http。要设置https协议，如果启用了认证，则此参数必须为https。
3、db_password：用于db_auth的MySQL数据库的根密码。更改此密码以用于任何生产使用！
4、max_job_workers：（缺省值为3）作业服务中的最大复制worker数。对于每个图像复制作业，工作程序将存储库的所有标记同步到远程目标。增加此数量允许系统中更多的并发复制作业。但是，由于每个工人消耗一定量的网络/ CPU / IO资源，请根据主机的硬件资源仔细选择此属性的值。
5、customize_crt：（开启或关闭，默认为开启），如果此属性开启，在准备脚本创建注册表的令牌生成/验证私钥和根证书。当外部源提供密钥和根证书时，将此属性设置为off。以下属性：crt_country，crt_state，crt_location，crt_organization，crt_organizationalunit，crt_commonname，crt_email用作生成密钥的参数。当密钥和根证书由外部源提供时，将此属性设置为off。
6、ssl_cert：SSL证书的路径，仅在协议设置为https时应用。
7、ssl_cert_key：SSL密钥的路径，仅在协议设置为https时应用。
8、secretkey_path：用于加密或解密复制策略中远程注册表密码的密钥路径。
9、log_rotate_count：日志文件在被删除之前会被轮询log_rotate_count次数。如果count为0，则删除旧版本而不是轮询。
10、log_rotate_size：仅当日志文件大于log_rotate_size字节时才会轮换日志文件。如果大小后跟k，则大小以千字节为单位。如果使用M，则大小以兆字节为单位，如果使用G，则大小为千兆字节。
11、harbour_admin_password：管理员的初始密码。此密码仅在harbor首次发布时生效。之后，将忽略此设置，并且应在UI中设置管理员的密码。请注意，默认用户名/密码为admin / Harbor12345。
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可选参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、电子邮件设置：Harbor需要这些参数才能向用户发送“密码重置”电子邮件，并且只有在需要该功能时才需要。还有，千万注意，在默认情况下SSL连接是没有启用-如果你的SMTP服务器需要SSL，但不支持STARTTLS，那么你应该通过设置启用SSL email_ssl = TRUE。
    email_server = smtp.mydomain.com
    email_server_port = 25
    email_username = sample_admin@mydomain.com
    email_password = abc
    email_from = admin \&amp;lt;sample_admin@mydomain.com\&amp;gt;
    email_ssl = false

2、auth_mode：使用的认证类型。默认情况下，它是db_auth，即凭据存储在数据库中。对于LDAP认证，请将其设置为ldap_auth。
    ldap_url：LDAP端点URL（例如ldaps://ldap.mydomain.com）。 仅当auth_mode设置为ldap_auth时使用。
    ldap_searchdn：具有搜索LDAP / AD服务器（例如uid=admin,ou=people,dc=mydomain,dc=com）的权限的用户的DN 。
    ldap_search_pwd：由指定的用户的密码ldap_searchdn。
    ldap_basedn：查找用户的基本DN，例如ou=people,dc=mydomain,dc=com。 仅当auth_mode设置为ldap_auth时使用。
    ldap_filter：用于查找用户的搜索过滤器，例如(objectClass=person)。
    ldap_uid：用于在LDAP搜索期间匹配用户的属性，可以是uid，cn，电子邮件或其他属性。
    ldap_scope：用于搜索用户的范围，1-LDAP_SCOPE_BASE，2-LDAP_SCOPE_ONELEVEL，3-LDAP_SCOPE_SUBTREE。默认值为3。
3、self_registration：（on或off。默认为on）启用/禁用用户注册自己的能力。禁用时，新用户只能由管理员用户创建，只有管理员用户才能在Harbor中创建新用户。 注意：当auth_mode设置为ldap_auth时，将始终禁用自注册功能，并且将忽略此标志。
4、use_compressed_js：（on或off。默认为on）对于生产使用，将此标志设置为on。在开发模式下，将其设置为off，以便可以单独修改js文件。
5、token_expiration：令牌服务创建的令牌的过期时间（以分钟为单位），默认为30分钟。
6、verify_remote_cert：（on或off。默认为on）此标志确定当Harbor与远程注册表实例通信时是否验证SSL / TLS证书。将此属性设置为关闭将绕过SSL / TLS验证，这通常在远程实例具有自签名或不受信任的证书时使用。
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;后端存储调整&lt;/p&gt;

&lt;p&gt;主要在common/templates/registry/config.yml文件，这块其实就是去改registry的配置文件。改完之后prepare一下，然后docker-compse up -d就可以了。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、安装&lt;/p&gt;

&lt;p&gt;直接执行安装脚本就行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#执行安装脚本
sh ./install.sh
[Step 0]: checking installation environment ...
Note: docker version: 18.06.1
Note: docker-compose version: 1.24.1

[Step 1]: loading Harbor images ...
b80136ee24a4: Loading layer [==================================================&amp;gt;]  34.25MB/34.25MB

[Step 2]: preparing environment ...
prepare base dir is set to /data1/software/harbor
Generated configuration file: /config/log/logrotate.conf
Generated configuration file: /config/nginx/nginx.conf
Generated configuration file: /config/core/env
Generated configuration file: /config/core/app.conf
Generated configuration file: /config/registry/config.yml
Generated configuration file: /config/registryctl/env
Generated configuration file: /config/db/env
Generated configuration file: /config/jobservice/env
Generated configuration file: /config/jobservice/config.yml
Generated and saved secret to file: /secret/keys/secretkey
Generated certificate, key file: /secret/core/private_key.pem, cert file: /secret/registry/root.crt
Generated configuration file: /compose_location/docker-compose.yml
Clean up the input dir


[Step 3]: starting Harbor ...
Creating network &amp;quot;harbor_harbor&amp;quot; with the default driver
Creating harbor-log ... done
Creating redis       ... done
Creating harbor-db   ... done
Creating registry    ... done
Creating registryctl ... done
Creating harbor-core ... done
Creating harbor-jobservice ... done
Creating harbor-portal     ... done
Creating nginx             ... done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装启动后可以查看相关组件的运行情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# docker ps
CONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS                            PORTS                                                              NAMES
248ae75cf72b        vmware/nginx-photon:v1.4.0             &amp;quot;nginx -g &#39;daemon of…&amp;quot;   4 seconds ago       Up 3 seconds                      0.0.0.0:80-&amp;gt;80/tcp, 0.0.0.0:443-&amp;gt;443/tcp, 0.0.0.0:4443-&amp;gt;4443/tcp   nginx
2f4278759096        vmware/harbor-jobservice:v1.4.0        &amp;quot;/harbor/start.sh&amp;quot;       4 seconds ago       Up 4 seconds (health: starting)                                                                      harbor-jobservice
5977ecfd082b        vmware/harbor-ui:v1.4.0                &amp;quot;/harbor/start.sh&amp;quot;       5 seconds ago       Up 4 seconds (health: starting)                                                                      harbor-ui
ff6fc31844a9        vmware/harbor-db:v1.4.0                &amp;quot;/usr/local/bin/dock…&amp;quot;   5 seconds ago       Up 3 seconds (health: starting)   3306/tcp                                                           harbor-db
2ed6ff381ab9        vmware/harbor-adminserver:v1.4.0       &amp;quot;/harbor/start.sh&amp;quot;       5 seconds ago       Up 4 seconds (health: starting)                                                                      harbor-adminserver
d3e1e93bce1b        vmware/registry-photon:v2.6.2-v1.4.0   &amp;quot;/entrypoint.sh serv…&amp;quot;   5 seconds ago       Up 4 seconds (health: starting)   5000/tcp                                                           registry
096310feb030        vmware/harbor-log:v1.4.0               &amp;quot;/bin/sh -c /usr/loc…&amp;quot;   6 seconds ago       Up 5 seconds (health: starting)   127.0.0.1:1514-&amp;gt;10514/tcp                                          harbor-log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、测试访问&lt;/p&gt;

&lt;p&gt;安装完毕后可以测试访问页面： registry.test.myop.com 账号默认是admin，密码默认Harbor12345，这时候就能通过harbor的基本界面进行操作了。&lt;/p&gt;

&lt;h2 id=&#34;基本使用&#34;&gt;基本使用&lt;/h2&gt;

&lt;p&gt;1、启停harbor&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker-compose down -v
$ vim harbor.yml
$ sudo prepare
$ sudo docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是docker-compose中的命令，k8s中一样使用kubectl或者helm就可以了。&lt;/p&gt;

&lt;p&gt;2、https配置&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;修改配置文件sudo vim harbor.cfg&lt;/p&gt;

&lt;p&gt;将hostname更改为xxxxxx.com,ui_url_protocol更改为https方式。&lt;/p&gt;

&lt;p&gt;将ssl_cert以及ssl_cert_key的名字更改为你要生成证书的名字。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;生成证书&lt;/p&gt;

&lt;p&gt;官方文档有Harbor生成证书的说明，直接照做就好。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;生成CA证书
 openssl req \
    -newkey rsa:4096 -nodes -sha256 -keyout ca.key \
    -x509 -days 365 -out ca.crt
生成证书签名
openssl req \
    -newkey rsa:4096 -nodes -sha256 -keyout yourdomain.com.key \
    -out yourdomain.com.csr
FQDN方式生成注册表主机的证书
openssl x509 -req -days 365 -in yourdomain.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out yourdomain.com.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上yourdomain.com替换为要使用的FQDN必须和harbor中的hostname以及ssl_cert配置相同。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;证书配置以及安装&lt;/p&gt;

&lt;p&gt;获取yourdomain.com.crt和yourdomain.com.key文件后，可以将它们放入如下目录/root/cert/（我将放在/data/cert目录下）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp yourdomain.com.crt /data/cert/
cp yourdomain.com.key /data/cert/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为Harbor生成配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo ./prepare
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后重启Harbor：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、登陆出错&lt;/p&gt;

&lt;p&gt;Harbor是搭建完成了，在我们上传项目时可能会出现一些问题,在另外一个服务器(client)登录harbor,会出错!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$docker login  registry.test.myop.com
Error response from daemon: Get https://registry.test.myop.com/v2/: dial tcp registry.test.myop.com:443: connect: connection refused
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是因为docker1.3.2版本开始默认docker registry使用的是https，我们设置Harbor默认http方式，所以当执行用docker login、pull、push等命令操作非https的docker regsitry的时就会报错。&lt;/p&gt;

&lt;p&gt;解决方法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;编辑harbor及client机器的docker配置文件/etc/docker/daemon.json&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim /etc/docker/daemon.json
{
 &amp;quot;insecure-registries&amp;quot;: [
 &amp;quot;harbor_ip or harbor_domain&amp;quot;
 ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重启docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#reload docker
systemctl daemon-reload
#docker ps |grep -v CONTAINER |awk &#39;{print $1}&#39;&amp;gt;docker_online.txt
#cat docker_online.txt  |while read line; do echo &amp;quot;$line&amp;quot;; docker start $line; done;
#systemctl start docker #服务会停止,使用reload较好。
systemctl reload docker
systemctl status docker.service -l
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;登录仓库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker login registry.test.myop.com
Username: admin
Password:
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、镜像管理&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;配置http镜像仓库可信任&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/docker/daemon.json
{&amp;quot;insecure-registries&amp;quot;:[&amp;quot;registry.test.myop.com&amp;quot;]}
systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打标签&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker tag centos:6 registry.test.myop.com/library/centos:6
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;上传&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker push registry.test.myop.com/library/centos:6
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;下载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker pull registry.test.myop.com/library/centos:6
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#推送之前先登录Harbor
docker login docker login registry.test.myop.com
admin
Harbor12345
提示success登录成功

查看自己有哪些镜像;docker images
把需要上传到Harbor的镜像运行如下命令就可以了
#镜像打标签
docker tag 镜像名:标签 私服地址/仓库项目名/镜像名:标签

#推送到私服
docker push  私服地址/仓库项目名/镜像名：标签

#从私服拉取镜像
docker pull 私服地址/仓库项目名/镜像名：标签
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、用户账户&lt;/p&gt;

&lt;p&gt;Harbor支持两种身份验证方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Database(db_auth)： 这种情况下，所有用户被存放在一个本地数据库&lt;/p&gt;

&lt;p&gt;当注册或添加一个新的用户到Harbor中时，Harbor系统中的用户名、email必须是唯一的。密码至少要有8个字符长度，并且至少要包含一个大写字母(uppercase letter)、一个小写字母(lowercase letter)以及一个数字(numeric character)。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;LDAP/Active Directory(ldap_auth): 在这种认证模式下，用户的credentials都被存放在外部的LDAP或AD服务器中，用户在那边完成认证后可以直接登录到Harbor系统。&lt;/p&gt;

&lt;p&gt;当一个LDAP/AD用户通过username和password的方式登录进系统时，Harbor会用LDAP Search DN及LDAP Search Password绑定LDAP/AD服务器（请参看installation guide)。假如成功的话，Harbor会在LDAP的LDAP Base DN目录及其子目录来查询该用户。通过LDAP UID指定的一些属性(比如: uid、cn)会与username一起共同来匹配一个用户。假如被成功匹配，用户的密码会通过一个发送到LDAP/AD服务器的bind request所验证。假如LDAP/AD服务器使用自签名证书(self-signed certificate)或者不受信任的证书的话，请不要检查LDAP Verify Cert&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;6、项目管理&lt;/p&gt;

&lt;p&gt;Harbor中的一个工程包含了一个应用程序所需要的所有repositories。在工程创建之前，并不允许推送镜像到Harbor中。Harbor对于project采用基于角色的访问控制。在Harbor中projects有两种类型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Public: 所有的用户都具有读取public project的权限， 其主要是为了方便你分享一些repositories&lt;/li&gt;
&lt;li&gt;Private: 一个私有project只能被特定用户以适当的权限进行访问&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在登录之后，你就可以创建一个工程(project)。默认情况下，创建的工程都是私有的，你可以通过在创建时选中Access Level复选框来使创建的工程变为public的。&lt;/p&gt;

&lt;p&gt;在项目中，你可以通过导航标签Logs选项卡来查看所有的日志，可以使用Configuration选项卡设置工程相关属性，可以使用member来新增和删除项目成员。&lt;/p&gt;

&lt;p&gt;7、权限&lt;/p&gt;

&lt;p&gt;Harbor基于角色的访问控制，与project关联的角色简单地分为Guest/Developer/Admin三类，角色/project/镜像三者之间进行关联，不同角色的权限不同，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;权限说明&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;角色  权限说明
Guest   对于指定项目拥有只读权限
Developer   对于指定项目拥有读写权限
ProjectAdmin    除了读写权限，同时拥有用户管理/镜像扫描等管理权限
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以到数据库中看一下角色管理，Harbor的数据库的信息非常简单，从Access表中可以看到其将访问权限进行地划分&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MariaDB [registry]&amp;gt; select * from access;
+-----------+-------------+-------------------------------+
| access_id | access_code | comment                       |
+-----------+-------------+-------------------------------+
|         1 | M           | Management access for project |
|         2 | R           | Read access for project       |
|         3 | W           | Write access for project      |
|         4 | D           | Delete access for project     |
|         5 | S           | Search access for project     |
+-----------+-------------+-------------------------------+
5 rows in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;项目级别的角色目前的5种细粒度的访问权限分别为：M/R/W/D/S&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;访问权限    说明
M   管理操作的权限
R   读操作的权限
W   写操作的权限
D   删除访问权限的权限
R   查询权限
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再来看项目权限和角色的关联&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MariaDB [registry]&amp;gt; select * from role;
+---------+-----------+-----------+--------------+
| role_id | role_mask | role_code | name         |
+---------+-----------+-----------+--------------+
|       1 |         0 | MDRWS     | projectAdmin |
|       2 |         0 | RWS       | developer    |
|       3 |         0 | RS        | guest        |
+---------+-----------+-----------+--------------+
3 rows in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很明显的就能看出对应的角色的权限，除此之外，还有两种系统级别的角色，在harbor创建的时候就被创建出来了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;角色  权限说明
SysAdmin    具有最多的权限，除了以上提及的权限，可以跨项目操作，查询所有项目，设定某个用户作为管理员以及扫描策略等
Anonymous:  没有登录的用户被视作匿名用户。匿名用户对private的项目不具访问权限，对public的项目具有只读权限
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、镜像复制&lt;/p&gt;

&lt;p&gt;镜像复制被用于从一个Harbor实例向另一个Harbor实例复制repositories。，Harnor镜像复制可在不同的数据中心、不同的运行环境之间同步镜像，并提供友好的管理界面，大大简化了实际运维中的镜像管理工作.&lt;/p&gt;

&lt;p&gt;Harbor仍然以“项目”为中心， 通过对项目配置“复制策略”，标明需要复制的项目以及镜像。管理员在复制策略中指明目标实例，即复制的“目的地”，并对它的地址和连接时使用的用户名密码进行设置。当复制策略被激活时，源项目下的所有镜像，都会被复制到目标实例；此外，当源项目下的镜像被添加或删除（push或delete), 只要策略还在激活状态，镜像的变化都会同步到目标实例上去， 如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在较大的容器集群中，往往需要多个Registry服务器做负载均衡，可以采用主从发布模式，镜像只需要发布一次，就可以推送到多个Registry实例中。同时还支持双主复制和层次型的多级镜像发布，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;具体的搭建和使用这边就不多说了。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从架构图中可以看出，Harbor由6个大的模块所组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Proxy: Harbor的registry、UI、token services等组件，都处在一个反向代理后边。该代理将来自浏览器、docker clients的请求转发到后端服务上。&lt;/li&gt;
&lt;li&gt;Registry: 负责存储Docker镜像，以及处理Docker push/pull请求。因为Harbor强制要求对镜像的访问做权限控制， 在每一次push/pull请求时，Registry会强制要求客户端从token service那里获得一个有效的token。&lt;/li&gt;
&lt;li&gt;Core services: Harbor的核心功能，主要包括如下3个服务:

&lt;ul&gt;
&lt;li&gt;UI: 作为Registry Webhook, 以图像用户界面的方式辅助用户管理镜像。&lt;/li&gt;
&lt;li&gt;WebHook是在registry中配置的一种机制， 当registry中镜像发生改变时，就可以通知到Harbor的webhook endpoint。Harbor使用webhook来更新日志、初始化同步job等。&lt;/li&gt;
&lt;li&gt;Token service会根据该用户在一个工程中的角色，为每一次的push/pull请求分配对应的token。假如相应的请求并没有包含token的话，registry会将该请求重定向到token service。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Database 用于存放工程元数据、用户数据、角色数据、同步策略以及镜像元数据。&lt;/li&gt;
&lt;li&gt;Job services: 主要用于镜像复制，本地镜像可以被同步到远程Harbor实例上。&lt;/li&gt;
&lt;li&gt;Log collector: 负责收集其他模块的日志到一个地方&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们与运行的7个容器对比，多了一个harbor-adminserver主要是作为一个后端的配置数据管理，并没有太多的其他功能。harbor-ui所要操作的所有数据都通过harbor-adminserver这样一个数据配置管理中心来完成。&lt;/p&gt;

&lt;h2 id=&#34;其他组件&#34;&gt;其他组件&lt;/h2&gt;

&lt;p&gt;harbor的发展，有着很多其他的组件逐渐的成熟被需要，所以很多组件也被加入到了harbor生态中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;clair&#34;&gt;clair&lt;/h3&gt;

&lt;p&gt;harbor仓库中的镜像扫描这个功能，看似很高大上，其实等你了解了它的底层原理与流程，你就会发现就是做了那么一件事而已，用通俗的一句话概括，就是找到每个镜像文件系统中已经安装的软件包与版本，然后跟官方系统公布的信息比对，官方已经给出了在哪个系统版本上哪个软件版本有哪些漏洞，比如Debian 7系统上，nginx 1.12.1有哪些CVE漏洞，通过对逐个安装的软件包比对，就能知道当前这个镜像一共有多少CVE。&lt;/p&gt;

&lt;p&gt;镜像就是由许多Layer层组成的文件系统，重要的是每个镜像有一个manifest，这个东西跟springboot中的一个概念，就是文件清单的意思。一个镜像是由许多Layer组成，总需要这个manifest文件来记录下到底由哪几个层联合组成的。要扫描分析一个镜像，首先你就必须获取到这个镜像的manifest文件，通过manifest文件获取到镜像所有的Layer的地址digest，digest在docker镜像存储系统中代表的是一个地址，类似操作系统中的一个内存地址概念，通过这个地址，可以找到文件的内容，这种可寻址的设计是v2版本的重大改变。在docker hub储存系统中，所有文件都是有地址的，这个digest就是由某种高效的sha算法通过对文件内容计算出来的。&lt;/p&gt;

&lt;p&gt;clair是 coreos 开源的容器漏洞扫描工具，在容器逐渐普及的今天，容器镜像安全问题日益严重。clair 是目前少数的开源安全扫描工具，主要提供OS（centos，debian，ubuntu等）的软件包脆弱性扫描。clair的可以单机部署也可以部署到k8s上，可以与现有的registry集成。harbor 很好的整合了 clair ，通过简单的UI就可以对上传的镜像扫描，还可以通过每天的定时扫描对所有镜像进行统一扫描，架构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Clair主要包括以下模块：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;获取器（Fetcher）- 从公共源收集漏洞数据
检测器（Detector）- 指出容器镜像中包含的Feature
容器格式器（Image Format）- Clair已知的容器镜像格式，包括Docker，ACI
通知钩子（Notification Hook）- 当新的漏洞被发现时或者已经存在的漏洞发生改变时通知用户/机器
数据库（Databases）- 存储容器中各个层以及漏洞
Worker - 每个Post Layer都会启动一个worker进行Layer Detect
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体的部署官方也有，主要是用官方提供的镜像进行操作，目前也在发展中，可以集成到harbor中用启动参数进行启动，这边不多说。&lt;/p&gt;

&lt;p&gt;我们主要讲一下他的工作流程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Clair定期从配置的源获取漏洞元数据然后存进数据库。
客户端使用Clair API处理镜像，获取镜像的特征并存进数据库。
客户端使用Clair API从数据库查询特定镜像的漏洞情况，为每个请求关联漏洞和特征，避免需要重新扫描镜像。
当更新漏洞元数据时，将会有系统通知产生。另外，还有webhook用于配置将受影响的镜像记录起来或者拦截其部署。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体针对镜像的校验过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.UI向Job发起镜像扫描请求，参数中包含了仓库名称以及tag
2.Job收到请求之后，向registry发起一个Head请求(/v2/nginx/manifest/v1.12.1)，判断当前镜像的manifest是否存在，取出当前manifest的digest，这个digest是存放在响应头中的Docker-Content-Digest。
3.Job把第2步获取到的digest以及仓库名、tag作为一条记录插入job表中，job的状态为pending。
    这个时候Job系统则会新建一个扫描任务的job进行调度，这里则涉及到一个状态机处理流程。
4.Job系统通过manifest文件获取镜像的所有Layer digest，针对每一层，封装一个ClairLayer参数对象，然后根据层的数量，循环请求Clair系统，ClairLayer参数结构如下：
    Name:    sha256:7d99455a045a6c89c0dbee6e1fe659eb83bd3a19e171606bc0fd10eb0e34a7dc
    Headers: tokenHeader,
    Format:  &amp;quot;Docker&amp;quot;,
    Path:    http://registry:5000/v2/nginx/blobs/7d99455a045a6c89c0dbee6e1fe659eb83bd3a19e171606bc0fd10eb0e34a7dc
    ParentName: a55bba68cd4925f13c34562c891c8c0b5d446c7e3d65bf06a360e81b993902e1
5.Clair系统收到请求之后，根据ParentName首先校验父Layer是否存在，不存在则报错。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/clair&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们正常除了给容器做镜像扫描，还会将Clair可以集成到CI/CD管道中，如此一来当生成镜像时，将镜像推送到仓库之后触发Clair扫描该镜像的请求。 集成思路如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户推送镜像到容器仓库，仓库根据设置的黑白名单选择是否调用Clair进行扫描
一旦触发Clair扫描，则等待扫描结果返回，然后通知用户
如果发现漏洞，则CI也同时阻止CD流程启动，否则CD流程开启
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/clair1&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/clair2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;目前Docker Hub上的镜像上大部分都是存在漏洞的，所以安全扫描还是很有必要的。&lt;/p&gt;

&lt;h3 id=&#34;notary&#34;&gt;Notary&lt;/h3&gt;

&lt;p&gt;Notary是一套docker镜像的签名工具， 用来保证镜像在pull，push和传输工程中的一致性和完整性。避免中间人攻击，避免非法的镜像更新和运行。&lt;/p&gt;

&lt;p&gt;可以看见只有认证的签名的镜像才能进行pull，push，从而保证仓库的安全。具体可以查看&lt;a href=&#34;https://github.com/theupdateframework/notary&#34;&gt;官网&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;高可用&#34;&gt;高可用&lt;/h2&gt;

&lt;p&gt;目前有两种主流的方案来解决这个问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;双主复制
多harbor实例共享后端存储
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;双主复制&#34;&gt;双主复制&lt;/h3&gt;

&lt;p&gt;1、主从同步&lt;/p&gt;

&lt;p&gt;主从复制的原理我们已经在上面讲过了，harbor官方默认提供主从复制的方案来解决镜像同步问题，通过复制的方式，我们可以实时将测试环境harbor仓库的镜像同步到生产环境harbor，类似于如下流程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在实际生产运维的中，往往需要把镜像发布到几十或上百台集群节点上。这时，单个Registry已经无法满足大量节点的下载需求，因此要配置多个Registry实例做负载均衡。手工维护多个Registry实例上的镜像，将是十分繁琐的事情。Harbor可以支持一主多从的镜像发布模式，可以解决大规模镜像发布的难题：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor9&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;只要往一台Registry上发布，镜像就像“仙女散花”般地同步到多个Registry中，高效可靠。&lt;/p&gt;

&lt;p&gt;如果是地域分布较广的集群，还可以采用层次型发布方式，如从集团总部同步到省公司，从省公司再同步到市公司：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor10&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然而单靠主从同步，仍然解决不了harbor主节点的单点问题。&lt;/p&gt;

&lt;p&gt;2、双主复制&lt;/p&gt;

&lt;p&gt;所谓的双主复制其实就是复用主从同步实现两个harbor节点之间的双向同步，来保证数据的一致性，然后在两台harbor前端顶一个负载均衡器将进来的请求分流到不同的实例中去，只要有一个实例中有了新的镜像，就是自动的同步复制到另外的的实例中去，这样实现了负载均衡，也避免了单点故障，在一定程度上实现了Harbor的高可用性：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor11&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个方案有一个问题就是有可能两个Harbor实例中的数据不一致。假设如果一个实例A挂掉了，这个时候有新的镜像进来，那么新的镜像就会在另外一个实例B中，后面即使恢复了挂掉的A实例，Harbor实例B也不会自动去同步镜像，这样只能手动的先关掉Harbor实例B的复制策略，然后再开启复制策略，才能让实例B数据同步，让两个实例的数据一致。&lt;/p&gt;

&lt;p&gt;根据我的使用经验，在实际生产使用中，主从复制十分的不靠谱。所以一般企业都是会使用下面这种方案。&lt;/p&gt;

&lt;h3 id=&#34;多harbor实例共享后端存储&#34;&gt;多harbor实例共享后端存储&lt;/h3&gt;

&lt;p&gt;共享后端存储算是一种比较标准的方案，就是多个Harbor实例共享同一个后端存储，任何一个实例持久化到存储的镜像，都可被其他实例中读取。通过前置LB进来的请求，可以分流到不同的实例中去处理，这样就实现了负载均衡，也避免了单点故障：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor12&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个方案在实际生产环境中部署需要考虑三个问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、共享存储的选取，Harbor的后端存储目前支持AWS S3、Openstack Swift, Ceph等，在我们的测试环境里，都可以直接使用nfs
2、Session在不同的实例上共享，这个现在其实已经不是问题了，在最新的harbor中，默认session会存放在redis中，我们只需要将redis独立出来即可。可以通过redis sentinel或者redis cluster等方式来保证redis的可用性。在我们的实验环境里，仍然使用单台redis
3、Harbor多实例数据库问题，这个也只需要将harbor中的数据库拆出来独立部署即可。让多实例共用一个外部数据库，数据库的高可用也可以通过数据库的高可用方案保证。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;api&#34;&gt;API&lt;/h1&gt;

&lt;p&gt;Harbor 提供了一些列 API 用于镜像仓库的管理，完整的 API 文档可以查看 Harbor 的 Swagger文件：&lt;a href=&#34;https://raw.githubusercontent.com/goharbor/harbor/master/docs/swagger.yaml&#34;&gt;https://raw.githubusercontent.com/goharbor/harbor/master/docs/swagger.yaml&lt;/a&gt;  可以通过 Swagger 工具 &lt;a href=&#34;https://editor.swagger.io/&#34;&gt;https://editor.swagger.io/&lt;/a&gt; 在线查看，将上述文件内容黏贴进改工具的编辑框中即可。这边就不详细说明了，开发的时候直接查询就好。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;登陆&#34;&gt;登陆&lt;/h2&gt;

&lt;p&gt;在我们使用docker login harborip之后，Docker Client会发送一个HTTP GET请求到192.168.1.10/v2/地址处，Harbor的不同容器组件将会按照如下步骤进行处理：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、首先，该请求将会被监听在80端口上的代理容器所接收到。容器中的Nginx将会把该请求转发给后端的Registry容器&lt;/p&gt;

&lt;p&gt;2、由于registry容器已经被配置为基于token的认证，因此其会返回一个401错误码，用于通知docker客户端从一个指定的URL处获得一个有效的token。在Harbor中，该URL会指向Core service中的token service。&lt;/p&gt;

&lt;p&gt;3、当Docker Client接收到这个错误码，其就会发送一个请求到token service URL，会根据HTTP基本认证协议在请求头中内嵌username和password相关信息&lt;/p&gt;

&lt;p&gt;4、在该请求被发送到代理的80端口上后，Nginx会根据预先所配置的规则将请求转发到UI容器上。UI容器中的token service接收到该请求之后，其就会对该请求进行解码然后获得相应的用户名及密码&lt;/p&gt;

&lt;p&gt;5、在成功获得用户名及密码之后，token Service就会检查mysql数据库以完成用户的认证。当token service被配置为LDAP/AD认证的时候，其就会通过外部的LDAP/AD服务来完成认证。在成功认证之后，token Service就会返回一个认证成功的http code， Http body部分会返回一个通过private key所产生的token&lt;/p&gt;

&lt;p&gt;到这里为止，Docker login就处理完成。Docker client会将步骤&amp;copy;所产生的username及password编码后保存到一个隐藏的文件中&lt;/p&gt;

&lt;h2 id=&#34;上传&#34;&gt;上传&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/harbor/harbor5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;省略nginx转发，直接展示组件之间的交互&lt;/p&gt;

&lt;p&gt;1、首先，docker client执行类似登录时的流程发送一个请求到registry，然后返回一个token service的URL&lt;/p&gt;

&lt;p&gt;2、然后，docker client通过提供一些额外的信息与ui/token交互以获得push镜像library/hello-world的token&lt;/p&gt;

&lt;p&gt;3、在成功获得来自Nginx转发的请求之后，Token Service查询数据库以寻找用户推送镜像的角色及权限。假如用户有相应的权限，token service就会编码相应的push操作信息，并用一个private key进行签名。然后返回一个token给Docker client&lt;/p&gt;

&lt;p&gt;4、在docker client获得token之后，其就会发送一个push请求到registry，在该push请求的头中包含有上面返回的token信息。一旦registry收到了该请求，其就会使用public key来解码该token，然后再校验其内容。该public key对应于token service处的private key。假如registry发现该token有效，则会开启镜像的传输流程。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- Cache</title>
          <link>https://kingjcy.github.io/post/architecture/cache/</link>
          <pubDate>Sat, 15 Jun 2019 20:09:52 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/cache/</guid>
          <description>&lt;p&gt;缓存是一种提高系统读性能的常见技术，对于读多写少的应用场景，我们经常使用缓存来进行优化。&lt;/p&gt;

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;p&gt;我们在实际情况下总是遇到读多写少的场景，比如用户的余额信息表account(uid, money)，对于查询余额的需求，占99%，对于更改余额的需求只有1%，这个时候我们就要使用缓存来降低数据的压力，提高查询效率。&lt;/p&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;h2 id=&#34;读操作&#34;&gt;读操作&lt;/h2&gt;

&lt;p&gt;有了数据库和缓存两个地方存放数据之后（uid-&amp;gt;money），每当需要读取相关数据时（money），操作流程一般是这样的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;读取缓存中是否有相关数据，uid-&amp;gt;money&lt;/li&gt;
&lt;li&gt;如果缓存中有相关数据money，则返回【这就是所谓的数据命中“hit”】&lt;/li&gt;
&lt;li&gt;如果缓存中没有相关数据money，则从数据库读取相关数据money【这就是所谓的数据未命中“miss”】，放入缓存中uid-&amp;gt;money，再返回&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缓存的命中率 = 命中缓存请求个数/总缓存访问请求个数 = hit/(hit+miss)&lt;/p&gt;

&lt;h2 id=&#34;写操作&#34;&gt;写操作&lt;/h2&gt;

&lt;p&gt;写操作就比较复杂了，涉及三个问题&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;更新缓存 VS 淘汰缓存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;更新缓存（setCache(uid, money)）：数据不但写入数据库，还会写入缓存。一般数据获取并不是太复杂，我们都会更新缓存。&lt;/p&gt;

&lt;p&gt;淘汰缓存（deleteCache(uid)）：数据只会写入数据库，不会写入缓存，只会把数据淘汰掉，也就是删除。一般数据需要很复杂的获取方式，就会先把数据删除，然后在需要的时候再计算存入。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;先操作数据库 vs 先操作缓存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数据和缓存的操作时序，结论是清楚的：先淘汰缓存，再写数据库。&lt;/p&gt;

&lt;p&gt;假设先写数据库，再淘汰缓存,则会出现DB中是新数据，Cache中是旧数据，数据不一致。&lt;/p&gt;

&lt;p&gt;假设先淘汰缓存，再写数据库：第一步淘汰缓存成功，第二步写数据库失败，则只会引发一次Cache miss。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;数据不一致&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1、单库情况下，服务层的并发读写，缓存与数据库的操作交叉进行&lt;/p&gt;

&lt;p&gt;其实在先淘汰缓存，再写数据库中间还是会出现数据不一致的情况：就是在写入数据库之前，查询来一次，将数据存储到来缓存。&lt;/p&gt;

&lt;p&gt;遇到这种情况，我们最常想到的就是使用锁，但是如果使用全局锁的话影响很大，影响并发量，其实锁的思想就是串行化，我们可以通过相同的id走同一个服务实例和db连接来实现串行化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;修改服务Service连接池，id取模选取服务连接，能够保证同一个数据的读写都落在同一个后端服务上&lt;/li&gt;
&lt;li&gt;修改数据库DB连接池，id取模选取DB连接，能够保证同一个数据的读写在数据库层面是串行的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、主从同步，读写分离的情况下，读从库读到旧数据&lt;/p&gt;

&lt;p&gt;还有一种情况就是在主从同步，读写分离的架构情况下，如果查询数据的时候数据库主从同步还没有完成，导致数据不一致，这种架构还是我们常用的架构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请求A发起一个写操作，第一步淘汰了cache，如上图步骤1&lt;/li&gt;
&lt;li&gt;请求A写数据库了，写入了最新的数据，如上图步骤2&lt;/li&gt;
&lt;li&gt;请求B发起一个读操作，读cache，cache miss，如上图步骤3&lt;/li&gt;
&lt;li&gt;请求B继续读DB，读的是从库，此时主从同步还没有完成，读出来一个脏数据，然后脏数据入cache，如上图步4&lt;/li&gt;
&lt;li&gt;最后数据库的主从同步完成了，如上图步骤5&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在这种情况下，我们可以使用&amp;rdquo;缓存双淘汰”法：思想就是淘汰缓存两次，保证数据最新，第二次缓存什么时候淘汰就是一个关键，可以直接暴力的直接1s后再次删除缓存，但是这种方式需要等待，大大降低来并发，业务是接收不了的，所以还是需要异步完成。&lt;/p&gt;

&lt;p&gt;1、想到异步就想到MQ，所以我们可以通过mq来再次删除缓存&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、还没有使用日志来二次删除缓存，与业务解耦，对业务线完全没有入侵，比较推荐。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;缓存服务的优化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上述缓存架构有一个缺点：业务方需要同时关注缓存与DB，我们可以通过服务化来屏蔽数据的细节，实现解耦。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;加入一个服务层，向上游提供帅气的数据访问接口，向上游屏蔽底层数据存储的细节，这样业务线不需要关注数据是来自于cache还是DB。其实golang中同步的map也是这么一个逻辑。&lt;/p&gt;

&lt;p&gt;还可以通过异步缓存更新来实现&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;业务线所有的写操作都走数据库，所有的读操作都总缓存，由一个异步的工具来做数据库与缓存之间数据的同步。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;要有一个init cache的过程，将需要缓存的数据全量写入cache&lt;/li&gt;
&lt;li&gt;如果DB有写操作，异步更新程序读取binlog，更新cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样也可以，但是比较浪费资源，还用同步的逻辑需要好好处理。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;针对这种架构思想，最多实现的就是&lt;a href=&#34;https://kingjcy.github.io/post/database/mysql/redis-mysql/&#34;&gt;mysql+redis&lt;/a&gt;组合了，上面的问题解决方案都可以用到这组实现中。&lt;/p&gt;

&lt;h1 id=&#34;问题&#34;&gt;问题&lt;/h1&gt;

&lt;h2 id=&#34;缓存穿透&#34;&gt;缓存穿透&lt;/h2&gt;

&lt;p&gt;我们在项目中使用缓存通常都是先检查缓存中是否存在，如果存在直接返回缓存内容，如果不存在就直接查询数据库然后再缓存查询结果返回。这个时候如果我们查询的某一个数据在缓存中一直不存在，就会造成每一次请求都查询DB，这样缓存就失去了意义，在流量大时，可能DB就挂掉了。&lt;/p&gt;

&lt;p&gt;那这种问题有什么好办法解决呢？&lt;/p&gt;

&lt;p&gt;要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。&lt;/p&gt;

&lt;p&gt;有一个比较巧妙的作法是，可以将这个不存在的key预先设定一个值。&lt;/p&gt;

&lt;p&gt;比如，”key” , “&amp;amp;&amp;amp;”。&lt;/p&gt;

&lt;p&gt;在返回这个&amp;amp;&amp;amp;
值的时候，我们的应用就可以认为这是不存在的key，那我们的应用就可以决定是否继续等待继续访问，还是放弃掉这次操作。如果继续等待访问，过一个时间轮询点后，再次请求这个key，如果取到的值不再是&amp;amp;&amp;amp;，则可以认为这时候key有值了，从而避免了透传到数据库，从而把大量的类似请求挡在了缓存之中。&lt;/p&gt;

&lt;h2 id=&#34;缓存并发&#34;&gt;缓存并发&lt;/h2&gt;

&lt;p&gt;有时候如果网站并发访问高，一个缓存如果失效，可能出现多个进程同时查询DB，同时设置缓存的情况，如果并发确实很大，这也可能造成DB压力过大，还有缓存频繁更新的问题。&lt;/p&gt;

&lt;p&gt;我现在的想法是对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询。&lt;/p&gt;

&lt;p&gt;这种情况和刚才说的预先设定值问题有些类似，只不过利用锁的方式，会造成部分请求等待。&lt;/p&gt;

&lt;h2 id=&#34;缓存失效&#34;&gt;缓存失效&lt;/h2&gt;

&lt;p&gt;引起这个问题的主要原因还是高并发的时候，平时我们设定一个缓存的过期时间时，可能有一些会设置1分钟啊，5分钟这些，并发很高时可能会出在某一个时间同时生成了很多的缓存，并且过期时间都一样，这个时候就可能引发一当过期时间到后，这些缓存同时失效，请求全部转发到DB，DB可能会压力过重。&lt;/p&gt;

&lt;p&gt;那如何解决这些问题呢？
其中的一个简单方案就时讲缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。&lt;/p&gt;

&lt;p&gt;我们讨论的第二个问题时针对同一个缓存，第三个问题时针对很多缓存。&lt;/p&gt;

&lt;h2 id=&#34;总结-1&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;1、缓存穿透：查询一个必然不存在的数据。比如文章表，查询一个不存在的id，每次都会访问DB，如果有人恶意破坏，很可能直接对DB造成影响。&lt;/p&gt;

&lt;p&gt;2、缓存失效：如果缓存集中在一段时间内失效，DB的压力凸显。这个没有完美解决办法，但可以分析用户行为，尽量让失效时间点均匀分布。
当发生大量的缓存穿透，例如对某个失效的缓存的大并发访问就造成了缓存雪崩。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 冗余表</title>
          <link>https://kingjcy.github.io/post/architecture/redundanttable/</link>
          <pubDate>Fri, 14 Jun 2019 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/redundanttable/</guid>
          <description>&lt;p&gt;冗余表的架构设计就是牺牲空间一份数据存多张表，可以通过不同索引查询提高效率的一种架构思想。&lt;/p&gt;

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;p&gt;互联网很多业务场景的数据量很大，此时数据库架构要进行水平切分，水平切分会有一个patition key，通过patition key的查询能够直接定位到库，但是非patition key上的查询可能就需要扫描多个库了。&lt;/p&gt;

&lt;p&gt;例如订单表，业务上对用户和商家都有订单查询需求，如果用buyer_id来分库，seller_id的查询就需要扫描多库。如果用seller_id来分库，buyer_id的查询就需要扫描多库。这类需求，为了做到高吞吐量低延时的查询，往往使用“数据冗余”的方式来实现，同一个数据，冗余两份，一份以buyer_id来分库，满足买家的查询需求；一份以seller_id来分库，满足卖家的查询需求。&lt;/p&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;h2 id=&#34;服务同步写&#34;&gt;服务同步写&lt;/h2&gt;

&lt;p&gt;顾名思义，由服务层同步写冗余数据，先后向两个表中同时插入数据。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;不复杂，服务层由单次写，变两次写&lt;/li&gt;
&lt;li&gt;数据一致性相对较高（因为双写成功才返回）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请求的处理时间增加（要插入次，时间加倍）&lt;/li&gt;
&lt;li&gt;数据仍可能不一致，例如第二步写入T1完成后服务重启，则数据不会写入T2&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;服务异步写&#34;&gt;服务异步写&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据的双写并不再由服务来完成，服务层异步发出一个消息，通过消息总线发送给一个专门的数据复制服务来写入冗余数据。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请求处理时间短（只插入1次）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;系统的复杂性增加了，多引入了一个组件（消息总线）和一个服务（专用的数据复制服务）&lt;/li&gt;
&lt;li&gt;因为返回业务线数据插入成功时，数据还不一定插入到T2中，因此数据有一个不一致时间窗口（这个窗口很短，最终是一致的）&lt;/li&gt;
&lt;li&gt;在消息总线丢失消息时，冗余表数据会不一致&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;线下异步写&#34;&gt;线下异步写&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据的双写不再由服务层来完成，而是由线下的一个服务或者任务来读取数据的log来完成。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据双写与业务完全解耦&lt;/li&gt;
&lt;li&gt;请求处理时间短（只插入1次）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;返回业务线数据插入成功时，数据还不一定插入到T2中，因此数据有一个不一致时间窗口（这个窗口很短，最终是一致的）&lt;/li&gt;
&lt;li&gt;数据的一致性依赖于线下服务或者任务的可靠性&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;先写正表还是反表&#34;&gt;先写正表还是反表&lt;/h1&gt;

&lt;p&gt;上述三种方案各有优缺点，但不管哪种方案，都会面临“究竟先写T1还是先写T2”的问题，对于一个不能保证事务性的操作，一定涉及“哪个任务先做，哪个任务后做”的问题，解决这个问题的方向是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;【如果出现不一致】，谁先做对业务的影响较小，就谁先执行，需要根据业务逻辑来做处理。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比如还是对订单的业务，用户下单时，如果“先插入buyer表T1，再插入seller冗余表T2”，当第一步成功、第二步失败时，出现的业务影响是“买家能看到自己的订单，卖家看不到推送的订单”，相反，如果“先插入seller表T2，再插入buyer冗余表T1”，当第一步成功、第二步失败时，出现的业务影响是“卖家能看到推送的订单，卖家看不到自己的订单”，由于这个生成订单的动作是买家发起的，买家如果看不到订单，会觉得非常奇怪，并且无法支付以推动订单状态的流转，此时即使卖家看到有人下单也是没有意义的，因此，在此例中，应该先插入buyer表T1，再插入seller表T2。&lt;/p&gt;

&lt;h1 id=&#34;保证数据的一致性&#34;&gt;保证数据的一致性&lt;/h1&gt;

&lt;p&gt;不管哪种方案，因为两步操作不能保证原子性，总有出现数据不一致的可能，基本解决方案。&lt;/p&gt;

&lt;h2 id=&#34;线下扫面正反冗余表全部数据&#34;&gt;线下扫面正反冗余表全部数据&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;线下启动一个离线的扫描工具，不停的比对正表T1和反表T2，如果发现数据不一致，就进行补偿修复。这个是我们最容易想到的方法，也是最消耗资源的方法。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;比较简单，开发代价小&lt;/li&gt;
&lt;li&gt;线上服务无需修改，修复工具与线上服务解耦&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;扫描效率低，会扫描大量的“已经能够保证一致”的数据&lt;/li&gt;
&lt;li&gt;由于扫描的数据量大，扫描一轮的时间比较长，即数据如果不一致，不一致的时间窗口比较长&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;线下扫描增量数据&#34;&gt;线下扫描增量数据&lt;/h2&gt;

&lt;p&gt;有没有只扫描“可能存在不一致可能性”的数据，而不是每次扫描全部数据，每次只扫描增量的日志数据，就能够极大提高效率，缩短数据不一致的时间窗口。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然，我们还是需要一个离线的日志扫描工具，不停的比对日志log1和日志log2，如果发现数据不一致，就进行补偿修复&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;虽比方法一复杂，但仍然是比较简单的&lt;/li&gt;
&lt;li&gt;数据扫描效率高，只扫描增量数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;线上服务略有修改（代价不高，多写了2条日志）&lt;/li&gt;
&lt;li&gt;虽然比方法一更实时，但时效性还是不高，不一致窗口取决于扫描的周期&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;实时线上-消息对-检测&#34;&gt;实时线上“消息对”检测&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;有些系统要求比较高，需要实现实时的检测，所以就不能使用日志了，需要使用消息系统。假设正常情况下，msg1和msg2的接收时间应该在3s以内，如果检测服务在收到msg1后没有收到msg2，就尝试检测数据的一致性，不一致时进行补偿修复&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;效率高&lt;/li&gt;
&lt;li&gt;实时性高&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;方案比较复杂，上线引入了消息总线这个组件&lt;/li&gt;
&lt;li&gt;线下多了一个订阅总线的检测服务&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Runtime</title>
          <link>https://kingjcy.github.io/post/golang/go-runtime/</link>
          <pubDate>Thu, 13 Jun 2019 19:39:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-runtime/</guid>
          <description>&lt;p&gt;尽管 Go 编译器产生的是本地可执行代码，这些代码仍旧运行在 Go 的 runtime（这部分的代码可以在 runtime 包中找到）当中。这个 runtime 类似 Java 和 .NET 语言所用到的虚拟机，它负责管理包括内存分配、垃圾回收、栈处理、goroutine、channel、切片（slice）、map 和反射（reflection）等等。&lt;/p&gt;

&lt;h1 id=&#34;runtime&#34;&gt;runtime&lt;/h1&gt;

&lt;p&gt;runtime包含Go运行时的系统交互的操作，例如控制goruntine的功能，还有debug，pprof进行排查问题和运行时性能分析，tracer来抓取异常事件信息，总的来说运行时是调度器包括GC。&lt;/p&gt;

&lt;h2 id=&#34;调度器&#34;&gt;调度器&lt;/h2&gt;

&lt;p&gt;runtime 调度器是个非常有用的东西，关于 runtime 包几个方法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NumCPU：返回当前系统的 CPU 核数量&lt;/li&gt;
&lt;li&gt;GOMAXPROCS：设置最大的可同时使用的 CPU 核数，Golang 默认所有任务都运行在一个 cpu 核里，如果要在 goroutine 中使用多核，可以使用 runtime.GOMAXPROCS 函数修改，当参数小于 1 时使用默认值1。&lt;/li&gt;
&lt;li&gt;Gosched：让当前线程让出 cpu 以让其它线程运行,它不会挂起当前线程，因此当前线程未来会继续执行&lt;/li&gt;
&lt;li&gt;Goexit：退出当前 goroutine(但是defer语句会照常执行)&lt;/li&gt;
&lt;li&gt;NumGoroutine：返回正在执行和排队的任务总数&lt;/li&gt;
&lt;li&gt;GOOS：目标操作系统，可以查看目标操作系统&lt;/li&gt;
&lt;li&gt;runtime.GC：会让运行时系统进行一次强制性的垃圾收集&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;获取goroot和os&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//获取goroot目录：
fmt.Println(&amp;quot;GOROOT--&amp;gt;&amp;quot;,runtime.GOROOT())

//获取操作系统
fmt.Println(&amp;quot;os/platform--&amp;gt;&amp;quot;,runtime.GOOS) // GOOS--&amp;gt; darwin，mac系统
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取CPU数量，和设置CPU数量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init(){
    //1.获取逻辑cpu的数量
    fmt.Println(&amp;quot;逻辑CPU的核数：&amp;quot;,runtime.NumCPU())
    //2.设置go程序执行的最大的：[1,256]
    n := runtime.GOMAXPROCS(runtime.NumCPU())
    fmt.Println(n)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让出cpu&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    go func() {
        for i := 0; i &amp;lt; 5; i++ {
            fmt.Println(&amp;quot;goroutine。。。&amp;quot;)
        }
​
    }()
​
    for i := 0; i &amp;lt; 4; i++ {
        //让出时间片，先让别的协议执行，它执行完，再回来执行此协程
        runtime.Gosched()
        fmt.Println(&amp;quot;main。。&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;终止协程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    //创建新建的协程
    go func() {
        fmt.Println(&amp;quot;goroutine开始。。。&amp;quot;)
​
        //调用了别的函数
        fun()
​
        fmt.Println(&amp;quot;goroutine结束。。&amp;quot;)
    }() //别忘了()
​
    //睡一会儿，不让主协程结束
    time.Sleep(3*time.Second)
}
​
​
​
func fun() {
    defer fmt.Println(&amp;quot;defer。。。&amp;quot;)
​
    //return           //终止此函数
    runtime.Goexit() //终止所在的协程
​
    fmt.Println(&amp;quot;fun函数。。。&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列----VictoriaMetrics</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/</link>
          <pubDate>Thu, 13 Jun 2019 16:19:46 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/</guid>
          <description>&lt;p&gt;VictoriaMetrics是一个高性能的，长期存储的prometheus的远程解决方案，实现集群使用的federation的方式，只不过性能很优秀，包括write和query，聚合数据也解决了查询问题。&lt;/p&gt;

&lt;h1 id=&#34;优势&#34;&gt;优势&lt;/h1&gt;

&lt;p&gt;VictoriaMetrics不仅仅是时序数据库,它的优势主要体现在一下几点:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对外支持Prometheus相关的API，所以它可以直接用于Grafana作为Prometheus数据源使用, 同时扩展了PromQL, 详细使用可参考&lt;a href=&#34;https://github.com/VictoriaMetrics/VictoriaMetrics/wiki/ExtendedPromQL&#34;&gt;https://github.com/VictoriaMetrics/VictoriaMetrics/wiki/ExtendedPromQL&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;针对Prometheus的Metrics插入查询具备高性能和良好的扩展性。甚至性能比InfluxDB和TimescaleDB高出20x&lt;/li&gt;
&lt;li&gt;内存占用方面也做出了优化, 比InfluxDB少10x&lt;/li&gt;
&lt;li&gt;高性能的数据压缩方式,使存入存储的数据量比TimescaleDB多达70x&lt;/li&gt;
&lt;li&gt;优化了高延迟IO和低iops的存储&lt;/li&gt;
&lt;li&gt;操作简单&lt;/li&gt;
&lt;li&gt;支持从第三方时序数据库获取数据源&lt;/li&gt;
&lt;li&gt;异常关闭情况下可以保护存储数据损坏&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;部署&#34;&gt;部署&lt;/h1&gt;

&lt;h2 id=&#34;单点&#34;&gt;单点&lt;/h2&gt;

&lt;h3 id=&#34;编译&#34;&gt;编译&lt;/h3&gt;

&lt;p&gt;1、二进制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make victoria-metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make victoria-metrics-prod
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;启动&#34;&gt;启动&lt;/h3&gt;

&lt;p&gt;直接使用二进制文件进行启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup /opt/victoria-metrics/victoria-metrics-prod -storageDataPath=&amp;quot;/data/victoria&amp;quot; -retentionPeriod=2  &amp;gt;&amp;gt;/opt/promes/victoria-metrics/logs/start.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;-storageDataPath - path to data directory. VictoriaMetrics stores all the data in this directory.&lt;/li&gt;
&lt;li&gt;-retentionPeriod - retention period in months for the data. Older data is automatically deleted.&lt;/li&gt;
&lt;li&gt;-httpListenAddr - TCP address to listen to for http requests. By default it listens port 8428 on all the network interfaces.&lt;/li&gt;
&lt;li&gt;-graphiteListenAddr - TCP and UDP address to listen to for Graphite data. By default it is disabled.&lt;/li&gt;
&lt;li&gt;-opentsdbListenAddr - TCP and UDP address to listen to for OpenTSDB data. By default it is disabled.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见他也是一个时序数据库，支持将prometheus，influxdb，graphite，opentsdb的数据的写入，比如使用的是prometheus，只使用了http的端口，在我们对应的prometheus文件中配置远程写入，将数据写入到victoria-metrics中去，配置如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remote_write:
  - url: http://&amp;lt;victoriametrics-addr&amp;gt;:8428/api/v1/write
    queue_config:
      max_samples_per_send: 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数据存储到victoria-metrics，我们还是通过8428端口来读取，我们在grafana中配置datasource：&lt;a href=&#34;http://victoriametrics-addr-ip:8428&#34;&gt;http://victoriametrics-addr-ip:8428&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;停止&#34;&gt;停止&lt;/h3&gt;

&lt;p&gt;发送SIGINT给进程&lt;/p&gt;

&lt;h3 id=&#34;高可用&#34;&gt;高可用&lt;/h3&gt;

&lt;p&gt;启动多个实例，将prometheus的数据分别写入到这些节点中，加一层负载均衡，就可以实现高可用，解决单点问题，prometheus配置如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remote_write:
  - url: http://&amp;lt;victoriametrics-addr-1&amp;gt;:8428/api/v1/write
    queue_config:
      max_samples_per_send: 10000
  # ...
  - url: http://&amp;lt;victoriametrics-addr-N&amp;gt;:8428/api/v1/write
    queue_config:
      max_samples_per_send: 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边讲一下高可用和水平扩展&lt;/p&gt;

&lt;p&gt;高可用是指多活，解决单点故障，正常就是多个相同的服务同时提供服务，来确保一个节点挂了，就能转移到其他的节点上，不影响外部整体的使用，比如redis的主备切换，sentinel机制，还有上面的virtoria-metrics的方式&lt;/p&gt;

&lt;p&gt;水平扩展是一种分布式的能力，一个节点不能处理，就多个节点一起处理，这样分担一下，整体的量就上去了，比如redis的cluster集群，理论上只要加节点，就可以存储月来越多的数据，实际集群内部交互还是有瓶颈的&lt;/p&gt;

&lt;p&gt;正常的服务，可以说在集群同时解决高可用和水平扩展是很困难的，正常的一个集群的作用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;集群内主节点都获取全部数据，然后其他节点都重主节点复制数据，对外一直提供主节点查询，当主节点出现问题的时候，主备切换，这样实现了高可用，但是有单节点数量瓶颈，不能水平扩展。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群内每个节点获取一部分数据，然后集群内节点相互复制，实现最终一致性，每个节点都保存完整的数据，这个时候一个节点挂了，会出问题，单个节点也会有瓶颈，所以在这个基础上收取前加一层负载均衡，这样当一个节点挂了之后，负载均衡会分配到其他节点上，这样实现了高可用，也实现了水平扩展，但是这个很难实现，而且还是有单节点瓶颈，一般是适用这种数据量很小的需要一致性的服务发现。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群内每个节点获取一部分数据，并且只存储这一部分数据，然后集群内使用一些数据库或者自身实现关系映射，然后对外查询会路由到对应的节点上去查询数据。这种模式就是支持水平扩展的，但是有一个节点
出问题，查询就会出问题，没有实现高可用，所以在这个基础上实现高可用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个单节点的设置主从复制，相互切换&lt;/li&gt;
&lt;li&gt;完成集群间的复制&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后可以说是目前比较好的解决方式。&lt;/p&gt;

&lt;h3 id=&#34;其他操作&#34;&gt;其他操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;可以删除数据&lt;/li&gt;
&lt;li&gt;可以导出数据&lt;/li&gt;
&lt;li&gt;目前不支持Downsampling，但是victoria-metrics的压缩率和查询效率足以使用&lt;/li&gt;
&lt;li&gt;单节点不支持水平扩展，但是单节点足以媲美thanos和M3，timescaleDB的性能，如果还是觉得不够用，可以尝试集群版本&lt;/li&gt;
&lt;li&gt;virtoria-metrics的参数基本不用调整，都是优化后的合理设计，自身也支持prometheus监控&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;测试&#34;&gt;测试&lt;/h3&gt;

&lt;p&gt;启动脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test victoria-metrics]# cat start.sh
nohup /opt/victoria-metrics/victoria-metrics-prod -storageDataPath=&amp;quot;/data/victoria&amp;quot; -retentionPeriod=2  &amp;gt;&amp;gt;/opt/victoria-metrics/logs/start.log 2&amp;gt;&amp;amp;1 &amp;amp;


-storageDataPath=&amp;quot;/data/victoria&amp;quot;：数据存储目录

-retentionPeriod=2：数据存储时间，两个月
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;11天的数据量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test victoria-metrics]# du -sh /data/victoria
20G /data/victoria
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cpu和内存消耗&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
21899 root      20   0 42.9g  26g 6280 S 188.5 20.8  13486:10 victoria-metric
 4560 root      20   0  159g  26g 365m S 1110.1 20.8   9846:56 prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;集群模式是采用的分布式部署，将数据分别存储在不同的节点上，实现了水平扩展，目前还没有relaese版本，需要自己编译，但是解决了数据量的问题，同时在性能方面并没有发生太大的影响。&lt;/p&gt;

&lt;h3 id=&#34;编译-1&#34;&gt;编译&lt;/h3&gt;

&lt;p&gt;直接make就会在bin目录下生成可执行文件vmstorage, vmselect and vminsert。&lt;/p&gt;

&lt;h3 id=&#34;架构原理图&#34;&gt;架构原理图&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/vm/vm.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;组件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;vmstorage - stores the data&lt;/p&gt;

&lt;p&gt;vmstore其实就是我们数据存在的地方，需要先启动，否则insert会找不到插入的节点，导致数据丢失。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vminsert - proxies the ingested data to vmstorage shards using consistent hashing&lt;/p&gt;

&lt;p&gt;vminsert对采集的提供的代理接口，同时选择将数据插入到我们指定的store节点，可以是单节点，也可以是集群上所有的机器都部署，通过nginx来负载均衡，可以减少节点压力，但是并不能解决单点问题。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vmselect - performs incoming queries using the data from vmstorage&lt;/p&gt;

&lt;p&gt;vmselect是给外部进行查询的接口，同时也负责查询数据的聚合功能。负载均衡和vmisert一样，使用nginx。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;http-api&#34;&gt;HTTP api&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;insert&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;vminsert-ip&amp;gt;:8480/insert/&amp;lt;accountID&amp;gt;/&amp;lt;suffix&amp;gt;:

&amp;lt;accountID&amp;gt; is an arbitrary number identifying namespace for data ingestion (aka tenant)
&amp;lt;suffix&amp;gt; may have the following values:

    1.prometheus - for inserting data with Prometheus remote write API
    2.influx/write or influx/api/v2/write - for inserting data with Influx line protocol
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;querying&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;vmselect-ip&amp;gt;:8481/select/&amp;lt;accountID&amp;gt;/prometheus/&amp;lt;suffix&amp;gt;:

&amp;lt;accountID&amp;gt; is an arbitrary number identifying data namespace for the query (aka tenant)
&amp;lt;suffix&amp;gt; may have the following values:

    1.api/v1/query - performs PromQL instant query
    2.api/v1/query_range - performs PromQL range query
    3.api/v1/series - performs series query
    4.api/v1/labels - returns a list of label names
    5.api/v1/label/&amp;lt;label_name&amp;gt;/values - returns values for the given &amp;lt;label_name&amp;gt; according to API
    6.federate - returns federated metrics
    7.api/v1/export - exports raw data. See this article for details
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;delete&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;vmselect-ip&amp;gt;:8481/delete/&amp;lt;accountID&amp;gt;/prometheus/api/v1/admin/tsdb/delete_series?match[]=&amp;lt;timeseries_selector_for_delete&amp;gt;.
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;vmstorage&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;vmstore保留了8482端口，提供一下URL：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/snapshot/create - create instant snapshot, which can be used for backups in background. Snapshots are created in &amp;lt;storageDataPath&amp;gt;/snapshots folder, where &amp;lt;storageDataPath&amp;gt; is the corresponding command-line flag value.
/snapshot/list - list available snasphots.
/snapshot/delete?snapshot=&amp;lt;id&amp;gt; - delete the given snapshot.
/snapshot/delete_all - delete all the snapshots.
Snapshots may be created independently on each vmstorage node. There is no need in synchronizing snapshots&#39; creation across vmstorage nodes.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;扩展&#34;&gt;扩展&lt;/h3&gt;

&lt;p&gt;1、vminsert and vmselect是可扩展的，无状态的，可以随时扩展或者缩容，并不影响，只是需要在负载均衡中将相关节点处理一下&lt;/p&gt;

&lt;p&gt;2、vmstore是有状态的，因为是分布式存储数据的，所以新增节点需要如下步骤&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Start new vmstorage node with the same -retentionPeriod as existing nodes in the cluster.&lt;/li&gt;
&lt;li&gt;Gradually restart all the vmselect nodes with new -storageNode arg containing &lt;new_vmstorage_host&gt;:8401.&lt;/li&gt;
&lt;li&gt;Gradually restart all the vminsert nodes with new -storageNode arg containing &lt;new_vmstorage_host&gt;:8400.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;备份和恢复&#34;&gt;备份和恢复&lt;/h3&gt;

&lt;p&gt;1、主要使用vmstore的url来进行备份&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Create an instant snapshot by navigating to /snapshot/create HTTP handler. It will create snapshot and return its name.
Archive the created snapshot from &amp;lt;-storageDataPath&amp;gt;/snapshots/&amp;lt;snapshot_name&amp;gt; folder using any suitable tool that follows symlinks. For instance, cp -L, rsync -L or scp -r. The archival process doesn&#39;t interfere with vmstorage work, so it may be performed at any suitable time. Incremental backups are possible with rsync --delete, which should remove extraneous files from backup dir.
Delete unused snapshots via /snapshot/delete?snapshot=&amp;lt;snapshot_name&amp;gt; or /snapshot/delete_all in order to free up occupied storage space.
There is no need in synchronizing backups among all the vmstorage nodes.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、恢复&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stop vmstorage node with kill -INT.&lt;/li&gt;
&lt;li&gt;Delete all the contents of the directory pointed by -storageDataPath command-line flag.&lt;/li&gt;
&lt;li&gt;Copy all the contents of the backup directory to -storageDataPath directory.&lt;/li&gt;
&lt;li&gt;Start vmstorage node.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;实现原理&#34;&gt;实现原理&lt;/h1&gt;

&lt;h2 id=&#34;vminsert&#34;&gt;vminsert&lt;/h2&gt;

&lt;p&gt;插入数据就比较简单了，使用了prometheus差不多的数据结构体来存储数据，只要将数据转化为对应的结构体直接存入数据就可以。对于其他的时序数据库比如influxdb都是差不多的数据结构，只要稍微进行转换，就可以将数据存储到存储节点去。&lt;/p&gt;

&lt;h2 id=&#34;vmstore&#34;&gt;vmstore&lt;/h2&gt;

&lt;p&gt;存储数据可以比常规的节省10倍的内存&lt;/p&gt;

&lt;h2 id=&#34;vmselect&#34;&gt;vmselect&lt;/h2&gt;

&lt;p&gt;查询数据很快&lt;/p&gt;

&lt;h2 id=&#34;mergetree&#34;&gt;MergeTree&lt;/h2&gt;

&lt;p&gt;VictoriaMetrics将数据存储在相似于ClickHouse的 MergeTree表 数据结构中。它是用于剖析数据和其余事件流的最快的数据库。在典型的剖析查问上，它的性能要比PostgreSQL和MySQL等传统数据库高10到1000倍。&lt;/p&gt;

&lt;h1 id=&#34;特性&#34;&gt;特性&lt;/h1&gt;

&lt;h2 id=&#34;扩展了promeql&#34;&gt;扩展了promeql&lt;/h2&gt;

&lt;p&gt;1 、模版&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;((node_memory_MemTotal_bytes{instance=~&amp;quot;$node:$port&amp;quot;, job=~&amp;quot;$job&amp;quot;} - node_memory_MemFree_bytes{instance=~&amp;quot;$node:$port&amp;quot;, job=~&amp;quot;$job&amp;quot;}) /
node_memory_MemTotal_bytes{instance=~&amp;quot;$node:$port&amp;quot;, job=~&amp;quot;$job&amp;quot;}) * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用模版&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WITH (
    commonFilters = {instance=~&amp;quot;$node:$port&amp;quot;,job=~&amp;quot;$job&amp;quot;}
)
(node_memory_MemTotal_bytes{commonFilters} - node_memory_MemFree_bytes{commonFilters}) /
    node_memory_MemTotal_bytes{commonFilters} * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;发展&#34;&gt;发展&lt;/h1&gt;

&lt;h2 id=&#34;vmagent&#34;&gt;vmagent&lt;/h2&gt;

&lt;p&gt;vmagent是一个很小巧但优秀的代理，它可以帮助您从各种来源收集指标并将其存储到VictoriaMetrics或任何其他支持remote_write协议的与Prometheus兼容的存储系统。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/vm/vm2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以用作Prometheus的直接替代品，用于抓取目标（例如node_exporter）。&lt;/li&gt;
&lt;li&gt;可以像Prometheus那样，重新添加，删除和修改标签。可以在将数据发送到远程存储之前对其进行过滤。&lt;/li&gt;
&lt;li&gt;支持多种VictoriaMetrics支持的数据格式，比如Influx，OpenTSDB，Graphite，Prometheus等。&lt;/li&gt;
&lt;li&gt;可以将收集的指标同时复制到多个远程存储系统。在与远程存储连接不稳定的环境中工作。如果远程存储不可用，则将收集的指标缓存在-remoteWrite.tmpDataPath中。一旦恢复远程存储的连接，缓冲的metrcis即发送到远程存储。可以通过-remoteWrite.maxDiskUsagePerURL限制缓冲区的最大磁盘使用量。&lt;/li&gt;
&lt;li&gt;与Prometheus相比，使用较少的RAM，CPU，磁盘IO和网络带宽。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前来讲，Prometheus依旧不可或缺。vmagent 还处于开发阶段。但是vmagent有取代prometheus的想法是可以看出来的。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Cortex</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/</link>
          <pubDate>Thu, 13 Jun 2019 14:28:39 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/</guid>
          <description>&lt;p&gt;crotex是一个为了支持prometheus扩展的服务，支持水平扩展，高可用，多租户，长期存储。主要开发者也是promehteus的开发者&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/cortex/architecture.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;组件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Distributor&lt;/p&gt;

&lt;p&gt;Distributor就是负责接收promtheus发送过来的数据，然后将数据分发给lngester。&lt;/p&gt;

&lt;p&gt;Distributor只要和lngester进行交互，使用的是grpc&lt;/p&gt;

&lt;p&gt;Distributor使用一致性hash来将数据分发给哪个lngester实例，consistent hash ring is stored in Consul&lt;/p&gt;

&lt;p&gt;建议使用负载均衡来运行多个distributors实例。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;lngester&lt;/p&gt;

&lt;p&gt;lngester组件主要是接受Distributor发来的数据，然后发送到后段的数据库存储&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ruler&lt;/p&gt;

&lt;p&gt;ruler组件主要是负责处理alertmanager产生的告警&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Query frontend&lt;/p&gt;

&lt;p&gt;Query frontend组件主要是接受http请求，把他们按着tenant ID排列，并且重试一些返回错误的请求，比如large query&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Querier&lt;/p&gt;

&lt;p&gt;Querier组件主要是处理promql&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Chunk store&lt;/p&gt;

&lt;p&gt;Chunk store组件就是长期存储&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;就是我们常用的集群架构：聚合，将所有数据都发送到一个节点，用于存储+查询&lt;/p&gt;

&lt;h1 id=&#34;安装使用&#34;&gt;安装使用&lt;/h1&gt;

&lt;p&gt;编译启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build ./cmd/cortex
$ ./cortex -config.file=./docs/single-process-config.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置prometheus的远程写，将prometheus数据写入到cortex中去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remote_write:
- url: http://localhost:9009/api/prom/push
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup /opt/promes/cortex/cortex -config.file=/opt/promes/cortex/config/single-process-config.yaml -distributor.ingestion-rate-limit=100000 -ring.store=consul -consul.hostname=10.47.182.224:9996 -distributor.replication-factor=2 &amp;gt;&amp;gt;/opt/promes/cortex/logs/start.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;-distributor.ingestion-rate-limit=100000：限制数据量为100000，其实达不到这个量，prometheus默认remote_write的10000个并发，每个包含100个数据，这个时候会大量出错，所以在写入性能上达不到这个量，测试最大每个包含25个数据可以处理，同样的机器上victoria-metrics可以达到10000个数据而不出错。&lt;/li&gt;
&lt;li&gt;-ring.store=consul -consul.hostname=10.47.182.224:9996：一个令牌存储在consul上，用我们现有的consul&lt;/li&gt;
&lt;li&gt;-distributor.replication-factor=2：集群节点的数量，这边主要是高可用，两个节点互相复制，完成一致性哈希&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前没有看到cortex的可扩展的优秀的方面，可能是社区开发还没有完成，等release。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 中台建设</title>
          <link>https://kingjcy.github.io/post/architecture/electronic-commerce/</link>
          <pubDate>Tue, 04 Jun 2019 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/electronic-commerce/</guid>
          <description>&lt;p&gt;中台建设其实就是将一些能够统一的业务进行统一规划，所以系统的接入和流出都是标准化的操作。&lt;/p&gt;

&lt;h1 id=&#34;组织架构的演变&#34;&gt;组织架构的演变&lt;/h1&gt;

&lt;p&gt;其实我们在另一篇文章&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/microservices/&#34;&gt;架构的演变&lt;/a&gt;中已经主要架构的设计思路，其实也是和组织架构相互对应，相互衍生的。&lt;/p&gt;

&lt;h2 id=&#34;中台&#34;&gt;中台&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;前台---中台---后台
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而到了大中台时代，中台的核心价值是在于，在对企业业务有了柔性支撑和贯通的前提下，再形成协同与智慧的运营体系。&lt;/p&gt;

&lt;p&gt;一般企业架构分成了三个层次：前台、中台、后台。中台又分成三大块，业务中台、数据中台和技术中台。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/electroniccommerce/middle&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;技术中台支撑企业业务发展，通过打通企业内异构系统，支持业务中台；&lt;/li&gt;
&lt;li&gt;业务中台围绕公司业务运营进行服务，将获取的多维度数据传递给数据中台，由数据中台分析反馈给业务中台，以优化业务运营。同时数据中台通过BI智能分析，帮助企业管理者更好的做决策分析。三者是相辅相成，相互协作的。
业务中台其实就是把原有的前端的会员中心、营销中心、商品中心，后端的供应链中心、采配中心等重点模块放在业务中台模块，以后前端不管对接多少个第三方，线上线下增加多少家门店，都能进行统一会员、统一商品编码、统一供应链整合，整个系统一体化。真正做到用技术支持业务，通过业务收集大量数据进行决策，统一高效的进行管理。&lt;/li&gt;
&lt;li&gt;数据中台：一般指以数据采集、数据集成、数据治理，指标体系和数据仓库统一建设等数据管理活动为典型特征的中台。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;大中台&#34;&gt;大中台&lt;/h1&gt;

&lt;p&gt;其实中台严格意义上来说，不是一种架构，也不是一种系统，而是一种战略。&lt;/p&gt;

&lt;p&gt;当前最需要建设的中台有两种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;狭义的业务中台：一般指在线业务为典型特征的中台。在OLDI（Online Data-Intensive）时代，越来越多的企业的核心业务都是在线业务，因此把在线业务中台简称为业务中台。&lt;/li&gt;
&lt;li&gt;数据中台：一般指以数据采集、数据集成、数据治理，指标体系和数据仓库统一建设等数据管理活动为典型特征的中台。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对业务中台来说，比较符合的场景主要有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;业务系统研发团队至少大几十人（含外包的），需求多变化快，系统又涉及多个领域（比如做ERP、电商的），业务逻辑比较复杂。&lt;/li&gt;
&lt;li&gt;这时业务中台可以把系统和业务领域划分清楚，提高研发效率。做相似行业的外包项目为主，业务规模也做的比较大的（一年有两位数的项目）。&lt;/li&gt;
&lt;li&gt;这时业务中台可以提升软件复用，降低定制化成本，提高研发效率。如果每个项目都完全不一样，那中台也救不了你。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;支持业务中台的技术体系，包括微服务、DevOps、云原生和分布式事务等。&lt;/p&gt;

&lt;p&gt;将需求设计成微服务架构，然后每个服务使用各种技术栈来开发业务，比如golang的技术栈的高并发的特性来开发web服务等，然后将一些统一的模块进行统一的接入和输出，使用devops的开发模式，在业务中还是需要解决分布式事务等问题。&lt;/p&gt;

&lt;p&gt;比如在网易，是网易轻舟微服务平台，提供微服务应用全生命周期的完整支持，包括下一代微服务Service Mesh支持、经典微服务框架NSF、包括CI/CD的DevOps、分布式事务框架GXTS、APM、API网关、GoAPI全自动化测试以及容器应用管理服务等。&lt;/p&gt;

&lt;p&gt;对数据中台来说，比较符合的场景：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数据产品比较多，每天要看数据如果没数据就不知道怎么工作的运营人员比较多的业务。
比如电商就是典型。尤其是数据产品和运营人员还在多个团队。
用数据的姿势比较复杂，问题比较多，比如经常出现指标不一致、数据出错、想要的数据不知道哪里有等问题。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;支持数据中台的技术体系，包括指标管理、数据服务、元数据管理、数仓开发与管理、数据安全管理、数据资产管理、大数据计算引擎、数据集成/同步/交换引擎等，&lt;/p&gt;

&lt;p&gt;其实数据中台就是将数据进行处理，不同数据资源，统一的输出标准，中间用到大部分就是数据引擎，比如kafka队列，sprak，flink等流式引擎，hadoop，hbase和hive等大数据引擎。&lt;/p&gt;

&lt;p&gt;比如在网易，是以网易猛犸为核心的网易全链路数据中台解决方案。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 广告投放系统</title>
          <link>https://kingjcy.github.io/post/architecture/advertising/</link>
          <pubDate>Tue, 04 Jun 2019 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/advertising/</guid>
          <description>&lt;p&gt;互联网智能广告系统简易流程与架构。&lt;/p&gt;

&lt;h1 id=&#34;业务简述&#34;&gt;业务简述&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/advertising/advertising.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;广告主在业务端投递广告&lt;/p&gt;

&lt;p&gt;广告主登录业务端后台，进行设置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;•今日投放地域是“北京-上地”
•投放类别是“租房”
•定向人群为“女”，“30岁以下”
•需要推广的广告内容是他发布的一条“房屋出租”的帖子
•竞价设置的是0.2元
•单日预算是20元
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些数据，当然通过业务端存储到了数据层，即数据库和缓存里。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;用户来到了网站，进入了“北京-上地-租房”类别，广告初筛实施&lt;/p&gt;

&lt;p&gt;合适的广告，必须符合“语义相关性”，即基础检索属性（广告属性）必须符合（广告能否满足用户的需求，满足了点击率才高），这个工作是通过BS-basic search检索服务完成的。BS从数据层检索到“北京-上地-租房”的广告帖子。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;用户属性与广告主属性匹配，广告精筛实施&lt;/p&gt;

&lt;p&gt;步骤二中，基础属性初筛了以后，要进行更深层次的策略筛选（用户能否满足广告的需求），此例中，广告主的精准需求为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;•用户性别为“女”
•用户年龄为“30岁以下”
•用户访问IP是“北京”
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;系统将初筛出来的M条广告和用户属性进行匹配筛选，又过滤掉了一部分，最后剩余N条待定广告，这些广告既满足用户的需求（初筛），这些用户也满足广告主的需求（精筛），后者是在AS-advanced search策略服务完成的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;综合排序，并返回Top X的广告&lt;/p&gt;

&lt;p&gt;经过步骤2和步骤3的初筛和精筛之后，待选的N条广告既能满足用户当前的需求，用户亦能满足广告主的筛选需求，但实际情况是，广告位只有3个，怎么办呢？就需要我们对N条广告进行综合打分排序（满足平台的需求，广告平台要多赚钱嘛）。&lt;/p&gt;

&lt;p&gt;出价高，但没人点击，广告平台没有收益；点击率高，但出价低，广告平台还是没有收益。最终应该按照广告的出价与CTR的乘积作为综合打分排序的依据，bid*CTR。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;展现端展示了广告，用户点击了广告&lt;/p&gt;

&lt;p&gt;展示了广告后，展现端js会上报广告展示日志，有部分用户点击了广告，服务端会记录点击日志，这些日志可以作为广告算法实施的数据源，同时，他们经过统计分析之后，会被展示给广告主，让他们能够看到自己广告的展示信息，点击信息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对广告主进行扣费&lt;/p&gt;

&lt;p&gt;用户既然点击了广告，平台就要对投放广告的广告主进行扣费了，扣费前当然要经过反作弊系统的过滤（主要是恶意点击），扣费后信息会实时反映到数据层，费用扣光后，广告就要从数据层下线。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- Distributed config</title>
          <link>https://kingjcy.github.io/post/distributed/distributed-config/</link>
          <pubDate>Sun, 26 May 2019 20:10:41 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/distributed-config/</guid>
          <description>&lt;p&gt;配置信息就是程序加载时候需要设置的信息，一般我们可以通过在代码中设置，在配置文件中设置，在配置中心配置来设置我们需要的信息。&lt;/p&gt;

&lt;h1 id=&#34;配置方式&#34;&gt;配置方式&lt;/h1&gt;

&lt;p&gt;配置方式一般有两种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;本地配置，包括代码中耦合，配置文件加载。&lt;/li&gt;
&lt;li&gt;适用于分布式系统的集中式资源配置。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;本地配置&#34;&gt;本地配置&lt;/h2&gt;

&lt;h3 id=&#34;代码耦合&#34;&gt;代码耦合&lt;/h3&gt;

&lt;p&gt;将配置直接写在代码中很方便，但是对于后期的维护修改十分的不友好，需要重新发布才能修改完成，基本上是不用的。&lt;/p&gt;

&lt;h3 id=&#34;配置文件&#34;&gt;配置文件&lt;/h3&gt;

&lt;p&gt;将配置信息写在配置文件中是最常见的方式，程序自动加载配置文件的信息，然后进行处理，有需要只要修改对应的配置信息就可以，实现了对业务代码的解耦。&lt;/p&gt;

&lt;p&gt;但是随着系统的发展，分布式系统的出现，我们很多实例都是通过集群的方式进行部署，这个时候在集群的每个节点上都会有一份配置文件，随着规模的扩大，修改每个节点上的配置文件又变的容易出错难以维护，因此我们需要一种集中式的配置方式。&lt;/p&gt;

&lt;h2 id=&#34;集中式资源配置&#34;&gt;集中式资源配置&lt;/h2&gt;

&lt;p&gt;在分布式的情况下，我们需要一个共享配置的中心，使用集中式的资源管理配置平台的好处&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置信息统一管理&lt;/li&gt;
&lt;li&gt;动态获取更新配置文件&lt;/li&gt;
&lt;li&gt;降低运维成本&lt;/li&gt;
&lt;li&gt;降低配置出错率&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;分布式配置管理服务的本质是一个发布订阅模式的实现。&lt;/p&gt;

&lt;p&gt;集中式的配置中心一般都是作为服务发现和动态注册的基础，也是我们常用的几个框架，比如zookeeper，consul，etcd等。我们一般的配置平台都是基于这些组件来实现的，目前这一块做的比较好的就是淘宝的diamonds和百度的disconf，还有一些比如携程的apollo等。&lt;/p&gt;

&lt;h3 id=&#34;zookeeper&#34;&gt;zookeeper&lt;/h3&gt;

&lt;p&gt;常规使用一些服务发现动态注册组件做配置中心都是一个思路，我们以zookeeper为例，看下他是如何实现分布式配置管理的&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/zookeeper&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;整体可以分为3部分&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;zookeeper集群
提供了稳定的配置管理服务，对外提供了接口，外部可以添加、修改配置信息，可以监听配置的变化&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置管理中心
需要自己开发，负责维护配置信息&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;各个分布式应用
每个应用只需要调用一下ZK的接口，把自己注册到ZK，就可以自动接收配置的变化信息&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;工作流程原理也很简答&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/zookeeper1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;diamond&#34;&gt;diamond&lt;/h3&gt;

&lt;p&gt;diamond是淘宝内部使用的一个管理持久配置的系统，它的特点是简单、可靠、易用，目前淘宝内部绝大多数系统的配置，由diamond来进行统一管理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/diamond&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;作为一个配置中心，diamond的功能分为发布和订阅两部分。因为diamond存放的是持久数据，这些数据的变化频率不会很高，甚至很低，所以发布采用手工的形式，通过diamond后台管理界面发布；订阅是diamond的核心功能，订阅通过diamond-client的API进行。&lt;/li&gt;
&lt;li&gt;diamond服务端采用mysql加本地文件的形式存放配置数据。发布数据时，数据先写到mysql，再写到本地文件；订阅数据时，直接获取本地文件，不查询数据库，这样可以最大程度减少对数据库的压力。&lt;/li&gt;
&lt;li&gt;diamond服务端是一个集群，集群中的每台机器连接同一个mysql，集群之间的数据同步通过两种方式进行，一是每台server定时去mysqldump数据到本地文件，二是某一台server接收发布数据请求，在更新完mysql和本机的本地文件后，发送一个HTTP请求（通知）到集群中的其他几台server，其他server收到通知，去mysql中将刚刚更新的数据dump到本地文件。&lt;/li&gt;
&lt;li&gt;每一台server前端都有一个nginx，用来做流量控制。&lt;/li&gt;
&lt;li&gt;图中没有将地址服务器画出，地址服务器是一台有域名的机器，上面运行有一个HTTPserver，其中有一个静态文件，存放着diamond服务器的地址列表。客户端启动时，根据自身的域名绑定，连接到地址服务器，取回diamond服务器的地址列表，从中随机选择一台diamond服务器进行连接。&lt;/li&gt;
&lt;li&gt;可以看到，整个diamond的架构非常简单，使用的都是最常用的一些技术以及产品，它之所以表现得非常稳定，跟其架构简单是分不开的，当然，稳定的另一个主要原因是它具备一套比较完善的容灾机制，容灾机制将在下一篇文章中讲述。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disconf&#34;&gt;disconf&lt;/h3&gt;

&lt;p&gt;disconf是一套完整的基于zookeeper的分布式配置统一解决方案。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/disconf&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每个模块的简单介绍如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Disconf-core

&lt;ul&gt;
&lt;li&gt;分布式通知模块：支持配置更新的实时化通知&lt;/li&gt;
&lt;li&gt;路径管理模块：统一管理内部配置路径URL&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disconf-client

&lt;ul&gt;
&lt;li&gt;配置仓库容器模块：统一管理用户实例中本地配置文件和配置项的内存数据存储&lt;/li&gt;
&lt;li&gt;配置reload模块：监控本地配置文件的变动，并自动reload到指定bean&lt;/li&gt;
&lt;li&gt;扫描模块：支持扫描所有disconf注解的类和域&lt;/li&gt;
&lt;li&gt;下载模块：restful风格的下载配置文件和配置项&lt;/li&gt;
&lt;li&gt;watch模块：监控远程配置文件和配置项的变化&lt;/li&gt;
&lt;li&gt;主备分配模块：主备竞争结束后，统一管理主备分配与主备监控控制&lt;/li&gt;
&lt;li&gt;主备竞争模块：支持分布式环境下的主备竞争&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disconf-web

&lt;ul&gt;
&lt;li&gt;配置存储模块：管理所有配置的存储和读取&lt;/li&gt;
&lt;li&gt;配置管理模块：支持配置的上传、下载、更新&lt;/li&gt;
&lt;li&gt;通知模块：当配置更新后，实时通知使用这些配置的所有实例&lt;/li&gt;
&lt;li&gt;配置自检监控模块：自动定时校验实例本地配置与中心配置是否一致&lt;/li&gt;
&lt;li&gt;权限控制：web的简单权限控制&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disconf-tools

&lt;ul&gt;
&lt;li&gt;context共享模块：提供多实例间context的共享。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;原理&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/disconf&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动事件A：以下按顺序发生。

&lt;ul&gt;
&lt;li&gt;A3：扫描静态注解类数据，并注入到配置仓库里。&lt;/li&gt;
&lt;li&gt;A4+A2：根据仓库里的配置文件、配置项，去 disconf-web 平台里下载配置数据。这里会有主备竞争&lt;/li&gt;
&lt;li&gt;A5：将下载得到的配置数据值注入到仓库里。&lt;/li&gt;
&lt;li&gt;A6：根据仓库里的配置文件、配置项，去ZK上监控结点。&lt;/li&gt;
&lt;li&gt;A7+A2：根据XML配置定义，到 disconf-web 平台里下载配置文件，放在仓库里，并监控ZK结点。这里会有主备竞争。&lt;/li&gt;
&lt;li&gt;A8：A1-A6均是处理静态类数据。A7是处理动态类数据，包括：实例化配置的回调函数类；将配置的值注入到配置实体里。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;更新配置事件B：以下按顺序发生。

&lt;ul&gt;
&lt;li&gt;B1：管理员在 Disconf-web 平台上更新配置。&lt;/li&gt;
&lt;li&gt;B2：Disconf-web 平台发送配置更新消息给ZK指定的结点。&lt;/li&gt;
&lt;li&gt;B3：ZK通知 Disconf-cient 模块。&lt;/li&gt;
&lt;li&gt;B4：与A4一样。&lt;/li&gt;
&lt;li&gt;B5：与A5一样。&lt;/li&gt;
&lt;li&gt;B6：基本与A4一样，唯一的区别是，这里还会将配置的新值注入到配置实体里。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;主备机切换事件C：以下按顺序发生。

&lt;ul&gt;
&lt;li&gt;C1：发生主机挂机事件。&lt;/li&gt;
&lt;li&gt;C2：ZK通知所有被影响到的备机。&lt;/li&gt;
&lt;li&gt;C4：与A2一样。&lt;/li&gt;
&lt;li&gt;C5：与A4一样。&lt;/li&gt;
&lt;li&gt;C6：与A5一样。&lt;/li&gt;
&lt;li&gt;C7：与A6一样。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算数据库系列---- etcd-operator</title>
          <link>https://kingjcy.github.io/post/cloud/paas/db/etcd-operator/</link>
          <pubDate>Sat, 04 May 2019 11:26:37 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/db/etcd-operator/</guid>
          <description>&lt;p&gt;etcd operator管理部署到Kubernetes的 etcd集群，并自动执行与操作etcd集群相关的任务(创建，销毁，调整，故障转移，滚动升级，备份还原)。&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;安装etcd-opertaor&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm -ivh etcd-operator-1.0.0-1.el7.x86_64.rpm          // 安装rpm包
rpm -qa | grep etcd-operator                            // 检查rpm包是否安装成功
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建etcdtask crd和etcdcluster crd对象&lt;/p&gt;

&lt;p&gt;将etcdtask-crd.yaml和etcdcluster-crd.yaml文件拷贝至目标服务器，文件见附件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f etcdtask-crd.yaml                           // 创建etcdtask对象
kubectl create -f etcdcluster-crd.yaml                        // 创建etcdcluster对象
kubectl get crd | grep etcdtasks.extensions.sncloud.com       // 检查etcdtask对象是否建立成功
kubectl get crd | grep etcdclusters.extensions.sncloud.com    // 检查etcdcluster对象是否建立成功
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动etcd-operator服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl enable etcd-operator          // 启动etcd-operator服务
systemctl start etcd-operator          // 启动etcd-operator服务
systemctl status etcd-operator         // 查看etcd-operator服务状态，确定状态为running。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;etcd-operator 所包含的几个自定义资源对象(CRDs):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;EtcdCluster : etcdcluster 用来描述用户自定义的 etcd 集群，可一键式部署和配置一个相关的 etcd 集群。&lt;/li&gt;
&lt;li&gt;EtcdBackup : etcdbackup 用来描述和管理一个 etcd 集群的备份，当前支持定期备份到云端存储，如 AWS s3, Aliyun oss(oss 当前需使用 quay.io/coreos/etcd-operator:dev 镜像)。&lt;/li&gt;
&lt;li&gt;EtcdRestore: etcdrestore 用来帮助将 etcdbackup 服务所创建的备份恢复到一个指定的 etcd 的集群。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;h2 id=&#34;创建etcd集群&#34;&gt;创建etcd集群&lt;/h2&gt;

&lt;p&gt;使用EtcdCluster来创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
apiVersion: etcd.database.coreos.com/v1beta2
kind: EtcdCluster
metadata:
  name: &amp;quot;etcd-cluster&amp;quot;
spec:
  size: 3 # 默认etcd节点数
  version: &amp;quot;3.2.25&amp;quot; # etcd版本号
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get etcdcluster
NAME            AGE
etcd-cluster    2m

$ kubectl get pod
NAME                     READY   STATUS  RESTARTS AGE
etcd-cluster-g28f552vvx  1/1   Running    0      2m
etcd-cluster-lpftgqngl8  1/1   Running    0      2m
etcd-cluster-sdpcfrtv99  1/1   Running    0      2m
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;备份&#34;&gt;备份&lt;/h2&gt;

&lt;p&gt;下面以阿里云的 OSS 举例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
apiVersion: etcd.database.coreos.com/v1beta2
kind: EtcdBackup
metadata:
  name: example-etcd-cluster-periodic-backup
spec:
  etcdEndpoints: [http://etcd-cluster-client:2379] #内网可使用svc地址，外网可用NodePort或LB代理地址
  storageType: OSS
  backupPolicy:
    backupIntervalInSecond: 120 #备份时间间隔
    maxBackups: 4 #最大备份数
  oss:
    path: my-bucket/etcd.backup
    ossSecret: oss-secret #需预先创建oss secret
    endpoint: oss-cn-hangzhou.aliyuncs.com
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;待 etcdbackup 创建成功后，用户可以通过 kubectl describe etcdbackup 或查看 etcd-backup controller 日志来查看备份状态，如状态显示为 Succeeded: true，可以前往 oss 查看具体的备份内容。&lt;/p&gt;

&lt;h2 id=&#34;恢复&#34;&gt;恢复&lt;/h2&gt;

&lt;p&gt;假设我们要将 etcd 集群 A 的备份数据恢复到另一个新的 etcd 集群 B，那么我们先手动创建一个名为 etcd-cluster2 的新集群(oss 备份/恢复当前需使用 quay.io/coreos/etcd-operator:dev 镜像)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
apiVersion: etcd.database.coreos.com/v1beta2
kind: EtcdCluster
metadata:
  name: &amp;quot;etcd-cluster2&amp;quot;
spec:
  size: 3
  version: &amp;quot;3.2.25&amp;quot;
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过创建 etcdresotre 将备份数据恢复到 etcd-cluster2 集群&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
apiVersion: etcd.database.coreos.com/v1beta2
kind: EtcdRestore
metadata:
  # name必须与下面的spec.etcdCluster.name保持一致
  name: etcd-cluster2
spec:
  etcdCluster:
    name: etcd-cluster2
  backupStorageType: OSS
  oss:
    path: my-bucket/etcd.backup_v1_2019-08-07-06:44:17
    ossSecret: oss-secret
    endpoint: oss-cn-hangzhou.aliyuncs.com
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;待 etcdresotre 对象创建成功后，可以查看 etcd-operator-restore 的日志，大致内容如下，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl logs -f etcd-operator-restore
...
time=&amp;quot;2019-08-07T06:50:26Z&amp;quot; level=info msg=&amp;quot;listening on 0.0.0.0:19999&amp;quot;
time=&amp;quot;2019-08-07T06:50:26Z&amp;quot; level=info msg=&amp;quot;starting restore controller&amp;quot; pkg=controller
time=&amp;quot;2019-08-07T06:56:25Z&amp;quot; level=info msg=&amp;quot;serving backup for restore CR etcd-cluster2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;实战&#34;&gt;实战&lt;/h1&gt;

&lt;h2 id=&#34;创建备份任务-立即备份-定时备份-根据实际需求去选择&#34;&gt;创建备份任务（立即备份、定时备份，根据实际需求去选择）&lt;/h2&gt;

&lt;h3 id=&#34;立即备份&#34;&gt;立即备份&lt;/h3&gt;

&lt;p&gt;根据需求修改backup-now.yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat backup-now.yaml

apiVersion: extensions.sncloud.com/v1beta1
kind: EtcdTask
metadata:
  name: backup-now-052701           #命名规则待定
  namespace: kube-system
spec:
  type: backup                      #任务类型，包括backup、restore
  backup:
    backupSpec:                     #备份信息
      etcdCluster: etcd-cluster     #需要备份的etcd集群的EtcdCluster名称
      storageType: OSS              #备份类型，暂时只支持OSS和Local
      backupPolicy:                            #备份策略
        backupCron: &amp;quot;0&amp;quot;             #定时备份，用crontab格式设置
        localMaxBackups: 10                       #本地最大备份份数
        remoteMaxBackups: 10                    #远端最大备份份数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建立即备份任务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f backup-now.yaml     // 创建立即备份任务

kubectl get etcdtask -n kube-system | grep &amp;lt;back-task-name&amp;gt;   // 检查备份任务是否创建成功
kubectl describe etcdtask -n kube-system  &amp;lt;back-task-name&amp;gt;    // 查看备份任务结果
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;定时备份任务&#34;&gt;定时备份任务&lt;/h3&gt;

&lt;p&gt;根据需求修改backup.yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat backup.yaml

apiVersion: extensions.sncloud.com/v1beta1
kind: EtcdTask
metadata:
  name: backup-cron-052602           #命名规则待定
  namespace: kube-system
spec:
  type: backup                      #任务类型，包括backup、restore
  backup:
    backupSpec:                        #备份信息
      etcdCluster: etcd-cluster     #需要备份的etcd集群的EtcdCluster名称
      storageType: OSS              #备份类型，暂时只支持OSS和Local
      backupPolicy:                                #备份策略
        backupCron: &amp;quot;0 */10 * * * ?&amp;quot;       #定时备份，用crontab格式设置
        localMaxBackups: 10                       #本地最大备份份数
        remoteMaxBackups: 20                    #远端最大备份份数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建定时备份任务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f  backup.yaml                                // 创建定时备份任务

kubectl get etcdtask -n kube-system | grep &amp;lt;back-task-name&amp;gt;   // 检查备份任务是否创建成功
kubectl describe etcdtask -n kube-system  &amp;lt;back-task-name&amp;gt;    // 查看备份任务结果
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;创建恢复任务&#34;&gt;创建恢复任务&lt;/h2&gt;

&lt;p&gt;查看备份文件列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /var/lib/etcd_backup/etcd-cluster.meta   // 查看备份元数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;在集群的所有master节点执行如下命令

$  systemctl stop etcd-operator; systemctl stop etcd; systemctl stop kube-apiserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;etcd数据恢复&lt;/p&gt;

&lt;p&gt;在所有的master节点上执行命令，根据提示操作，会提示确认是否停止，停止完毕则输入yes,其中etcd-data-2020-11-12_22_0_0.tar文件换成所有恢复到的版本文件&lt;/p&gt;

&lt;p&gt;使用本地备份文件恢复，使用本地备份文件恢复，确认所有master节点存在备份文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ etcd-operator  -restore=true -restore.file=etcd-data-2020-11-12_22_0_0.tar -restore.metafile=/var/lib/etcd_backup/etcd-cluster.meta  -restore.type=local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用远端oss备份文件进行恢复&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ etcd-operator  -restore=true -restore.file=etcd-data-2020-11-12_22_0_0.tar -restore.metafile=/var/lib/etcd_backup/etcd-cluster.meta  -restore.type=oss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;恢复服务&lt;/p&gt;

&lt;p&gt;在所有的master节点执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$  systemctl start etcd; systemctl start kube-apiserver;systemctl start etcd-operator
$  systemctl status etcd; systemctl status kube-apiserver;systemctl status etcd-operator
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;手动使用备份文件恢复etcd&#34;&gt;手动使用备份文件恢复ETCD&lt;/h2&gt;

&lt;p&gt;若集群etcd出错，需要使用备份文件来恢复数据时，可按以下步骤操作：&lt;/p&gt;

&lt;p&gt;使用本地备份恢复etcd&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在集群的每个master节点上的 /var/lib/etcd_backup 目录下，有etcd的备份文件，文件的命名规则是 etcd-year-month-day-hour-minute-second.tar ，可以从 文件名中看出备份的时间；&lt;/li&gt;
&lt;li&gt;找到需要的备份文件；&lt;/li&gt;
&lt;li&gt;使用 tar zxvf xxx.tar 命令解压；&lt;/li&gt;
&lt;li&gt;将解压出来的etcd.conf 和 etcd.service 和 etcd_snapshot.db 文件拷贝到每个 master 节点上；&lt;/li&gt;
&lt;li&gt;停止每个master节点的apiserver 和 etcd 服务  systemctl stop kube-apiserver  ;   kubectl stop etcd;&lt;/li&gt;
&lt;li&gt;删除 /var/lib/etcd/default.etcd 目录 rm -rf /var/lib/etcd/default.etcd/ ； 如果etcd的配置文件或服务文件丢失，将etcd.conf拷贝至/etc/etcd/目录下，将etcd.service 拷贝至/usr/lib/systemd/system/下；etcd.conf中  ETCD_LISTEN_PEER_URLS 、 ETCD_LISTEN_CLIENT_URLS 、 ETCD_INITIAL_ADVERTISE_PEER_URLS 、 ETCD_ADVERTISE_CLIENT_URLS 需要按照拷贝到的节点ip进行相应的修改；(在每个master节点上执行)&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用如下命令进行数据的恢复， 其中的一些参数可在etcd.conf配置文件中得到：（在每个master节点上执行）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ETCDCTL_API=3 /usr/bin/etcdctl \
snapshot restore etcd_snapshot.db \
--name $ETCD_NAME \
--data-dir /var/lib/etcd/default.etcd/ \
--initial-cluster $ETCD_INITIAL_CLUSTER \
--initial-cluster-token $ETCD_INITIAL_CLUSTER_TOKEN \
--initial-advertise-peer-urls $ETCD_INITIAL_ADVERTISE_PEER_URLS \
--cacert=$ETCD_CA_PEM --cert=$ETCD_ETCD_PEM --key=$ETCD_ETCDKEY_PEM
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改etcd数据目录权限：   chown etcd:etcd -R /var/lib/etcd/default.etcd/   （在每个master节点上执行）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;启动每个master节点的 apiserver和etcd服务：  systemctl start etcd ;  systemctl start kube-apiserver ;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>服务发现系列---- Zookeeper</title>
          <link>https://kingjcy.github.io/post/middleware/serverdiscovery/zookeeper/</link>
          <pubDate>Fri, 19 Apr 2019 09:37:11 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/serverdiscovery/zookeeper/</guid>
          <description>&lt;p&gt;zookeeper是Hadoop的一个子项目，它是分布式系统中的协调系统，可提供的服务主要有：配置服务、名字服务、分布式同步、组服务等。就是提供高可用的数据管理、应用程序协调服务的分布式服务框架，基于对Paxos算法的实现，使该框架保证了分布式环境中数据的强一致性，提供的功能包括：配置维护、统一命名服务、状态同步服务、集群管理等。&lt;/p&gt;

&lt;h1 id=&#34;zookeeper&#34;&gt;Zookeeper&lt;/h1&gt;

&lt;p&gt;Zookeeper的特点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;简单&lt;/p&gt;

&lt;p&gt;Zookeeper的核心是一个精简的文件系统，它支持一些简单的操作和一些抽象操作，例如，排序和通知。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;丰富&lt;/p&gt;

&lt;p&gt;Zookeeper的原语操作是很丰富的，可实现一些协调数据结构和协议。例如，分布式队列、分布式锁和一组同级别节点中的“领导者选举”。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;高可靠&lt;/p&gt;

&lt;p&gt;Zookeeper支持集群模式，可以很容易的解决单点故障问题。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;松耦合交互&lt;/p&gt;

&lt;p&gt;不同进程间的交互不需要了解彼此，甚至可以不必同时存在，某进程在zookeeper中留下消息后，该进程结束后其它进程还可以读这条消息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;资源库&lt;/p&gt;

&lt;p&gt;Zookeeper实现了一个关于通用协调模式的开源共享存储库，能使开发者免于编写这类通用协议。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装并启动&#34;&gt;安装并启动&lt;/h2&gt;

&lt;p&gt;1、配置文件conf/zoo.cfg&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
initLimit=10
syncLimit=5
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;tickTime:指定了ZooKeeper的基本时间单位（以毫秒为单位）。&lt;/li&gt;
&lt;li&gt;dataDir:存储内存数据快照位置。&lt;/li&gt;
&lt;li&gt;clientPort：监听客户端连接端口。&lt;/li&gt;
&lt;li&gt;initLimmit：启动zookeeper，从节点至主节点连接超时时间。（上面为10个tickTime）&lt;/li&gt;
&lt;li&gt;syncLimit:zookeeper正常运行，若主从同步时间超过syncLimit，则丢弃该从节点。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、配置完，启动zookeeper&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、检查是否运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo ruok | nc localhost 2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、连接到zookeeper&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/zkCli.sh -server 127.0.0.1:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在shell命令中使用help查看命令列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[zkshell: 0] help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个zookeeper节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create /zk_test my_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取节点数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;get /zk_test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改节点数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set /zk_test junk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;delete /zk_test
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;其实原理简单来说，就是要选举leader，会生成一个zxid，然后分发给所有的server（所以这里一台server可以接受多台server给他发送要选举leader的请求），然后各个server根据发送给自己的zxid，选择一个值最大的，然后将这个选择返回给发送这个zxid的server，只要这个server收到的答复大于等于2/n+1个（也就是超过半数的同意票），则表明自己当选为leader，然后会向所有server广播自己已经成为leader。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeeper数据模型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;类似于文件系统，所有的节点都是绝对路径，将文件和目录抽象成znode。和Unix中的文件系统路径格式很想，但是只支持绝对路径，不支持相对路径，也不支持点号（”.”和”..”）。&lt;/p&gt;

&lt;p&gt;1、znode&lt;/p&gt;

&lt;p&gt;维护了数据和ACL改变的版本号，每一次数据改变版本号增加，当有一个client去执行update和delete时候，必须提供一个数据变更版本号，如果与数据不符合，则更新失败。&lt;/p&gt;

&lt;p&gt;2、存储小数据&lt;/p&gt;

&lt;p&gt;一般不超过1M，大量数据会花费更多的时间和延迟来完成数据拷贝，由于网络的消耗。&lt;/p&gt;

&lt;p&gt;3、瞬时节点&lt;/p&gt;

&lt;p&gt;随着会话结束而删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;When the session ends the znode is deleted. Because
of this behavior ephemeral znodes are not allowed to have children.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、操作的原子性&lt;/p&gt;

&lt;p&gt;Znode的数据读写是原子的，要么读或写了完整的数据，要么就失败，不会出现只读或写了部分数据。&lt;/p&gt;

&lt;p&gt;5、顺序znode&lt;/p&gt;

&lt;p&gt;名称中包含Zookeeper指定顺序号的znode。若在创建znode时设置了顺序标识，那么该znode被创建后，名字的后边将会附加一串数字，该数字是由一个单调递增的计数器来生成的。例如，创建节点时传入的path是”/aa/bb”，创建后的则可能是”/aa/bb0002”，再次创建后是”/aa/bb0003”。&lt;/p&gt;

&lt;p&gt;Znode的创建模式CreateMode有四种，分别是：EPHEMERAL（短暂的znode）、EPHEMERAL_SEQUENTIAL（短暂的顺序znode）、PERSISTENT（持久的znode）和PERSISTENT_SEQUENTIAL（持久的顺序znode）。如果您已经看过了上篇博文，那么这里的api调用应该是很好理解的，见：&lt;a href=&#34;http://www.cnblogs.com/leocook/p/zk_0.html。&#34;&gt;http://www.cnblogs.com/leocook/p/zk_0.html。&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeeper中的时间&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Zxid：zookeeper状态的改变都会有事物id自增来维护。&lt;/li&gt;
&lt;li&gt;Version numbers ：znode的数据和ACL变更维护。&lt;/li&gt;
&lt;li&gt;ticks:zookeeper集群部署时的时间单位。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeepr  watches&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以在读操作exists、getChildren和getData上设置观察，在执行写操作create、delete和setData将会触发观察事件，当然，在执行写的操作时，也可以选择是否触发znode上设置的观察器，具体可查看相关的api。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当观察的znode被创建、删除或其数据被更新时，设置在exists上的观察将会被触发；&lt;/li&gt;
&lt;li&gt;当观察的znode被删除或数据被更新时，设置在getData上的观察将会被触发；&lt;/li&gt;
&lt;li&gt;当观察的znode的子节点被创建、删除或znode自身被删除时，设置在getChildren上的观察将会被触发，可通过观察事件的类型来判断被删除的是znode还是它的子节点。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：在收到收到触发事件到执行读操作之间，znode的状态可能会发生状态，这点需要牢记。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ACL&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ACL权限：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CREATE: you can create a child node&lt;/li&gt;
&lt;li&gt;READ: you can get data from a node and list its children.&lt;/li&gt;
&lt;li&gt;WRITE: you can set data for a node&lt;/li&gt;
&lt;li&gt;DELETE: you can delete a child node&lt;/li&gt;
&lt;li&gt;ADMIN: you can set permissions&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;身份验证模式：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;world 无验证&lt;/li&gt;
&lt;li&gt;auth 只能使用sessionID&lt;/li&gt;
&lt;li&gt;digest  username:password 验证&lt;/li&gt;
&lt;li&gt;ip 客户端IP验证&lt;/li&gt;
&lt;li&gt;host 客户端主机名验证&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;添加验证&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可使用zk对象的addAuthInfo()方法来添加验证模式，如使用digest模式进行身份验证：zk.addAuthInfo(“digest”,”username:passwd”.getBytes());&lt;/p&gt;

&lt;p&gt;在zookeeper对象被创建时，初始化会被添加world验证模式。world身份验证模式的验证id是”anyone”。&lt;/p&gt;

&lt;p&gt;若该连接创建了znode，那么他将会被添加auth身份验证模式的验证id是””，即空字符串，这里将使用sessionID进行验证。&lt;/p&gt;

&lt;p&gt;创建自定义验证：创建ACL对象时，可用ACL类的构造方法ACL(int perms, Id id)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeeper  内部原理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、选举领导&lt;/p&gt;

&lt;p&gt;集群中所有的zk实例会选举出来一个“领导实例”（leader），其它实例称之为“随从实例”（follower）。如果leader出现故障，其余的实例会选出一台leader，并一起提供服务，若之前的leader恢复正常，便成为follower。选举follower是一个很快的过程，性能影响不明显。&lt;/p&gt;

&lt;p&gt;Leader主要功能是协调所有实例实现写操作的原子性，即：所有的写操作都会转发给leader，然后leader会将更新广播给所有的follower，当半数以上的实例都完成写操作后，leader才会提交这个写操作，随后客户端会收到写操作执行成功的响应。&lt;/p&gt;

&lt;p&gt;2、原子广播&lt;/p&gt;

&lt;p&gt;上边已经说到：所有的写操作都会转发给leader，然后leader会将更新广播给所有的follower，当半数以上的实例都完成写操作后，leader才会提交这个写操作，随后客户端会收到写操作执行成功的响应。这么来的话，就实现了客户端的写操作的原子性，每个写操作要么成功要么失败。逻辑和数据库的两阶段提交协议很像。&lt;/p&gt;

&lt;p&gt;Znode的每次写操作都相当于数据库里的一次事务提交，每个写操作都有个全局唯一的ID，称为：zxid（ZooKeeper Transaction）。ZooKeeper会根据写操作的zxid大小来对操作进行排序，zxid小的操作会先执行。zk下边的这些特性保证了它的数据一致性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;顺序一致性&lt;/p&gt;

&lt;p&gt;任意客户端的写操作都会按其发送的顺序被提交。如果一个客户端把某znode的值改为a，然后又把值改为b（后面没有其它任何修改），那么任何客户端在读到值为b之后都不会再读到a。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;原子性&lt;/p&gt;

&lt;p&gt;这一点再前面已经说了，写操作只有成功和失败两种状态，不存在只写了百分之多少这么一说。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;单一系统映像&lt;/p&gt;

&lt;p&gt;客户端只会连接host列表中状态最新的那些实例。如果正在连接到的实例挂了，客户端会尝试重新连接到集群中的其他实例，那么此时滞后于故障实例的其它实例都不会接收该连接请求，只有和故障实例版本相同或更新的实例才接收该连接请求。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;持久性&lt;/p&gt;

&lt;p&gt;写操作完成之后将会被持久化存储，不受服务器故障影响。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;及时性&lt;/p&gt;

&lt;p&gt;在对某个znode进行读操作时，应该先执行sync方法，使得读操作的连接所连的zk实例能与leader进行同步，从而能读到最新的类容。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：sync调用是异步的，无需等待调用的返回，zk服务器会保证所有后续的操作会在sync操作完成之后才执行，哪怕这些操作是在执行sync之前被提交的。&lt;/p&gt;

&lt;h1 id=&#34;zookeeper-典型的应用场景&#34;&gt;ZooKeeper 典型的应用场景&lt;/h1&gt;

&lt;p&gt;Zookeeper 从设计模式角度来看，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些观察者做出相应的反应，从而实现集群中类似 Master/Slave 管理模式，关于 Zookeeper 的详细架构等内部细节可以阅读 Zookeeper 的源码&lt;/p&gt;

&lt;p&gt;下面详细介绍这些典型的应用场景，也就是 Zookeeper 到底能帮我们解决那些问题？下面将给出答案。&lt;/p&gt;

&lt;h2 id=&#34;统一命名服务-name-service&#34;&gt;统一命名服务（Name Service）&lt;/h2&gt;

&lt;p&gt;分布式应用中，通常需要有一套完整的命名规则，既能够产生唯一的名称又便于人识别和记住，通常情况下用树形的名称结构是一个理想的选择，树形的名称结构是一个有层次的目录结构，既对人友好又不会重复。说到这里你可能想到了 JNDI，没错 Zookeeper 的 Name Service 与 JNDI 能够完成的功能是差不多的，它们都是将有层次的目录结构关联到一定资源上，但是 Zookeeper 的 Name Service 更加是广泛意义上的关联，也许你并不需要将名称关联到特定资源上，你可能只需要一个不会重复名称，就像数据库中产生一个唯一的数字主键一样。&lt;/p&gt;

&lt;p&gt;Name Service 已经是 Zookeeper 内置的功能，你只要调用 Zookeeper 的 API 就能实现。如调用 create 接口就可以很容易创建一个目录节点。&lt;/p&gt;

&lt;h2 id=&#34;配置管理-configuration-management&#34;&gt;配置管理（Configuration Management）&lt;/h2&gt;

&lt;p&gt;配置的管理在分布式应用环境中很常见，例如同一个应用系统需要多台 PC Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 PC Server，这样非常麻烦而且容易出错。&lt;/p&gt;

&lt;p&gt;像这样的配置信息完全可以交给 Zookeeper 来管理，将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中.&lt;/p&gt;

&lt;h2 id=&#34;集群管理-group-membership&#34;&gt;集群管理（Group Membership）&lt;/h2&gt;

&lt;p&gt;Zookeeper 能够很容易的实现集群管理的功能，如有多台 Server 组成一个服务集群，那么必须要一个“总管”知道当前集群中每台机器的服务状态，一旦有机器不能提供服务，集群中其它集群必须知道，从而做出调整重新分配服务策略。同样当增加集群的服务能力时，就会增加一台或多台 Server，同样也必须让“总管”知道。&lt;/p&gt;

&lt;p&gt;Zookeeper 不仅能够帮你维护当前的集群中机器的服务状态，而且能够帮你选出一个“总管”，让这个总管来管理集群，这就是 Zookeeper 的另一个功能 Leader Election。&lt;/p&gt;

&lt;p&gt;它们的实现方式都是在 Zookeeper 上创建一个 EPHEMERAL 类型的目录节点，然后每个 Server 在它们创建目录节点的父目录节点上调用 getChildren(String path, boolean watch) 方法并设置 watch 为 true，由于是 EPHEMERAL 目录节点，当创建它的 Server 死去，这个目录节点也随之被删除，所以 Children 将会变化，这时 getChildren上的 Watch 将会被调用，所以其它 Server 就知道已经有某台 Server 死去了。新增 Server 也是同样的原理。&lt;/p&gt;

&lt;p&gt;Zookeeper 如何实现 Leader Election，也就是选出一个 Master Server。和前面的一样每台 Server 创建一个 EPHEMERAL 目录节点，不同的是它还是一个 SEQUENTIAL 目录节点，所以它是个 EPHEMERAL_SEQUENTIAL 目录节点。之所以它是 EPHEMERAL_SEQUENTIAL 目录节点，是因为我们可以给每台 Server 编号，我们可以选择当前是最小编号的 Server 为 Master，假如这个最小编号的 Server 死去，由于是 EPHEMERAL 节点，死去的 Server 对应的节点也被删除，所以当前的节点列表中又出现一个最小编号的节点，我们就选择这个节点为当前 Master。这样就实现了动态选择 Master，避免了传统意义上单 Master 容易出现单点故障的问题。&lt;/p&gt;

&lt;h2 id=&#34;zookeeper-实现-locks&#34;&gt;Zookeeper 实现 Locks&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;加锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ZooKeeper 将按照如下方式实现加锁的操作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ZooKeeper 调用 create （）方法来创建一个路径格式为“ &lt;em&gt;locknode&lt;/em&gt;/lock- ”的节点，此节点类型为sequence （连续）和 ephemeral （临时）。也就是说，创建的节点为临时节点，并且所有的节点连续编号，即“ lock-i ”的格式。&lt;/li&gt;
&lt;li&gt;在创建的锁节点上调用 getChildren （）方法，来获取锁目录下的最小编号节点，并且不设置 watch 。&lt;/li&gt;
&lt;li&gt;步骤 2 中获取的节点恰好是步骤 1 中客户端创建的节点，那么此客户端获得此种类型的锁，然后退出操作。&lt;/li&gt;
&lt;li&gt;客户端在锁目录上调用 exists （）方法，并且设置 watch 来监视锁目录下比自己小一个的连续临时节点的状态。&lt;/li&gt;
&lt;li&gt;如果监视节点状态发生变化，则跳转到第 2 步，继续进行后续的操作，直到退出锁竞争。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;解锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ZooKeeper 解锁操作非常简单，客户端只需要将加锁操作步骤 1 中创建的临时节点删除即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Locks extends TestMainClient {
    public static final Logger logger = Logger.getLogger(Locks.class);
    String myZnode;

    public Locks(String connectString, String root) {
        super(connectString);
        this.root = root;
        if (zk != null) {
            try {
                //创建锁节点，并不设置观察
                Stat s = zk.exists(root, false);
                if (s == null) {
                    zk.create(root, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
                }
            } catch (KeeperException e) {
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }
        }
    }
    void getLock() throws KeeperException, InterruptedException{
        List&amp;lt;String&amp;gt; list = zk.getChildren(root, false);
        String[] nodes = list.toArray(new String[list.size()]);
        //对锁目录下 的所有子节点排序
        Arrays.sort(nodes);
        //判断该zkclient创建的临时顺序节点是否为集群中最小的节点
        if(myZnode.equals(root+&amp;quot;/&amp;quot;+nodes[0])){
            doAction();
        }
        else{
            waitForLock(nodes[0]);
        }
    }
    //创建zk客户端的临时瞬时节点，并尝试获取锁
    void check() throws InterruptedException, KeeperException {
        myZnode = zk.create(root + &amp;quot;/lock_&amp;quot; , new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.EPHEMERAL_SEQUENTIAL);
        getLock();
    }
    void waitForLock(String lower) throws InterruptedException, KeeperException {
        Stat stat = zk.exists(root + &amp;quot;/&amp;quot; + lower,true);
        if(stat != null){   //发现最小的目录节点还未被移除，则等待
            mutex.wait();
        }
        else{
            getLock();
        }
    }
    @Override //发现有节点移除，该等待状态的客户端被notify
    public void process(WatchedEvent event) {
        if(event.getType() == Event.EventType.NodeDeleted){
            System.out.println(&amp;quot;得到通知&amp;quot;);
            super.process(event);
            doAction();
        }
    }
    /**
     * 执行其他任务
     */
    private void doAction(){
        System.out.println(&amp;quot;同步队列已经得到同步，可以开始执行后面的任务了&amp;quot;);
    }

    public static void main(String[] args) {
        TestMainServer.start();
        String connectString = &amp;quot;localhost:&amp;quot;+TestMainServer.CLIENT_PORT;

        Locks lk = new Locks(connectString, &amp;quot;/locks&amp;quot;);
        try {
            lk.check();
        } catch (InterruptedException e) {
            logger.error(e);
        } catch (KeeperException e) {
            logger.error(e);
        }
    }


}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;同步队列&#34;&gt;同步队列&lt;/h2&gt;

&lt;p&gt;同步队列用 Zookeeper 实现的实现思路如下：创建一个父目录 /synchronizing，每个成员都监控标志（Set Watch）位目录 /synchronizing/start 是否存在，然后每个成员都加入这个队列，加入队列的方式就是创建 /synchronizing/member_i 的临时目录节点，然后每个成员获取 / synchronizing 目录的所有目录节点，也就是 member_i。判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /synchronizing/start 的出现，如果已经相等就创建 /synchronizing/start。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列。
 */
public class Synchronizing extends TestMainClient {
    int size;
    String name;
    public static final Logger logger = Logger.getLogger(Synchronizing.class);

    /**
     * 构造函数
     *
     * @param connectString
     *            服务器连接
     * @param root
     *            根目录
     * @param size
     *            队列大小
     */
    Synchronizing(String connectString, String root, int size) {
        super(connectString);
        this.root = root;
        this.size = size;

        if (zk != null) {
            try {
                Stat s = zk.exists(root, false);
                if (s == null) {
                    zk.create(root, new byte[0], Ids.OPEN_ACL_UNSAFE,
                            CreateMode.PERSISTENT);
                }
            } catch (KeeperException e) {
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }
        }
        try {
            name = new String(InetAddress.getLocalHost().getCanonicalHostName()
                    .toString());
        } catch (UnknownHostException e) {
            logger.error(e);
        }

    }

    /**
     * 加入队列
     *
     * @return
     * @throws KeeperException
     * @throws InterruptedException
     */

    void addQueue() throws KeeperException, InterruptedException {
        zk.exists(root + &amp;quot;/start&amp;quot;, true);
        zk.create(root + &amp;quot;/&amp;quot; + name, new byte[0], Ids.OPEN_ACL_UNSAFE,
                CreateMode.EPHEMERAL_SEQUENTIAL);
        synchronized (mutex) {
            List&amp;lt;String&amp;gt; list = zk.getChildren(root, false);
            //如果队列中子节点数小于size，则等待，如果不小于size，则创建start目录，其他client则触发事件，执行doAction
            if (list.size() &amp;lt; size) {
                mutex.wait();
            } else {
                zk.create(root + &amp;quot;/start&amp;quot;, new byte[0], Ids.OPEN_ACL_UNSAFE,
                        CreateMode.PERSISTENT);
            }
        }
    }

    @Override
    public void process(WatchedEvent event) {
        if (event.getPath().equals(root + &amp;quot;/start&amp;quot;)
                &amp;amp;&amp;amp; event.getType() == Event.EventType.NodeCreated) {
            System.out.println(&amp;quot;得到通知&amp;quot;);
            super.process(event);
            doAction();
        }
    }

    /**
     * 执行其他任务
     */
    private void doAction() {
        System.out.println(&amp;quot;同步队列已经得到同步，可以开始执行后面的任务了&amp;quot;);
    }

    public static void main(String args[]) {
        // 启动Server
        TestMainServer.start();
        String connectString = &amp;quot;localhost:&amp;quot; + TestMainServer.CLIENT_PORT;
        int size = 1;
        Synchronizing b = new Synchronizing(connectString, &amp;quot;/synchronizing&amp;quot;,
                size);
        try {
            b.addQueue();
        } catch (KeeperException e) {
            logger.error(e);
        } catch (InterruptedException e) {
            logger.error(e);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fifo队列-生产者-消费者&#34;&gt;FIFO队列(生产者-消费者)&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 队列按照 FIFO 方式进行入队和出队操作，例如实现生产者和消费者模型。
 *
 * 实现的思路也非常简单，就是在特定的目录下创建 SEQUENTIAL 类型的子目录
 * /queue_i，这样就能保证所有成员加入队列时都是有编号的，出队列时通过 getChildren( )
 * 方法可以返回当前所有的队列中的元素，然后消费其中最小的一个，这样就能保证 FIFO。
 */
public class FIFOQueue extends TestMainClient {
    public static final Logger logger = Logger.getLogger(FIFOQueue.class);

    /**
     * Constructor
     *
     * @param connectString
     * @param root
     */
    FIFOQueue(String connectString, String root) {
        super(connectString);
        this.root = root;
        if (zk != null) {
            try {
                Stat s = zk.exists(root, false);
                if (s == null) {
                    zk.create(root, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,
                            CreateMode.PERSISTENT);
                }
            } catch (KeeperException e) {
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }
        }
    }

    /**
     * 生产者
     *
     * @param i
     * @return
     */

    boolean produce(int i) throws KeeperException, InterruptedException {
        ByteBuffer b = ByteBuffer.allocate(4);
        byte[] value;
        b.putInt(i);
        value = b.array();
        zk.create(root + &amp;quot;/element&amp;quot;, value, ZooDefs.Ids.OPEN_ACL_UNSAFE,
                CreateMode.PERSISTENT_SEQUENTIAL);
        return true;
    }

    /**
     * 消费者
     *
     * @return
     * @throws KeeperException
     * @throws InterruptedException
     */
    int consume() throws KeeperException, InterruptedException {
        int retvalue = -1;
        Stat stat = null;
        while (true) {
            synchronized (mutex) {
                // 对root的子节点设置监听
                List&amp;lt;String&amp;gt; list = zk.getChildren(root, true);
                // 如果没有任何子节点，则wait
                if (list.size() == 0) {
                    mutex.wait();
                } else {
                    Integer min = new Integer(list.get(0).substring(7));
                    for (String s : list) {
                        Integer tempValue = new Integer(s.substring(7));
                        if (tempValue &amp;lt; min)
                            min = tempValue;
                    }
                    byte[] b = zk.getData(root + &amp;quot;/element&amp;quot; + min, false, stat);
                    // 获取到子节点数据之后 执行delete，并触发事件，执行所有cliet的notify
                    zk.delete(root + &amp;quot;/element&amp;quot; + min, 0);
                    ByteBuffer buffer = ByteBuffer.wrap(b);
                    retvalue = buffer.getInt();
                    return retvalue;
                }
            }
        }
    }

    @Override
    public void process(WatchedEvent event) {
        super.process(event);
    }

    public static void main(String args[]) {
        // 启动Server
        TestMainServer.start();
        String connectString = &amp;quot;localhost:&amp;quot; + TestMainServer.CLIENT_PORT;

        FIFOQueue q = new FIFOQueue(connectString, &amp;quot;/app1&amp;quot;);
        int i;
        Integer max = new Integer(5);

        System.out.println(&amp;quot;Producer&amp;quot;);
        for (i = 0; i &amp;lt; max; i++)
            try {
                q.produce(10 + i);
            } catch (KeeperException e) {
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }

        for (i = 0; i &amp;lt; max; i++) {
            try {
                int r = q.consume();
                System.out.println(&amp;quot;Item: &amp;quot; + r);
            } catch (KeeperException e) {
                i--;
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }
        }

    }
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- K8s addons</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-addons/</link>
          <pubDate>Wed, 17 Apr 2019 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-addons/</guid>
          <description>&lt;p&gt;k8s有很多的插件是必须的，我们下面来看看一些重要的组件。&lt;/p&gt;

&lt;h1 id=&#34;dns&#34;&gt;DNS&lt;/h1&gt;

&lt;p&gt;DNS 有两种配置方式，在 1.3 之前使用 etcd + kube2sky + skydns 的方式，在 1.3 之后可以使用 kubedns + dnsmasq 的方式。从 Kubernetes v1.12 开始，CoreDNS 是推荐的 DNS 服务器，取代了kube-dns。&lt;/p&gt;

&lt;h2 id=&#34;k8s-dns-策略&#34;&gt;K8s DNS 策略&lt;/h2&gt;

&lt;p&gt;Kubernetes 中 Pod 的 DNS 策略有四种类型。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Default：Pod 继承所在主机上的 DNS 配置；&lt;/li&gt;
&lt;li&gt;ClusterFirst：K8s 的默认设置；先在 K8s 集群配置的 coreDNS 中查询，查不到的再去继承自主机的上游 nameserver 中查询；&lt;/li&gt;
&lt;li&gt;ClusterFirstWithHostNet：对于网络配置为 hostNetwork 的 Pod 而言，其 DNS 配置规则与 ClusterFirst 一致；&lt;/li&gt;
&lt;li&gt;None：忽略 K8s 环境的 DNS 配置，只认 Pod 的 dnsConfig 设置。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;coredns-解析域名的过程&#34;&gt;coreDNS 解析域名的过程&lt;/h2&gt;

&lt;p&gt;在部署 pod 的时候，如果用的是 K8s 集群的 DNS(修改 kubelet 的启动配置项，告诉 kubelet，给每个启动的 pod 设置对应的 DNS 信息，一共有两个参数：&amp;ndash;cluster_dns=10.10.10.10 &amp;ndash;cluster_domain=cluster.local，分别是 DNS 在集群中的 vip 和域名后缀，要和 DNS rc 中保持一致)，那么 kubelet 在起 pause 容器的时候，会将其 DNS 解析配置初始化成集群内的配置。&lt;/p&gt;

&lt;p&gt;比如我创建了一个叫 my-nginx 的 deployment，其 pod 中的 resolv.conf 文件如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# kubectl exec -it my-nginx-b67c7f44-hsnpv cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在集群中 pod 之间互相用 svc name 访问的时候，会根据 resolv.conf 文件的 DNS 配置来解析域名，resolv.conf 文件可以由 K8s 指定，也可以通过 pod.spec.dnsConfig 字段自定义，下面来分析具体的过程。&lt;/p&gt;

&lt;p&gt;1、nameserver&lt;/p&gt;

&lt;p&gt;resolv.conf 文件的第一行 nameserver 指定的是 DNS 服务的 IP，这里就是 coreDNS 的 clusterIP：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# kubectl -n kube-system get svc |grep dns
kube-dns   ClusterIP   10.96.0.10   &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   32d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说所有域名的解析，都要经过 coreDNS 的虚拟 IP 10.96.0.10 进行解析，不论是 Kubernetes 内部域名还是外部的域名。&lt;/p&gt;

&lt;p&gt;2、search 域&lt;/p&gt;

&lt;p&gt;resolv.conf 文件的第二行指定的是 DNS search 域。解析域名的时候，将要访问的域名依次带入 search 域，进行 DNS 查询（不同namespace下的pod，第一个域会有所不同）。比如我要在刚才那个 pod 中访问一个域名为 your-nginx 的服务，其进行的 DNS 域名查询的顺序是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;your-nginx.default.svc.cluster.local. -&amp;gt; your-nginx.svc.cluster.local. -&amp;gt; your-nginx.cluster.local.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、options&lt;/p&gt;

&lt;p&gt;resolv.conf 文件的第三行指定的是其他项，最常见的是 dnots。dnots 指的是如果查询的域名包含的点 “.” 小于 5，则先走 search 域，再用绝对域名；如果查询的域名包含点数大于或等于 5，则先用绝对域名，再走 search 域。K8s 中默认的配置是 5。&lt;/p&gt;

&lt;p&gt;也就是说，如果我访问的是 a.b.c.e.f.g ，那么域名查找的顺序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a.b.c.e.f.g. -&amp;gt; a.b.c.e.f.g.default.svc.cluster.local. -&amp;gt; a.b.c.e.f.g.svc.cluster.local. -&amp;gt; a.b.c.e.f.g.cluster.local.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我访问的是 a.b.c.e，那么域名查找的顺序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a.b.c.e.default.svc.cluster.local. -&amp;gt; a.b.c.e.svc.cluster.local. -&amp;gt; a.b.c.e.cluster.local. -&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;k8s域名的策略&#34;&gt;k8s域名的策略&lt;/h2&gt;

&lt;p&gt;Kubernetes 中，域名的全称，必须是 service-name.namespace.svc.cluster.local 这种模式，服务名，就是Kubernetes中 Service 的名称，namespace就是namespace的名称。&lt;/p&gt;

&lt;h2 id=&#34;配置文件&#34;&gt;配置文件&lt;/h2&gt;

&lt;p&gt;CoreDNS 的配置文件是 Corefile 形式的，我们来看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# kubectl -n kube-system get cm coredns -oyaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           upstream
           fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2019-06-10T03:19:01Z&amp;quot;
  name: coredns
  namespace: kube-system
  resourceVersion: &amp;quot;3380134&amp;quot;
  selfLink: /api/v1/namespaces/kube-system/configmaps/coredns
  uid: 7e845ca2-8b2e-11e9-b4eb-005056b40224
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、第一部分&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubernetes cluster.local in-addr.arpa ip6.arpa {
   pods insecure
   upstream
   fallthrough in-addr.arpa ip6.arpa
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指明 cluster.local 后缀的域名，都是 kubernetes 内部域名，coredns 会监听 service 的变化来维护域名关系，所以cluster.local 相关域名都在这里解析。在这里也实现了启动服务后进行的DNS注册&lt;/p&gt;

&lt;p&gt;2、第二部分&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;proxy . /etc/resolv.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;proxy 指 coredns 中没有找到记录，则去 /etc/resolv.conf 中的 nameserver 请求解析，而 coredns 容器中的 /etc/resolv.conf 是继承自宿主机的。实际效果是如果不是 k8s 内部域名，就会去默认的 dns 服务器请求解析，并返回给 coredns 的请求者。&lt;/p&gt;

&lt;p&gt;3、其他&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;prometheus：CoreDNS 的监控地址为： &lt;a href=&#34;http://localhost:9153/metrics&#34;&gt;http://localhost:9153/metrics&lt;/a&gt; ，满足 Prometheus 的格式。&lt;/li&gt;
&lt;li&gt;cache：允许缓存&lt;/li&gt;
&lt;li&gt;loop：如果找到循环，则检测简单的转发循环并停止 CoreDNS 进程。&lt;/li&gt;
&lt;li&gt;reload：允许 Corefile 的配置自动更新。在更改 ConfigMap 后两分钟，修改生效&lt;/li&gt;
&lt;li&gt;loadbalance：这是一个循环 DNS 负载均衡器，可以在答案中随机化 A，AAAA 和 MX 记录的顺序。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;k8s服务注册&#34;&gt;k8s服务注册&lt;/h2&gt;

&lt;p&gt;Kubernetes 使用 DNS 作为服务注册表。为了满足这一需要，每个 Kubernetes 集群都会在 kube-system 命名空间中用 Pod 的形式运行一个 DNS 服务，通常称之为集群 DNS。DNS会监听service的变化，每个 Kubernetes 服务都会自动注册到集群 DNS 之中。注册过程大致如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;向 API Server 用 POST 方式提交一个新的 Service 定义；&lt;/li&gt;
&lt;li&gt;这个请求需要经过认证、鉴权以及其它的准入策略检查过程之后才会放行；&lt;/li&gt;
&lt;li&gt;Service 得到一个 ClusterIP（虚拟 IP 地址），并保存到集群数据仓库；&lt;/li&gt;
&lt;li&gt;在集群范围内传播 Service 配置；&lt;/li&gt;
&lt;li&gt;集群 DNS 服务得知该 Service 的创建，据此创建必要的 DNS A 记录。DNS 中注册的名称就是 metadata.name，而 ClusterIP 则由 Kubernetes 自行分配。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CoreDNS 实现了一个控制器，会对 API Server 进行监听，一旦发现有新建的 Service 对象，就创建一个从 Service 名称映射到 ClusterIP 的域名记录。这样 Service 就不必自行向 DNS 进行注册，CoreDNS 控制器会关注新创建的 Service 对象，并实现后续的 DNS 过程。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- operator</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-operator/</link>
          <pubDate>Wed, 17 Apr 2019 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-operator/</guid>
          <description>&lt;p&gt;Operator 是指一类基于 Kubernetes 自定义资源对象(CRD)和控制器(Controller)的云原生拓展服务，其中 CRD 定义了每个 operator 所创建和管理的自定义资源对象，Controller 则包含了管理这些对象所相关的运维逻辑代码。&lt;/p&gt;

&lt;p&gt;其实operator和控制器是差不多，只不过operator是针对特定应用程序的控制器，比如数据库etcd，需要结合很多组件的专业知识做逻辑处理。&lt;/p&gt;

&lt;h1 id=&#34;operator-pattern&#34;&gt;Operator Pattern&lt;/h1&gt;

&lt;p&gt;Operator是由CoreOS公司开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的一些专业知识，比如创建一个数据库的Operator，则必须对创建的数据库的各种运维方式非常了解，创建Operator的关键是CRD（自定义资源）的设计。&lt;/p&gt;

&lt;p&gt;CRD是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在YAML文件里定义的那些spec都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。&lt;/p&gt;

&lt;p&gt;Operator是将运维人员对软件操作的知识给代码化，同时利用 Kubernetes 强大的抽象来管理大规模的软件应用。目前CoreOS官方提供了几种Operator的实现，其中就包括我们今天的主角：Prometheus Operator，Operator的核心实现就是基于 Kubernetes 的以下两个概念：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;资源：对象的状态定义&lt;/li&gt;
&lt;li&gt;控制器：观测、分析和行动，以调节资源的分布&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前已经有很多已经实现的operator，可以直接去&lt;a href=&#34;https://operatorhub.io/&#34;&gt;社区&lt;/a&gt;查看Kubernetes 社区推荐的一些 Operator 范例。&lt;/p&gt;

&lt;h1 id=&#34;实例分析&#34;&gt;实例分析&lt;/h1&gt;

&lt;p&gt;以mysql-operator这个operator为例，我们具体分析一下一个Kubernetes Operator具体是如何实现的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;入口函数main&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要启动run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
  if err := app.Run(opts); err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;%v\n&amp;quot;, err)
        os.Exit(1)
    }
...
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;run函数&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;/ Run starts the mysql-operator controllers. This should never exit.
func Run(s *operatoropts.MySQLOperatorOpts) error {
// 构造kubeconfig以便连接kubernetes的APIServer
kubeconfig, err := clientcmd.BuildConfigFromFlags(s.Master, s.KubeConfig)
if err != nil {
    return err
}

...

// 构造kubeClient、 mysqlopClient, 以便操作Kubernetes里的一些资源
kubeClient := kubernetes.NewForConfigOrDie(kubeconfig)
mysqlopClient := clientset.NewForConfigOrDie(kubeconfig)
 // 构造一些共享的informer，以便监听自定义对象及kubernetes里的一些核心资源
// Shared informers (non namespace specific).
operatorInformerFactory := informers.NewFilteredSharedInformerFactory(mysqlopClient, resyncPeriod(s)(), s.Namespace, nil)
kubeInformerFactory := kubeinformers.NewFilteredSharedInformerFactory(kubeClient, resyncPeriod(s)(), s.Namespace, nil)
 var wg sync.WaitGroup
 // 构造自定义类型mysqlcluster的控制器
clusterController := cluster.NewController(
    *s,
    mysqlopClient,
    kubeClient,
    operatorInformerFactory.MySQL().V1alpha1().Clusters(),
    kubeInformerFactory.Apps().V1beta1().StatefulSets(),
    kubeInformerFactory.Core().V1().Pods(),
    kubeInformerFactory.Core().V1().Services(),
    30*time.Second,
    s.Namespace,
)
wg.Add(1)
go func() {
    defer wg.Done()
    clusterController.Run(ctx, 5)
}()

// 下面分别为每个自定义类型构造了相应的控制器
...
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;控制器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kubernetes Operator的核心逻辑就在自定义类型的控制器里面&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewController creates a new MySQLController.
func NewController(
    ...
) *MySQLController {
  // 构造MySQLController
  m := MySQLController{
        ...
    }
  // 监控自定义类型mysqlcluster的变化(增加、更新、删除)，这里看一看m.enqueueCluster函数可以发现都只是把发生变化的自定义对象的名称放入工作队列中
    clusterInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: m.enqueueCluster,
        UpdateFunc: func(old, new interface{}) {
            m.enqueueCluster(new)
        },
        DeleteFunc: func(obj interface{}) {
            cluster, ok := obj.(*v1alpha1.Cluster)
            if ok {
                m.onClusterDeleted(cluster.Name)
            }
        },
    })


// Run函数里会启动工作协程处理上述放入工作队列的自定义对象的名称
func (m *MySQLController) Run(ctx context.Context, threadiness int) {
  ...
  // Launch two workers to process Foo resources
    for i := 0; i &amp;lt; threadiness; i++ {
        go wait.Until(m.runWorker, time.Second, ctx.Done())
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从runWorker函数一步步跟踪过程，发现真正干活的是syncHandler函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *MySQLController) syncHandler(key string) error {
    ...
    nsName := types.NamespacedName{Namespace: namespace, Name: name}
    // Get the Cluster resource with this namespace/name.
    cluster, err := m.clusterLister.Clusters(namespace).Get(name)
    if err != nil {
      // 如果自定义资源对象已不存在，则不用处理
        // The Cluster resource may no longer exist, in which case we stop processing.
        if apierrors.IsNotFound(err) {
            utilruntime.HandleError(fmt.Errorf(&amp;quot;mysqlcluster &#39;%s&#39; in work queue no longer exists&amp;quot;, key))
            return nil
        }
        return err
    }
    cluster.EnsureDefaults()
    // 校验自定义资源对象
    if err = cluster.Validate(); err != nil {
        return errors.Wrap(err, &amp;quot;validating Cluster&amp;quot;)
    }
  // 给自定义资源对象设置一些默认属性
    if cluster.Spec.Repository == &amp;quot;&amp;quot; {
        cluster.Spec.Repository = m.opConfig.Images.DefaultMySQLServerImage
    }
    ...
    svc, err := m.serviceLister.Services(cluster.Namespace).Get(cluster.Name)
    // If the resource doesn&#39;t exist, we&#39;ll create it
    // 如果该自定义资源对象存在，则应该要创建相应的Serivce，如Serivce不存在，则创建
    if apierrors.IsNotFound(err) {
        glog.V(2).Infof(&amp;quot;Creating a new Service for cluster %q&amp;quot;, nsName)
        svc = services.NewForCluster(cluster)
        err = m.serviceControl.CreateService(svc)
    }
    // If an error occurs during Get/Create, we&#39;ll requeue the item so we can
    // attempt processing again later. This could have been caused by a
    // temporary network failure, or any other transient reason.
    if err != nil {
        return err
    }
    // If the Service is not controlled by this Cluster resource, we should
    // log a warning to the event recorder and return.
    if !metav1.IsControlledBy(svc, cluster) {
        msg := fmt.Sprintf(MessageResourceExists, &amp;quot;Service&amp;quot;, svc.Namespace, svc.Name)
        m.recorder.Event(cluster, corev1.EventTypeWarning, ErrResourceExists, msg)
        return errors.New(msg)
    }
    ss, err := m.statefulSetLister.StatefulSets(cluster.Namespace).Get(cluster.Name)
    // If the resource doesn&#39;t exist, we&#39;ll create it
    // 如果该自定义资源对象存在，则应该要创建相应的StatefulSet，如StatefulSet不存在，则创建
    if apierrors.IsNotFound(err) {
        glog.V(2).Infof(&amp;quot;Creating a new StatefulSet for cluster %q&amp;quot;, nsName)
        ss = statefulsets.NewForCluster(cluster, m.opConfig.Images, svc.Name)
        err = m.statefulSetControl.CreateStatefulSet(ss)
    }
    // If an error occurs during Get/Create, we&#39;ll requeue the item so we can
    // attempt processing again later. This could have been caused by a
    // temporary network failure, or any other transient reason.
    if err != nil {
        return err
    }
    // If the StatefulSet is not controlled by this Cluster resource, we
    // should log a warning to the event recorder and return.
    if !metav1.IsControlledBy(ss, cluster) {
        msg := fmt.Sprintf(MessageResourceExists, &amp;quot;StatefulSet&amp;quot;, ss.Namespace, ss.Name)
        m.recorder.Event(cluster, corev1.EventTypeWarning, ErrResourceExists, msg)
        return fmt.Errorf(msg)
    }
    // Upgrade the required component resources the current MySQLOperator version.
    // 确保StatefulSet上的BuildVersion与自定义资源对象上的一致，如不一致，则修改得一致
    if err := m.ensureMySQLOperatorVersion(cluster, ss, buildversion.GetBuildVersion()); err != nil {
        return errors.Wrap(err, &amp;quot;ensuring MySQL Operator version&amp;quot;)
    }
    // Upgrade the MySQL server version if required.
    if err := m.ensureMySQLVersion(cluster, ss); err != nil {
        return errors.Wrap(err, &amp;quot;ensuring MySQL version&amp;quot;)
    }
    // If this number of the members on the Cluster does not equal the
    // current desired replicas on the StatefulSet, we should update the
    // StatefulSet resource.
    // 如果StatefulSet的Replicas值与自定义资源对象上配置不一致，则更新StatefulSet
    if cluster.Spec.Members != *ss.Spec.Replicas {
        glog.V(4).Infof(&amp;quot;Updating %q: clusterMembers=%d statefulSetReplicas=%d&amp;quot;,
            nsName, cluster.Spec.Members, ss.Spec.Replicas)
        old := ss.DeepCopy()
        ss = statefulsets.NewForCluster(cluster, m.opConfig.Images, svc.Name)
        if err := m.statefulSetControl.Patch(old, ss); err != nil {
            // Requeue the item so we can attempt processing again later.
            // This could have been caused by a temporary network failure etc.
            return err
        }
    }
    // Finally, we update the status block of the Cluster resource to
    // reflect the current state of the world.
    // 最后更新自定义资源对象的状态
    err = m.updateClusterStatus(cluster, ss)
    if err != nil {
        return err
    }
    m.recorder.Event(cluster, corev1.EventTypeNormal, SuccessSynced, MessageResourceSynced)
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;整个Operator大概就是这样了。&lt;/p&gt;

&lt;p&gt;Operator 其实就是一段代码，这段代码 Watch 了 etcd 里一个描述分布式应用集群的API 对象，然后这段代码通过实现 Kubernetes 的控制器模式，来保证这个集群始终跟用户的定义完全相同。而在这个过程中，Operator 也有能力利用 Kubernetes 的存储、网络插件等外部资源，协同的为应用状态的保持提供帮助。&lt;/p&gt;

&lt;p&gt;​所以说，Operator 本身在实现上，其实是在 Kubernetes 声明式 API 基础上的一种“微创新”。它合理的利用了 Kubernetes API 可以添加自定义 API 类型的能力，然后又巧妙的通过 Kubernetes 原生的“控制器模式”，完成了一个面向分布式应用终态的调谐过程。&lt;/p&gt;

&lt;h1 id=&#34;自己开发一个operator&#34;&gt;自己开发一个operator&lt;/h1&gt;

&lt;p&gt;上面的代码是在官方推出的工具之前来写的，大部分代码的逻辑其实就是控制器的逻辑，很多都是一样的，所以官方推出了一个工具&lt;a href=&#34;https://github.com/operator-framework/operator-sdk&#34;&gt;operator-sdk&lt;/a&gt;来生成相关模块代码，让我们专注于控制器逻辑的开发处理。&lt;/p&gt;

&lt;p&gt;还有很多类似的工具&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;KUDO (Kubernetes 通用声明式 Operator)&lt;/li&gt;
&lt;li&gt;kubebuilder，kubernetes SIG 在维护的一个项目，用于写控制器的，所以也是可以写operator&lt;/li&gt;
&lt;li&gt;Metacontroller，可与 Webhook 结合使用，以实现自己的功能。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装operator-sdk&#34;&gt;安装operator sdk&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;export RELEASE_VERSION=v0.13.0
curl -LO https://github.com/operator-framework/operator-sdk/releases/download/${RELEASE_VERSION}/operator-sdk-${RELEASE_VERSION}-x86_64-linux-gnu
chmod +x operator-sdk-${RELEASE_VERSION}-x86_64-linux-gnu &amp;amp;&amp;amp; sudo mkdir -p /usr/local/bin/ &amp;amp;&amp;amp; sudo cp operator-sdk-${RELEASE_VERSION}-x86_64-linux-gnu /usr/local/bin/operator-sdk &amp;amp;&amp;amp; rm operator-sdk-${RELEASE_VERSION}-x86_64-linux-gnu
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;创建项目&#34;&gt;创建项目&lt;/h2&gt;

&lt;p&gt;用operator sdk 创建项目模板，这里用官方提供的一个sample-controller的模板：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;operator-sdk new &amp;lt;controller-name&amp;gt; --repo github.com/kubernetes/sample-controller
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;创建项目&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;项目结构目录创建完成，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ operator-sdk new test-controller --repo github.com/kubernetes/sample-controller
$ tree
.
├── build
│   ├── bin
│   │   ├── entrypoint
│   │   └── user_setup
│   └── Dockerfile
├── cmd
│   └── manager
│       └── main.go
├── deploy
│   ├── operator.yaml
│   ├── role_binding.yaml
│   ├── role.yaml
│   └── service_account.yaml
├── go.mod
├── go.sum
├── pkg
│   ├── apis
│   │   └── apis.go
│   └── controller
│       └── controller.go
├── tools.go
└── version
    └── version.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单做一个说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cmd - 包含 main.go 文件，使用 operator-sdk API 初始化和启动当前 Operator 的入口。&lt;/li&gt;
&lt;li&gt;deploy - 包含一组用于在 Kubernetes 集群上进行部署的通用的 Kubernetes 资源清单文件。&lt;/li&gt;
&lt;li&gt;pkg/apis - 包含定义的 API 和自定义资源（CRD）的目录树，这些文件允许 sdk 为 CRD 生成代码并注册对应的类型，以便正确解码自定义资源对象。&lt;/li&gt;
&lt;li&gt;pkg/controller - 用于编写所有的操作业务逻辑的地方&lt;/li&gt;
&lt;li&gt;vendor - golang vendor 文件夹，其中包含满足当前项目的所有外部依赖包，通过 go dep 管理该目录。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;创建CRD&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;创建CRD：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;operator-sdk add api --api-version=&amp;lt;api的版本&amp;gt; --kind=&amp;lt;类型名称&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建CRD后，多出来了文件夹：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ operator-sdk add api --api-version=test.k8s.realibox.com/v1 --kind=Realibox

INFO[0000] Generating api version test.k8s.realibox.com/v1 for kind Realibox.
INFO[0000] Created pkg/apis/test/group.go
INFO[0002] Created pkg/apis/test/v1/realibox_types.go
INFO[0002] Created pkg/apis/addtoscheme_test_v1.go
INFO[0002] Created pkg/apis/test/v1/register.go
INFO[0002] Created pkg/apis/test/v1/doc.go
INFO[0002] Created deploy/crds/test.k8s.realibox.com_v1_realibox_cr.yaml
INFO[0004] Created deploy/crds/test.k8s.realibox.com_realiboxes_crd.yaml
INFO[0004] Running deepcopy code-generation for Custom Resource group versions: [test:[v1], ]
INFO[0014] Code-generation complete.
INFO[0014] Running CRD generation for Custom Resource group versions: [test:[v1], ]
INFO[0014] Created deploy/crds/test.k8s.realibox.com_realiboxes_crd.yaml
INFO[0014] CRD generation complete.
INFO[0014] API generation complete.

$ tree
...
├── pkg
│   ├── apis
│   │   ├── addtoscheme_test_v1.go
│   │   ├── apis.go
│   │   └── test
│   │       ├── group.go
│   │       └── v1
│   │           ├── doc.go
│   │           ├── realibox_types.go
│   │           ├── register.go
│   │           └── zz_generated.deepcopy.go
│   └── controller
│       └── controller.go
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test 文件夹下面放得就是 CRD，我们通过pkg/apis/test/v1/*_types.go文件定义我们的CRD结构，主要是Spec和Status：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim pkg/apis/test/v1/realibox_types.go
...
// RealiboxSpec defines the desired state of Realibox
type RealiboxSpec struct {
        // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
        // Important: Run &amp;quot;operator-sdk generate k8s&amp;quot; to regenerate code after modifying this file
        // Add custom validation using kubebuilder tags: https://book-v1.book.kubebuilder.io/beyond_basics/generating_crd.html
}

type RealiboxStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
    // Important: Run &amp;quot;operator-sdk generate k8s&amp;quot; to regenerate code after modifying this file
    // Add custom validation using kubebuilder tags: https://book-v1.book.kubebuilder.io/beyond_basics/generating_crd.html
}
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我们只改Spec字段，将RealiboxSpec结构体改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type RealiboxSpec struct {
    Domain string `json:&amp;quot;domain,omitempty&amp;quot;`
    OSS string `json:&amp;quot;oss,omitempty&amp;quot;`
    Size    string `json:&amp;quot;size,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新CRD文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;operator-sdk generate k8s
operator-sdk generate crds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CRD本质是一种k8s的资源，因此要使用crd，需要在K8s集群上创建CRD：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f deploy/crds/test.k8s.realibox.com_realiboxes_crd.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看集群CRD：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get crd
NAME                                      CREATED AT
clusterauthtokens.cluster.cattle.io       2020-08-29T06:41:42Z
clusteruserattributes.cluster.cattle.io   2020-08-29T06:41:42Z
realiboxes.test.k8s.realibox.com          2020-08-29T07:57:44Z
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;编写controller&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;创建好 CRD 后，我们可以编写 controller 了，先创建一个 controller 监听和核对新创建的realibox资源类型：&lt;/p&gt;

&lt;p&gt;命令行说明:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;operator-sdk add controller --api-version=&amp;lt;api的版本&amp;gt; --kind=&amp;lt;类型名称&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ operator-sdk add controller --api-version=test.k8s.realibox.com/v1 --kind=Realibox

$ tree
...
├── pkg
│   ├── apis
│   │   ├── addtoscheme_test_v1.go
│   │   ├── apis.go
│   │   └── test
│   │       ├── group.go
│   │       └── v1
│   │           ├── doc.go
│   │           ├── realibox_types.go
│   │           ├── register.go
│   │           └── zz_generated.deepcopy.go
│   └── controller
│       ├── add_realibox.go
│       ├── controller.go
│       └── realibox
│           └── realibox_controller.go
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在pkg/controller目录下生成了controller代码，在pkg/controller/realibox/realibox_controller.go编写代码逻辑即可，在这里，我将CR信息在创建pod之前打印到日志里：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
func (r *ReconcileRealibox) Reconcile(request reconcile.Request) (reconcile.Result, error) {
  ...
    reqLogger.Info(fmt.Sprintf(&amp;quot;Domain: %v created, oss info:%v, size: %v&amp;quot;,instance.Spec.Domain,instance.Spec.OSS, instance.Spec.Size))
    // Define a new Pod object
    pod := newPodForCR(instance)

    ...
}
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面就可以运行 controller 了。&lt;/p&gt;

&lt;p&gt;注：如果希望对集群进行更多地复杂操作，可以使用client-go来操作 Kubernetes 的资源，client-go是一个对 Kubernetes API 进行封装的库，由 Kubernetes 官方提供，还是十分好用的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;运行 controller&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;运行controller有两种方法，可以在本地直接运行controller，也可以打包到k8s运行。&lt;/p&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/db/etcd-operator/&#34;&gt;etcd-operator&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;发展&#34;&gt;发展&lt;/h1&gt;

&lt;p&gt;官方是希望通过Operator封装大部分基础服务软件的运维操作的，但目前很多Operator并不完善。比如虽然形式上给Operator划分了5个成熟度等级，但实际上大部分Operator仅只能完成安装部署而已。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/opetator/operator/&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;还有很多Operator明确说明目前只是alpha状态，目前不建议投入生产。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus gpu_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/gpu_exporter/</link>
          <pubDate>Mon, 15 Apr 2019 19:21:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/gpu_exporter/</guid>
          <description>&lt;p&gt;随着区块链、人工智能的盛行，越来越多的场景开始使用GPU，而其监控也随之受到重视。目前生产环境中大部分GPU为NVIDIA厂商，今天就聊聊NVIDIA如何进行GPU的监控。&lt;/p&gt;

&lt;h1 id=&#34;nvml&#34;&gt;NVML&lt;/h1&gt;

&lt;p&gt;NVIDIA Management Library 是英伟达提供用于监控、管理GPU的API，底层是用C实现。我们常用的nvidia-smi命令也是基于该库进行的封装。官方同时也提供了perl、python版本的库。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@host104722317 ~]# nvidia-smi
Tue Sep  8 10:16:02 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P40           Off  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0    49W / 250W |   7083MiB / 22912MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P40           Off  | 00000000:88:00.0 Off |                    0 |
| N/A   31C    P0    50W / 250W |  12843MiB / 22912MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      8336      C   ./image_ds                                   193MiB |
|    0     13531      C   ./image_ds                                   193MiB |
|    0     17306      C   ./image_ds                                   193MiB |
|    0     36354      C   ...entCS/imagesearch/cs/cs01_self/image_cs  1809MiB |
|    0     37691      C   ...rc/trandeploy/imagesearch/as00/image_as  1561MiB |
|    0     37714      C   ...rc/trandeploy/imagesearch/as01/image_as  1561MiB |
|    0     37738      C   ...rc/trandeploy/imagesearch/as02/image_as  1561MiB |
|    1      8336      C   ./image_ds                                   411MiB |
|    1     13531      C   ./image_ds                                   411MiB |
|    1     17306      C   ./image_ds                                   411MiB |
|    1     36376      C   ...entCS/imagesearch/cs/cs11_self/image_cs  1809MiB |
|    1     37622      C   ...rc/trandeploy/imagesearch/as10/image_as  1561MiB |
|    1     37645      C   ...rc/trandeploy/imagesearch/as11/image_as  4401MiB |
|    1     37668      C   ...rc/trandeploy/imagesearch/as12/image_as  1561MiB |
|    1     38160      C   ...entDS/commentVideoDS/ds01_self/image_ds  1131MiB |
|    1     38181      C   ...entDS/commentVideoDS/ds11_self/image_ds  1131MiB |
+-----------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;监控项&#34;&gt;监控项&lt;/h1&gt;

&lt;p&gt;常规有如下几个监控项，基本可以覆盖大部分应用场景。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;温度&lt;/li&gt;
&lt;li&gt;GPU利用率&lt;/li&gt;
&lt;li&gt;内存总量&lt;/li&gt;
&lt;li&gt;已分配内存&lt;/li&gt;
&lt;li&gt;内存利用率
内存利用率跟已分配内存是有区别的，我理解只要有任务被分配到GPU就会分配内存，而分配的内存究竟有没有被使用才是内存利用率。根据官方的描述：内存利用率是指在每个采集周期内（可能在1/6至1秒）读写占用的时间百分比。&lt;/li&gt;
&lt;li&gt;电源使用情况&lt;/li&gt;
&lt;li&gt;风扇速度
风扇并不是所有GPU都有&lt;/li&gt;
&lt;li&gt;GPU数量&lt;/li&gt;
&lt;li&gt;GPU平均利用率&lt;/li&gt;
&lt;li&gt;GPU平均内存利用率&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;p&gt;需要借助第三方库github.com/mindprince/gonvml，利用cgo的特性直接使用官方的API。&lt;/p&gt;

&lt;p&gt;这个方案是在Linux环境下，依赖libnvidia-ml.so.1库。这个库主要包含2个文件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nvml.h的存在让我们不依赖于构建环境中存在的NVML。&lt;/li&gt;
&lt;li&gt;bindings.go是调用NVML函数的cgo桥。 bindings.go中的cgo前导代码使用dlopen动态加载NVML并使其功能可用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细的代码直接到github看就可以，不在这里贴出了。&lt;/p&gt;

&lt;p&gt;Agent相关代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package funcs

import (
    &amp;quot;log&amp;quot;

    &amp;quot;github.com/mindprince/gonvml&amp;quot;
    &amp;quot;github.com/open-falcon/falcon-plus/common/model&amp;quot;
)

// 需要load libnvidia-ml.so.1库
func GpuMetrics() (L []*model.MetricValue) {

    if err := gonvml.Initialize(); err != nil {
        log.Println(&amp;quot;Initialize error: &amp;quot;, err)
        return
    }

    defer gonvml.Shutdown()

    count, err := gonvml.DeviceCount()
    if err != nil {
        log.Println(&amp;quot;DeviceCount error: &amp;quot;, err)
        return
    }

    if count == 0 {
        return
    }

    temperature := uint(0)
    totalMemory := uint64(0)
    usedMemory := uint64(0)
    gpuUtilization := uint(0)
    memoryUtilization := uint(0)
    powerUsage := uint(0)
    allUtilization := uint(0)
    allMemoryUtilization := uint(0)

    for i := 0; i &amp;lt; int(count); i++ {
        dev, err := gonvml.DeviceHandleByIndex(uint(i))
        if err != nil {
            log.Println(&amp;quot;DeviceHandleByIndex error:&amp;quot;, err)
            continue
        }

        uuid, err := dev.UUID()
        if err != nil {
            log.Println(&amp;quot;dev.UUID error&amp;quot;, err)
        }

        tag := &amp;quot;uuid=&amp;quot; + uuid

        // 不是所有gpu都有风扇
        fanSpeed, err := dev.FanSpeed()
        if err != nil {
            log.Println(&amp;quot;dev.FanSpeed error: &amp;quot;, err)
        } else {
            L = append(L, GaugeValue(&amp;quot;gpu.fan.speed&amp;quot;, fanSpeed, tag))
        }

        temperature, err = dev.Temperature()
        if err != nil {
            log.Println(&amp;quot;dev.Temperature error: &amp;quot;, err)
            continue
        }

        totalMemory, usedMemory, err = dev.MemoryInfo()
        if err != nil {
            log.Println(&amp;quot;dev.MemoryInfo error: &amp;quot;, err)
            continue
        }

        // 单位换算为兆
        totalBillion := float64(totalMemory / 1024 / 1024)
        usedBillion := float64(usedMemory / 1024 / 1024)

        gpuUtilization, memoryUtilization, err = dev.UtilizationRates()
        if err != nil {
            log.Println(&amp;quot;dev.UtilizationRates error: &amp;quot;, err)
            continue
        }

        allUtilization += gpuUtilization
        allMemoryUtilization += memoryUtilization

        powerUsage, err = dev.PowerUsage()
        if err != nil {
            log.Println(&amp;quot;dev.PowerUsage error: &amp;quot;, err)
        }

        // 单位换算为瓦特
        powerWatt := float64(powerUsage / 1000)

        L = append(L, GaugeValue(&amp;quot;gpu.temperature&amp;quot;, temperature, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.memory.total&amp;quot;, totalBillion, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.memory.used&amp;quot;, usedBillion, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.memory.util&amp;quot;, memoryUtilization, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.util&amp;quot;, gpuUtilization, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.power.usage&amp;quot;, powerWatt, tag))
    }

    L = append(L, GaugeValue(&amp;quot;gpu.count&amp;quot;, count))
    L = append(L, GaugeValue(&amp;quot;gpu.util.avg&amp;quot;, allUtilization/count))
    L = append(L, GaugeValue(&amp;quot;gpu.memory.util.avg&amp;quot;, allMemoryUtilization/count))
    return L
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Process Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/process_exporter/</link>
          <pubDate>Tue, 09 Apr 2019 16:44:29 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/process_exporter/</guid>
          <description>&lt;p&gt;Process-exporter 主要是对进程进行监控。&lt;/p&gt;

&lt;h1 id=&#34;安装使用&#34;&gt;安装使用&lt;/h1&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;wget https://github.com/ncabatoff/process-exporter/releases/download/v0.4.0/process-exporter-0.4.0.linux-amd64.tar.gz 
tar -xvf process-exporter-0.4.0.linux-amd64.tar.gz -C /usr/local/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置&#34;&gt;配置&lt;/h2&gt;

&lt;p&gt;选择要监视的进程并将它的分组，提供命令行参数或者使用YAML配置文件两种方法。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;整体模版&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;process_names:
  - matcher1
  - matcher2
  ...
  - matcherN
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;默认定义全部进程监控&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim conf.yaml
process_names:
  - name: &amp;quot;{{.Comm}}&amp;quot;
    cmdline:
    - &#39;.+&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;name 进程名，可以使用变量模版匹配&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;模版变量&lt;/p&gt;

&lt;p&gt;可用的模板变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{.Comm}} 包含原始可执行文件的basename，换句话说 在/proc/&amp;lt;pid&amp;gt;/stat
{{.ExeBase}} 包含可执行文件的basename，这个是name的默认值
{{.ExeFull}} 包含可执行文件的完全限定路径
{{.Username}} contains the username of the effective user
{{.Matches}} 映射包含应用命令行所产生的所有匹配项
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建配置文件定义进程名监控&lt;/p&gt;

&lt;p&gt;Process-exporter 可以进程名字匹配进程，获取进程信息。匹配规则由name对应的模板变量决定，以下表示监控进程名字为nginx 与 zombie 的进程状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim process-name.yaml
process_names:
  - name: &amp;quot;{{.Matches}}&amp;quot;
    cmdline:
    - &#39;nginx&#39;

  - name: &amp;quot;{{.Matches}}&amp;quot;
    cmdline:
    - &#39;zombie&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;进程选择器（process selectors）&lt;/p&gt;

&lt;p&gt;process_names 中的每个项必须包含一个或者多个选择器(comm，exe &amp;ndash;OR或者 cmdline&amp;ndash;AND) ；如果存在多个选择器，则它们都必须匹配&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;启动服务&#34;&gt;启动服务&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;./process-exporter -config.path process-name.yaml &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;查看数据&#34;&gt;查看数据&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;curl http://localhost:9256/metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;监控项&#34;&gt;监控项&lt;/h1&gt;

&lt;p&gt;常用进程监控项&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;总进程数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(namedprocess_namegroup_states)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;总僵尸进程数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(namedprocess_namegroup_states{state=&amp;quot;Zombie&amp;quot;})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;namedprocess_namegroup_都是以这个开头的&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;num_procs：Number of processes in this group&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;cpu_seconds_total：CPU usage，类似于node_cpu_seconds_total&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;read_bytes_total：io read&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;write_bytes_total：io write&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;major_page_faults_total：Number of major page faults based on /proc/[pid]/stat&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;minor_page_faults_total：Number of minor page faults based on /proc/[pid]/stat&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;context_switches_total：voluntary_ctxt_switches&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;memory_bytes：memory used&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;open_filedesc：Number of file descriptors，基于/proc/[pid]/fd&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;worst_fd_ratio：fd limit&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;oldest_start_time_seconds：the oldest process in the group started&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;num_threads：Sum of number of threads of all process in the group&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;states：Running, Sleeping, Waiting, Zombie, Other：Number of threads in the group&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有线程的一些指标，默认关闭，需要开启&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;读取/proc/[pid]/（stat，fd）等文件的数据进行解析来获取进程的相关状态和资源情况。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- K8s Tutorial</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-tutorial/</link>
          <pubDate>Thu, 04 Apr 2019 11:26:37 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-tutorial/</guid>
          <description>&lt;p&gt;kubernetes是一种容器管理系统，可以支持容器自身的不足&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;解决了容器安全问题：密钥与配置管理&lt;/li&gt;
&lt;li&gt;容器管理：部署，升级回滚，扩缩容，自愈等。&lt;/li&gt;
&lt;li&gt;网络问题：服务发现和负载均衡&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;直白的说，Kubernetes用户可以通过编写一个yaml或者json格式的配置文件，也可以通过工具/代码生成或直接请求kubernetes API创建应用，该配置文件中包含了用户想要应用程序保持的状态，不论整个kubernetes集群中的个别主机发生什么问题，都不会影响应用程序的状态，你还可以通过改变该配置文件或请求kubernetes API来改变应用程序的状态。&lt;/p&gt;

&lt;h1 id=&#34;组件和概念&#34;&gt;组件和概念&lt;/h1&gt;

&lt;p&gt;Kubernetes主要由以下几个核心组件组成：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;master&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；可以说是k8s的内部交互的网关总线。&lt;/li&gt;
&lt;li&gt;controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；是控制器的管理者，负责很多的控制器。&lt;/li&gt;
&lt;li&gt;scheduler负责资源的调度，按照（预选优选）调度策略将Pod调度到相应的机器上；&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;node&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;kubelet负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；主要干活的对象，负责和docker进行交互，创建容器，维护容器。&lt;/li&gt;
&lt;li&gt;kube-proxy负责为Service提供cluster内部的服务发现和负载均衡；提供了一种网络模式。&lt;/li&gt;
&lt;li&gt;docker engine docker引擎，负责docker的创建和管理，目前大有使用containerd的趋势。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;网络&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;fannel实现pod网络的互通&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;数据库&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;etcd保存了整个集群的状态；实现持久化数据的k-vdb&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;重要概念&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;namespaces:命名空间，主要是实现资源的隔离，Namespace是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。比如我们将namespace命名为系统名，一个系统一个namespace。&lt;/li&gt;
&lt;li&gt;pod: 一种应用的实例，例如一个web站点，包含前端，后端，数据库这三个容器，放在一个pod中对外就是一个web服务。&lt;/li&gt;
&lt;li&gt;service：pod的路由代理的抽象，集群内通过service提供的固定地址进行交互，而service来做服务发现和负载均衡，来和变化的pod进行交互。&lt;/li&gt;
&lt;li&gt;ingress：类似于nginx的转发功能，暴露出service的地址来对外暴露服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面是最基本要了解的资源概念，还有很多&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-principle/&#34;&gt;核心概念&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;安装包&#34;&gt;安装包&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;源码编译二进制文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、You have a working [Go environment].&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go get -d k8s.io/kubernetes
$ cd $GOPATH/src/k8s.io/kubernetes
$ make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、You have a working [Docker environment].&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/kubernetes/kubernetes
$ cd kubernetes
$ make quick-release
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;直接下载&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;官方提供了直接下载的路径&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;rpm&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;打rpm包，直接工程下载工程&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/release&#34;&gt;https://github.com/kubernetes/release&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;将上面编译好的文件可执行文件复制到这个项目的rpm目录下&lt;/p&gt;

&lt;p&gt;修改这个目录下的spec文件，比如版本信息，使用二进制文件为本地文件，也就是上面穿过来的，而不是去网上下载&lt;/p&gt;

&lt;p&gt;再下载一些基础镜像&lt;/p&gt;

&lt;p&gt;最后直接执行当前目录下的./docker-build.sh就可以自己启动镜像编译成对应的rpm文件&lt;/p&gt;

&lt;p&gt;对应执行的&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-rpm-build/&#34;&gt;基本过程&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;yum源&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;最后提供一个k8s的yum源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[virt7-testing]
name=virt7-testing
baseurl=http://cbs.centos.org/repos/virt7-testing/x86_64/os/
gpgcheck=0
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可见，能上网直接yum源，不能上网，就是直接下载二进制文件&lt;/p&gt;

&lt;h1 id=&#34;安装部署&#34;&gt;安装部署&lt;/h1&gt;

&lt;h2 id=&#34;单例&#34;&gt;单例&lt;/h2&gt;

&lt;p&gt;就是在一台机器上部署一个k8s集群，可以下载后一个个组件镜像然后在k8s启动，我们这边推荐k8s官方推出的minikube工具来部署，一般给我们正常做开发测试使用是足够的。&lt;/p&gt;

&lt;h3 id=&#34;minikube&#34;&gt;minikube&lt;/h3&gt;

&lt;p&gt;Kubernetes 集群的搭建是有一定难度的，尤其是对于初学者来说，好多概念和原理不懂，即使有现成的教程也会出现很多不可预知的问题，很容易打击学习的积极性，就此弃坑。好在 Kubernetes 社区提供了可以在本地开发和体验的极简集群安装 MiniKube，对于入门学习来说很方便。&lt;/p&gt;

&lt;p&gt;MiniKube其实就是把k8s的几个组件的镜像下载下来，然后使用docker进行启动，当然设置了启动参数 ，在一台机器上启动了所有的组件然后就是一个单节点的k8s集群，所以说minikube就是一个部署工具，和k8s集群本身没有关系，将k8s集群运行在docker中，不想我们直接运行在物理机上，但是在国内由于网络访问原因（懂的），即使有梯子也很折腾，所以需要修改源为阿里的后MiniKube 安装。使用阿里修改后的 MiniKube 就可以从阿里云的镜像地址来获取所需 Docker 镜像和配置，其它的并没有差异。&lt;/p&gt;

&lt;p&gt;minikube本身就提供了可执行的二进制文件，可以直接下载，用命令行启动，就能自动的将k8s部署好&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;前置工作&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、vm&lt;/p&gt;

&lt;p&gt;首先macOS是否支持虚拟化，在终端上运行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysctl -a | grep -E --color &#39;machdep.cpu.features|VMX&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果尚未安装管理程序，请立即安装以下之一：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、HyperKit
2、VirtualBox
3、VMware Fusion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为minikube是运行在一个本地的VM上的，而不是运行在本地，所以需要vm。&lt;/p&gt;

&lt;p&gt;2、docker已经安装&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;安装包&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们以macos为例&lt;/p&gt;

&lt;p&gt;macOS 安装 Minikube 最简单的方法是使用 Homebrew：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以通过下载单节点二进制文件进行安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 \
&amp;amp;&amp;amp; chmod +x minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个简单的将 Minikube 可执行文件添加至 path 的方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo mv minikube /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;清理本地状态&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果您之前安装过 Minikube，并运行了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并且 minikube start 返回了一个错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;machine does not exist
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么，你需要清理 minikube 的本地状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube delete
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;使用minikube安装k8s&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用minikube安装k8s集群十分简单，一条命令就可以，minikube会去创建一个vm虚拟机，然后在机器上部署docker，k8s集群，完成内部的基本设置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube start --image-mirror-country=&#39;cn&#39; --image-repository=&#39;registry.cn-hangzhou.aliyuncs.com/google_containers&#39; --cache-images=true --vm-driver=virtualbox --kubernetes-version=1.14.10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--vm-driver=*** 从1.5.0版本开始，Minikube缺省使用本地最好的驱动来创建Kubernetes本地环境。

    Minikube在不同操作系统上支持不同的驱动

    macOS：xhyve driver 缺省驱动, VirtualBox 或 VMware Fusion
    Linux： VirtualBox 或 KVM2
    Windows：VirtualBox 或 Hyper-V

--image-mirror-country=&#39;cn&#39;
--image-repository=&#39;registry.cn-hangzhou.aliyuncs.com/google_containers&#39;
将缺省利用 registry.cn-hangzhou.aliyuncs.com/google_containers 作为安装Kubernetes的容器镜像仓库 （阿里云版本可选）

--iso-url=*** 利用阿里云的镜像地址下载相应的 .iso 文件 （阿里云版本可选）
--registry-mirror=***为了拉取Docker Hub镜像，需要为 Docker daemon 配置镜像加速，参考阿里云镜像服务
--cpus=2: 为minikube虚拟机分配CPU核数
--memory=2048mb: 为minikube虚拟机分配内存数
--kubernetes-version=***: minikube 虚拟机将使用的 kubernetes 版本

//设置docker环境变量，这个是代理，一般不需要设置。
–docker-env HTTP_PROXY=http://proxy.sha.sap.corp:8080/
–docker-env HTTPS_PROXY=http://proxy.sha.sap.corp:8080/
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;安装kubectl&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;minikube只是在创建的虚拟机minikube中安装了k8s集群和相关组件，我们还需要在本地去安装管理工具kubectl来管理这个虚拟机中的集群。&lt;/p&gt;

&lt;p&gt;我是macOS 系统并使用 Homebrew 包管理器，通过 Homebrew 安装 kubectl。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;运行安装命令：

    brew install kubernetes-cli

测试以确保您安装的版本是最新的：

MacBook-Pro:minikube chunyinjiang$ kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;18&amp;quot;, GitVersion:&amp;quot;v1.18.3&amp;quot;, GitCommit:&amp;quot;2e7996e3e2712684bc73f0dec0200d64eec7fe40&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2020-05-21T14:51:23Z&amp;quot;, GoVersion:&amp;quot;go1.14.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;darwin/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;18&amp;quot;, GitVersion:&amp;quot;v1.18.3&amp;quot;, GitCommit:&amp;quot;2e7996e3e2712684bc73f0dec0200d64eec7fe40&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2020-05-20T12:43:34Z&amp;quot;, GoVersion:&amp;quot;go1.13.9&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然也可以下载二进制文件进行安装，下面有kubectl的具体说明。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;检查 kubectl 的配置&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过获取集群状态检查 kubectl 是否被正确配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl cluster-info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果您看到一个 URL 被返回，那么 kubectl 已经被正确配置，能够正常访问您的 Kubernetes 集群。&lt;/p&gt;

&lt;p&gt;如果您看到类似以下的信息被返回，那么 kubectl 没有被正确配置，无法正常访问您的 Kubernetes 集群。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The connection to the server &amp;lt;server-name:port&amp;gt; was refused - did you specify the right host or port?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 kubectl cluster-info 能够返回 url 响应，但您无法访问您的集群，可以使用下面的命令检查配置是否正确：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl cluster-info dump
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;启用 shell 自动补全功能&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;kubectl 支持自动补全功能，可以节省大量输入！&lt;/p&gt;

&lt;p&gt;自动补全脚本由 kubectl 产生，您仅需要在您的 shell 配置文件中调用即可。&lt;/p&gt;

&lt;p&gt;以下仅提供了使用命令补全的常用示例，更多详细信息，请查阅 kubectl completion -h 帮助命令的输出。&lt;/p&gt;

&lt;p&gt;1、Linux 系统，使用 bash&lt;/p&gt;

&lt;p&gt;在 CentOS Linux系统上，您可能需要安装默认情况下未安装的 bash-completion 软件包。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install bash-completion -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行 source &amp;lt;(kubectl completion bash) 命令在您目前正在运行的 shell 中开启 kubectl 自动补全功能。&lt;/p&gt;

&lt;p&gt;可以将上述命令添加到 shell 配置文件中，这样在今后运行的 shell 中将自动开启 kubectl 自动补全：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;quot;source &amp;lt;(kubectl completion bash)&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、macOS 系统，使用 bash&lt;/p&gt;

&lt;p&gt;macOS 系统需要先通过 Homebrew 安装 bash-completion：&lt;/p&gt;

&lt;p&gt;如果您运行的是 macOS 自带的 Bash 3.2，请运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install bash-completion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果您使用的是 Bash 4.1+，请运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install bash-completion@2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请根据 Homebrew 输出的”注意事项（caveats）”部分的内容将 bash-completion 的路径添加到本地 .bashrc 文件中。&lt;/p&gt;

&lt;p&gt;如果您是按照 Homebrew 指示中的步骤安装的 kubectl，那么无需其他配置，kubectl 的自动补全功能已经被启用。&lt;/p&gt;

&lt;p&gt;如果您是手工下载并安装的 kubectl，那么您需要将 kubectl 自动补全添加到 bash-completion：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl completion bash &amp;gt; $(brew --prefix)/etc/bash_completion.d/kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 Homebrew 项目与 Kubernetes 无关，所以并不能保证 bash-completion 总能够支持 kubectl 的自动补全功能。&lt;/p&gt;

&lt;p&gt;3、macOS 系统，使用 Zsh&lt;/p&gt;

&lt;p&gt;如果您使用的是 zsh,请编辑 ~/.zshrc 文件并添加以下代码以启用 kubectl 自动补全功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if [ $commands[kubectl] ]; then
  source &amp;lt;(kubectl completion zsh)
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果您使用的是 Oh-My-Zsh，请编辑 ~/.zshrc 文件并更新 plugins= 行以包含 kubectl 插件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plugins=(kubectl)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个只能在自己的机器上做学习开发研究使用，企业还是需要集群的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;查看集群状态&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:minikube chunyinjiang$ kubectl get pod --all-namespaces -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP               NODE       NOMINATED NODE   READINESS GATES
kube-system   coredns-546565776c-dcvb4           1/1     Running   0          10m   172.17.0.3       minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-546565776c-m9qlc           1/1     Running   0          10m   172.17.0.2       minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-minikube                      1/1     Running   0          10m   192.168.99.101   minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-minikube            1/1     Running   0          10m   192.168.99.101   minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-minikube   1/1     Running   0          10m   192.168.99.101   minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-hw2qb                   1/1     Running   0          10m   192.168.99.101   minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-minikube            1/1     Running   0          10m   192.168.99.101   minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   storage-provisioner                1/1     Running   0          10m   192.168.99.101   minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
MacBook-Pro:exercise chunyinjiang$ kubectl get nodes -o wide
NAME       STATUS   ROLES    AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE               KERNEL-VERSION   CONTAINER-RUNTIME
minikube   Ready    master   2d5h   v1.18.3   192.168.99.101   &amp;lt;none&amp;gt;        Buildroot 2019.02.10   4.19.107         docker://19.3.8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边讲一下vm虚拟机，安装了vmbox，可以直接打开客户端，你会发现一个minikube的虚拟机在运行，你可以直接在这边登陆root，不需要密码，也可以用其他的终端ssh登陆上面的master主机来查看相关信息：ssh docker@192.168.99.101 现在我们ssh到我们的master节点，默认用户名：docker 密码：tcuser&lt;/p&gt;

&lt;p&gt;也可以直接使用minikube ssh来直接连接登陆。&lt;/p&gt;

&lt;p&gt;k8s进程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ps -ef | grep kube
root      3977  3936  1 Jun10 ?        00:22:19 kube-controller-manager --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/var/lib/minikube/certs/ca.crt --cluster-name=mk --cluster-signing-cert-file=/var/lib/minikube/certs/ca.crt --cluster-signing-key-file=/var/lib/minikube/certs/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --root-ca-file=/var/lib/minikube/certs/ca.crt --service-account-private-key-file=/var/lib/minikube/certs/sa.key --use-service-account-credentials=true
root      4012  3947  0 Jun10 ?        00:04:57 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true
root      4046  3967  1 Jun10 ?        00:24:05 etcd --advertise-client-urls=https://192.168.99.101:2379 --cert-file=/var/lib/minikube/certs/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/minikube/etcd --initial-advertise-peer-urls=https://192.168.99.101:2380 --initial-cluster=minikube=https://192.168.99.101:2380 --key-file=/var/lib/minikube/certs/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.99.101:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.99.101:2380 --name=minikube --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/var/lib/minikube/certs/etcd/peer.key --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
root      4066  3993  3 Jun10 ?        00:55:50 kube-apiserver --advertise-address=192.168.99.101 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key
docker    4155  4114  0 06:49 pts/0    00:00:00 grep kube
root      4362     1  2 Jun10 ?        00:38:48 /var/lib/minikube/binaries/v1.18.3/kubelet --authorization-mode=Webhook --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --cgroup-driver=systemd --client-ca-file=/var/lib/minikube/certs/ca.crt --cluster-domain=cluster.local --config=/var/lib/kubelet/config.yaml --container-runtime=docker --fail-swap-on=false --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.99.101 --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 --pod-manifest-path=/etc/kubernetes/manifests
root      4688  4668  0 Jun10 ?        00:00:35 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;k8s镜像&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker images
REPOSITORY                                                                    TAG                 IMAGE ID            CREATED             SIZE
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy                v1.18.3             3439b7546f29        3 weeks ago         117MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler            v1.18.3             76216c34ed0c        3 weeks ago         95.3MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager   v1.18.3             da26705ccb4b        3 weeks ago         162MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver            v1.18.3             7e28efa976bd        3 weeks ago         173MB
registry.cn-hangzhou.aliyuncs.com/google_containers/dashboard                 v2.0.0              8b32422733b3        7 weeks ago         222MB
registry.cn-hangzhou.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        3 months ago        683kB
registry.cn-hangzhou.aliyuncs.com/google_containers/coredns                   1.6.7               67da37a9a360        4 months ago        43.8MB
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd                      3.4.3-0             303ce5db0e90        7 months ago        288MB
registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-scraper           v1.0.2              3b08661dc379        7 months ago        40.1MB
registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner       v1.8.1              4689081edb10        2 years ago         80.8MB
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;管理集群&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;查看状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开启dashboard&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube dashborad启动pod
kubernetes-dashboard   dashboard-metrics-scraper-84bfdf55ff-m6hlx   1/1     Running   0          4d20h
kubernetes-dashboard   kubernetes-dashboard-696dbcc666-vfrks        1/1     Running   0          4d20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后可以&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:~ chunyinjiang$ minikube dashboard --url
🤔  Verifying dashboard health ...
🚀  Launching proxy ...
🤔  Verifying proxy health ...
http://127.0.0.1:59625/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实还有有用的组件都是通过插件模式可以开启的&lt;/p&gt;

&lt;p&gt;列出当前支持的插件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ minikube addons list
|-----------------------------|----------|--------------|
|         ADDON NAME          | PROFILE  |    STATUS    |
|-----------------------------|----------|--------------|
| ambassador                  | minikube | disabled     |
| dashboard                   | minikube | enabled ✅   |
| default-storageclass        | minikube | enabled ✅   |
| efk                         | minikube | disabled     |
| freshpod                    | minikube | disabled     |
| gvisor                      | minikube | disabled     |
| helm-tiller                 | minikube | disabled     |
| ingress                     | minikube | disabled     |
| ingress-dns                 | minikube | disabled     |
| istio                       | minikube | disabled     |
| istio-provisioner           | minikube | disabled     |
| logviewer                   | minikube | disabled     |
| metallb                     | minikube | disabled     |
| metrics-server              | minikube | disabled     |
| nvidia-driver-installer     | minikube | disabled     |
| nvidia-gpu-device-plugin    | minikube | disabled     |
| olm                         | minikube | disabled     |
| registry                    | minikube | disabled     |
| registry-aliases            | minikube | disabled     |
| registry-creds              | minikube | disabled     |
| storage-provisioner         | minikube | enabled ✅   |
| storage-provisioner-gluster | minikube | disabled     |
|-----------------------------|----------|--------------|
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开启插件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;例如 metrics-server：

minikube addons enable metrics-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关闭插件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube addons disable metrics-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;登陆minikube虚拟机&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;miniube ssh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止集群&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除集群&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube delete
minikube delete --all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看minikube的状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看minikube的服务列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ minikube service list
|----------------------|-----------------------------------|--------------|-----------------------------|
|      NAMESPACE       |               NAME                | TARGET PORT  |             URL             |
|----------------------|-----------------------------------|--------------|-----------------------------|
| default              | kubernetes                        | No node port |
| default              | nginx                             | http/80      | http://192.168.99.101:30080 |
| default              | redis-master                      | No node port |
| default              | redis-slave                       | No node port |
| kruise-system        | kruise-controller-manager-service | No node port |
| kruise-system        | kruise-webhook-server-service     | No node port |
| kube-system          | elasticsearch-logging             | No node port |
| kube-system          | etcd-k8s                          | No node port |
| kube-system          | kibana-logging                    |         5601 | http://192.168.99.101:30003 |
| kube-system          | kube-dns                          | No node port |
| kube-system          | kube-scheduler                    | No node port |
| kube-system          | kubelet                           | No node port |
| kube-system          | metrics-server                    | No node port |
| kubernetes-dashboard | dashboard-metrics-scraper         | No node port |
| kubernetes-dashboard | kubernetes-dashboard              | No node port |
| monitoring           | alertmanager-main                 | web/9093     | http://192.168.99.101:31234 |
| monitoring           | alertmanager-operated             | No node port |
| monitoring           | grafana                           | http/3000    | http://192.168.99.101:30267 |
| monitoring           | kube-state-metrics                | No node port |
| monitoring           | node-exporter                     | No node port |
| monitoring           | prometheus-adapter                | No node port |
| monitoring           | prometheus-k8s                    | web/9090     | http://192.168.99.101:31174 |
| monitoring           | prometheus-operated               | No node port |
| monitoring           | prometheus-operator               | No node port |
|----------------------|-----------------------------------|--------------|-----------------------------|
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pause Kubernetes without impacting deployed applications:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube pause
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Halt the cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Increase the default memory limit (requires a restart):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube config set memory 16384
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Browse the catalog of easily installed Kubernetes services:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube addons list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a second cluster running an older Kubernetes release:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube start -p aged --kubernetes-version=v1.16.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete all of the minikube clusters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube delete --all
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;就是多台机器上部署真正的cluster，在 1.4版本之前都是手动部署，在1.4之后提供了部署工具kubeadm，kubeadm帮忙大大优化了k8s的部署，所以目前有两种部署方式，当然kubeadm是对手动部署的封装，目前大部分还是使用手动部署k8s，熟悉启动参数意义，当然大家都在做部署工具，后面使用kubeadm是一种趋势。&lt;/p&gt;

&lt;p&gt;首先不管什么部署，一些机器上的基础工作都是一样需要做&lt;/p&gt;

&lt;p&gt;1、关闭防火墙&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl stop firewalld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置开启不启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl disable firewalld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关闭SELINUX&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;setenforce 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;永久关闭&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELINUX sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、安装iptables&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install iptables -y
yum install iptables-services -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;清空iptables规则&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;iptables -F
iptables -X
iptables -Z
iptables -F -t nat
service iptables save
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、ntpdate 时间同步&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ntpdate time1.aliyun.com
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;kubeadm&#34;&gt;kubeadm&lt;/h3&gt;

&lt;p&gt;kubeadm是Kubernetes 1.4开始新增的特性，用于快速搭建Kubernetes集群环境，两个命令就能把一个k8s集群搭建起来。&lt;/p&gt;

&lt;p&gt;kubeadm一共提供了5个子命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubeadm init
kubeadm join
kubeadm token
kubeadm reset
kubeadm version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubeadm init主要工作：&lt;/p&gt;

&lt;p&gt;创建集群安全相关的的key、certs和conf文件。
创建kube-apiserver、kube-controller-manager、kube-scheduler、etcd(如果没有配置external etcd)这些static pod的json格式的manifest文件，kubelet负责启动这些master组件。
通过addons方式启动kube-discovery deployment、kube-proxy daemonSet、kube-dns deployment。&lt;/p&gt;

&lt;p&gt;kubeadm join主要负责创建kubelet.conf，使kubelet能与API Server建立连接：&lt;/p&gt;

&lt;p&gt;访问kube-discovery服务获取cluster info（包含cluster ca证书、API Server endpoint列表和token。
利用定的token，检验cluster info的签名。
检验成功后，再与API Server建立连接，请求API Server为该node创建证书。
根据获取到的证书创建kubelet.conf。&lt;/p&gt;

&lt;p&gt;随着Kubernetes 1.13 的发布，现在Kubeadm正式成为GA。之前都是测试版本，并不能用于生产。&lt;/p&gt;

&lt;h3 id=&#34;手动部署&#34;&gt;手动部署&lt;/h3&gt;

&lt;blockquote&gt;
&lt;h4 id=&#34;shell一键部署&#34;&gt;shell一键部署&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面kubeadm是一种部署方式的趋势，在现在很多情况下还不够成熟，都在探索，在这个过度期间大部分都会使用shell脚本进行一键部署。&lt;/p&gt;

&lt;p&gt;这一块就比较偏运维，需要写大量的运维脚本，使用一些成熟的运维工具，比如puppet，cobbler，absible，supervisor等,都是&lt;a href=&#34;https://kingjcy.github.io/post/linux/tool/ops-tool/&#34;&gt;服务器自动化部署及运维常见工具&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;h4 id=&#34;二进制部署&#34;&gt;二进制部署&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;就是我们最常用的手动部署方式，也是需要多次练习了解的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;权限设置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、设置ca证书和秘钥&lt;/p&gt;

&lt;p&gt;安装&lt;a href=&#34;https://kingjcy.github.io/posts/linux/server/ssl/&#34;&gt;ssl&lt;/a&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
  wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
  wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64

  sudo cp cfssl_linux-amd64 /usr/local/bin/cfssl 
  sudo cp cfssljson_linux-amd64 /usr/local/bin/cfssljson
  sudo cp cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo

  cd /usr/local/bin/ 

  chmod +x cfssl
  chmod +x cfssljson
  chmod +x cfssl-certinfo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir ~/cfssl
cd ~/cfssl
cfssl print-defaults config &amp;gt; ca-config.json
cfssl print-defaults csr &amp;gt; ca-csr.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建ca中心&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; ca-config.json &amp;lt;&amp;lt;EOF
{
  &amp;quot;signing&amp;quot;: {
    &amp;quot;default&amp;quot;: {
      &amp;quot;expiry&amp;quot;: &amp;quot;87600h&amp;quot;
    },
    &amp;quot;profiles&amp;quot;: {
      &amp;quot;kubernetes&amp;quot;: {
        &amp;quot;usages&amp;quot;: [
            &amp;quot;signing&amp;quot;,
            &amp;quot;key encipherment&amp;quot;,
            &amp;quot;server auth&amp;quot;,
            &amp;quot;client auth&amp;quot;
        ],
        &amp;quot;expiry&amp;quot;: &amp;quot;87600h&amp;quot;
      }
    }
  }
}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；&lt;/p&gt;

&lt;p&gt;signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；&lt;/p&gt;

&lt;p&gt;server auth：表示client可以用该 CA 对server提供的证书进行验证；&lt;/p&gt;

&lt;p&gt;client auth：表示server可以用该CA对client提供的证书进行验证；&lt;/p&gt;

&lt;p&gt;创建 CA 证书签名请求文件&lt;/p&gt;

&lt;p&gt;创建 ca-csr.json 文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;ldquo;CN&amp;rdquo;：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；&lt;/p&gt;

&lt;p&gt;&amp;ldquo;O&amp;rdquo;：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生产对应的ca证书，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@promesdevapp18 ssl]# vi ca-config.json
[root@promesdevapp18 ssl]# vi ca-csr.json
[root@promesdevapp18 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca
2019/04/11 11:06:53 [INFO] generating a new CA key and certificate from CSR
2019/04/11 11:06:53 [INFO] generate received request
2019/04/11 11:06:53 [INFO] received CSR
2019/04/11 11:06:53 [INFO] generating key: rsa-2048
2019/04/11 11:06:54 [INFO] encoded CSR
2019/04/11 11:06:54 [INFO] signed certificate with serial number 551260756352944283434991089528819033235135295505
[root@promesdevapp18 ssl]# ll
total 18828
-rw-r--r-- 1 root root      292 Apr 11 11:06 ca-config.json
-rw-r--r-- 1 root root     1001 Apr 11 11:06 ca.csr
-rw-r--r-- 1 root root      208 Apr 11 11:06 ca-csr.json
-rw------- 1 root root     1679 Apr 11 11:06 ca-key.pem
-rw-r--r-- 1 root root     1359 Apr 11 11:06 ca.pem
-rwxrwxrwx 1 root root  6595195 Apr 11 11:02 cfssl-certinfo_linux-amd64
-rwxrwxrwx 1 root root  2277873 Apr 11 11:02 cfssljson_linux-amd64
-rwxrwxrwx 1 root root 10376657 Apr 11 11:02 cfssl_linux-amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成运行CA所必需的文件ca-key.pem（私钥）和ca.pem（证书），还会生成ca.csr（证书签名请求），用于交叉签名或重新签名。&lt;/p&gt;

&lt;p&gt;请保持ca-key.pem文件的安全。此密钥允许在CA中创建任何类型的证书。*.csr 文件在整个过程中不会使用&lt;/p&gt;

&lt;p&gt;这边所有ca相关证书就是根证书，用于签发下面的每个组件的证书，在这个集群中，他就是权威机构，最高权限，默认签发的证书是一年，需要注意修改。&lt;/p&gt;

&lt;p&gt;2、创建 kubernetes 证书&lt;/p&gt;

&lt;p&gt;创建 kubernetes 证书签名请求文件 kubernetes-csr.json：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
    &amp;quot;hosts&amp;quot;: [
      &amp;quot;127.0.0.1&amp;quot;,
      &amp;quot;192.168.56.107&amp;quot;,
      &amp;quot;192.168.56.106&amp;quot;,
      &amp;quot;192.168.56.105&amp;quot;,
      &amp;quot;10.254.0.1&amp;quot;,
      &amp;quot;kubernetes&amp;quot;,
      &amp;quot;kubernetes.default&amp;quot;,
      &amp;quot;kubernetes.default.svc&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster.local&amp;quot;
    ],
    &amp;quot;key&amp;quot;: {
        &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
        &amp;quot;size&amp;quot;: 2048
    },
    &amp;quot;names&amp;quot;: [
        {
            &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
            &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
            &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
            &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
            &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP 和 kubernetes 服务的服务 IP（一般是 kube-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1）。&lt;/p&gt;

&lt;p&gt;这是最小化安装的kubernetes集群，包括一个私有镜像仓库，三个节点的kubernetes集群，以上物理节点的IP也可以更换为主机名。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes -hostname=&amp;quot;127.0.0.1,192.168.56.107,192.168.56.106,192.168.56.105,10.254.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local&amp;quot; kubernetes-csr.json | cfssljson -bare kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &#39;{&amp;quot;CN&amp;quot;:&amp;quot;kubernetes&amp;quot;,&amp;quot;hosts&amp;quot;:[&amp;quot;&amp;quot;],&amp;quot;key&amp;quot;:{&amp;quot;algo&amp;quot;:&amp;quot;rsa&amp;quot;,&amp;quot;size&amp;quot;:2048}}&#39; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes -hostname=&amp;quot;127.0.0.1,172.20.0.112,172.20.0.113,172.20.0.114,172.20.0.115,kubernetes,kubernetes.default&amp;quot; - | cfssljson -bare kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成kubernetes的对应三个文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@promesdevapp18 ssl]# vi kubernetes-csr.json
[root@promesdevapp18 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes -hostname=&amp;quot;127.0.0.1,10.47.210.31,10.47.210.30,10.47.210.94,10.254.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local&amp;quot; kubernetes-csr.json | cfssljson -bare kubernetes
2019/04/11 11:45:17 [INFO] generate received request
2019/04/11 11:45:17 [INFO] received CSR
2019/04/11 11:45:17 [INFO] generating key: rsa-2048
2019/04/11 11:45:17 [INFO] encoded CSR
2019/04/11 11:45:17 [INFO] signed certificate with serial number 4144021668890124555398076217136881830789156682
[root@promesdevapp18 ssl]#
[root@promesdevapp18 ssl]#
[root@promesdevapp18 ssl]# ll
total 18844
-rw-r--r-- 1 root root      292 Apr 11 11:43 ca-config.json
-rw-r--r-- 1 root root     1001 Apr 11 11:44 ca.csr
-rw-r--r-- 1 root root      209 Apr 11 11:44 ca-csr.json
-rw------- 1 root root     1679 Apr 11 11:44 ca-key.pem
-rw-r--r-- 1 root root     1359 Apr 11 11:44 ca.pem
-rwxrwxrwx 1 root root  6595195 Apr 11 11:02 cfssl-certinfo_linux-amd64
-rwxrwxrwx 1 root root  2277873 Apr 11 11:02 cfssljson_linux-amd64
-rwxrwxrwx 1 root root 10376657 Apr 11 11:02 cfssl_linux-amd64
-rw-r--r-- 1 root root     1261 Apr 11 11:45 kubernetes.csr
-rw-r--r-- 1 root root      556 Apr 11 11:45 kubernetes-csr.json
-rw------- 1 root root     1675 Apr 11 11:45 kubernetes-key.pem
-rw-r--r-- 1 root root     1627 Apr 11 11:45 kubernetes.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、创建 admin 证书&lt;/p&gt;

&lt;p&gt;创建 admin 证书签名请求文件 admin-csr.json：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;admin&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;system:masters&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；&lt;/p&gt;

&lt;p&gt;kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限；&lt;/p&gt;

&lt;p&gt;O 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限；&lt;/p&gt;

&lt;p&gt;注意：这个admin 证书，是将来生成管理员用的kube config 配置文件用的，现在我们一般建议使用RBAC 来对kubernetes 进行角色权限控制， kubernetes 将证书中的CN 字段 作为User， O 字段作为 Group（具体参考 Kubernetes中的用户与身份认证授权中 X509 Client Certs 一段）。&lt;/p&gt;

&lt;p&gt;在搭建完 kubernetes 集群后，我们可以通过命令: kubectl get clusterrolebinding cluster-admin -o yaml ,查看到 clusterrolebinding cluster-admin 的 subjects 的 kind 是 Group，name 是 system:masters。 roleRef 对象是 ClusterRole cluster-admin。 意思是凡是 system:masters Group 的 user 或者 serviceAccount 都拥有 cluster-admin 的角色。 因此我们在使用 kubectl 命令时候，才拥有整个集群的管理权限。可以使用 kubectl get clusterrolebinding cluster-admin -o yaml 来查看。&lt;/p&gt;

&lt;p&gt;生成 admin 证书和私钥：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、创建 kube-proxy 证书&lt;/p&gt;

&lt;p&gt;创建 kube-proxy 证书签名请求文件 kube-proxy-csr.json：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;system:kube-proxy&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CN 指定该证书的 User 为 system:kube-proxy；&lt;/p&gt;

&lt;p&gt;kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；&lt;/p&gt;

&lt;p&gt;生成 kube-proxy 客户端证书和私钥&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
$ ls kube-proxy*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、校验证书&lt;/p&gt;

&lt;p&gt;1.openssl&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ openssl x509  -noout -text -in  kubernetes.pem



确认 Issuer 字段的内容和 ca-csr.json 一致；
确认 Subject 字段的内容和 kubernetes-csr.json 一致；
确认 X509v3 Subject Alternative Name 字段的内容和 kubernetes-csr.json 一致；
确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 ca-config.json 中 kubernetes profile 一致；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.cfssl-certinfo&lt;/p&gt;

&lt;p&gt;使用 cfssl-certinfo 命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cfssl-certinfo -cert kubernetes.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、分发证书&lt;/p&gt;

&lt;p&gt;将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器的 /etc/kubernetes/ssl 目录下备用&lt;/p&gt;

&lt;p&gt;7、创建kubectl kubeconfig&lt;/p&gt;

&lt;p&gt;先在master上安装kubectl&lt;/p&gt;

&lt;p&gt;创建 kubectl kubeconfig 文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export MASTER_IP=192.168.56.107
export KUBE_APISERVER=&amp;quot;https://${MASTER_IP}:6443&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置集群参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes \
  --certificate-authority=/home/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置客户端认证参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-credentials admin \
  --client-certificate=/home/ssl/admin.pem \
  --embed-certs=true \
  --client-key=/home/ssl/admin-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置上下文参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置默认上下文&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;admin.pem 证书 OU 字段值为 system:masters，kube-apiserver 预定义的 RoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 相关 API 的权限；&lt;/p&gt;

&lt;p&gt;生成的 kubeconfig 被保存到 ~/.kube/config 文件；&lt;/p&gt;

&lt;p&gt;注意：~/.kube/config文件拥有对该集群的最高权限，请妥善保管。&lt;/p&gt;

&lt;p&gt;8、TLS Bootstrapping使用kubeapiserver给kubelet生成证书&lt;/p&gt;

&lt;p&gt;kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权；&lt;/p&gt;

&lt;p&gt;kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书；&lt;/p&gt;

&lt;p&gt;以下操作只需要在master节点上执行，生成的*.kubeconfig文件可以直接拷贝到node节点的/etc/kubernetes目录下。&lt;/p&gt;

&lt;p&gt;创建 TLS Bootstrapping Token&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;)
cat &amp;gt; token.csv &amp;lt;&amp;lt;EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&amp;quot;system:kubelet-bootstrap&amp;quot;
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;BOOTSTRAP_TOKEN 将被写入到 kube-apiserver 使用的 token.csv 文件和 kubelet 使用的 bootstrap.kubeconfig 文件，如果后续重新生成了 BOOTSTRAP_TOKEN，则需要：&lt;/p&gt;

&lt;p&gt;1.更新 token.csv 文件，分发到所有机器 (master 和 node）的 /etc/kubernetes/ 目录下，分发到node节点上非必需；
2.重新生成 bootstrap.kubeconfig 文件，分发到所有 node 机器的 /etc/kubernetes/ 目录下；
3.重启 kube-apiserver 和 kubelet 进程；
4.重新 approve kubelet 的 csr 请求；
5.cp token.csv /etc/kubernetes/&lt;/p&gt;

&lt;p&gt;9、创建 kubelet bootstrapping kubeconfig 文件&lt;/p&gt;

&lt;p&gt;cd /etc/kubernetes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export MASTER_IP=192.168.56.107
export KUBE_APISERVER=&amp;quot;https://${MASTER_IP}:6443&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置集群参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes \
  --certificate-authority=/home/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置客户端认证参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置上下文参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置默认上下文&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;ndash;embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；&lt;/p&gt;

&lt;p&gt;设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成；&lt;/p&gt;

&lt;p&gt;10、创建 kube-proxy kubeconfig 文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /etc/kubernetes
export KUBE_APISERVER=&amp;quot;https://192.168.56.107:6443&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置集群参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes \
  --certificate-authority=/home/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置客户端认证参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-credentials kube-proxy \
  --client-certificate=/home/ssl/kube-proxy.pem \
  --client-key=/home/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置上下文参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置默认上下文&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置集群参数和客户端认证参数时 &amp;ndash;embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；&lt;/p&gt;

&lt;p&gt;kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；&lt;/p&gt;

&lt;p&gt;分发：&lt;/p&gt;

&lt;p&gt;将两个 kubeconfig 文件分发到所有 Node 机器的 /etc/kubernetes/ 目录&lt;/p&gt;

&lt;p&gt;cp bootstrap.kubeconfig kube-proxy.kubeconfig /etc/kubernetes/&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;安装etcd&amp;mdash;&amp;ndash;带tls的&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;本次安装用到的变量设置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export NODE_NAME=opt6 # 当前部署的机器名称(随便定义，只要能区分不同机器即可)
$ export NODE_IP=192.168.56.107 # 当前部署的机器 IP
$ export NODE_IPS=&amp;quot;192.168.56.107 192.168.56.106 192.168.56.105&amp;quot; # etcd 集群所有机器 IP
$ # etcd 集群间通信的IP和端口
$ export ETCD_NODES=opt6=https://192.168.56.105:2380,opt7=https://192.168.56.106:2380,opt8=https://192.168.56.107:2380
$ # 导入用到的其它全局变量：ETCD_ENDPOINTS、FLANNEL_ETCD_PREFIX、CLUSTER_CIDR
$ source /root/local/bin/environment.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.设置etcd二进制文件&lt;/p&gt;

&lt;p&gt;Etcd是Kubernetes集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息。&lt;/p&gt;

&lt;p&gt;整个kubernetes系统中一共有两个服务需要用到etcd用来协同和存储配置，分别是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;网络插件flannel、对于其它网络插件也需要用到etcd存储网络的配置信息
kubernetes本身，包括各种对象的状态和元信息配置
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前使用etcd版本3.1.6&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解压设置环境变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -xvf etcd-v3.1.5-linux-amd64.tar.gz
echo &#39;export PATH=/home/etcd/etcd-v3.1.6-linux-amd64:$PATH&#39; &amp;gt;&amp;gt; /etc/profile
source /etc/profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者把二进制文件移到系统路径下，比如/usr/local/src&lt;/p&gt;

&lt;p&gt;2.创建 etcd的TLS 秘钥和证书&lt;/p&gt;

&lt;p&gt;编辑etcd-csr.json&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; etcd-csr.json &amp;lt;&amp;lt;EOF
{
  &amp;quot;CN&amp;quot;: &amp;quot;etcd&amp;quot;,
  &amp;quot;hosts&amp;quot;: [
    &amp;quot;127.0.0.1&amp;quot;,
    &amp;quot;192.168.56.107&amp;quot;----设置本地ip,也就是上面的${NODE_IP}
  ],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成 etcd 证书和私钥：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfssl gencert -ca=/home/ssl/ca.pem \
  -ca-key=/home/ssl/ca-key.pem \
  -config=/home/ssl/ca-config.json \
  -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边可以使用kubernete的证书，因为证书包含了etcd节点&lt;/p&gt;

&lt;p&gt;因为包含主机的ip每台机器的证书都要单独生成，不能拷贝&lt;/p&gt;

&lt;p&gt;3.创建 etcd 的 systemd unit 文件&lt;/p&gt;

&lt;p&gt;在/usr/lib/systemd/system/目录下创建文件etcd.service&lt;/p&gt;

&lt;p&gt;opt8&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; etcd.service &amp;lt;&amp;lt;EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/home/etcd/etcd-v3.1.6-linux-amd64/etcd \\
  --name=opt8 \\
  --cert-file=/home/ssl/etcd.pem \\
  --key-file=/home/ssl/etcd-key.pem \\
  --peer-cert-file=/home/ssl/etcd.pem \\
  --peer-key-file=/home/ssl/etcd-key.pem \\
  --trusted-ca-file=/home/ssl/ca.pem \\
  --peer-trusted-ca-file=/home/ssl/ca.pem \\
  --initial-advertise-peer-urls=https://192.168.56.107:2380 \\
  --listen-peer-urls=https://192.168.56.107:2380 \\
  --listen-client-urls=https://192.168.56.107:2379,http://127.0.0.1:2379 \\
  --advertise-client-urls=https://192.168.56.107:2379 \\
  --initial-cluster-token=etcd-cluster-0 \\
  --initial-cluster=opt8=https://192.168.56.107:2380,opt7=https://192.168.56.106:2380,opt6=https://192.168.56.105:2380 \\
  --initial-cluster-state=new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;opt7&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; etcd.service &amp;lt;&amp;lt;EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/home/etcd/etcd-v3.1.6-linux-amd64/etcd \\
  --name=opt7 \\
  --cert-file=/home/ssl/etcd.pem \\
  --key-file=/home/ssl/etcd-key.pem \\
  --peer-cert-file=/home/ssl/etcd.pem \\
  --peer-key-file=/home/ssl/etcd-key.pem \\
  --trusted-ca-file=/home/ssl/ca.pem \\
  --peer-trusted-ca-file=/home/ssl/ca.pem \\
  --initial-advertise-peer-urls=https://192.168.56.106:2380 \\
  --listen-peer-urls=https://192.168.56.106:2380 \\
  --listen-client-urls=https://192.168.56.106:2379,http://127.0.0.1:2379 \\
  --advertise-client-urls=https://192.168.56.106:2379 \\
  --initial-cluster-token=etcd-cluster-0 \\
  --initial-cluster=opt8=https://192.168.56.107:2380,opt7=https://192.168.56.106:2380,opt6=https://192.168.56.105:2380 \\
  --initial-cluster-state=new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;opt6&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; etcd.service &amp;lt;&amp;lt;EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/home/etcd/etcd-v3.1.6-linux-amd64/etcd \\
  --name=opt6 \\
  --cert-file=/home/ssl/etcd.pem \\
  --key-file=/home/ssl/etcd-key.pem \\
  --peer-cert-file=/home/ssl/etcd.pem \\
  --peer-key-file=/home/ssl/etcd-key.pem \\
  --trusted-ca-file=/home/ssl/ca.pem \\
  --peer-trusted-ca-file=/home/ssl/ca.pem \\
  --initial-advertise-peer-urls=https://192.168.56.105:2380 \\
  --listen-peer-urls=https://192.168.56.105:2380 \\
  --listen-client-urls=https://192.168.56.105:2379,http://127.0.0.1:2379 \\
  --advertise-client-urls=https://192.168.56.105:2379 \\
  --initial-cluster-token=etcd-cluster-0 \\
  --initial-cluster=opt8=https://192.168.56.107:2380,opt7=https://192.168.56.106:2380,opt6=https://192.168.56.105:2380 \\
  --initial-cluster-state=new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动参数在service中不用加“”，否则启动失败。&lt;/p&gt;

&lt;p&gt;也可以使用命令行启动&lt;/p&gt;

&lt;p&gt;opt8&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcd -name niub1 -debug \
    -initial-advertise-peer-urls http://192.168.56.107:2380 \
    -listen-peer-urls http://192.168.56.107:2380 \
    -listen-client-urls http://192.168.56.107:2379,http://127.0.0.1:2379 \
    -advertise-client-urls http://192.168.56.107:2379 \
    -initial-cluster-token etcd-cluster-1 \
    -initial-cluster niub1=http://192.168.56.107:2380,niub2=http://192.168.56.106:2380,niub3=http://192.168.56.105:2380 \
    -initial-cluster-state new  &amp;gt;&amp;gt; ./etcd.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;opt7&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcd -name niub2 -debug \
    -initial-advertise-peer-urls http://192.168.56.106:2380 \
    -listen-peer-urls http://192.168.56.106:2380 \
    -listen-client-urls http://192.168.56.106:2379,http://127.0.0.1:2379 \
    -advertise-client-urls http://192.168.56.106:2379 \
    -initial-cluster-token etcd-cluster-1 \
    -initial-cluster niub1=http://192.168.56.107:2380,niub2=http://192.168.56.106:2380,niub3=http://192.168.56.105:2380 \
    -initial-cluster-state new  &amp;gt;&amp;gt; ./etcd.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;opt6&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcd -name niub3 -debug \
    -initial-advertise-peer-urls http://192.168.56.105:2380 \
    -listen-peer-urls http://192.168.56.105:2380 \
    -listen-client-urls http://192.168.56.105:2379,http://127.0.0.1:2379 \
    -advertise-client-urls http://192.168.56.105:2379 \
    -initial-cluster-token etcd-cluster-1 \
    -initial-cluster niub1=http://192.168.56.107:2380,niub2=http://192.168.56.106:2380,niub3=http://192.168.56.105:2380 \
    -initial-cluster-state new  &amp;gt;&amp;gt; ./etcd.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.加载service，启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5.测试集群&amp;mdash;这个是带ca的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcdctl \
  --ca-file=/home/ssl/ca.pem \
  --cert-file=/home/ssl/kubernetes.pem \
  --key-file=/home/ssl/kubernetes-key.pem \
  cluster-health
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看集群中k8s的数据&lt;/p&gt;

&lt;p&gt;Kubenretes1.6中使用etcd V3版本的API，使用etcdctl直接ls的话只能看到/kube-centos一个路径。需要在命令前加上ETCDCTL_API=3这个环境变量才能看到kuberentes在etcd中保存的数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;-w指定输出格式&lt;/p&gt;

&lt;p&gt;使用&amp;ndash;prefix可以看到所有的子目录，如查看集群中的namespace：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ETCDCTL_API=3 etcdctl get /registry/namespaces --prefix -w=json|python -m json.tool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;key的值是经过base64编码，需要解码后才能看到实际值，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@opt8 ssl]# echo L3JlZ2lzdHJ5L25hbWVzcGFjZXMva3ViZS1zeXN0ZW0=|base64 -d
/registry/namespaces/kube-system[root@opt8 ssl]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;etcd中kubernetes的元数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
# Get kubernetes keys from etcd
export ETCDCTL_API=3
keys=`etcdctl get /registry --prefix -w json|python -m json.tool|grep key|cut -d &amp;quot;:&amp;quot; -f2|tr -d &#39;&amp;quot;&#39;|tr -d &amp;quot;,&amp;quot;`
for x in $keys;do
  echo $x|base64 -d|sort
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到所有的Kuberentes的所有元数据都保存在/registry目录下，下一层就是API对象类型（复数形式），再下一层是namespace，最后一层是对象的名字。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;master安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;部署机器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;10.47.210.30
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;部署组件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kube-apiserver
kube-controller-manager
kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、配置和启动 kube-apiserver&lt;/p&gt;

&lt;p&gt;创建 kube-apiserver 使用的客户端 token 文件&lt;/p&gt;

&lt;p&gt;kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token.csv 一致，如果一致则自动为 kubelet生成证书和秘钥。&lt;/p&gt;

&lt;p&gt;目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/etc/systemd/system


cat  &amp;gt; kube-apiserver.service &amp;lt;&amp;lt;EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/usr/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
  --advertise-address=10.47.210.30 \
  --insecure-bind-address=10.47.210.30 \
  --authorization-mode=RBAC \
  --runtime-config=rbac.authorization.k8s.io/v1alpha1 \
  --kubelet-https=true \
  --enable-bootstrap-token-auth \
  --service-cluster-ip-range=10.254.0.0/16 \
  --service-node-port-range=8400-9000 \
  --etcd-cafile=/root/ssl/ca.pem \
  --etcd-certfile=/root/ssl/etcd.pem \
  --etcd-keyfile=/root/ssl/etcd-key.pem \
  --etcd-servers=https://10.47.210.30:2379,https://10.47.210.31:2379,https://10.47.210.94:2379 \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/lib/audit.log \
  --event-ttl=1h \
  --v=0 \
  --logtostderr=true \
  --bind-address=10.47.210.30 \
  --token-auth-file=/etc/kubernetes/token.csv \
  --tls-cert-file=/root/ssl/kubernetes.pem \
  --tls-private-key-file=/root/ssl/kubernetes-key.pem \
  --client-ca-file=/root/ssl/ca.pem \
  --service-account-key-file=/root/ssl/ca-key.pem
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数解析&lt;/p&gt;

&lt;p&gt;&amp;ndash;v=2 #debug级别是0&lt;/p&gt;

&lt;p&gt;&amp;ndash;logtostderr=false \ #不输出到&lt;/p&gt;

&lt;p&gt;&amp;ndash;allow-privileged=true \   # docker run &amp;ndash;privileged&lt;/p&gt;

&lt;p&gt;&amp;ndash;etcd-servers=“”       #etcd的集群地址&lt;/p&gt;

&lt;p&gt;&amp;ndash;insecure-bind-address。      #非安全端口监听的ip&lt;/p&gt;

&lt;p&gt;&amp;ndash;advertise-address=192.168.14.132 \ #告诉别人在我是谁&lt;/p&gt;

&lt;p&gt;&amp;ndash;bind-address=0.0.0.0 \ # 安全端口监听的ip&lt;/p&gt;

&lt;p&gt;kube-apiserver 1.6 版本开始使用 etcd v3 API 和存储格式；&lt;/p&gt;

&lt;p&gt;&amp;ndash;authorization-mode=RBAC 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求；&lt;/p&gt;

&lt;p&gt;kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信;&lt;/p&gt;

&lt;p&gt;kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，如果通过安全端口访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权；&lt;/p&gt;

&lt;p&gt;kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的；&lt;/p&gt;

&lt;p&gt;如果使用了 kubelet TLS Boostrap 机制，则不能再指定 &amp;ndash;kubelet-certificate-authority、&amp;ndash;kubelet-client-certificate 和 &amp;ndash;kubelet-client-key 选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；&lt;/p&gt;

&lt;p&gt;&amp;ndash;admission-control 值必须包含 ServiceAccount，否则部署集群插件时会失败；&lt;/p&gt;

&lt;p&gt;&amp;ndash;bind-address 不能为 127.0.0.1；&lt;/p&gt;

&lt;p&gt;&amp;ndash;service-cluster-ip-range 指定 Service Cluster IP 地址段，该地址段不能路由可达；&lt;/p&gt;

&lt;p&gt;&amp;ndash;service-node-port-range=${NODE_PORT_RANGE} 指定 NodePort 的端口范围；&lt;/p&gt;

&lt;p&gt;缺省情况下 kubernetes 对象保存在 etcd /registry 路径下，可以通过 &amp;ndash;etcd-prefix 参数进行调整；&lt;/p&gt;

&lt;p&gt;&amp;ndash;experimental-bootstrap-token-auth Bootstrap Token Authentication在1.9版本已经变成了正式feature，参数名称改为&amp;ndash;enable-bootstrap-token-auth&lt;/p&gt;

&lt;p&gt;如果中途修改过&amp;ndash;service-cluster-ip-range地址，则必须将default命名空间的kubernetes的service给删除，使用命令：kubectl delete service kubernetes，然后系统会自动用新的ip重建这个service，不然apiserver的log有报错the cluster IP x.x.x.x for service kubernetes/default is not within the service CIDR x.x.x.x/16; please recreate&lt;/p&gt;

&lt;p&gt;runtime-config配置为rbac.authorization.k8s.io/v1beta1，表示运行时的apiVersion；&lt;/p&gt;

&lt;p&gt;如果需要开通http的无认证的接口，则可以增加以下两个参数：&amp;ndash;insecure-port=8080&lt;/p&gt;

&lt;p&gt;&amp;ndash;insecure-bind-address=127.0.0.1。注意，生产上不要绑定到非127.0.0.1的地址上&lt;/p&gt;

&lt;p&gt;Kubernetes 1.9&lt;/p&gt;

&lt;p&gt;对于Kubernetes1.9集群，需要注意配置KUBE_API_ARGS环境变量中的&amp;ndash;authorization-mode=Node,RBAC，增加对Node授权的模式，否则将无法注册node。
&amp;ndash;experimental-bootstrap-token-auth Bootstrap Token Authentication在kubernetes 1.9版本已经废弃，参数名称改为&amp;ndash;enable-bootstrap-token-auth&lt;/p&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、创建 kube-controller-manager 的 systemd unit 文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; kube-controller-manager.service &amp;lt;&amp;lt;EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/bin/kube-controller-manager \
  --address=127.0.0.1 \
  --master=http://10.47.210.30:8080 \
  --allocate-node-cidrs=true \
  --service-cluster-ip-range=10.254.0.0/16 \
  --cluster-cidr=172.30.0.0/16 \
  --cluster-name=kubernetes \
  --leader-elect=true \
  --v=2 \
  --cluster-signing-cert-file=/root/ssl/ca.pem \
  --cluster-signing-key-file=/root/ssl/ca-key.pem  \
  --service-account-private-key-file=/root/ssl/ca-key.pem \
  --root-ca-file=/root/ssl/ca.pem
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数解析&lt;/p&gt;

&lt;p&gt;&amp;ndash;address值必须是127.0.0.1，因为当前的kube-apiserver期望scheduler和controller-manager在同一台机器上&lt;/p&gt;

&lt;p&gt;&amp;ndash;master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信；&lt;/p&gt;

&lt;p&gt;&amp;ndash;cluster-cidr 指定 Cluster 中 Pod 的 CIDR 范围，该网段在各 Node 间必须路由可达(flanneld保证)；&lt;/p&gt;

&lt;p&gt;&amp;ndash;service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；&lt;/p&gt;

&lt;p&gt;&amp;ndash;cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；&lt;/p&gt;

&lt;p&gt;&amp;ndash;root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；&lt;/p&gt;

&lt;p&gt;&amp;ndash;leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程；&lt;/p&gt;

&lt;p&gt;启动 kube-controller-manager&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们启动每个组件后可以通过执行命令kubectl get componentstatuses，来查看各个组件的状态;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                        ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused
controller-manager   Healthy     ok
etcd-2               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}
etcd-0               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}
etcd-1               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、创建 kube-scheduler 的 systemd unit 文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; kube-scheduler.service &amp;lt;&amp;lt;EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/bin/kube-scheduler \
  --address=127.0.0.1 \
  --master=http://10.47.210.30:8080 \
  --leader-elect=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数解析&lt;/p&gt;

&lt;p&gt;&amp;ndash;address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器；&lt;/p&gt;

&lt;p&gt;&amp;ndash;master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信；&lt;/p&gt;

&lt;p&gt;&amp;ndash;leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程；&lt;/p&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable kube-scheduler
systemctl start kube-scheduler
systemctl status kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证 master 节点功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}
etcd-1               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}
etcd-2               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;node部署&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;部署机器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;10.47.210.30
10.47.210.31
10.47.210.94
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;部署组件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flannel
docker
kubelet
kube-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、flannel&lt;/p&gt;

&lt;p&gt;1.安装二进制文件&lt;/p&gt;

&lt;p&gt;Flannel是作为一个二进制文件的方式部署在每个node上，主要实现两个功能：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.为每个node分配subnet，容器将自动从该子网中获取IP地址

2.当有node加入到网络中时，为每个node增加路由配置
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前使用版本默认安装的是0.7.1版本的flannel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget  https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我没有下载下来，直接github下载&lt;/p&gt;

&lt;p&gt;解压设置环境变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf flannel-v0.7.1-linux-amd64.tar.gz
echo &#39;export PATH=/home/flannel:$PATH&#39; &amp;gt;&amp;gt; /etc/profile
source /etc/profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者把二进制文件移到系统路径下，比如/usr/local/src&lt;/p&gt;

&lt;p&gt;2.创建ca证书&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; flanneld-csr.json &amp;lt;&amp;lt;EOF
{
  &amp;quot;CN&amp;quot;: &amp;quot;flanneld&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成 flanneld 证书和私钥：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cfssl gencert -ca=/home/ssl/ca.pem \
  -ca-key=/home/ssl/ca-key.pem \
  -config=/home/ssl/ca-config.json \
  -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;拷贝到所有主机&lt;/p&gt;

&lt;p&gt;向 etcd 写入集群 Pod 网段信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcdctl \
  --endpoints=https://10.47.210.30:2379,https://10.47.210.31:2379,https://10.47.210.94:2379 \
  --ca-file=/root/ssl/ca.pem \
  --cert-file=/root/ssl/flanneld.pem \
  --key-file=/root/ssl/flanneld-key.pem \
  set /kubernetes/network/config &#39;{&amp;quot;Network&amp;quot;:&amp;quot;&#39;172.30.0.0/16&#39;&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: {&amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot;}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写入的 Pod 网段(${CLUSTER_CIDR}，172.30.0.0/16) 必须与 kube-controller-manager 的 &amp;ndash;cluster-cidr 选项值一致；&lt;/p&gt;

&lt;p&gt;也可以这样写&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcdctl --endpoints=https://192.168.56.107:2379,https://192.168.56.106:2379,https://192.168.56.105:2379 \
  --ca-file=/home/ssl/ca.pem \
  --cert-file=/home/ssl/kubernetes.pem \
  --key-file=/home/ssl/kubernetes-key.pem \
  mkdir /kubernetes/network
etcdctl --endpoints=https://192.168.56.107:2379,https://192.168.56.106:2379,https://192.168.56.105:2379 \
  --ca-file=/home/ssl/ca.pem \
  --cert-file=/home/ssl/kubernetes.pem \
  --key-file=/home/ssl/kubernetes-key.pem \
  mk /kubernetes/network/config &#39;{&amp;quot;Network&amp;quot;:&amp;quot;172.30.0.0/16&amp;quot;,&amp;quot;SubnetLen&amp;quot;:24,&amp;quot;Backend&amp;quot;:{&amp;quot;Type&amp;quot;:&amp;quot;vxlan&amp;quot;}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你要使用host-gw模式，可以直接将vxlan改成host-gw即可&lt;/p&gt;

&lt;p&gt;创建 flanneld 的 systemd unit 文件&lt;/p&gt;

&lt;p&gt;目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/lib/systemd/system



cat &amp;gt; flanneld.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
ExecStart=/usr/bin/flanneld \\
  -etcd-cafile=/root/ssl/ca.pem \\
  -etcd-certfile=/root/ssl/flanneld.pem \\
  -etcd-keyfile=/root/ssl/flanneld-key.pem \\
  -etcd-endpoints=https://10.47.210.30:2379,https://10.47.210.31:2379,https://10.47.210.94:2379 \
  -etcd-prefix=/kubernetes/network \\
  -iface=eth0
ExecStartPost=/root/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在参数最后有空格也会导致启动失败&lt;/p&gt;

&lt;p&gt;etcd集群启动了双向tls认证，所以需要为flanneld指定与etcd集群通信的ca和密钥&lt;/p&gt;

&lt;p&gt;mk-docker-opts.sh脚本将分配给flanneld的pod子网网段信息写入到/run/flannel/docker文件中，后续docker启动时使用这个文件中参数值设置为docker0的网桥&lt;/p&gt;

&lt;p&gt;-iface 选项指定flanneld和其他node通信的接口，如果机器有内外网，则最好指定为内网接口&lt;/p&gt;

&lt;p&gt;4.加载service，启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable flanneld
systemctl start flanneld
systemctl status flanneld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查 flanneld 服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ journalctl  -u flanneld |grep &#39;Lease acquired&#39;
$ ifconfig flannel.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看集群 Pod 网段(/16)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /root/local/bin/etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/flanneld/ssl/flanneld.pem \
  --key-file=/etc/flanneld/ssl/flanneld-key.pem \
  get ${FLANNEL_ETCD_PREFIX}/config
{ &amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; } }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看已分配的 Pod 子网段列表(/24)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /root/local/bin/etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/flanneld/ssl/flanneld.pem \
  --key-file=/etc/flanneld/ssl/flanneld-key.pem \
  ls ${FLANNEL_ETCD_PREFIX}/subnets
/kubernetes/network/subnets/172.30.19.0-24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看某一 Pod 网段对应的 flanneld 进程监听的 IP 和网络参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /root/local/bin/etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/flanneld/ssl/flanneld.pem \
  --key-file=/etc/flanneld/ssl/flanneld-key.pem \
  get ${FLANNEL_ETCD_PREFIX}/subnets/172.30.19.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;10.64.3.7&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;d6:51:2e:80:5c:69&amp;quot;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确保各节点间 Pod 网段能互联互通&lt;/p&gt;

&lt;p&gt;在各节点上部署完 Flannel 后，查看已分配的 Pod 子网段列表(/24)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcdctl \
  --endpoints=https://192.168.56.107:2379,https://192.168.56.106:2379,https://192.168.56.105:2379 \
  --ca-file=/home/ssl/ca.pem \
  --cert-file=/home/ssl/kubernetes.pem \
  --key-file=/home/ssl/kubernetes-key.pem \
  ls /kubernetes/network/subnets
/kubernetes/network/subnets/172.30.19.0-24
/kubernetes/network/subnets/172.30.20.0-24
/kubernetes/network/subnets/172.30.21.0-24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当前三个节点分配的 Pod 网段分别是：172.30.19.0-24、172.30.20.0-24、172.30.21.0-24。&lt;/p&gt;

&lt;p&gt;在各节点上分配 ping 这三个网段的网关地址，确保能通：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ping 172.30.19.1
$ ping 172.30.20.2
$ ping 172.30.21.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、docker&lt;/p&gt;

&lt;p&gt;安装和配置 docker&lt;/p&gt;

&lt;p&gt;下载最新的 docker 二进制文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://get.docker.com/builds/Linux/x86_64/docker-17.04.0-ce.tgz
$ tar -xvf docker-17.04.0-ce.tgz
$ cp docker/docker* /root/local/bin
$ cp docker/completion/bash/docker /etc/bash_completion.d/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 docker 的 systemd unit 文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; docker.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Docker Application Container Engine
Documentation=http://docs.docker.io

[Service]
Environment=&amp;quot;PATH=/root/local/bin:/bin:/sbin:/usr/bin:/usr/sbin&amp;quot;
EnvironmentFile=-/run/flannel/docker
ExecStart=/usr/bin/dockerd --log-level=error $DOCKER_NETWORK_OPTIONS
ExecReload=/bin/kill -s HUP $MAINPID
Restart=on-failure
RestartSec=5
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；&lt;/p&gt;

&lt;p&gt;flanneld 启动时将网络配置写入到 /run/flannel/docker 文件中的变量 DOCKER_NETWORK_OPTIONS，dockerd 命令行上指定该变量值来设置 docker0 网桥参数；&lt;/p&gt;

&lt;p&gt;如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；&lt;/p&gt;

&lt;p&gt;不能关闭默认开启的 &amp;ndash;iptables 和 &amp;ndash;ip-masq 选项；&lt;/p&gt;

&lt;p&gt;如果内核版本比较新，建议使用 overlay 存储驱动；&lt;/p&gt;

&lt;p&gt;docker 从 1.13 版本开始，可能将 iptables FORWARD chain的默认策略设置为DROP，从而导致 ping 其它 Node 上的 Pod IP 失败，遇到这种情况时，需要手动设置策略为 ACCEPT：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo iptables -P FORWARD ACCEPT
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并且把以下命令写入/etc/rc.local文件中，防止节点重启iptables FORWARD chain的默认策略又还原为DROP&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sleep 60 &amp;amp;&amp;amp; /sbin/iptables -P FORWARD ACCEPT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了加快 pull image 的速度，可以使用国内的仓库镜像服务器，同时增加下载的并发数。(如果 dockerd 已经运行，则需要重启 dockerd 生效。)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ cat /etc/docker/daemon.json
  {
    &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;, &amp;quot;hub-mirror.c.163.com&amp;quot;],
    &amp;quot;max-concurrent-downloads&amp;quot;: 10
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl stop firewalld
systemctl disable firewalld
iptables -F &amp;amp;&amp;amp; sudo iptables -X &amp;amp;&amp;amp; sudo iptables -F -t nat &amp;amp;&amp;amp; sudo iptables -X -t nat
systemctl enable docker
systemctl start docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；&lt;/p&gt;

&lt;p&gt;4、kubelet&lt;/p&gt;

&lt;p&gt;kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;ndash;user=kubelet-bootstrap 是文件 /etc/kubernetes/token.csv 中指定的用户名，同时也写入了文件 /etc/kubernetes/bootstrap.kubeconfig；&lt;/p&gt;

&lt;p&gt;下载最新的 kubelet 和 kube-proxy 二进制文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz
$ tar -xzvf kubernetes-server-linux-amd64.tar.gz
$ cd kubernetes
$ tar -xzvf  kubernetes-src.tar.gz
$ sudo cp -r ./server/bin/{kube-proxy,kubelet} /root/local/bin/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 kubelet 的 systemd unit 文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir /var/lib/kubelet # 必须先创建工作目录


cat &amp;gt; kubelet.service &amp;lt;&amp;lt;EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/bin/kubelet \\
  --address=10.47.210.30 \\
  --hostname-override=10.47.210.30 \\
  --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\
  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --cert-dir=/root/ssl \\
  --cluster-dns=10.254.0.2 \\
  --cluster-domain=cluster.local. \\
  --hairpin-mode promiscuous-bridge \\
  --allow-privileged=true \\
  --serialize-image-pulls=false \\
  --logtostderr=true \\
  --runtime-cgroups=/systemd/system.slice \\
  --kubelet-cgroups=/systemd/system.slice \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数解析&lt;/p&gt;

&lt;p&gt;&amp;ndash;address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；&lt;/p&gt;

&lt;p&gt;如果设置了 &amp;ndash;hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；&lt;/p&gt;

&lt;p&gt;&amp;ndash;experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；&lt;/p&gt;

&lt;p&gt;管理员通过了 CSR 请求后，kubelet 自动在 &amp;ndash;cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 &amp;ndash;kubeconfig 文件；&lt;/p&gt;

&lt;p&gt;建议在 &amp;ndash;kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 &amp;ndash;api-servers 选项，则必须指定 &amp;ndash;require-kubeconfig 选项后才从配置文件中读取 kube-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;1.14不再支持require-kubeconfig选项&lt;/p&gt;

&lt;p&gt;&amp;ndash;cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，&amp;ndash;cluster-domain 指定域名后缀，这两个参数同时指定后才会生效；&lt;/p&gt;

&lt;p&gt;对于kuberentes1.8集群中的kubelet配置，取消了KUBELET_API_SERVER的配置，而改用kubeconfig文件来定义master地址，所以请注释掉&amp;ndash;api-servers=&lt;a href=&#34;http://172.20.0.113:8080配置。&#34;&gt;http://172.20.0.113:8080配置。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;如果使用systemd方式启动，则需要额外增加两个参数&amp;ndash;runtime-cgroups=/systemd/system.slice &amp;ndash;kubelet-cgroups=/systemd/system.slice，1.14不加也可以&lt;/p&gt;

&lt;p&gt;&amp;ndash;experimental-bootstrap-kubeconfig 在1.9版本已经变成了&amp;ndash;bootstrap-kubeconfig&lt;/p&gt;

&lt;p&gt;&amp;rdquo;&amp;ndash;cgroup-driver 配置成 systemd，不要使用cgroup，否则在 CentOS 系统中 kubelet 将启动失败（保持docker和kubelet中的cgroup driver配置一致即可，不一定非使用systemd）。&lt;/p&gt;

&lt;p&gt;&amp;ndash;cluster-domain 指定 pod 启动时 /etc/resolve.conf 文件中的 search domain ，起初我们将其配置成了 cluster.local.，这样在解析 service 的 DNS 名称时是正常的，可是在解析 headless service 中的 FQDN pod name 的时候却错误，因此我们将其修改为 cluster.local，去掉嘴后面的 ”点号“ 就可以解决该问题，关于 kubernetes 中的域名/服务名称解析请参见我的另一篇文章。&lt;/p&gt;

&lt;p&gt;&amp;ndash;kubeconfig=/etc/kubernetes/kubelet.kubeconfig中指定的kubelet.kubeconfig文件在第一次启动kubelet之前并不存在，请看下文，当通过CSR请求后会自动生成kubelet.kubeconfig文件，如果你的节点上已经生成了~/.kube/config文件，你可以将该文件拷贝到该路径下，并重命名为kubelet.kubeconfig，所有node节点可以共用同一个kubelet.kubeconfig文件，这样新添加的节点就不需要再创建CSR请求就能自动添加到kubernetes集群中。同样，在任意能够访问到kubernetes集群的主机上使用kubectl &amp;ndash;kubeconfig命令操作集群时，只要使用~/.kube/config文件就可以通过权限认证，因为这里面已经有认证信息并认为你是admin用户，对集群拥有所有权限。&lt;/p&gt;

&lt;p&gt;KUBELET_POD_INFRA_CONTAINER 是基础镜像容器，这里我用的是私有镜像仓库地址，大家部署的时候需要修改为自己的镜像。我上传了一个到时速云上，可以直接 docker pull index.tenxcloud.com/jimmy/pod-infrastructure 下载。pod-infrastructure镜像是Redhat制作的，大小接近80M，下载比较耗时，其实该镜像并不运行什么具体进程，可以使用Google的pause镜像gcr.io/google_containers/pause-amd64:3.0，这个镜像只有300多K，或者通过DockerHub下载jimmysong/pause-amd64:3.0。&lt;/p&gt;

&lt;p&gt;kubelet cAdvisor 默认在所有接口监听 4194 端口的请求，对于有外网的机器来说不安全，ExecStartPost 选项指定的 iptables 规则只允许内网机器访问 4194 端口；&lt;/p&gt;

&lt;p&gt;不支持&lt;/p&gt;

&lt;p&gt;启动错误&lt;/p&gt;

&lt;p&gt;1.遇到kubelet报错，该服务每没隔几秒重启一下，然后自动停止。日志提示信息中有一行：container_manager_linux.go:205] Running with swap on is not supported, please disable swap! This will be a fatal error by default starting in K8s v1.6!。尝试关闭swap再试，虽然日志中没有该提示信息了，&lt;/p&gt;

&lt;p&gt;一、不重启电脑，禁用启用swap，立刻生效&lt;/p&gt;

&lt;p&gt;1、禁用命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo swapoff -a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、启用命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo swapon -a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、查看交换分区的状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo free -m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;二、重新启动电脑，永久禁用Swap&lt;/p&gt;

&lt;p&gt;1、把根目录文件系统设为可读写&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo mount -n -o remount,rw /
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、用vi修改/etc/fstab文件，在swap分区这行前加 # 禁用掉，保存退出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/fstab

i      #进入insert 插入模式

:wq   #保存退出
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、重新启动电脑，使用free -m查看分区状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reboot

sudo free -m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相对于kubernetes1.6集群必须进行的配置有：&lt;/p&gt;

&lt;p&gt;对于kuberentes1.8集群，必须关闭swap，否则kubelet启动将失败。&lt;/p&gt;

&lt;p&gt;修改/etc/fstab将，swap系统注释掉。&lt;/p&gt;

&lt;p&gt;2.failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: &amp;ldquo;cgroupfs&amp;rdquo;&lt;/p&gt;

&lt;p&gt;修改docker的cgroup驱动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim /lib/systemd/system/docker.service
# 将 --exec-opt native.cgroupdriver=systemd  修改为：
#  --exec-opt native.cgroupdriver=cgroupfs
# systemctl daemon-reload 
# systemctl restart docker.service
# kubelet显示正常
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.重启了docker后还要重启kubelet，这时又遇到问题，kubelet启动失败。报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Mar 31 16:44:41 sz-pg-oam-docker-test-002.tendcloud.com kubelet[81047]: error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: &amp;quot;cgroupfs&amp;quot; is different from docker cgroup driver: &amp;quot;systemd&amp;quot;
这是kubelet与docker的cgroup driver不一致导致的，kubelet启动的时候有个—cgroup-driver参数可以指定为&amp;quot;cgroupfs&amp;quot;或者“systemd”。

--cgroup-driver string                                    Driver that the kubelet uses to manipulate cgroups on the host.  Possible values: &#39;cgroupfs&#39;, &#39;systemd&#39; (default &amp;quot;cgroupfs&amp;quot;)
配置docker的service配置文件/usr/lib/systemd/system/docker.service，设置ExecStart中的--exec-opt native.cgroupdriver=systemd。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 kubelet 的 TLS 证书请求&lt;/p&gt;

&lt;p&gt;kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。&lt;/p&gt;

&lt;p&gt;查看未授权的 CSR 请求：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-2b308   4m        kubelet-bootstrap   Pending
$ kubectl get nodes
No resources found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 CSR 请求：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl certificate approve csr-2b308
certificatesigningrequest &amp;quot;csr-2b308&amp;quot; approved



$ kubectl get nodes
NAME        STATUS    AGE       VERSION
10.64.3.7   Ready     49m       v1.6.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;自动生成了 kubelet kubeconfig 文件和公私钥：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -l /etc/kubernetes/kubelet.kubeconfig
-rw------- 1 root root 2284 Apr  7 02:07 /etc/kubernetes/kubelet.kubeconfig
$ ls -l /etc/kubernetes/ssl/kubelet*
-rw-r--r-- 1 root root 1046 Apr  7 02:07 /etc/kubernetes/ssl/kubelet-client.crt
-rw------- 1 root root  227 Apr  7 02:04 /etc/kubernetes/ssl/kubelet-client.key
-rw-r--r-- 1 root root 1103 Apr  7 02:07 /etc/kubernetes/ssl/kubelet.crt
-rw------- 1 root root 1675 Apr  7 02:07 /etc/kubernetes/ssl/kubelet.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、kube-proxy&lt;/p&gt;

&lt;p&gt;创建 kube-proxy 的 systemd unit 文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir -p /var/lib/kube-proxy # 必须先创建工作目录




cat &amp;gt; kube-proxy.service &amp;lt;&amp;lt;EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/bin/kube-proxy \
  --bind-address=10.47.210.30 \
  --hostname-override=10.47.210.30 \
  --cluster-cidr=172.30.0.0/16 \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
  --logtostderr=true \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数解析&lt;/p&gt;

&lt;p&gt;&amp;ndash;hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；&lt;/p&gt;

&lt;p&gt;&amp;ndash;cluster-cidr 必须与 kube-controller-manager 的 &amp;ndash;cluster-cidr 选项值一致；&lt;/p&gt;

&lt;p&gt;kube-proxy 根据 &amp;ndash;cluster-cidr 判断集群内部和外部流量，指定 &amp;ndash;cluster-cidr 或 &amp;ndash;masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；&lt;/p&gt;

&lt;p&gt;&amp;ndash;kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；&lt;/p&gt;

&lt;p&gt;预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；&lt;/p&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边集群就已经手动部署完成了，可以直接使用了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;harbor&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;私有镜像仓库是我们自己使用k8s集群必须要使用的，可以使用原生的registry，目前最活跃的就是harbor，是国人写的，各种比较友好，最好使用1.7.5版本之后的&lt;/p&gt;

&lt;p&gt;harbor是直接使用docker-compose单机编排的&lt;/p&gt;

&lt;p&gt;所以安装docker和docker-compose&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#下载离线安装软件
wget http://harbor.orientsoft.cn/harbor-v1.3.0-rc4/harbor-offline-installer-v1.3.0-rc4.tgz
#解压文件
tar -zxf harbor-offline-installer-v1.3.0-rc4.tgz
#解压后的文件夹是harbor
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置文件：域名，端口，密码，存放路径&lt;/p&gt;

&lt;p&gt;然后直接使用harbor目录中的install.sh就可以安装启动了，然后就可以根据域名+port来访问，当然我们正常使用的情况下都是会在harbor前加一层nginx转发，这个nginx需要注意转发大小的设置&lt;/p&gt;

&lt;h1 id=&#34;参数&#34;&gt;参数&lt;/h1&gt;

&lt;p&gt;由于参数比较多，可以直接参考K8s权威指南第二张第一节的部署中有很详细的解释，这边就不多写了，需要查阅。&lt;/p&gt;

&lt;h1 id=&#34;应用&#34;&gt;应用&lt;/h1&gt;

&lt;h2 id=&#34;k8s运行的服务&#34;&gt;k8s运行的服务&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;无状态：deployment&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;认为所有pod都是一样的，不具备与其他实例有不同的关系。&lt;/li&gt;
&lt;li&gt;没有顺序的要求。&lt;/li&gt;
&lt;li&gt;不用考虑再哪个Node运行。&lt;/li&gt;
&lt;li&gt;随意扩容缩容。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;有状态：SatefulSet&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;集群节点之间的关系。&lt;/li&gt;
&lt;li&gt;数据不完全一致。&lt;/li&gt;
&lt;li&gt;实例之间不对等的关系。&lt;/li&gt;
&lt;li&gt;依靠外部存储的应用。&lt;/li&gt;
&lt;li&gt;通过dns维持身份&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还可以细分，在K8S运行的服务，从简单到复杂可以分成三类：无状态服务、普通有状态服务和有状态集群服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/server&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;无状态服务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;无状态服务，K8S使用RC（或更新的Replica Set）来保证一个服务的实例数量，如果说某个Pod实例由于某种原因Crash了，RC会立刻用这个Pod的模版新启一个Pod来替代它，由于是无状态的服务，新启的Pod与原来健康状态下的Pod一模一样。在Pod被重建后它的IP地址可能发生变化，为了对外提供一个稳定的访问接口，K8S引入了Service的概念。一个Service后面可以挂多个Pod，实现服务的高可用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;普通有状态服务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;普通有状态服务，和无状态服务相比，它多了状态保存的需求。Kubernetes提供了以Volume和Persistent Volume为基础的存储系统，可以实现服务的状态保存。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;有状态集群服务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有状态集群服务，与普通有状态服务相比，它多了集群管理的需求。K8S为此开发了一套以Pet Set为核心的全新特性，方便了有状态集群服务在K8S上的部署和管理。具体来说是通过Init Container来做集群的初始化工作，用 Headless Service 来维持集群成员的稳定关系，用动态存储供给来方便集群扩容，最后用Pet Set来综合管理整个集群。&lt;/p&gt;

&lt;p&gt;要运行有状态集群服务要解决的问题有两个,一个是状态保存，另一个是集群管理。&lt;/p&gt;

&lt;p&gt;1、状态保存&lt;/p&gt;

&lt;p&gt;Kubernetes 有一套以Volume插件为基础的存储系统，通过这套存储系统可以实现应用和服务的状态保存。&lt;/p&gt;

&lt;p&gt;K8S的存储系统从基础到高级又大致分为三个层次：普通Volume，Persistent Volume 和动态存储供应。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;普通Volume&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;最简单的普通Volume是单节点Volume。它和Docker的存储卷类似，使用的是Pod所在K8S节点的本地目录。&lt;/p&gt;

&lt;p&gt;第二种类型是跨节点存储卷，这种存储卷不和某个具体的K8S节点绑定，而是独立于K8S节点存在的，整个存储集群和K8S集群是两个集群，相互独立。&lt;/p&gt;

&lt;p&gt;跨节点的存储卷在Kubernetes上用的比较多，如果已有的存储不能满足要求，还可以开发自己的Volume插件，只需要实现Volume.go 里定义的接口。如果你是一个存储厂商，想要自己的存储支持Kubernetes 上运行的容器，就可以去开发一个自己的Volume插件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Persistent Volume&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;普通Volume和使用它的Pod之间是一种静态绑定关系，在定义Pod的文件里，同时定义了它使用的Volume。Volume 是Pod的附属品，我们无法单独创建一个Volume，因为它不是一个独立的K8S资源对象。&lt;/p&gt;

&lt;p&gt;而Persistent Volume 简称PV是一个K8S资源对象，所以我们可以单独创建一个PV。它不和Pod直接发生关系，而是通过Persistent Volume Claim，简称PVC来实现动态绑定。Pod定义里指定的是PVC，然后PVC会根据Pod的要求去自动绑定合适的PV给Pod使用。&lt;/p&gt;

&lt;p&gt;PV的访问模式有三种：&lt;/p&gt;

&lt;p&gt;第一种，ReadWriteOnce：是最基本的方式，可读可写，但只支持被单个Pod挂载。&lt;/p&gt;

&lt;p&gt;第二种，ReadOnlyMany：可以以只读的方式被多个Pod挂载。&lt;/p&gt;

&lt;p&gt;第三种，ReadWriteMany：这种存储可以以读写的方式被多个Pod共享。不是每一种存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是NFS。在PVC绑定PV时通常根据两个条件来绑定，一个是存储的大小，另一个就是访问模式。&lt;/p&gt;

&lt;p&gt;刚才提到说PV与普通Volume的区别是动态绑定，我们来看一下这个过程是怎样的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/server1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这是PV的生命周期，首先是Provision，即创建PV，这里创建PV有两种方式，静态和动态。所谓静态，是管理员手动创建一堆PV，组成一个PV池，供PVC来绑定。动态方式是通过一个叫 Storage Class的对象由存储系统根据PVC的要求自动创建。&lt;/p&gt;

&lt;p&gt;一个PV创建完后状态会变成Available，等待被PVC绑定。&lt;/p&gt;

&lt;p&gt;一旦被PVC邦定，PV的状态会变成Bound，就可以被定义了相应PVC的Pod使用。&lt;/p&gt;

&lt;p&gt;Pod使用完后会释放PV，PV的状态变成Released。&lt;/p&gt;

&lt;p&gt;变成Released的PV会根据定义的回收策略做相应的回收工作。有三种回收策略，Retain、Delete 和 Recycle。Retain就是保留现场，K8S什么也不做，等待用户手动去处理PV里的数据，处理完后，再手动删除PV。Delete 策略，K8S会自动删除该PV及里面的数据。Recycle方式，K8S会将PV里的数据删除，然后把PV的状态变成Available，又可以被新的PVC绑定使用。&lt;/p&gt;

&lt;p&gt;在实际使用场景里，PV的创建和使用通常不是同一个人。这里有一个典型的应用场景：管理员创建一个PV池，开发人员创建Pod和PVC，PVC里定义了Pod所需存储的大小和访问模式，然后PVC会到PV池里自动匹配最合适的PV给Pod使用。&lt;/p&gt;

&lt;p&gt;前面在介绍PV的生命周期时，提到PV的供给有两种方式，静态和动态。其中动态方式是通过StorageClass来完成的，这是一种新的存储供应方式。&lt;/p&gt;

&lt;p&gt;使用StorageClass有什么好处呢？除了由存储系统动态创建，节省了管理员的时间，还有一个好处是可以封装不同类型的存储供PVC选用。在StorageClass出现以前，PVC绑定一个PV只能根据两个条件，一个是存储的大小，另一个是访问模式。在StorageClass出现后，等于增加了一个绑定维度。&lt;/p&gt;

&lt;p&gt;比如这里就有两个StorageClass，它们都是用谷歌的存储系统，但是一个使用的是普通磁盘，我们把这个StorageClass命名为slow。另一个使用的是SSD，我们把它命名为fast。&lt;/p&gt;

&lt;p&gt;在PVC里除了常规的大小、访问模式的要求外，还通过annotation指定了Storage Class的名字为fast，这样这个PVC就会绑定一个SSD，而不会绑定一个普通的磁盘。&lt;/p&gt;

&lt;p&gt;到这里Kubernetes的整个存储系统就都介绍完了。总结一下，两种存储卷：普通Volume 和Persistent Volume。普通Volume在定义Pod的时候直接定义，Persistent Volume通过Persistent Volume Claim来动态绑定。PV可以手动创建,也可以通过StorageClass来动态创建。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;动态存储供应&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面重介绍Kubernetes与有状态集群服务相关的两个新特性：Init Container 和 Pet Set  。&lt;/p&gt;

&lt;p&gt;1、Init Container&lt;/p&gt;

&lt;p&gt;做初始化工作的容器。可以有一个或多个，如果有多个，这些 Init Container 按照定义的顺序依次执行，只有所有的Init Container 执行完后，主容器才启动。由于一个Pod里的存储卷是共享的，所以 Init Container 里产生的数据可以被主容器使用到。&lt;/p&gt;

&lt;p&gt;Init Container可以在多种 K8S 资源里被使用到如 Deployment、Daemon Set, Pet Set, Job等，但归根结底都是在Pod启动时，在主容器启动前执行，做初始化工作。&lt;/p&gt;

&lt;p&gt;第一种场景是等待其它模块Ready，比如我们有一个应用里面有两个容器化的服务，一个是Web Server，另一个是数据库。其中Web Server需要访问数据库。但是当我们启动这个应用的时候，并不能保证数据库服务先启动起来，所以可能出现在一段时间内Web Server有数据库连接错误。为了解决这个问题，我们可以在运行Web Server服务的Pod里使用一个Init Container，去检查数据库是否准备好，直到数据库可以连接，Init Container才结束退出，然后Web Server容器被启动，发起正式的数据库连接请求。&lt;/p&gt;

&lt;p&gt;第二种场景是做初始化配置，比如集群里检测所有已经存在的成员节点，为主容器准备好集群的配置信息，这样主容器起来后就能用这个配置信息加入集群。&lt;/p&gt;

&lt;p&gt;还有其它使用场景，如将pod注册到一个中央数据库、下载应用依赖等。&lt;/p&gt;

&lt;p&gt;这个例子创建一个Pod，这个Pod里跑的是一个nginx容器，Pod里有一个叫workdir的存储卷，访问nginx容器服务的时候，就会显示这个存储卷里的index.html 文件。&lt;/p&gt;

&lt;p&gt;而这个index.html 文件是如何获得的呢？是由一个Init Container从网络上下载的。这个Init Container 使用一个busybox镜像，起来后，执行一条wget命令，获取index.html文件，然后结束退出。&lt;/p&gt;

&lt;p&gt;由于Init Container和nginx容器共享一个存储卷（这里这个存储卷的名字叫workdir），所以在Init container里下载的index.html文件可以在nginx容器里被访问到。&lt;/p&gt;

&lt;p&gt;可以看到 Init Container 是在 annotation里定义的。Annotation 是K8S新特性的实验场，通常一个新的Feature出来一般会先在Annotation 里指定，等成熟稳定了，再给它一个正式的属性名或资源对象名。&lt;/p&gt;

&lt;p&gt;2、Pet Set&lt;/p&gt;

&lt;p&gt;Pet代表有状态服务，Set是集合的意思&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/server2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;具体来说，一个Pet有三个特征。&lt;/p&gt;

&lt;p&gt;一是有稳定的存储，这是通过我们前面介绍的PV/PVC 来实现的。&lt;/p&gt;

&lt;p&gt;二是稳定的网络身份，这是通过一种叫 Headless Service 的特殊Service来实现的。要理解Headless Service是如何工作的，需要先了解Service是如何工作。我们提到过Service可以为多个Pod实例提供一个稳定的对外访问接口。这个稳定的接口是如何实现的的呢，是通过Cluster IP来实现的，Cluster IP是一个虚拟IP，不是真正的IP，所以稳定。K8S会在每个节点上创建一系列的IPTables规则，实现从Cluster IP到实际Pod IP的转发。同时还会监控这些Pod的IP地址变化，如果变了，会更新IP Tables规则，使转发路径保持正确。所以即使Pod IP有变化，外部照样能通过Service的ClusterIP访问到后面的Pod。&lt;/p&gt;

&lt;p&gt;普通Service的Cluster IP 是对外的，用于外部访问多个Pod实例。而Headless Service的作用是对内的，用于为一个集群内部的每个成员提供一个唯一的DNS名字，这样集群成员之间就能相互通信了。所以Headless Service没有Cluster IP，这是它和普通Service的区别。&lt;/p&gt;

&lt;p&gt;Headless Service为每个集群成员创建的DNS名字是什么样的呢？右下角是一个例子，第一个部分是每个Pet自己的名字，后面foo是Headless Service的名字，default是PetSet所在命名空间的名字，cluser.local是K8S集群的域名。对于同一个Pet Set里的每个Pet，除了Pet自己的名字，后面几部分都是一样的。所以要有一个稳定且唯一的DNS名字，就要求每个Pet的名字是稳定且唯一的。&lt;/p&gt;

&lt;p&gt;三是序号命名规则。Pet是一种特殊的Pod，那么Pet能不能用Pod的命名规则呢？答案是不能，因为Pod的名字是不稳定的。Pod的命名规则是，如果一个Pod是由一个RC创建的，那么Pod的名字是RC的名字加上一个随机字符串。为什么要加一个随机字符串，是因为RC里指定的是Pod的模版，为了实现高可用，通常会从这个模版里创建多个一模一样的Pod实例，如果没有这个随机字符串，同一个RC创建的Pod之间就会由名字冲突。&lt;/p&gt;

&lt;p&gt;如果说某个Pod由于某种原因死掉了，RC会新建一个来代替它，但是这个新建里的Pod名字里的随机字符串与原来死掉的Pod是不一样的。所以Pod的名字跟它的IP一样是不稳定的。&lt;/p&gt;

&lt;p&gt;为了解决名字不稳定的问题，K8S对Pet的名字不再使用随机字符串，而是为每个Pet分配一个唯一不变的序号，比如 Pet Set 的名字叫 mysql，那么第一个启起来的Pet就叫 mysql-0，第二个叫 mysql-1，如此下去。&lt;/p&gt;

&lt;p&gt;当一个Pet down 掉后，新创建的Pet 会被赋予跟原来Pet一样的名字。由于Pet名字不变所以DNS名字也跟以前一样，同时通过名字还能匹配到原来Pet用到的存储，实现状态保存。&lt;/p&gt;

&lt;p&gt;这些是Pet Set 相关的一些操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Peer discovery，这和我们上面的Headless Service有密切关系。通过Pet Set的 Headless Service，可以查到该Service下所有的Pet 的 DNS 名字。这样就能发现一个Pet Set 里所有的Pet。当一个新的Pet起来后，就可以通过Peer Discovery来找到集群里已经存在的所有节点的DNS名字，然后用它们来加入集群。
更新Replicas的数目、实现扩容和缩容。
更新Pet Set里Pet的镜像版本，实现升级。
删除 Pet Set。删除一个Pet Set 会先把这个Pet Set的Replicas数目缩减为0，等到所有的Pet都被删除了，再删除 Pet Set本身。注意Pet用到的存储不会被自动删除。这样用户可以把数据拷贝走了，再手动删除。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kubectl&#34;&gt;kubectl&lt;/h2&gt;

&lt;p&gt;我们可以通过三种方式来访问apiserver提供的接口&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;REST API
比如访问nodes，我们可以访问/api/v1/proxy/nodes/{names}&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;各种语言的client lib&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令行kubectl&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我们最直接的就是使用kubectl来看各种应用的情况&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我是macOS 系统并使用 Homebrew 包管理器，通过 Homebrew 安装 kubectl。&lt;/p&gt;

&lt;p&gt;运行安装命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install kubernetes-cli
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试以确保您安装的版本是最新的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:minikube chunyinjiang$ kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;18&amp;quot;, GitVersion:&amp;quot;v1.18.3&amp;quot;, GitCommit:&amp;quot;2e7996e3e2712684bc73f0dec0200d64eec7fe40&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2020-05-21T14:51:23Z&amp;quot;, GoVersion:&amp;quot;go1.14.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;darwin/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;18&amp;quot;, GitVersion:&amp;quot;v1.18.3&amp;quot;, GitCommit:&amp;quot;2e7996e3e2712684bc73f0dec0200d64eec7fe40&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2020-05-20T12:43:34Z&amp;quot;, GoVersion:&amp;quot;go1.13.9&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然也可以下载二进制文件进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;通过以下命令下载 kubectl 的最新版本：

curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl

若需要下载特定版本的 kubectl，请将上述命令中的 $(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt) 部分替换成为需要下载的 kubectl 的具体版本即可。

例如，如果需要下载 v1.18.0 版本在 macOS 系统上,需要使用如下命令：

    curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/darwin/amd64/kubectl
    修改所下载的 kubectl 二进制文件为可执行模式。

    chmod +x ./kubectl
    将 kubectl 可执行文件放置到你的 PATH 目录下。

    sudo mv ./kubectl /usr/local/bin/kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;kubectl默认情况下，kubectl 会从 $HOME/.kube 目录下查找文件名为 config 的文件。您可以通过设置环境变量 KUBECONFIG 或者通过设置 &amp;ndash;kubeconfig 去指定其它 kubeconfig 文件。&lt;/p&gt;

&lt;p&gt;我们看一下这个默认的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:exercise chunyinjiang$ cat ~/.kube/config
apiVersion: v1
clusters:（集群，下面是集群相关的信息）
- cluster:
    certificate-authority: /Users/chunyinjiang/.minikube/ca.crt
    server: https://192.168.99.101:8443
  name: minikube
contexts:（上下文，关联集群，用户和命名空间）
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:（用户，下面是用户相关的信息）
- name: minikube
  user:
    client-certificate: /Users/chunyinjiang/.minikube/profiles/minikube/client.crt
    client-key: /Users/chunyinjiang/.minikube/profiles/minikube/client.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到配置文件中有三部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cluster（集群）: 存储api-server的CA根证书、api-server地址、集群名称等。&lt;/li&gt;
&lt;li&gt;user（用户）: 真正配置用户认证时的凭证信息，使用不同的认证策略，包含不同的字段。&lt;/li&gt;
&lt;li&gt;context（上下文）: 把cluster和user关联起来组成一个集群环境信息，声明通过哪个user连哪个cluster。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;context&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;kubeconfig 文件可以包含 context 元素，每个 context 都是一个由（集群、命名空间、用户）描述的三元组。您可以使用 kubectl config use-context 去设置当前的 context。命令行工具 kubectl 与当前 context 中指定的集群和命名空间进行通信，并且使用当前 context 中包含的用户凭证&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;环境变量 KUBECONFIG&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;环境变量 KUBECONFIG 保存一个 kubeconfig 文件列表。对于 Linux 和 Mac 系统，列表使用冒号将文件名进行分隔；对于 Windows 系统，则以分号分隔。环境变量 KUBECONFIG 不是必需的，如果它不存在，kubectl 就使用默认的 kubeconfig 文件 $HOME/.kube/config。&lt;/p&gt;

&lt;p&gt;如果环境变量 KUBECONFIG 存在，那么 kubectl 使用的有效配置，是环境变量 KUBECONFIG 中列出的所有文件融合之后的结果&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;生成&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;手动编辑config文件非常麻烦，kubectl config子命令提供了大部分的参数自动填充kubeconfig文件，分别对应set-cluster、set-credentials、set-context，相对应的有get-clusters、get-contexts以及delete-cluster、delete-context，目前没有对应credential get和delete操作,只能手动编辑kubeconfig文件。&lt;/p&gt;

&lt;p&gt;kubeconfig生成有三步&lt;/p&gt;

&lt;p&gt;1、设置集群信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes   --certificate-authority=/etc/kubernetes/ssl/ca.pem   --embed-certs=true   --server=${KUBE_APISERVER}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群参数主要设置了所需要访问的集群的信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、set-cluster设置了需要访问的集群，如上为kubernetes；
2、--certificate-authority设置了该集群的公钥；
3、--embed-certs为true表示将--certificate-authority证书写入到kubeconfig中；
4、--server则表示该集群的kube-apiserver地址。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、设置用户信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-credentials admin   --client-certificate=/etc/kubernetes/ssl/admin.pem   --embed-certs=true   --client-key=/etc/kubernetes/ssl/admin-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用户参数主要设置用户的相关信息，主要是用户证书。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、用户名为admin，
2、证书为：/etc/kubernetes/ssl/admin.pem，
3、私钥为：/etc/kubernetes/ssl/admin-key.pem。注意客户端的证书首先要经过集群CA的签署，否则不会被集群认可。此处使用的是ca认证方式，也可以使用token认证，如kubelet的 TLS Boostrap机制下的bootstrapping使用的就是token认证方式。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、设置上下文&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-context kubernetes   --cluster=kubernetes   --user=admin #可以指定路径kubeconfig=/root/config.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上下文参数将集群参数和用户参数关联起来。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、上下文名称为kubenetes，集群为kubenetes，用户为admin，表示使用admin的用户凭证来访问kubenetes集群的default命名空间，也可以增加--namspace来指定访问的命名空间。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、设置当前使用的上下文&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置对多集群的访问&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1、定义集群、用户和上下文&lt;/p&gt;

&lt;p&gt;假设用户有两个集群，一个用于正式开发工作（development），一个用于其它临时用途（scratch）。 在 development 集群中，前端开发者在名为 frontend 的命名空间下工作， 存储开发者在名为 storage 的命名空间下工作。 在 scratch 集群中， 开发人员可能在默认命名空间下工作，也可能视情况创建附加的命名空间。 访问开发集群需要通过证书进行认证。 访问其它临时用途的集群需要通过用户名和密码进行认证。&lt;/p&gt;

&lt;p&gt;创建名为 config-exercise 的目录。 在 config-exercise 目录中，创建名为 config-demo 的文件，其内容为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Config
preferences: {}

clusters:
- cluster:
  name: development
- cluster:
  name: scratch

users:
- name: developer
- name: experimenter

contexts:
- context:
  name: dev-frontend
- context:
  name: dev-storage
- context:
  name: exp-scratch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置文件描述了集群、用户名和上下文。 config-demo 文件中含有描述两个集群、两个用户和三个上下文的框架。&lt;/p&gt;

&lt;p&gt;进入 config-exercise 目录。 输入以下命令&lt;/p&gt;

&lt;p&gt;1、将群集详细信息添加到配置文件中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo set-cluster development --server=https://1.2.3.4 --certificate-authority=fake-ca-file
kubectl config --kubeconfig=config-demo set-cluster scratch --server=https://5.6.7.8 --insecure-skip-tls-verify
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、将用户详细信息添加到配置文件中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo set-credentials developer --client-certificate=fake-cert-file --client-key=fake-key-seefile
kubectl config --kubeconfig=config-demo set-credentials experimenter --username=exp --password=some-password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、将上下文详细信息添加到配置文件中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo set-context dev-frontend --cluster=development --namespace=frontend --user=developer
kubectl config --kubeconfig=config-demo set-context dev-storage --cluster=development --namespace=storage --user=developer
kubectl config --kubeconfig=config-demo set-context exp-scratch --cluster=scratch --namespace=default --user=experimenter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开 config-demo 文件查看添加的详细信息。 也可以使用 config view 命令进行查看：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo view
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出展示了两个集群、两个用户和三个上下文：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
clusters:
- cluster:
    certificate-authority: fake-ca-file
    server: https://1.2.3.4
  name: development
- cluster:
    insecure-skip-tls-verify: true
    server: https://5.6.7.8
  name: scratch
contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: dev-frontend
- context:
    cluster: development
    namespace: storage
    user: developer
  name: dev-storage
- context:
    cluster: scratch
    namespace: default
    user: experimenter
  name: exp-scratch
current-context: &amp;quot;&amp;quot;
kind: Config
preferences: {}
users:
- name: developer
  user:
    client-certificate: fake-cert-file
    client-key: fake-key-file
- name: experimenter
  user:
    password: some-password
    username: exp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个上下文包含三部分（集群、用户和命名空间），例如， dev-frontend 上下文表明：使用 developer 用户的凭证来访问 development 集群的 frontend 命名空间。&lt;/p&gt;

&lt;p&gt;设置当前上下文：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo use-context dev-frontend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在当输入 kubectl 命令时，相应动作会应用于 dev-frontend 上下文中所列的集群和命名空间，同时，命令会使用 dev-frontend 上下文中所列用户的凭证。&lt;/p&gt;

&lt;p&gt;使用 &amp;ndash;minify 参数，来查看与当前上下文相关联的配置信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo view --minify
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果展示了 dev-frontend 上下文相关的配置信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
clusters:
- cluster:
    certificate-authority: fake-ca-file
    server: https://1.2.3.4
  name: development
contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: dev-frontend
current-context: dev-frontend
kind: Config
preferences: {}
users:
- name: developer
  user:
    client-certificate: fake-cert-file
    client-key: fake-key-file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在假设用户希望在其它临时用途集群中工作一段时间。&lt;/p&gt;

&lt;p&gt;将当前上下文更改为 exp-scratch：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo use-context exp-scratch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在用户 kubectl 下达的任何命令都将应用于 scratch 集群的默认命名空间。 同时，命令会使用 exp-scratch 上下文中所列用户的凭证。&lt;/p&gt;

&lt;p&gt;查看更新后的当前上下文 exp-scratch 相关的配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo view --minify
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，假设用户希望在 development 集群中的 storage 命名空间下工作一段时间。&lt;/p&gt;

&lt;p&gt;将当前上下文更改为 dev-storage：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo use-context dev-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看更新后的当前上下文 dev-storage 相关的配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config --kubeconfig=config-demo view --minify
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、创建第二个配置文件&lt;/p&gt;

&lt;p&gt;在 config-exercise 目录中，创建名为 config-demo-2 的文件，其中包含以下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Config
preferences: {}

contexts:
- context:
    cluster: development
    namespace: ramp
    user: developer
  name: dev-ramp-up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述配置文件定义了一个新的上下文，名为 dev-ramp-up。&lt;/p&gt;

&lt;p&gt;设置 KUBECONFIG 环境变量&lt;/p&gt;

&lt;p&gt;查看是否有名为 KUBECONFIG 的环境变量。 如有，保存 KUBECONFIG 环境变量当前的值，以便稍后恢复。 例如，在 Linux 中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export  KUBECONFIG_SAVED=$KUBECONFIG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;KUBECONFIG 环境变量是配置文件路径的列表，该列表在 Linux 和 Mac 中以冒号分隔，在 Windows 中以分号分隔。 如果有 KUBECONFIG 环境变量，请熟悉列表中的配置文件。&lt;/p&gt;

&lt;p&gt;临时添加两条路径到 KUBECONFIG 环境变量中。 例如，在 Linux 中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export  KUBECONFIG=$KUBECONFIG:config-demo:config-demo-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 config-exercise 目录中输入以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config view
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出展示了 KUBECONFIG 环境变量中所列举的所有文件合并后的信息。 特别地， 注意合并信息中包含来自 config-demo-2 文件的 dev-ramp-up 上下文和来自 config-demo 文件的三个上下文：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: dev-frontend
- context:
    cluster: development
    namespace: ramp
    user: developer
  name: dev-ramp-up
- context:
    cluster: development
    namespace: storage
    user: developer
  name: dev-storage
- context:
    cluster: scratch
    namespace: default
    user: experimenter
  name: exp-scratch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多关于 kubeconfig 文件如何合并的信息，请参考 使用 kubeconfig 文件组织集群访问&lt;/p&gt;

&lt;p&gt;3、探索 $HOME/.kube 目录&lt;/p&gt;

&lt;p&gt;如果用户已经拥有一个集群，可以使用 kubectl 与集群进行交互。 那么很可能在 $HOME/.kube 目录下有一个名为 config 的文件。&lt;/p&gt;

&lt;p&gt;进入 $HOME/.kube 目录， 看看那里有什么文件。 通常会有一个名为 config 的文件，目录中可能还有其他配置文件。 请简单地熟悉这些文件的内容。&lt;/p&gt;

&lt;p&gt;4、将 $HOME/.kube/config 追加到 KUBECONFIG 环境变量中&lt;/p&gt;

&lt;p&gt;如果有 $HOME/.kube/config 文件，并且还未列在 KUBECONFIG 环境变量中， 那么现在将它追加到 KUBECONFIG 环境变量中。 例如，在 Linux 中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;检查配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过获取集群状态检查 kubectl 是否被正确配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl cluster-info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果您看到一个 URL 被返回，那么 kubectl 已经被正确配置，能够正常访问您的 Kubernetes 集群。&lt;/p&gt;

&lt;p&gt;如果您看到类似以下的信息被返回，那么 kubectl 没有被正确配置，无法正常访问您的 Kubernetes 集群。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The connection to the server &amp;lt;server-name:port&amp;gt; was refused - did you specify the right host or port?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 kubectl cluster-info 能够返回 url 响应，但您无法访问您的集群，可以使用下面的命令检查配置是否正确：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl cluster-info dump
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;启用 shell 自动补全功能&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;kubectl 支持自动补全功能，可以节省大量输入！&lt;/p&gt;

&lt;p&gt;自动补全脚本由 kubectl 产生，您仅需要在您的 shell 配置文件中调用即可。&lt;/p&gt;

&lt;p&gt;以下仅提供了使用命令补全的常用示例，更多详细信息，请查阅 kubectl completion -h 帮助命令的输出。&lt;/p&gt;

&lt;p&gt;1、Linux 系统，使用 bash&lt;/p&gt;

&lt;p&gt;在 CentOS Linux系统上，您可能需要安装默认情况下未安装的 bash-completion 软件包。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install bash-completion -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行 source &amp;lt;(kubectl completion bash) 命令在您目前正在运行的 shell 中开启 kubectl 自动补全功能。&lt;/p&gt;

&lt;p&gt;可以将上述命令添加到 shell 配置文件中，这样在今后运行的 shell 中将自动开启 kubectl 自动补全：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;quot;source &amp;lt;(kubectl completion bash)&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、macOS 系统，使用 bash&lt;/p&gt;

&lt;p&gt;macOS 系统需要先通过 Homebrew 安装 bash-completion：&lt;/p&gt;

&lt;p&gt;如果您运行的是 macOS 自带的 Bash 3.2，请运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install bash-completion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果您使用的是 Bash 4.1+，请运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install bash-completion@2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请根据 Homebrew 输出的”注意事项（caveats）”部分的内容将 bash-completion 的路径添加到本地 .bashrc 文件中。&lt;/p&gt;

&lt;p&gt;如果您是按照 Homebrew 指示中的步骤安装的 kubectl，那么无需其他配置，kubectl 的自动补全功能已经被启用。&lt;/p&gt;

&lt;p&gt;如果您是手工下载并安装的 kubectl，那么您需要将 kubectl 自动补全添加到 bash-completion：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl completion bash &amp;gt; $(brew --prefix)/etc/bash_completion.d/kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 Homebrew 项目与 Kubernetes 无关，所以并不能保证 bash-completion 总能够支持 kubectl 的自动补全功能。&lt;/p&gt;

&lt;p&gt;3、macOS 系统，使用 Zsh&lt;/p&gt;

&lt;p&gt;如果您使用的是 zsh,请编辑 ~/.zshrc 文件并添加以下代码以启用 kubectl 自动补全功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if [ $commands[kubectl] ]; then
  source &amp;lt;(kubectl completion zsh)
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果您使用的是 Oh-My-Zsh，请编辑 ~/.zshrc 文件并更新 plugins= 行以包含 kubectl 插件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plugins=(kubectl)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;使用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;kubectl的原理是将输入的转化为REST API来调用，将返回结果输出。只是对REST API的一种封装，可以说是apiserver的一个客户端&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/kubectl.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;用法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl [command] [options]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要命令详解&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl annotate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;更新某个资源的注解&lt;/p&gt;

&lt;p&gt;支持的资源包括但不限于（大小写不限）：pods (po)、services (svc)、 replicationcontrollers (rc)、nodes (no)、events (ev)、componentstatuses (cs)、 limitranges (limits)、persistentvolumes (pv)、persistentvolumeclaims (pvc)、 resourcequotas (quota)和secrets。&lt;/p&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 更新pod “foo”，设置其注解description的值为my frontend。
# 如果同一个注解被赋值了多次，只保存最后一次设置的值。
$ kubectl annotate pods foo description=&#39;my frontend&#39;

# 更新“pod.json”文件中type和name字段指定的pod的注解。
$ kubectl annotate -f pod.json description=&#39;my frontend&#39;

# 更新pod “foo”，设置其注解description的值为my frontend running nginx，已有的值将被覆盖。
$ kubectl annotate --overwrite pods foo description=&#39;my frontend running nginx&#39;

# 更新同一namespace下所有的pod。
$ kubectl annotate pods --all description=&#39;my frontend running nginx&#39;

# 仅当pod “foo”当前版本为1时，更新其注解
$ kubectl annotate pods foo description=&#39;my frontend running nginx&#39; --resource-version=1

# 更新pod “foo”，删除其注解description。
# 不需要--override选项。
$ kubectl annotate pods foo description-
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl api-versions&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;以“组/版本”的格式输出服务端支持的API版本。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl apply&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过文件名或控制台输入，对资源进行配置。&lt;/p&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 将pod.json中的配置应用到pod,当然yaml也可以
$ kubectl apply -f ./pod.json

# 将控制台输入的JSON配置应用到Pod
$ cat pod.json | kubectl apply -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl attach&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;连接到一个正在运行的容器。&lt;/p&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 获取正在运行中的pod 123456-7890的输出，默认连接到第一个容器
$ kubectl attach 123456-7890

# 获取pod 123456-7890中ruby-container的输出
$ kubectl attach 123456-7890 -c ruby-container date

# 切换到终端模式，将控制台输入发送到pod 123456-7890的ruby-container的“bash”命令，并将其输出到控制台/
# 错误控制台的信息发送回客户端。
$ kubectl attach 123456-7890 -c ruby-container -i -t
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl cluster-info&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;显示集群信息。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl config&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;修改kubeconfig配置文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl create&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过文件名或控制台输入，创建资源。&lt;/p&gt;

&lt;p&gt;接受JSON和YAML格式的描述文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f FILENAME
kubectl create -f rc-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl delete&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过文件名、控制台输入、资源名或者label selector删除资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete -f rc-nginx.yaml
kubectl delete po rc-nginx-btv4j
kubectl delete po -lapp=nginx-2
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl describe&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;输出指定的一个/多个资源的详细信息。&lt;/p&gt;

&lt;p&gt;支持的资源包括但不限于（大小写不限）：pods (po)、services (svc)、 replicationcontrollers (rc)、nodes (no)、events (ev)、componentstatuses (cs)、 limitranges (limits)、persistentvolumes (pv)、persistentvolumeclaims (pvc)、 resourcequotas (quota)和secrets。&lt;/p&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 描述一个node
$ kubectl describe nodes kubernetes-minion-emt8.c.myproject.internal

# 描述一个pod
$ kubectl describe pods/nginx

# 描述pod.json中的资源类型和名称指定的pod
$ kubectl describe -f pod.json

# 描述所有的pod
$ kubectl describe pods

# 描述所有包含label name=myLabel的pod
$ kubectl describe po -l name=myLabel

# 描述所有被replication controller “frontend”管理的pod（rc创建的pod都以rc的名字作为前缀）
$ kubectl describe pods frontend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node详解&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@xgpccsit02m010243129129 ~]# kubectl get nodes
NAME                                  STATUS   ROLES         AGE    VERSION
xgpccsit02m010243129129.sncloud.com   Ready    master.node   515d   v1.14.8-sn.3
xgpccsit02m010243129130.sncloud.com   Ready    master.node   515d   v1.14.8-sn.3
xgpccsit02m010243129131.sncloud.com   Ready    master.node   515d   v1.14.8-sn.3
xgpccsit02n010243129139.sncloud.com   Ready    node          305d   v1.14.8-sn.3
xgpccsit02n010243129140.sncloud.com   Ready    node          509d   v1.14.8-sn.3
xgpccsit02n010243129142.sncloud.com   Ready    node          504d   v1.14.8-sn.3
xgpccsit02n010243129144.sncloud.com   Ready    node          261d   v1.14.8-sn.3
xgpccsit02n010243129156.sncloud.com   Ready    node          212d   v1.14.8-sn.3
xgpccsit02n010243129158.sncloud.com   Ready    node          369d   v1.14.8-sn.3
xgpccsit02n010243129163.sncloud.com   Ready    node          74d    v1.14.8-sn.3
xgpccsit02n010243129164.sncloud.com   Ready    node          86d    v1.14.8-sn.3
xgpccsit02n010243129167.sncloud.com   Ready    node          40d    v1.14.8-sn.3
xgpccsit02n010243129170.sncloud.com   Ready    node          32d    v1.14.8-sn.3
xgpccsit02n010243133002.sncloud.com   Ready    node          152d   v1.14.8-sn.3
[root@xgpccsit02m010243129129 ~]# kubectl describe node xgpccsit02n010243133002.sncloud.com
Name:               xgpccsit02n010243133002.sncloud.com
Roles:              node
Labels:             APP=true
                    WEB=true
                    beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    dragonflyTest=true
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=xgpccsit02n010243133002.sncloud.com
                    kubernetes.io/os=linux
                    netType=B2C
                    node-network-driver=ovs
                    node-role.kubernetes.io/node=
Annotations:        csi.volume.kubernetes.io/nodeid: {&amp;quot;ossplugin.csi.sncloud.com&amp;quot;:&amp;quot;10.243.133.2&amp;quot;}
                    node.alpha.kubernetes.io/ttl: 0
                    sncloud.com/cpu-policy: none
                    sncloud.com/diskIOPSCapacity: 1000000
                    sncloud.com/gpu.usage: null
                    sncloud.com/gpu.used: 0
                    sncloud.com/networkBandwidthCapacity: 2000000000
CreationTimestamp:  Wed, 11 Dec 2019 17:20:26 +0800
Taints:             &amp;lt;none&amp;gt;
Unschedulable:      false
Conditions:
  Type                     Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                     ------  -----------------                 ------------------                ------                       -------
  ServiceReady             False   Tue, 12 May 2020 10:16:14 +0800   Tue, 12 May 2020 10:16:14 +0800   NodeServiceReadyChange       [cni status:unknown,]
  FrequentLxcfsRestart     False   Tue, 12 May 2020 10:16:30 +0800   Wed, 06 May 2020 18:26:52 +0800   NoFrequentLxcfsRestart       lxcfs is functioning properly
  FrequentKubeletRestart   False   Tue, 12 May 2020 10:16:30 +0800   Wed, 06 May 2020 18:26:52 +0800   NoFrequentKubeletRestart     kubelet is functioning properly
  FrequentDockerRestart    False   Tue, 12 May 2020 10:16:30 +0800   Wed, 06 May 2020 18:26:52 +0800   NoFrequentDockerRestart      docker is functioning properly
  OrphanedPodFileExist     False   Fri, 27 Mar 2020 01:53:49 +0800   Tue, 24 Mar 2020 12:10:13 +0800   NoOrphanedPodFileExist       OrphanedPod is not exist
  MemoryPressure           False   Tue, 12 May 2020 10:17:09 +0800   Wed, 08 Apr 2020 18:03:40 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure             False   Tue, 12 May 2020 10:17:09 +0800   Wed, 08 Apr 2020 18:03:40 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure              False   Tue, 12 May 2020 10:17:09 +0800   Wed, 08 Apr 2020 18:03:40 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                    True    Tue, 12 May 2020 10:17:09 +0800   Thu, 07 May 2020 09:57:01 +0800   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  10.243.133.2
  Hostname:    xgpccsit02n010243133002.sncloud.com
Capacity:
 cpu:                24
 ephemeral-storage:  51877124Ki
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             263852704Ki
 pods:               110
Allocatable:
 cpu:                22
 ephemeral-storage:  47809957400
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             261653152Ki
 pods:               110
System Info:
 Machine ID:                 6328e225a9fe47d1bac0d3a6004c0ada
 System UUID:                6328e225a9fe47d1bac0d3a6004c0ada
 Boot ID:                    7b9342ff-16ae-4bcd-8631-dc367652de52
 Kernel Version:             4.18.0-80.11.1.el7.centos.sn11.x86_64
 OS Image:                   CentOS Linux 7 (Core)
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://18.9.9
 Kubelet Version:            v1.14.8-sn.3
 Kube-Proxy Version:         v1.14.8-sn.3
Non-terminated Pods:         (10 in total)
  Namespace                  Name                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                  ----                           ------------  ----------  ---------------  -------------  ---
  kube-system                cadvisor-proxy-5t9jk           100m (0%)     500m (2%)   100Mi (0%)       250Mi (0%)     18d
  kube-system                csi-ossplugin-gpvz8            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d23h
  kube-system                dfclient-cgzdm                 1 (4%)        2 (9%)      2Gi (0%)         3Gi (1%)       12d
  kube-system                filebeat-8ckl9                 3750m (17%)   4 (18%)     2273Mi (0%)      2660Mi (1%)    19d
  kube-system                machine-worker-nx2gc           100m (0%)     100m (0%)   100Mi (0%)       100Mi (0%)     126d
  kube-system                node-exporter-m8jc8            100m (0%)     200m (0%)   128Mi (0%)       256Mi (0%)     3d16h
  kube-system                node-problem-detector-qcwln    25m (0%)      200m (0%)   100Mi (0%)       100Mi (0%)     5d15h
  kube-system                preheat-worker-2sn7g           200m (0%)     500m (2%)   128Mi (0%)       512Mi (0%)     46d
  kube-system                voyage-agent-qb9rd             1100m (5%)    1100m (5%)  1124Mi (0%)      1124Mi (0%)    4d1h
  kube-system                voyage-openvswitch-ccdg8       1 (4%)        4 (18%)     1Gi (0%)         4Gi (1%)       113d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                7375m (33%)  12600m (57%)
  memory             7025Mi (2%)  12170Mi (4%)
  ephemeral-storage  0 (0%)       0 (0%)
Events:              &amp;lt;none&amp;gt;


解析

1、node的基本信息：名称，标签，创建时间
2、node当前运行的状态，node启动后会做一些自检工作，比如磁盘满了，如果满了就标注OutOfDisk=True，否则就继续检查内存是够不足，最后一切正常，就Ready=True，可以在上面查看，这种情况代表node处于健康状态，可以在上创建pod了。
3、node主机名和主机地址
4、node上的资源总量，比如cpu，内存，最大可调度pod的数量
5、node可分配的资源
6、本机系统信息：UUID，版本等
7、当前正在运行的pod的概要信息
8、已分配的资源使用情况
9、node相关的事件event
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl replace&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;更新替换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl replace -f rc-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl patch&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果一个容器已经在运行，这时需要对一些容器属性进行修改，又不想删除容器，或不方便通过replace的方式进行更新。kubernetes还提供了一种在容器运行时，直接对容器进行修改的方式，就是patch命令.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl patch pod rc-nginx-2-kpiqt -p &#39;{&amp;quot;metadata&amp;quot;:{&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;nginx-3&amp;quot;}}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl edit&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;编辑服务端的资源。&lt;/p&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  # 编辑名为“docker-registry”的service
  $ kubectl edit svc/docker-registry

  # 使用一个不同的编辑器
  $ KUBE_EDITOR=&amp;quot;nano&amp;quot; kubectl edit svc/docker-registry

  # 编辑名为“docker-registry”的service，使用JSON格式、v1 API版本
  $ kubectl edit svc/docker-registry --output-version=v1 -o json
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl exec&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在容器内部执行命令。&lt;/p&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 默认在pod 123456-7890的第一个容器中运行“date”并获取输出
$ kubectl exec 123456-7890 date

# 在pod 123456-7890的容器ruby-container中运行“date”并获取输出
$ kubectl exec 123456-7890 -c ruby-container date

# 切换到终端模式，将控制台输入发送到pod 123456-7890的ruby-container的“bash”命令，并将其输出到控制台/
# 错误控制台的信息发送回客户端。
$ kubectl exec 123456-7890 -c ruby-container -i -t -- bash -il

kubectl exec filebeat-27 -c container -n namespaces sh
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl logs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;输出pod中一个容器的日志。&lt;/p&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 返回仅包含一个容器的pod nginx的日志快照
$ kubectl logs nginx

# 返回pod ruby中已经停止的容器web-1的日志快照
$ kubectl logs -p -c ruby web-1

# 持续输出pod ruby中的容器web-1的日志
$ kubectl logs -f -c ruby web-1

# 仅输出pod nginx中最近的20条日志
$ kubectl logs --tail=20 nginx

# 输出pod nginx中最近一小时内产生的所有日志
$ kubectl logs --since=1h nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl version&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;输出服务端和客户端的版本信息。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl get&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;获取信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get po
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl rolling-update&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;滚动更新.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl rolling-update rc-nginx-2 -f rc-nginx.yaml，
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个还提供如果在升级过程中，发现有问题还可以中途停止update，并回滚到前面版本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl rolling-update rc-nginx-2 —rollback
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl scale&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;扩容缩容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl scale rc rc-nginx-3 —replicas=4
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl cp&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将文件直接拷进容器内&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl cp 文件 pod：路径
kubectl cp ./test.war logtestjbossforone-7b89dd5c9-297pf:/opt/wildfly/standalone/deployments
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;kubectl help&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;help 帮助命令，可以查找所有的命令，在我们不会用的适合，要学会使用这个命令。&lt;/p&gt;

&lt;p&gt;###&lt;/p&gt;

&lt;p&gt;kubectl默认会从$HOME/.kube目录下查找文件名为 config 的文件&lt;/p&gt;

&lt;h2 id=&#34;yaml文件详解&#34;&gt;yaml文件详解&lt;/h2&gt;

&lt;p&gt;其实最后都是转化为api对象，所以符合k8s设计的顶级api对象，所以直接看&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-api/&#34;&gt;顶级api对象的组成&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;rc实例详解&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1 #指定api版本，此值必须在kubectl apiversion中
kind: ReplicationController #指定创建资源的角色/类型
metadata: #资源的元数据/属性
  name: test-rc #资源的名字，在同一个namespace中必须唯一
  labels: #设定资源的标签，详情请见http://blog.csdn.net/liyingke112/article/details/77482384
  k8s-app: apache
    software: apache
    project: test
    app: test-rc
    version: v1
  annotations:            #自定义注解列表
    - name: String        #自定义注解名字
spec:
  replicas: 2 #副本数量2
  selector: #RC通过spec.selector来筛选要控制的Pod
    software: apache
    project: test
    app: test-rc
    version: v1
    name: test-rc
  template: #这里Pod的定义
    metadata:
      labels: #Pod的label，可以看到这个label与spec.selector相同
        software: apache
        project: test
        app: test-rc
        version: v1
        name: test-rc
    spec:#specification of the resource content 指定该资源的内容
      restartPolicy: Always #表明该容器一直运行，默认k8s的策略，在此容器退出后，会立即创建一个相同的容器
      nodeSelector:     #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1
        zone: node1
      containers:
      - name: web04-pod #容器的名字
        image: web:apache #容器使用的镜像地址
        imagePullPolicy: Never #三个选择Always、Never、IfNotPresent，每次启动时检查和更新（从registery）images的策略，
                               # Always，每次都检查,重远程拉去
                               # Never，每次都不检查（不管本地是否有）
                               # IfNotPresent，如果本地有就不检查，如果没有就拉取
        command: [&#39;sh&#39;] #启动容器的运行命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT
        args: [&amp;quot;$(str)&amp;quot;] #启动容器的命令参数，对应Dockerfile中CMD参数
        env: #指定容器中的环境变量
        - name: str #变量的名字
          value: &amp;quot;/etc/run.sh&amp;quot; #变量的值
        resources: #资源管理，请求请见http://blog.csdn.net/liyingke112/article/details/77452630
          requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行
            cpu: 0.1 #CPU资源（核数），两种方式，浮点数或者是整数+m，0.1=100m，最少值为0.001核（1m）
            memory: 32Mi #内存使用量
          limits: #资源限制
            cpu: 0.5
            memory: 32Mi
        ports:
        - containerPort: 80 #容器开发对外的端口
          name: httpd  #名称
          protocol: TCP
        livenessProbe: #pod内容器健康检查的设置，详情请见http://blog.csdn.net/liyingke112/article/details/77531584
          httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常
            path: / #URI地址
            port: 80
            #host: 127.0.0.1 #主机地址
            scheme: HTTP
          initialDelaySeconds: 180 #表明第一次检测在容器启动后多长时间后开始
          timeoutSeconds: 5 #检测的超时时间
          periodSeconds: 15  #检查间隔时间
          #也可以用这种方法
          #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常
          #  command:
          #    - cat
          #    - /tmp/health
          #也可以用这种方法
          #tcpSocket: //通过tcpSocket检查健康
          #  port: number
        lifecycle: #生命周期管理
          postStart: #容器运行之前运行的任务
            exec:
              command:
                - &#39;sh&#39;
                - &#39;yum upgrade -y&#39;
          preStop:#容器关闭之前运行的任务
            exec:
              command: [&#39;service httpd stop&#39;]
        volumeMounts:  #详情请见http://blog.csdn.net/liyingke112/article/details/76577520
        - name: volume #挂载设备的名字，与volumes[*].name 需要对应
          mountPath: /data #挂载到容器的某个路径下
          readOnly: True
  volumes: #定义一组挂载设备
  - name: volume #定义一个挂载设备的名字
    #meptyDir: {}
    hostPath:
      path: /opt #挂载设备类型为hostPath，路径为宿主机下的/opt,这里设备类型支持很多种
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;迁移&#34;&gt;迁移&lt;/h2&gt;

&lt;p&gt;迁移现有应用步骤说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将原有应用拆解为服务&lt;/li&gt;
&lt;li&gt;容器化、制作镜像&lt;/li&gt;
&lt;li&gt;准备应用配置文件&lt;/li&gt;
&lt;li&gt;准备kubernetes YAML文件&lt;/li&gt;
&lt;li&gt;编写bootstarp脚本&lt;/li&gt;
&lt;li&gt;创建ConfigMaps&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上云的步骤&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;搭建基础平台，提供能力&lt;/li&gt;
&lt;li&gt;将单体应用制作镜像云上运行&lt;/li&gt;
&lt;li&gt;制定规范&lt;/li&gt;
&lt;li&gt;将中间件应用上云&lt;/li&gt;
&lt;li&gt;最后在以上能力的基础上建设微服务架构，将业务上云&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;管理&#34;&gt;管理&lt;/h2&gt;

&lt;p&gt;1、陈列式&lt;/p&gt;

&lt;p&gt;就是我们正常使用的命令行，在查，创建，删除的时候，比较好操作，但是修改就比较麻烦，使用声明式的文件方式比较好操作&lt;/p&gt;

&lt;p&gt;2、声明式&lt;/p&gt;

&lt;p&gt;就是我们使用的yaml或者json格式，修改文件就简单了。&lt;/p&gt;

&lt;p&gt;在线修改&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl edit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;离线修改&lt;/p&gt;

&lt;p&gt;3、GUI&lt;/p&gt;

&lt;p&gt;就是我们dashboard，目前1.16后的dashboard还不能正常的运行。&lt;/p&gt;

&lt;h2 id=&#34;问题处理&#34;&gt;问题处理&lt;/h2&gt;

&lt;p&gt;1、创建Nginx Pod过程中报如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#kubectlcreate -f nginx-pod.yaml

Error from server: error when creating &amp;quot;nginx-pod.yaml&amp;quot;: Pod &amp;quot;nginx&amp;quot; is forbidden: no API token found for service account default/default, retry after the token is automatically created and added to the service account
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决方法：&lt;/p&gt;

&lt;p&gt;修改/etc/kubernetes/apiserver文件中KUBE_ADMISSION_CONTROL参数。&lt;/p&gt;

&lt;p&gt;修改前：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBE_ADMISSION_CONTROL=&amp;quot;--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;去掉“ServiceAccount”选项。&lt;/p&gt;

&lt;p&gt;2、创建Pod过程中，显示异常，通过查看日志/var/log/messages，有以下报错信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Nov 26 21:52:06 localhost kube-apiserver: E1126 21:52:06.697708    1963 server.go:454] Unable to generate self signed cert: open /var/run/kubernetes/apiserver.crt: permission denied
Nov 26 21:52:06 localhost kube-apiserver: E1126 21:52:06.697818    1963 server.go:464] Unable to listen for secure (open /var/run/kubernetes/apiserver.crt: no such file or directory); will try again.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决方法有两种：&lt;/p&gt;

&lt;p&gt;第一种方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/kube-apiserver.service

[Service]
PermissionsStartOnly=true
ExecStartPre=-/usr/bin/mkdir /var/run/kubernetes
ExecStartPre=/usr/bin/chown -R kube:kube /var/run/kubernetes/

# systemctl daemon-reload
# systemctl restart kube-apiserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二种方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# vim /etc/kubernetes/apiserver

KUBE_API_ARGS=&amp;quot;--secure-port=0&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在KUBE_API_ARGS加上&amp;ndash;secure-port=0参数。&lt;/p&gt;

&lt;p&gt;原因如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--secure-port=6443: The port on which to serve HTTPS with authentication and authorization. If 0, don&#39;t serve HTTPS at all.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;高级特性&#34;&gt;高级特性&lt;/h1&gt;

&lt;h2 id=&#34;配置和可扩展&#34;&gt;配置和可扩展&lt;/h2&gt;

&lt;p&gt;Kubernetes 是高度可配置和可扩展的。因此，极少需要分发或提交补丁代码给 Kubernetes 项目。&lt;/p&gt;

&lt;p&gt;配置 一般就是更改标志参数、本地配置文件或 API 资源。这个可以参考官方文档各种组件的启动参数的&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/&#34;&gt;配置&lt;/a&gt;，不用详细说明，主要还是高度可扩展的这种设计模式，使得k8s十分灵活。&lt;/p&gt;

&lt;p&gt;整个k8s集群的基本各个点都是支持可扩展的，如何扩展，我们可以看官方这幅图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/k8s/Expand&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户通常使用 kubectl 与 Kubernetes API 进行交互。kubectl 插件扩展了 kubectl 二进制程序。它们只影响个人用户的本地环境，因此不能执行站点范围的策略。&lt;/li&gt;
&lt;li&gt;apiserver 处理所有请求。apiserver 中的几种类型的扩展点允许对请求进行身份认证或根据其内容对其进行阻止、编辑内容以及处理删除操作。 API 访问扩展。&lt;/li&gt;
&lt;li&gt;apiserver 提供各种内置的资源种类 ，如 pods，由 Kubernetes 项目定义，不能更改。但是可以添加您自己定义的资源或其他项目已定义的资源来进行扩展。自定义资源通常与 API 访问扩展一起使用。&lt;/li&gt;
&lt;li&gt;Kubernetes 调度器决定将 Pod 放置到哪个节点。有几种方法可以扩展调度器。调度器可扩展。&lt;/li&gt;
&lt;li&gt;Kubernetes 的大部分行为都是由称为控制器的程序实现的。自定义控制器通常与自定义资源一起使用来完成扩展。&lt;/li&gt;
&lt;li&gt;kubelet 在主机上运行，并帮助 pod 看起来就像在集群网络上拥有自己的 IP 的虚拟服务器。网络插件让您可以实现不同的 pod 网络的扩展。&lt;/li&gt;
&lt;li&gt;kubelet 也挂载和卸载容器的卷。新的存储类型可以通过存储插件支持扩展。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;kubectl-插件扩展&#34;&gt;kubectl 插件扩展&lt;/h3&gt;

&lt;p&gt;kubectl 插件在 v1.8.0 版本中正式作为 alpha 特性引入。它们已经在 v1.12.0 版本中工作，以支持更广泛的用例，建议使用 1.12.0 或更高版本的 &lt;code&gt;kubectl&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;1、安装 kubectl 插件&lt;/p&gt;

&lt;p&gt;插件只不过是一个独立的可执行文件，名称以 kubectl- 开头，Kubernetes 不提供包管理器或任何类似于安装或更新插件的东西，所以我们只要将此可执行文件放置到系统路径下就可以&lt;/p&gt;

&lt;p&gt;目前无法创建覆盖现有 kubectl 命令的插件，例如，创建一个插件 kubectl-version 将导致该插件永远不会被执行，因为现有的 kubectl-version 命令总是优先于它执行。&lt;/p&gt;

&lt;p&gt;2、发现插件&lt;/p&gt;

&lt;p&gt;kubectl 提供一个命令 kubectl plugin list，用于搜索系统路径查找有效的插件可执行文件。 执行此命令将遍历路径中的所有文件。任何以 kubectl- 开头的可执行文件都将在这个命令的输出中以它们在路径中出现的顺序显示。 任何以 kubectl- 开头的文件如果不可执行，都将包含一个警告。 对于任何相同的有效插件文件，都将包含一个警告。&lt;/p&gt;

&lt;p&gt;3、编写 kubectl 插件&lt;/p&gt;

&lt;p&gt;你可以用任何编程语言或脚本编写插件，允许您编写命令行命令，最总就是一个二进制文件，不需要安装插件或预加载，直接执行即可，比如一个插件想要提供一个新的命令 kubectl foo，它将被简单地命名为 kubectl-foo&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

# optional argument handling
if [[ &amp;quot;$1&amp;quot; == &amp;quot;version&amp;quot; ]]
then
    echo &amp;quot;1.0.0&amp;quot;
    exit 0
fi

# optional argument handling
if [[ &amp;quot;$1&amp;quot; == &amp;quot;config&amp;quot; ]]
then
    echo $KUBECONFIG
    exit 0
fi

echo &amp;quot;I am a plugin named kubectl-foo&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、使用插件&lt;/p&gt;

&lt;p&gt;要使用上面的插件，只需使其可执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo chmod +x ./kubectl-foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并将它放在你的路径中的任何地方：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo mv ./kubectl-foo /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你现在可以调用你的插件作为 kubectl 命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl foo
I am a plugin named kubectl-foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所有参数和标记按原样传递给可执行文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl foo version
1.0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、命名规则&lt;/p&gt;

&lt;p&gt;虽然 kubectl 插件机制在插件文件名中使用破折号（-）分隔插件处理的子命令序列，但是仍然可以通过在文件名中使用下划线（-）来创建命令行中包含破折号的插件命令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# create a plugin containing an underscore in its filename
echo -e &#39;#!/bin/bash\n\necho &amp;quot;I am a plugin with a dash in my name&amp;quot;&#39; &amp;gt; ./kubectl-foo_bar
sudo chmod +x ./kubectl-foo_bar

# move the plugin into your PATH
sudo mv ./kubectl-foo_bar /usr/local/bin

# our plugin can now be invoked from `kubectl` like so:
kubectl foo-bar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于插件文件名而言还有另一种弊端，给定用户路径中的两个插件 kubectl-foo-bar 和 kubectl-foo-bar-baz ，kubectl 插件机制总是为给定的用户命令选择尽可能长的插件名称。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# for a given kubectl command, the plugin with the longest possible filename will always be preferred
kubectl foo bar baz
Plugin kubectl-foo-bar-baz is executed
kubectl foo bar
Plugin kubectl-foo-bar is executed
kubectl foo bar baz buz
Plugin kubectl-foo-bar-baz is executed, with &amp;quot;buz&amp;quot; as its first argument
kubectl foo bar buz
Plugin kubectl-foo-bar is executed, with &amp;quot;buz&amp;quot; as its first argument
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;custom-resource-自定义资源&#34;&gt;custom resource（自定义资源）&lt;/h3&gt;

&lt;p&gt;自定义资源是k8s api的扩展。&lt;/p&gt;

&lt;p&gt;资源就是api对象的一种，比如pod就是一种资源，自定义就是自己定义一种这样的在原生集群中没有的类型。&lt;/p&gt;

&lt;p&gt;自定义资源只是一种数据结构对象，只有结合控制器才能提供真正的声明式api。一个声明式API 允许你声明或指定的资源的理想状态，控制器将结构化数据同步到为用户所需状态的记录，并持续保持该状态。&lt;/p&gt;

&lt;p&gt;声明式api和命令式api&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Declarative（声明式设计）指的是这么一种软件设计理念和做法：我们向一个工具描述我们想要让一个事物达到的目标状态，由这个工具自己内部去figure out如何令这个事物达到目标状态。&lt;/li&gt;
&lt;li&gt;Imperative（命令式设计）模式中，我们描述的是一系列的动作。这一系列的动作如果被正确的顺利执行，最终结果是这个事物达到了我们期望的目标状态的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes提供了两种向集群添加自定义资源的方法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-api/#crd-自定义资源类型&#34;&gt;CRD&lt;/a&gt;很简单，无需任何编程即可通过CustomResourceDefinition API资源类型进行创建。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-api/#kubernetes-aggregated-api-servers&#34;&gt;API聚合&lt;/a&gt;需要编程，但可以更好地控制API行为，例如如何存储数据以及在API版本之间进行转换。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CRD更易于使用，聚合的API更灵活。&lt;/p&gt;

&lt;p&gt;我们在来聊聊api group 和 api version&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl api-versions
admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1beta1
apiextensions.k8s.io/v1
apiextensions.k8s.io/v1beta1
apiregistration.k8s.io/v1
apiregistration.k8s.io/v1beta1
apps.kruise.io/v1alpha1
apps/v1
authentication.k8s.io/v1
authentication.k8s.io/v1beta1
authorization.k8s.io/v1
authorization.k8s.io/v1beta1
autoscaling/v1
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1beta1
coordination.k8s.io/v1
coordination.k8s.io/v1beta1
custom.metrics.k8s.io/v1beta1
discovery.k8s.io/v1beta1
events.k8s.io/v1beta1
extensions/v1beta1
metrics.k8s.io/v1beta1
monitoring.coreos.com/v1
networking.k8s.io/v1
networking.k8s.io/v1beta1
node.k8s.io/v1beta1
policy/v1beta1
rbac.authorization.k8s.io/v1
rbac.authorization.k8s.io/v1beta1
scheduling.k8s.io/v1
scheduling.k8s.io/v1beta1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个 admissionregistration.k8s.io/v1beta1 中，admissionregistration.k8s.io 是 api group，v1beta1 表示它的版本。所以api都是有APIgroup/apiversion组成的，如果 api group 为空表示核心 api。&lt;/p&gt;

&lt;p&gt;每个api其实都是通过apiservice这种类型进行注册的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get apiservice
NAME                                   SERVICE                                   AVAILABLE                  AGE
v1.                                    Local                                     True                       20d
v1.admissionregistration.k8s.io        Local                                     True                       20d
v1.apiextensions.k8s.io                Local                                     True                       20d
v1.apps                                Local                                     True                       20d
v1.authentication.k8s.io               Local                                     True                       20d
v1.authorization.k8s.io                Local                                     True                       20d
v1.autoscaling                         Local                                     True                       20d
v1.batch                               Local                                     True                       20d
v1.coordination.k8s.io                 Local                                     True                       20d
v1.monitoring.coreos.com               Local                                     True                       2d20h
v1.networking.k8s.io                   Local                                     True                       20d
v1.rbac.authorization.k8s.io           Local                                     True                       20d
v1.scheduling.k8s.io                   Local                                     True                       20d
v1.storage.k8s.io                      Local                                     True                       20d
v1alpha1.apps.kruise.io                Local                                     True                       2d20h
v1beta1.admissionregistration.k8s.io   Local                                     True                       20d
v1beta1.apiextensions.k8s.io           Local                                     True                       20d
v1beta1.authentication.k8s.io          Local                                     True                       20d
v1beta1.authorization.k8s.io           Local                                     True                       20d
v1beta1.batch                          Local                                     True                       20d
v1beta1.certificates.k8s.io            Local                                     True                       20d
v1beta1.coordination.k8s.io            Local                                     True                       20d
v1beta1.custom.metrics.k8s.io          custom-metrics/custom-metrics-apiserver   False (MissingEndpoints)   3h18m
v1beta1.discovery.k8s.io               Local                                     True                       20d
v1beta1.events.k8s.io                  Local                                     True                       20d
v1beta1.extensions                     Local                                     True                       20d
v1beta1.metrics.k8s.io                 monitoring/prometheus-adapter             True                       18h
v1beta1.networking.k8s.io              Local                                     True                       20d
v1beta1.node.k8s.io                    Local                                     True                       20d
v1beta1.policy                         Local                                     True                       20d
v1beta1.rbac.authorization.k8s.io      Local                                     True                       20d
v1beta1.scheduling.k8s.io              Local                                     True                       20d
v1beta1.storage.k8s.io                 Local                                     True                       20d
v2beta1.autoscaling                    Local                                     True                       20d
v2beta2.autoscaling                    Local                                     True                       20d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubernetes 的资源都是由 api group 提供的。那么如何知道哪些资源是由哪些 api group 提供的呢？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl api-resources
NAME                              SHORTNAMES   APIGROUP                       NAMESPACED   KIND
bindings                                                                      true         Binding
componentstatuses                 cs                                          false        ComponentStatus
configmaps                        cm                                          true         ConfigMap
endpoints                         ep                                          true         Endpoints
events                            ev                                          true         Event
limitranges                       limits                                      true         LimitRange
namespaces                        ns                                          false        Namespace
nodes                             no                                          false        Node
persistentvolumeclaims            pvc                                         true         PersistentVolumeClaim
persistentvolumes                 pv                                          false        PersistentVolume
pods                              po                                          true         Pod
podtemplates                                                                  true         PodTemplate
replicationcontrollers            rc                                          true         ReplicationController
resourcequotas                    quota                                       true         ResourceQuota
secrets                                                                       true         Secret
serviceaccounts                   sa                                          true         ServiceAccount
services                          svc                                         true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io           false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io         false        APIService
controllerrevisions                            apps                           true         ControllerRevision
daemonsets                        ds           apps                           true         DaemonSet
deployments                       deploy       apps                           true         Deployment
replicasets                       rs           apps                           true         ReplicaSet
statefulsets                      sts          apps                           true         StatefulSet
broadcastjobs                     bj           apps.kruise.io                 true         BroadcastJob
clonesets                         clone        apps.kruise.io                 true         CloneSet
sidecarsets                                    apps.kruise.io                 false        SidecarSet
statefulsets                      sts          apps.kruise.io                 true         StatefulSet
uniteddeployments                 ud           apps.kruise.io                 true         UnitedDeployment
tokenreviews                                   authentication.k8s.io          false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io           true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io           false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io           false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io           false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling                    true         HorizontalPodAutoscaler
cronjobs                          cj           batch                          true         CronJob
jobs                                           batch                          true         Job
certificatesigningrequests        csr          certificates.k8s.io            false        CertificateSigningRequest
leases                                         coordination.k8s.io            true         Lease
endpointslices                                 discovery.k8s.io               true         EndpointSlice
events                            ev           events.k8s.io                  true         Event
ingresses                         ing          extensions                     true         Ingress
nodes                                          metrics.k8s.io                 false        NodeMetrics
pods                                           metrics.k8s.io                 true         PodMetrics
alertmanagers                                  monitoring.coreos.com          true         Alertmanager
podmonitors                                    monitoring.coreos.com          true         PodMonitor
prometheuses                                   monitoring.coreos.com          true         Prometheus
prometheusrules                                monitoring.coreos.com          true         PrometheusRule
servicemonitors                                monitoring.coreos.com          true         ServiceMonitor
thanosrulers                                   monitoring.coreos.com          true         ThanosRuler
ingressclasses                                 networking.k8s.io              false        IngressClass
ingresses                         ing          networking.k8s.io              true         Ingress
networkpolicies                   netpol       networking.k8s.io              true         NetworkPolicy
runtimeclasses                                 node.k8s.io                    false        RuntimeClass
poddisruptionbudgets              pdb          policy                         true         PodDisruptionBudget
podsecuritypolicies               psp          policy                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io      false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io      false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io      true         RoleBinding
roles                                          rbac.authorization.k8s.io      true         Role
priorityclasses                   pc           scheduling.k8s.io              false        PriorityClass
csidrivers                                     storage.k8s.io                 false        CSIDriver
csinodes                                       storage.k8s.io                 false        CSINode
storageclasses                    sc           storage.k8s.io                 false        StorageClass
volumeattachments                              storage.k8s.io                 false        VolumeAttachment
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NAME 列就是资源名，它的功能由 APIGROUP 列的 api group 提供。SHORTNAMES 列就是这些资源的缩写了，缩写在使用 kubectl 时非常好用。同样APIGROUP 都为空，表示这些资源都是核心 api 提供的。&lt;/p&gt;

&lt;p&gt;所以对于自定义api，我们就需要用到他们的 api group 和 api version 进行注册，就可以同样的作用了。&lt;/p&gt;

&lt;p&gt;这边只是资源定义，实现还是要依赖于&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-controller/&#34;&gt;controller&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;schedulers&#34;&gt;Schedulers&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-scheduler/&#34;&gt;调度扩展&lt;/a&gt;主要是通过调度框架来实现。&lt;/p&gt;

&lt;h3 id=&#34;网络插件&#34;&gt;网络插件&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network-cni/&#34;&gt;网络扩展&lt;/a&gt;主要通过CNI接口来实现。&lt;/p&gt;

&lt;h3 id=&#34;存储插件&#34;&gt;存储插件&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network-csi/&#34;&gt;存储扩展&lt;/a&gt;主要通过CSI接口来实现。&lt;/p&gt;

&lt;h1 id=&#34;k8s原理&#34;&gt;k8s原理&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-principle/&#34;&gt;k8s原理详解&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- Distributed Lock</title>
          <link>https://kingjcy.github.io/post/distributed/distributed-lock/</link>
          <pubDate>Wed, 03 Apr 2019 19:57:56 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/distributed-lock/</guid>
          <description>&lt;p&gt;锁可以分为正常的进程内锁和分布式的进程间的锁。&lt;/p&gt;

&lt;h1 id=&#34;正常锁&#34;&gt;正常锁&lt;/h1&gt;

&lt;p&gt;我们一般所说的锁，就是指单进程多线程的锁机制。在单进程中，如果有多个线程并发访问某个某个全局资源，存在并发修改的问题。如果要避免这个问题，我们需要对资源进行同步，同步其实就是可以加一个锁来保证同一时刻只有一个线程能操作这个资源。&lt;/p&gt;

&lt;p&gt;具体的锁可以看go的&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-mutex/&#34;&gt;锁&lt;/a&gt;,当然在golang中主要是使用channel来实现进程内通信和共享。&lt;/p&gt;

&lt;h1 id=&#34;分布式锁&#34;&gt;分布式锁&lt;/h1&gt;

&lt;p&gt;涉及到分布式环境，以集群为例，就是多个实例，也就是多个进程，而且这些进程完全可能不在同一个机器上。我们知道多线程可以共享父进程的资源，包括内存。所以多线程可以看见锁，但是多进程之间无法共享资源，甚至都不在一台机器上，所以这时候分布式环境下，就需要其他的方式来让所有进程都可以知道这个锁，来控制对全局资源的并发修改。&lt;/p&gt;

&lt;p&gt;为了解决分布式的问题，我们可以把这个锁放入所有进程都可以访问的地方，比如数据库，redis，memcached或者是zookeeper。这些也是目前实现分布式锁的主要实现方式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基于数据库表实现分布式锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们可以将我们分布式要操作的资源都定义成表，然后对表进行查询数据，如果查到了没有数据，可以进行update，否则，说明该锁被其他线程持有，还没有释放&lt;/p&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;p&gt;更新之前会多一次查询，增加了数据库的操作&lt;/p&gt;

&lt;p&gt;数据库链接资源宝贵，如果并发量太大，数据库的性能有影响&lt;/p&gt;

&lt;p&gt;如果单个数据库存在单点问题，所以最好是高可用的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基于Redis实现分布式锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;通过Redis的setnx key命令，如果不存在某个key就设置值，设置成功表示获取锁。&lt;/p&gt;

&lt;p&gt;缺点：如果设置成功后，还没有释放锁，对应的业务节点就挂掉了，那么这时候锁就没有释放。其他业务节点也无法获取这个锁。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用setnx设置命令成功后，则使用expire命令设置到期时间，就算业务节点还没有释放锁就挂掉了，但是我们还是可以保证这个锁到期就会释放。&lt;/p&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;p&gt;setnx 和 expire不是原子操作，即设置了setnx还没有来得及设置到期时间，业务节点就挂了。&lt;/p&gt;

&lt;p&gt;而且key到期了，业务节点业务还没有执行完，怎么办？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用set命令 我们知道set命令格式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set key value [EX seconds] [PX milliseconds][NX|XX]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即首先可以根据这个key不存在，则设置值，即使用NX。然后可以设置到期时间，EX表示秒数，PX表示毫秒数，这个操作就是原子性的，解决了上述问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set id EX 10 NX
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeeper，memcached&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;利用Memcached的add命令。此命令是原子性操作，只有在key不存在的情况下，才能add成功，也就意味着线程得到了锁。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;利用Zookeeper的顺序临时节点，来实现分布式锁和等待队列。Zookeeper设计的初衷，就是为了实现分布式锁服务的。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;1、简单实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;time&amp;quot;

    &amp;quot;github.com/garyburd/redigo/redis&amp;quot;
)

type Lock struct {
    resource string
    token    string
    conn     redis.Conn
    timeout  int
}

func (lock *Lock) tryLock() (ok bool, err error) {
    _, err = redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(lock.timeout), &amp;quot;NX&amp;quot;))
    if err == redis.ErrNil {
        // The lock was not successful, it already exists.
        return false, nil
    }
    if err != nil {
        return false, err
    }
    return true, nil
}

func (lock *Lock) Unlock() (err error) {
    _, err = lock.conn.Do(&amp;quot;del&amp;quot;, lock.key())
    return
}

func (lock *Lock) key() string {
    return fmt.Sprintf(&amp;quot;redislock:%s&amp;quot;, lock.resource)
}

func (lock *Lock) AddTimeout(ex_time int64) (ok bool, err error) {
    ttl_time, err := redis.Int64(lock.conn.Do(&amp;quot;TTL&amp;quot;, lock.key()))
    fmt.Println(ttl_time)
    if err != nil {
        log.Fatal(&amp;quot;redis get failed:&amp;quot;, err)
    }
    if ttl_time &amp;gt; 0 {
        fmt.Println(11)
        _, err := redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(ttl_time+ex_time)))
        if err == redis.ErrNil {
            return false, nil
        }
        if err != nil {
            return false, err
        }
    }
    return false, nil
}

func TryLock(conn redis.Conn, resource string, token string, DefaulTimeout int) (lock *Lock, ok bool, err error) {
    return TryLockWithTimeout(conn, resource, token, DefaulTimeout)
}

func TryLockWithTimeout(conn redis.Conn, resource string, token string, timeout int) (lock *Lock, ok bool, err error) {
    lock = &amp;amp;Lock{resource, token, conn, timeout}

    ok, err = lock.tryLock()

    if !ok || err != nil {
        lock = nil
    }

    return
}

func main() {
    fmt.Println(&amp;quot;start&amp;quot;)
    DefaultTimeout := 10
    conn, err := redis.Dial(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:6379&amp;quot;)

    lock, ok, err := TryLock(conn, &amp;quot;xiaoru.cc&amp;quot;, &amp;quot;token&amp;quot;, int(DefaultTimeout))
    if err != nil {
        log.Fatal(&amp;quot;Error while attempting lock&amp;quot;)
    }
    if !ok {
        log.Fatal(&amp;quot;Lock&amp;quot;)
    }
    lock.AddTimeout(100)

    time.Sleep(time.Duration(DefaultTimeout) * time.Second)
    fmt.Println(&amp;quot;end&amp;quot;)
    defer lock.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段golang代码运行后的正常结果是:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run lock.go
start
10
11
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果同时起多个进程去测试，会遇到这么一个结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run lock.go
start
2016/03/23 01:23:22 Lock
exit status 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、promes告警事件进行加锁&lt;/p&gt;

&lt;p&gt;后台是分布式的admin程序，分布在几台机器上，同时去消费kafka中的告警信息事件，对于同一个id的有着不同的事件，对于同一个id可能在不同的实例进行事件的处理，比如一个发生事件，一个恢复事件，这个时候就要使用分布式锁了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set id EX 10 NX
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;redis分布式锁实现乐观锁-悲观锁&#34;&gt;Redis分布式锁实现乐观锁、悲观锁&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;乐观锁的实现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;乐观锁实现中的锁就是商品的键值对。使用jedis的watch方法监视商品键值对，如果事务提交exec时发现监视的键值对发生变化，事务将被取消，商品数目不会被改动。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1).multi，开启Redis的事务，置客户端为事务态。

2).exec，提交事务，执行从multi到此命令前的命令队列，置客户端为非事务态。

3).discard，取消事务，置客户端为非事务态。

4).watch,监视键值对，作用时如果事务提交exec时发现监视的监视对发生变化，事务将被取消。
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;悲观锁实现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;悲观锁中的锁是一个唯一标识的锁lockKey和该锁的过期时间。首先确定缓存中有商品，然后在拿数据(商品数目改动)之前先获取到锁，之后对商品数目进行减一操作，操作完成释放锁，一个秒杀操作完成。这个锁是基于redis的setNX操作实现的阻塞式分布式锁。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Rocketmq</title>
          <link>https://kingjcy.github.io/post/middleware/mq/rocketmq/</link>
          <pubDate>Thu, 28 Mar 2019 21:26:44 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/rocketmq/</guid>
          <description>&lt;p&gt;RocketMQ 一个纯java、分布式、队列模型的开源消息中间件，是阿里巴巴在2012年开源的分布式消息中间件，目前已经捐赠给 Apache 软件基金会，并于2017年9月25日成为 Apache 的顶级项目。作为经历过多次阿里巴巴双十一这种“超级工程”的洗礼并有稳定出色表现的国产中间件，以其高性能、低延时和高可靠等特性近年来已经也被越来越多的国内企业使用。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;RocketMQ具有以下特点：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;灵活可扩展性&lt;/p&gt;

&lt;p&gt;RocketMQ 天然支持集群，其核心四组件（Name Server、Broker、Producer、Consumer）每一个都可以在没有单点故障的情况下进行水平扩展。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;海量消息堆积能力&lt;/p&gt;

&lt;p&gt;RocketMQ 采用零拷贝原理实现超大的消息的堆积能力，据说单机已可以支持亿级消息堆积，而且在堆积了这么多消息后依然保持写入低延迟。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;支持顺序消息&lt;/p&gt;

&lt;p&gt;可以保证消息消费者按照消息发送的顺序对消息进行消费。顺序消息分为全局有序和局部有序，一般推荐使用局部有序，即生产者通过将某一类消息按顺序发送至同一个队列来实现。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多种消息过滤方式&lt;/p&gt;

&lt;p&gt;消息过滤分为在服务器端过滤和在消费端过滤。服务器端过滤时可以按照消息消费者的要求做过滤，优点是减少不必要消息传输，缺点是增加了消息服务器的负担，实现相对复杂。消费端过滤则完全由具体应用自定义实现，这种方式更加灵活，缺点是很多无用的消息会传输给消息消费者。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;支持事务消息&lt;/p&gt;

&lt;p&gt;RocketMQ 除了支持普通消息，顺序消息之外还支持事务消息，这个特性对于分布式事务来说提供了又一种解决思路。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;回溯消费&lt;/p&gt;

&lt;p&gt;回溯消费是指消费者已经消费成功的消息，由于业务上需求需要重新消费，RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒，可以向前回溯，也可以向后回溯。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多客户端&lt;/p&gt;

&lt;p&gt;RocketMQ 目前支持 Java、C++、Go 三种语言访问。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;通过下面的基本部署图，我们看一下一些概念&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/rocketmq/20180328.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、生产者(Producer）&lt;/p&gt;

&lt;p&gt;生产者（Producer）负责产生消息，生产者向消息服务器发送由业务应用程序系统生成的消息。 RocketMQ 提供了三种方式发送消息：同步、异步和单向。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;同步发送&lt;/p&gt;

&lt;p&gt;同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。一般用于重要通知消息，例如重要通知邮件、营销短信。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;异步发送&lt;/p&gt;

&lt;p&gt;异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包，一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;单向发送&lt;/p&gt;

&lt;p&gt;单向发送是指只负责发送消息而不等待服务器回应且没有回调函数触发，适用于某些耗时非常短但对可靠性要求并不高的场景，例如日志收集。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2、生产者组（Producer Group）&lt;/p&gt;

&lt;p&gt;生产者组（Producer Group）是一类 Producer 的集合，这类 Producer 通常发送一类消息并且发送逻辑一致，所以将这些 Producer 分组在一起。从部署结构上看生产者通过 Producer Group 的名字来标记自己是一个集群。&lt;/p&gt;

&lt;p&gt;3、消费者（Consumer&lt;/p&gt;

&lt;p&gt;消费者（Consumer）负责消费消息，消费者从消息服务器拉取信息并将其输入用户应用程序。站在用户应用的角度消费者有两种类型：拉取型消费者、推送型消费者。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;拉取型消费者&lt;/p&gt;

&lt;p&gt;拉取型消费者（Pull Consumer）主动从消息服务器拉取信息，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型。这是第一代的模式，典型代表包括Notify、Napoli。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;推送型消费者&lt;/p&gt;

&lt;p&gt;推送型消费者（Push Consumer）封装了消息的拉取、消费进度和其他的内部维护工作，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型，但从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器，当监听器处触发后才开始消费消息。这是第二代的模式，典型代表MetaQ。&lt;/p&gt;

&lt;p&gt;最后第三代是以拉模式为主，兼有推模式低延迟消息引擎RocketMQ，在二代功能特性的基础上，为电商金融领域添加了可靠重试、基于文件存储的分布式事务等特性。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;4、消费者组（Consumer Group）&lt;/p&gt;

&lt;p&gt;消费者组（Consumer Group）一类 Consumer 的集合名称，这类 Consumer 通常消费同一类消息并且消费逻辑一致，所以将这些 Consumer 分组在一起。消费者组与生产者组类似，都是将相同角色的分组在一起并命名，分组是个很精妙的概念设计，RocketMQ 正是通过这种分组机制，实现了天然的消息负载均衡。消费消息时通过 Consumer Group 实现了将消息分发到多个消费者服务器实例，比如某个 Topic 有9条消息，其中一个 Consumer Group 有3个实例（3个进程或3台机器），那么每个实例将均摊3条消息，这也意味着我们可以很方便的通过加机器来实现水平扩展。&lt;/p&gt;

&lt;p&gt;5、消息服务器（Broker）&lt;/p&gt;

&lt;p&gt;消息服务器（Broker）是消息存储中心，主要作用是接收来自 Producer 的消息并存储， Consumer 从这里取得消息。它还存储与消息相关的元数据，包括用户组、消费进度偏移量、队列信息等。从部署结构图中可以看出 Broker 有 Master 和 Slave 两种类型，Master 既可以写又可以读，Slave 不可以写只可以读。从物理结构上看 Broker 的集群部署方式有四种：单 Master 、多 Master 、多 Master 多 Slave（同步刷盘）、多 Master多 Slave（异步刷盘）。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;单 Master&lt;/p&gt;

&lt;p&gt;这种方式一旦 Broker 重启或宕机会导致整个服务不可用，这种方式风险较大，所以显然不建议线上环境使用。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多 Master&lt;/p&gt;

&lt;p&gt;所有消息服务器都是 Master ，没有 Slave 。这种方式优点是配置简单，单个 Master 宕机或重启维护对应用无影响。缺点是单台机器宕机期间，该机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受影响。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多 Master 多 Slave（异步复制）&lt;/p&gt;

&lt;p&gt;每个 Master 配置一个 Slave，所以有多对 Master-Slave，消息采用异步复制方式，主备之间有毫秒级消息延迟。这种方式优点是消息丢失的非常少，且消息实时性不会受影响，Master 宕机后消费者可以继续从 Slave 消费，中间的过程对用户应用程序透明，不需要人工干预，性能同多 Master 方式几乎一样。缺点是 Master 宕机时在磁盘损坏情况下会丢失极少量消息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多 Master 多 Slave（同步双写）&lt;/p&gt;

&lt;p&gt;每个 Master 配置一个 Slave，所以有多对 Master-Slave ，消息采用同步双写方式，主备都写成功才返回成功。这种方式优点是数据与服务都没有单点问题，Master 宕机时消息无延迟，服务与数据的可用性非常高。缺点是性能相对异步复制方式略低，发送消息的延迟会略高。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;6、名称服务器（NameServer）&lt;/p&gt;

&lt;p&gt;名称服务器（NameServer）用来保存 Broker 相关元信息并给 Producer 和 Consumer 查找 Broker 信息。NameServer 被设计成几乎无状态的，可以横向扩展，节点之间相互之间无通信，通过部署多台机器来标记自己是一个伪集群。每个 Broker 在启动的时候会到 NameServer 注册，Producer 在发送消息前会根据 Topic 到 NameServer 获取到 Broker 的路由信息，Consumer 也会定时获取 Topic 的路由信息。所以从功能上看应该是和 ZooKeeper 差不多，据说 RocketMQ 的早期版本确实是使用的 ZooKeeper ，后来改为了自己实现的 NameServer 。&lt;/p&gt;

&lt;p&gt;7、消息（Message）&lt;/p&gt;

&lt;p&gt;消息（Message）就是要传输的信息。一条消息必须有一个主题（Topic），主题可以看做是你的信件要邮寄的地址。一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 key 并在 Broker 上查找此消息以便在开发期间查找问题。&lt;/p&gt;

&lt;p&gt;8、主题（Topic）&lt;/p&gt;

&lt;p&gt;主题（Topic）可以看做消息的规类，它是消息的第一级类型。比如一个电商系统可以分为：交易消息、物流消息等，一条消息必须有一个 Topic 。Topic 与生产者和消费者的关系非常松散，一个 Topic 可以有0个、1个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。一个 Topic 也可以被 0个、1个、多个消费者订阅。&lt;/p&gt;

&lt;p&gt;9、标签（Tag）&lt;/p&gt;

&lt;p&gt;标签（Tag）可以看作子主题，它是消息的第二级类型，用于为用户提供额外的灵活性。使用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识。比如交易消息又可以分为：交易创建消息、交易完成消息等，一条消息可以没有 Tag 。标签有助于保持您的代码干净和连贯，并且还可以为 RocketMQ 提供的查询系统提供帮助。&lt;/p&gt;

&lt;p&gt;10、消息队列（Message Queue）&lt;/p&gt;

&lt;p&gt;消息队列（Message Queue），主题被划分为一个或多个子主题，即消息队列。一个 Topic 下可以设置多个消息队列，发送消息时执行该消息的 Topic ，RocketMQ 会轮询该 Topic 下的所有队列将消息发出去。下图 Broker 内部消息情况：&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/rocketmq/rocketmq&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实在上面已经看过架构图了，我们对集群工作流程做一个整理&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动Namesrv，Namesrv起来后监听端口，等待Broker、Produer、Consumer连上来，相当于一个路由控制中心。&lt;/li&gt;
&lt;li&gt;Broker启动，跟所有的Namesrv保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有topic信息。注册成功后，namesrv集群中就有Topic跟Broker的映射关系。&lt;/li&gt;
&lt;li&gt;收发消息前，先创建topic，创建topic时需要指定该topic要存储在哪些Broker上。也可以在发送消息时自动创建Topic。&lt;/li&gt;
&lt;li&gt;Producer发送消息，启动时先跟Namesrv集群中的其中一台建立长连接，并从Namesrv中获取当前发送的Topic存在哪些Broker上，然后跟对应的Broker建长连接，直接向Broker发消息。&lt;/li&gt;
&lt;li&gt;Consumer跟Producer类似。跟其中一台Namesrv建立长连接，获取当前订阅Topic存在哪些Broker，然后直接跟Broker建立连接通道，开始消费消息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;大体的流程就是这样，其实基本原理也就清楚了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;消息消费模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;消息消费模式有两种：集群消费（Clustering）和广播消费（Broadcasting）。默认情况下就是集群消费，该模式下一个消费者集群共同消费一个主题的多个队列，一个队列只会被一个消费者消费，如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。而广播消费消息会发给消费者组中的每一个消费者进行消费。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;消息顺序&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;消息顺序（Message Order）有两种：顺序消费（Orderly）和并行消费（Concurrently）。顺序消费表示消息消费的顺序同生产者为每个消息队列发送的顺序一致，所以如果正在处理全局顺序是强制性的场景，需要确保使用的主题只有一个消息队列。并行消费不再保证消息顺序，消费的最大并行数量受每个消费者客户端指定的线程池限制。&lt;/p&gt;

&lt;h1 id=&#34;应用&#34;&gt;应用&lt;/h1&gt;

&lt;p&gt;其实也就是一般消息队列的的基本应用。&lt;/p&gt;

&lt;p&gt;1.削峰填谷&lt;/p&gt;

&lt;p&gt;比如如秒杀等大型活动时会带来较高的流量脉冲，如果没做相应的保护，将导致系统超负荷甚至崩溃。如果因限制太过导致请求大量失败而影响用户体验，可以利用MQ 超高性能的消息处理能力来解决。&lt;/p&gt;

&lt;p&gt;2.异步解耦&lt;/p&gt;

&lt;p&gt;通过上、下游业务系统的松耦合设计，比如：交易系统的下游子系统（如积分等）出现不可用甚至宕机，都不会影响到核心交易系统的正常运转。&lt;/p&gt;

&lt;p&gt;3.顺序消息&lt;/p&gt;

&lt;p&gt;与FIFO原理类似，MQ提供的顺序消息即保证消息的先进先出，可以应用于交易系统中的订单创建、支付、退款等流程。&lt;/p&gt;

&lt;p&gt;4.分布式事务消息&lt;/p&gt;

&lt;p&gt;比如阿里的交易系统、支付红包等场景需要确保数据的最终一致性，需要引入 MQ 的分布式事务，既实现了系统之间的解耦，又可以保证最终的数据一致性。&lt;/p&gt;

&lt;p&gt;将大事务拆分成小事务，减少系统间的交互，既高效又可靠。再利用MQ 的可靠传输与多副本技术确保消息不丢，At-Least-Once 特性来最终确保数据的最终一致性。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Concurrence</title>
          <link>https://kingjcy.github.io/post/golang/go-concurrence/</link>
          <pubDate>Tue, 26 Mar 2019 11:05:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-concurrence/</guid>
          <description>&lt;p&gt;并发编程是我们计算机技术中最常用的一种编程技术,是一种基于多元程序的一种应用。&lt;/p&gt;

&lt;h1 id=&#34;并发演进&#34;&gt;并发演进&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;多进程&amp;mdash;&amp;mdash;-开销太大，都是基于内核的调用&lt;/li&gt;
&lt;li&gt;多线程&amp;mdash;&amp;mdash;-相对开销小，但是远远达不到需求，最多并发1万这样&lt;/li&gt;
&lt;li&gt;基于回调的非阻塞/异步io&amp;mdash;-共享内存式的同步异步，导致编程相当复杂&lt;/li&gt;
&lt;li&gt;协程&amp;mdash;-轻量级线程，轻松达到100w的并发，使用成本低、消耗资源低、能效高&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;C10K问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为什么需要不断的并发演进，支撑这么高并发的请求呢？我们先从C10K问题说起：2001年左右的时候，有一个叫Dan Kegel的人在网上提出：现在的硬件应该能够让一台机器支持10000个并发的client。然后他讨论了用不同的方式实现大规模并发服务的技术。&lt;/p&gt;

&lt;p&gt;当然， 现在C10K 已经不是问题了, 任何一个普通的程序员, 都能利用手边的语言和库, 轻松地写出 C10K 的服务器. 这既得益于软件的进步, 也得益于硬件性能的提高，现在应该扩展讨论的是应该是C10M问题了。&lt;/p&gt;

&lt;p&gt;目前使用的最多的就是Coroutine模型 和 非阻塞/异步IO(callback)&lt;/p&gt;

&lt;p&gt;不论线程还是进程，都不可能一个连接创建一个，相应的成本太大，多进程和多线程都有资源耗费比较大的问题，所以在高并发量的服务器端使用并不多。解决方案是一个线程或者进程处理多个连接，更具体的现在比较主流的是：Coroutine模型 和 非阻塞/异步IO(callback)，在分析这两个之前，我们先看看多进程和多线程的情况。&lt;/p&gt;

&lt;p&gt;1、多进程&lt;/p&gt;

&lt;p&gt;这种模型在linux下面的服务程序广泛采用，比如大名鼎鼎的apache，主进程负责监听和管理连接，而具体的业务处理都会交给子进程来处理。&lt;/p&gt;

&lt;p&gt;这种架构的最大的好处是隔离性，子进程万一crash并不会影响到父进程。缺点就是对系统的负担过重，想像一下如果有上万的连接，会需要多少进程来处理。所以这种模型比较合适那种不需要太多并发量的服务器程序。另外，进程间的通讯效率也是一个瓶颈之一，大部分会采用share memory等技术来减低通讯开销。&lt;/p&gt;

&lt;p&gt;这种模型的问题在于服务器的性能会随着连接数的增多而变差，关键性能和可扩展性并不是一回事，并不是扩展和性能是成比例上升。比如：&lt;/p&gt;

&lt;p&gt;持续几秒的短期连接，比如快速事务，如果每秒处理1000个事务，只有约1000个并发连接到服务器。事务延长到10秒，要维持每秒1000个事务，必须打开1万个并发连接。这种情况下：尽管你不顾DoS攻击，Apache也会性能陡降；同时大量的下载操作也会使Apache崩溃。如果每秒处理的连接从5千增加到1万，你会怎么做？比方说，你升级硬件并且提高处理器速度到原来的2倍。发生了什么？你得到两倍的性能，但你没有得到两倍的处理规模。每秒处理的连接可能只达到了6000。你继续提高速度，情况也没有改善。甚至16倍的性能时，仍然不能处理1万个并发连接。所以说性能和可扩展性是不一样的。&lt;/p&gt;

&lt;p&gt;其实这也是apache的核心问题，Apache会创建一个CGI进程，然后关闭，这个步骤并没有扩展。为什么呢？内核使用的O(N^2)算法使服务器无法处理1万个并发连接。&lt;/p&gt;

&lt;p&gt;内核算法中的两个基本问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;连接数=线程数/进程数。当一个数据包进来，内核会遍历其所有进程以决定由哪个进程来处理这个数据包。&lt;/li&gt;
&lt;li&gt;连接数=选择数/轮询次数（单线程）。同样的可扩展性问题，每个包都要走一遭列表上所有的socket。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解决方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;改进内核使其在常数时间内查找。&lt;/li&gt;
&lt;li&gt;使线程切换时间与线程数量无关。&lt;/li&gt;
&lt;li&gt;使用一个新的可扩展epoll()/IOCompletionPort常数时间去做socket查询。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、多线程&lt;/p&gt;

&lt;p&gt;这种模型在windows下面比较常见。它使用一个线程来处理一个client。他的好处是编程简单，最重要的是你会有一个清晰连续顺序的work flow。简单意味着不容易出错，但是这种模型的问题就是太多的线程会减低软件的运行效率，当然比进程节省资源。&lt;/p&gt;

&lt;p&gt;我们知道，操作系统的最小调度单元是“线程”，要执行任何一段代码，都必须落实到“线程”上。可惜线程太重，资源占用太高，频繁创建销毁会带来比较严重的性能问题，于是又诞生出线程池之类的常见使用模式。也是类似的原因，“阻塞”一个线程往往不是一个好主意，因为线程虽然暂停了，但是它所占用的资源还在。线程的暂停和继续对于调度器都会带来压力，而且线程越多，调度时的开销便越大，这其中的平衡很难把握。&lt;/p&gt;

&lt;p&gt;针对这个问题，有两类架构解决它：基于callback和coroutine的架构。&lt;/p&gt;

&lt;p&gt;3、Callback- 非阻塞/异步IO&lt;/p&gt;

&lt;p&gt;这种架构的特点是使用非阻塞的IO，这样服务器就可以持续运转，而不需要等待，可以使用很少的线程，即使只有一个也可以。需要定期的任务可以采取定时器来触发。把这种架构发挥到极致的就是node.js,一个用javascript来写服务器端程序的框架。在node.js中，所有的io都是non-block的，可以设置回调。&lt;/p&gt;

&lt;p&gt;举个例子来说明一下。
传统的写法:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var file = open(‘my.txt’);
var data = file.read(); //block
sleep(1);
print(data); //block
node.js的写法:

fs.open(‘my.txt’,function(err,data){
setTimeout(1000,function(){
console.log(data);
}
}); //non-block
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种架构的好处是performance会比较好，缺点是编程复杂，把以前连续的流程切成了很多片段。另外也不能充分发挥多核的能力。&lt;/p&gt;

&lt;p&gt;4、Coroutine-协程&lt;/p&gt;

&lt;p&gt;Coroutine是一种概念，很多语言都是支持启动coroutine，包括java，做的最好的就是goroutine。&lt;/p&gt;

&lt;p&gt;coroutine本质上是一种轻量级的thread，它的开销会比使用thread少很多。多个coroutine可以按照次序在一个thread里面执行，一个coroutine如果处于block状态，可以交出执行权，让其他的coroutine继续执行。&lt;/p&gt;

&lt;p&gt;非阻塞I/O模型协程(Coroutines)使得开发者可以采用阻塞式的开发风格,却能够实现非阻塞I/O的效果隐式事件调度,&lt;/p&gt;

&lt;p&gt;简单来说：协程十分轻量，可以在一个进程中执行有数以十万计的协程，依旧保持高性能。&lt;/p&gt;

&lt;p&gt;协程和线程的区别是：协程避免了无意义的调度，由此可以提高性能，但也因此，程序员必须自己承担调度的责任。&lt;/p&gt;

&lt;p&gt;执行协程只需要极少的栈内存（大概是4～5KB），默认情况下，线程栈的大小为1MB。&lt;/p&gt;

&lt;p&gt;goroutine就是一段代码，一个函数入口，以及在堆上为其分配的一个堆栈。所以它非常廉价，我们可以很轻松的创建上万个goroutine，但它们并不是被操作系统所调度执行。&lt;/p&gt;

&lt;p&gt;Google go语言对coroutine使用了语言级别的支持，使用关键字go来启动一个coroutine(从这个关键字可以看出Go语言对coroutine的重视),结合chan(类似于message queue的概念)来实现coroutine的通讯，实现了Go的理念 ”Do not communicate by sharing memory; instead, share memory by communicating.”。&lt;/p&gt;

&lt;p&gt;goroutine 的一个主要特性就是它们的消耗；创建它们的初始内存成本很低廉（与需要 1 至 8MB 内存的传统 POSIX 线程形成鲜明对比）以及根据需要动态增长和缩减占用的资源。这使得 goroutine 会从 4096 字节的初始栈内存占用开始按需增长或缩减内存占用，而无需担心资源的耗尽。&lt;/p&gt;

&lt;p&gt;为了实现这个目标，链接器（5l、6l 和 8l）会在每个函数前插入一个序文，这个序文会在函数被调用之前检查判断当前的资源是否满足调用该函数的需求（备注 1）。如果不满足，则调用 runtime.morestack 来分配新的栈页面（备注 2），从函数的调用者那里拷贝函数的参数，然后将控制权返回给调用者。此时，已经可以安全地调用该函数了。当函数执行完毕，事情并没有就此结束，函数的返回参数又被拷贝至调用者的栈结构中，然后释放无用的栈空间。&lt;/p&gt;

&lt;p&gt;通过这个过程，有效地实现了栈内存的无限使用。假设你并不是不断地在两个栈之间往返，通俗地讲叫栈分割，则代价是十分低廉的。&lt;/p&gt;

&lt;p&gt;简单来说:Go语言通过系统的线程来多路派遣这些函数的执行，使得每个用go关键字执行的函数可以运行成为一个单位协程。当一个协程阻塞的时候，调度器就会自动把其他协程安排到另外的线程中去执行，从而实现了程序无等待并行化运行。而且调度的开销非常小，一颗CPU调度的规模不下于每秒百万次，这使得我们能够创建大量的goroutine，从而可以很轻松地编写高并发程序，达到我们想要的目的。&lt;/p&gt;

&lt;p&gt;Coroutine模型 和 非阻塞/异步IO(callback)性能对比&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从性能角度来说，callback的典型node.js和golang的性能测试结果，两者差不多。&lt;/li&gt;
&lt;li&gt;不过从代码可读性角度来说，callback确实有点不太好。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;进程、线程、协程的关系和区别：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;进程拥有自己独立的堆和栈，既不共享堆，亦不共享栈，进程由操作系统调度。&lt;/li&gt;
&lt;li&gt;线程拥有自己独立的栈和共享的堆，共享堆，不共享栈，线程亦由操作系统调度(标准线程是的)。&lt;/li&gt;
&lt;li&gt;协程和线程一样共享堆，不共享栈，协程由程序员在协程的代码里显示调度。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;多进程并发&#34;&gt;多进程并发&lt;/h1&gt;

&lt;h2 id=&#34;进程&#34;&gt;进程&lt;/h2&gt;

&lt;p&gt;我们把一个运行的程序就叫做进程，可以说是操作系统运行和资源分配的一个基本单位。在进程中有进程id作为唯一标识，来对进程进行操作。进程进一步衍生出子进程的概念。对应的也有子父进程的关系。&lt;/p&gt;

&lt;p&gt;简单说一下go中获取进程id和对应的父进程id&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pid := os.Getpid()
ppid := os.Getppid()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常大部分的父进程都是1，也就是内核启动进程。&lt;/p&gt;

&lt;h2 id=&#34;进程状态&#34;&gt;进程状态&lt;/h2&gt;

&lt;p&gt;1、R (TASK_RUNNING)，可执行状态。&lt;/p&gt;

&lt;p&gt;只有在该状态的进程才可能在CPU上运行。而同一时刻可能有多个进程处于可执行状态，这些进程的task_struct结构（进程控制块）被放入对应CPU的可执行队列中（一个进程最多只能出现在一个CPU的可执行队列中）。进程调度器的任务就是从各个CPU的可执行队列中分别选择一个进程在该CPU上运行。&lt;/p&gt;

&lt;p&gt;很多操作系统教科书将正在CPU上执行的进程定义为RUNNING状态、而将可执行但是尚未被调度执行的进程定义为READY状态，这两种状态在linux下统一为 TASK_RUNNING状态。&lt;/p&gt;

&lt;p&gt;2、S (TASK_INTERRUPTIBLE)，可中断的睡眠状态。&lt;/p&gt;

&lt;p&gt;处于这个状态的进程因为等待某某事件的发生（比如等待socket连接、等待信号量），而被挂起。这些进程的task_struct结构被放入对应事件的等待队列中。当这些事件发生时（由外部中断触发、或由其他进程触发），对应的等待队列中的一个或多个进程将被唤醒。&lt;/p&gt;

&lt;p&gt;通过ps命令我们会看到，一般情况下，进程列表中的绝大多数进程都处于TASK_INTERRUPTIBLE状态（除非机器的负载很高）。毕竟CPU就这么一两个，进程动辄几十上百个，如果不是绝大多数进程都在睡眠，CPU又怎么响应得过来。&lt;/p&gt;

&lt;p&gt;3、D (TASK_UNINTERRUPTIBLE)，不可中断的睡眠状态。&lt;/p&gt;

&lt;p&gt;与TASK_INTERRUPTIBLE状态类似，进程处于睡眠状态，但是此刻进程是不可中断的。不可中断，指的并不是CPU不响应外部硬件的中断，而是指进程不响应异步信号。
绝大多数情况下，进程处在睡眠状态时，总是应该能够响应异步信号的。否则你将惊奇的发现，kill -9竟然杀不死一个正在睡眠的进程了！于是我们也很好理解，为什么ps命令看到的进程几乎不会出现TASK_UNINTERRUPTIBLE状态，而总是TASK_INTERRUPTIBLE状态。&lt;/p&gt;

&lt;p&gt;而TASK_UNINTERRUPTIBLE状态存在的意义就在于，内核的某些处理流程是不能被打断的。如果响应异步信号，程序的执行流程中就会被插入一段用于处理异步信号的流程（这个插入的流程可能只存在于内核态，也可能延伸到用户态），于是原有的流程就被中断了。（参见《linux内核异步中断浅析》）
在进程对某些硬件进行操作时（比如进程调用read系统调用对某个设备文件进行读操作，而read系统调用最终执行到对应设备驱动的代码，并与对应的物理设备进行交互），可能需要使用TASK_UNINTERRUPTIBLE状态对进程进行保护，以避免进程与设备交互的过程被打断，造成设备陷入不可控的状态。这种情况下的TASK_UNINTERRUPTIBLE状态总是非常短暂的，通过ps命令基本上不可能捕捉到。&lt;/p&gt;

&lt;p&gt;linux系统中也存在容易捕捉的TASK_UNINTERRUPTIBLE状态。执行vfork系统调用后，父进程将进入TASK_UNINTERRUPTIBLE状态，直到子进程调用exit或exec（参见《神奇的vfork》）。
通过下面的代码就能得到处于TASK_UNINTERRUPTIBLE状态的进程：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include

void main()
{
    if (!vfork())
        sleep(100);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译运行，然后ps一下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kouu@kouu-one:~/test$ ps -ax | grep a\.out
4371 pts/0    D+     0:00 ./a.out
4372 pts/0    S+     0:00 ./a.out
4374 pts/1    S+     0:00 grep a.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们可以试验一下TASK_UNINTERRUPTIBLE状态的威力。不管kill还是kill -9，这个TASK_UNINTERRUPTIBLE状态的父进程依然屹立不倒。我们介绍了Linux进程的R、S、D三种状态，这里接着上面的文章介绍另外三个状态。&lt;/p&gt;

&lt;p&gt;4、T (TASK_STOPPED or TASK_TRACED)，暂停状态或跟踪状态。&lt;/p&gt;

&lt;p&gt;向进程发送一个SIGSTOP信号，它就会因响应该信号而进入TASK_STOPPED状态（除非该进程本身处于TASK_UNINTERRUPTIBLE状态而不响应信号）。（SIGSTOP与SIGKILL信号一样，是非常强制的。不允许用户进程通过signal系列的系统调用重新设置对应的信号处理函数。）
向进程发送一个SIGCONT信号，可以让其从TASK_STOPPED状态恢复到TASK_RUNNING状态。&lt;/p&gt;

&lt;p&gt;当进程正在被跟踪时，它处于TASK_TRACED这个特殊的状态。“正在被跟踪”指的是进程暂停下来，等待跟踪它的进程对它进行操作。比如在gdb中对被跟踪的进程下一个断点，进程在断点处停下来的时候就处于TASK_TRACED状态。而在其他时候，被跟踪的进程还是处于前面提到的那些状态。&lt;/p&gt;

&lt;p&gt;对于进程本身来说，TASK_STOPPED和TASK_TRACED状态很类似，都是表示进程暂停下来。
而TASK_TRACED状态相当于在TASK_STOPPED之上多了一层保护，处于TASK_TRACED状态的进程不能响应SIGCONT信号而被唤醒。只能等到调试进程通过ptrace系统调用执行PTRACE_CONT、PTRACE_DETACH等操作（通过ptrace系统调用的参数指定操作），或调试进程退出，被调试的进程才能恢复TASK_RUNNING状态。&lt;/p&gt;

&lt;p&gt;5、Z (TASK_DEAD - EXIT_ZOMBIE)，退出状态，进程成为僵尸进程。&lt;/p&gt;

&lt;p&gt;进程在退出的过程中，处于TASK_DEAD状态。&lt;/p&gt;

&lt;p&gt;在这个退出过程中，进程占有的所有资源将被回收，除了task_struct结构（以及少数资源）以外。于是进程就只剩下task_struct这么个空壳，故称为僵尸。
之所以保留task_struct，是因为task_struct里面保存了进程的退出码、以及一些统计信息。而其父进程很可能会关心这些信息。比如在shell中，$?变量就保存了最后一个退出的前台进程的退出码，而这个退出码往往被作为if语句的判断条件。
当然，内核也可以将这些信息保存在别的地方，而将task_struct结构释放掉，以节省一些空间。但是使用task_struct结构更为方便，因为在内核中已经建立了从pid到task_struct查找关系，还有进程间的父子关系。释放掉task_struct，则需要建立一些新的数据结构，以便让父进程找到它的子进程的退出信息。&lt;/p&gt;

&lt;p&gt;父进程可以通过wait系列的系统调用（如wait4、waitid）来等待某个或某些子进程的退出，并获取它的退出信息。然后wait系列的系统调用会顺便将子进程的尸体（task_struct）也释放掉。
子进程在退出的过程中，内核会给其父进程发送一个信号，通知父进程来“收尸”。这个信号默认是SIGCHLD，但是在通过clone系统调用创建子进程时，可以设置这个信号。&lt;/p&gt;

&lt;p&gt;通过下面的代码能够制造一个EXIT_ZOMBIE状态的进程：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include

void main()
{
    if (fork())
        while(1)
            sleep(100);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译运行，然后ps一下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kouu@kouu-one:~/test$ ps -ax | grep a\.out
10410 pts/0    S+     0:00 ./a.out
10411 pts/0    Z+     0:00 [a.out]
10413 pts/1    S+     0:00 grep a.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只要父进程不退出，这个僵尸状态的子进程就一直存在。那么如果父进程退出了呢，谁又来给子进程“收尸”？
当进程退出的时候，会将它的所有子进程都托管给别的进程（使之成为别的进程的子进程）。托管给谁呢？可能是退出进程所在进程组的下一个进程（如果存在的话），或者是1号进程。所以每个进程、每时每刻都有父进程存在。除非它是1号进程。&lt;/p&gt;

&lt;p&gt;1号进程，pid为1的进程，又称init进程。
linux系统启动后，第一个被创建的用户态进程就是init进程。它有两项使命：
1、执行系统初始化脚本，创建一系列的进程（它们都是init进程的子孙）；
2、在一个死循环中等待其子进程的退出事件，并调用waitid系统调用来完成“收尸”工作；
init进程不会被暂停、也不会被杀死（这是由内核来保证的）。它在等待子进程退出的过程中处于TASK_INTERRUPTIBLE状态，“收尸”过程中则处于TASK_RUNNING状态。&lt;/p&gt;

&lt;p&gt;6、X (TASK_DEAD - EXIT_DEAD)，退出状态，进程即将被销毁。&lt;/p&gt;

&lt;p&gt;而进程在退出过程中也可能不会保留它的task_struct。比如这个进程是多线程程序中被detach过的进程（进程？线程？参见《linux线程浅析》）。或者父进程通过设置SIGCHLD信号的handler为SIG_IGN，显式的忽略了SIGCHLD信号。（这是posix的规定，尽管子进程的退出信号可以被设置为SIGCHLD以外的其他信号。）
此时，进程将被置于EXIT_DEAD退出状态，这意味着接下来的代码立即就会将该进程彻底释放。所以EXIT_DEAD状态是非常短暂的，几乎不可能通过ps命令捕捉到。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;进程的初始状态&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;进程是通过fork系列的系统调用（fork、clone、vfork）来创建的，内核（或内核模块）也可以通过kernel_thread函数创建内核进程。这些创建子进程的函数本质上都完成了相同的功能——将调用进程复制一份，得到子进程。（可以通过选项参数来决定各种资源是共享、还是私有。）
那么既然调用进程处于TASK_RUNNING状态（否则，它若不是正在运行，又怎么进行调用？），则子进程默认也处于TASK_RUNNING状态。
另外，在系统调用调用clone和内核函数kernel_thread也接受CLONE_STOPPED选项，从而将子进程的初始状态置为 TASK_STOPPED。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;进程状态变迁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;进程自创建以后，状态可能发生一系列的变化，直到进程退出。而尽管进程状态有好几种，但是进程状态的变迁却只有两个方向——从TASK_RUNNING状态变为非TASK_RUNNING状态、或者从非TASK_RUNNING状态变为TASK_RUNNING状态。
也就是说，如果给一个TASK_INTERRUPTIBLE状态的进程发送SIGKILL信号，这个进程将先被唤醒（进入TASK_RUNNING状态），然后再响应SIGKILL信号而退出（变为TASK_DEAD状态）。并不会从TASK_INTERRUPTIBLE状态直接退出。&lt;/p&gt;

&lt;p&gt;进程从非TASK_RUNNING状态变为TASK_RUNNING状态，是由别的进程（也可能是中断处理程序）执行唤醒操作来实现的。执行唤醒的进程设置被唤醒进程的状态为TASK_RUNNING，然后将其task_struct结构加入到某个CPU的可执行队列中。于是被唤醒的进程将有机会被调度执行。&lt;/p&gt;

&lt;p&gt;而进程从TASK_RUNNING状态变为非TASK_RUNNING状态，则有两种途径：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;响应信号而进入TASK_STOPED状态、或TASK_DEAD状态；&lt;/li&gt;
&lt;li&gt;执行系统调用主动进入TASK_INTERRUPTIBLE状态（如nanosleep系统调用）、或TASK_DEAD状态（如exit系统调用）；或由于执行系统调用需要的资源得不到满足，而进入TASK_INTERRUPTIBLE状态或TASK_UNINTERRUPTIBLE状态（如select系统调用）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;显然，这两种情况都只能发生在进程正在CPU上执行的情况下。&lt;/p&gt;

&lt;h2 id=&#34;空间&#34;&gt;空间&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;内核空间&amp;mdash;专门给内核用的，内核可以和硬件进行交互（0-TASK_SIZE）&lt;/li&gt;
&lt;li&gt;用户空间&amp;mdash;专门给用户进程用的，用户进程不可以和硬件进行交互，内核对用户空间进行分配（TASK_SIZE-2（32/64）次方）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两个空间合成了机器的虚拟内存，根据机器的位数不同，分别使用0-2的32/64次方来表示虚拟内存的地址，内核和用户的地址是以TASK_SIZE来分界的，TASK_SIZE根据不同的系统数值也不同，&lt;/p&gt;

&lt;p&gt;Linux的虚拟地址空间范围为0～4G，Linux内核将这4G字节的空间分为两部分， 将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF）供内核使用，称为“内核空间”。而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF）供各个进程使用，称为“用户空间。因为每个进程可以通过系统调用进入内核，因此，Linux内核由系统内的所有进程共享。于是，从具体进程的角度来看，每个进程可以拥有4G字节的虚拟空间。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/concurrence/concurrence&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1.用户进程只能分配到这个用户的用户空间。&lt;/p&gt;

&lt;p&gt;2.不同的进程之间是相互不可见的，不会互相干扰。&lt;/p&gt;

&lt;p&gt;3.内核划分分配虚拟内存，cpu划分物理内存，虚拟内存和物理内存是相互映射的。&lt;/p&gt;

&lt;h2 id=&#34;系统调用&#34;&gt;系统调用&lt;/h2&gt;

&lt;p&gt;系统调用就是让用户空间和内核空间交互的一座桥梁，用户进程在用户空间是不能操作计算机硬件的，但是内核空间中却可以，所以内核开放来一些接口来给用户来操作，这些接口就是系统调用，&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/concurrence/Linux.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每一种语言都对系统调用做好了封装，比如go中&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-syscall/&#34;&gt;syscall&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;多线程并发&#34;&gt;多线程并发&lt;/h1&gt;

&lt;p&gt;线程的并发在java中得到了很好的应用和封装，当然c中也是可以实现的，只不过一开始用c/c++的时候比较倾向于子进程的使用。&lt;/p&gt;

&lt;p&gt;线程相对于进程相对开销小，提高的并发量，最多并发到1万，可见极大的提高了承受的能力。我们可以简单的看一下&lt;a href=&#34;https://kingjcy.github.io/post/linux/c++/thread/&#34;&gt;线程的并发&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;目前还有很多网站的并发使用的就是线程并发的能力在支持，并且能够满足要求，但是随着用户并发量越来越大，需要能够支持更加强大的并发能力，这个时候可以看一下协程并发。&lt;/p&gt;

&lt;h1 id=&#34;goroutine并发&#34;&gt;goroutine并发&lt;/h1&gt;

&lt;p&gt;go是天生为并发而生的，能够轻轻松松达到百万级的轻量并发，可见其并发承受能力之强，具体的并发可以看&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-goroutinechannel/&#34;&gt;goroutine&lt;/a&gt;,我们这边主要看一下并发的使用以及其能力情况。&lt;/p&gt;

&lt;h2 id=&#34;简单的并发&#34;&gt;简单的并发&lt;/h2&gt;

&lt;p&gt;一般用于启动多协程来处理后端数据业务，类似于多线程处理数据，直接使用for循环并发一定数量的goroutine，然后对数据进行处理，如果是处理同一个数据，则需要使用锁，通过传递参数，还可以对业务进行分通道处理&lt;/p&gt;

&lt;p&gt;使用这种并发方式我们没法控制goroutine产生数量，如果处理程序稍微耗时，在单机万级十万级qps请求下，goroutine大规模爆发，内存暴涨，处理效率会很快下降甚至引发程序崩溃。&lt;/p&gt;

&lt;p&gt;我想，作为golang拥趸的Gopher们一定都使用过它的net/http标准库，我们常用的go的net/http库就是这种使用方式，很多人都说用golang写web server完全可以不用借助第三方的web framework，仅用net/http标准库就能写一个高性能的web server，的确，我也用过它写过web server，简洁高效，性能表现也相当不错，除非有比较特殊的需求否则一般的确不用借助第三方web framework，但是天下没有白吃的午餐，net/http为啥这么快？&lt;/p&gt;

&lt;p&gt;其实每次处理业务都是启动了一个新的goroutine去执行处理逻辑，而且这是在一个无限循环体里面，所以意味着，每来一个请求它就会开一个goroutine去处理，相当任性粗暴啊…，不过有Go调度器背书，一般来说也没啥压力，然而，如果，我是说如果哈，突然一大波请求涌进来了（比方说黑客搞了成千上万的肉鸡DDOS你，没错！就这么倒霉！），这时候，就很成问题了，他来10w个请求你就要开给他10w个goroutine，来100w个你就要老老实实开给他100w个，线程调度压力陡升，内存爆满，再然后，你就跪了…，也就是我们上面说的，崩溃，正常使用没有问题，归功于go强大的调度性能和goroutine的资源消耗。但是危险存在着。&lt;/p&gt;

&lt;p&gt;在这边也对我们这边的系统做了一个压测，看看到底能爆发到什么程度，是不是和360推送的京东并发一样强大，360消息推送的数据如下，可见单机并发可以达到100W：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;16台机器，标配：24个硬件线程，64GB内存
Linux Kernel 2.6.32 x86_64
单机80万并发连接，load 0.2~0.4，CPU 总使用率 7%~10%，内存占用20GB (res)
目前接入的产品约1280万在线用户
2分钟一次GC，停顿2秒 (1.0.3 的 GC 不给力，直接升级到 tip，再次吃螃蟹)
15亿个心跳包/天，占大多数。
京东云消息推送系统
(团队人数:4)
单机并发tcp连接数峰值118w
内存占用23G(Res)
Load 0.7左右
心跳包 4k/s
gc时间2-3.x s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们的数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;8核16G内存
Linux promespreapp03 2.6.32-279.19.1.el6_sn.11.x86_64 #10 SMP Tue May 16 20:11:22 CST 2017 x86_64 x86_64 x86_64 GNU/Linux



[root@promesdevapp02 ants]# vi ants_benchmark_test.go
[root@promesdevapp02 ants]# go test -bench=&amp;quot;Goroutine$&amp;quot; -benchmem=true -run=none
goos: linux
goarch: amd64
pkg: github.com/panjf2000/ants
BenchmarkGoroutine-8          20     170688713 ns/op    32741982 B/op     155597 allocs/op
PASS
ok      github.com/panjf2000/ants   6.189s
[root@promesdevapp02 ants]# go test -bench=&amp;quot;AntsPool$&amp;quot; -benchmem=true -run=none
goos: linux
goarch: amd64
pkg: github.com/panjf2000/ants
BenchmarkAntsPool-8           10     107166741 ns/op     1076056 B/op       8851 allocs/op
PASS
ok      github.com/panjf2000/ants   1.265s
[root@promesdevapp02 ants]# vi ants_benchmark_test.go
[root@promesdevapp02 ants]# go test -bench=&amp;quot;Goroutine$&amp;quot; -benchmem=true -run=none
goos: linux
goarch: amd64
pkg: github.com/panjf2000/ants
BenchmarkGoroutine-8           1    4251624524 ns/op    492471736 B/op   1678022 allocs/op
PASS
ok      github.com/panjf2000/ants   4.487s
[root@promesdevapp02 ants]# go test -bench=&amp;quot;AntsPool$&amp;quot; -benchmem=true -run=none
goos: linux
goarch: amd64
pkg: github.com/panjf2000/ants
BenchmarkAntsPool-8            1    1059289888 ns/op    11469768 B/op      91975 allocs/op
PASS
ok      github.com/panjf2000/ants   1.069s
[root@promesdevapp02 ants]# vi ants_benchmark_test.go
[root@promesdevapp02 ants]# go test -bench=&amp;quot;Goroutine$&amp;quot; -benchmem=true -run=none
*** Test killed: ran too long (10m0s).
signal: segmentation fault (core dumped)
FAIL    github.com/panjf2000/ants   607.204s
[root@promesdevapp02 ants]# go test -bench=&amp;quot;AntsPool$&amp;quot; -benchmem=true -run=none
goos: linux
goarch: amd64
pkg: github.com/panjf2000/ants
BenchmarkAntsPool-8            1    10892840360 ns/op   14485872 B/op     114421 allocs/op
PASS
ok      github.com/panjf2000/ants   10.904s
[root@promesdevapp02 ants]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对应的是10W，100W，1000W的是否使用线程池的操作所消耗的资源的对比&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                    内存                                      cpu
10W     Nopool      32M 
        pool（20W）  1M

100W    Nopool      490M    
        pool（20W）  11M

1000W   Nopool      16G完全被被消耗，崩溃                        
        pool（20W）   15M                                     cpu不是性能瓶颈，最高就是100w的并发占用了70%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面是直接使用goroutine的资源消耗，但是正常还要结合http来处理&lt;/p&gt;

&lt;p&gt;单CPU测试&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;net/http&amp;quot;
    &amp;quot;runtime&amp;quot;
)

func main() {
    // 限制为1个CPU
    runtime.GOMAXPROCS(1)

    http.HandleFunc(&amp;quot;/&amp;quot;, func(w http.ResponseWriter, r  *http.Request) {
        fmt.Fprint(w, &amp;quot;Hello, world.&amp;quot;)
    })

    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;共测试五次，五次结果分别如下：（cpu 50%）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ab -c 100 -n 5000 http://127.0.0.1:8080/

Requests per second:    8427.78 [#/sec] (mean)
Requests per second:    7980.73 [#/sec] (mean)
Requests per second:    7509.63 [#/sec] (mean)
Requests per second:    8242.47 [#/sec] (mean)
Requests per second:    8898.19 [#/sec] (mean)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;多CPU测试&lt;/p&gt;

&lt;p&gt;因为是在同一台机器上测试的，所以限制使用的CPU数为机器的CPU数减一。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;net/http&amp;quot;
    &amp;quot;runtime&amp;quot;
)

func main() {
    // 限制为CPU的数量减一
    runtime.GOMAXPROCS( runtime.NumCPU() - 1 )

    http.HandleFunc(&amp;quot;/&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
        fmt.Fprint(w, &amp;quot;Hello, world.&amp;quot;)
    })

    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;golang的多CPU ab测试结果，共测试5次，结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ab -c 100 -n 5000 http://127.0.0.1:8080/（65%）

Requests per second:    14391.80 [#/sec] (mean)
Requests per second:    14307.09 [#/sec] (mean)
Requests per second:    14285.31 [#/sec] (mean)
Requests per second:    15182.34 [#/sec] (mean)
Requests per second:    14020.53 [#/sec] (mean)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见go的并发性能很高，但是会有崩溃的风险，最好使用工作池，直接goroutine正常使用于不是很频繁的api调用。例如探针，一般处理万级的请求。&lt;/p&gt;

&lt;h2 id=&#34;工作池-job队列&#34;&gt;工作池+job队列&lt;/h2&gt;

&lt;p&gt;这种使用方式，先启动一定数量的goroutine，使用channel，让当前goroutine处于阻塞状态，当有task往通道里传输，然后进行处理&lt;/p&gt;

&lt;p&gt;将请求放入队列,通过一定数量(例如CPU核心数)goroutine组成一个worker池(pool),workder池中的worker读取队列执行任务&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;工作者工作协程，挂入调度器，取Job，执行Job，周而复始&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;调度器，从Job队列取Job，分配给工作者，周而复始&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;web响应里，模拟了客户的请求-Job，并将此Job放入Job队列，只有有客户端请求，就周而复始的工作&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;正常使用使用于大规模的并发请求场景，可以处理百万级请求（通过benchmark来调优参数）&lt;/p&gt;

&lt;p&gt;10M的并发连接挑战意味着什么：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1千万的并发连接数
100万个连接/秒——每个连接以这个速率持续约10秒
10GB/秒的连接——快速连接到互联网。
1千万个数据包/秒——据估计目前的服务器每秒处理50K的数据包，以后会更多。过去服务器每秒可以处理100K的中断，并且每一个数据包都产生中断。
10微秒的延迟——可扩展服务器也许可以处理这个规模，但延迟可能会飙升。
10微秒的抖动——限制最大延迟
并发10核技术——软件应支持更多核的服务器。通常情况下，软件能轻松扩展到四核。服务器可以扩展到更多核，因此需要重写软件，以支持更多核的服务器。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;异步处理&#34;&gt;异步处理&lt;/h2&gt;

&lt;p&gt;这种使用方式，将一定数量的goroutine启动处理，留一个channenl返回，使用select读取channel中的数据，完成处理&lt;/p&gt;

&lt;h1 id=&#34;大并发场景&#34;&gt;大并发场景&lt;/h1&gt;

&lt;h2 id=&#34;数据打点-文件上传&#34;&gt;数据打点，文件上传&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;场景描述&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在一些场景下，有大规模请求（十万或百万级qps），我们处理的请求可能不需要立马知道结果，例如数据的打点，文件的上传等等。这时候我们需要异步化处理。常用的方法有使用resque、MQ、RabbitMQ等。这里我们在Golang语言里进行设计实践。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;方案以及演进&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、直接使用goroutine&lt;/p&gt;

&lt;p&gt;在Go语言原生并发的支持下，我们可以直接使用一个goroutine（如下方式）去并行处理这个请求。但是，这种方法明显有些不好的地方，我们没法控制goroutine产生数量，如果处理程序稍微耗时，在单机万级十万级qps请求下，goroutine大规模爆发，内存暴涨，处理效率会很快下降甚至引发程序崩溃。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
go handle(request)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、goroutine协同带缓存的管道&lt;/p&gt;

&lt;p&gt;我们定义一个带缓存的管道；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var queue = make(chan job, MAX_QUEUE_SIZE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后起一个协程处理管道传来的请求；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go func(){
   for {
    select {
        case job := &amp;lt;-queue:
            job.Do(request)
        case &amp;lt;- quit:
            return
    }

   }
}()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接收请求，发送job进行处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;job := &amp;amp;Job{request}
queue &amp;lt;- job
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种方法使用了缓冲队列一定程度上了提高了并发，但也是治标不治本，大规模并发只是推迟了问题的发生时间。当请求速度远大于队列的处理速度时，缓冲区很快被打满，后面的请求一样被堵塞了。&lt;/p&gt;

&lt;p&gt;3、job队列+工作池&lt;/p&gt;

&lt;p&gt;只用缓冲队列不能解决根本问题，这时候我们可以参考一下线程池的概念，定一个工作池（协程池），来限定最大goroutine数目。每次来新的job时，从工作池里取出一个可用的worker来执行job。这样一来即保障了goroutine的可控性，也尽可能大的提高了并发处理能力。&lt;/p&gt;

&lt;p&gt;工作池实现&lt;/p&gt;

&lt;p&gt;首先，我们定义一个job的接口, 具体内容由具体job实现；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Job interface {
    Do() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后定义一下job队列和work池类型，这里我们work池也用golang的channel实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// define job channel
type JobChan chan Job

// define worker channer
type WorkerChan chan JobChan
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们分别维护一个全局的job队列和工作池。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    JobQueue          JobChan
    WorkerPool        WorkerChan
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;worker的实现。每一个worker都有一个job channel，在启动worker的时候会被注册到work pool中。启动后通过自身的job channel取到job并执行job。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Worker struct {
    JobChannel JobChan
    quit       chan bool
}

func (w *Worker) Start() {
    go func() {
        for {
            // regist current job channel to worker pool
            WorkerPool &amp;lt;- w.JobChannel
            select {
            case job := &amp;lt;-w.JobChannel:
                if err := job.Do(); err != nil {
                    fmt.printf(&amp;quot;excute job failed with err: %v&amp;quot;, err)
                }
            // recieve quit event, stop worker
            case &amp;lt;-w.quit:
                return
            }
        }
    }()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现一个分发器（Dispatcher）。分发器包含一个worker的指针数组，启动时实例化并启动最大数目的worker，然后从job全局队列中不断取job选择可用的worker，然后将这个job仍向这个worker的channel中去，然后这个worker来执行job。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Dispatcher struct {
    Workers []*Worker
    quit    chan bool
}

func (d *Dispatcher) Run() {
    for i := 0; i &amp;lt; MaxWorkerPoolSize; i++ {
        worker := NewWorker()
        d.Workers = append(d.Workers, worker)
        worker.Start()
    }

    for {
        select {
        case job := &amp;lt;-JobQueue:
            go func(job Job) {
                jobChan := &amp;lt;-WorkerPool
                jobChan &amp;lt;- job
            }(job)
        // stop dispatcher
        case &amp;lt;-d.quit:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完整的实例源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;net/http&amp;quot;
    &amp;quot;fmt&amp;quot;
)

type Job struct {
    request string
}

func (j *Job)Handle(){
    fmt.Println(&amp;quot;test&amp;quot;)
}

type worker struct {
    work  JobChan
    quit chan bool
}

func (w *worker)start(i int)  {
    fmt.Println(&amp;quot;start worker:&amp;quot;,i)
    go func(i int) {
        for {
            fmt.Println(&amp;quot;add free worklist&amp;quot;)
            workList &amp;lt;- w.work
            select {
            case Task := &amp;lt;- w.work:
                fmt.Println(&amp;quot;worker&amp;quot;,i,&amp;quot;handle job .....&amp;quot;)
                Task.Handle()
                fmt.Println(&amp;quot;worker&amp;quot;,i,&amp;quot;handle over .....&amp;quot;)
            case &amp;lt;- w.quit:
                return
            }
        }
    }(i)

}



type schedule struct {
    workers []*worker
    quit chan bool
}

func newWorker() *worker {
    workchan := make(chan Job,1)
    return &amp;amp;worker{work:workchan}
}


func (s *schedule)schedule() {
    workList = make(chan JobChan,10)
    fmt.Println(&amp;quot;start pool&amp;quot;)
    for i := 0; i &amp;lt; 10; i++ {
        w := newWorker()
        s.workers = append(s.workers,w)
        w.start(i)
    }

    for {
        fmt.Println(&amp;quot;get task and get worker&amp;quot;)
        select {
        case job := &amp;lt;-queue:
            go func(job Job) {
                fmt.Println(&amp;quot;get worker&amp;quot;)
                jobChan := &amp;lt;-workList
                fmt.Println(&amp;quot;insert task into job&amp;quot;)
                jobChan &amp;lt;- job
            }(job)
            // stop dispatcher
        case &amp;lt;-s.quit:
            return
        }
    }


}

//define type queue and work
type JobChan chan Job
type WorkChan chan JobChan


var queue JobChan
var workList WorkChan

func newschedule() schedule  {
    fmt.Println(&amp;quot;newschedule&amp;quot;)
    return schedule{}
}


func init(){
    s := newschedule()
    go s.schedule()
}

func main()  {
    fmt.Println(&amp;quot;main&amp;quot;)
    queue = make(chan Job,1024)

    http.HandleFunc(&amp;quot;/metrics&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
        job := Job{&amp;quot;test&amp;quot;}
        queue &amp;lt;- job
    })


    fmt.Println(&amp;quot;start sueccess and listen at 9000!!&amp;quot;)
    http.ListenAndServe(&amp;quot;localhost:9000&amp;quot;,nil)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;邮件状态跟踪&#34;&gt;邮件状态跟踪&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;场景描述&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;邮件状态跟踪，有很多办法来跟踪这些状态的改变。不外乎通过定期的轮询或者系统通知来得到状态的变化。这两种方法都有它们的优缺点。对邮件这个产品来说，让用户尽快收到新的邮件是一个考量指标。邮件的轮询会产生大概每秒5万个HTTP请求，其中60%的请求会返回304状态（表示邮箱没有变化）。因此，为了减少服务器的负荷并加速邮件的接收，我们决定重写一个publisher-subscriber服务（这个服务通常也会称作bus，message broker或者event-channel）。这个服务负责接收状态更新的通知，然后还处理对这些更新的订阅。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;旧的架构。浏览器（Browser）会定期轮询API服务来获得邮件存储服务（Storage）的更新。&lt;/p&gt;

&lt;p&gt;新的架构。浏览器（Browser）和通知API服务（notificcation API）建立一个WebSocket连接。通知API服务会发送相关的订阅到Bus服务上。当收到新的电子邮件时，存储服务（Storage）向Bus发送一个通知，Bus又将通知发送给相应的订阅者。API服务为收到的通知找到相应的连接，然后把通知推送到用户的浏览器。&lt;/p&gt;

&lt;p&gt;我们今天就来讨论一下这个API服务（也可以叫做WebSocket服务）。在开始之前，我想提一下这个在线服务处理将近3百万个连接。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;惯用的做法（The idiomatic way）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先，我们看一下不做任何优化会如何用Go来实现这个服务的部分功能。在使用net/http 实现具体功能前，让我们先讨论下我们将如何发送和接收数据。这些数据是定义在WebSocket协议之上的（例如JSON对象）,我们在下文中会成他们为packet。&lt;/p&gt;

&lt;p&gt;我们先来实现Channel 结构,它包含相应的逻辑来通过WebScoket连接发送和接收packet。&lt;/p&gt;

&lt;p&gt;Channel结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Packet represents application level data.
type Packet struct {
    ...
}
// Channel wraps user connection.
type Channel struct {
    conn net.Conn    // WebSocket connection.
    send chan Packet // Outgoing packets queue.
}
func NewChannel(conn net.Conn) *Channel {
    c := &amp;amp;Channel{
        conn: conn,
        send: make(chan Packet, N),
    }
    go c.reader()
    go c.writer()
    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我要强调的是读和写这两个goroutines。每个goroutine都需要各自的内存栈。栈的初始大小由操作系统和Go的版本决定，通常在2KB到8KB之间。我们之前提到有3百万个在线连接，如果每个goroutine栈需要4KB的话，所有连接就需要24GB的内存。这还没算上给Channel 结构，发送packet用的ch.send 和其它一些内部字段分配的内存空间。&lt;/p&gt;

&lt;p&gt;接下来看一下I/O goroutines的“reader”的实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Channel) reader() {
    // We make a buffered read to reduce read syscalls.
    buf := bufio.NewReader(c.conn)
    for {
        pkt, _ := readPacket(buf)
        c.handle(pkt)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我们使用了bufio.Reader ，每次都会在buf 大小允许的范围内尽量读取多的字节，从而减少read() 系统调用的次数。在无限循环中，我们期望会接收到新的数据，请记住之前这句话：期望接收到新的数据。我们之后会讨论到这一点。&lt;/p&gt;

&lt;p&gt;我们把packet的解析和处理逻辑都忽略掉了，因为它们和我们要讨论的优化不相关。不过buf 值得我们的关注：它的缺省大小是4KB。这意味着所有连接将消耗掉额外的12 GB内存。“writer”也是类似的情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Channel) writer() {
    // We make buffered write to reduce write syscalls. 
    buf := bufio.NewWriter(c.conn)
    for pkt := range c.send {
        _ := writePacket(buf, pkt)
        buf.Flush()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在待发送packet的c.send channel上循环将packet写到缓存（buffer）里，细心的读者肯定已经发现，这又是额外的4KB内存。3百万个连接会占用12GB的内存。&lt;/p&gt;

&lt;p&gt;接下来我们看一下http的实现，我们已经有了一个简单的Channel 实现，现在我们需要一个WebSocket连接。&lt;/p&gt;

&lt;p&gt;注：如果你不知道WebSocket是怎么工作的，那么这里值得一提的是客户端是通过一个叫升级（Upgrade）请求的特殊HTTP机制来建立WebSocket的。在成功处理升级请求以后，服务端和客户端使用TCP连接来交换二进制的WebSocket帧（frames）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;net/http&amp;quot;
    &amp;quot;some/websocket&amp;quot;
)
http.HandleFunc(&amp;quot;/v1/ws&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
    conn, _ := websocket.Upgrade(r, w)
    ch := NewChannel(conn)
    //...
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请注意这里的http.ResponseWriter 结构包含bufio.Reader 和bufio.Writer （各自分别包含4KB的缓存）。它们用于*http.Request 初始化和返回结果。&lt;/p&gt;

&lt;p&gt;不管是哪个WebSocket，在成功回应一个升级请求之后，服务端在调用responseWriter.Hijack() 之后会接收到一个I/O缓存和对应的TCP连接。&lt;/p&gt;

&lt;p&gt;注：有时候我们可以通过net/http.putBufio{Reader,Writer} 调用把缓存释放回net/http 里的sync.Pool 。&lt;/p&gt;

&lt;p&gt;这样，这3百万个连接又需要额外的24GB内存。&lt;/p&gt;

&lt;p&gt;所以，为了这个什么都不干的程序，我们已经占用了72GB的内存！&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;优化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们来回顾一下前面介绍的用户连接的工作流程。在建立WebSocket之后，客户端会发送请求订阅相关事件（我们这里忽略类似ping/pong 的请求）,接下来，在整个连接的生命周期里，客户端可能就不会发送任何其它数据了,连接的生命周期可能会持续几秒钟到几天。&lt;/p&gt;

&lt;p&gt;所以在大部分时间里，Channel.reader() 和Channel.writer() 都在等待接收和发送数据。与它们一起等待的是各自分配的4 KB的I/O缓存。&lt;/p&gt;

&lt;p&gt;现在，我们发现有些地方是可以做进一步优化的，对吧？&lt;/p&gt;

&lt;p&gt;1、使用Netpoll&lt;/p&gt;

&lt;p&gt;你还记得Channel.reader() 的实现使用了bufio.Reader.Read() 吗？bufio.Reader.Read() 又会调用conn.Read() 。这个调用会被阻塞以等待接收连接上的新数据。如果连接上有新的数据，Go的运行环境（runtime）就会唤醒相应的goroutine让它去读取下一个packet。之后，goroutine会被再次阻塞来等待新的数据。我们来研究下Go的运行环境是怎么知道goroutine需要被唤醒的。&lt;/p&gt;

&lt;p&gt;如果我们看一下conn.Read() 的实现，就会看到它调用了net.netFD.Read() ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// net/fd_unix.go
func (fd *netFD) Read(p []byte) (n int, err error) {
    //...
    for {
        n, err = syscall.Read(fd.sysfd, p)
        if err != nil {
            n = 0
            if err == syscall.EAGAIN {
                if err = fd.pd.waitRead(); err == nil {
                    continue
                }
            }
        }
        //...
        break
    }
    //...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Go使用了sockets的非阻塞模式。EAGAIN表示socket里没有数据了但不会阻塞在空的socket上，OS会把控制权返回给用户进程。&lt;/p&gt;

&lt;p&gt;这里它首先对连接文件描述符进行read() 系统调用。如果read() 返回的是EAGAIN 错误，运行环境就是调用pollDesc.waitRead() ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// net/fd_poll_runtime.go
func (pd *pollDesc) waitRead() error {
   return pd.wait(&#39;r&#39;)
}
func (pd *pollDesc) wait(mode int) error {
   res := runtime_pollWait(pd.runtimeCtx, mode)
   //...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果继续深挖，我们可以看到netpoll的实现在Linux里用的是epoll而在BSD里用的是kqueue。我们的这些连接为什么不采用类似的方式呢？只有在socket上有可读数据时，才分配缓存空间并启用读数据的goroutine。&lt;/p&gt;

&lt;p&gt;在github.com/golang/go上，有一个关于 暴露(export) netpoll函数的请求，于是我们可以干掉goroutines&lt;/p&gt;

&lt;p&gt;假设我们用Go语言实现了netpoll。我们现在可以避免创建Channel.reader() 的goroutine，取而代之的是从订阅连接里收到新数据的事件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ch := NewChannel(conn)
// Make conn to be observed by netpoll instance.
poller.Start(conn, netpoll.EventRead, func() {
    // We spawn goroutine here to prevent poller wait loop
    // to become locked during receiving packet from ch.
    go Receive(ch)
})
// Receive reads a packet from conn and handles it somehow.
func (ch *Channel) Receive() {
    buf := bufio.NewReader(ch.conn)
    pkt := readPacket(buf)
    c.handle(pkt)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Channel.writer() 相对容易一点，因为我们只需在发送packet的时候创建goroutine并分配缓存。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (ch *Channel) Send(p Packet) {
    if c.noWriterYet() {
        go ch.writer()
    }
    ch.send &amp;lt;- p
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，这里我们没有处理write() 系统调用时返回的EAGAIN 。我们依赖Go运行环境去处理它。这种情况很少发生。如果需要的话我们还是可以像之前那样来处理。&lt;/p&gt;

&lt;p&gt;从ch.send 读取待发送的packets之后，ch.writer() 会完成它的操作，最后释放goroutine的栈和用于发送的缓存。通过避免这两个连续运行的goroutine所占用的I/O缓存和栈内存，我们已经节省了48GB。&lt;/p&gt;

&lt;p&gt;2、控制资源-Goroutine池&lt;/p&gt;

&lt;p&gt;大量的连接不仅仅会造成大量的内存消耗。在开发服务端的时候，我们还不停地遇到竞争条件（race conditions）和死锁（deadlocks）。随之而来的是所谓的自我分布式阻断攻击（self-DDOS）。在这种情况下，客户端会悍然地尝试重新连接服务端而把情况搞得更加糟糕。&lt;/p&gt;

&lt;p&gt;举个例子，如果因为某种原因我们突然无法处理ping/pong 消息，这些空闲连接就会不断地被关闭（它们会以为这些连接已经无效因此不会收到数据）。然后客户端每N秒就会以为失去了连接并尝试重新建立连接，而不是继续等待服务端发来的消息。&lt;/p&gt;

&lt;p&gt;在这种情况下，比较好的办法是让负载过重的服务端停止接受新的连接，这样负载均衡器（例如nginx）就可以把请求转到其它的服务端上去。&lt;/p&gt;

&lt;p&gt;撇开服务端的负载不说，如果所有的客户端突然（很可能是因为某个bug）向服务端发送一个packet，我们之前节省的48 GB内存又将会被消耗掉。因为这时我们又会和开始一样给每个连接创建goroutine并分配缓存。&lt;/p&gt;

&lt;p&gt;可以用一个goroutine池来限制同时处理packets的数目。下面的代码是一个简单的实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package gopool
func New(size int) *Pool {
    return &amp;amp;Pool{
        work: make(chan func()),
        sem:  make(chan struct{}, size),
    }
}
func (p *Pool) Schedule(task func()) error {
    select {
    case p.work &amp;lt;- task:
    case p.sem &amp;lt;- struct{}{}:
        go p.worker(task)
    }
}
func (p *Pool) worker(task func()) {
    defer func() { &amp;lt;-p.sem }
    for {
        task()
        task = &amp;lt;-p.work
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们使用netpoll的代码就变成下面这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pool := gopool.New(128)
poller.Start(conn, netpoll.EventRead, func() {
    // We will block poller wait loop when
    // all pool workers are busy.
    pool.Schedule(func() {
        Receive(ch)
    })
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在我们不仅要等可读的数据出现在socket上才能读packet，还必须等到从池里获取到空闲的goroutine。&lt;/p&gt;

&lt;p&gt;同样的，我们修改下Send() 的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pool := gopool.New(128)
func (ch *Channel) Send(p Packet) {
    if c.noWriterYet() {
        pool.Schedule(ch.writer)
    }
    ch.send &amp;lt;- p
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我们没有调用go ch.writer() ，而是想重复利用池里goroutine来发送数据。 所以，如果一个池有N 个goroutines的话，我们可以保证有N 个请求被同时处理。而N + 1 个请求不会分配N + 1 个缓存。goroutine池允许我们限制对新连接的Accept() 和Upgrade() ，这样就避免了大部分DDoS的情况。&lt;/p&gt;

&lt;p&gt;3、零拷贝升级（Zero-copy upgrade）&lt;/p&gt;

&lt;p&gt;之前已经提到，客户端通过HTTP升级（Upgrade）请求切换到WebSocket协议。下面显示的是一个升级请求：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /ws HTTP/1.1
Host: mail.ru
Connection: Upgrade
Sec-Websocket-Key: A3xNe7sEB9HixkmBhVrYaA==
Sec-Websocket-Version: 13
Upgrade: websocket
HTTP/1.1 101 Switching Protocols
Connection: Upgrade
Sec-Websocket-Accept: ksu0wXWG+YmkVx+KQR2agP0cQn4=
Upgrade: websocket
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们接收HTTP请求和它的头部只是为了切换到WebSocket协议，而http.Request 里保存了所有头部的数据。从这里可以得到启发，如果是为了优化，我们可以放弃使用标准的net/http 服务并在处理HTTP请求的时候避免无用的内存分配和拷贝。&lt;/p&gt;

&lt;p&gt;举个例子，http.Request 包含了一个叫做Header的字段。标准net/http 服务会将请求里的所有头部数据全部无条件地拷贝到Header字段里。你可以想象这个字段会保存许多冗余的数据，例如一个包含很长cookie的头部。&lt;/p&gt;

&lt;p&gt;我们如何来优化呢？&lt;/p&gt;

&lt;p&gt;WebSocket实现&lt;/p&gt;

&lt;p&gt;不幸的是，在我们优化服务端的时候所有能找到的库只支持对标准net/http 服务做升级。而且没有一个库允许我们实现上面提到的读和写的优化。为了使这些优化成为可能，我们必须有一套底层的API来操作WebSocket。为了重用缓存，我们需要类似下面这样的协议函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ReadFrame(io.Reader) (Frame, error)
func WriteFrame(io.Writer, Frame) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我们有一个包含这样API的库，我们就按照下面的方式从连接上读取packets：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// getReadBuf, putReadBuf are intended to
// reuse *bufio.Reader (with sync.Pool for example).
func getReadBuf(io.Reader) *bufio.Reader
func putReadBuf(*bufio.Reader)
// readPacket must be called when data could be read from conn.
func readPacket(conn io.Reader) error {
    buf := getReadBuf()
    defer putReadBuf(buf)
    buf.Reset(conn)
    frame, _ := ReadFrame(buf)
    parsePacket(frame.Payload)
    //...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简而言之，我们需要自己写一个库。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;github.com/gobwas/ws
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ws 库的主要设计思想是不将协议的操作逻辑暴露给用户。所有读写函数都接受通用的io.Reader 和io.Writer 接口。因此它可以随意搭配是否使用缓存以及其它I/O的库。&lt;/p&gt;

&lt;p&gt;除了标准库net/http 里的升级请求，ws 还支持零拷贝升级。它能够处理升级请求并切换到WebSocket模式而不产生任何内存分配或者拷贝。ws.Upgrade() 接受io.ReadWriter （net.Conn 实现了这个接口）。换句话说，我们可以使用标准的net.Listen() 函数然后把从ln.Accept() 收到的连接马上交给ws.Upgrade() 去处理。库也允许拷贝任何请求数据来满足将来应用的需求（举个例子，拷贝Cookie 来验证一个session）。&lt;/p&gt;

&lt;p&gt;下面是处理升级请求的性能测试：标准net/http 库的实现和使用零拷贝升级的net.Listen() ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BenchmarkUpgradeHTTP    5156 ns/op    8576 B/op    9 allocs/op
BenchmarkUpgradeTCP     973 ns/op     0 B/op       0 allocs/op
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用ws 以及零拷贝升级为我们节省了24 GB的空间。这些空间原本被用做net/http 里处理请求的I/O缓存。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;回顾&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;让我们来回顾一下之前提到过的优化：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;一个包含缓存的读goroutine会占用很多内存。方案： netpoll（epoll, kqueue）；重用缓存。
一个包含缓存的写goroutine会占用很多内存。方案： 在需要的时候创建goroutine；重用缓存。
存在大量连接请求的时候，netpoll不能很好工作。方案： 重用goroutines并且限制它们的数目。
net/http 对升级到WebSocket请求的处理不是最高效的。方案： 在TCP连接上实现零拷贝升级。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是服务端的大致实现代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;net&amp;quot;
    &amp;quot;github.com/gobwas/ws&amp;quot;
)
ln, _ := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:8080&amp;quot;)
for {
    // Try to accept incoming connection inside free pool worker.
    // If there no free workers for 1ms, do not accept anything and try later.
    // This will help us to prevent many self-ddos or out of resource limit cases.
    err := pool.ScheduleTimeout(time.Millisecond, func() {
        conn := ln.Accept()
        _ = ws.Upgrade(conn)
        // Wrap WebSocket connection with our Channel struct.
        // This will help us to handle/send our app&#39;s packets.
        ch := NewChannel(conn)
        // Wait for incoming bytes from connection.
        poller.Start(conn, netpoll.EventRead, func() {
            // Do not cross the resource limits.
            pool.Schedule(func() {
                // Read and handle incoming packet(s).
                ch.Recevie()
            })
        })
    })
    if err != nil {   
        time.Sleep(time.Millisecond)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;结论&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在程序设计时，过早优化是万恶之源。&lt;/p&gt;

&lt;p&gt;上面的优化是有意义的，但不是所有情况都适用。举个例子，如果空闲资源（内存，CPU）与在线连接数之间的比例很高的话，优化就没有太多意义。当然，知道什么地方可以优化以及如何优化总是有帮助的。&lt;/p&gt;

&lt;p&gt;最终，原先每个连接平均占用 65KB的内存， 优化后只占10KB的内存，优化效果明显。&lt;/p&gt;

&lt;h1 id=&#34;性能问题&#34;&gt;性能问题&lt;/h1&gt;

&lt;p&gt;我们先看两个用Go做消息推送的案例实际处理能力&lt;/p&gt;

&lt;p&gt;60消息推送的数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;16台机器，标配：24个硬件线程，64GB内存 Linux Kernel 2.6.32 x86_64
单机80万并发连接，load 0.2~0.4，CPU 总使用率 7%~10%，内存占用20GB (res)
目前接入的产品约1280万在线用户
2分钟一次GC，停顿2秒 (1.0.3 的 GC 不给力，直接升级到 tip，再次吃螃蟹)
15亿个心跳包/天，占大多数。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;京东云消息推送系统(团队人数:4):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;单机并发tcp连接数峰值118w,内存占用23G(Res),Load 0.7左右
心跳包 4k/s
gc时间2-3.x s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出来性能很强劲。再来看看go中一些性能测试：&lt;/p&gt;

&lt;p&gt;1、管道chan吞吐极限10,000,000，单次Put,Get耗时大约100ns/op，无论是采用单Go程，还是多Go程并发(并发数:100, 10000, 100000)，耗时均没有变化，Go内核这对chan进行优化。&lt;/p&gt;

&lt;p&gt;解决之道：在系统设计时，避免使用管道chan传递主业务数据，避免将业务流程处理流程分割到对个Go程中执行，这样做减少chan传输耗时，和Go程调度耗时，性能会有很大的提升。&lt;/p&gt;

&lt;p&gt;案例分析：nsq和nats都是实时消息队列，nsq在客户端端和服务端大量使用chan转发消息，导致性能不佳，只有100,000／s；而nats服务端在分发消息流程中，没有使用chan，只在客户端接收时使用chan，性能可达到1,000,000／s。&lt;/p&gt;

&lt;p&gt;2、互斥锁Mutex在单Go程时Lock,Unlock耗时大约20ns/op，但是采用多Go程时，性能急剧下降，并发越大耗时越长，在Go1.5并发数达到1024耗时900ns/op，Go1.6优化到300ns/op，究其原因，是构建在CPU的原子操作之上，抢占过于频繁将导致，消耗大量CPU时钟，进而CPU多核无法并行。&lt;/p&gt;

&lt;p&gt;解决之道：采用分区，将需要互斥保护的数据，分成多个固定分区(建议是2的整数倍，如256)，访问时先定位分区(不互斥)，这样就可降低多个Go程竞争1个数据分区的概率。&lt;/p&gt;

&lt;p&gt;案例分析：Golang的Go程调度模块，在管理大量的Go程，使用的就是数据分区。&lt;/p&gt;

&lt;p&gt;3、select异步操作在单管道时耗时120ns/op，但是随着管道数增加，性能线性下降，每增加1个管道增加100ns/op，究其原因，slelect时当chan数超过1后，Go内部是创建一个Go程，有它每1ms轮训的方式检查每个chan是否可用，而不是采用事件触发。&lt;/p&gt;

&lt;p&gt;解决之道：在select中避免使用过多的管道chan分支，或者把无法用到的chan置为nil；解决select超时，避免使用单独的超时管道，应与数据返回管道共享。&lt;/p&gt;

&lt;p&gt;案例分析：nsq和nats都是实时消息队列，由于nsq大量使用chan，这就必然导致大量使用select对多chan操作，结果是性能不高。&lt;/p&gt;

&lt;p&gt;4、Go调度性能低下，当出现1,000,000Go程时，Go的调度器的性能急剧下降。&lt;/p&gt;

&lt;p&gt;解决之道：避免动态创建Go程，服务端收到数据并处理的流程中，避免使用chan传递业务数据，这样会引起Go程调度。&lt;/p&gt;

&lt;p&gt;案例分析：nsq和nats都是实时消息队列，由于nsq大量使用chan，这就必然导致在服务过程中，引起Go调度，结果是性能不高。&lt;/p&gt;

&lt;p&gt;5、defer性能不高，每次defer耗时100ns，，在一个func内连续出现多次，性能消耗是100ns*n，累计出来浪费的cpu资源很大的。&lt;/p&gt;

&lt;p&gt;解决之道：除了需要异常捕获时，必须使用defer；其它资源回收类defer，可以判断失败后，使用goto跳转到资源回收的代码区。&lt;/p&gt;

&lt;p&gt;6、内存管理器性能低下，申请16字节的内存，单次消耗30ns，64字节单次消耗70ns，随着申请内存尺寸的增长，耗时会迅速增长。加上GC的性能在1.4, 1.5是都不高，直到1.6, 1.7才得到改善。&lt;/p&gt;

&lt;p&gt;解决之道：建议使用pool，单次Put，Get的耗时大约在28ns，在并发情况下可达到18ns，比起每次创建，会节省很多的CPU时钟。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>golang使用系列---- go基本使用积累</title>
          <link>https://kingjcy.github.io/post/golang/go/</link>
          <pubDate>Sat, 23 Mar 2019 14:54:09 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go/</guid>
          <description>&lt;p&gt;学习使用go语言已经有一段时间了，积累了很多的经验，这边进行不断接触的知识点进行零散的整理并持续积累，也便于后期的备忘。&lt;/p&gt;

&lt;h1 id=&#34;为什么使用go&#34;&gt;为什么使用go？&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;1、相对于c/c++开发来说，go的封装和丰富的标准库，极大的提高了开发的效率。基本的服务端开发都可以使用golang来开发。
2、go的编译和执行效率相对来说很快，部署也很简单
3、go原生支持高并发这块也是很优秀的。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要使用领域：微服务解决方案（istio），云平台（k8s），web服务（各种web框架，http原生支持并发）&amp;mdash;服务端（高并发），区块链，分布式（存储，推送系统）&lt;/p&gt;

&lt;p&gt;Go语言的三位最初的缔造者 — Rob Pike、Robert Griesemer 和 Ken Thompson 中，Robert Griesemer 参与设计了Java的HotSpot虚拟机和Chrome浏览器的JavaScript V8引擎，Rob Pike 在大名鼎鼎的bell lab侵淫多年，参与了Plan9操作系统、C编译器以及多种语言编译器的设计和实现，Ken Thompson 更是图灵奖得主、Unix之父、C语言之父。这三人在计算机史上可是元老级别的人物，特别是 Ken Thompson ，是一手缔造了Unix和C语言计算机领域的上古大神，所以Go语言的设计哲学有着深深的Unix烙印：简单、模块化、正交、组合、pipe、功能短小且聚焦等；而令许多开发者青睐于Go的简洁、高效编程模式的原因，也正在于此。&lt;/p&gt;

&lt;h1 id=&#34;语言设计&#34;&gt;语言设计&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;传统的面向过程的语言开发，编译器其中最最基础和原始的目标之一就是把一份代码里的函数名称，转化成一个相对内存地址，把调用这个函数的语句转换成一个jmp跳转指令。在程序开始运行时候，调用语句可以正确跳转到对应的函数地址。直白，但是。。。太死板了&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;我们希望灵活，于是需要开发面向对象的语言,c++在c的基础上增加了类的部分,就是让编译器多绕个弯，在严格的c编译器上增加一层类处理的机制，把一个函数限制在它处在的class环境里，每次请求一个函数调用，先找到它的对象, 其类型,返回值，参数等等，确定了这些后再jmp跳转到需要的函数。这样很多程序增加了灵活性同样一个函数调用会根据请求参数和类的环境返回完全不同的结果。增加类机制后，就模拟了现实世界的抽象模式，不同的对象有不同的属性和方法。同样的方法，不同的类有不同的行为,还是死板, 我们仍然叫c++是static language。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;runtime环境注册所有全局的类，函数，变量等等信息等等，我们可以无限的为这个层增加必要的功能。调用函数时候，会先从这个运行时环境里检测所以可能的参数再做jmp跳转，这就是runtime。编译器开发起来比上面更加弯弯绕。但是这个层极大增加了程序的灵活性。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;尽管 Go 编译器产生的是本地可执行代码，这些代码仍旧运行在 Go 的 runtime（这部分的代码可以在 runtime
包中找到）当中。这个 runtime 类似 Java 和 .NET 语言所用到的虚拟机，它负责管理包括内存分配、垃圾回收、栈处理、goroutine、channel、切片（slice）、map 和反射（reflection）等等。&lt;/p&gt;

&lt;p&gt;runtime 主要由 C 语言编写（Go 1.5 开始自举），并且是每个 Go 包的最顶级包。你可以在目录
$GOROOT/src/runtime 中找到相关内容。&lt;/p&gt;

&lt;h1 id=&#34;基础语法&#34;&gt;基础语法&lt;/h1&gt;

&lt;p&gt;go语言的所有源代码都必须由unicode编码规范的UTF-8编码格式进行编码。下面是基础语法。&lt;/p&gt;

&lt;h2 id=&#34;变量&#34;&gt;变量&lt;/h2&gt;

&lt;p&gt;1.用关键字var来声明&lt;/p&gt;

&lt;p&gt;2.可以类型判断&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var i = 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.支持多重赋值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var width, height int = 100, 50 // 声明多个变量
i，j = j，i这样就完成了交换。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.可以直接用:=这个服务声明初始化，但必须不是声明过的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := 10类似于
var i int
i = 10；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5.:= 符号不能使用在函数外，在函数外必须要var进行声明。&lt;/p&gt;

&lt;p&gt;6.像err这种可以重复使用:=来定义，即使被声明过，但是要满足以下条件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;该变量在一个作用域内&lt;/li&gt;
&lt;li&gt;初始化中相应的值是可以赋给这个变量的&lt;/li&gt;
&lt;li&gt;声明中至少有一个其他的变量是被声明的新的变量&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;7.支持匿名变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func getname（）（firstname,lastname,nickname）{
}

调用可以用_来表达我们不想要的变量---&amp;gt;_,_,nickname := getname();
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;常量&#34;&gt;常量&lt;/h2&gt;

&lt;p&gt;1.iota 一个特殊的常量，出现一次自动加一，当出现const的时候会自动置零&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    A = iota    //0
    B           //1
    C = &amp;quot;a&amp;quot;     //a
    D           //a
    E = iota    //4
    F           //5
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.常量不能使用 := 语法定义&lt;/p&gt;

&lt;p&gt;3.枚举类型不需要enum&lt;/p&gt;

&lt;p&gt;正常定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const（
    Sunday = iota
    monday
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中的大写字母开头的在包外可用，小写字母开头的包内私有&lt;/p&gt;

&lt;p&gt;我们可以使用下划线跳过不想要的值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type AudioOutput int

const (
    OutMute AudioOutput = iota // 0
    OutMono                    // 1
    OutStereo                  // 2
    _
    _
    OutSurround                // 5
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.常量的定义格式:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const identifier [type] = value
[type]类型可以省略,编译器可以根据变量的值来推断其类型.
 显示类型 const b string = &amp;quot;abc&amp;quot;
 隐式类型 const b = &amp;quot;abc&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;运算符&#34;&gt;运算符&lt;/h2&gt;

&lt;p&gt;Go 编程语言支持以下按位运算符：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;amp;   bitwise AND
 |   bitwise OR
 ^   bitwise XOR
&amp;amp;^   AND NOT
&amp;lt;&amp;lt;   left shift
&amp;gt;&amp;gt;   right shift
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;amp; 运算符&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 Go 中， &amp;amp; 运算符在两个整型操作数中执行按位 AND 操作。AND 操作具有以下属性，只要有一个失败就是false:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var x uint8 = 0xAC    // x = 10101100
    x = x &amp;amp; 0xF0          // x = 10100000
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所有的位运算都支持简写的赋值形式。 例如，前面的例子可以重写为如下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var x uint8 = 0xAC    // x = 10101100
    x &amp;amp;= 0xF0             // x = 10100000
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外一个巧妙的技巧是：你可以用 &amp;amp; 操作去测试一个数字是奇数还是偶数。原因是当一个数字的二进制的最低位是 1 的时候，那他就是奇数。我们可以用一个数字和 1 进行 &amp;amp; 操作，如果的到的结果是 1 ，那么这个原始的数字就是奇数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/rand&amp;quot;
)
func main() {
    for x := 0; x &amp;lt; 100; x+&amp;gt;{
        num := rand.Int()
        if num&amp;amp;1 == 1 {
            fmt.Printf(&amp;quot;%d is odd\n&amp;quot;, num)
        } else {
            fmt.Printf(&amp;quot;%d is even\n&amp;quot;, num)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;| 操作符&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;| 对其整型操作数执行按位或操作。回想一下或（OR）操作符具备以下性质，只有有一个成立就是true：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var a uint8 = 0
    a |= 196
    fmt.Printf(&amp;quot;%b&amp;quot;, a)
}

// 打印结果  11000100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在使用位掩码技术为给定的整型数字设置任意位时，或运算非常有用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;位运算的配置用法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们可以结合使用 OR 和 AND 运算的方式来分别设置和读取某位的配置值。接下来的源码片段演示了这个操作。函数 procstr 会转换字符串的内容。它需要两个参数：第一个， str，是将要被转换的字符串，第二个， conf，是一个使用位掩码的方式指定多重转换配置的整数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    UPPER  = 1 // 大写字符串
    LOWER  = 2 // 小写字符串
    CAP    = 4 // 字符串单词首字母大写
    REV    = 8 // 反转字符串
)

func main() {
    fmt.Println(procstr(&amp;quot;HELLO PEOPLE!&amp;quot;, LOWER|REV|CAP))
}

func procstr(str string, conf byte) string {
    // 反转字符串
    rev := func(s string) string {
        runes := []rune(s)
        n := len(runes)
        for i := 0; i &amp;lt; n/2; i+&amp;gt;{
            runes[i], runes[n-1-i] = runes[n-1-i], runes[i]
        }
        return string(runes)
    }

    // 查询配置中的位操作
    if (conf &amp;amp; UPPER) != 0 {
        str = strings.ToUpper(str)
    }
    if (conf &amp;amp; LOWER) != 0 {
        str = strings.ToLower(str)
    }
    if (conf &amp;amp; CAP) != 0 {
        str = strings.Title(str)
    }
    if (conf &amp;amp; REV) != 0 {
        str = rev(str)
    }
    return str
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的 procstr(&amp;ldquo;HELLO PEOPLE!&amp;rdquo;, LOWER|REV|CAP) 方法会把字符串变成小写，然后反转字符串，最后把字符串里面的单词首字母变成大写。这个功能是通过设置 conf 里的第二，三，四位的值为 14 来完成的。然后代码使用连续的 if 语句块来获取这些位操作进行对应的字符串转换。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;^ 操作符&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 Go 中 按位 异或 操作是用 ^ 来表示的。 异或运算符有如下的特点，相同为false，相异为true，也叫不进位加法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var a uint16 = 0xCEFF
    a ^= 0xFF00 // same a = a ^ 0xFF00
}

// a = 0xCEFF   (11001110 11111111)
// a ^=0xFF00   (11001110 11111111 + 1111111100000000 = 00110001 11111111)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在前面的代码片段中，与 1 进行异或的位被翻转（从 0 到 1 或从 1 到 0）。&lt;/p&gt;

&lt;p&gt;异或 运算的一个实际用途，例如，可以利用 异或运算去比较两个数字的符号是否一样。当 (a ^ b) ≥ 0 （或相反符号的 (a ^ b) &amp;lt; 0 ）为 true 的时候，两个整数 a，b 具有相同的符号，如下面的程序所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    a, b := -12, 25
    fmt.Println(&amp;quot;a and b have same sign?&amp;quot;, (a ^ b) &amp;gt;= 0)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当执行上面这个程序的时候，将会打印出：a and b have same sign? false。&lt;/p&gt;

&lt;p&gt;^ 作为取反位运算符 （非）&lt;/p&gt;

&lt;p&gt;不像其他语言 （c/c++，Java，Python，Javascript等可能使用～取反）， Go 没有专门的一元取反位运算符。取而代之的是，XOR 运算符 ^，也可作为一元取反运算符作用于一个数字。对于给定位 x，在 Go 中 x = 1 ^ x 可以翻转该位。在以下的代码段中我们可以看到使用 ^a 获取变量 a 的取反值的操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var a byte = 0x0F
    fmt.Printf(&amp;quot;%08b\n&amp;quot;, a)
    fmt.Printf(&amp;quot;%08b\n&amp;quot;, ^a)
}

// 打印结果
00001111     // var a
11110000     // ^a
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;amp;^ 操作符&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;amp;^ 操作符意为 与非，是 与 和 非 操作符的简写形式，也就是先与操作再非操作。&lt;/p&gt;

&lt;p&gt;接下来的代码片段使用 AND NOT 操作符，将变量值 1010 1011 变为 1010 0000，清除了操作数上的低四位。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var a byte = 0xAB
     fmt.Printf(&amp;quot;%08b\n&amp;quot;, a)
     a &amp;amp;^= 0x0F
     fmt.Printf(&amp;quot;%08b\n&amp;quot;, a)
}

// 打印：
10101011
10100000
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;lt;&amp;lt;和&amp;gt;&amp;gt; 操作符&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;与其他 C 的衍生语言类似， Go 使用 &amp;lt;&amp;lt; 和 &amp;gt;&amp;gt; 来表示左移运算符和右移运算符，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Given integer operands a and n,
a &amp;lt;&amp;lt; n; shifts all bits in a to the left n times
a &amp;gt;&amp;gt; n; shifts all bits in a to the right n times
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如，在下面的代码片段中变量 a （00000011）的值将会左移位运算符分别移动三次。每次输出结果都是为了说明左移的目的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var a int8 = 3
    fmt.Printf(&amp;quot;%08b\n&amp;quot;, a)
    fmt.Printf(&amp;quot;%08b\n&amp;quot;, a&amp;lt;&amp;lt;1)
    fmt.Printf(&amp;quot;%08b\n&amp;quot;, a&amp;lt;&amp;lt;2)
    fmt.Printf(&amp;quot;%08b\n&amp;quot;, a&amp;lt;&amp;lt;3)
}

// 输出的结果:
00000011
00000110
00001100
00011000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意每次移动都会将低位右侧补零。相对应，使用右移位操作符进行运算时，每个位均向右方移动，空出的高位补零，如下示例 （有符号数除外，参考下面的算术移位注释）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
 var a uint8 = 120
 fmt.Printf(&amp;quot;%08b\n&amp;quot;, a)
 fmt.Printf(&amp;quot;%08b\n&amp;quot;, a&amp;gt;&amp;gt;1)
 fmt.Printf(&amp;quot;%08b\n&amp;quot;, a&amp;gt;&amp;gt;2)
}

// 打印：
01111000
00111100
00011110
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以利用左移和右移运算中，每次移动都表示一个数的 2 次幂这个特性，来作为某些乘法和除法运算的小技巧。例如，如下代码中，我们可以使用右移运算将 200（存储在变量 a 中）除以 2 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    a := 200
    fmt.Printf(&amp;quot;%d\n&amp;quot;, a&amp;gt;&amp;gt;1)
}

// 打印：
100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或是通过左移 2 位，将一个数乘以 4：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    a := 12
    fmt.Printf(&amp;quot;%d\n&amp;quot;, a&amp;lt;&amp;lt;2)
}
// 打印：

48
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;位移运算符提供了有趣的方式处理二进制值中特定位置的值。例如，下列的代码中，| 和 &amp;lt;&amp;lt; 用于设置变量 a 的第三个 bit 位。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var a int8 = 8
    fmt.Printf(&amp;quot;%08b\n&amp;quot;, a)
    a = a | (1&amp;lt;&amp;lt;2)
    fmt.Printf(&amp;quot;%08b\n&amp;quot;, a)
}
// prints:
00001000
00001100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者，您可以组合位移运算符和 &amp;amp; 测试是否设置了第 n 位，如下面示例所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var a int8 = 12
    if a&amp;amp;(1&amp;lt;&amp;lt;2) != 0 {
        fmt.Println(&amp;quot;take action&amp;quot;)
    }
}

// 打印:
take action
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 &amp;amp;^ 和位移运算符，我们可以取消设置一个值的某个位。例如，下面的示例将变量 a 的第三位置为 0 ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var a int8 = 13
    fmt.Printf(&amp;quot;%04b\n&amp;quot;, a)
    a = a &amp;amp;^ (1 &amp;lt;&amp;lt; 2)
    fmt.Printf(&amp;quot;%04b\n&amp;quot;, a)
}

// 打印:
1101
1001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当要位移的值（左操作数）是有符号值时，Go 自动应用算术位移。在右移操作期间，复制（或扩展）二进制补码符号位以填充位移的空隙。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;与其它现代运算符一样，Go 支持所有二进制位操作运算符。这篇文章仅仅提供了可以用这些操作符完成的各种黑科技示例。你可以在网络上找到很多文章，特别是 Sean Eron Anderson写的 Bit Twiddling Hacks。&lt;/p&gt;

&lt;h2 id=&#34;数组&#34;&gt;数组&lt;/h2&gt;

&lt;p&gt;数组其实是一段连续的内存，通过唯一索引下标（由于地址也是连续的，所以下标根据地址循序来就行，不需要存）来获取对应内存的值。&lt;/p&gt;

&lt;p&gt;1、Go语言的数组不同于C语言或者其他语言的数组，C语言的数组变量是指向数组第一个元素的指针；而Go语言的数组是一个值，Go语言中的数组是值类型，一个数组变量就表示着整个数组，意味着Go语言的数组在传递的时候，传递的是原数组的拷贝。
重点是值类型，每次传参都是一个副本，要传址，需要用数组切片&lt;/p&gt;

&lt;p&gt;2、遍历数组元素，除了用下标，也可以用关键字range，有两个返回值，一个是下标，一个是value,在对strings进行遍历的时候一个是下标，一个是ACSII&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var a [10]int  

var a = [10]int{0,1,2,3,4,5,6,7,8,9}  
var a = [...]int{0,1,2,3,4,5,6,7,8,9}  

var a = [2][2]int{[2]int{1,1}, [2]int{2,2}}  
var a = [2][2]int{{1,1}, {2,2}}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Golang动态数组&lt;/p&gt;

&lt;p&gt;数组申明&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var dynaArr []string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;动态添加成员&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dynaArr = append(dynaArr, &amp;quot;one&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边讲解一下append&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;就是将后面一个值拷贝一份到前面，如果是值就拷贝值，如果是地址就拷贝地址，所以当是地址的时候，源数据发现变化，append后的数据也会发生变化，变成重复的，所以传地址最好每次重新定义一个实例，或者就不要传输地址，定义一个实例就好。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import  (
    &amp;quot;fmt&amp;quot;
)

func main() {

    var dynaArr []string

    dynaArr = append(dynaArr, &amp;quot;one&amp;quot;)
    dynaArr = append(dynaArr, &amp;quot;two&amp;quot;)
    dynaArr = append(dynaArr, &amp;quot;three&amp;quot;)

    fmt.Println(dynaArr)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面这个输出是0，0，0，0，0，1，2，3,可见初始化的也是占空间的，所以赋值需要make&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    s := make([]int, 5)
    s = append(s, 1, 2, 3)
    fmt.Println(s)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、结构体数组&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import  (
    &amp;quot;fmt&amp;quot;
)

type A struct{
    Path    string
    Length  int 
}


func main() {

    var dynaArr []A


    t := A{&amp;quot;/tmp&amp;quot;, 1023}

    dynaArr = append(dynaArr, t)
    dynaArr = append(dynaArr, A{&amp;quot;~&amp;quot;, 2048})
    t.Path, t.Length = &amp;quot;/&amp;quot;, 4096
    dynaArr = append(dynaArr, t)

    fmt.Println(dynaArr)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意大小写，大写为公有，小写为私有&lt;/p&gt;

&lt;p&gt;5、数组切片&lt;/p&gt;

&lt;p&gt;切片是一个很小的对象，是对数组进行了抽象，并提供相关的操作方法。切片有三个属性字段：长度、容量和指向数组的指针。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/slice_1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图中，ptr指的是指向array的pointer，len是指切片的长度, cap指的是切片的容量。现在，我想你对数组和切片有了一个本质的认识。&lt;/p&gt;

&lt;p&gt;切片是数组的指针管理，数组就是存储数据的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;items := make([]interface{}, 0, len(proxyInfos))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;items只是一个指针，当给他一个新地址的时候，就会指向新的空间，原来的对象在函数执行完以后，GC 就可以工作，把对象释放掉，但是如果像slice删除这种操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return append(slice[:i], slice[i+1:]...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还是指向原来的数组的地址，就可以读取到这个地址下分配的所有空间，就会出现数据多余的情况，所以只要把slice当成指针就很好理解了。&lt;/p&gt;

&lt;p&gt;切记：slice是一个指针&lt;/p&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、在一个数组的基础上用[:]来创建--golang slice对数组是址引用，同用一个地址空间，slice变化时，原数组是发生变化的。slice基于数组建立的也是一种引用，当slice改变的时候，原来的数组也会变化，但是当slice空间翻倍了，就会重新开辟一片空间，就不会互相影响变化了

2、直接用make函数创建

创建一个初始元素个数为5的数组切片，元素初始值为0：
    mySlice1 := make([] int, 5)
创建一个初始元素个数为5的数组切片，元素初始值为0，并预留10个元素的存储空间：
    mySlice2 := make([] int, 5, 10)
直接创建并初始化包含5个元素的数组切片：
    mySlice3 := [] int{1, 2, 3, 4, 5}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;求数组长度的函数区别&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cap():分配的空间
len()：所占的元素，实际数据的长度
copy()：复制，以小的为准
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;切片优点&lt;/p&gt;

&lt;p&gt;可以动态增减元素，原始数组增减数据是要重新分配内存，然后将数据搬过去，这样比较消耗性能，但是数据切片则可以用cap（）知道分配的空间，然后充分利用，而后在内存不够会自动扩大内存。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;

func main() {
    a := make([]byte,2)
    b := make([]byte,4)

    c := []byte{1,2,3,4}
    b = c
    a = c

    fmt.Println(a,b,len(a),len(b),cap(a),cap(b))
}

$ go run slicedistribution.go
[1 2 3 4] [1 2 3 4] 4 4 4 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常我们我们还使用append，不用make来指定内存大小，直接在使用的时候就可以自动分配。&lt;/p&gt;

&lt;p&gt;切片扩容请记住以下两条规则：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;如果切片的容量小于1024个元素，那么扩容的时候slice的cap就翻番，乘以2；一旦元素个数超过1024个元素，增长因子就变成1.25，即每次增加原来容量的四分之一。
如果扩容之后，还没有触及原数组的容量，那么，切片中的指针指向的位置，就还是原数组，如果扩容之后，超过了原数组的容量，那么，Go就会开辟一块新的内存，把原来的值拷贝过来，这种情况丝毫不会影响到原数组。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上还有更加复杂的其他规则&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;append单个元素，或者append少量的多个元素，这里的少量指double之后的容量能容纳，这样就会走以下扩容流程，不足1024，双倍扩容，超过1024的，1.25倍扩容。
若是append多个元素，且double后的容量不能容纳，直接使用预估的容量。
敲重点！！！！此外，以上两个分支得到新容量后，均需要根据slice的类型size，算出新的容量所需的内存情况capmem，然后再进行capmem向上取整，得到新的所需内存，除上类型size，得到真正的最终容量,作为新的slice的容量。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于切片的比较&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;切片之间不允许比较。切片只能与nil值比较。
map之间不允许比较。map只能与nil值比较。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那怎么比较切片&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reflect比较的方法
func StringSliceReflectEqual(a, b []string) bool {
    return reflect.DeepEqual(a, b)
}

循环遍历比较的方法
func StringSliceEqual(a, b []string) bool {
    if len(a) != len(b) {
        return false
    }

    if (a == nil) != (b == nil) {
        return false
    }

    for i, v := range a {
        if v != b[i] {
            return false
        }
    }

    return true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、slice基本操作增删改查：&lt;/p&gt;

&lt;p&gt;删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func remove(slice []interface{}, i int) []interface{} {
    //    copy(slice[i:], slice[i+1:])
    //    return slice[:len(slice)-1]
    return append(slice[:i], slice[i+1:]...)
}


获取新值的时候，必须使用append后的值来赋值，虽然slice是引用，原来的值删除是使用append，最后一个值还是存在的，这样就会出现最后的值一直重复，大小不变。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;循环删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;由于每次删除后对应的数组是变化的，并不是保持原来的遍历，所以最好不要使用range，使用i来控制，如下

chars := []string{&amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;}

for i := 0; i &amp;lt; len(chars); i+&amp;gt;{
    if chars[i] == &amp;quot;a&amp;quot; {
        chars = append(chars[:i], chars[i+1:]...)
        i-- // form the remove item index to start iterate next item
    }
}

fmt.Printf(&amp;quot;%+v&amp;quot;, chars)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个关于下标的问题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;当数据循环到最后一个数据的时候会不会out of range--不会


func main() {
    var a = [10]int{0,1,2,3,4,5,6,7,8,9}

    b := a[1:5]

    b[1] = 10


    fmt.Println(a)

    fmt.Println(b)

    c := append(a[:9],a[10:]...)
    fmt.Println(c)
    fmt.Println(a[10:])

}


chunyindeMacBook-Pro:test chunyinjiang$ go run array.go
[0 1 10 3 4 5 6 7 8 9]
[1 10 3 4]
[0 1 10 3 4 5 6 7 8]
[]

slice切片包含前面的下标不包含后面的指标，最大下标可以是数组的数量，也就是比下标大一，所以上面的a[10:]不会出错，但是a[10]是不对的，a[11:]也是不对的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新增&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func add(slice []interface{}, value interface{}) []interface{} {
    return append(slice, value)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;插入&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func insert(slice *[]interface{}, index int, value interface{}) {
    rear := append([]interface{}{}, (*slice)[index:]...)
    *slice = append(append((*slice)[:index], value), rear...)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func update(slice []interface{}, index int, value interface{}) {
    slice[index] = value
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查找&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func find(slice []interface{}, index int) interface{} {
    return slice[index]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;清空slice&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func empty(slice *[]interface{}) {
    //    *slice = nil
    *slice = append([]interface{}{})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;遍历&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func list(slice []interface{}) {
    for _, v := range slice {
        fmt.Printf(&amp;quot;%d &amp;quot;, v)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;range slice是会获取到cap slice的所有的值，所以在遍历删除slice中的元素的时候，不要使用range，使用for正常的循环&lt;/p&gt;

&lt;p&gt;7、注意点&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;b[1:4]就生成了一个新切片，切片元素范围为1,2,3，不包括4.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;切片并不是值拷贝，而是引用拷贝。对切片的操作就会影响到原始数组中的值&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;切片和数组的下标都是重0开始的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;cannot use []int literal (type []int) as type int in append&lt;/p&gt;

&lt;p&gt;他会告诉你 正常的使用应该是int 类型而不是[]int 类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;test = append(test, []int{5, 6, 7})
//正确的玩法 切记记得加 3个点
test = append(test, []int{5, 6, 7}...)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;slice的引用&lt;/p&gt;

&lt;p&gt;slice是引用类型，在内存中并没有属于自己的内存空间，而是通过指针指向进行切片的队列。由于队列分配的内存空间是连续的，所以如果slice的最后一个元素不是list的最后一个元素，那么在append的时候，新追加的元素就会覆盖掉原数组的元素。而由于slice是指针组织的，所以这个list的所有slice都会被影响。如果切片末尾元素就是队列的末尾元素，返回的 slice 数组指针将指向这个空间，而原数组的内容将保持不变，其它引用此数组的 slice 则不受影响。&lt;/p&gt;

&lt;p&gt;Slice是引用类型，指向的都是内存中的同一块内存，不过在实际应用中，有的时候却会发生“意外”，这种情况只有在像切片append元素的时候出现，Slice的处理机制是这样的，当Slice的容量还有空闲的时候，append进来的元素会直接使用空闲的容量空间，但是一旦append进来的元素个数超过了原来指定容量值的时候，内存管理器就是重新开辟一个更大的内存空间，用于存储多出来的元素，并且会将原来的元素复制一份，放到这块新开辟的内存空间。可以看到执行了append操作如果扩容后，内存地址发生了变化，说明已经不是引用传递。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这边对引用类型做一个总结，golang中有哪些是引用类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;切片（slice）
字典（map）
通道（channel）
接口（interface）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8、数组和切片的对比&lt;/p&gt;

&lt;p&gt;数组&lt;/p&gt;

&lt;p&gt;在Go中，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数组是值。将一个数组赋值给另一个，会拷贝所有的元素。 
特别是，如果你给函数传递一个数组，其将收到一个数组的拷贝，而不是它的指针。
数组的大小是其类型的一部分。类型 [10]int 和 [20]int 是不同的。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;切片&lt;/p&gt;

&lt;p&gt;切片持有对底层数组的引用，如果你将一个切片赋值给另一个，二者都将引用同一个数组。如果函数接受 一个切片参数，那么其对切片的元素所做的改动，对于调用者是可见的，好比是传递了一个底层数组的指 针。&lt;/p&gt;

&lt;p&gt;关于接收者对指针和值 的规则是这样的，值方法可以在指针和值上进行调用，而指针方法只能在指针上调用。这是因为指针方法 可以修改接收者;使用拷贝的值来调用它们，将会导致那些修改会被丢弃。&lt;/p&gt;

&lt;p&gt;9、数组传递地址的方法（二维数组也是一样）&lt;/p&gt;

&lt;p&gt;If you’re reading this post you’re probably searching on Google how to solve this problem: you’re passing a pointer to a slice or map in a function, and when referencing an item with *variable[0], you get that error.&lt;/p&gt;

&lt;p&gt;How do I solve it?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The solution is simple: instead of using

*variable[0]
use

(*variable)[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Why am I getting this weird error?&lt;/p&gt;

&lt;p&gt;*variable[0] is interpreted by the Go compiler as *(variable[0]). So what you’re telling the compiler to do is, get the first element in the slice, or the map item with key 0, and dereference that pointer.&lt;/p&gt;

&lt;p&gt;This explains the error: variable in that context is a pointer, not a value, so you cannot get the [0] item of a pointer to an address, you need to dereference it first to get the value, which is what I think you are trying to do in the first place.&lt;/p&gt;

&lt;p&gt;10、真实空间&lt;/p&gt;

&lt;p&gt;我们可以直接通过这个实例来看切片在数组上的建立和append的区别&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
)
func main() {
    str1 := []string{&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;}
    str2 := str1[1:]
    str2[1] = &amp;quot;new&amp;quot;
    fmt.Println(str1)
    str2 = append(str2, &amp;quot;z&amp;quot;, &amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)
    fmt.Println(str1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[a b new]
[a b new]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，golang 中的切片底层其实使用的是数组。当使用str1[1:] 使，str2 和 str1 底层共享一个数组，这回导致 str2[1] = &amp;ldquo;new&amp;rdquo; 语句影响 str1。而 append 会导致底层数组扩容，生成新的数组，因此追加数据后的 str2 不会影响 str1。&lt;/p&gt;

&lt;p&gt;11、数组只能与相同纬度长度以及类型的其他数组比较，切片之间不能直接比较。。&lt;/p&gt;

&lt;h2 id=&#34;map&#34;&gt;map&lt;/h2&gt;

&lt;p&gt;map就是k/v的映射，map持有对底层数据结构的引用。如果将map传递给函数，其对map的内容做了改 变，则这 些改变对于调用者是可见的。&lt;/p&gt;

&lt;p&gt;定义申明&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;map[key] = value
定义 var a map[string]value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 先声明map
var m1 map[string]string
// 再使用make函数创建一个非nil的map，nil map不能赋值
m1 = make(map[string]string)
// 最后给已声明的map赋值
m1[&amp;quot;a&amp;quot;] = &amp;quot;aa&amp;quot;
m1[&amp;quot;b&amp;quot;] = &amp;quot;bb&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;map元素查找&lt;/p&gt;

&lt;p&gt;在Go语言中，map的查找功能设计得比较精巧。判断是否成功找到特定的键，不需要检查取到的值是否为nil，只需查看第二个返回值。要从map中查找一个特定的键，可以通过下面的代码来实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;value, ok := myMap[&amp;quot;1234&amp;quot;]
if ok{
    //处理找到的value
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;map不像array和基础类型在你定义就会给你初始化一个默认值,所以在使用map进行赋值的时候，必须先对map进行初始化：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conf.Scripts = make(map[string]*Script)
tmpScript.Cmd = value.(string)
conf.Scripts[metric] = &amp;amp;tmpScript
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;未初始化会报错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; assignment to entry in nil map
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要删除一个map项，使用 delete 内建函数，其参数为map和要删除的key。即使key已经不在map中， 这样做也是安全的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;delete(timeZone, &amp;quot;PDT&amp;quot;)  // Now on Standard Time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现map的有序输出&lt;/p&gt;

&lt;p&gt;就是人为的对key进行排序，然后遍历key，或者先遍历再排序。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sort&amp;quot;
)

func main() {
    /* 声明一个字符串切片，存储map的key值 */
    var Name []string
    Name = append(Name, &amp;quot;Bob&amp;quot;, &amp;quot;Andy&amp;quot;, &amp;quot;Clark&amp;quot;, &amp;quot;David&amp;quot;, &amp;quot;Ella&amp;quot;)

    /* 声明索引类型为字符串的map */
    var Person = make(map[string]string)
    Person[&amp;quot;Bob&amp;quot;] = &amp;quot;B&amp;quot;
    Person[&amp;quot;Andy&amp;quot;] = &amp;quot;A&amp;quot;
    Person[&amp;quot;Clark&amp;quot;] = &amp;quot;C&amp;quot;
    Person[&amp;quot;David&amp;quot;] = &amp;quot;D&amp;quot;
    Person[&amp;quot;Ella&amp;quot;] = &amp;quot;E&amp;quot;

    fmt.Println(&amp;quot;未排序输出:&amp;quot;)
    for key, value := range Person {
        fmt.Println(key, &amp;quot;:&amp;quot;, value)
    }

    /* 对slice数组进行排序，然后就可以根据key值顺序读取map */
    sort.Strings(Name)
    fmt.Println(&amp;quot;排序输出:&amp;quot;)
    for _, Key := range Name {
        /* 按顺序从MAP中取值输出 */
        if Value, ok := Person[Key]; ok {
            fmt.Println(Key, &amp;quot;:&amp;quot;, Value)
        }
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Go 原生的 map 数据类型是非并发安全的，在go1.9开始发布了sync.map是线程安全的。&lt;/p&gt;

&lt;p&gt;sync.map&lt;/p&gt;

&lt;p&gt;在golang1.9以前，我们都是使用sync.RWMutex枷锁来实现线程安全&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package beego

import (
    &amp;quot;sync&amp;quot;
)



type BeeMap struct {
    lock *sync.RWMutex
    bm   map[interface{}]interface{}

}



func NewBeeMap() *BeeMap {
    return &amp;amp;BeeMap{
        lock: new(sync.RWMutex),
        bm:   make(map[interface{}]interface{}),
    }
}



//Get from maps return the k&#39;s value
func (m *BeeMap) Get(k interface{}) interface{} {
    m.lock.RLock()
    defer m.lock.RUnlock()
    if val, ok := m.bm[k]; ok {
        return val
    }
    return nil
}



// Maps the given key and value. Returns false
// if the key is already in the map and changes nothing.
func (m *BeeMap) Set(k interface{}, v interface{}) bool {
    m.lock.Lock()
    defer m.lock.Unlock()
    if val, ok := m.bm[k]; !ok {
        m.bm[k] = v
    } else if val != v {
        m.bm[k] = v
    } else {
        return false
    }
    return true
}



// Returns true if k is exist in the map.
func (m *BeeMap) Check(k interface{}) bool {
    m.lock.RLock()
    defer m.lock.RUnlock()
    if _, ok := m.bm[k]; !ok {
        return false
    }
    return true
}



func (m *BeeMap) Delete(k interface{}) {
    m.lock.Lock()
    defer m.lock.Unlock()
    delete(m.bm, k)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在项目中还可以通过数组、map、sync.RWMutex来实现普通map的并发读写，采用map数组，把key hash到相应的map，每个map单独加锁以降低锁的粒度&lt;/p&gt;

&lt;p&gt;那么golang sync.map是如何实现线程安全的呢？&lt;/p&gt;

&lt;p&gt;简单总结就是使用了互斥量和原子操作，用空间换时间。使用无锁读和读写分离的方式，详细了解可以看&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-map/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;数据类型&#34;&gt;数据类型&lt;/h2&gt;

&lt;p&gt;下面是 Go 支持的基本类型：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;bool&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数字类型&lt;/p&gt;

&lt;p&gt;数字类型有很多&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int8, int16, int32, int64, int--有符号
int：根据不同的底层平台（Underlying Platform），表示 32 或 64 位整型。除非对整型的大小有特定的需求，否则你通常应该使用 int 表示整型。

uint8, uint16, uint32, uint64, uint--无符号
float32, float64
complex64, complex128
complex64：实部和虚部都是 float32 类型的的复数。
complex128：实部和虚部都是 float64 类型的的复数。

内建函数 complex 用于创建一个包含实部和虚部的复数。complex 函数的定义如下：

func complex(r, i FloatType) ComplexType
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;byte&lt;/p&gt;

&lt;p&gt;byte 是 uint8 的别名。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;rune&lt;/p&gt;

&lt;p&gt;rune 是 int32 的别名。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;string&lt;/p&gt;

&lt;p&gt;Go 有着非常严格的强类型特征。Go 没有自动类型提升或类型转换&lt;/p&gt;

&lt;p&gt;比如在 C 语言中是完全合法的，然而在 Go 中，却是行不通的。i 的类型是 int ，而 j 的类型是 float64 ，我们正试图把两个不同类型的数相加，Go 不允许这样的操作。&lt;/p&gt;

&lt;p&gt;数据类型转化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {  
    var e interface{}  
    e = 10  
    switch v := e.(type) {  
    case int:  
        fmt.Println(&amp;quot;整型&amp;quot;, v)    
        var s int  
        s = v  
        fmt.Println(s)  
    case string:  
        fmt.Println(&amp;quot;字符串&amp;quot;, v)  
    }  
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边有一个重点，&amp;rdquo;type&amp;rdquo;只能用与interface，下面这个就是错误的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    i := GetValue()

    switch i.(type) {
    case int:
        println(&amp;quot;int&amp;quot;)
    case string:
        println(&amp;quot;string&amp;quot;)
    case interface{}:
        println(&amp;quot;interface&amp;quot;)
    default:
        println(&amp;quot;unknown&amp;quot;)
    }

}

func GetValue() int {
    return 1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;golang 中字符串是不能赋值 nil 的，也不能跟 nil 比较。&lt;/p&gt;

&lt;p&gt;字符串转成切片，会产生拷贝。严格来说，只要是发生类型强转都会发生内存拷贝。可以通过使用unsafe标准库来实现不拷贝来提高性能。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;字符类型&lt;/p&gt;

&lt;p&gt;在Go语言中支持两个字符类型，一个是 byte （实际上是 uint8 的别名），代表UTF-8字符串的单个字节的值；
另一个是 rune ，代表单个Unicode字符。出于简化语言的考虑，Go语言的多数API都假设字符串为UTF-8编码。
尽管Unicode字符在标准库中有支持，但实际上较少使用。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;语句&#34;&gt;语句&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;控制语句&lt;/p&gt;

&lt;p&gt;顺序&lt;/p&gt;

&lt;p&gt;选择&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if条件中初始化的变量可以在else中使用
switch可以获取接口变量的动态类型
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;支持for，不支持while和do_while
goto语句---&amp;gt;跳到标志位
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;函数&lt;/p&gt;

&lt;p&gt;1.关键字func、函数名、参数列表、返回值、函数体和返回语句。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Add(a int, b int) (ret int, err error) {
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.函数调用&lt;/p&gt;

&lt;p&gt;需要先牢记这样的规则：小写字母开头的函数只在本包内可见，大写字母开头的函数才能被其他包使用。调用模式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;n, err := Foo(0)
if err != nil {
 // 错误处理
} else {
 // 使用返回值n
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.不定参数&lt;/p&gt;

&lt;p&gt;传入参数的数量不定&lt;/p&gt;

&lt;p&gt;语法糖：&amp;hellip;type&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func myfunc(args ... int) {
}
如果是不同的类型则用interface{}
func myfunc(args ... interface{}) {
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你的 slice 已经有了多个值，想把它们作为变参使用，你要这样调用 func(slice&amp;hellip;)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nums := []int{1, 2, 3, 4}
myfunc(noms…)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;什么是可变参数函数？
可变参数函数即其参数数量是可变的 —— 0 个或多个。声明可变参数函数的方式是在其参数类型前带上省略符（三个点）前缀。也就是不定参数&lt;/p&gt;

&lt;p&gt;原理：
可变参数函数会在其内部创建一个”新的切片”。事实上，可变参数是一个简化了切片类型参数传入的语法糖。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;names ...string-----&amp;gt;names := []string{}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可变参数的使用场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;避免创建仅作传入参数用的临时切片
当参数数量未知
传达你希望增加可读性的意图
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用&lt;/p&gt;

&lt;p&gt;1.传入已有的切片&lt;/p&gt;

&lt;p&gt;你可以通过向一个已有的切片添加可变参数运算符 ”…“ 后缀的方式将其传入可变参数函数。函数会在内部直接使用这个传入的切片，并不会创建一个的新的。&lt;/p&gt;

&lt;p&gt;2.一些切片传入后的特异表现&lt;/p&gt;

&lt;p&gt;传入的切片是会被改变的，源切片也同时被改变&lt;/p&gt;

&lt;p&gt;这是因为，传入的切片和函数内部使用的切片共享同一个底层数组，因此在函数内部改变这个数组的值同样会影响到传入的切片：&lt;/p&gt;

&lt;p&gt;3.返回传入的切片&lt;/p&gt;

&lt;p&gt;返回值的类型不可以是可变参数的形式，但你可以将它作为一个切片返回：&lt;/p&gt;

&lt;p&gt;你也可以像下面这样将数组转化成切片后传入可变参数函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;names := [2]string{&amp;quot;carl&amp;quot;, &amp;quot;sagan&amp;quot;}

toFullname(names[:]...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;[]string 和 …string 是不可互相替代的&lt;/p&gt;

&lt;p&gt;可以通过将非可变参数置于可变参数前面的方式来混合使用它们，不能在可变参数之后再声明参数&lt;/p&gt;

&lt;p&gt;4.支持多返回值，改进了c中笨拙的语言风格&lt;/p&gt;

&lt;p&gt;5.支持在函数参数中给返回值命名&lt;/p&gt;

&lt;p&gt;6.支持匿名函数&lt;/p&gt;

&lt;p&gt;7.参数类型&lt;/p&gt;

&lt;p&gt;Go语言的函数调用参数全部是传值的, 包括 slice/map/chan 在内所有类型, 没有传引用的说法.但是slice/map/chan是引用类型，因为在make的时候返回的就是指针，所以是变成了地址引用&lt;/p&gt;

&lt;p&gt;8.传值和传指针&amp;ndash;和c++一样&lt;/p&gt;

&lt;p&gt;当我们传一个参数值到被调用函数里面时，实际上是传了这个值的一份copy，当在被调用函数中修改参数值的时候， 调用函数中相应实参不会发生任何变化，因为数值变化只作用在copy上。
为了验证我们上面的说法，我们来看一个例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main import &amp;quot;fmt&amp;quot;
//简单的一个函数，实现了参数+1的操作 func add1(a int) int {
a = a+1 // 我们改变了a的值
return a //返回一个新值 }
func main() { x := 3
fmt.Println(&amp;quot;x = &amp;quot;, x) // 应该输出 &amp;quot;x = 3&amp;quot; x1 := add1(x) //调用add1(x)
fmt.Println(&amp;quot;x+1 = &amp;quot;, x1) // 应该输出&amp;quot;x+1 = 4&amp;quot;
fmt.Println(&amp;quot;x = &amp;quot;, x) // 应该输出&amp;quot;x = 3&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看到了吗?虽然我们调用了add1函数，并且在add1中执行a = a+1操作，但是上面例子中x变量的值没有发生变化 理由很简单:因为当我们调用add1的时候，add1接收的参数其实是x的copy，而不是x本身。 那你也许会问了，如果真的需要传这个x本身,该怎么办呢?
这就牵扯到了所谓的指针。我们知道，变量在内存中是存放于一定地址上的，修改变量实际是修改变量地址处的内 存。只有add1函数知道x变量所在的地址，才能修改x变量的值。所以我们需要将x所在地址&amp;amp;x传入函数，并将函数 的参数的类型由int改为*int，即改为指针类型，才能在函数中修改x变量的值。此时参数仍然是按copy传递的，只 是copy的是一个指针。请看下面的例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main import &amp;quot;fmt&amp;quot;
//简单的一个函数，实现了参数+1的操作 func add1(a *int) int { // 请注意，
*a = *a+1 // 修改了a的值
return *a // 返回新值 }
func main() { x := 3
fmt.Println(&amp;quot;x = &amp;quot;, x) // 应该输出 &amp;quot;x = 3&amp;quot; x1 := add1(&amp;amp;x) // 调用 add1(&amp;amp;x) 传x的地址
fmt.Println(&amp;quot;x+1 = &amp;quot;, x1) // 应该输出 &amp;quot;x+1 = 4&amp;quot;
fmt.Println(&amp;quot;x = &amp;quot;, x) // 应该输出 &amp;quot;x = 4&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，我们就达到了修改x的目的。那么到底传指针有什么好处呢?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;传指针使得多个函数能操作同一个对象。
传指针比较轻量级 (8bytes),只是传内存地址，我们可以用指针传递体积大的结构体。如果用参数值传递的 话, 在每次copy上面就会花费相对较多的系统开销(内存和时间)。所以当你要传递大的结构体的时候，用 指针是一个明智的选择。 Go语言中string，slice，map这三种类型的实现机制类似指针，所以可以直接传递，而不用取地址后传递 指针。(注:若函数需改变slice的长度，则仍需要取地址传递指针)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;错误处理&lt;/p&gt;

&lt;p&gt;error接口&lt;/p&gt;

&lt;p&gt;两个函数panic，recover&amp;ndash;这是一个强大的工具，应该减少使用&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;关键字defer,panic,recover&lt;/p&gt;

&lt;p&gt;1.defer 语句会延迟函数的执行直到上层函数返回。&lt;/p&gt;

&lt;p&gt;2.defer要求获得一个函数的实例，return只需要类型定义。&lt;/p&gt;

&lt;p&gt;3.defer 是后进先出。也就是最后的语句最先执行。&lt;/p&gt;

&lt;p&gt;4.panic 需要等defer 结束后才会向上传递。 出现panic恐慌时候，会先按照defer的后入先出的顺序执行，最后才会执行panic。&lt;/p&gt;

&lt;p&gt;5.Defer调用的函数可以在返回语句执行后读取或修改命名的返回值.&lt;/p&gt;

&lt;p&gt;6.defer语句用于延迟一个函数或者方法（或者当前所创建的匿名函数）的执行，它会在外围函数或者方法返回之前但是其返回值（如果有的话）计算之后执行。这样就有可能在一个被延迟执行的函数内部修改外围函数的命名返回值&lt;/p&gt;

&lt;p&gt;7.recover方法只可以在defer方法中调用，这是因为panic链的方法中只有defer方法可以被执行。&lt;/p&gt;

&lt;p&gt;8.如果recover方法被调用，但是没有任何的panic发生，recover方法只会返回nil。如果有panic发生，那么panic就停止并且给panic的赋值会被返回。&lt;/p&gt;

&lt;p&gt;一个真实的panic 和 recover配合使用的用例可以参考标准库: json package. 它提供JSON格式的解码, 当 遇到非法格式的输入时会抛出panic异常, 然后panicking扩散到上一级调用者堆栈, 由上一级调用者通过recover捕获panic和错误信息(参考 decode.go 中的 &amp;lsquo;error&amp;rsquo; 和 &amp;lsquo;unmarshal&amp;rsquo;).&lt;/p&gt;

&lt;p&gt;Go库的实现习惯: 即使在pkg内部使用了panic, 但是在导出API时会被转化为明确的错误值.&lt;/p&gt;

&lt;p&gt;下面这个函数检查作为其参数的函数在执行时是否会产生panic:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func throwsPanic(f func()) (b bool) {
    defer func() {
        if x := recover(); x != nil {
            b = true
        }
    }()

    f() //执行函数f，如果f中出现了panic，那么就可以恢复回来
    return
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;面向对象&lt;/p&gt;

&lt;p&gt;很直接，没有隐藏this指针&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a Integer) Less(b Integer) bool { // 面向对象
return a &amp;lt; b
}
func Integer_Less(a Integer, b Integer) bool { // 面向过程
return a &amp;lt; b
}
a.Less(2) // 面向对象的用法
Integer_Less(a, 2) // 面向过程的用法
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;struct&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;结构体与数组一样，是复合类型，无论是作为实参传递给函数时，还是赋值给其他变量，都是值传递，即复一个副本。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一个结构体，并没有包含自身，比如Member中的字段不能是Member类型，但却可能是*Member。&lt;/p&gt;

&lt;p&gt;一个命名为S的结构体类型将不能再包含S类型的成员：因为一个聚合的值不能包含它自身。（该限制同样适应于数组。）但是S类型的结构体可以包含*S指针类型的成员，这可以让我们创建递归的数据结构，比如链表和树结构等&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type tree struct {
    value       int
    left, right *tree
}

// Sort sorts values in place.
func Sort(values []int) {
    var root *tree
    for _, v := range values {
        root = add(root, v)
    }
    appendValues(values[:0], root)
}

// appendValues appends the elements of t to values in order
// and returns the resulting slice.
func appendValues(values []int, t *tree) []int {
    if t != nil {
        values = appendValues(values, t.left)
        values = append(values, t.value)
        values = appendValues(values, t.right)
    }
    return values
}

func add(t *tree, value int) *tree {
    if t == nil {
        // Equivalent to return &amp;amp;tree{value: value}.
        t = new(tree)
        t.value = value
        return t
    }
    if value &amp;lt; t.value {
        t.left = add(t.left, value)
    } else {
        t.right = add(t.right, value)
    }
    return t
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;STRUCT基本和c差不多，也有继承采取了组合的文法&amp;ndash;通过结构体继承，没有private，public关键字来保护，而是通过字母大小写来处理&lt;/p&gt;

&lt;p&gt;初始化常用方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rect1 := new(Rect)
rect2 := &amp;amp;Rect{}
rect3 := &amp;amp;Rect{0, 0, 100, 200}
rect4 := &amp;amp;Rect{width: 100, height: 200}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意这几个变量全部为指向Rect结构的指针(指针变量)，因为使用了new()函数和&amp;amp;操作符．而如果使用方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := Rect{}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则表示这个是一个Rect{}类型．两者是不一样的．&lt;/p&gt;

&lt;p&gt;未显式初始化的都是对应的零值&lt;/p&gt;

&lt;p&gt;构造函数用一个全局函数NEWXXXX来表示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewRect(x, y, width, height float64) *Rect {
    return &amp;amp;Rect{x, y, width, height}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;struct可以实现golang的面向对象编程，每个struct的对应的实现函数 ，就是相当于这个struct的成员函数，就像访问自己的成员一样。&lt;/p&gt;

&lt;p&gt;继承也是使用struct的组合实现的&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Animal struct {
    Name   string  //名称
    Color  string  //颜色
    Height float32 //身高
    Weight float32 //体重
    Age    int     //年龄
}
//奔跑
func (a Animal)Run() {
    fmt.Println(a.Name &amp;gt;&amp;quot;is running&amp;quot;)
}
//吃东西
func (a Animal)Eat() {
    fmt.Println(a.Name &amp;gt;&amp;quot;is eating&amp;quot;)
}

type Cat struct {
    a Animal
}

func main() {
    var c = Cat{
        a: Animal{
            Name:   &amp;quot;猫猫&amp;quot;,
            Color:  &amp;quot;橙色&amp;quot;,
            Weight: 10,
            Height: 30,
            Age:    5,
        },
    }
    fmt.Println(c.a.Name)
    c.a.Run()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;匿名组合，就是不使用变量名，Go语言支持直接将类型作为结构体的字段，而不需要取变量名，这种字段叫匿名字段&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Lion struct {
    Animal //匿名字段
}

func main(){
    var lion = Lion{
        Animal{
            Name:  &amp;quot;小狮子&amp;quot;,
            Color: &amp;quot;灰色&amp;quot;,
        },
    }
    lion.Run()
    fmt.Println(lion.Name)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;go的内嵌和组合实现了go的继承和派生的作用。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;关键字&lt;/p&gt;

&lt;p&gt;golang中关键字&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;break case chan const continue
default func defer go else goto fallthrough if
for import
interface map package range return
select struct switch type var
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体作用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var和const Go语言基础里面的变量和常量申明
package和import 已经有过短暂的接触，就是包
func 用于定义函数和方法
return 用于从函数返回
defer 用于类似析构函数
go 用于并行
select 用于选择不同类型的通讯，监听
interface 用于定义接口
struct 用于定义抽象数据类型，
break、case、continue、for、fallthrough、else、if、switch、goto、default 这流程介绍里面
chan 用于channel通讯
type 用于声明自定义类型
map 用于声明map类型数据
range 用于读取slice、map、channel数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;go语言的三大核心&#34;&gt;go语言的三大核心&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-goroutinechannel/&#34;&gt;goroutine&lt;/a&gt;和&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-channel/&#34;&gt;channel&lt;/a&gt;是Go并发的两大基石，那么&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-interface/&#34;&gt;接口&lt;/a&gt;是Go语言编程中数据类型的关键。&lt;/p&gt;

&lt;p&gt;Golang使用Groutine和channels实现了CSP(Communicating Sequential Processes)模型，即通信顺序进程模型。Goroutine 和 Channel 分别对应 CSP 中的实体和传递信息的媒介，Go 语言中的 Goroutine 会通过 Channel 传递数据。&lt;/p&gt;

&lt;p&gt;在Go语言的实际编程中，几乎所有的数据结构都围绕接口展开，接口是Go语言中所有数据结构的核心。&lt;/p&gt;

&lt;p&gt;这三个可以说是go语言的核心所在，&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-goroutinechannel/&#34;&gt;goroutine&lt;/a&gt;和&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-channel/&#34;&gt;channel&lt;/a&gt;和&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-interface/&#34;&gt;interface&lt;/a&gt;都单独成篇。&lt;/p&gt;

&lt;h1 id=&#34;零碎基础知识&#34;&gt;零碎基础知识&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;golang是一门静态类型开发语言&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个 Go 程序都是由包组成的，程序运行的入口是包 &lt;code&gt;main&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;go自己的设计哲学：&lt;/p&gt;

&lt;p&gt;1、不得包含源代码文件中没有用到的包&lt;/p&gt;

&lt;p&gt;2、函数的左括号{位置&lt;/p&gt;

&lt;p&gt;3、函数名大小写规则&lt;/p&gt;

&lt;p&gt;4、不一定要用分号结束语句&lt;/p&gt;

&lt;p&gt;5、package的名称必须和目录名保持一致&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Go语言的工作空间结构：&lt;/p&gt;

&lt;p&gt;Go语言的工作空间其实就是一个文件目录，目录中必须包含src、pkg、bin三个目录。&lt;/p&gt;

&lt;p&gt;其中src目录用于存放Go源代码，pkg目录用于package对象，bin目录用于存放可执行对象&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;环境变量&lt;/p&gt;

&lt;p&gt;go命名行用到了GOPATH环境变量，在这个目录下收索&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export GOPATH=你的工作空间路径

export PATH=$PATH:$GOPATH/bin----安装用这个路劲
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;go的初始化函数init是在main函数的之前执行的&lt;/p&gt;

&lt;p&gt;Go里面有两个保留的函数:init函数(能够应用于所有的package)和main函数(只能应用于package main)。 这两个函数在定义时不能有任何的参数和返回值。虽然一个package里面可以写任意多个init函数，但这无论是对 于可读性还是以后的可维护性来说，我们都强烈建议用户在一个package中每个文件只写一个init函数。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;全局唯一性操作&lt;/p&gt;

&lt;p&gt;var once sync.Once&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;go环境变量：&lt;/p&gt;

&lt;p&gt;GOROOT ：go的安装目录&lt;/p&gt;

&lt;p&gt;GOPATH ：你自己开发go语言代码的目录，目录结构为bin,pkg,src，如果你有多个目录，那么使用分号分隔。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命名函数&lt;/p&gt;

&lt;p&gt;我们给 fmt.Println 一个短名字的别名&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var p = fmt.Println
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令行参数&lt;/p&gt;

&lt;p&gt;os.Args 提供原始命令行参数访问功能。注意，切片中的第一个参数是该程序的路径，并且 os.Args[1:]保存所有程序的的参数。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;go提供了安装包，直接下载解压设置/etc/profile环境变量就可以使用go了，简单便捷&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export GOROOT=/home/test/Go/go—-源码安装路径

export PATH=$GOROOT/bin:$PATH——声明应用

export GOPATH=/home/test/Go/go-project—你的项目路劲
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;import&lt;/p&gt;

&lt;p&gt;import &amp;ldquo;fmt&amp;rdquo;最常用的一种形式&lt;/p&gt;

&lt;p&gt;import &amp;ldquo;./test&amp;rdquo;导入同一目录下test包中的内容&lt;/p&gt;

&lt;p&gt;import f &amp;ldquo;fmt&amp;rdquo;导入fmt，并给他启别名ｆ&lt;/p&gt;

&lt;p&gt;import . &amp;ldquo;fmt&amp;rdquo;，将fmt启用别名&amp;rdquo;.&amp;ldquo;，这样就可以直接使用其内容，而不用再添加ｆｍｔ，如fmt.Println可以直接写成Println&lt;/p&gt;

&lt;p&gt;import  _ &amp;ldquo;fmt&amp;rdquo; 表示不使用该包，而是只是使用该包的init函数，并不显示的使用该包的其他内容。注意：这种形式的import，当import时就执行了fmt包中的init函数，而不能够使用该包的其他函数。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;struct&lt;/p&gt;

&lt;p&gt;定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type person struct {
    name string
    age int
}   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;直接初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var P person  // P现在就是person类型的变量了
P.name = &amp;quot;Astaxie&amp;quot;  // 赋值&amp;quot;Astaxie&amp;quot;给P的name属性.
P.age = 25  // 赋值&amp;quot;25&amp;quot;给变量P的age属性
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;按照顺序提供初始化值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;P := person{&amp;quot;Tom&amp;quot;, 25}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通过field:value的方式初始化，这样可以任意顺序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;P := person{age:24, name:&amp;quot;Tom&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;进行结构体比较时候，只有相同类型的结构体才可以比较，结构体是否相同不但与属性类型个数有关，还与属性顺序相关。结构体是相同的，但是结构体属性中有不可以比较的类型，如map,slice，还是不可比较的。&lt;/p&gt;

&lt;p&gt;如果该结构属性都是可以比较的，那么就可以使用“==”进行比较操作。可以使用reflect.DeepEqual进行比较&lt;/p&gt;

&lt;p&gt;struct内部的的成员一般需要给外部使用，不管是同一个文件还是几个文件，所以都是需要大写开头&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;错误&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Handler crashed with error runtime error: invalid memory address or nil pointer dereference
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般报这个错误原因&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;值为nil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;找到这个位置，值为nil 判断一下再输出或者赋值
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;nil 可以用作 interface、function、pointer、map、slice 和 channel 的“空值”。但是如果不特别指定的话，Go 语言不能识别类型，所以会报错。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;goto&lt;/p&gt;

&lt;p&gt;goto不能跳转到其他函数或者内层代码&lt;/p&gt;

&lt;p&gt;在golang中同属于流程控制，和判断，循环一样。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Go 1.9 新特性&lt;/p&gt;

&lt;p&gt;Alias and defintion&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;基于一个类型创建一个新类型，称之为defintion；
基于一个类型创建一个别名，称之为alias。

defintion，虽然底层类型为int类型，但是不能直接赋值，需要强转；
alias，可以直接赋值。
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命名&lt;/p&gt;

&lt;p&gt;go提倡命名不使用下划线&lt;/p&gt;

&lt;p&gt;if后不提倡使用括号&lt;/p&gt;

&lt;p&gt;提出命名不带数据类型，不够简洁，而且现在ide功能也是很全面清晰&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;隐藏&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for k:= range labels {
        labelNames = append(labelNames, k)
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边这个v不需要使用_就可以不写。&lt;/p&gt;

&lt;p&gt;_代表空白标志符&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;range分配的变量&lt;/p&gt;

&lt;p&gt;range对应的变量的地址是不变的，地址的上的数据是变化的，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type student struct {
    Name string
    Age  int
}

func pase_student() {
    m := make(map[string]*student)
    stus := []student{
        {Name: &amp;quot;zhou&amp;quot;, Age: 24},
        {Name: &amp;quot;li&amp;quot;, Age: 23},
        {Name: &amp;quot;wang&amp;quot;, Age: 22},
    }
    for _, stu := range stus {
        m[stu.Name] = &amp;amp;stu
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后获取的m中value值是一样的，都是stu的地址，stu不会随着range重新分配变量，一直都是一个，所以内部的赋值一直都是stu的同一个地址，正常的写法是定义的时候不用地址&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type student struct {
    Name string
    Age  int
}

func pase_student() {
    m := make(map[string]student)
    stus := []student{
        {Name: &amp;quot;zhou&amp;quot;, Age: 24},
        {Name: &amp;quot;li&amp;quot;, Age: 23},
        {Name: &amp;quot;wang&amp;quot;, Age: 22},
    }
    for _, stu := range stus {
        m[stu.Name] = stu
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;换一个角度，rang遍历数组，切片，map时，都是值拷贝，在循环中更改值不会影响原来的&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;变量声明&lt;/p&gt;

&lt;p&gt;“:=”只能在声明“局部变量”的时候使用，而“var”没有这个限制。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;运算符&lt;/p&gt;

&lt;p&gt;在Go中 &amp;ldquo;++&amp;rdquo; 和 &amp;ldquo;- -&amp;rdquo; 只能作为语句而非表达式
示例代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := 1
a ++ // 注意：不能写成 ++a 或 -- a 必须放在右边使用
// b := a++// 此处为错误的用法，不能写在一行，要单独作为语句使用

fmt.Println(a) // 2
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;封装实例（deafulat）&lt;/p&gt;

&lt;p&gt;go标准库中，经常这么做&lt;/p&gt;

&lt;p&gt;定义了一个类型，提供了很多方法；为了方便使用，会实例化一个该类型的实例（通用），这样便可以直接使用该实例调用方法。比如：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;encoding/base64 中提供了 StdEncoding 和 URLEncoding 实例，使用时：base64.StdEncoding.Encode()&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在 flag 包使用了有类似的方法，比如 CommandLine 实例，只不过 flag 进行了进一步封装：将 FlagSet 的方法都重新定义了一遍，也就是提供了一序列函数，而函数中只是简单的调用已经实例化好了的 FlagSet 实例：CommandLine 的方法。这样，使用者是这么调用：flag.Parse() 而不是 flag. CommandLine.Parse()。（Go 1.2 起，将 CommandLine 导出，之前是非导出的）&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;map和slice遍历顺序问题&lt;/p&gt;

&lt;p&gt;我理解为在range时为引用类型（slice，map，channel)创建索引，而map的索引是未被指定的，所以无序。&lt;/p&gt;

&lt;p&gt;因此如果需要保证顺序输出，我是使用了slice。当然range 数组是有序的输出。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;调用&lt;/p&gt;

&lt;p&gt;当package无法调用对应的方法时，第一个看是否是大写，小写对外不公开，第二个看是否包名和变量名重复&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;无法修改map中的成员变量&lt;/p&gt;

&lt;p&gt;在开始代码设计的时候想要将原struct中的成员变量进行修改或者替换。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = y 这种赋值的方式，你必须知道 x的地址，然后才能把值 y 赋给 x。
但 go 中的 map 的 value 本身是不可寻址的，因为 map 的扩容的时候，可能要做 key/val pair迁移
value 本身地址是会改变的
不支持寻址的话又怎么能赋值呢
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码示例如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;

var m = map[string]struct{ x, y int } {
    &amp;quot;foo&amp;quot;: {2, 3}
}

func main() {
    m[&amp;quot;foo&amp;quot;].x = 4
    fmt.Printf(&amp;quot;result is : %+v&amp;quot;, m)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;本以为这个会将 m[“foo”] 中的 x 替换成 4， 从而打印出来的效果是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;result is : map[foo:{x:4 y:3}]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然而，并不是的，这段代码在保存后编译时提示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cannot assign to struct field m[&amp;quot;foo&amp;quot;].x in map
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就尴尬了，无法在已经存在的key的节点中修改值，这是为什么？&lt;/p&gt;

&lt;p&gt;m中已经存在”foo”这个节点了啊，&lt;/p&gt;

&lt;p&gt;简单来说就是map不是一个并发安全的结构，所以，并不能修改他在结构体中的值。&lt;/p&gt;

&lt;p&gt;这如果目前的形式不能修改的话，就面临两种选择，&lt;/p&gt;

&lt;p&gt;1.修改原来的设计;&lt;/p&gt;

&lt;p&gt;2.想办法让map中的成员变量可以修改，&amp;mdash;-使用指针&lt;/p&gt;

&lt;p&gt;因为懒得该这个结构体，就选择了方法2，&lt;/p&gt;

&lt;p&gt;但是不支持这种方式传递值，应该如何进行修改现在已经存在在struct中的map的成员变量呢？&lt;/p&gt;

&lt;p&gt;热心的网友们倒是提供了一种方式，示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;

var m = map[string]struct{ x, y int } {
    &amp;quot;foo&amp;quot;: {2, 3}
}

func main() {
    tmp := m[&amp;quot;foo&amp;quot;]
    tmp.x = 4
    m[&amp;quot;foo&amp;quot;] = tmp
    fmt.Printf(&amp;quot;result is : %+v&amp;quot;, m)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;果然和预期结果一致，不过，总是觉得有点怪怪的，&lt;/p&gt;

&lt;p&gt;既然是使用了类似临时空间的方式，那我们用地址引用传值不也是一样的么&amp;hellip;&lt;/p&gt;

&lt;p&gt;于是，我们就使用了另外一种方式来处理这个东西，&lt;/p&gt;

&lt;p&gt;示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;

var m = map[string]*struct{ x, y int } {
    &amp;quot;foo&amp;quot;: &amp;amp;{2, 3}
}

func main() {
   m[&amp;quot;foo&amp;quot;].x = 4
   fmt.Println(&amp;quot;result is : %+v \n&amp;quot;, m)
   fmt.Println(&amp;quot;m&#39;s node is : %+v \n&amp;quot;, *m[&amp;quot;foo&amp;quot;])
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后的展示结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;result is : map[foo:0xc42000cff0]
m&#39;s node is : {4, 3}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;代理&lt;/p&gt;

&lt;p&gt;以前我一直以为只需要给git加代理就可以了，其实go命令本身就有proxy的。比如http_proxy=socks5://127.0.0.1:1080 go get -u -v github.com/gin-gonic/gin&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行原理&lt;/p&gt;

&lt;p&gt;Go 的可执行文件都比相对应的源代码文件要大很多，这恰恰说明了 Go 的 runtime 嵌入到了每一个可执行文件当中。当然，在部署到数量巨大的集群时，较大的文件体积也是比较头疼的问题。但总得来说，Go 的部署工作还是要比 Java 和 Python 轻松得多。因为 Go 不需要依赖任何其它文件，它只需要一个单独的静态文件，这样你也不会像使用其它语言一样在各种不同版本的依赖文件之间混淆。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;参数interface&lt;/p&gt;

&lt;p&gt;接口interface作为返回参数，只要是实现了这个接口内的所有的方法的struct，都可以返回给这个参数&lt;/p&gt;

&lt;p&gt;例如filebeat中filebeat的new函数的返回，就是把fileabeat的结构体返回给了beat.Beater接口interface&lt;/p&gt;

&lt;p&gt;另外一方面也可以解释
    var a beat.Beater = &amp;amp;Filebeat{}&lt;/p&gt;

&lt;p&gt;可以把一个结构体赋值给一个接口&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;调用子进程&lt;/p&gt;

&lt;p&gt;编程中遇到使用相对路径再配置文件中，调用process b的可以先获取当前进程运行的绝对路径，正常大部分都是使用绝对路径。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;google开源的包&lt;/p&gt;

&lt;p&gt;为什么gRPC-go在github的地址是&amp;rdquo;&lt;a href=&#34;https://github.com/grpc/grpc-go&amp;quot;&#34;&gt;https://github.com/grpc/grpc-go&amp;quot;&lt;/a&gt; , 但是为什么要用“google.golang.org/grpc”进行安装呢？应该grpc原本是google内部的项目，归属golang，就放在了google.golang.org下面了，后来对外开放，又将其迁移到github上面了，又因为golang比较坑爹的import路径规则，所以就都没有改路径名了。&lt;/p&gt;

&lt;p&gt;但是这样就有个问题了。要如何去管理版本呢？这个目前我还没有什么比较好的方法，希望知道的朋友一起分享下。目前想到一个方法是手动下载某个版本，然后写个脚本统一修改代码中的import里面的路径.&lt;/p&gt;

&lt;p&gt;google在github上也有一个账号&lt;a href=&#34;https://github.com/google/，对应的google开源的项目在这个下面，由于google去get&#34;&gt;https://github.com/google/，对应的google开源的项目在这个下面，由于google去get&lt;/a&gt; 包不太友好，可以来这边来下载。&lt;/p&gt;

&lt;p&gt;也有去一下专门下载包的第三方网站去下载，比如&lt;a href=&#34;https://gopm.io/&#34;&gt;https://gopm.io/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每一个可独立运行的Go程序，必定包含一个package main，在这个main包中必定包含一个入口函数main，而这 个函数既没有参数，也没有返回值。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;求余&lt;/p&gt;

&lt;p&gt;求余, B % A&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;hashcode&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
        &amp;quot;fmt&amp;quot;
        &amp;quot;hash/crc32&amp;quot;
)

// String hashes a string to a unique hashcode.
//
// crc32 returns a uint32, but for our use we need
// and non negative integer. Here we cast to an integer
// and invert it if the result is negative.
func String(s string) int {
        v := int(crc32.ChecksumIEEE([]byte(s)))
        if v &amp;gt;= 0 {
                return v
        }
        if -v &amp;gt;= 0 {
                return -v
        }
        // v == MinInt
        return 0
}


func main(){
        str := &amp;quot;123456abcd&amp;quot;

        for i:=0; i&amp;lt;3;i+&amp;gt;{
                hc := String(str)
                fmt.Println(&amp;quot;hashcode:&amp;quot;, hc)

        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;gouroutine执行失败需要退出整个进程，可以使用panic&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;golang字符串去除空格和换行符&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;strings&amp;quot;
)

func main() {
    str := &amp;quot;welcome to bai\ndu\n.com&amp;quot;
    // 去除空格
    str = strings.Replace(str, &amp;quot; &amp;quot;, &amp;quot;&amp;quot;, -1)
    // 去除换行符
    str = strings.Replace(str, &amp;quot;\n&amp;quot;, &amp;quot;&amp;quot;, -1)
    fmt.Println(str)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;range空的内容的时候&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
)

func main() {
    var res = []string {&amp;quot;&amp;quot;}
    var flag = true
    for _, value := range res {
        fmt.Println(&amp;quot;have value:&amp;quot;,value)
        flag = false

    }
    for i:=0;i&amp;lt;len(res);i++{
        fmt.Println(&amp;quot;have value:&amp;quot;,res[i])
    }

    if !flag {
        fmt.Println(&amp;quot;response array the number of row UnMatch&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chunyindeMacBook-Pro:test chunyinjiang$ go run range.go
have value:
have value:
response array the number of row UnMatch
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[]byte to string&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
)

func main() {
    data := [4]byte{0x31, 0x32, 0x33, 0x34}
    str := string(data[:])
    fmt.Println(str)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;read内容的问题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;buf := make([]byte, 512)
_, err := conn.Read(buf)
input := string(buf)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果接收的数据不够512bytes怎么办？会变成buf末尾跟了很多个0（make([]byte, 512)会把全部都初始化为0），虽然0在c里面代表字符串的结尾，但是go可不是哦。&lt;/p&gt;

&lt;p&gt;所以，正确的代码应该是这样&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;buf := make([]byte, 512)
for {
    n, err := conn.Read(buf) // n代表读取的数量。
    if err != nil {
        break
    }
    input := string(buf[:n])
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;获取字符的数值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int(&#39;9&#39; - &#39;0&#39;)//9,所以这个用于获取正常的数值的方式
int(&#39;9&#39;)//49--ACSII
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;fmt打印结构体内容&lt;/p&gt;

&lt;p&gt;fmt包内置的方法，本来就可以展示类似Json的形式，没必要自己瞎搞。把上面的输出代码改了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%+v\n&amp;quot;, a)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (

        &amp;quot;fmt&amp;quot;
)


type Power struct{
        age int
        high int
        name string
}

func main() {

        var i Power = Power{age: 10, high: 178, name: &amp;quot;NewMan&amp;quot;}

        fmt.Printf(&amp;quot;type:%T\n&amp;quot;, i)
        fmt.Printf(&amp;quot;value:%v\n&amp;quot;, i)
        fmt.Printf(&amp;quot;value+:%+v\n&amp;quot;, i)
        fmt.Printf(&amp;quot;value#:%#v\n&amp;quot;, i)


        fmt.Println(&amp;quot;========interface========&amp;quot;)
        var interf interface{} = i
        fmt.Printf(&amp;quot;%v\n&amp;quot;, interf)
        fmt.Println(interf)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type:main.Power
value:{10 178 NewMan}
value+:{age:10 high:178 name:NewMan}
value#:main.Power{age:10, high:178, name:”NewMan”}
========interface========
{10 178 NewMan}
{10 178 NewMan}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;metrics name buf : %s\n&amp;quot;,ic.MetricNameBuf )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等价于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(&amp;quot;metrics name buf :&amp;quot;,string(ic.MetricNameBuf) )
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;go可以通过gdb来调试&lt;/p&gt;

&lt;p&gt;编译Go程序的时候需要注意以下几点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 传递参数-ldflags &amp;quot;-s&amp;quot;，忽略debug的打印信息
2. 传递-gcflags &amp;quot;-N -l&amp;quot; 参数，这样可以忽略Go内部做的一些优化，聚合变量和函数等优化，这样对于GDB调
   试来说非常困难，所以在编译的时候加入这两个参数避免这些优化。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用命令&lt;/p&gt;

&lt;p&gt;GDB的一些常用命令如下所示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;list
简写命令l，用来显示源代码，默认显示十行代码，后面可以带上参数显示的具体行，例如:list 15，显示 十行代码，其中第15行在显示的十行里面的中间，如下所示。
10 time.Sleep(2 * time.Second)
11 c &amp;lt;- i
12 }
13 close(c)
14 }
15
16 func main() {
17 18 19
break
msg := &amp;quot;Starting main&amp;quot; fmt.Println(msg)
bus := make(chan int)
简写命令 b,用来设置断点，后面跟上参数设置断点的行数，例如b 10在第十行设置断点。
delete 简写命令 d,用来删除断点，后面跟上断点设置的序号，这个序号可以通过info breakpoints获取
相应的设置的断点序号，如下是显示的设置断点序号。
Num Type Disp Enb Address What
2 breakpoint keep y 0x0000000000400dc3 in main.main at /home/xiemengjun/gdb.go:2 breakpoint already hit 1 time
backtrace
简写命令 bt,用来打印执行的代码过程，如下所示:
#0 main.main () at /home/xiemengjun/gdb.go:23
#1 0x000000000040d61e in runtime.main () at /home/xiemengjun/go/src/pkg/runtime/proc.c:244 #2 0x000000000040d6c1 in schedunlock () at /home/xiemengjun/go/src/pkg/runtime/proc.c:267 #3 0x0000000000000000 in ?? ()
231
3
info
info命令用来显示信息，后面有几种参数，我们常用的有如下几种:
print
info locals
显示当前执行的程序中的变量值
info breakpoints
显示当前设置的断点列表
info goroutines
显示当前执行的goroutine列表，如下代码所示,带*的表示当前执行的
* 1 running runtime.gosched
* 2 syscall runtime.entersyscall
3 waiting runtime.gosched 4 runnable runtime.gosched
简写命令p，用来打印变量或者其他信息，后面跟上需要打印的变量名，当然还有一些很有用的函数$len()和 $cap()，用来返回当前string、slices或者maps的长度和容量。
whatis
用来显示当前变量的类型，后面跟上变量名，例如whatis msg,显示如下: type = struct string
next
简写命令 n,用来单步调试，跳到下一步，当有断点之后，可以输入n跳转到下一步继续执行 coutinue
简称命令 c，用来跳出当前断点处，后面可以跟参数N，跳过多少次断点
set variable
该命令用来改变运行过程中的变量值，格式如:set variable &amp;lt;var&amp;gt;=&amp;lt;value&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调试过程&lt;/p&gt;

&lt;p&gt;我们通过下面这个代码来演示如何通过GDB来调试Go程序，下面是将要演示的代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import ( &amp;quot;fmt&amp;quot;
&amp;quot;time&amp;quot; )
func counting(c chan&amp;lt;- int) { for i := 0; i &amp;lt; 10; i+&amp;gt;{
time.Sleep(2
c &amp;lt;- i }
close(c) }
func main() {
msg := &amp;quot;Starting
fmt.Println(msg)
bus := make(chan
msg = &amp;quot;starting a gofunc&amp;quot; go counting(bus)
for count := range bus {
* time.Second)
main&amp;quot;
int)
232

fmt.Println(&amp;quot;count:&amp;quot;, count) }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译文件，生成可执行文件gdbfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go build -gcflags &amp;quot;-N -l&amp;quot; -ldflags &amp;quot;-s&amp;quot; gdbfile.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过gdb命令启动调试:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gdb gdbfile 启动之后首先看看这个程序是不是可以运行起来，只要输入run命令回车后程序就开始运行，程序正常的话可以看到
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序输出如下，和我们在命令行直接执行程序输出是一样的:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(gdb) run
Starting
Starting
count: 0
count: 1
count: 2
count: 3
count: 4
count: 5
count: 6
count: 7
count: 8
count: 9
[LWP 2771 exited]
[Inferior 1 (process 2771) exited normally]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;好了，现在我们已经知道怎么让程序跑起来了，接下来开始给代码设置断点:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(gdb) b 23
Breakpoint 1 at 0x400d8d: file /home/xiemengjun/gdbfile.go, line 23. (gdb) run
Starting program: /home/xiemengjun/gdbfile
Starting main
[New LWP 3284]
[Switching to LWP 3284]
Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(&amp;quot;count:&amp;quot;, count)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面例子b 23表示在第23行设置了断点，之后输入run开始运行程序。现在程序在前面设置断点的地方停住了，我们 需要查看断点相应上下文的源码，输入list就可以看到源码显示从当前停止行的前五行开始:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(gdb) list
18 fmt.Println(msg)
19 bus := make(chan int)
20 msg = &amp;quot;starting a gofunc&amp;quot;
21 go counting(bus)
22 for count := range bus {
23 fmt.Println(&amp;quot;count:&amp;quot;, count)
24 }
25 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在GDB在运行当前的程序的环境中已经保留了一些有用的调试信息，我们只需打印出相应的变量，查看相应变量的类型及值:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(gdb) info locals count = 0
bus = 0xf840001a50 (gdb) p count
$1 = 0
(gdb) p bus
$2 = (chan int) 0xf840001a50 (gdb) whatis bus
type = chan int
program: /home/xiemengjun/gdbfile main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来该让程序继续往下执行，请继续看下面的命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(gdb) c
Continuing.
count: 0
[New LWP 3303] [Switching to LWP 3303]
Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(&amp;quot;count:&amp;quot;, count)
(gdb) c
Continuing.
count: 1
[Switching to LWP 3302]
Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(&amp;quot;count:&amp;quot;, count)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每次输入c之后都会执行一次代码，又跳到下一次for循环，继续打印出来相应的信息。 设想目前需要改变上下文相关变量的信息，跳过一些过程，并继续执行下一步，得出修改后想要的结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(gdb) info locals
count = 2
bus = 0xf840001a50
(gdb) set variable count=9 (gdb) info locals
count = 9
bus = 0xf840001a50 (gdb) c
Continuing.
count: 9
[Switching to LWP 3302]
Breakpoint 1, main.main () at /home/xiemengjun/gdbfile.go:23 23 fmt.Println(&amp;quot;count:&amp;quot;, count)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后稍微思考一下，前面整个程序运行的过程中到底创建了多少个goroutine，每个goroutine都在做什么:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(gdb) info goroutines
* 1 running runtime.gosched
* 2 syscall runtime.entersyscall
3 waiting runtime.gosched
4 runnable runtime.gosched
(gdb) goroutine 1 bt
#0 0x000000000040e33b in runtime.gosched () at /home/xiemengjun/go/src/pkg/runtime/proc.c:927 #1 0x0000000000403091 in runtime.chanrecv (c=void, ep=void, selected=void, received=void)
at /home/xiemengjun/go/src/pkg/runtime/chan.c:327
#2 0x000000000040316f in runtime.chanrecv2 (t=void, c=void)
at /home/xiemengjun/go/src/pkg/runtime/chan.c:420
#3 0x0000000000400d6f in main.main () at /home/xiemengjun/gdbfile.go:22
#4 0x000000000040d0c7 in runtime.main () at /home/xiemengjun/go/src/pkg/runtime/proc.c:244
#5 0x000000000040d16a in schedunlock () at /home/xiemengjun/go/src/pkg/runtime/proc.c:267
#6 0x0000000000000000 in ?? ()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过查看goroutines的命令我们可以清楚地了解goruntine内部是怎么执行的，每个函数的调用顺序已经明明白白地显示出来了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;利用cpu多核来处理http请求，下面这个没有用go默认就是单核处理http的.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;runtime.GOMAXPROCS(runtime.NumCPU());
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;interface转化string&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var x interface{} = &amp;quot;abc&amp;quot;
str := fmt.Sprintf(&amp;quot;%v&amp;quot;, x)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;golang中json中出现list该怎么定义&lt;/p&gt;

&lt;p&gt;使用map进行struct定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;plugin_params&amp;quot;: map&amp;lt;string&amp;gt;string,}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;return&lt;/p&gt;

&lt;p&gt;退出执行，不指定返回值&lt;/p&gt;

&lt;p&gt;通常有两种情况不需要指定返回值退出函数执行过程。第一是：函数没有返回值；第二是：函数返回值有变量名，不需要显示的指定返回值。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connection reset by peer&lt;/p&gt;

&lt;p&gt;1.如果一端的Socket被关闭（或主动关闭，或因为异常退出而 引起的关闭），另一端仍发送数据，发送的第一个数据包引发该异常(Connect reset by peer)。&lt;/p&gt;

&lt;p&gt;Socket默认连接60秒，60秒之内没有进行心跳交互，即读写数据，就会自动关闭连接。&lt;/p&gt;

&lt;p&gt;2.一端退出，但退出时并未关闭该连接，另一端如果在从连接中读数据则抛出该异常（Connection reset）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;时间比较&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time1 := &amp;quot;2015-03-20 08:50:29&amp;quot;
time2 := &amp;quot;2015-03-21 09:04:25&amp;quot;
//先把时间字符串格式化成相同的时间类型
t1, err := time.Parse(&amp;quot;2006-01-02 15:04:05&amp;quot;, time1)
t2, err := time.Parse(&amp;quot;2006-01-02 15:04:05&amp;quot;, time2)
if err == nil &amp;amp;&amp;amp; t1.Before(t2) {
    //处理逻辑
    fmt.Println(&amp;quot;true&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;定时12点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func startTimer(f func()) {
    go func() {
        for {
            f()
            now := time.Now()
            // 计算下一个零点
            next := now.Add(time.Hour * 24)
            next = time.Date(next.Year(), next.Month(), next.Day(), 0, 0, 0, 0, next.Location())
            t := time.NewTimer(next.Sub(now))
            &amp;lt;-t.C
        }
    }()
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;golang中交换字符串中的字符，需要先转化为[]byte来转化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func smallestStringWithSwaps(s string, pairs [][]int) string {
    n := len(s)
    res := make([]byte,n)
    for k,v := range s{
        res[k] = byte(v)
    }
    for _,v := range pairs{
        left := v[0]
        right := v[1]
        if left &amp;lt; right {
            if res[left] &amp;gt; res[right]{
                res[left],res[right] =  res[right],res[left]
            }
        }
    }
    return string(res)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;常规使用&#34;&gt;常规使用&lt;/h1&gt;

&lt;h2 id=&#34;go-内存和指针&#34;&gt;go 内存和指针&lt;/h2&gt;

&lt;p&gt;Go语言中的指针和C语言中在使用上几乎没有什么差别。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;make和new&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Go语言中有两个分配内存的机制，分别是内建的函数new和make。但是new和make并不能等同，他们所做的事情其实并不同，所应用到的类型也不相同。
 &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、 new(T)函数是一个分配内存的内建函数，但是不同于其他语言中内建new函数所做的工作，在Go语言中，new只是将内存清零，并没有初始化内存。所以在Go语言中，new(T)所做的工作是为T类型分配了值为零的内存空间并返回其地址，即返回*T。也就是说，new(T)返回一个指向新分配的类型T的零值指针

内置函数 new 分配空间。传递给new 函数的是一个类型，不是一个值。返回值是 指向这个新分配的零值的指针。
即：var pInt *int = new(int) //*pInt = nil
 
2、 make(T, args)函数与new(T)函数的目的不同。make(T, args)仅用于创建切片、map和channel(消息管道)，make(T, args)返回类型T的一个被初始化了的实例。而new（T）返回指向类型T的零值指针。也就是说new函数返回的是*T的未初始化零值指针，而make函数返回的是T的初始化了的实例

内建函数 make 分配并且初始化 一个 slice, 或者 map 或者 chan 对象。 并且只能是这三种对象。 和 new 一样，第一个参数是 类型，不是一个值。 但是make 的返回值就是这个类型（即使一个引用类型），而不是指针。 具体的返回值，依赖具体传入的类型。
//创建一个初始元素个数为5的数组切片，元素初始值为0
a := make([]int, 5)  // len(a)=5
 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Go语言中出现new和make两个分配内存的函数，并且使用起来还有差异，主要原因是切片、map、channel这三种类型在使用前必须初始化相关的数据结构。例如，切片是一个有三项内容的数据类型，包括指向数据的指针（在一个数组内部进行切片）、长度和容量，在这三项内容被初始化之前，切片值为nil。换句话说：对于切片、map、channel，make(T, args)初始化了其内部的数据结构并为他们准备了将要使用的值&lt;/p&gt;

&lt;p&gt;例如：make([]int, 10, 100); //分配一个整形数组，长度为10，容量为100，并返回其前10个数组的切片&lt;/p&gt;

&lt;p&gt;所以，在为切片、map、channel这三种类型分配内存时，为了不必要的使问题复杂化，应该使用Go的内建make函数&lt;/p&gt;

&lt;p&gt;记住 make 只用于map，slice和channel，并且不返回指针。要获得一个显式的指针，使用 new 进行分配，或者显式地使用一个变量的地址。&lt;/p&gt;

&lt;p&gt;这些例子阐释了 new 和 make 之间的差别。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var p *[]int = new([]int)       // allocates slice structure; *p == nil; rarely useful
var v  []int = make([]int, 100) // the slice v now refers to a new array of 100 ints
// Unnecessarily complex:
var p *[]int = new([]int)
*p = make([]int, 100, 100)
// Idiomatic:
v := make([]int, 100)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1.panic: runtime error: invalid memory address or nil pointer dereference&lt;/p&gt;

&lt;p&gt;正常都是内存没有分配&lt;/p&gt;

&lt;p&gt;2.routers/routers.go:14:61: cannot call pointer method on controllers.UserController literal&lt;/p&gt;

&lt;p&gt;原因: 指针不能作为接收者, 需要需要定义指针变量来接送地址&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type myTreeNode struct{
    node *tree.TreeNode
}

func (mynode *myTreeNode) postOrader(){
    if mynode==nil || mynode.node==nil{
        return
    }

    //错误的写法
    myTreeNode{mynode.node.Left}.postOrader()
    right := myTreeNode{mynode.node.Right}.postOrader()
    mynode.node.Print()

    //正确的写法
    left := myTreeNode{mynode.node.Left}
    left.postOrader()
    right := myTreeNode{mynode.node.Right}
    right.postOrader()
    mynode.node.Print()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;golang中-json-map-struct-之间的相互转化&#34;&gt;GoLang中 json、map、struct 之间的相互转化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;golang 中 json 转 struct&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用 json.Unmarshal 时，结构体的每一项必须是导出项(import field)。也就是说结构体的 key 对应的首字母必须为大写。&lt;/p&gt;

&lt;p&gt;请看下面的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;testing&amp;quot;
    &amp;quot;encoding/json&amp;quot;
)

type Person struct {
    name string
    age int
}

func TestStruct2Json(t *testing.T) {
    jsonStr := `
    {
        &amp;quot;name&amp;quot;:&amp;quot;liangyongxing&amp;quot;,
        &amp;quot;age&amp;quot;:12
    }
    `
    var person Person
    json.Unmarshal([]byte(jsonStr), &amp;amp;person)
    t.Log(person)
}

输出的结果如下：

1
{ 0}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从结果可以看出，json 数据并没有写入 Person 结构体中。结构体 key 首字母大写的话就可以，修改后：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;testing&amp;quot;
    &amp;quot;encoding/json&amp;quot;
)

type Person struct {
    Name string
    Age int
}

func TestStruct2Json(t *testing.T) {
    jsonStr := `
    {
        &amp;quot;name&amp;quot;:&amp;quot;liangyongxing&amp;quot;,
        &amp;quot;age&amp;quot;:12
    }
    `
    var person Person
    json.Unmarshal([]byte(jsonStr), &amp;amp;person)
    t.Log(person)
}

打印结果如下：

1
{liangyongxing 12}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从以上结果我们可以发现一个很重要的信息，json 里面的 key 和 struct 里面的 key一个是小写一个是大写，即两者大小写并没有对上。从这里我们就可以得出一个结论，要想能够附上值需要结构体中的变量名首字母大写，而在转换的 json 串中大小写都可以，即在 json 传中字段名称大小写不敏感。那么经过验证发现，在 json 中如果写成如下方式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jsonStr :=
    {
        &amp;quot;NaMe&amp;quot;:&amp;quot;liangyongxing&amp;quot;,
        &amp;quot;agE&amp;quot;:12
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最终结果仍然是有值的，那么就验证了我们上面的结论，json 串中对字段名大小写不敏感(不一定是首字母，这点需要注意)&lt;/p&gt;

&lt;p&gt;在结构体中是可以引入 tag 标签的，这样在匹配的时候 json 串对应的字段名需要与 tag 标签中定义的字段名匹配，当然在 tag 中定义的名称就不需要首字母大写了，且对应的 json 串中字段名称仍然大小写不敏感，和上面的结论一致。(注意：此时结构体中对应的字段名可以不用和匹配的一致，但是也必须首字母大写，只有大写的才是可对外提供访问的)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;testing&amp;quot;
    &amp;quot;encoding/json&amp;quot;
)

//这里对应的 N 和 A 不能为小写，首字母必须为大写，这样才可对外提供访问，具体 json 匹配是通过后面的 tag 标签进行匹配的，与 N 和 A 没有关系
//tag 标签中 json 后面跟着的是字段名称，都是字符串类型，要求必须加上双引号，否则 golang 是无法识别它的类型
type Person struct {
    N string     `json:&amp;quot;name&amp;quot;`
    A int        `json:&amp;quot;age&amp;quot;`
}

func TestStruct2Json(t *testing.T) {
    jsonStr := `
    {
        &amp;quot;name&amp;quot;:&amp;quot;liangyongxing&amp;quot;,
        &amp;quot;age&amp;quot;:12
    }
    `
    var person Person
    json.Unmarshal([]byte(jsonStr), &amp;amp;person)
    t.Log(person)
}
复制代码
这样输出的结果如下：

1
{liangyongxing 12}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，你也可以再做一个实验，验证 tag 标签中对应的字段名称大小写不敏感，这里就不做冗余介绍了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;golang 中 struct 转 json 串&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;直接使用Marshal&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package commontest

import (
    &amp;quot;testing&amp;quot;
    &amp;quot;encoding/json&amp;quot;
)

func TestStruct2Json(t *testing.T) {
    p := Person{
        Name: &amp;quot;liangyongxing&amp;quot;,
        Age: 29,
    }

    t.Logf(&amp;quot;Person 结构体打印结果:%v&amp;quot;, p)

    //Person 结构体转换为对应的 Json
    jsonBytes, err := json.Marshal(p)
    if err != nil {
        t.Fatal(err)
    }
    t.Logf(&amp;quot;转换为 json 串打印结果:%s&amp;quot;, string(jsonBytes))
}


打印结果如下所示：


/usr/local/go/bin/go test -v commontest -run ^TestStruct2Json$
    struct2json_test.go:14: Person 结构体打印结果:{liangyongxing 29}
    struct2json_test.go:21: 转换为 json 串打印结果:{&amp;quot;name&amp;quot;:&amp;quot;liangyongxing&amp;quot;,&amp;quot;age&amp;quot;:29}
ok      commontest  0.006s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;golang 中 json 转 map&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;也是使用Unmarshal&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package commontest

import (
    &amp;quot;testing&amp;quot;
    &amp;quot;encoding/json&amp;quot;
)

func TestJson2Map(t *testing.T) {
    jsonStr := `
    {
        &amp;quot;name&amp;quot;:&amp;quot;liangyongxing&amp;quot;,
        &amp;quot;age&amp;quot;:12
    }
    `
    var mapResult map[string]interface{}
    //使用 json.Unmarshal(data []byte, v interface{})进行转换,返回 error 信息
    if err := json.Unmarshal([]byte(jsonStr), &amp;amp;mapResult); err != nil {
        t.Fatal(err)
    }
    t.Log(mapResult)
}

打印结果信息如下：


/usr/local/go/bin/go test -v commontest -run ^TestJson2Map$
    json2map_test.go:19: map[name:liangyongxing age:12]
ok      commontest  0.007s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;golang 中 map 转 json 串&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;也是使用Marshal&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;testing&amp;quot;
    &amp;quot;encoding/json&amp;quot;
)

func TestMap2Json(t *testing.T) {
    mapInstance := make(map[string]interface{})
    mapInstance[&amp;quot;Name&amp;quot;] = &amp;quot;liang637210&amp;quot;
    mapInstance[&amp;quot;Age&amp;quot;] = 28
    mapInstance[&amp;quot;Address&amp;quot;] = &amp;quot;北京昌平区&amp;quot;

    jsonStr, err := json.Marshal(mapInstance)

    if err != nil {
        t.Fatal(err)
    }

    t.Logf(&amp;quot;Map2Json 得到 json 字符串内容:%s&amp;quot;, jsonStr)
}


/usr/local/go/bin/go test -v commontest -run ^TestMap2Json$
    map2json_test.go:20: Map2Json 得到 json 字符串内容:{&amp;quot;Address&amp;quot;:&amp;quot;北京昌平区&amp;quot;,&amp;quot;Age&amp;quot;:28,&amp;quot;Name&amp;quot;:&amp;quot;liang637210&amp;quot;}
ok      commontest  0.008s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;golang中map转 struct&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实在map和struct之间的转化完全可以通过上面的两次转换来完成，就是比较消耗资源，可以通过下列方法一次完成相互转换。&lt;/p&gt;

&lt;p&gt;这个转换网上有比较完整的项目，已经上传到对应的 github 上了，需要下载之后使用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go get github.com/goinggo/mapstructure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后我们就可以直接使用它提供的方法将 map 转换为 struct，让我们直接上代码吧&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;testing&amp;quot;
    &amp;quot;github.com/goinggo/mapstructure&amp;quot;
)

func TestMap2Struct(t *testing.T) {
    mapInstance := make(map[string]interface{})
    mapInstance[&amp;quot;Name&amp;quot;] = &amp;quot;liang637210&amp;quot;
    mapInstance[&amp;quot;Age&amp;quot;] = 28

    var person Person
    //将 map 转换为指定的结构体
    if err := mapstructure.Decode(mapInstance, &amp;amp;person); err != nil {
        t.Fatal(err)
    }
    t.Logf(&amp;quot;map2struct后得到的 struct 内容为:%v&amp;quot;, person)
}

/usr/local/go/bin/go test -v commontest -run ^TestMap2Struct$
    map2struct_test.go:18: map2struct后得到的 struct 内容为:{liang637210 28}
ok      commontest  0.009s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;golang 中 struct 转 map&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用reflect&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package commontest

import (
    &amp;quot;testing&amp;quot;
    &amp;quot;reflect&amp;quot;
)

type User struct {
    Id        int    `json:&amp;quot;id&amp;quot;`
    Username    string    `json:&amp;quot;username&amp;quot;`
    Password    string    `json:&amp;quot;password&amp;quot;`
}

func Struct2Map(obj interface{}) map[string]interface{} {
    t := reflect.TypeOf(obj)
    v := reflect.ValueOf(obj)

    var data = make(map[string]interface{})
    for i := 0; i &amp;lt; t.NumField(); i+&amp;gt;{
        data[t.Field(i).Name] = v.Field(i).Interface()
    }
    return data
}

func TestStruct2Map(t *testing.T) {
    user := User{5, &amp;quot;zhangsan&amp;quot;, &amp;quot;password&amp;quot;}
    data := Struct2Map(user)
    t.Logf(&amp;quot;struct2map得到的map内容为:%v&amp;quot;, data)
}



打印结果如下：

/usr/local/go/bin/go test -v commontest -run ^TestStruct2Map$
    struct2map_test.go:28: struct2map得到的map内容为:map[Id:5 Username:zhangsan Password:password]
ok      commontest  0.007s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于&lt;a href=&#34;#reflect&#34;&gt;relflect&lt;/a&gt;的使用可以参考标准库。&lt;/p&gt;

&lt;h2 id=&#34;优雅重启升级&#34;&gt;优雅重启升级&lt;/h2&gt;

&lt;p&gt;使用 Go 语言实现优雅的服务器重启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、不关闭现有连接：例如我们不希望关掉已部署的运行中的程序。但又想不受限制地随时升级服务。
2、socket连接要随时响应用户请求：任何时刻socket的关闭可能使用户返回&#39;连接被拒绝&#39;的消息，而这是不可取的。
3、新的进程要能够启动并替换掉旧的。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;p&gt;在基于Unix的操作系统中，signal(信号)是与长时间运行的进程交互的常用方法.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SIGTERM: 优雅地停止进程
SIGHUP: 重启/重新加载进程 (例如: nginx, sshd, apache)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果收到SIGHUP信号，优雅地重启进程需要以下几个步骤:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、服务器要拒绝新的连接请求，但要保持已有的连接。
2、启用新版本的进程
3、将socket“交给”新进程，新进程开始接受新连接请求
4、旧进程处理完毕后立即停止。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;标准库-常用&#34;&gt;标准库（常用）&lt;/h1&gt;

&lt;p&gt;标准库：&lt;a href=&#34;https://books.studygolang.com/The-Golang-Standard-Library-by-Example/&#34;&gt;https://books.studygolang.com/The-Golang-Standard-Library-by-Example/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;直接去网页版查询，经常使用，方可记住，没事也可以看看，可以根据实例巩固，golang的标准库是相当的丰富，足够我们实现很多功能。&lt;/p&gt;

&lt;h2 id=&#34;reflect&#34;&gt;reflect&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-reflect/&#34;&gt;relflect&lt;/a&gt;就是动态运行时的状态，对自己行为的描述（self-representation）和监测（examination）。&lt;/p&gt;

&lt;h2 id=&#34;io&#34;&gt;io&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-io/&#34;&gt;io&lt;/a&gt;是所有需要交互的输入输出模式的基础。&lt;/p&gt;

&lt;h2 id=&#34;ioutil&#34;&gt;ioutil&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-ioutil/&#34;&gt;ioutil&lt;/a&gt;主要是提供了一些常用、方便的IO操作函数。&lt;/p&gt;

&lt;h2 id=&#34;fmt&#34;&gt;fmt&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-fmt/&#34;&gt;fmt&lt;/a&gt;是实现了格式化的I/O函数，这点类似Ｃ语言中的printf和scanf，但是更加简单。&lt;/p&gt;

&lt;h2 id=&#34;strconv&#34;&gt;strconv&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strconv/&#34;&gt;strconv&lt;/a&gt;包实现了基本数据类型和其字符串表示的相互转换。&lt;/p&gt;

&lt;h2 id=&#34;time&#34;&gt;time&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-time/&#34;&gt;time&lt;/a&gt;包中包括两类时间：时间点（某一时刻）和时长（某一段时间）的基本操作。&lt;/p&gt;

&lt;h2 id=&#34;sync&#34;&gt;sync&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-sync/&#34;&gt;sync&lt;/a&gt;包提供了基本的同步基元，包括底层的原子级内存操作。&lt;/p&gt;

&lt;h2 id=&#34;runtime包&#34;&gt;runtime包&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-runtime/&#34;&gt;runtime&lt;/a&gt;包提供和go运行时环境的互操作，如控制go协程的函数。&lt;/p&gt;

&lt;h2 id=&#34;math-rand&#34;&gt;math/rand&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-math/&#34;&gt;math&lt;/a&gt;包实现的就是数学函数计算。&lt;/p&gt;

&lt;h2 id=&#34;strings&#34;&gt;strings&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/&#34;&gt;strings&lt;/a&gt;包实现的就是字符串的操作。&lt;/p&gt;

&lt;h2 id=&#34;flag&#34;&gt;flag&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-flag/&#34;&gt;flag&lt;/a&gt;包实现的就是命令行参数设置解析，一般我们会选择一个我们会针对不同的功能需求选用不同的命令行解析包。&lt;/p&gt;

&lt;h2 id=&#34;sort&#34;&gt;sort&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-sort/&#34;&gt;sort&lt;/a&gt;包实现的就是排序算法。&lt;/p&gt;

&lt;h2 id=&#34;archive&#34;&gt;archive&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-archive/&#34;&gt;archive&lt;/a&gt;就是使用tar和zip两种方式对文档进行归档。&lt;/p&gt;

&lt;h2 id=&#34;builtin&#34;&gt;builtin&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-builtin/&#34;&gt;builtin&lt;/a&gt;包是go的预声明定义，包括go语言中常用的各种类型和方法声明，包括变量和常量两部分。&lt;/p&gt;

&lt;h2 id=&#34;compress&#34;&gt;compress&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-compress/&#34;&gt;compress&lt;/a&gt;一般用于压缩。&lt;/p&gt;

&lt;h2 id=&#34;bytes&#34;&gt;bytes&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/&#34;&gt;bytes&lt;/a&gt;该包定义了一些操作 byte slice 的便利操作。&lt;/p&gt;

&lt;h2 id=&#34;context&#34;&gt;context&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-context&#34;&gt;context&lt;/a&gt;控制并发。&lt;/p&gt;

&lt;h2 id=&#34;http&#34;&gt;http&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net-http&#34;&gt;http&lt;/a&gt;实现基本http请求。&lt;/p&gt;

&lt;h2 id=&#34;pprof&#34;&gt;pprof&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net-http-pporf&#34;&gt;pprof&lt;/a&gt;性能调优的工具。&lt;/p&gt;

&lt;h2 id=&#34;interface&#34;&gt;interface&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-interface&#34;&gt;interface&lt;/a&gt;实现接口。&lt;/p&gt;

&lt;h1 id=&#34;技术栈&#34;&gt;技术栈&lt;/h1&gt;

&lt;p&gt;技术栈就是我们开发中经常使用的框架，golang标准库非常丰富，一般我们开发直接可以使用标准库就可以，但是也有一些比较好的技术栈可以直接使用开发，减少重复找轮子的代价，golang开发主要在两个方面：服务器和云计算&lt;/p&gt;

&lt;h2 id=&#34;服务器&#34;&gt;服务器&lt;/h2&gt;

&lt;p&gt;后端服务我们主要使用的是golang的高并发能力和简单的&lt;a href=&#34;https://kingjcy.github.io/post/golang/frame/frame/&#34;&gt;开发框架&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;云计算&#34;&gt;云计算&lt;/h2&gt;

&lt;p&gt;云计算目前主要即使&lt;a href=&#34;https://kingjcy.github.io/post/cloud/cncf/&#34;&gt;k8s以及其生态建设&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;开发工具&#34;&gt;开发工具&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;sublime text&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;尝试了liteide和sublime text，感觉还是sublime text比较好，它支持源码的查看的跳转，编译执行，而liteide在跳转上有点问题，不能跳转到自定义的函数，不知道是不是我配置的问题（基本上就是在查看选项设定了go环境变量），所以自己开发的时候决定用sublime&lt;/p&gt;

&lt;p&gt;sublime text go环境搭建&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在mac上安装go并设置好环境变量&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装一个Package Control，这个应该是一个插件管理工具，用于安装很多其他插件的，只要用ctrl&amp;gt;` 打开命令行输入执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import urllib.request,os,hashlib; h = &#39;7183a2d3e96f11eeadd761d777e62404&#39; &amp;gt;&#39;e330c659d4bb41d3bdf022e94cab3cd0&#39;; pf = &#39;Package Control.sublime-package&#39;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &#39;http://sublime.wbond.Net/&#39; &amp;gt;pf.replace(&#39; &#39;, &#39;%20&#39;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&#39;Error validating download (got %s instead of %s), please try manual install&#39; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &#39;wb&#39; ).write(by)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装好后可以在preferences中找到这个插件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装go插件gosublime&lt;/p&gt;

&lt;p&gt;我们点击Package Control插件或者用shift+ctrl+p来打开，输入pcip（Package Control:Install Package的缩写）也就是安装插件的意思。然后输入gosublime就会自动安装了，安装好了依旧可以在preferences下找到这个gosublime插件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后就可以在gopath下建src，src下建对应的项目，使用sublime进行编码，查看，按command+b进入到shell模式进行编译执行。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;sublime text只能做一个简单的文本编辑器，不算一个完整的IDE,放弃用于golang编程，用于简单的文本编辑器，使用goland编码。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;goland&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一款类似于idea的IDE，同一个公司出品，目前正在使用，比较好用，目前主打开发工具。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;vscode&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用的人也是比较多的，功能强大，主要是占用内存很小只有几十M，相比于goland的几个G来说，好很多&lt;/p&gt;

&lt;h1 id=&#34;go-谚语&#34;&gt;Go 谚语&lt;/h1&gt;

&lt;p&gt;本文译自go-proverbs, 脱胎于 Rob Pike 振奋人心的演讲视频 talk at Gopherfest SV 2015 (bilibili).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;不要通过共享内存进行通信, 通过通信共享内存 (Don&amp;rsquo;t communicate by sharing memory, share memory by communicating)，&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;传统的线程模型（通常在编写 Java, C++和 Python 程序时使用）要求程序员使用共享内存在线程之间进行通信. 通常, 共享数据结构受锁保护, 线程将争夺这些锁访问数据, 在某些情况下, 通过使用 Python 的 Queue 等线程安全的数据结构可以使这变得更容易.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Go 的并发原语 (goroutines 和 channels) 为构造并发软件提供了一种优雅而独特的手段. (这些概念有一个有趣的历史, 要从 C.A.R.Hoare 的通信顺序进程说起.) Go 鼓励使用 channels 在 goroutines 之间传递对数据的引用, 而不是显式地使用锁来调解对共享数据的访问. 这种方法确保只有一个 goroutine 可以在给定的时间访问数据. 这个概念总结在 Effective Go 文档中 (任何 Go 程序员都必须阅读).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Go 官方博客中有一篇文章对该谚语解读, 可以参见原文.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;并发不是并行 (Concurrency is not parallelism)&lt;/p&gt;

&lt;p&gt;当人们听到 并发 这个词的时候, 他们经常会想到并行, 这是一个相关的, 但非常独特的概念. 在编程中, 并发是独立执行的进程的组成, 而并行则是 (可能相关的) 计算的同时执行. 并发是一次处理很多事情. 并行是一次做很多事情.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Channels 重排序; 互斥量串行化 (Channels orchestrate; mutexes serialize)&lt;/p&gt;

&lt;p&gt;这个看中文（翻译待商榷）是不是一脸懵 (虽然英文也看不懂) ? 其实分号前后说的是一个意思, 该谚语按我的个人理解可以用 go 程序 (来自 go tour) 解释成如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;

func sum(s []int, c chan int) {
    sum := 0
    for _, v := range s {
        sum += v
    }
    c &amp;lt;- sum // 此处如果改成互斥量一样可以做到
}

func main() {
    s := []int{7, 2, 8, -9, 4, 0}

    c := make(chan int)
    go sum(s[:len(s)/2], c)
    go sum(s[len(s)/2:], c)
    x, y := &amp;lt;-c, &amp;lt;-c

    fmt.Println(x, y, x+y)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接口越大, 抽象越弱 (The bigger the interface, the weaker the abstraction)&lt;/p&gt;

&lt;p&gt;接口背后的概念是通过将对象的行为抽象为简单的契约来允许重用性. 虽然接口不是 Go 专有的, 但由于 Go 接口通常趋向于小型化, Go 程序员才广泛使用它们. 通常情况下, 一个接口只限于一到两个方法.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Go io 包接口就是典型的例子.&lt;/p&gt;

&lt;p&gt;充分利用零值 (Make the zero value useful)&lt;/p&gt;

&lt;p&gt;零值的典型例子如 bytes.Buffer 和 sync.Mutex:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var buf bytes.Buffer
buf.Write([]byte(&amp;quot;hello&amp;quot;))
fmt.Println(buf.String())

var mu sync.Mutex
mu.Lock()
mu.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样看起来是不是感觉一点用没有 ? 如果这样呢 ?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Map struct {
    mu sync.RWMutex
    // ...
}

func (m *Map) Set(k, v interface{}) {
    m.mu.Lock()
    defer m.mu.Unlock()
    if m.m == nil {
        m.m = make(map[interface{}]interface{})
    }
    m.m[k] = v
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;interface{} 言之无物 (interface{} says nothing)&lt;/p&gt;

&lt;p&gt;该谚语不是说 interface {} 不代表任何东西, 而是说该类型无静态检查以及调用时保证, 比如你的 func 接收一个 interface{} 类型, 你写的时候是可用的, 但是某个时间你进行了代码重构可能坏掉了.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gofmt 的风格没有人喜欢, 但是 gofmt 是每个人的最爱 (Gofmt&amp;rsquo;s style is no one&amp;rsquo;s favorite, yet gofmt is everyone&amp;rsquo;s favorite)&lt;/p&gt;

&lt;p&gt;该谚语告诉我们少些风格之争, 用这些时间多写代码.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;小复制好过小依赖 (A little copying is better than a little dependency)&lt;/p&gt;

&lt;p&gt;简单说就是如果你可以手动撸小快代码就不要导入一个库去做, 比如 UUID:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// see: https://groups.google.com/d/msg/golang-nuts/d0nF_k4dSx4/rPGgfXv6QCoJ
package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os&amp;quot;
)

func main() {
    f, _ := os.Open(&amp;quot;/dev/urandom&amp;quot;) // 演示用忽略 errcheck
    b := make([]byte, 16)
    f.Read(b)
    f.Close()
    uuid := fmt.Sprintf(&amp;quot;%x-%x-%x-%x-%x&amp;quot;, b[0:4], b[4:6], b[6:8], b[8:10], b[10:])
    fmt.Println(uuid)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;虽然有一堆写好的 UUID 库, 当你仅仅需要一个 UUID v4 实现.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;系统调用必须始终使用构建标签保证 (Syscall must always be guarded with build tags)&lt;/p&gt;

&lt;p&gt;不同的系统 (*NIX, Windows) 调用导致你同一个 func (实现并不一样) 可能需要在不同的系统上构建才能得到你想要的结果. 简单说就是系统调用不可移植才这么干. 示例可参见 Go 标准库 syscall.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cgo 必须始终使用构建标签保证 (Cgo must always be guarded with build tags)&lt;/p&gt;

&lt;p&gt;基本上原因同上一条.&lt;/p&gt;

&lt;p&gt;Cgo 不是 Go (Cgo is not Go)&lt;/p&gt;

&lt;p&gt;如果可能不要用 Cgo.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;unsafe 包无保证 (With the unsafe package there are no guarantees)&lt;/p&gt;

&lt;p&gt;包如其名, 不安全. 你可以使用 unsafe 包如果你准备好了有一天它会坏掉.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;清晰好过聪明 (Clear is better than clever)&lt;/p&gt;

&lt;p&gt;Rob Pike 在他与别人合著的 &amp;lt;程序设计实践&amp;gt; 中写到: &amp;ldquo;写清晰的代码, 不要写聪明的代码&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;反射永远不是清晰的 (Reflection is never clear)&lt;/p&gt;

&lt;p&gt;很多人在 Stackoverflow 上抱怨 Go 的反射不工作, 因为那不是为你准备的! 只有很少很少的人应该用反射这个非常强大而又非常难的特性. 新手应该远离反射和 interface{}.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;错误也是一种值 (Errors are values)&lt;/p&gt;

&lt;p&gt;值可以被编程, 并且由于错误是值, 所以错误可以被编程. Go 官方博客有对此的解读.&lt;/p&gt;

&lt;p&gt;不要止步于检查错误而要优雅的处理 (Don&amp;rsquo;t just check errors, handle them gracefully)&lt;/p&gt;

&lt;p&gt;Dave Cheney 有篇博客详细解读了该谚语.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;设计架构, 命名组件, 记录细节 (Design the architecture, name the components, document the details)&lt;/p&gt;

&lt;p&gt;当你写一个大型系统的时候, 你把它设计成一种结构化的东西. 想象组件的每一个部分并行工作, 为不同的组件起好的名字, 因为这些名字会出现在稿纸上.&lt;/p&gt;

&lt;p&gt;拿 Go 程序来说, 如果名字不错, 组件就好理解, 那么程序的结构设计就会清晰, 程序会感觉很自然.&lt;/p&gt;

&lt;p&gt;但是还有很多东西你需要解释, 所以这些是你需要解释的细节. 但是命名会帮助你解释很大一部分设计. 细节只是填补材料的缺口可能用来为用户打印工程图解文档.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文档是针对用户的 (Documentation is for users)&lt;/p&gt;

&lt;p&gt;很多人写文档表明某个 func 是做什么的, 但是他们不想想这个 func 是为谁而写. 这有很大的不同. 你知道这个 func 返回什么是对的, 但是它为什么返回了你使用的时候不一样的结果?&lt;/p&gt;

&lt;p&gt;把自己当成使用者而不是写它的人, 那么 godoc 上的文档就是对用户有用的. 这对于其他语言一样适用.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不要慌 (Don&amp;rsquo;t panic)&lt;/p&gt;

&lt;p&gt;不要使用 panic 进行正常的错误处理. 使用错误 (error) 和多个返回值.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;协程调度是很轻的操作，这当然没错。但他们都没有告诉你更重要的一点：协程调度反复高频出现没有goroutine可供调度的代价在Go的当前实现里是显著的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;好的习惯是，稍大的类型存到map都存储指针而不是值。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;定义数据结构的时候，减少后面使用的转换&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;defer是性能杀手，我的原则是能不用尽量避开。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;能不在循环内部做的，就不要在循环内存处理&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;减少内存的频繁分配，减少使用全局锁的可能&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- M3db</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/</link>
          <pubDate>Wed, 13 Mar 2019 17:13:10 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/</guid>
          <description>&lt;p&gt;Uber开发了指标平台M3和分布式时间序列数据库M3DB。来解决Uber在发展过程当中遇到的问题：使用开源软件后，因为可靠性，成本等问题，在操做密集型方面没法大规模使用这些开源软件。因此Uber逐步构建了本身的指标平台。咱们利用经验来帮助咱们构建本地分布式时间序列数据库，高度动态和高性能的聚合服务，查询引擎以及其余支持基础架构。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;M3包括了以下的组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;M3DB &amp;ndash; M3db是一个使用TSDB（时间数据库），保存全部Prometheus指标，M3db是分布式，高可用性和复制的数据库，它使用Etcd做为共识算法。&lt;/li&gt;
&lt;li&gt;M3Coordinator &amp;ndash; 是Prometheus实例与M3db之间的适配器，它公开了Prometheus用来从数据库中推送和提取数据的读/写端点。&lt;/li&gt;
&lt;li&gt;M3Query &amp;ndash; 众所周知，Prometheus努力处理显示大量数据的查询，而不是从Prometheus提取数据，M3Query实现了相同的PromQL并能够响应此类请求。&lt;/li&gt;
&lt;li&gt;M3Aggregator &amp;ndash; 可选但很重要，此服务将下降指标的采样率，为长期存储作好准备。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总体架构图以下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/m3/m3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关于M3，咱们目前积累了一些生产实践。目前的问题是，社区不够活跃，文档也不够丰富。不少时候遇到问题，只能去研究代码。M3query对PromSql支持的不够，因此M3query并不能生产环境使用。&lt;/p&gt;

&lt;h1 id=&#34;调研&#34;&gt;调研&lt;/h1&gt;

&lt;p&gt;首先，我们想把大量的数据存储到m3中，给prometheus进行查询告警。但是数据量很大，m3db数据插入性能是有必要进行保证的。&lt;/p&gt;

&lt;p&gt;之前写过一个adapter，将数据转化为json调用json api将数据已经插入了m3db中，当时对数据性能没有要求，这次使用json api进行压测的时候，发现性能很差，而且在数据并发达到100个goroutine，一个goroutine发送100条数据，m3db就崩溃了，不接受连接。然后简单的测试了一下得到以下的数据&lt;/p&gt;

&lt;p&gt;这个远远达不到要求啊，于是看看有没有批量操作的接口，官方文档说明，有两种方式插入数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Test RPC
To test out some of the functionality of M3DB there are some user friendly HTTP JSON APIs that you can use. These use the DB node cluster service endpoints.

Note: performance sensitive users are expected to use the more performant endpoints via either the Go src/dbnode/client/Session API, or the GRPC endpoints exposed via src/coordinator.
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;go api（src/dbnode/client/Session），session看代码使用的是apache的thrift rpc。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GRPC（src/coordinator）&lt;/p&gt;

&lt;p&gt;官方提供了benchmark（src/query/benchmark），于是去编译进行测试，但是m3开源的太差了，很多第三方库都是使用的老版本，兼容性很差，各种api对不上，也不把自己的vendor包一同开源，踩了许多坑：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;github.com/thrift &amp;mdash;0.10.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/uber-go/tally&amp;mdash;3.3.7&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;client_golang&amp;mdash;0.8.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/coreos/etcd&amp;mdash;&amp;ndash;3.2.0，还是缺少参数，坑&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;google.golang.org/grpc&amp;ndash;laster&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;golang.org/x/text&amp;mdash;&amp;mdash;laster&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/satori/go.uuid&amp;mdash;-1.2.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/couchbase/vellum&amp;mdash;&amp;ndash;master&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/pilosa/pilosa-最新班都缺少参数&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;还是没有编译完成，后续持续跟进，看看官方有没有继续开源和改进。&lt;/p&gt;

&lt;p&gt;目前得到以下结论&lt;/p&gt;

&lt;p&gt;m3db目前没有发现批量处理的方式&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Coordinator的方式最后还是一条一条的发送（通过查看代码，未能运行）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;session方式，其中一个包github.com/pilosa/pilosa/roaring目前最新版本都没有m3中使用的参数，最终无法编译使用。网上使用session运行成功的，我未能找到他使用了什么版本的github.com/pilosa/pilosa/roaring包。但是通过他运行的结果来看（结合代码api），也是一条一条发送的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;m3中很多都是老版本的库，兼容性很差，api很多不兼容，官方也未推出他使用了什么库，如果继续，应该需要大量的时间去校验和编译&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>算法思想系列---- Raft</title>
          <link>https://kingjcy.github.io/post/algorithm/raft/</link>
          <pubDate>Tue, 12 Mar 2019 16:56:08 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/algorithm/raft/</guid>
          <description>&lt;p&gt;Raft 实际上是一个一致性算法的一种实现，和Paxos等价，但是在实现上，简化了一些，并且更加易用。&lt;/p&gt;

&lt;h1 id=&#34;一致性-consensus&#34;&gt;一致性（consensus）&lt;/h1&gt;

&lt;p&gt;分布式存储系统通常通过维护多个副本来进行容错，提高系统的可用性。要实现此目标，就必须要解决分布式存储系统的最核心问题：维护多个副本的一致性。&lt;/p&gt;

&lt;p&gt;首先需要解释一下什么是一致性（consensus）,它是构建具有容错性（fault-tolerant）的分布式系统的基础。 在一个具有一致性的性质的集群里面，同一时刻所有的结点对存储在其中的某个值都有相同的结果，即对其共享的存储保持一致。集群具有自动恢复的性质，当少数结点失效的时候不影响集群的正常工作，当大多数集群中的结点失效的时候，集群则会停止服务（不会返回一个错误的结果）。&lt;/p&gt;

&lt;p&gt;一致性协议就是用来干这事的，用来保证即使在部分(确切地说是小部分)副本宕机的情况下，系统仍然能正常对外提供服务。一致性协议通常基于replicated state machines，即所有结点都从同一个state出发，都经过同样的一些操作序列（log），最后到达同样的state。&lt;/p&gt;

&lt;p&gt;对于一致性，一致的程度不同大体可以分为强、弱、最终一致性三类。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;强一致性: 对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。比如小明更新V0到V1，那么小华读取的时候也应该是V1。&lt;/li&gt;
&lt;li&gt;弱一致性: 如果能容忍后续的部分或者全部访问不到，则是弱一致性。比如小明更新VO到V1，可以容忍那么小华读取的时候是V0。&lt;/li&gt;
&lt;li&gt;最终一致性: 如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。比如小明更新VO到V1，可以使得小华在一段时间之后读取的时候是V0。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;raft协议实现的是多副本数据的强一致性。&lt;/p&gt;

&lt;h1 id=&#34;rsm复制状态机-replicated-state-machine&#34;&gt;RSM复制状态机(replicated state machine)&lt;/h1&gt;

&lt;p&gt;上面讲到了RSM，我们先来了解一下，一个分布式的复制状态机系统由多个复制单元组成，每个复制单元均是一个状态机，它的状态保存在一组状态变量中，状态机的变量只能通过外部命令来改变。简单理解的话，可以想象成是一组服务器，每个服务器是一个状态机，服务器的运行状态只能通过一行行的命令来改变。每一个状态机存储一个包含一系列指令的日志，严格按照顺序逐条执行日志中的指令，如果所有的状态机都能按照相同的日志执行指令，那么它们最终将达到相同的状态。因此，在复制状态机模型下，只要保证了操作日志的一致性，我们就能保证该分布式系统状态的一致性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在上图中，服务器中的一致性模块(Consensus Modle)接受来自客户端的指令，并写入到自己的日志中，然后通过一致性模块和其他服务器交互，确保每一条日志都能以相同顺序写入到其他服务器的日志中，即便服务器宕机了一段时间。一旦日志命令都被正确的复制，每一台服务器就会顺序的处理命令，并向客户端返回结果。&lt;/p&gt;

&lt;p&gt;系统中每个结点有三个组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;状态机: 当我们说一致性的时候，实际就是在说要保证这个状态机的一致性。状态机会从log里面取出所有的命令，然后执行一遍，得到的结果就是我们对外提供的保证了一致性的数据&lt;/li&gt;
&lt;li&gt;Log: 保存了所有修改记录&lt;/li&gt;
&lt;li&gt;一致性模块: 一致性模块算法就是用来保证写入的log的命令的一致性，这也是raft算法核心内容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了让一致性协议变得简单可理解，Raft协议主要使用了两种策略。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一是将复杂问题进行分解，在Raft协议中，一致性问题被分解为：leader election、log replication、safety三个简单问题；&lt;/li&gt;
&lt;li&gt;二是减少状态空间中的状态数目。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我们详细看一下Raft协议是怎样设计的。&lt;/p&gt;

&lt;h1 id=&#34;基础概念&#34;&gt;基础概念&lt;/h1&gt;

&lt;h2 id=&#34;状态&#34;&gt;状态&lt;/h2&gt;

&lt;p&gt;Raft协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Leader：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步至多个其他副本；&lt;/li&gt;
&lt;li&gt;Follower：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件&lt;/li&gt;
&lt;li&gt;Candidate：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;集群刚启动时，所有节点都是follower，之后在time out信号的驱使下，follower会转变成candidate去拉取选票，获得大多数选票后就会成为leader，这时候如果其他候选人发现了新的leader已经诞生，就会自动转变为follower；而如果另一个time out信号发出时，还没有选举出leader，将会重新开始一次新的选举。可见，time out信号是促使角色转换得关键因素，类似于操作系统中得中断信号。&lt;/p&gt;

&lt;h2 id=&#34;term&#34;&gt;term&lt;/h2&gt;

&lt;p&gt;在Raft协议中，将时间分成了一些任意长度的时间片，称为term，term使用连续递增的编号的进行识别，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每一个term都从新的选举开始，candidate们会努力争取称为leader。一旦获胜，它就会在剩余的term时间内保持leader状态，在某些情况下(如term3)选票可能被多个candidate瓜分，形不成多数派，因此term可能直至结束都没有leader，下一个term很快就会到来重新发起选举。&lt;/p&gt;

&lt;p&gt;term也起到了系统中逻辑时钟的作用，每一个server都存储了当前term编号，在server之间进行交流的时候就会带有该编号，如果一个server的编号小于另一个的，那么它会将自己的编号更新为较大的那一个；如果leader或者candidate发现自己的编号不是最新的了，就会自动转变为follower；如果接收到的请求的term编号小于自己的当前term将会拒绝执行。&lt;/p&gt;

&lt;p&gt;其实就是这个server处于什么时间段的状态，然后用于对比，来处理当前的状态。&lt;/p&gt;

&lt;h2 id=&#34;传输协议&#34;&gt;传输协议&lt;/h2&gt;

&lt;p&gt;server之间的交流是通过RPC进行的。只需要实现两种RPC就能构建一个基本的Raft集群：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RequestVote RPC：它由选举过程中的candidate发起，用于拉取选票&lt;/li&gt;
&lt;li&gt;AppendEntries RPC：它由leader发起，用于复制日志或者发送心跳信号。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft4.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;h2 id=&#34;leader-election&#34;&gt;Leader election&lt;/h2&gt;

&lt;p&gt;Raft通过心跳机制发起leader选举。节点都是从follower状态开始的，如果收到了来自leader或candidate的RPC，那它就保持follower状态，避免争抢成为candidate。Leader会发送空的AppendEntries RPC作为心跳信号来确立自己的地位，如果follower一段时间(election timeout)没有收到心跳，它就会认为leader已经挂了，发起新的一轮选举。&lt;/p&gt;

&lt;p&gt;选举发起后&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Follower将自己维护的current_term_id加1。&lt;/li&gt;
&lt;li&gt;然后将自己的状态转成Candidate&lt;/li&gt;
&lt;li&gt;它会首先投自己一票，然后发送RequestVoteRPC消息(带上current_term_id) 给 其它所有server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个过程会有三种结果：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自己被选成了主。当收到了majority的投票后，状态切成Leader，并且定期给其它的所有server发心跳消息（不带log的AppendEntriesRPC）以告诉对方自己是current_term_id所标识的term的leader。每个term最多只有一个leader，term id作为logical clock，在每个RPC消息中都会带上，用于检测过期的消息。当一个server收到的RPC消息中的rpc_term_id比本地的current_term_id更大时，就更新current_term_id为rpc_term_id，并且如果当前state为leader或者candidate时，将自己的状态切成follower。如果rpc_term_id比本地的current_term_id更小，则拒绝这个RPC消息。&lt;/li&gt;
&lt;li&gt;别人成为了主。如1所述，当Candidator在等待投票的过程中，收到了大于或者等于本地的current_term_id的声明对方是leader的AppendEntriesRPC时，则将自己的state切成follower，并且更新本地的current_term_id。&lt;/li&gt;
&lt;li&gt;没有选出主。当投票被平均瓜分，没有任何一个candidate收到了majority的vote时，没有leader被选出。这种情况下，每个candidate等待的投票的过程就超时了，接着candidates都会将本地的current_term_id再加1，发起RequestVoteRPC进行新一轮的leader election。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;投票策略：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个节点只会给每个term投一票，具体的是否同意和后续的Safety有关。&lt;/li&gt;
&lt;li&gt;当投票被瓜分后，所有的candidate同时超时，然后有可能进入新一轮的票数被瓜分，为了避免这个问题，Raft采用一种很简单的方法：每个Candidate的election timeout从150ms-300ms之间随机取，那么第一个超时的Candidate就可以发起新一轮的leader election，带着最大的term_id给其它所有server发送RequestVoteRPC消息，从而自己成为leader，然后给他们发送心跳消息以告诉他们自己是主。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;log-replication&#34;&gt;Log Replication&lt;/h2&gt;

&lt;p&gt;一旦leader被选举成功，就可以对客户端提供服务了。客户端提交每一条命令都会被按顺序记录到leader的日志中，每一条命令都包含term编号和顺序索引的结构体log entry，然后向其他节点并行发送AppendEntries RPC用以复制命令(如果命令丢失会不断重发)，当复制成功也就是大多数节点成功复制后，leader就会提交命令，即执行该命令并且将执行结果返回客户端，raft保证已经提交的命令最终也会被其他节点成功执行。leader会保存有当前已经提交的最高日志编号。顺序性确保了相同日志索引处的命令是相同的，而且之前的命令也是相同的。当发送AppendEntries RPC时，会包含leader上一条刚处理过的命令，接收节点如果发现上一条命令不匹配，就会拒绝执行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft19&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;日志冲突&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在这个过程中可能会出现一种特殊故障：如果leader崩溃了，它所记录的日志没有完全被复制，会造成日志不一致的情况，follower相比于当前的leader可能会丢失几条日志，也可能会额外多出几条日志，这种情况可能会持续几个term。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在上图中，框内的数字是term编号，a、b丢失了一些命令，c、d多出来了一些命令，e、f既有丢失也有增多，这些情况都有可能发生。比如f可能发生在这样的情况下：f节点在term2时是leader，在此期间写入了几条命令，然后再提交之前崩溃了，在之后的term3种它很快重启并再次成为leader，又写入了几条日志，在提交之前又崩溃了，等他苏醒过来时新的leader来了，就形成了上图情形。&lt;/p&gt;

&lt;p&gt;因此，需要有一种机制来让leader和follower对log达成一致，在Raft中，leader通过强制follower复制自己的日志来解决上述日志不一致的情形，那么冲突的日志将会被重写。为了让日志一致，先找到最新的一致的那条日志(如f中索引为3的日志条目)，然后把follower之后的日志全部删除，leader再把自己在那之后的日志一股脑推送给follower，这样就实现了一致。而寻找该条日志，可以通过AppendEntries RPC，该RPC中包含着下一次要执行的命令索引，如果能和follower的当前索引对上，那就执行，否则拒绝，然后leader将会逐次递减索引，直到找到相同的那条日志。&lt;/p&gt;

&lt;p&gt;leader会为每个follower维护一个nextIndex，表示leader给各个follower发送的下一条log entry在log中的index，初始化为leader的最后一条log entry的下一个位置。leader给follower发送AppendEntriesRPC消息，带着(term_id, (nextIndex-1))， term_id即(nextIndex-1)这个槽位的log entry的term_id，follower接收到AppendEntriesRPC后，会从自己的log中找是不是存在这样的log entry，如果不存在，就给leader回复拒绝消息，然后leader则将nextIndex减1，再重复，知道AppendEntriesRPC消息被接收。&lt;/p&gt;

&lt;p&gt;然而这样也还是会有问题，比如某个follower在leader提交时宕机了，也就是少了几条命令，然后它又经过选举成了新的leader，这样它就会强制其他follower跟自己一样，使得其他节点上刚刚提交的命令被删除，导致客户端提交的一些命令被丢失了，下面一节内容将会解决这个问题。Raft通过为选举过程添加一个限制条件，解决了上面提出的问题，该限制确保leader包含之前term已经提交过的所有命令。Raft通过投票过程确保只有拥有全部已提交日志的candidate能成为leader。由于candidate为了拉选票需要通过RequestVote RPC联系其他节点，而之前提交的命令至少会存在于其中某一个节点上,因此只要candidate的日志至少和其他大部分节点的一样新就可以了, follower如果收到了不如自己新的candidate的RPC,就会将其丢弃.&lt;/p&gt;

&lt;p&gt;还可能会出现另外一个问题, 如果命令已经被复制到了大部分节点上,但是还没来的及提交就崩溃了,这样后来的leader应该完成之前term未完成的提交. Raft通过让leader统计当前term内还未提交的命令已经被复制的数量是否半数以上, 然后进行提交.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;日志压缩&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;随着日志大小的增长，会占用更多的内存空间，处理起来也会耗费更多的时间，对系统的可用性造成影响，因此必须想办法压缩日志大小。Snapshotting是最简单的压缩方法，系统的全部状态会写入一个snapshot保存起来，然后丢弃截止到snapshot时间点之前的所有日志。Raft中的snapshot内容如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每一个server都有自己的snapshot，它只保存当前状态，如上图中的当前状态为x=0,y=9，而last included index和last included term代表snapshot之前最新的命令，用于AppendEntries的状态检查。&lt;/p&gt;

&lt;p&gt;Snapshot中包含以下内容：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日志元数据，最后一条commited log entry的 (log index, last_included_term)。这两个值在Snapshot之后的第一条log entry的AppendEntriesRPC的consistency check的时候会被用上，之前讲过。一旦这个server做完了snapshot，就可以把这条记录的最后一条log index及其之前的所有的log entry都删掉。&lt;/li&gt;
&lt;li&gt;系统状态机：存储系统当前状态（这是怎么生成的呢？）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;虽然每一个server都保存有自己的snapshot，但是当follower严重落后于leader时，leader需要把自己的snapshot发送给follower加快同步，此时用到了一个新的RPC：InstallSnapshot RPC。follower收到snapshot时，需要决定如何处理自己的日志，如果收到的snapshot包含有更新的信息，它将丢弃自己已有的日志，按snapshot更新自己的状态，如果snapshot包含的信息更少，那么它会丢弃snapshot中的内容，但是自己之后的内容会保存下来。RPC的定义如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;safety&#34;&gt;Safety&lt;/h2&gt;

&lt;p&gt;哪些follower有资格成为leader?&lt;/p&gt;

&lt;p&gt;Raft保证被选为新leader的节点拥有所有已提交的log entry，这与ViewStamped Replication不同，后者不需要这个保证，而是通过其他机制从follower拉取自己没有的提交的日志记录
这个保证是在RequestVoteRPC阶段做的，candidate在发送RequestVoteRPC时，会带上自己的最后一条日志记录的term_id和index，其他节点收到消息时，如果发现自己的日志比RPC请求中携带的更新，拒绝投票。日志比较的原则是，如果本地的最后一条log entry的term id更大，则更新，如果term id一样大，则日志更多的更大(index更大)。&lt;/p&gt;

&lt;p&gt;哪些日志记录被认为是commited?&lt;/p&gt;

&lt;p&gt;leader正在replicate当前term（即term 2）的日志记录给其它Follower，一旦leader确认了这条log entry被majority写盘了，这条log entry就被认为是committed。如图a，S1作为当前term即term2的leader，log index为2的日志被majority写盘了，这条log entry被认为是commited
leader正在replicate更早的term的log entry给其它follower。图b的状态是这么出来的。&lt;/p&gt;

&lt;p&gt;对协议的一点修正&lt;/p&gt;

&lt;p&gt;在实际的协议中，需要进行一些微调，这是因为可能会出现下面这种情况：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft9&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在阶段a，term为2，S1是Leader，且S1写入日志（term, index）为(2, 2)，并且日志被同步写入了S2；&lt;/li&gt;
&lt;li&gt;在阶段b，S1离线，触发一次新的选主，此时S5被选为新的Leader，此时系统term为3，且写入了日志（term, index）为（3， 2）;&lt;/li&gt;
&lt;li&gt;S5尚未将日志推送到Followers变离线了，进而触发了一次新的选主，而之前离线的S1经过重新上线后被选中变成Leader，此时系统term为4，此时S1会将自己的日志同步到Followers，按照上图就是将日志（2， 2）同步到了S3，而此时由于该日志已经被同步到了多数节点（S1, S2, S3），因此，此时日志（2，2）可以被commit了（即更新到状态机）；&lt;/li&gt;
&lt;li&gt;在阶段d，S1又很不幸地下线了，系统触发一次选主，而S5有可能被选为新的Leader（这是因为S5可以满足作为主的一切条件：1. term = 3 &amp;gt; 2, 2. 最新的日志index为2，比大多数节点（如S2/S3/S4的日志都新），然后S5会将自己的日志更新到Followers，于是S2、S3中已经被提交的日志（2，2）被截断了，这是致命性的错误，因为一致性协议中不允许出现已经应用到状态机中的日志被截断。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了避免这种致命错误，需要对协议进行一个微调：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;只允许主节点提交包含当前term的日志
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;针对上述情况就是：即使日志（2，2）已经被大多数节点（S1、S2、S3）确认了，但是它不能被Commit，因为它是来自之前term(2)的日志，直到S1在当前term（4）产生的日志（4， 3）被大多数Follower确认，S1方可Commit（4，3）这条日志，当然，根据Raft定义，（4，3）之前的所有日志也会被Commit。此时即使S1再下线，重新选主时S5不可能成为Leader，因为它没有包含大多数节点已经拥有的日志（4，3）。&lt;/p&gt;

&lt;h1 id=&#34;实战&#34;&gt;实战&lt;/h1&gt;

&lt;h2 id=&#34;raft在etcd中的实现&#34;&gt;raft在etcd中的实现&lt;/h2&gt;

&lt;p&gt;etcd 是一个被广泛应用于共享配置和服务发现的分布式、一致性的 kv 存储系统。作为分布式 kv，其底层使用的 是 raft 算法来实现多副本数据的强一致复制，etcd-raft 作为 raft 开源实现的杰出代表，在设计上，将 raft 算法逻辑和持久化、网络、线程等完全抽离出来单独实现，充分解耦，在工程上，实现了诸多性能优化，是 raft 开源实践中较早的工业级的实现，很多后来的 raft 实践者都直接或者间接的参考了 ectd-raft 的设计和实现，算是 raft 实现的一个典范。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft10&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;功能支持：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Election（vote）：选举&lt;/li&gt;
&lt;li&gt;Pre-vote：在发起 election vote 之前，先进行 pre-vote，可以避免在网络分区的情况避免反复的 election 打断当前 leader，触发新的选举造成可用性降低的问题&lt;/li&gt;
&lt;li&gt;Config changes：配置变更，增加，删除节点等&lt;/li&gt;
&lt;li&gt;Leaner：leaner 角色，仅参与 log replication，不参与投票和提交的 Op log entry，增加节点时，复制追赶 使用 leader 角色&lt;/li&gt;
&lt;li&gt;Transfer leader：主动变更 Leader，用于关机维护，leader 负载等&lt;/li&gt;
&lt;li&gt;ReadIndex：优化 raft read 走 Op log 性能问题，每次 read Op，仅记录 commit index，然后发送所有 peers heartbeat 确认 leader 身份，如果 leader 身份确认成功，等到 applied index &amp;gt;= commit index，就可以返回 client read 了&lt;/li&gt;
&lt;li&gt;Lease read：通过 lease 保证 leader 的身份，从而省去了 ReadIndex 每次 heartbeat 确认 leader 身份，性能更好，但是通过时钟维护 lease 本身并不是绝对的安全&lt;/li&gt;
&lt;li&gt;snapshot：raft 主动生成 snapshot，实现 log compact 和加速启动恢复，install snapshot 实现给 follower 拷贝数据等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从功能上，etcd-raft 完备的实现了 raft 几乎所需的功能。&lt;/p&gt;

&lt;p&gt;性能优化：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Batch：网络batch发送、batch持久化 Op log entries到WAL&lt;/li&gt;
&lt;li&gt;Pipeline：Leader 向 Follower 发送 Message 可以 pipeline 发送的（相对的 ping-pong 模式发送和接收）（pipeline 是grpc的一重要特性）&lt;/li&gt;
&lt;li&gt;Append Log Parallelly：Leader 发送 Op log entries message 给 Followers 和 Leader 持久化 Op log entries 是并行的&lt;/li&gt;
&lt;li&gt;Asynchronous Apply：由单独的 coroutine（协程） 负责异步的 Apply&lt;/li&gt;
&lt;li&gt;Asynchronous GC：WAL 和 snapshot 文件会分别开启单独的 coroutine 进行 GC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;etcd-raft 几乎实现了 raft 大论文和工程上该有的性能优化，实际上 ReadIndex 和 Lease Read 本身也算是性能优化。&lt;/p&gt;

&lt;h3 id=&#34;架构设计&#34;&gt;架构设计&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft11&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上图，整个 etcd-server 的整体架构，其主要分为三层：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;网络层&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;图中最上面一层是网络层，负责使用 grpc 收发 etcd-raft 和 client 的各种 messages，etcd-raft 会通过网络层收发各种 message，包括 raft append entries、vote、client 发送过来的 request，以及 response 等等，其都是由 rpc 的 coroutine 完成（PS：可以简单理解所有 messages 都是通过网络模块异步收发的）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;持久化层&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;图中最下面一层是持久化层，其提供了对 raft 各种数据的持久化存储，WAL - 持久化 raft Op log entries；Snapshot - 持久化 raft snapshot；KV - raft apply 的数据就是写入 kv 存储中，因为 etcd 是一个分布式的 kv 存储，所以，对 raft 来说，applied 的数据自然也就是写入到 kv 中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Raft 层&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;中间这一层就是 raft 层，也是 etcd-raft 的核心实现。etcd 设计上将 raft 算法的逻辑和持久化、网络、线程（实际上是 coroutine）等完全解耦成一个单独的模块，拍脑袋思考下，将网络、持久化层抽离出来，并不难，但是如何将 raft 算法逻辑和网络、持久化、coroutine 完成解耦呢 ？&lt;/p&gt;

&lt;p&gt;其核心的思路就是将 raft 所有算法逻辑实现封装成一个 StateMachine，也就是图中的 raft StateMachine，注意和 raft 复制状态机区别，这里 raft StateMachine 只是对 raft 算法的多个状态 （Leader、Follower、Candidate 等），多个阶段的一种代码实现，类似网络处理实现中也会通过一个 StateMachine 来实现网络 message 异步不同阶段的处理。&lt;/p&gt;

&lt;p&gt;为了更加形象，这里以 client 发起 一个 put kv request 为例子，来看看 raft StateMachine 的输入、运转和输出，这里以 Leader 为例，分为如下阶段：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一阶段：client 发送一个 put kv request 给 etcd server，grpc server 解析后，生成一个 Propose Message作为 raft StateMachine 输入，如果你驱动 raft StateMachine 运转，就会生成两个输出，一个需要写入 WAL 的 Op log entry，2 条发送给另外两个副本的 Append entries Msg，输出会封装在 Ready 结构中&lt;/li&gt;
&lt;li&gt;第二阶段：如果把第一阶段的输出 WAL 写到了盘上，并且把 Append entries Msg 发送给了其他两个副本，那么两个副本会收到 Append entries Msg，持久化之后就会给 Leader 返回 Append entries Response Msg，etcd server 收到 Msg 之后，依然作为输入交给 raft StateMachine 处理，驱动 StateMachine 运转，如果超过大多数 response，那么就会产生输出：已经 commit 的 committed entries&lt;/li&gt;
&lt;li&gt;第三阶段：外部将上面 raft StateMachine 输出 committed entries 拿到后，然后就可以返回 client put kv success 的 response 了&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上面的例子展示了 raft StateMachine 输入，输出，运转的情况，尽管已经有了网络层和持久化层，但是，显然还缺少很多其的模块，例如：coroutine 驱动状态机运转，coroutine 将驱动网络发 message 和 持久化写盘等，下面介绍的raft 层的三个小模块就是完成这些事情的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;raft StateMachine&lt;/p&gt;

&lt;p&gt;就是一个 raft 算法的逻辑实现，其输入统一被抽象成了 Msg，输出则统一封装在 Ready 结构中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;node（raft StateMachine 接口 - 输入+运转）&lt;/p&gt;

&lt;p&gt;node 模块提供了如下功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;raft StateMachine 和外界交互的接口，就是提供ectd使用一致性协议算法的接口，供上层向 raft StateMachine 提交 request，也就是输入，已上面例子的 put kv request 为例，就是通过func (n *node) Propose(ctx context.Context, data []byte)接口向 raft StateMachine 提交一个 Propose，这个接口将用户请求转换成 raft StateMachine 认识的 MsgProp Msg，并通过 Channel 传递给驱动 raft StateMachine 运转的 coroutine；&lt;/li&gt;
&lt;li&gt;提供驱动 raft 运转的 coroutine，其负责监听在各个 Msg 输入 Channel 中，一旦收到 Msg 就会调用 raft StateMachine 处理 Msg 接口 func (r *raft) Step(m pb.Message) 得到输出 Ready 结构，并将 Channel 传递给其他 coroutine 处理&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;raftNode（处理 raft StateMachine 输出 Ready）&lt;/p&gt;

&lt;p&gt;raftNode 模块会有一个 coroutine，负责从处理 raft StateMachine 的输出 Ready 结构，该持久化的调用持久化的接口持久化，该发送其他副本的，通过网络接口发送给其他副本，该 apply 的提交给其他 coroutine apply。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;交互架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft17&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其中红色虚线框起来的代表一个 coroutine，下面将对各个协程的作用基本的描述&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ticker：golang 的 Ticker struct 会定期触发 Tick 滴答时钟，etcd raft 的都是通过滴答时钟往前推进，从而触发相应的 heartbeat timeout 和 election timeout，从而触发发送心跳和选举。&lt;/li&gt;
&lt;li&gt;ReadLoop：这个 coroutine 主要负责处理 Read request，负责将 Read 请求通过 node 模块的 Propose 提交给 raft StateMachine，然后监听 raft StateMachine，一旦 raft StateMachine 完成 read 请求的处理，会通过 readStateC 通知 ReadLoop coroutine 此 read 的commit index，然后 ReadLoop coroutine 就可以根据当前 applied index 的推进情况，一旦 applied index &amp;gt;= commit index，ReadLoop coroutine 就会 Read 数据并通过网络返回 client read response&lt;/li&gt;
&lt;li&gt;raftNode：raftNode 模块会有一个 coroutine 负责处理 raft StateMachine 的输出 Ready，上文已经描述了，这里不在赘述&lt;/li&gt;
&lt;li&gt;node：node 模块也会有一个 coroutine 负责接收输入，运行状态机和准备输出，上文已经描述，这里不在赘述&lt;/li&gt;
&lt;li&gt;apply：raftNode 模块在 raft StateMachine 输出 Ready 中已经 committed entries 的时候，会将 apply 逻辑放在单独的 coroutine 处理，这就是 Async apply。&lt;/li&gt;
&lt;li&gt;GC：WAL 和 snapshot 的 GC 回收也都是分别在两个单独的 coroutine 中完成的。etcd 会在配置文中分别设置 WAL 和 snapshot 文件最大数量，然后两个 GC 后台异步 GC&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;代码分析&#34;&gt;代码分析&lt;/h3&gt;

&lt;p&gt;Etcd将raft协议实现为一个library，然后本身作为一个应用使用它。当然，可能是为了推广它所实现的这个library，etcd还额外提供了一个叫&lt;a href=&#34;https://github.com/etcd-io/etcd/tree/v3.3.10/contrib/raftexample&#34;&gt;raftexample&lt;/a&gt;的示例程序，向用户展示怎样在它所提供的raft library的基础上构建出一个分布式的KV存储引擎。&lt;/p&gt;

&lt;p&gt;我们来看看raft library，其实也就是raft层的实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tree --dirsfirst -L 1 -I &#39;*test*&#39; -P &#39;*.go&#39;
.
├── raftpb
├── doc.go
├── log.go
├── log_unstable.go
├── logger.go
├── node.go
├── progress.go
├── raft.go
├── rawnode.go
├── read_only.go
├── status.go
├── storage.go
└── util.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来详细说明&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;raftpb&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Raft中的序列化是借助于Protocol Buffer来实现的，这个文件夹就定义了需要序列化的几个数据结构，比如Entry和Message。&lt;/p&gt;

&lt;p&gt;1、Entry&lt;/p&gt;

&lt;p&gt;从整体上来说，一个集群中的每个节点都是一个状态机，而raft管理的就是对这个状态机进行更改的一些操作，这些操作在代码中被封装为一个个Entry。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/raftpb/raft.pb.go#L203
type Entry struct {
    Term             uint64
    Index            uint64
    Type             EntryType
    Data             []byte
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Term：选举任期，每次选举之后递增1。它的主要作用是标记信息的时效性，比方说当一个节点发出来的消息中携带的term是2，而另一个节点携带的term是3，那我们就认为第一个节点的信息过时了。&lt;/li&gt;
&lt;li&gt;Index：当前这个entry在整个raft日志中的位置索引。有了Term和Index之后，一个log entry就能被唯一标识。&lt;/li&gt;
&lt;li&gt;Type：当前entry的类型，目前etcd支持两种类型：EntryNormal和EntryConfChange，EntryNormal代表当前Entry是对状态机的操作，EntryConfChange则代表对当前集群配置进行更改的操作，比如增加或者减少节点。&lt;/li&gt;
&lt;li&gt;Data：一个被序列化后的byte数组，代表当前entry真正要执行的操作，比方说如果上面的Type是EntryNormal，那这里的Data就可能是具体要更改的key-value pair，如果Type是EntryConfChange，那Data就是具体的配置更改项ConfChange。raft算法本身并不关心这个数据是什么，它只是把这段数据当做log同步过程中的payload来处理，具体对这个数据的解析则有上层应用来完成。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、Message&lt;/p&gt;

&lt;p&gt;Raft集群中节点之间的通讯都是通过传递不同的Message来完成的，这个Message结构就是一个非常general的大容器，它涵盖了各种消息所需的字段。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/raftpb/raft.pb.go#L239
type Message struct {
    Type             MessageType
    To               uint64
    From             uint64
    Term             uint64
    LogTerm          uint64
    Index            uint64
    Entries          []Entry
    Commit           uint64
    Snapshot         Snapshot
    Reject           bool
    RejectHint       uint64
    Context          []byte
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Type：当前传递的消息类型，它的取值有很多个，但大致可以分成两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Raft 协议相关的，包括心跳MsgHeartbeat、日志MsgApp、投票消息MsgVote等。&lt;/li&gt;
&lt;li&gt;上层应用触发的（没错，上层应用并不是通过api与raft库交互的，而是通过发消息），比如应用对数据更改的消息MsgProp(osal)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所有的 Msg 在 bp.Message 中详细定义，下面给出所有的 message 类型并且依次介绍：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    const (
        MsgHup            MessageType = 0   // 本地消息：选举，可能会触发 pre-vote 或者 vote
        MsgBeat           MessageType = 1   // 本地消息：心跳，触发放给 peers 的 Msgheartbeat
        MsgProp           MessageType = 2   // 本地消息：Propose，触发 MsgApp
        MsgApp            MessageType = 3   // 非本地：Op log 复制/配置变更 request
        MsgAppResp        MessageType = 4   // 非本地：Op log 复制 response
        MsgVote           MessageType = 5   // 非本地：vote request
        MsgVoteResp       MessageType = 6   // 非本地：vote response
        MsgSnap           MessageType = 7   // 非本地：Leader 向 Follower 拷贝 Snapshot，response Message 就是 MsgAppResp，通过这个值告诉 Leader 继续复制后面的日志
        MsgHeartbeat      MessageType = 8   // 非本地：心跳 request
        MsgHeartbeatResp  MessageType = 9   // 非本地：心跳 response
        MsgUnreachable    MessageType = 10  // 本地消息：EtcdServer 通过这个消息告诉 raft 状态某个 Follower 不可达，让其发送 message方式由 pipeline 切成 ping-pong 模式
        MsgSnapStatus     MessageType = 11  // 本地消息：EtcdServer 通过这个消息告诉 raft 状态机 snapshot 发送成功还是失败
        MsgCheckQuorum    MessageType = 12  // 本地消息：CheckQuorum，用于 Lease read，Leader lease
        MsgTransferLeader MessageType = 13  // 本地消息：可能会触发一个空的 MsgApp 尽快完成日志复制，也有可能是 MsgTimeoutNow 出 Transferee 立即进入选举
        MsgTimeoutNow     MessageType = 14  // 非本地：触发 Transferee 立即进行选举
        MsgReadIndex      MessageType = 15  // 非本地：Read only ReadIndex
        MsgReadIndexResp  MessageType = 16  // 非本地：Read only ReadIndex response
        MsgPreVote        MessageType = 17  // 非本地：pre vote request
        MsgPreVoteResp    MessageType = 18  // 非本地：pre vote response
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不同类型的消息会用到下面不同的字段：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To, From分别代表了这个消息的接受者和发送者。&lt;/li&gt;
&lt;li&gt;Term：这个消息发出时整个集群所处的任期。&lt;/li&gt;
&lt;li&gt;LogTerm：消息发出者所保存的日志中最后一条的任期号，一般MsgVote会用到这个字段。&lt;/li&gt;
&lt;li&gt;Index：日志索引号。如果当前消息是MsgVote的话，代表这个candidate最后一条日志的索引号，它跟上面的LogTerm一起代表这个candidate所拥有的最新日志信息，这样别人就可以比较自己的日志是不是比candidata的日志要新，从而决定是否投票。&lt;/li&gt;
&lt;li&gt;Entries：需要存储的日志。&lt;/li&gt;
&lt;li&gt;Commit：已经提交的日志的索引值，用来向别人同步日志的提交信息。&lt;/li&gt;
&lt;li&gt;Snapshot：一般跟MsgSnap合用，用来放置具体的Snapshot值。&lt;/li&gt;
&lt;li&gt;Reject，RejectHint：代表对方节点拒绝了当前节点的请求(MsgVote/MsgApp/MsgSnap…)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;log_unstable.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;log_unstable顾名思义，unstable数据结构用于还没有被用户层持久化的数据，它维护了两部分内容snapshot和entries:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/log_unstable.go#L23
type unstable struct {
    // the incoming unstable snapshot, if any.
    snapshot *pb.Snapshot
    // all entries that have not yet been written to storage.
    entries []pb.Entry
    offset  uint64

    logger Logger
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;entries代表的是要进行操作的日志&lt;/li&gt;
&lt;li&gt;snapshot代表快照数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两者的关系是相辅相成的，共同组成了全部的数据，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft13&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里的前半部分是快照数据，而后半部分是日志条目组成的数组entries，另外unstable.offset成员保存的是entries数组中的第一条数据在raft日志中的索引，即第i条entries在raft日志中的索引为i + unstable.offset。在同步的时候，如果entries日志不能完全同步，就需要使用到snapshot。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;storage.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个文件定义了一个Storage接口，因为etcd中的raft实现并不负责数据的持久化，所以它希望上面的应用层能实现这个接口，以便提供给它查询log的能力。&lt;/p&gt;

&lt;p&gt;另外，这个文件也提供了Storage接口的一个内存版本的实现MemoryStorage，这个实现同样也维护了snapshot和entries这两部分，他们的排列跟unstable中的类似，也是snapshot在前，entries在后。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;log.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边主要实现raftLog，这个结构体承担了raft日志相关的操作。&lt;/p&gt;

&lt;p&gt;raftLog由以下成员组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;storage Storage：前面提到的存放已经持久化数据的Storage接口。&lt;/li&gt;
&lt;li&gt;unstable unstable：前面分析过的unstable结构体，用于保存应用层还没有持久化的数据。&lt;/li&gt;
&lt;li&gt;committed uint64：保存当前提交的日志数据索引。&lt;/li&gt;
&lt;li&gt;applied uint64：保存当前传入状态机的数据最高索引。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;raftLog结构体中，几部分数据的排列如下图所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft14&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个数据排布的情况，可以从raftLog的初始化函数中看出来：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/log.go#L45
// newLog returns log using the given storage. It recovers the log to the state
// that it just commits and applies the latest snapshot.
func newLog(storage Storage, logger Logger) *raftLog {
    if storage == nil {
        log.Panic(&amp;quot;storage must not be nil&amp;quot;)
    }
    log := &amp;amp;raftLog{
        storage: storage,
        logger:  logger,
    }
    firstIndex, err := storage.FirstIndex()
    if err != nil {
        panic(err) // TODO(bdarnell)
    }
    lastIndex, err := storage.LastIndex()
    if err != nil {
        panic(err) // TODO(bdarnell)
    }
    log.unstable.offset = lastIndex + 1
    log.unstable.logger = logger
    // Initialize our committed and applied pointers to the time of the last compaction.
    log.committed = firstIndex - 1
    log.applied = firstIndex - 1

    return log
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从这里的代码可以看出，raftLog的两部分，持久化存储和非持久化存储，它们之间的分界线就是lastIndex，在此之前都是Storage管理的已经持久化的数据，而在此之后都是unstable管理的还没有持久化的数据。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;progress.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Leader通过Progress这个数据结构来追踪一个follower的状态，并根据Progress里的信息来决定每次同步的日志项。这里介绍三个比较重要的属性：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/progress.go#L37
// Progress represents a follower’s progress in the view of the leader. Leader maintains
// progresses of all followers, and sends entries to the follower based on its progress.
type Progress struct {
    Match, Next uint64

    State ProgressStateType

    ins *inflights
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;用来保存当前follower节点的日志状态的属性

&lt;ul&gt;
&lt;li&gt;Match：保存目前为止，已复制给该follower的日志的最高索引值。如果leader对该follower上的日志情况一无所知的话，这个值被设为0。&lt;/li&gt;
&lt;li&gt;Next：保存下一次leader发送append消息给该follower的日志索引，即下一次复制日志时，leader会从Next开始发送日志。
在正常情况下，Next = Match + 1，也就是下一个要同步的日志应当是对方已有日志的下一条。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;State属性用来保存该节点当前的同步状态，它会有一下几种取值

&lt;ul&gt;
&lt;li&gt;ProgressStateProbe
探测状态，当follower拒绝了最近的append消息时，那么就会进入探测状态，此时leader会试图继续往前追溯该follower的日志从哪里开始丢失的。在probe状态时，leader每次最多append一条日志，如果收到的回应中带有RejectHint信息，则回退Next索引，以便下次重试。在初始时，leader会把所有follower的状态设为probe，因为它并不知道各个follower的同步状态，所以需要慢慢试探。&lt;/li&gt;
&lt;li&gt;ProgressStateReplicate
当leader确认某个follower的同步状态后，它就会把这个follower的state切换到这个状态，并且用pipeline的方式快速复制日志。leader在发送复制消息之后，就修改该节点的Next索引为发送消息的最大索引+1。&lt;/li&gt;
&lt;li&gt;ProgressStateSnapshot
接收快照状态。当leader向某个follower发送append消息，试图让该follower状态跟上leader时，发现此时leader上保存的索引数据已经对不上了，比如leader在index为10之前的数据都已经写入快照中了，但是该follower需要的是10之前的数据，此时就会切换到该状态下，发送快照给该follower。当快照数据同步追上之后，并不是直接切换到Replicate状态，而是首先切换到Probe状态。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ins属性用来做流量控制，因为如果同步请求非常多，再碰上网络分区时，leader可能会累积很多待发送消息，一旦网络恢复，可能会有非常大流量发送给follower，所以这里要做flow control。它的实现有点类似TCP的滑动窗口，这里不再赘述。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Progress其实也是个状态机，下面是它的状态转移图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft15&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;raft.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;raft协议的具体实现就在这个文件里。这其中，大部分的逻辑是由Step函数驱动的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/raft.go#L752
func (r *raft) Step(m pb.Message) error {
    //...
    switch m.Type {
        case pb.MsgHup:
        //...
        case pb.MsgVote, pb.MsgPreVote:
        //...
        default:
            r.step(r, m)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Step的主要作用是处理不同的消息,调用不同的处理函数，step属性是一个函数指针，根据当前节点的不同角色，指向不同的消息处理函数：stepLeader/stepFollower/stepCandidate。与它类似的还有一个tick函数指针，根据角色的不同，也会在tickHeartbeat和tickElection之间来回切换，分别用来触发定时心跳和选举检测。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft16&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;node.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;node其实就是消息通道，or-select-channel组成的事件循环。&lt;/p&gt;

&lt;p&gt;node的主要作用是应用层（etcdserver）和共识模块（raft）的衔接。将应用层的消息传递给底层共识模块，并将底层共识模块共识后的结果反馈给应用层。所以它的初始化函数创建了很多用来通信的channel，然后就在另一个goroutine里面开始了事件循环，不停的在各种channel中倒腾数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/node.go#L286
for {
  select {
    case m := &amp;lt;-propc:
        r.Step(m)
    case m := &amp;lt;-n.recvc:
        r.Step(m)
    case cc := &amp;lt;-n.confc:
        // Add/remove/update node according to cc.Type
    case &amp;lt;-n.tickc:
        r.tick()
    case readyc &amp;lt;- rd:
        // Cleaning after result is consumed by application
    case &amp;lt;-advancec:
        // Stablize logs
    case c := &amp;lt;-n.status:
        // Update status
    case &amp;lt;-n.stop:
        close(n.done)
        return
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;输入（msg）：propc和recvc中拿到的是从上层应用传进来的消息，这个消息会被交给raft层的Step函数处理。&lt;/p&gt;

&lt;p&gt;所有的外部处理请求经过 raft StateMachine 处理都会首先被转换成统一抽象的输入 Message（Msg），Msg 会通过 raft.Step(m) 接口完成 raft StateMachine 的处理，Msg 分两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;本地 Msg，term = 0，这种 Msg 并不会经过网络发送给 Peer，只是将 Node 接口的一些请求转换成 raft StateMachine 统一处理的抽象 Msg，这里以 Propose 接口为例，向 raft 提交一个 Op 操作，其会被转换成 MsgProp，通过 raft.Step() 传递给 raft StateMachine，最后可能被转换成给 Peer 复制 Op log 的 MsgApp Msg；（即发送给本地peer的消息）&lt;/li&gt;
&lt;li&gt;非本地 Msg，term 非 0，这种 Msg 会经过网络发送给 Peer；这里以 Msgheartbeat 为例子，就是 Leader 给 Follower 发送的心跳包。但是这个 MsgHeartbeat Msg 是通过 Tick 接口传入的，这个接口会向 raft StateMachine 传递一个 MsgBeat Msg，raft StateMachine 处理这个 MsgBeat 就是向复制组其它 Peer 分别发送一个 MsgHeartbeat Msg&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体的类型我们在上面的结构体定义的时候已经说明过了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;输出（ready）：readyc这个channel对外通知有数据要处理了，并将这些需要外部处理的数据打包到一个Ready结构体中，其实就是底层消息传递到应用层。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于 etcd 的网络、持久化模块和 raft 核心是分离的，所以当 raft 处理到某一些阶段的时候，需要输出一些东西，给外部处理，例如 Op log entries 持久化，Op log entries 复制的 Msg 等；以 heartbeat 为例，输入是 MsgBeat Msg，经过状态机状态化之后，就变成了给复制组所有的 Peer 发送心跳的 MsgHeartbeat Msg；在 ectd 中就是通过一个 Ready 的数据结构来封装当前 Raft state machine 已经准备好的数据和 Msg 供外部处理。&lt;/p&gt;

&lt;p&gt;我们来看看这个Ready结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/node.go#L52
// Ready encapsulates the entries and messages that are ready to read,
// be saved to stable storage, committed or sent to other peers.
// All fields in Ready are read-only.
type Ready struct {
    // The current volatile state of a Node.
    // SoftState will be nil if there is no update.
    // It is not required to consume or store SoftState.
    *SoftState

    // The current state of a Node to be saved to stable storage BEFORE
    // Messages are sent.
    // HardState will be equal to empty state if there is no update.
    pb.HardState

    // ReadStates can be used for node to serve linearizable read requests locally
    // when its applied index is greater than the index in ReadState.
    // Note that the readState will be returned when raft receives msgReadIndex.
    // The returned is only valid for the request that requested to read.
    ReadStates []ReadState

    // Entries specifies entries to be saved to stable storage BEFORE
    // Messages are sent.
    Entries []pb.Entry

    // Snapshot specifies the snapshot to be saved to stable storage.
    Snapshot pb.Snapshot

    // CommittedEntries specifies entries to be committed to a
    // store/state-machine. These have previously been committed to stable
    // store.
    CommittedEntries []pb.Entry

    // Messages specifies outbound messages to be sent AFTER Entries are
    // committed to stable storage.
    // If it contains a MsgSnap message, the application MUST report back to raft
    // when the snapshot has been received or has failed by calling ReportSnapshot.
    Messages []pb.Message

    // MustSync indicates whether the HardState and Entries must be synchronously
    // written to disk or if an asynchronous write is permissible.
    MustSync bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ready 是 raft 状态机和外面交互传递的核心数据结构，其包含了一批更新操作&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SoftState：当前 node 的状态信息，主要记录了 Leader 是谁 ？当前 node 处于什么状态，是 Leader，还是 Follower，用于更新 etcd server 的状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// SoftState provides state that is useful for logging and debugging.
// The state is volatile and does not need to be persisted to the WAL.
type SoftState struct {
    Lead      uint64 // must use atomic operations to access; keep 64-bit aligned.
    RaftState StateType
}
​
type StateType uint64
​
var stmap = [...]string{
    &amp;quot;StateFollower&amp;quot;,
    &amp;quot;StateCandidate&amp;quot;,
    &amp;quot;StateLeader&amp;quot;,
    &amp;quot;StatePreCandidate&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pb.HardState: 包含当前节点见过的最大的 term，以及在这个 term 给谁投过票，以及当前节点知道的commit index，这部分数据会持久化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type HardState struct {
    Term             uint64 protobuf:&amp;quot;varint,1,opt,name=term&amp;quot; json:&amp;quot;term&amp;quot;
    Vote             uint64 protobuf:&amp;quot;varint,2,opt,name=vote&amp;quot; json:&amp;quot;vote&amp;quot;
    Commit           uint64 protobuf:&amp;quot;varint,3,opt,name=commit&amp;quot; json:&amp;quot;commit&amp;quot;
    XXX_unrecognized []byte json:&amp;quot;-&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ReadStates：用于返回已经确认 Leader 身份的 read 请求的 commit index&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Messages: 需要广播给所有peers的消息&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CommittedEntries：已经commit了，还没有apply到状态机的日志&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Snapshot：需要持久化的快照&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;应用程序得到这个Ready之后，需要：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将HardState, Entries, Snapshot持久化到storage。&lt;/li&gt;
&lt;li&gt;将Messages广播给其他节点。&lt;/li&gt;
&lt;li&gt;将CommittedEntries（已经commit还没有apply）应用到状态机。&lt;/li&gt;
&lt;li&gt;如果发现CommittedEntries中有成员变更类型的entry，调用node.ApplyConfChange()方法让node知道。&lt;/li&gt;
&lt;li&gt;最后再调用node.Advance()告诉raft，这批状态更新处理完了，状态已经演进了，可以给我下一批Ready让我处理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;基本实现&#34;&gt;基本实现&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;投票流程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、首先，在node的大循环里，有一个会定时输出的tick channel，它来触发raft.tick()函数，根据上面的介绍可知，如果当前节点是follower，那它的tick函数会指向tickElection。tickElection的处理逻辑是给自己发送一个MsgHup的内部消息，Step函数看到这个消息后会调用campaign函数，进入竞选状态。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// tickElection is run by followers and candidates after r.electionTimeout.
func (r *raft) tickElection() {
    r.electionElapsed++

    if r.promotable() &amp;amp;&amp;amp; r.pastElectionTimeout() {
        r.electionElapsed = 0
        r.Step(pb.Message{From: r.id, Type: pb.MsgHup})
    }
}

func (r *raft) Step(m pb.Message) error {
    //...
    switch m.Type {
    case pb.MsgHup:
        r.campaign(campaignElection)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、campaign则会调用becomeCandidate把自己切换到candidate模式，并递增Term值。然后再将自己的Term及日志信息发送给其他的节点，请求投票。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) campaign(t CampaignType) {
    //...
    r.becomeCandidate()
    // Get peer id from progress
    for id := range r.prs {
        //...
        r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、另一方面，其他节点在接受到这个请求后，会首先比较接收到的Term是不是比自己的大，以及接受到的日志信息是不是比自己的要新，从而决定是否投票。在对应节点的step函数中有对投票请求这种消息的处理逻辑。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) Step(m pb.Message) error {
    //...
    switch m.Type {
    case pb.MsgVote, pb.MsgPreVote:
        // We can vote if this is a repeat of a vote we&#39;ve already cast...
        canVote := r.Vote == m.From ||
            // ...we haven&#39;t voted and we don&#39;t think there&#39;s a leader yet in this term...
            (r.Vote == None &amp;amp;&amp;amp; r.lead == None) ||
            // ...or this is a PreVote for a future term...
            (m.Type == pb.MsgPreVote &amp;amp;&amp;amp; m.Term &amp;gt; r.Term)
        // ...and we believe the candidate is up to date.
        if canVote &amp;amp;&amp;amp; r.raftLog.isUpToDate(m.Index, m.LogTerm) {
            r.send(pb.Message{To: m.From, Term: m.Term, Type: voteRespMsgType(m.Type)})
        } else {
            r.send(pb.Message{To: m.From, Term: r.Term, Type: voteRespMsgType(m.Type), Reject: true})
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、最后当candidate节点收到投票回复后，就会计算收到的选票数目是否大于所有节点数的一半，如果大于则自己成为leader，并昭告天下，否则将自己置为follower，同样是在即自己的step的逻辑中处理投票恢复的消息逻辑。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) Step(m pb.Message) error {
    //...
    switch m.Type {
    case myVoteRespType:
        gr := r.poll(m.From, m.Type, !m.Reject)
        switch r.quorum() {
        case gr:
            if r.state == StatePreCandidate {
                r.campaign(campaignElection)
            } else {
                r.becomeLeader()
                r.bcastAppend()
            }
        case len(r.votes) - gr:
            r.becomeFollower(r.Term, None)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;put kv 请求&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft18&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;client 通过 grpc 发送一个 Put kv request，etcd server 的 rpc server 收到这个请求，通过 node 模块的 Propose 接口提交，node 模块的Propose方法将这个 Put kv request 转换成 raft StateMachine 认识的 MsgProp Msg 并通过 propc Channel 传递给 node 模块的 coroutine；&lt;/li&gt;
&lt;li&gt;node 模块 coroutine 监听在 propc Channel 中，收到 MsgProp Msg 之后，通过 raft.Step(Msg) 接口将其提交给 raft StateMachine 处理；&lt;/li&gt;
&lt;li&gt;raft StateMachine 处理完这个 MsgProp Msg 会产生 1 个 Op log entry 和 2 个发送给另外两个副本的 Append entries 的 MsgApp messages，node 模块会将这两个输出打包成 Ready，然后通过 readyc Channel 传递给 raftNode 模块的 coroutine；&lt;/li&gt;
&lt;li&gt;raftNode 模块的 coroutine 通过 readyc 读取到 Ready，首先通过网络层将 2 个 append entries 的 messages 发送给两个副本(PS:这里是异步发送的)；&lt;/li&gt;
&lt;li&gt;raftNode 模块的 coroutine 自己将 Op log entry 通过持久化层的 WAL 接口同步的写入 WAL 文件中&lt;/li&gt;
&lt;li&gt;raftNode 模块的 coroutine 通过 advancec Channel 通知当前 Ready 已经处理完，请给我准备下一个 带出的 raft StateMachine 输出Ready；&lt;/li&gt;
&lt;li&gt;其他副本的返回 Append entries 的 response： MsgAppResp message，会通过 node 模块的接口经过 recevc Channel 提交给 node 模块的 coroutine；&lt;/li&gt;
&lt;li&gt;node 模块 coroutine 从 recev Channel 读取到 MsgAppResp，然后提交给 raft StateMachine 处理。node 模块 coroutine 会驱动 raft StateMachine 得到关于这个 committedEntires，也就是一旦大多数副本返回了就可以 commit 了，node 模块 new 一个新的 Ready其包含了 committedEntries，通过 readyc Channel 传递给 raftNode 模块 coroutine 处理；&lt;/li&gt;
&lt;li&gt;raftNode 模块 coroutine 从 readyc Channel 中读取 Ready结构，然后取出已经 commit 的 committedEntries 通过 applyc 传递给另外一个 etcd server coroutine 处理，其会将每个 apply 任务提交给 FIFOScheduler 调度异步处理，这个调度器可以保证 apply 任务按照顺序被执行，因为 apply 的执行是不能乱的；&lt;/li&gt;
&lt;li&gt;raftNode 模块的 coroutine 通过 advancec Channel 通知当前 Ready 已经处理完，请给我准备下一个待处理的 raft StateMachine 输出Ready；&lt;/li&gt;
&lt;li&gt;FIFOScheduler 调度执行 apply 已经提交的 committedEntries&lt;/li&gt;
&lt;li&gt;AppliedIndex 推进，通知 ReadLoop coroutine，满足 applied index&amp;gt;= commit index 的 read request 可以返回；&lt;/li&gt;
&lt;li&gt;调用网络层接口返回 client 成功。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面主要是leader的一个流程，其实还是有fallow，整体一个流程总结&lt;/p&gt;

&lt;p&gt;1、一个写请求一般会通过调用node.Propose开始，Propose方法将这个写请求封装到一个MsgProp消息里面，发送给自己处理。&lt;/p&gt;

&lt;p&gt;2、消息处理函数Step无法直接处理这个消息，它会调用那个小写的step函数，来根据当前的状态进行处理。&lt;/p&gt;

&lt;p&gt;如果当前是follower，那它会把这个消息转发给leader。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func stepFollower(r *raft, m pb.Message) error {
    switch m.Type {
    case pb.MsgProp:
        //...
        m.To = r.lead
        r.send(m)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Leader收到这个消息后（不管是follower转发过来的还是自己内部产生的）会有两步操作
- 将这个消息添加到自己的log里
- 向其他follower广播这个消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    func stepLeader(r *raft, m pb.Message) error {
        switch m.Type {
        case pb.MsgProp:
            //...
            if !r.appendEntry(m.Entries...) {
                return ErrProposalDropped
            }
            r.bcastAppend()
            return nil
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、在follower接受完这个log后，会返回一个MsgAppResp消息。&lt;/p&gt;

&lt;p&gt;5、当leader确认已经有足够多的follower接受了这个log后，它首先会commit这个log，然后再广播一次，告诉别人它的commit状态。这里的实现就有点像两阶段提交了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func stepLeader(r *raft, m pb.Message) error {
    switch m.Type {
    case pb.MsgAppResp:
        //...
        if r.maybeCommit() {
            r.bcastAppend()
        }
    }
}

// maybeCommit attempts to advance the commit index. Returns true if
// the commit index changed (in which case the caller should call
// r.bcastAppend).
func (r *raft) maybeCommit() bool {
    //...
    mis := r.matchBuf[:len(r.prs)]
    idx := 0
    for _, p := range r.prs {
        mis[idx] = p.Match
        idx++
    }
    sort.Sort(mis)
    mci := mis[len(mis)-r.quorum()]
    return r.raftLog.maybeCommit(mci, r.Term)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;状态转换&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;etcd-raft StateMachine 封装在 raft struct 中，其状态转换如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft12&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图中就是对应的状态转化接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) becomeFollower(term uint64, lead uint64)
func (r *raft) becomePreCandidate()
func (r *raft) becomeCandidate()
func (r *raft) becomeLeader()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;etcd将请求数据都转化为msg，然后通过step接口进行处理，我们来看一下step接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) Step(m pb.Message) error {
    r.step(r, m)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在调用的step接口中，会针对不同的state调用不同的函数，如下，其中 stepCandidate 会处理 PreCandidate 和 Candidate 两种状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func stepFollower(r *raft, m pb.Message) error
func stepCandidate(r *raft, m pb.Message) error
func stepLeader(r *raft, m pb.Message) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们简单看一下stepCandidate，对各种 Msg 进行处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func stepCandidate(r *raft, m pb.Message) error {
    ......
    switch m.Type {
    case pb.MsgProp:
        r.logger.Infof(&amp;quot;%x no leader at term %d; dropping proposal&amp;quot;, r.id, r.Term)
        return ErrProposalDropped
    case pb.MsgApp:
        r.becomeFollower(m.Term, m.From) // always m.Term == r.Term
        r.handleAppendEntries(m)
    case pb.MsgHeartbeat:
        r.becomeFollower(m.Term, m.From) // always m.Term == r.Term
        r.handleHeartbeat(m)
    case pb.MsgSnap:
        r.becomeFollower(m.Term, m.From) // always m.Term == r.Term
        r.handleSnapshot(m)
    case myVoteRespType:
        ......
    case pb.MsgTimeoutNow:
        r.logger.Debugf(&amp;quot;%x [term %d state %v] ignored MsgTimeoutNow from %x&amp;quot;, r.id, r.Term, r.state, m.From)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;核心模块&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、node 模块，对应一个 coroutine&lt;/p&gt;

&lt;p&gt;其实就是我们说的node的功能，负责raft和应用层的交互，也就是那个for-select-channel的coroutine。&lt;/p&gt;

&lt;p&gt;2、raftNode 模块：也会有一个 coroutine&lt;/p&gt;

&lt;p&gt;主要完成的工作是把 raft StateMachine 处理的阶段性输出 Ready 拿来处理，该持久化的通过持久化接口写入盘中，该发送给 Peer 的通过网络层发送给 Peers 等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raftNode) start(rh *raftReadyHandler) {
    go func() {
        defer r.onStop()
        islead := false
​
        for {
            select {
            // 监听 Ticker 事件，并通知 raft StateMachine
            case &amp;lt;-r.ticker.C:
                r.tick()
            // 监听待处理的 Ready，并处理
            case rd := &amp;lt;-r.Ready():
                ......
                  // 这部分处理 Ready 的逻辑下面单独文字描述
                ......
                // 通知 raft StateMachine 运转，返回新的待处理的 Ready
                r.Advance()
            case &amp;lt;-r.stopped:
                return
            }
        }
    }()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;raftNode 模块的 cortoutine 核心就是处理 raft StateMachine 的 Ready，下面将用文字单独描述，这里仅考虑Leader 分支，Follower 分支省略：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;取出 Ready.SoftState 更新 EtcdServer 的当前节点身份信息（leader、follower&amp;hellip;.）等&lt;/li&gt;
&lt;li&gt;取出 Ready.ReadStates（保存了 commit index），通过 raftNode.readStateC 通道传递给 EtcdServer 处理 read 的 coroutine&lt;/li&gt;
&lt;li&gt;取出 Ready.CommittedEntires 封装成 apply 结构，通过 raftnode.applyc 通道传递给 EtcdServer 异步 Apply 的 coroutine，并更新 EtcdServer 的 commit index&lt;/li&gt;
&lt;li&gt;取出 Ready.Messages，通过网络模块 raftNode.transport 发送给 Peers&lt;/li&gt;
&lt;li&gt;取出 Ready.HardState 和 Entries，通过 raftNode.storage 持久化到 WAL 中&lt;/li&gt;
&lt;li&gt;（Follower分支）取出 Ready.snapshot（Leader 发送过来的），（1）通过 raftNode.storage 持久化 Snapshot 到盘中的 Snapshot，（2）通知异步 Apply coroutine apply snapshot 到 KV 存储中，（3）Apply snapshot 到 raftNode.raftStorage 中（all raftLog in memory）&lt;/li&gt;
&lt;li&gt;取出 Ready.entries，append 到 raftLog 中&lt;/li&gt;
&lt;li&gt;调用 raftNode.Advance 通知 raft StateMachine coroutine，当前 Ready 已经处理完，可以投递下一个准备好的 Ready 给 raftNode cortouine 处理了（raft StateMachine 中会删除 raftLog 中 unstable 中 log entries 拷贝到 raftLog 的 Memory storage 中）&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Map</title>
          <link>https://kingjcy.github.io/post/golang/go-map/</link>
          <pubDate>Thu, 28 Feb 2019 17:42:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-map/</guid>
          <description>&lt;p&gt;map是我们经常使用的一种数据结构，也是很重要的一种数据结构，我们来详细的了解一下map。&lt;/p&gt;

&lt;h1 id=&#34;map&#34;&gt;map&lt;/h1&gt;

&lt;p&gt;map就是k/v的映射，map持有对底层数据结构的引用。如果将map传递给函数，其对map的内容做了改变，则这些改变对于调用者是可见的。&lt;/p&gt;

&lt;h2 id=&#34;map的实现&#34;&gt;map的实现&lt;/h2&gt;

&lt;p&gt;所有的Map底层一般都是使用数组+链表的hashmap来实现，会借用哈希算法辅助。对于给定的 key，一般先进行 hash 操作，然后相对哈希表的长度取模，将 key 映射到指定的链表中。&lt;/p&gt;

&lt;p&gt;Golang的map正常是使用哈希表作为底层实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;hashtable是一种根据key，key在通过hash函数来对应的数组的数据。&lt;/li&gt;
&lt;li&gt;hashmap是hashtable使用拉链法实现的一种方式，数组+链表的实现方式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;正常情况下，golang使用数组+桶（buckets）来实现，默认每个桶的数量是8个，在超出8个的情况下，新增一个桶，使用链表连接起来，这是时候就变成了hash数组 + 桶 + 溢出的桶链表了。&lt;/p&gt;

&lt;p&gt;其实map不是并发安全的，也是因为hashmap不是并发安全的，实现并发安全的几种方式，可以参考&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;数据结构&#34;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;map数据结构由runtime/map.go/hmap定义:&lt;/p&gt;

&lt;p&gt;hmap&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/map.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type hmap struct {
    count     int
    flags     uint8
    B         uint8
    noverflow uint16
    hash0     uint32

    buckets    unsafe.Pointer
    oldbuckets unsafe.Pointer
    nevacuate  uintptr

    extra *mapextra
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;count 表示当前哈希表中的元素数量；类似于&lt;code&gt;buckets[count]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;B表示当前哈希表持有的buckets数量，但是因为哈希表中桶的数量都2的倍数，所以该字段会存储对数，也就是 len(buckets) == 2^B；类似于&lt;code&gt;buckets[2^B]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;hash0 是哈希的种子，它能为哈希函数的结果引入随机性，这个值在创建哈希表时确定，并在调用哈希函数时作为参数传入；&lt;/li&gt;
&lt;li&gt;oldbuckets 是哈希在扩容时用于保存之前 buckets 的字段，它的大小是当前 buckets 的一半；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;bmap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type bmap struct {
    tophash [8]uint8 //存储哈希值的高8位
    //data和overflow并不是在结构体中显示定义的，而是直接通过指针运算进行访问的。
    data    byte[1]  //key value数据:key/key/key/.../value/value/value...
    overflow *bmap   //溢出bucket的地址
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;tophash是个长度为8的数组，哈希值相同的键（准确的说是哈希值低位相同的键）存入当前bucket时会将哈希值的高位存储在该数组中，以方便后续匹配。&lt;/li&gt;
&lt;li&gt;data区存放的是key-value数据，存放顺序是key/key/key/&amp;hellip;value/value/value，如此存放是为了节省字节对齐带来的空间浪费。&lt;/li&gt;
&lt;li&gt;overflow 指针指向的是下一个bucket，据此将所有冲突的键连接起来。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我们能根据编译期间的 cmd/compile/internal/gc.bmap 函数对它的结构重建：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type bmap struct {
    topbits  [8]uint8
    keys     [8]keytype
    values   [8]valuetype
    pad      uintptr
    overflow uintptr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hmap.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hmap2.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hmap3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;基本机制&#34;&gt;基本机制&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;负载因子和rehash&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;负载因子用于衡量一个哈希表冲突情况，公式为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;负载因子 = 键数量/bucket数量
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如，对于一个bucket数量为4，包含4个键值对的哈希表来说，这个哈希表的负载因子为1.&lt;/p&gt;

&lt;p&gt;哈希表需要将负载因子控制在合适的大小，超过其阀值需要进行rehash，也即键值对重新组织：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;哈希因子过小，说明空间利用率低&lt;/li&gt;
&lt;li&gt;哈希因子过大，说明冲突严重，存取效率低&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个哈希表的实现对负载因子容忍程度不同，比如Redis实现中负载因子大于1时就会触发rehash，而Go则在在负载因子达到6.5时才会触发rehash，因为Redis的每个bucket只能存1个键值对，而Go的bucket可能存8个键值对，所以Go可以容忍更高的负载因子。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map的删除机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;采用的是惰性删除的策略，打上一个empty的标记，实际上并没有删除，也不会释放内存，还可以在后面进行复用，这样做主要为了解决遍历过程的溢出问题，因为是用数组实现的。实现迭代安全&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map的扩容机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;增量扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当负载因子大于6.5也即平均每个bucket存储的键值对达到6.5个，就新建一个bucket，新的bucket长度是原来的2倍，然后旧bucket数据搬迁到新的bucket。&lt;/p&gt;

&lt;p&gt;考虑到如果map存储了数以亿计的key-value，一次性搬迁将会造成比较大的延时，Go采用逐步搬迁策略，即每次访问map时都会触发一次搬迁，每次搬迁2个键值对。&lt;/p&gt;

&lt;p&gt;扩容流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/dilatation.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当前map存储了7个键值对，只有1个bucket。此地负载因子为7。再次插入数据时将会触发扩容操作，扩容之后再将新插入键写入新的bucket。&lt;/p&gt;

&lt;p&gt;当第8个键值对插入时，将会触发扩容，扩容后示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/dilatation2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;hmap数据结构中oldbuckets成员指身原bucket，而buckets指向了新申请的bucket。新的键值对被插入新的bucket中。 后续对map的访问操作会触发迁移，将oldbuckets中的键值对逐步的搬迁过来。当oldbuckets中的键值对全部搬迁完毕后，删除oldbuckets。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/dilatation3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;等量扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实际上并不是扩大容量，buckets数量不变，重新做一遍类似增量扩容的搬迁动作，把松散的键值对重新排列一次，以使bucket的使用率更高，进而保证更快的存取。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map的缩容机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;溢出的桶数量noverflow&amp;gt;=32768(1&amp;lt;&lt;15)或者&gt;=hash数组大小。&lt;/p&gt;

&lt;p&gt;但是缩容并不会释放已经占用的空间，真的要释放空间，就新建一个map进行迁移&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;什么时候转化为红黑树&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang中没有转化为红黑树，就是正常的扩容使用数组，在java中超过8个桶，就会转化为红黑树，和查询次数也即是时间复杂度有关，因为Map中桶的元素初始化是数组保存的，其查找性能是O(n)，而树结构能将查找性能提升到O(log(n))。当数组长度很小的时候，即使遍历，速度也非常快，但是当链表长度不断变长，肯定会对查询性能有一定的影响，所以才需要转成树。有利于减少查询的次数&lt;/p&gt;

&lt;p&gt;8个桶这个其实是概率统计出来的，8最合适。golang中的桶有8个kv应该是也是这个道理。&lt;/p&gt;

&lt;h1 id=&#34;map并发安全&#34;&gt;map并发安全&lt;/h1&gt;

&lt;p&gt;Go 原生的 map 数据类型是非并发安全的，在go1.9开始发布了sync.map是线程安全的。&lt;/p&gt;

&lt;p&gt;我们先看看基于原生map的基础上加mutex实现并发安全。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map+mutex&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang1.9以前，我们都是使用读写锁（sync.RWMutex）来实现并发安全，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package beego

import (
    &amp;quot;sync&amp;quot;
)

type BeeMap struct {
    lock *sync.RWMutex
    bm   map[interface{}]interface{}

}

func NewBeeMap() *BeeMap {
    return &amp;amp;BeeMap{
        lock: new(sync.RWMutex),
        bm:   make(map[interface{}]interface{}),
    }
}

//Get from maps return the k&#39;s value
func (m *BeeMap) Get(k interface{}) interface{} {
    m.lock.RLock()
    defer m.lock.RUnlock()
    if val, ok := m.bm[k]; ok {
        return val
    }
    return nil
}

// Maps the given key and value. Returns false
// if the key is already in the map and changes nothing.
func (m *BeeMap) Set(k interface{}, v interface{}) bool {
    m.lock.Lock()
    defer m.lock.Unlock()
    if val, ok := m.bm[k]; !ok {
        m.bm[k] = v
    } else if val != v {
        m.bm[k] = v
    } else {
        return false
    }
    return true
}

// Returns true if k is exist in the map.
func (m *BeeMap) Check(k interface{}) bool {
    m.lock.RLock()
    defer m.lock.RUnlock()
    if _, ok := m.bm[k]; !ok {
        return false
    }
    return true
}

func (m *BeeMap) Delete(k interface{}) {
    m.lock.Lock()
    defer m.lock.Unlock()
    delete(m.bm, k)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在项目中经常使用的方式：通过数组、map、sync.RWMutex来实现原生map的并发读写（采用map数组，把key hash到相应的map，每个map单独加锁以降低锁的粒度）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;sync.map&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;那么golang sync.map是如何实现并发安全的呢？&lt;/p&gt;

&lt;p&gt;简单总结就是使用了用空间换时间（多存储一份map作为缓存，减少锁的使用）的思想来实现来一个高效的并发安全。主要下面的函数接口实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Load：读取指定 key 返回 value&lt;/li&gt;
&lt;li&gt;Store： 存储（增或改）key-value&lt;/li&gt;
&lt;li&gt;Delete： 删除指定 key&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4种操作：读key、增加key、更新key、删除key的基本流程，其实在代码中只有上面三个函数实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;读key：先到read中读取，如果有则直接返回结果，如果没有就加锁，再次检查read是否存在，因为刚刚是无锁读，如果没有，则到dirty加锁中读取，如果有返回结果并更新miss数（用于数据迁移），这边在read这边设置了一个amended，如果为true则表明当前read只读map的数据不完整，dirty map中包含部分数据，这边决定了刚刚上一步是否执行到dirty中查找&lt;/li&gt;
&lt;li&gt;更新key（增加key）：其实更新和增加使用的是同一个函数store，首先查找read中是否存在，如果存在直接更新，如果没有就加锁，再次检查read是否存在，因为刚刚是无锁读，如果不存在，同样有上面的判断是否是全量数据，不是就继续到dirty中查找，找到了就更新，找不到就新建一个存储，就是新增key&lt;/li&gt;
&lt;li&gt;删除key：先到read中看看有没有，如果有p标记为nil，如果没有则到dirty中直接删除（同样有数据全不全的判断）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;源码详解解读&#34;&gt;源码详解解读&lt;/h2&gt;

&lt;h3 id=&#34;数据结构分析&#34;&gt;数据结构分析&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Map&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Map struct {
    mu Mutex
    // read是一个readOnly的指针，里面包含了一个map结构，就是我们说的只读map对该map的元素的访问
    // 不需要加锁，只需要通过atomic加载最新的指针即可
    read atomic.Value // readOnly

    // dirty包含部分map的键值对，如果要访问需要进行mutex获取
    // 最终dirty中的元素会被全部提升到read里面的map中
    dirty map[interface{}]*entry

   // misses是一个计数器用于记录从read中没有加载到数据
    // 尝试从dirty中进行获取的次数，从而决定将数据从dirty迁移到read的时机
    misses int
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;readOnly&lt;/p&gt;

&lt;p&gt;只读map,对该map元素的访问不需要加锁，但是该map也不会进行元素的增加，元素会被先添加到dirty中然后后续再转移到read只读map中，通过atomic原子操作不需要进行锁操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type readOnly struct {
    // m包含所有只读数据，不会进行任何的数据增加和删除操作
    // 但是可以修改entry的指针因为这个不会导致map的元素移动
    m       map[interface{}]*entry
    // 标志位，如果为true则表明当前read只读map的数据不完整，dirty map中包含部分数据
    amended bool
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;entry&lt;/p&gt;

&lt;p&gt;entry是sync.Map中值得指针，如果当p指针指向expunged这个指针的时候，则表明该元素被删除，但不会立即从map中删除，如果在未删除之前又重新赋值则会重用该元素&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type entry struct {
    // 指向元素实际值得指针
    p unsafe.Pointer // *interface{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;p 有三种状态：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;p == nil: 键值已经被删除，且 m.dirty == nil&lt;/li&gt;
&lt;li&gt;p == expunged: 键值已经被删除，但 m.dirty!=nil 且 m.dirty 不存在该键值（expunged 实际是空接口指针）&lt;/li&gt;
&lt;li&gt;键值对存在，存在于 m.read.m 中，如果 m.dirty!=nil 则也存在于 m.dirty&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;数据存储&#34;&gt;数据存储&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/date-store.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;元素存在read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;元素如果存储在只读map中，则只需要获取entry元素，然后修改其p的指针指向新的元素就可以了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    read, _ := m.read.Load().(readOnly)
    if e, ok := read.m[key]; ok &amp;amp;&amp;amp; e.tryStore(&amp;amp;value) {
        return
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边其实有两点需要特别说明的&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;map不是并发安全的，这边为什么可以直接修改呢，因为这边使用atomic中的CAS的乐观思想来实现乐观锁，实现并发安全。&lt;/li&gt;
&lt;li&gt;cas就是实现了对比并交互，这个操作是用来实现乐观锁这种思路的，乐观锁并不是真正的锁，它用版本好来标记数据是否被修改，如果被修改则重试。&lt;/li&gt;
&lt;li&gt;怎么同步到dirty中去，entry是一个指向实际值的地址，所以read和dirty是共享地址的。所以就一起改了&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;元素存在dirty中&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果元素存在dirty中其实同read map逻辑一样，只需要修改对应元素的指针即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;} else if e, ok := m.dirty[key]; ok {
    // 如果已经在dirty中就会直接存储
    e.storeLocked(&amp;amp;value)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边有两点需要说明&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;read中是有一个标识amended来判断，如果为true则表明当前read只读map的数据不完整，dirty map中包含部分数据,才会进来查询。&lt;/li&gt;
&lt;li&gt;在加锁之后，还是需要重read中查询一次的，查到就直接修改，因为是一开始的查询是无锁读，存在并发安全问题，可能在这段未加锁的时间内数据发生了改变。&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;元素不存在&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果元素之前不存在当前Map中则需要先将其存储在dirty map中，同时将amended标识为true,即当前read中的数据不全，有一部分数据存储在dirty中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 如果当前不是在修正状态
if !read.amended {
    // 新加入的key会先被添加到dirty map中， 并进行read标记为不完整
    // 如果dirty为空则将read中的所有没有被删除的数据都迁移到dirty中
    m.dirtyLocked()
    m.read.Store(readOnly{m: read.m, amended: true})
}
m.dirty[key] = newEntry(value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) Store(key, value interface{}) {
    read, _ := m.read.Load().(readOnly)
    // 如果 read 里存在，则尝试存到 entry 里
    if e, ok := read.m[key]; ok &amp;amp;&amp;amp; e.tryStore(&amp;amp;value) {
        return
    }

    // 如果上一步没执行成功，则要分情况处理
    m.mu.Lock()
    read, _ = m.read.Load().(readOnly)
    // 和 Load 一样，重新从 read 获取一次
    if e, ok := read.m[key]; ok {
        // 情况 1：read 里存在
        if e.unexpungeLocked() {
            // 如果 p == expunged，则需要先将 entry 赋值给 dirty（因为 expunged 数据不会留在 dirty）
            m.dirty[key] = e
        }
        // 用值更新 entry
        e.storeLocked(&amp;amp;value)
    } else if e, ok := m.dirty[key]; ok {
        // 情况 2：read 里不存在，但 dirty 里存在，则用值更新 entry
        e.storeLocked(&amp;amp;value)
    } else {
        // 情况 3：read 和 dirty 里都不存在
        if !read.amended {
            // 如果 amended == false，则调用 dirtyLocked 将 read 拷贝到 dirty（除了被标记删除的数据）
            m.dirtyLocked()
            // 然后将 amended 改为 true
            m.read.Store(readOnly{m: read.m, amended: true})
        }
        // 将新的键值存入 dirty
        m.dirty[key] = newEntry(value)
    }
    m.mu.Unlock()
}

func (e *entry) tryStore(i *interface{}) bool {
    for {
        p := atomic.LoadPointer(&amp;amp;e.p)
        if p == expunged {
            return false
        }
        //原子操作cas
        if atomic.CompareAndSwapPointer(&amp;amp;e.p, p, unsafe.Pointer(i)) {
            return true
        }
    }
}

func (e *entry) unexpungeLocked() (wasExpunged bool) {
    return atomic.CompareAndSwapPointer(&amp;amp;e.p, expunged, nil)
}

func (e *entry) storeLocked(i *interface{}) {
    atomic.StorePointer(&amp;amp;e.p, unsafe.Pointer(i))
}

func (m *Map) dirtyLocked() {
    if m.dirty != nil {
        return
    }

    read, _ := m.read.Load().(readOnly)
    m.dirty = make(map[interface{}]*entry, len(read.m))
    for k, e := range read.m {
        // 判断 entry 是否被删除，否则就存到 dirty 中
        if !e.tryExpungeLocked() {
            m.dirty[k] = e
        }
    }
}

func (e *entry) tryExpungeLocked() (isExpunged bool) {
    p := atomic.LoadPointer(&amp;amp;e.p)
    for p == nil {
        // 如果有 p == nil（即键值对被 delete），则会在这个时机被置为 expunged
        if atomic.CompareAndSwapPointer(&amp;amp;e.p, nil, expunged) {
            return true
        }
        p = atomic.LoadPointer(&amp;amp;e.p)
    }
    return p == expunged
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在数据存储里其实还有数据迁移的逻辑&lt;/p&gt;

&lt;p&gt;当read多次都没有命中数据，达到阈值，表示这个cache命中率太低，这时直接将整个read用dirty替换掉，然后dirty又重新置为nil，下一次再添加一个新key的时候，会触发一次read到dirty的复制，这样二者又保持了一致。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-migration.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在刚初始化和将所有元素迁移到read中后，dirty默认都是nil元素，而此时如果有新的元素增加，则需要先将read map中的所有未删除数据先迁移到dirty中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) dirtyLocked() {
    if m.dirty != nil {
        return
    }

    read, _ := m.read.Load().(readOnly)
    m.dirty = make(map[interface{}]*entry, len(read.m))
    for k, e := range read.m {
        if !e.tryExpungeLocked() {
            m.dirty[k] = e
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-migration2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当持续的从read访问穿透到dirty中后，也就是上面的miss值大于dirty存储数据的长度，就会触发一次从dirty到read的迁移，这也意味着如果我们的元素读写比差比较小，其实就会导致频繁的迁移操作，性能其实可能并不如rwmutex等实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) missLocked() {
    m.misses++
    if m.misses &amp;lt; len(m.dirty) {
        return
    }
    m.read.Store(readOnly{m: m.dirty})
    m.dirty = nil
    m.misses = 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据查询&#34;&gt;数据查询&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-query.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关于amended标识的使用，和存储是一样的&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据在read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Load数据的时候回先从read中获取，如果此时发现元素，则直接返回即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;read, _ := m.read.Load().(readOnly)
e, ok := read.m[key]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;数据在dirty&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加锁后会尝试从read和dirty中读取，同时进行misses计数器的递增，如果满足迁移条件则会进行数据迁移&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;read, _ = m.read.Load().(readOnly)
e, ok = read.m[key]
if !ok &amp;amp;&amp;amp; read.amended {
    e, ok = m.dirty[key]
    // 这里将采取缓慢迁移的策略
    // 只有当misses计数==len(m.dirty)的时候，才会将dirty里面的数据全部晋升到read中
    m.missLocked()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) Load(key interface{}) (value interface{}, ok bool) {
    // 首先尝试从 read 中读取 readOnly 对象
    read, _ := m.read.Load().(readOnly)
    e, ok := read.m[key]

    // 如果不存在则尝试从 dirty 中获取
    if !ok &amp;amp;&amp;amp; read.amended {
        m.mu.Lock()
        // 由于上面 read 获取没有加锁，为了安全再检查一次
        read, _ = m.read.Load().(readOnly)
        e, ok = read.m[key]

        // 确实不存在则从 dirty 获取
        if !ok &amp;amp;&amp;amp; read.amended {
            e, ok = m.dirty[key]
            // 调用 miss 的逻辑
            m.missLocked()
        }
        m.mu.Unlock()
    }

    if !ok {
        return nil, false
    }
    // 从 entry.p 读取值
    return e.load()
}

func (m *Map) missLocked() {
    m.misses++
    if m.misses &amp;lt; len(m.dirty) {
        return
    }
    // 当 miss 积累过多，会将 dirty 存入 read，然后 将 amended = false，且 m.dirty = nil
    m.read.Store(readOnly{m: m.dirty})
    m.dirty = nil
    m.misses = 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据删除&#34;&gt;数据删除&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-query.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果数据在read中，则就直接修改entry的标志位指向删除的指针即可，如果当前read中数据不全，则需要进行dirty里面的元素删除尝试，&lt;/li&gt;
&lt;li&gt;如果存在就直接从dirty中删除即可&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) Delete(key interface{}) {
    m.LoadAndDelete(key)
}

// LoadAndDelete 作用等同于 Delete，并且会返回值与是否存在
func (m *Map) LoadAndDelete(key interface{}) (value interface{}, loaded bool) {
    // 获取逻辑和 Load 类似，read 不存在则查询 dirty
    read, _ := m.read.Load().(readOnly)
    e, ok := read.m[key]
    if !ok &amp;amp;&amp;amp; read.amended {
        m.mu.Lock()
        read, _ = m.read.Load().(readOnly)
        e, ok = read.m[key]
        if !ok &amp;amp;&amp;amp; read.amended {
            e, ok = m.dirty[key]
            m.missLocked()
        }
        m.mu.Unlock()
    }
    // 查询到 entry 后执行删除
    if ok {
        // 将 entry.p 标记为 nil，数据并没有实际删除
        // 真正删除数据并被被置为 expunged，是在 Store 的 tryExpungeLocked 中
        return e.delete()
    }
    return nil, false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;设计思想&#34;&gt;设计思想&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;自动扩缩容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在Mutex和RWMutex实现的并发安全的map中map随着时间和元素数量的增加、删除，容量会不断的递增，在某些情况下比如在某个时间点频繁的进行大量数据的增加，然后又大量的删除，其map的容量并不会随着元素的删除而缩小，而在sync.Map中，当进行元素从dirty进行提升到read map的时候会进行重建，可以实现自动扩缩容。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;读写分离&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;并发访问map读的主要问题其实是在扩容的时候，可能会导致元素被hash到其他的地址，那如果我的读的map不会进行扩容操作，就可以进行并发安全的访问了，而sync.map里面正是采用了这种方式，对增加元素通过dirty来进行保存&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;无锁读&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过read只读和dirty写map将操作分离，其实就只需要通过原子指令对read map来进行读操作而不需要加锁了，从而提高读的性能&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;写加锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面提到增加元素操作可能会先增加大dirty写map中，那针对多个goroutine同时写，其实就需要进行Mutex加锁了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;延迟提升&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面提到了read只读map和dirty写map, 那就会有个问题，默认增加元素都放在dirty中，那后续访问新的元素如果都通过 mutex加锁，那read只读map就失去意义，sync.Map中采用一直延迟提升的策略，进行批量将当前map中的所有元素都提升到read只读map中从而为后续的读访问提供无锁支持&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;惰性删除&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;惰性删除是并发设计中一中常见的设计，比如删除某个个链表元素，如果要删除则需要修改前后元素的指针，而采用惰性删除，则通常只需要给某个标志位设定为删除，然后在后续修改中再进行操作，sync.Map中也采用这种方式，通过给指针指向某个标识删除的指针，从而实现惰性删除&lt;/p&gt;

&lt;p&gt;我觉得最重要的就是实现了读写分离，加锁分离，从而实现了空间换取时间的快速处理。read相当于dirty的缓存，read是原子操作，不需要加锁，快速，dirty可以延迟提升，就和缓存数据库做缓存是一个道理，包括热点数据的提升。&lt;/p&gt;

&lt;h2 id=&#34;应用场景&#34;&gt;应用场景&lt;/h2&gt;

&lt;p&gt;由以上方法可得知，无论是读操作，还是更新操作，亦或者删除操作，都会先从read进行操作，因为read的读取更新不需要锁，是原子操作，这样既做到了并发安全，又做到了尽量减少锁的争用，虽然采用的是空间换时间的策略，通过两个冗余的map，实现了这一点，但是底层存的都是指针类型，所以对于空间占用，也是做到了最大程度的优化。
但是同时也可以得知，当存在大量写操作时，会导致read中读不到数据，依然会频繁加锁，同时dirty升级为read，整体性能就会很低，所以sync.Map更加适合大量读、少量写的场景。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Sync</title>
          <link>https://kingjcy.github.io/post/golang/go-sync/</link>
          <pubDate>Thu, 28 Feb 2019 17:42:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-sync/</guid>
          <description>&lt;p&gt;sync包提供了基本的同步基元，如锁，WaitGroup、Once 和 Cond等同步原语。除了Once和WaitGroup类型，大部分都是适用于普通程序线程，大型并发同步使用channel通信（csp）更好一些。&lt;/p&gt;

&lt;h1 id=&#34;sync&#34;&gt;sync&lt;/h1&gt;

&lt;p&gt;sync同步功能主要提供了once，mutex，cond，并发安全map，安全并发pool，waitgroup。&lt;/p&gt;

&lt;h2 id=&#34;sync-once&#34;&gt;sync.Once&lt;/h2&gt;

&lt;p&gt;sync.Once是一个简单而强大的原语，可确保一个函数仅执行一次。&lt;/p&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Once struct {
        // 非暴露字段
}

func (o *Once) Do(f func())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用前先定义 Once 类型变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var once Once
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用的时候向 Once 类型变量传入函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;once.Do(func() { init() })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;多次调用 once.Do(f) 只会触发一次 f 的执行，即第一次 f 的执行。&lt;/p&gt;

&lt;p&gt;用法实例&lt;/p&gt;

&lt;p&gt;某些操作只需要执行一次（比如一些初始化动作），这时就可使用 Once，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var once sync.Once
    onceBody := func() {
        fmt.Println(&amp;quot;Only once&amp;quot;)
    }
    done := make(chan bool)

    // 创建 10 个 goroutine，但是 onceBody 只会执行 1 次
    for i := 0; i &amp;lt; 10; i++ {
        go func() {
            once.Do(onceBody)
            done &amp;lt;- true
        }()
    }

    // 等待 10 个 goroutine 结束
    for i := 0; i &amp;lt; 10; i++ {
        &amp;lt;-done
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实 Once 的实现非常简单，就是互斥锁+原子变量&lt;/p&gt;

&lt;h2 id=&#34;sync-waitgroup&#34;&gt;sync.WaitGroup&lt;/h2&gt;

&lt;p&gt;sync.WaitGroup拥有一个内部计数器。当计数器等于0时，则Wait()方法会立即返回。否则它将阻塞执行Wait()方法的goroutine直到计数器等于0时为止。&lt;/p&gt;

&lt;p&gt;WaitGroup 的使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type WaitGroup struct {
    // 包含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WaitGroup用于等待一组线程的结束。父线程调用Add方法来设定应等待的线程的数量。每个被等待的线程在结束时应调用Done方法。同时，主线程里可以调用Wait方法阻塞至所有线程结束。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*WaitGroup) Add
func (wg *WaitGroup) Add(delta int)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add方法向内部计数加上delta，delta可以是负数；如果内部计数器变为0，Wait方法阻塞等待的所有线程都会释放，如果计数器小于0，方法panic。注意Add加上正数的调用应在Wait之前，否则Wait可能只会等待很少的线程。一般来说本方法应在创建新的线程或者其他应等待的事件之前调用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*WaitGroup) Done
func (wg *WaitGroup) Done()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done方法减少WaitGroup计数器的值，应在线程的最后执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*WaitGroup) Wait
func (wg *WaitGroup) Wait()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait方法阻塞直到WaitGroup计数器减为0。&lt;/p&gt;

&lt;h2 id=&#34;sync-pool&#34;&gt;sync.Pool&lt;/h2&gt;

&lt;p&gt;sync.Pool是一个并发池，负责安全地保存一组对象。&lt;/p&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Pool struct {
        // 当 Get() 找不到一个对象时，会使用 New() 生成一个对象
        New func() interface{}

        // 剩下的是非暴露字段
}

// 任意从 pool 中挑选一个对象返回给客户端，如果找不到就使用 p.New 生成
func (p *pool) Get() interface{}

// 将对象 x 放回到 pool 中
func (p *pool) Put(x interface{})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 Pool 可以用以管理一些临时对象供多个 package 的客户端使用，客户端对 Pool 的逻辑是无感知的：需要的时候 Get，不需要的时候 Put，而且 Pool 可根据当前负载自动调整对象池的大小。&lt;/p&gt;

&lt;p&gt;一个典型的应用是日志，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var bufPool = sync.Pool{
    New: func() interface{} {
        return new(bytes.Buffer)
    },
}

func Log(w io.Writer, key, val string) {
    // 从对象池中获取 buffer
    b := bufPool.Get().(*bytes.Buffer)
    b.Reset()
    b.WriteString(time.Now().Format(time.RFC3339))
    b.WriteByte(&#39; &#39;)
    b.WriteString(key)
    b.WriteByte(&#39;=&#39;)
    b.WriteString(val)
    w.Write(b.Bytes())
    // 使用完毕，归还 buffer
    bufPool.Put(b)
}

func main() {
    Log(os.Stdout, &amp;quot;path&amp;quot;, &amp;quot;/search?q=flowers&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么什么时候使用sync.Pool？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一个是当我们必须重用共享的和长期存在的对象（例如，数据库连接）时。&lt;/li&gt;
&lt;li&gt;第二个是用于优化内存分配。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sync-mutex&#34;&gt;sync.Mutex&lt;/h2&gt;

&lt;p&gt;sync.Mutex可能是sync包中使用最广泛的原语。它允许在共享资源上互斥访问（不能同时访问），&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/#如何做到并发安全&#34;&gt;锁&lt;/a&gt;在并发安全中有着很重要的作用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mutex := &amp;amp;sync.Mutex{}

mutex.Lock()
// Update共享变量 (比如切片，结构体指针等)
mutex.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sync.RWMutex是一个读写互斥锁，它提供了我们上面的刚刚看到的sync.Mutex的Lock和UnLock方法（因为这两个结构都实现了sync.Locker接口）。但是，它还允许使用RLock和RUnlock方法进行并发读取：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mutex := &amp;amp;sync.RWMutex{}

mutex.Lock()
// Update 共享变量
mutex.Unlock()

mutex.RLock()
// Read 共享变量
mutex.RUnlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只有在频繁读取和不频繁写入的场景里，才应该使用sync.RWMutex。&lt;/p&gt;

&lt;h2 id=&#34;sync-cond&#34;&gt;sync.Cond&lt;/h2&gt;

&lt;p&gt;sync.Cond可能是sync包提供的同步原语中最不常用的一个，它用于发出信号（一对一）或广播信号（一对多）到goroutine。&lt;/p&gt;

&lt;p&gt;条件变量做的事情很简单：让多个 goroutine 等待在某个条件上，如果条件不满足，进入等待状态；如果条件满足，继续运行。&lt;/p&gt;

&lt;p&gt;Cond 内部维护着一个 notifyList，当条件不满足的时候，则将对应的 goroutine 添加到列表上然后进入等待状态。当条件满足时，一般会有其他执行者显式使用 Signal() 或者 Broadcast() 去唤醒 notifyList 上 goroutine。&lt;/p&gt;

&lt;p&gt;当进行条件的判断时，必须使用互斥锁来保证条件的安全，即在判断的时候条件没有被其他人修改。所以 Cond 一般会与一个符合 Lock 接口的 Mutex 一起使用。&lt;/p&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Cond struct {
    // 读写条件状态需要加锁
    L Locker
    // 剩下的是非暴露字段
}

func NewCond(l Locker) *Cond

// 广播所以等待的 goroutine，条件已经满足
func (c *Cond) Broadcast()

// 单播其中一个等待的 goroutine，条件已经满足
func (c *Cond) Signal()

// 如果条件不满足，调用 Wait() 进入等待状态
func (c *Cond) Wait()
此处要特别小心 Wait() 的使用。正如前文所说，条件的判断需要使用互斥锁来确保条件读取前后是一致的，即：

    c.L.Lock() // 进行条件判断，加锁
    if !condition() { // 如果不满足条件，进入 if 中
        c.Wait() // Wait() 内部会自动解锁
    }

    ... 这里可能会对 condition 作出改变 ...
    c.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述代码其实还有一个很严重的问题，为了说明这个问题，让我们来看看 Wait() 的实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cond) Wait() {
    c.checker.check()
    t := runtime_notifyListAdd(&amp;amp;c.notify) // 加入 notifyList
    c.L.Unlock() // 解锁
    runtime_notifyListWait(&amp;amp;c.notify, t) // 进入等待模式
    c.L.Lock() // 运行到此处说明条件已经满足，开始获取互斥锁，如果锁已经被别人用了，开始等待
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的例子可以看出，当 Wait() 返回时（即已经获取到了互斥锁），有可能条件已经被其他先获取互斥锁的 goroutine 改变了，所以此时必须再次判断一下条件，即：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c.L.Lock() // 进行条件判断，加锁
if !condition() { // 如果不满足条件，进入 if 中
    c.Wait() // Wait() 内部会自动解锁
}

if !condition() { // 如果不满足条件，进入 if 中
    c.Wait() // Wait() 内部会自动解锁
}
... 这里可能会对 condition 作出改变 ...
c.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果代码这么写，就太费劲了，上面代码可以简化为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c.L.Lock() // 进行条件判断，加锁
for !condition() { // 如果不满足条件，进入 if 中
    c.Wait() // Wait() 内部会自动解锁
}

... 这里可能会对 condition 作出改变 ...
c.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即将 if 替换为 for，从而当从 Wait() 返回时，再次判断条件是否满足。&lt;/p&gt;

&lt;p&gt;用法实例&lt;/p&gt;

&lt;p&gt;用一个简单的例子来介绍一下 Cond 如何使用，即：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    wakeup    = false
    workerNum = 3
)

func worker(workerID int, c *sync.Cond) {
    fmt.Printf(&amp;quot;Worker [%d] is RUNNING\n&amp;quot;, workerID)
    c.L.Lock()
    for !wakeup {
        fmt.Printf(&amp;quot;Worker [%d] check conditon\n&amp;quot;, workerID)
        c.Wait()
    }
    fmt.Printf(&amp;quot;Worker [%d] wakeup, DO something\n&amp;quot;, workerID)
    // 将唤醒标志改为 false
    // 此时其他已经醒来并抢夺互斥锁的 goroutine 重新判断条件后
    // 将再次进入 wait 状态
    wakeup = false 
    c.L.Unlock()
}

func main() {
    cond := sync.NewCond(&amp;amp;sync.Mutex{})
    for i := 0; i &amp;lt; workerNum; i++ {
        go worker(i, cond)
    }

    time.Sleep(2 * time.Second)
    wakeup = true
    cond.Broadcast() // 向所有 goroutine 进行广播，条件已经满足，即 wakeup = true

    time.Sleep(2 * time.Second)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行后的输出为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Worker [0] is RUNNING
Worker [1] is RUNNING
Worker [0] check conditon
Worker [1] check conditon
Worker [2] is RUNNING
Worker [2] check conditon
Worker [0] wakeup, DO something
Worker [1] check conditon
Worker [2] check conditon
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当 worker0 醒来后，又重新把条件变量进行了修改，从而导致 worker1 和 worker2 获取到互斥锁后重新检查到条件不满足，再次进入 wait 状态。&lt;/p&gt;

&lt;h2 id=&#34;map&#34;&gt;map&lt;/h2&gt;

&lt;p&gt;这是一个重点的数据结构，实现十分经典，具体看&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-map/&#34;&gt;map&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;sync-atomic&#34;&gt;sync/atomic&lt;/h1&gt;

&lt;p&gt;atomic包提供了底层的原子级内存操作，对于同步算法的实现很有用。支持类型共有六种：int32, int64, uint32, uint64, uintptr, unsafe.Pinter，实现的操作共五种：增减，比较并交换，载入，存储，交换。&lt;/p&gt;

&lt;h2 id=&#34;增或减&#34;&gt;增或减&lt;/h2&gt;

&lt;p&gt;顾名思义，原子增或减即可实现对被操作值的增大或减少。因此该操作只能操作数值类型。
　　
被用于进行增或减的原子操作都是以“Add”为前缀，并后面跟针对具体类型的名称。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func AddUint32(addr *uint32, delta uint32) (new uint32)

func AddInt64(addr *int64, delta int64) (new int64)
AddInt64原子性的将val的值添加到*addr并返回新值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;增&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;栗子：（在原来的基础上加n）
atomic.AddUint32(&amp;amp;addr,n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;减&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;栗子：(在原来的基础上加n（n为负数))
atomic.AddUint32(*addr,uint32(int32(n)))
//或
atomic.AddUint32(&amp;amp;addr,^uint32(-n-1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;比较并交换&#34;&gt;比较并交换&lt;/h2&gt;

&lt;p&gt;比较并交换&amp;mdash;-Compare And Swap 简称CAS&lt;/p&gt;

&lt;p&gt;他是假设被操作的值未曾被改变（即与旧值相等），并一旦确定这个假设的真实性就立即进行值替换，这是典型的乐观实现思想。&lt;/p&gt;

&lt;p&gt;如果想安全的并发一些类型的值，我们总是应该优先使用CAS&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子：（如果addr和old相同,就用new代替addr）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ok:=atomic.CompareAndSwapInt32(&amp;amp;addr,old,new)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;载入&#34;&gt;载入&lt;/h2&gt;

&lt;p&gt;如果一个写操作未完成，有一个读操作就已经发生了，这样读操作使很糟糕的。&lt;/p&gt;

&lt;p&gt;为了原子的读取某个值sync/atomic代码包同样为我们提供了一系列的函数。这些函数都以&amp;rdquo;Load&amp;rdquo;为前缀，意为载入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func LoadInt32(addr *int32) (val int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fun addValue(delta int32){
    for{
        v:=atomic.LoadInt32(&amp;amp;addr)
        if atomic.CompareAndSwapInt32(&amp;amp;v,addr,(delta+v)){
            break;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;存储&#34;&gt;存储&lt;/h2&gt;

&lt;p&gt;　　
与读操作对应的是写入操作，sync/atomic也提供了与原子的值载入函数相对应的原子的值存储函数。这些函数的名称均以“Store”为前缀
　　&lt;/p&gt;

&lt;p&gt;在原子的存储某个值的过程中，任何cpu都不会进行针对进行同一个值的读或写操作。如果我们把所有针对此值的写操作都改为原子操作，那么就不会出现针对此值的读操作读操作因被并发的进行而读到修改了一半的情况。
　　
原子操作总会成功，因为他不必关心被操作值的旧值是什么。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func StoreInt32(addr *int32, val int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;atomic.StoreInt32(被操作值的指针,新值)
atomic.StoreInt32(&amp;amp;value,newaddr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;交换&#34;&gt;交换&lt;/h2&gt;

&lt;p&gt;　　
原子交换操作，这类函数的名称都以“Swap”为前缀，与CAS不同，交换操作直接赋予新值，不管旧值。
　　
会返回旧值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func SwapInt32(addr *int32, new int32) (old int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;atomic.SwapInt32(被操作值的指针,新值)（返回旧值）
oldval：=atomic.StoreInt32(&amp;amp;value,newaddr)
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- mutex</title>
          <link>https://kingjcy.github.io/post/golang/go-mutex/</link>
          <pubDate>Thu, 28 Feb 2019 17:42:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-mutex/</guid>
          <description>&lt;p&gt;锁是一种常见的并发控制技术，我们一般会将锁分成乐观锁和悲观锁，即乐观并发控制和悲观并发控制。&lt;/p&gt;

&lt;h1 id=&#34;悲观锁&#34;&gt;悲观锁&lt;/h1&gt;

&lt;p&gt;悲观锁就是我们常用的锁机制，不管它会不会发生，只要存在并发安全问题，就在操作这个资源的时候给他先加上锁。常见的锁有互斥锁，读写锁等。&lt;/p&gt;

&lt;p&gt;golang中除了atomic其他都是悲观锁。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;互斥锁Mutex&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var l sync.Mutex
func foo() {
     l.Lock()
     defer l.Unlock()
     //...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中Mutex为互斥锁，Lock()加锁，Unlock()解锁，如果在使用Unlock()前未加锁，就会引起一个运行错误，使用Lock()加锁后，便不能再次对其进行加锁，直到利用Unlock()解锁对其解锁后，才能再次加锁．适用于读写不确定场景，即读写次数没有明显的区别，并且只允许只有一个读或者写的场景，所以该锁也叫做全局锁。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;读写锁RWMutex&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (rw *RWMutex) Lock()　　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写锁，如果在添加写锁之前已经有其他的读锁和写锁，则lock就会阻塞直到该锁可用，为确保该锁最终可用，已阻塞的 Lock 调用会从获得的锁中排除新的读取器，即写锁权限高于读锁，有写锁时优先进行写锁定&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rw *RWMutex) Unlock()　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写锁解锁，如果没有进行写锁定，则就会引起一个运行时错误&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rw *RWMutex) RLock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读锁，当有写锁时，无法加载读锁，当只有读锁或者没有锁时，可以加载读锁，读锁可以加载多个，所以适用于＂读多写少＂的场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rw *RWMutex)RUnlock()　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读锁解锁，RUnlock 撤销单次RLock 调用，它对于其它同时存在的读取器则没有效果。若 rw 并没有为读取而锁定，调用 RUnlock 就会引发一个运行时错误(注：这种说法在go1.3版本中是不对的，例如下面这个例子)。&lt;/p&gt;

&lt;p&gt;读写锁是针对于读写操作的互斥锁。&lt;/p&gt;

&lt;p&gt;基本遵循两大原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;加读锁后就不能写，允许存在多个读锁，但只能有一把写锁；读锁的时候不能写，可以随便读。多个goroutin同时读。&lt;/li&gt;
&lt;li&gt;加写锁的时候，当写锁未被释放时或此时有正被等待的写锁（只有当全部读锁结束，写锁才可用），读锁不可用；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面再用一个实例来简单介绍一下 RWMutex 的几条规则：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var rw sync.RWMutex

func reader(readerID int) {
    fmt.Printf(&amp;quot;[reader-%d] try to get read lock\n&amp;quot;, readerID)
    rw.RLock()
    fmt.Printf(&amp;quot;[reader-%d] get read lock and sleep\n&amp;quot;, readerID)
    time.Sleep(1 * time.Second)
    fmt.Printf(&amp;quot;[reader-%d] release read lock\n&amp;quot;, readerID)
    rw.RUnlock()
}

func writer(writerID int) {
    fmt.Printf(&amp;quot;[writer-%d] try to get write lock\n&amp;quot;, writerID)
    rw.Lock()
    fmt.Printf(&amp;quot;[writer-%d] get write lock and sleep\n&amp;quot;, writerID)
    time.Sleep(3 * time.Second)
    fmt.Printf(&amp;quot;[writer-%d] release write lock\n&amp;quot;, writerID)
    rw.Unlock()
}

func main() {
    // 启动多个 goroutine 获取 read lock 后 sleep 一段时间
    // 由于此时没有写者，所以两个 reader 都可以同时获取到读锁
    go reader(1)
    go reader(2)

    time.Sleep(500 * time.Millisecond)

    // 写者获取写锁，由于读锁未被释放，所以一开始写者获取不到写锁
    go writer(1)

    time.Sleep(1 * time.Second)

    // 由于写锁还未释放，新的读者获取不到读锁
    go reader(3)
    go reader(4)

    // 主 goroutine 等待足够长时间让所有 goroutine 执行完
    time.Sleep(10 * time.Second)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行后输出为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[reader-2] try to get read lock
[reader-1] try to get read lock
[reader-2] get read lock and sleep
[reader-1] get read lock and sleep
[writer-1] try to get write lock      --&amp;gt; 尝试获取写锁失败，因为读锁未释放
[reader-2] release read lock          --&amp;gt; 读锁释放
[reader-1] release read lock
[writer-1] get write lock and sleep   --&amp;gt; 读锁释放后，获取写锁成功
[reader-4] try to get read lock       --&amp;gt; 获取读锁失败因为写锁未释放
[reader-3] try to get read lock
[writer-1] release write lock         --&amp;gt; 写锁释放
[reader-3] get read lock and sleep    --&amp;gt; 写锁释放后，获取读锁成功
[reader-4] get read lock and sleep
[reader-4] release read lock
[reader-3] release read lock
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;乐观锁&#34;&gt;乐观锁&lt;/h1&gt;

&lt;p&gt;乐观锁并不是一把真正的锁，不像上面的锁一样有api，而是一种并发控制的思想：总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;版本号机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;CAS算法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要读写的内存值 V&lt;/li&gt;
&lt;li&gt;进行比较的值 A&lt;/li&gt;
&lt;li&gt;拟写入的新值 B&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。&lt;/p&gt;

&lt;p&gt;CAS在golang的库sync/atomic中得到了实现。atomic基本使用了乐观锁的原理，但是只是支持int32/int64/uint32/uint64/uintptr这几种数据类型的一些基础操作，操作共五种：增减， 比较并交换， 载入， 存储，交换。&lt;/p&gt;

&lt;p&gt;一般无锁的操作都是使用乐观并发控制思想来实现的。&lt;/p&gt;

&lt;h2 id=&#34;实现方式&#34;&gt;实现方式&lt;/h2&gt;

&lt;p&gt;1、基于数据库的version字段进行实现，在表中新增一个字段version就可以，执行sql的时候加上这个字段的比较&lt;/p&gt;

&lt;p&gt;2、基于缓存数据库实现，比如redis的watch，和memcached的gets和cas&lt;/p&gt;

&lt;h1 id=&#34;锁的原理&#34;&gt;锁的原理&lt;/h1&gt;

&lt;p&gt;锁的实现一般会依赖于信号量，信号量则是一个非负的整数计数器。&lt;/p&gt;

&lt;p&gt;信号量：多线程同步使用的；一个线程完成某个动作后通过信号告诉别的线程，别的线程才可以执行某些动作；信号量可以是多值的，当信号量在0和1之间操作时候就是互斥量&lt;/p&gt;

&lt;p&gt;互斥量：多线程互斥使用的；一个线程占用某个资源，那么别的线程就无法访问，直到该线程离开，其他线程才可以访问该资源；0或1&lt;/p&gt;

&lt;p&gt;具体互斥锁的实现原理可以参考这篇文章：&lt;a href=&#34;https://www.cnblogs.com/sylz/p/6030201.html&#34;&gt;https://www.cnblogs.com/sylz/p/6030201.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;简单来说，就是加锁时，就把信号量减一，如果是零说明加锁成功。释放锁时把信号量加一，如果是一说明释放成功。&lt;/p&gt;

&lt;p&gt;但是在实际应用中大家都使用信号量，因为信号量是多值得，可以通过信号量加等待队列，减少唤醒的次数。&lt;/p&gt;

&lt;p&gt;pv原语&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;P（S）：将信号量S的值减1，即S=S-1；
        如果S&amp;gt;=0，则该进程继续执行；否则该进程置为等待状态，排入等待队列。-----申请资源
V（S）：将信号量S的值加1，即S=S+1；
        如果S&amp;gt;0，则该进程继续执行；否则释放队列中第一个等待信号量的进程。------释放资源
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>数据库系列---- Elasticsearch</title>
          <link>https://kingjcy.github.io/post/database/elasticsearch/</link>
          <pubDate>Thu, 21 Feb 2019 19:28:32 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/database/elasticsearch/</guid>
          <description>&lt;p&gt;开源的 Elasticsearch （以下简称 Elastic）是目前全文搜索引擎的首选。它可以快速地储存、搜索和分析海量数据。并且支持分布式，解决Lucene（支持全文索引的数据库系统）单机问题，目前维基百科、Stack Overflow、Github 都采用它。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;存储结构&#34;&gt;存储结构&lt;/h2&gt;

&lt;p&gt;在ES中，存储结构主要有四种，与传统的关系型数据库对比如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;index（Indices）相当于一个database&lt;/li&gt;
&lt;li&gt;type相当于一个table&lt;/li&gt;
&lt;li&gt;document相当于一个row&lt;/li&gt;
&lt;li&gt;properties（Fields）相当于一个column&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以如下对比&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Relational DB -&amp;gt; Databases -&amp;gt; Tables -&amp;gt; Rows -&amp;gt; Columns
Elasticsearch -&amp;gt; Indices -&amp;gt; Types -&amp;gt; Documents -&amp;gt; Fields
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Node 与 Cluster&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Elastic 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。&lt;/p&gt;

&lt;p&gt;单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Index&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Elastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。&lt;/p&gt;

&lt;p&gt;所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。&lt;/p&gt;

&lt;p&gt;下面的命令可以查看当前节点的所有 Index。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X GET &#39;http://localhost:9200/_cat/indices?v&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Document&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。&lt;/p&gt;

&lt;p&gt;Document 使用 JSON 格式表示，下面是一个例子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;user&amp;quot;: &amp;quot;张三&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
  &amp;quot;desc&amp;quot;: &amp;quot;数据库管理&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Type&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Document 可以分组，比如weather这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。&lt;/p&gt;

&lt;p&gt;不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。&lt;/p&gt;

&lt;p&gt;下面的命令可以列出每个 Index 所包含的 Type。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/_mapping?pretty=true&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据规划，Elastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。&lt;/p&gt;

&lt;h1 id=&#34;基本操作&#34;&gt;基本操作&lt;/h1&gt;

&lt;h2 id=&#34;新建和删除-index&#34;&gt;新建和删除 Index&lt;/h2&gt;

&lt;p&gt;新建 Index，可以直接向 Elastic 服务器发出 PUT 请求。下面的例子是新建一个名叫weather的 Index。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/weather&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务器返回一个 JSON 对象，里面的acknowledged字段表示操作成功。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;acknowledged&amp;quot;:true,
  &amp;quot;shards_acknowledged&amp;quot;:true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，我们发出 DELETE 请求，删除这个 Index。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X DELETE &#39;localhost:9200/weather&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;中文分词设置&#34;&gt;中文分词设置&lt;/h2&gt;

&lt;p&gt;首先，安装中文分词插件。这里使用的是 ik，也可以考虑其他插件（比如 smartcn）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码安装的是5.5.1版的插件，与 Elastic 5.5.1 配合使用。&lt;/p&gt;

&lt;p&gt;接着，重新启动 Elastic，就会自动加载这个新安装的插件。&lt;/p&gt;

&lt;p&gt;然后，新建一个 Index，指定需要分词的字段。这一步根据数据结构而异，下面的命令只针对本文。基本上，凡是需要搜索的中文字段，都要单独设置一下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/accounts&#39; -d &#39;
{
  &amp;quot;mappings&amp;quot;: {
    &amp;quot;person&amp;quot;: {
      &amp;quot;properties&amp;quot;: {
        &amp;quot;user&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
          &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
          &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
        },
        &amp;quot;title&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
          &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
          &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
        },
        &amp;quot;desc&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
          &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
          &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
        }
      }
    }
  }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，首先新建一个名称为accounts的 Index，里面有一个名称为person的 Type。person有三个字段。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user
title
desc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这三个字段都是中文，而且类型都是文本（text），所以需要指定中文分词器，不能使用默认的英文分词器。&lt;/p&gt;

&lt;p&gt;Elastic 的分词器称为 analyzer。我们对每个字段指定分词器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;user&amp;quot;: {
  &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
  &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
  &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，analyzer是字段文本的分词器，search_analyzer是搜索词的分词器。ik_max_word分词器是插件ik提供的，可以对文本进行最大数量的分词。&lt;/p&gt;

&lt;h2 id=&#34;新增记录&#34;&gt;新增记录&lt;/h2&gt;

&lt;p&gt;向指定的 /Index/Type 发送 PUT 请求，就可以在 Index 里面新增一条记录。比如，向/accounts/person发送请求，就可以新增一条人员记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/accounts/person/1&#39; -d &#39;
{
  &amp;quot;user&amp;quot;: &amp;quot;张三&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
  &amp;quot;desc&amp;quot;: &amp;quot;数据库管理&amp;quot;
}&#39; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务器返回的 JSON 对象，会给出 Index、Type、Id、Version 等信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
  &amp;quot;_version&amp;quot;:1,
  &amp;quot;result&amp;quot;:&amp;quot;created&amp;quot;,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:2,&amp;quot;successful&amp;quot;:1,&amp;quot;failed&amp;quot;:0},
  &amp;quot;created&amp;quot;:true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你仔细看，会发现请求路径是/accounts/person/1，最后的1是该条记录的 Id。它不一定是数字，任意字符串（比如abc）都可以。&lt;/p&gt;

&lt;p&gt;新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X POST &#39;localhost:9200/accounts/person&#39; -d &#39;
{
  &amp;quot;user&amp;quot;: &amp;quot;李四&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
  &amp;quot;desc&amp;quot;: &amp;quot;系统管理&amp;quot;
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，向/accounts/person发出一个 POST 请求，添加一个记录。这时，服务器返回的 JSON 对象里面，_id字段就是一个随机字符串。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot;:&amp;quot;AV3qGfrC6jMbsbXb6k1p&amp;quot;,
  &amp;quot;_version&amp;quot;:1,
  &amp;quot;result&amp;quot;:&amp;quot;created&amp;quot;,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:2,&amp;quot;successful&amp;quot;:1,&amp;quot;failed&amp;quot;:0},
  &amp;quot;created&amp;quot;:true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，如果没有先创建 Index（这个例子是accounts），直接执行上面的命令，Elastic 也不会报错，而是直接生成指定的 Index。所以，打字的时候要小心，不要写错 Index 的名称。&lt;/p&gt;

&lt;h2 id=&#34;查看记录&#34;&gt;查看记录&lt;/h2&gt;

&lt;p&gt;向/Index/Type/Id发出 GET 请求，就可以查看这条记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/1?pretty=true&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码请求查看/accounts/person/1这条记录，URL 的参数pretty=true表示以易读的格式返回。&lt;/p&gt;

&lt;p&gt;返回的数据中，found字段表示查询成功，_source字段返回原始记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;_index&amp;quot; : &amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot; : &amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
  &amp;quot;_version&amp;quot; : 1,
  &amp;quot;found&amp;quot; : true,
  &amp;quot;_source&amp;quot; : {
    &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
    &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
    &amp;quot;desc&amp;quot; : &amp;quot;数据库管理&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 Id 不正确，就查不到数据，found字段就是false。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/weather/beijing/abc?pretty=true&#39;

{
  &amp;quot;_index&amp;quot; : &amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot; : &amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot; : &amp;quot;abc&amp;quot;,
  &amp;quot;found&amp;quot; : false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;删除记录&#34;&gt;删除记录&lt;/h2&gt;

&lt;p&gt;删除记录就是发出 DELETE 请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X DELETE &#39;localhost:9200/accounts/person/1&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里先不要删除这条记录，后面还要用到。&lt;/p&gt;

&lt;h2 id=&#34;更新记录&#34;&gt;更新记录&lt;/h2&gt;

&lt;p&gt;更新记录就是使用 PUT 请求，重新发送一次数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/accounts/person/1&#39; -d &#39;
{
    &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
    &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
    &amp;quot;desc&amp;quot; : &amp;quot;数据库管理，软件开发&amp;quot;
}&#39; 

{
  &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
  &amp;quot;_version&amp;quot;:2,
  &amp;quot;result&amp;quot;:&amp;quot;updated&amp;quot;,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:2,&amp;quot;successful&amp;quot;:1,&amp;quot;failed&amp;quot;:0},
  &amp;quot;created&amp;quot;:false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，我们将原始数据从&amp;rdquo;数据库管理&amp;rdquo;改成&amp;rdquo;数据库管理，软件开发&amp;rdquo;。 返回结果里面，有几个字段发生了变化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;_version&amp;quot; : 2,
&amp;quot;result&amp;quot; : &amp;quot;updated&amp;quot;,
&amp;quot;created&amp;quot; : false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，记录的 Id 没变，但是版本（version）从1变成2，操作类型（result）从created变成updated，created字段变成false，因为这次不是新建记录。&lt;/p&gt;

&lt;h2 id=&#34;数据查询&#34;&gt;数据查询&lt;/h2&gt;

&lt;p&gt;返回所有记录&lt;/p&gt;

&lt;p&gt;使用 GET 方法，直接请求/Index/Type/_search，就会返回所有记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;

{
  &amp;quot;took&amp;quot;:2,
  &amp;quot;timed_out&amp;quot;:false,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:5,&amp;quot;successful&amp;quot;:5,&amp;quot;failed&amp;quot;:0},
  &amp;quot;hits&amp;quot;:{
    &amp;quot;total&amp;quot;:2,
    &amp;quot;max_score&amp;quot;:1.0,
    &amp;quot;hits&amp;quot;:[
      {
        &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
        &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
        &amp;quot;_id&amp;quot;:&amp;quot;AV3qGfrC6jMbsbXb6k1p&amp;quot;,
        &amp;quot;_score&amp;quot;:1.0,
        &amp;quot;_source&amp;quot;: {
          &amp;quot;user&amp;quot;: &amp;quot;李四&amp;quot;,
          &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
          &amp;quot;desc&amp;quot;: &amp;quot;系统管理&amp;quot;
        }
      },
      {
        &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
        &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
        &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
        &amp;quot;_score&amp;quot;:1.0,
        &amp;quot;_source&amp;quot;: {
          &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
          &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
          &amp;quot;desc&amp;quot; : &amp;quot;数据库管理，软件开发&amp;quot;
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total：返回记录数，本例是2条。
max_score：最高的匹配程度，本例是1.0。
hits：返回的记录组成的数组。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。&lt;/p&gt;

&lt;h2 id=&#34;全文搜索&#34;&gt;全文搜索&lt;/h2&gt;

&lt;p&gt;Elastic 的查询非常特别，使用自己的查询语法，要求 GET 请求带有数据体。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;软件&amp;quot; }}
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码使用 Match 查询，指定的匹配条件是desc字段里面包含&amp;rdquo;软件&amp;rdquo;这个词。返回结果如下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;took&amp;quot;:3,
  &amp;quot;timed_out&amp;quot;:false,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:5,&amp;quot;successful&amp;quot;:5,&amp;quot;failed&amp;quot;:0},
  &amp;quot;hits&amp;quot;:{
    &amp;quot;total&amp;quot;:1,
    &amp;quot;max_score&amp;quot;:0.28582606,
    &amp;quot;hits&amp;quot;:[
      {
        &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
        &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
        &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
        &amp;quot;_score&amp;quot;:0.28582606,
        &amp;quot;_source&amp;quot;: {
          &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
          &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
          &amp;quot;desc&amp;quot; : &amp;quot;数据库管理，软件开发&amp;quot;
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Elastic 默认一次返回10条结果，可以通过size字段改变这个设置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;管理&amp;quot; }},
  &amp;quot;size&amp;quot;: 1
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码指定，每次只返回一条结果。&lt;/p&gt;

&lt;p&gt;还可以通过from字段，指定位移。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;管理&amp;quot; }},
  &amp;quot;from&amp;quot;: 1,
  &amp;quot;size&amp;quot;: 1
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码指定，从位置1开始（默认是从位置0开始），只返回一条结果。&lt;/p&gt;

&lt;h2 id=&#34;逻辑运算&#34;&gt;逻辑运算&lt;/h2&gt;

&lt;p&gt;如果有多个搜索关键字， Elastic 认为它们是or关系。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;软件 系统&amp;quot; }}
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码搜索的是软件 or 系统。&lt;/p&gt;

&lt;p&gt;如果要执行多个关键词的and搜索，必须使用布尔查询。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot;: {
    &amp;quot;bool&amp;quot;: {
      &amp;quot;must&amp;quot;: [
        { &amp;quot;match&amp;quot;: { &amp;quot;desc&amp;quot;: &amp;quot;软件&amp;quot; } },
        { &amp;quot;match&amp;quot;: { &amp;quot;desc&amp;quot;: &amp;quot;系统&amp;quot; } }
      ]
    }
  }
}&#39;
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus redis Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/redis_exporter/</link>
          <pubDate>Thu, 21 Feb 2019 15:10:33 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/redis_exporter/</guid>
          <description>&lt;p&gt;redis探针主要是监控redis相关情况，比如内存，连接数等。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;1、set log format,支持txt和json两种格式的日志&lt;/p&gt;

&lt;p&gt;2、set log level，支持正常的日志级别&lt;/p&gt;

&lt;p&gt;3、show version 支持option打印版本&lt;/p&gt;

&lt;p&gt;4、解析addr和passwd和alias，这几个参数支持参数配置，也可以重环境变量中获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Environment         Variables
Name                Description
REDIS_ADDR          Address of Redis node(s)
REDIS_PASSWORD      Password to use when authenticating to Redis
REDIS_ALIAS         Alias name of Redis node(s)
REDIS_FILE          Path to file containing Redis node(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、创建一个Exporter结构体，初始化，将上面的信息传进去，并创建对应的指标，checkkey支持对多数据库的查询&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Exporter implements the prometheus.Exporter interface, and exports Redis metrics.
type Exporter struct {
    redis        RedisHost
    namespace    string
    keys         []dbKeyPair
    keyValues    *prometheus.GaugeVec
    keySizes     *prometheus.GaugeVec
    duration     prometheus.Gauge
    scrapeErrors prometheus.Gauge
    totalScrapes prometheus.Counter
    metrics      map[string]*prometheus.GaugeVec
    metricsMtx   sync.RWMutex
    sync.RWMutex
}


// RedisHost represents a set of Redis Hosts to health check.
type RedisHost struct {
    Addrs     []string
    Passwords []string
    Aliases   []string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、然后调用client_golang的库函数进行数据采集，scrape连接数据库，调用命令，然后解析返回参数。完成监控。&lt;/p&gt;

&lt;p&gt;由此可见，redis_exporter只是简单的对redis的单节点进行了监控，并没有对redis的cluster和sentinel进行监控。&lt;/p&gt;

&lt;h1 id=&#34;sentinel监控&#34;&gt;sentinel监控&lt;/h1&gt;

&lt;p&gt;这边需要监控sentinel，这边做一个设计&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;运行天数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_start_time_seconds{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 1.551058403e+09
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标直接解析info信息中的sentinel信息，每次通过执行info命令获取，获取uptime_in_days字段值，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;注册shard数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_masters{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标直接解析info信息中的sentinel信息，每次通过执行info命令获取，获取sentinel_masters字段值，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正常shard的个数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_masters_ok{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标直接解析info信息中的sentinel信息，每次通过执行info命令获取，获取status=ok的个数，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;异常shard的个数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_masters_down{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标直接解析info信息中的sentinel信息，每次通过执行info命令获取，获取status=odown的个数，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;sentinel的连接数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_connects{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标是通过执行netstat命令来统计对应进程的连接数，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;检活&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_status{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标是通过执行redis PING命令来检测返回的是否是PONG，如果是则为1，否则为0，代表着sentinel的死活状态，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;26379端口检测&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_port_isalive{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标是通过执行netstat命令来统计对应端口为LiSTEN的数量，1代表端口正在监听，0代表没有监听端口，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前探针融合在redis的探针中，可以自动识别sentinel还是redis，然后放回对应的指标，支持多地址混合配置，可以返回所有需要的指标。&lt;/p&gt;

&lt;h1 id=&#34;连接池&#34;&gt;连接池&lt;/h1&gt;

&lt;p&gt;redis探针目前是短连接，在很多场景下，需要改短连接为长连接，可以只使用一个连接来解决这个问题，这个其实适用于长连接使用的场景，然后考虑长连接可能出来大量连接存在的场景，所以最好直接使用连接池，对连接的数量进行限制，这样就可以完美的解决问题。&lt;/p&gt;

&lt;p&gt;探针启动直接初始化一个连接池，大小为2，正常使用一个长连接来采集高频率的数据，出现异常，可能会使用到第二个长连接。&lt;/p&gt;

&lt;p&gt;验证在高频率的采集下连接数并没有出现增长。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;redis自身支持长连接需要设置参数，tcp_keepalive=1，默认是关闭的，所以都是以timeout时间为准，到时间后redis端会关闭连接，所以探针侧就会close_wait，当然这个连接是可以复用的，当重新请求的时候，又会建立连接在连接池上，按着我们每10s才是一次的频率，redis设置的180s的超时时间，基本上不会出现不断短连接的情形。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Alertmanager</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/alertmanager/</link>
          <pubDate>Tue, 12 Feb 2019 16:00:11 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/alertmanager/</guid>
          <description>&lt;p&gt;Alertmanager主要用于接收 Prometheus 发送的告警信息，它支持丰富的告警通知渠道，而且很容易做到告警信息进行去重，降噪，分组，策略路由，是一款前卫的告警通知系统。&lt;/p&gt;

&lt;h1 id=&#34;安装使用&#34;&gt;安装使用&lt;/h1&gt;

&lt;p&gt;1、获取二进制安装包，直接去prometheus官方去获取就行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export VERSION=0.18.0
wget https://github.com/prometheus/alertmanager/releases/download/v$VERSION/alertmanager-$VERSION.linux-amd64.tar.gz
tar xvf alertmanager-$VERSION.linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、解压成功后，使用 ./alertmanager &amp;ndash;version 来检查是否安装成功&lt;/p&gt;

&lt;p&gt;3、Alertmanager 默认端口为 9093。&lt;/p&gt;

&lt;p&gt;4、配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  resolve_timeout: 2h

route:
  group_by: [&#39;alertname&#39;]
  group_wait: 5s
  group_interval: 10s
  repeat_interval: 1h
  receiver: &#39;webhook&#39;

receivers:
- name: &#39;webhook&#39;
  webhook_configs:
  - url: &#39;http://example.com/xxxx&#39;
    send_resolved: true
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;全局配置（global）：用于定义一些全局的公共参数，如全局的SMTP配置，Slack配置等内容；&lt;/li&gt;
&lt;li&gt;模板（templates）：用于定义告警通知时的模板，如HTML模板，邮件模板等；&lt;/li&gt;
&lt;li&gt;告警路由（route）：根据标签匹配，确定当前告警应该如何处理；&lt;/li&gt;
&lt;li&gt;接收人（receivers）：接收人是一个抽象的概念，它可以是一个邮箱也可以是微信，Slack或者Webhook等，接收人一般配合告警路由使用；&lt;/li&gt;
&lt;li&gt;抑制规则（inhibit_rules）：合理设置抑制规则可以减少垃圾告警的产生&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如使用email告警，配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  smtp_smarthost: &#39;smtp.qq.com:587&#39;
  smtp_from: &#39;xxx@qq.com&#39;
  smtp_auth_username: &#39;xxx@qq.com&#39;
  smtp_auth_password: &#39;your_email_password&#39;

route：
  # If an alert has successfully been sent, wait &#39;repeat_interval&#39; to resend them.
  repeat_interval: 10s    
  #  A default receiver
  receiver: team-X-mails  

receivers:
  - name: &#39;team-X-mails&#39;
    email_configs:
    - to: &#39;team-X+alerts@example.org&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、产生告警，在prometheus下添加 alert.rules 文件，指定告警规则&lt;/p&gt;

&lt;p&gt;文件中写入以下简单规则作为示例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALERT memory_high
  IF prometheus_local_storage_memory_series &amp;gt;= 0
  FOR 15s
  ANNOTATIONS {
    summary = &amp;quot;Prometheus using more memory than it should {{ $labels.instance }}&amp;quot;,
    description = &amp;quot;{{ $labels.instance }} has lots of memory man (current value: {{ $value }}s)&amp;quot;,
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 prometheus.yml 文件&lt;/p&gt;

&lt;p&gt;添加以下规则：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rule_files:
  - &amp;quot;alert.rules&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动AlertManager服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./Alertmanager -config.file=simple.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动prometheus服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./prometheus -Alertmanager.url=http://localhost:9093
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据以上步骤设置，此时 “team-X+alerts@example.org” 应该就可以收到 “xxx@qq.com” 发送的告警邮件了。&lt;/p&gt;

&lt;h1 id=&#34;架构原理&#34;&gt;架构原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/alertmanager/alertmanager&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从左上开始，Prometheus 发送的警报到 Alertmanager;&lt;/li&gt;
&lt;li&gt;警报会被存储到 AlertProvider 中，Alertmanager 的内置实现就是包了一个 map，也就是存放在本机内存中，这里可以很容易地扩展其它 Provider;&lt;/li&gt;
&lt;li&gt;Dispatcher 是一个单独的 goroutine，它会不断到 AlertProvider 拉新的警报，并且根据 YAML 配置的 Routing Tree 将警报路由到一个分组中;&lt;/li&gt;
&lt;li&gt;分组会定时进行 flush (间隔为配置参数中的 group_interval), flush 后这组警报会走一个 Notification Pipeline 链式处理;&lt;/li&gt;
&lt;li&gt;Notification Pipeline 为这组警报确定发送目标，并执行抑制逻辑，静默逻辑，去重逻辑，发送与重试逻辑，实现警报的最终投递;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Client_golang</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/</link>
          <pubDate>Tue, 12 Feb 2019 15:59:42 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/</guid>
          <description>&lt;p&gt;client_golang 是Prometheus client的使用，基于golang语言。提供了prometheus的数据规范。&lt;/p&gt;

&lt;h1 id=&#34;库结构&#34;&gt;库结构&lt;/h1&gt;

&lt;p&gt;地址： &lt;a href=&#34;https://github.com/prometheus/client_golang&#34;&gt;https://github.com/prometheus/client_golang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Directories（描述）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;api                     Package api provides clients for the HTTP APIs.
api/prometheus/v1       Package v1 provides bindings to the Prometheus HTTP API v1: http://prometheus.io/docs/querying/api/

examples/random         A simple example exposing fictional RPC latencies with different types of random distributions (uniform, normal, and exponential) as Prometheus metrics.

examples/simple         A minimal example of how to include Prometheus instrumentation.
prometheus              Package prometheus is the core instrumentation package.
prometheus/graphite     Package graphite provides a bridge to push Prometheus metrics to a Graphite server.
prometheus/promauto     Package promauto provides constructors for the usual Prometheus metrics that return them already registered with the global registry (prometheus.DefaultRegisterer).
prometheus/promhttp     Package promhttp provides tooling around HTTP servers and clients.
prometheus/push         Package push provides functions to push metrics to a Pushgateway.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面对client_golang库的结构和使用进行了总结。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;原理解析&#34;&gt;原理解析&lt;/h2&gt;

&lt;p&gt;下面是客户端的UML图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/library/client_golang/client_golang.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、默认调用gocollect这个采集器，在引入package registry包的时候，就会调用init初始化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;registry会调用describe这个接口，实现就是gocollect这个对应的describe&lt;/li&gt;
&lt;li&gt;http.handle会调用registry的gather函数，然后函数调用collect接口，实现就是gocollect这个对应的collect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、上面只是一种特殊类型，其实对应的四种类型分别都有对应的结构体继承vec的基本函数接口，也有对应的接口，会有对应的实现&lt;/p&gt;

&lt;p&gt;然后这个四种类型就是就是四种collecter，同样的流程&lt;/p&gt;

&lt;p&gt;3、可以新建一个struct作为一个collecter，实现describe，collect接口，就可以实现自己的逻辑，最后其实还是调用四种类型，结合使用&lt;/p&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;p&gt;1、四种类型有实现的函数赋值，常用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set（）
WithLabelValues().set()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、注册的几种方式&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;第一种&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;//statement
proxyTodayTrafficIn := prometheus.NewGaugeVec(prometheus.GaugeOpts{
    Name: &amp;quot;&amp;quot;,
    Help: &amp;quot;The today trafficin of proxy.&amp;quot;,
},[]string{&amp;quot;type&amp;quot;,&amp;quot;laststarttime&amp;quot;,&amp;quot;lastclosetime&amp;quot;})
//get value
proxyTodayTrafficIn.With(prometheus.Labels{&amp;quot;type&amp;quot;:v.Type,&amp;quot;laststarttime&amp;quot;:v.LastStartTime,&amp;quot;lastclosetime&amp;quot;:v.LastCloseTime}).Set(float64(v.TodayTrafficIn))

//registry
prometheus.MustRegister(proxyTodayTrafficIn)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;第二种&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;serverBindPort := prometheus.NewGauge(prometheus.GaugeOpts{
    Name: &amp;quot;frps_server_bind_port&amp;quot;,
    Help: &amp;quot;The port of server frps.&amp;quot;,
})

serverBindPort.Set(float64(cfg.BindPort))

//registry
prometheus.MustRegister(serverBindPort)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面两种可以归为一类，都是采用的默认的方式，下面就涉及到自定义结构体，根据上面的原理，我们需要重自定义的结构体中获取到两个结构体的值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *结构体) Describe(ch chan&amp;lt;- *prometheus.Desc) {}----可见这个接口的实现需要将prometheus.Desc放倒channel中去
func (s *结构体) Collect(ch chan&amp;lt;- prometheus.Metric) {}----可见这个接口的实现需要将prometheus.Metric放倒channel中去
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看这两个结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Desc struct {
    // fqName has been built from Namespace, Subsystem, and Name.
    fqName string
    // help provides some helpful information about this metric.
    help string
    // constLabelPairs contains precalculated DTO label pairs based on
    // the constant labels.
    constLabelPairs []*dto.LabelPair
    // VariableLabels contains names of labels for which the metric
    // maintains variable values.
    variableLabels []string
    // id is a hash of the values of the ConstLabels and fqName. This
    // must be unique among all registered descriptors and can therefore be
    // used as an identifier of the descriptor.
    id uint64
    // dimHash is a hash of the label names (preset and variable) and the
    // Help string. Each Desc with the same fqName must have the same
    // dimHash.
    dimHash uint64
    // err is an error that occured during construction. It is reported on
    // registration time.
    err error
}

type Metric interface {
    // Desc returns the descriptor for the Metric. This method idempotently
    // returns the same descriptor throughout the lifetime of the
    // Metric. The returned descriptor is immutable by contract. A Metric
    // unable to describe itself must return an invalid descriptor (created
    // with NewInvalidDesc).
    Desc() *Desc
    // Write encodes the Metric into a &amp;quot;Metric&amp;quot; Protocol Buffer data
    // transmission object.
    //
    // Metric implementations must observe concurrency safety as reads of
    // this metric may occur at any time, and any blocking occurs at the
    // expense of total performance of rendering all registered
    // metrics. Ideally, Metric implementations should support concurrent
    // readers.
    //
    // While populating dto.Metric, it is the responsibility of the
    // implementation to ensure validity of the Metric protobuf (like valid
    // UTF-8 strings or syntactically valid metric and label names). It is
    // recommended to sort labels lexicographically. (Implementers may find
    // LabelPairSorter useful for that.) Callers of Write should still make
    // sure of sorting if they depend on it.
    Write(*dto.Metric) error
    // TODO(beorn7): The original rationale of passing in a pre-allocated
    // dto.Metric protobuf to save allocations has disappeared. The
    // signature of this method should be changed to &amp;quot;Write() (*dto.Metric,
    // error)&amp;quot;.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我看看如何获取这两种值，首先desc，每种数据类型都有一个desc函数可以直接获取，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name := fmt.Sprintf(&amp;quot;%s_%s&amp;quot;, namespace, metricMaps.Name)

gaugeDescription := prometheus.NewGauge(
    prometheus.GaugeOpts{
        Name:      name,
        Help:      metricMaps.Description,
    },
)

ch &amp;lt;- gaugeDescription.Desc()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以直接新建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewDesc(fqName, help string, variableLabels []string, constLabels Labels) *Desc {}

//new desc
desc := prometheus.NewDesc(name, metricMaps.Description, constLabels, nil)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看metrics这个接口，找到其相应的结构体实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MustNewConstMetric(desc *Desc, valueType ValueType, value float64, labelValues ...string) Metric {}

//channel
ch &amp;lt;- prometheus.MustNewConstMetric(desc, vtype, value, labelValue...)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;第三种&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;新建结构体，完成上面方法的使用，就可以了，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    //set var
    vtype := prometheus.CounterValue
    name := fmt.Sprintf(&amp;quot;%s_%s&amp;quot;, namespace, metricMaps.Name)
    log.Debugf(&amp;quot;counter name: %s&amp;quot;, name)

    //new desc
    desc := prometheus.NewDesc(name, metricMaps.Description, constLabels, nil)

    //deal Value
    value, err := dealValue(res[i])
    if err != nil {
        log.Errorf(&amp;quot;parse value error: %s&amp;quot;,err)
        break
    }
    log.Debugf(&amp;quot;counter value: %s&amp;quot;, value)

    //channel
    ch &amp;lt;- prometheus.MustNewConstMetric(desc, vtype, value, labelValue...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Newregistry调用和直接调用prometheus的对应的MustRegister其实是一样的，都是默认new一个registry的结构体&lt;/p&gt;

&lt;p&gt;4、duplicate metrics collector registration attempted&amp;mdash;重复注册&lt;/p&gt;

&lt;h2 id=&#34;collector&#34;&gt;Collector&lt;/h2&gt;

&lt;p&gt;Collector 中 Describe 和 Collect 方法都是无状态的函数，其中 Describe 暴露全部可能的 Metric 描述列表，在注册（Register）或注销（Unregister）Collector 时会调用 Describe 来获取完整的 Metric 列表，用以检测 Metric 定义的冲突，另外在 github.com/prometheus/client_golang/prometheus/promhttp 下的 Instrument Handler 中，也会通过 Describe 获取 Metric 列表，并检查 label 列表（InstrumentHandler 中只支持 code 和 method 两种自定义 label）；而通过 Collect 可以获取采样数据，然后通过 HTTP 接口暴露给 Prom Server。另外，一些临时性的进程，如批处理任务，可以把数据 push 到 Push Gateway，由 Push Gateway 暴露 pull 接口，此处不赘述。&lt;/p&gt;

&lt;p&gt;客户端对数据的收集大多是针对标准数据结构来进行的,如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Counter：收集事件次数等单调递增的数据&lt;/li&gt;
&lt;li&gt;Gauge：收集当前的状态，可增可减，比如数据库连接数&lt;/li&gt;
&lt;li&gt;Histogram：收集随机正态分布数据，比如响应延迟&lt;/li&gt;
&lt;li&gt;Summary：收集随机正态分布数据，和 Histogram 是类似的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每种标准数据结构还对应了 Vec 结构，通过 Vec 可以简洁的定义一组相同性质的 Metric，在采集数据的时候传入一组自定义的 Label/Value 获取具体的 Metric（Counter/Gauge/Histogram/Summary），最终都会落实到基本的数据结构上，这里不再赘述。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Counter 和 Gauge&lt;/p&gt;

&lt;p&gt;Gauge 和 Counter 基本实现上看是一个进程内共享的浮点数，基于 value 结构实现，而 Counter 和 Gauge 仅仅封装了对这个共享浮点数的各种操作和合法性检查逻辑。&lt;/p&gt;

&lt;p&gt;看 Counter 中 Inc 函数的实现&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/library/client_golang/add.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;value.Add 中修改共享数据时采用了“无锁”实现，相比“有锁 (Mutex)”实现可以更充分利用多核处理器的并行计算能力，性能相比加 Mutex 的实现会有很大提升。下图是 Go Benchmark 的测试结果，对比了“有锁”（用 defer 或不用 defer 来释放锁）和“无锁”实现在多核场景下对性能的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/library/client_golang/benchmark.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Histogram&lt;/p&gt;

&lt;p&gt;Histogram 实现了 Observer 接口，用来获取客户端状态初始化（重启）到某个时间点的采样点分布，监控数据常需要服从正态分布。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Summary&lt;/p&gt;

&lt;p&gt;Summary 是标准数据结构中最复杂的一个，用来收集服从正态分布的采样数据。在 Go 客户端 Summary 结构和 Histogram 一样，都实现了 Observer 接口&lt;/p&gt;

&lt;p&gt;这两个比较复杂，使用较少，可以先不研究，使用的时候研究&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;集成注意事项&#34;&gt;集成注意事项&lt;/h2&gt;

&lt;p&gt;Go 客户端为 HTTP 层的集成提供了方便的 API，但使用中需要注意不要使用 github.com/prometheus/client_golang/prometheus 下定义的已经 deprecated 的 Instrument 函数，除了会引入额外（通常不需要）的监控数据，不仅会对程序性能造成不利影响，而且可能存在危险的 race（如计算请求大小时存在 goroutine 并发地访问 Header 逻辑）。&lt;/p&gt;

&lt;h1 id=&#34;prometheus-exporter实例&#34;&gt;Prometheus Exporter实例&lt;/h1&gt;

&lt;p&gt;Exporter是基于Prometheus实施的监控系统中重要的组成部分，承担数据指标的采集工作，官方的exporter列表中已经包含了常见的绝大多数的系统指标监控，比如用于机器性能监控的node_exporter, 用于网络设备监控的snmp_exporter等等。这些已有的exporter对于监控来说，仅仅需要很少的配置工作就能提供完善的数据指标采集。&lt;/p&gt;

&lt;p&gt;有时我们需要自己去写一些与业务逻辑比较相关的指标监控，这些指标无法通过常见的exporter获取到。比如我们需要提供对于DNS解析情况的整体监控，了解如何编写exporter对于业务监控很重要，也是完善监控系统需要经历的一个阶段。接下来我们就介绍如何编写exporter, 本篇内容编写的语言为golang, 官方也提供了python, java等其他的语言实现的库，采集方式其实大同小异。编写exporter的方式也是大同小异，就是集成对应的prometheus库，我们使用golang语言，就是集成client_golang。&lt;/p&gt;

&lt;p&gt;下面我们就使用golang语言集成cleint_golang来开发一个exporter。&lt;/p&gt;

&lt;h2 id=&#34;搭建环境&#34;&gt;搭建环境&lt;/h2&gt;

&lt;p&gt;首先确保机器上安装了go语言(1.7版本以上)，并设置好了对应的GOPATH。接下来我们就可以开始编写代码了。以下是一个简单的exporter&lt;/p&gt;

&lt;p&gt;下载对应的prometheus包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/prometheus/client_golang/prometheus/promhttp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序主函数:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;log&amp;quot;
    &amp;quot;net/http&amp;quot;
    &amp;quot;github.com/prometheus/client_golang/prometheus/promhttp&amp;quot;
)
func main() {
    http.Handle(&amp;quot;/metrics&amp;quot;, promhttp.Handler())
    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个代码中我们仅仅通过http模块指定了一个路径，并将client_golang库中的promhttp.Handler()作为处理函数传递进去后，就可以获取指标信息了,两行代码实现了一个exporter。这里内部其实是使用了一个默认的收集器将通过NewGoCollector采集当前Go运行时的相关信息比如go堆栈使用,goroutine的数据等等。 通过访问&lt;a href=&#34;http://localhost:8080/metrics&#34;&gt;http://localhost:8080/metrics&lt;/a&gt; 即可查看详细的指标参数。&lt;/p&gt;

&lt;p&gt;上面的代码仅仅展示了一个默认的采集器，并且通过接口调用隐藏了太多实施细节，对于下一步开发并没什么作用，为了实现自定义的监控我们需要先了解一些基本概念。&lt;/p&gt;

&lt;h2 id=&#34;指标类别&#34;&gt;指标类别&lt;/h2&gt;

&lt;p&gt;Prometheus中主要使用的四类指标类型，如下所示&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Counter (累加指标)&lt;/li&gt;
&lt;li&gt;Gauge (测量指标)&lt;/li&gt;
&lt;li&gt;Summary (概略图)&lt;/li&gt;
&lt;li&gt;Histogram (直方图)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些我们在上面基本原理中已经介绍过了，这边详细的介绍，并在下面加以使用。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Counter 一个累加指标数据，这个值随着时间只会逐渐的增加，比如程序完成的总任务数量，运行错误发生的总次数。常见的还有交换机中snmp采集的数据流量也属于该类型，代表了持续增加的数据包或者传输字节累加值。&lt;/li&gt;
&lt;li&gt;Gauge代表了采集的一个单一数据，这个数据可以增加也可以减少，比如CPU使用情况，内存使用量，硬盘当前的空间容量等等&lt;/li&gt;
&lt;li&gt;Histogram和Summary使用的频率较少，两种都是基于采样的方式。另外有一些库对于这两个指标的使用和支持程度不同，有些仅仅实现了部分功能。这两个类型对于某一些业务需求可能比较常见，比如查询单位时间内：总的响应时间低于300ms的占比，或者查询95%用户查询的门限值对应的响应时间是多少。 使用Histogram和Summary指标的时候同时会产生多组数据，_count代表了采样的总数，_sum则代表采样值的和。 _bucket则代表了落入此范围的数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是使用historam来定义的一组指标，计算出了平均五分钟内的查询请求小于0.3s的请求占比总量的比例值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  sum(rate(http_request_duration_seconds_bucket{le=&amp;quot;0.3&amp;quot;}[5m])) by (job)
/
  sum(rate(http_request_duration_seconds_count[5m])) by (job)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果需要聚合数据，可以使用histogram. 并且如果对于分布范围有明确的值的情况下（比如300ms），也可以使用histogram。但是如果仅仅是一个百分比的值（比如上面的95%），则使用Summary&lt;/p&gt;

&lt;h2 id=&#34;定义指标&#34;&gt;定义指标&lt;/h2&gt;

&lt;p&gt;这里我们需要引入另一个依赖库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/prometheus/client_golang/prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面先来定义了两个指标数据，一个是Guage类型， 一个是Counter类型。分别代表了CPU温度和磁盘失败次数统计，使用上面的定义进行分类。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cpuTemp = prometheus.NewGauge(prometheus.GaugeOpts{
    Name: &amp;quot;cpu_temperature_celsius&amp;quot;,
    Help: &amp;quot;Current temperature of the CPU.&amp;quot;,
})
hdFailures = prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Name: &amp;quot;hd_errors_total&amp;quot;,
        Help: &amp;quot;Number of hard-disk errors.&amp;quot;,
    },
    []string{&amp;quot;device&amp;quot;},
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;加一个counter的用法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;totalScrapes: prometheus.NewCounter(prometheus.CounterOpts{
            Namespace: namespace,
            Name:      &amp;quot;exporter_scrapes_total&amp;quot;,
            Help:      &amp;quot;Current total redis scrapes.&amp;quot;,
        })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里还可以注册其他的参数，比如上面的磁盘失败次数统计上，我们可以同时传递一个device设备名称进去，这样我们采集的时候就可以获得多个不同的指标。每个指标对应了一个设备的磁盘失败次数统计。&lt;/p&gt;

&lt;h2 id=&#34;注册指标&#34;&gt;注册指标&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    // Metrics have to be registered to be exposed:
    prometheus.MustRegister(cpuTemp)
    prometheus.MustRegister(hdFailures)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用prometheus.MustRegister是将数据直接注册到Default Registry，就像上面的运行的例子一样，这个Default Registry不需要额外的任何代码就可以将指标传递出去。注册后既可以在程序层面上去使用该指标了，这里我们使用之前定义的指标提供的API（Set和With().Inc）去改变指标的数据内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    cpuTemp.Set(65.3)
    hdFailures.With(prometheus.Labels{&amp;quot;device&amp;quot;:&amp;quot;/dev/sda&amp;quot;}).Inc()

    // The Handler function provides a default handler to expose metrics
    // via an HTTP server. &amp;quot;/metrics&amp;quot; is the usual endpoint for that.
    http.Handle(&amp;quot;/metrics&amp;quot;, promhttp.Handler())
    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中With函数是传递到之前定义的label=”device”上的值，也就是生成指标类似于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cpu_temperature_celsius 65.3
hd_errors_total{&amp;quot;device&amp;quot;=&amp;quot;/dev/sda&amp;quot;} 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然我们写在main函数中的方式是有问题的，这样这个指标仅仅改变了一次，不会随着我们下次采集数据的时候发生任何变化，我们希望的是每次执行采集的时候，程序都去自动的抓取指标并将数据通过http的方式传递给我们。&lt;/p&gt;

&lt;p&gt;到这里，一套基本的采集流程也就完成了，这是最基本的使用方式，当然其中也还是封装了很多过程，比如采集器等，如果需要自定义一些东西，就要了解这些封装的过程，完成重写，下面我们自定义exporter。&lt;/p&gt;

&lt;h2 id=&#34;自定义exporter&#34;&gt;自定义exporter&lt;/h2&gt;

&lt;p&gt;counter数据采集实例，重写collecter&lt;/p&gt;

&lt;p&gt;下面是一个采集Counter类型数据的实例，这个例子中实现了一个自定义的，满足采集器(Collector)接口的结构体，并手动注册该结构体后，使其每次查询的时候自动执行采集任务。&lt;/p&gt;

&lt;p&gt;我们先来看下采集器Collector接口的实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Collector interface {
    // 用于传递所有可能的指标的定义描述符
    // 可以在程序运行期间添加新的描述，收集新的指标信息
    // 重复的描述符将被忽略。两个不同的Collector不要设置相同的描述符
    Describe(chan&amp;lt;- *Desc)

    // Prometheus的注册器调用Collect执行实际的抓取参数的工作，
    // 并将收集的数据传递到Channel中返回
    // 收集的指标信息来自于Describe中传递，可以并发的执行抓取工作，但是必须要保证线程的安全。
    Collect(chan&amp;lt;- Metric)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;了解了接口的实现后，我们就可以写自己的实现了，先定义结构体，这是一个集群的指标采集器，每个集群都有自己的Zone,代表集群的名称。另外两个是保存的采集的指标。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ClusterManager struct {
    Zone         string
    OOMCountDesc *prometheus.Desc
    RAMUsageDesc *prometheus.Desc
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来实现一个采集工作,放到了ReallyExpensiveAssessmentOfTheSystemState函数中实现，每次执行的时候，返回一个按照主机名作为键采集到的数据，两个返回值分别代表了OOM错误计数，和RAM使用指标信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *ClusterManager) ReallyExpensiveAssessmentOfTheSystemState() (
    oomCountByHost map[string]int, ramUsageByHost map[string]float64,
) {
    oomCountByHost = map[string]int{
        &amp;quot;foo.example.org&amp;quot;: int(rand.Int31n(1000)),
        &amp;quot;bar.example.org&amp;quot;: int(rand.Int31n(1000)),
    }
    ramUsageByHost = map[string]float64{
        &amp;quot;foo.example.org&amp;quot;: rand.Float64() * 100,
        &amp;quot;bar.example.org&amp;quot;: rand.Float64() * 100,
    }
    return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现Describe接口，传递指标描述符到channel&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Describe simply sends the two Descs in the struct to the channel.
func (c *ClusterManager) Describe(ch chan&amp;lt;- *prometheus.Desc) {
    ch &amp;lt;- c.OOMCountDesc
    ch &amp;lt;- c.RAMUsageDesc
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Collect函数将执行抓取函数并返回数据，返回的数据传递到channel中，并且传递的同时绑定原先的指标描述符。以及指标的类型（一个Counter和一个Guage）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *ClusterManager) Collect(ch chan&amp;lt;- prometheus.Metric) {
    oomCountByHost, ramUsageByHost := c.ReallyExpensiveAssessmentOfTheSystemState()
    for host, oomCount := range oomCountByHost {
        ch &amp;lt;- prometheus.MustNewConstMetric(
            c.OOMCountDesc,
            prometheus.CounterValue,
            float64(oomCount),
            host,
        )
    }
    for host, ramUsage := range ramUsageByHost {
        ch &amp;lt;- prometheus.MustNewConstMetric(
            c.RAMUsageDesc,
            prometheus.GaugeValue,
            ramUsage,
            host,
        )
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建结构体及对应的指标信息,NewDesc参数第一个为指标的名称，第二个为帮助信息，显示在指标的上面作为注释，第三个是定义的label名称数组，第四个是定义的Labels&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewClusterManager(zone string) *ClusterManager {
    return &amp;amp;ClusterManager{
        Zone: zone,
        OOMCountDesc: prometheus.NewDesc(
            &amp;quot;clustermanager_oom_crashes_total&amp;quot;,
            &amp;quot;Number of OOM crashes.&amp;quot;,
            []string{&amp;quot;host&amp;quot;},
            prometheus.Labels{&amp;quot;zone&amp;quot;: zone},
        ),
        RAMUsageDesc: prometheus.NewDesc(
            &amp;quot;clustermanager_ram_usage_bytes&amp;quot;,
            &amp;quot;RAM usage as reported to the cluster manager.&amp;quot;,
            []string{&amp;quot;host&amp;quot;},
            prometheus.Labels{&amp;quot;zone&amp;quot;: zone},
        ),
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行主程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    workerDB := NewClusterManager(&amp;quot;db&amp;quot;)
    workerCA := NewClusterManager(&amp;quot;ca&amp;quot;)

    // Since we are dealing with custom Collector implementations, it might
    // be a good idea to try it out with a pedantic registry.
    reg := prometheus.NewPedanticRegistry()
    reg.MustRegister(workerDB)
    reg.MustRegister(workerCA)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果直接执行上面的参数的话，不会获取任何的参数，因为程序将自动推出，我们并未定义http接口去暴露数据出来，因此数据在执行的时候还需要定义一个httphandler来处理http请求。&lt;/p&gt;

&lt;p&gt;添加下面的代码到main函数后面，即可实现数据传递到http接口上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gatherers := prometheus.Gatherers{
    prometheus.DefaultGatherer,
    reg,
}

h := promhttp.HandlerFor(gatherers,
    promhttp.HandlerOpts{
        ErrorLog:      log.NewErrorLogger(),
        ErrorHandling: promhttp.ContinueOnError,
    })
http.HandleFunc(&amp;quot;/metrics&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
    h.ServeHTTP(w, r)
})
log.Infoln(&amp;quot;Start server at :8080&amp;quot;)
if err := http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil); err != nil {
    log.Errorf(&amp;quot;Error occur when start server %v&amp;quot;, err)
    os.Exit(1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中prometheus.Gatherers用来定义一个采集数据的收集器集合，可以merge多个不同的采集数据到一个结果集合，这里我们传递了缺省的DefaultGatherer，所以他在输出中也会包含go运行时指标信息。同时包含reg是我们之前生成的一个注册对象，用来自定义采集数据。&lt;/p&gt;

&lt;p&gt;promhttp.HandlerFor()函数传递之前的Gatherers对象，并返回一个httpHandler对象，这个httpHandler对象可以调用其自身的ServHTTP函数来接手http请求，并返回响应。其中promhttp.HandlerOpts定义了采集过程中如果发生错误时，继续采集其他的数据。&lt;/p&gt;

&lt;p&gt;尝试刷新几次浏览器获取最新的指标信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clustermanager_oom_crashes_total{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 364
clustermanager_oom_crashes_total{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 90
clustermanager_oom_crashes_total{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 844
clustermanager_oom_crashes_total{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 801
# HELP clustermanager_ram_usage_bytes RAM usage as reported to the cluster manager.
# TYPE clustermanager_ram_usage_bytes gauge
clustermanager_ram_usage_bytes{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 10.738111282075208
clustermanager_ram_usage_bytes{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 19.003276633920805
clustermanager_ram_usage_bytes{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 79.72085409108028
clustermanager_ram_usage_bytes{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 13.041384617379178
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每次刷新的时候，我们都会获得不同的数据，类似于实现了一个数值不断改变的采集器。当然，具体的指标和采集函数还需要按照需求进行修改，满足实际的业务需求。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/</link>
          <pubDate>Tue, 12 Feb 2019 15:59:42 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/</guid>
          <description>&lt;p&gt;Exporter 本质上就是将收集的数据，转化为对应的文本格式，并提供 http 请求。&lt;/p&gt;

&lt;h1 id=&#34;文本格式&#34;&gt;文本格式&lt;/h1&gt;

&lt;p&gt;Exporter 收集的数据转化的文本内容以行 (\n) 为单位，空行将被忽略, 文本内容最后一行为空行。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;注释&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文本内容，如果以 # 开头通常表示注释。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;以 # HELP 开头表示 metric 帮助说明。&lt;/li&gt;
&lt;li&gt;以 # TYPE 开头表示定义 metric 类型，包含 counter, gauge, histogram, summary, 和 untyped 类型。&lt;/li&gt;
&lt;li&gt;其他表示一般注释，供阅读使用，将被 Prometheus 忽略。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;采样数据的样式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;内容如果不以 # 开头，表示采样数据。它通常紧挨着类型定义行，满足以下格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metric_name [
  &amp;quot;{&amp;quot; label_name &amp;quot;=&amp;quot; `&amp;quot;` label_value `&amp;quot;` { &amp;quot;,&amp;quot; label_name &amp;quot;=&amp;quot; `&amp;quot;` label_value `&amp;quot;` } [ &amp;quot;,&amp;quot; ] &amp;quot;}&amp;quot;
] value [ timestamp ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是一个完整的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# HELP http_requests_total The total number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{method=&amp;quot;post&amp;quot;,code=&amp;quot;200&amp;quot;} 1027 1395066363000
http_requests_total{method=&amp;quot;post&amp;quot;,code=&amp;quot;400&amp;quot;}    3 1395066363000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个 exporter 就是将收集的数据转化为文本格式，并提供 http 请求即可，那很容自己实现一个，prometheus官方提供了client各种语言的库，比如go语言的clent-golang，只要集成使用就能输出对应的指标。&lt;/p&gt;

&lt;h1 id=&#34;client-golang&#34;&gt;client_golang&lt;/h1&gt;

&lt;p&gt;我们开发探针都是基于官方提供的语言库，最多使用的还是golang，我们需要重点了解&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/&#34;&gt;client_golang&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;exporter&#34;&gt;exporter&lt;/h1&gt;

&lt;p&gt;通过各种client库，我们可以开发各种采集探针，将数据转为固定的文本格式来给监控提供数据。不同的监控通过不同的探针来实现，下面是一些常见探针的使用说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/node-exporter/&#34;&gt;Node_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/grok_exporter/&#34;&gt;grok_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/mtail/&#34;&gt;mtail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/server/nginx&#34;&gt;nginx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/blackbox_exporter/&#34;&gt;blackbox_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/gpu_exporter/&#34;&gt;gpu_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/mysqld-exporter/&#34;&gt;mysqld_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/postgres_exporter/&#34;&gt;postgres_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/process_exporter/&#34;&gt;process_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/redis_exporter/&#34;&gt;redis_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/snmp_exporter/&#34;&gt;snmp_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;其他探针&#34;&gt;其他探针&lt;/h2&gt;

&lt;p&gt;还有其他很多采集器，可以直接去prometheus官方网站看目前发布的&lt;a href=&#34;https://prometheus.io/docs/instrumenting/exporters/&#34;&gt;https://prometheus.io/docs/instrumenting/exporters/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;开发注意&#34;&gt;开发注意&lt;/h1&gt;

&lt;p&gt;1、探针日志注意事项：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;正常情况下应该不输出日志，只会在有错误的情况下输出错误日志&lt;/li&gt;
&lt;li&gt;如果输出日志了，应该设置定时清理脚本&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个也适用于很多服务器下面的情况，毕竟如果程序如果跑在了别人的机器上，你是不容易操作的。&lt;/p&gt;

&lt;p&gt;2、探针的管理&lt;/p&gt;

&lt;p&gt;prometheus的exporter都是独立的，简单几个使用还是不错，解耦还开箱即用，但是数量多了，运维的压力变大了，例如探针管理升级，运行情况的检查等，有几种方案解决&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;做一个管理平台，类似于后台系统，专门对exporter进行管理&lt;/li&gt;
&lt;li&gt;用一个主进程整合几个探针，每个探针依旧是原来的版本&lt;/li&gt;
&lt;li&gt;用telegraf来支持各种类型的input，all in one&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Sort</title>
          <link>https://kingjcy.github.io/post/golang/go-sort/</link>
          <pubDate>Fri, 01 Feb 2019 11:54:17 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-sort/</guid>
          <description>&lt;p&gt;golang中也实现了排序算法的包sort包．&lt;/p&gt;

&lt;h1 id=&#34;sort&#34;&gt;sort&lt;/h1&gt;

&lt;p&gt;sort包中实现了4种基本的排序算法：插入排序．归并排序，快排和堆排序．和其他语言中一样，这四种方式都是不公开的，他们只在sort包内部使用．所以用户在使用sort包进行排序时无需考虑使用那种排序方式，sort包会根据实际数据自动选择高效的排序算法，只要实现sort.Interface接口就行。&lt;/p&gt;

&lt;h1 id=&#34;接口&#34;&gt;接口&lt;/h1&gt;

&lt;p&gt;sort.Interface定义的三个方法：获取数据集合长度的Len()方法、比较两个元素大小的Less()方法和交换两个元素位置的Swap()方法，就可以顺利对数据集合进行排序。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Interface interface {

    Len() int    // Len 为集合内元素的总数

    Less(i, j int) bool　//如果index为i的元素小于index为j的元素，则返回true，否则返回false

    Swap(i, j int)  // Swap 交换索引为 i 和 j 的元素
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;任何实现了 sort.Interface 的类型（一般为集合），均可使用该包中的方法进行排序。&lt;/p&gt;

&lt;h1 id=&#34;实例&#34;&gt;实例&lt;/h1&gt;

&lt;p&gt;golang自身实现的interface有三种，Float64Slice，IntSlice，StringSlice，具体如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Float64Slice

type Float64Slice []float64
Float64Slice 针对 []float6 实现接口的方法，以升序排列。
func (p Float64Slice) Len() int　　　　//求长度
func (p Float64Slice) Less(i, j int) bool　//比大小
func (p Float64Slice) Search(x float64) int　//查找
func (p Float64Slice) Sort()　　　　　　//排序
func (p Float64Slice) Swap(i, j int)　　　//交换位置



type IntSlice

type IntSlice []int
IntSlice 针对 []int 实现接口的方法，以升序排列。

func (p IntSlice) Len() int
func (p IntSlice) Less(i, j int) bool
func (p IntSlice) Search(x int) int
func (p IntSlice) Sort()
func (p IntSlice) Swap(i, j int)


type StringSlice

type StringSlice []string
StringSlice 针对 []string 实现接口的方法，以升序排列。

func (p StringSlice) Len() int
func (p StringSlice) Less(i, j int) bool
func (p StringSlice) Search(x string) int
func (p StringSlice) Sort()
func (p StringSlice) Swap(i, j int)


func Reverse(data Interface) Interface
func Reverse(data Interface) Interface
Reverse实现对data的逆序排列
package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sort&amp;quot;
)

func main() {
    a := []int{1, 2, 5, 3, 4}
    fmt.Println(a)        // [1 2 5 3 4]
    sort.Sort(sort.Reverse(sort.IntSlice(a)))
    fmt.Println(a)        // [5 4 3 2 1]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内部已经实现的float，int，string等排序，这些方法要求集合内列出元素的索引为整数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Float64s(a []float64)     //Float64s将类型为float64的slice a以升序方式进行排序
func Float64sAreSorted(a []float64) bool　　//判定是否已经进行排序func Ints(a []int)

func Ints(a []int)                       //Ints 以升序排列 int 切片。
func IntsAreSorted(a []int) bool　　　  //IntsAreSorted 判断 int 切片是否已经按升序排列。
func IsSorted(data Interface) bool    IsSorted 判断数据是否已经排序。包括各种可sort的数据类型的判断．

func Strings(a []string)//Strings 以升序排列 string 切片。
func StringsAreSorted(a []string) bool//StringsAreSorted 判断 string 切片是否已经按升序排列。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sort&amp;quot;
)

//定义interface{},并实现sort.Interface接口的三个方法
type IntSlice []int

func (c IntSlice) Len() int {
    return len(c)
}
func (c IntSlice) Swap(i, j int) {
    c[i], c[j] = c[j], c[i]
}
func (c IntSlice) Less(i, j int) bool {
    return c[i] &amp;lt; c[j]
}

func main() {
    a := IntSlice{1, 3, 5, 7, 2}
    b := []float64{1.1, 2.3, 5.3, 3.4}
    c := []int{1, 3, 5, 4, 2}
    fmt.Println(sort.IsSorted(a)) //false
    if !sort.IsSorted(a) {
        sort.Sort(a) 
    }

    if !sort.Float64sAreSorted(b) {
        sort.Float64s(b)
    }
    if !sort.IntsAreSorted(c) {
        sort.Ints(c)
    }
    fmt.Println(a)//[1 2 3 5 7]
    fmt.Println(b)//[1.1 2.3 3.4 5.3]
    fmt.Println(c)// [1 2 3 4 5]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;search&#34;&gt;search&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;func Search(n int, f func(int) bool) int   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;search使用二分法进行查找，Search()方法回使用“二分查找”算法来搜索某指定切片[0:n]，并返回能够使f(i)=true的最小的i（0&amp;lt;=i&amp;lt;n）值，并且会假定，如果f(i)=true，则f(i+1)=true，即对于切片[0:n]，i之前的切片元素会使f()函数返回false，i及i之后的元素会使f()函数返回true。但是，当在切片中无法找到时f(i)=true的i时（此时切片元素都不能使f()函数返回true），Search()方法会返回n（而不是返回-1）。&lt;/p&gt;

&lt;p&gt;Search 常用于在一个已排序的，可索引的数据结构中寻找索引为 i 的值 x，例如数组或切片。这种情况下，实参 f，一般是一个闭包，会捕获所要搜索的值，以及索引并排序该数据结构的方式。&lt;/p&gt;

&lt;p&gt;为了查找某个值，而不是某一范围的值时，如果slice以升序排序，则　f func中应该使用＞＝,如果slice以降序排序，则应该使用&amp;lt;=. 例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sort&amp;quot;
)

func main() {
    a := []int{1, 2, 3, 4, 5}
    b := sort.Search(len(a), func(i int) bool { return a[i] &amp;gt;= 30 })
    fmt.Println(b)　　　　　　　//5，查找不到，返回a slice的长度５，而不是-1
    c := sort.Search(len(a), func(i int) bool { return a[i] &amp;lt;= 3 })
    fmt.Println(c)                             //0，利用二分法进行查找，返回符合条件的最左边数值的index，即为０
    d := sort.Search(len(a), func(i int) bool { return a[i] == 3 })
    fmt.Println(d)                          //2　　　
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;官网上面有趣的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func GuessingGame() {
    var s string
    fmt.Printf(&amp;quot;Pick an integer from 0 to 100.\n&amp;quot;)
    answer := sort.Search(100, func(i int) bool {
        fmt.Printf(&amp;quot;Is your number &amp;lt;= %d? &amp;quot;, i)
        fmt.Scanf(&amp;quot;%s&amp;quot;, &amp;amp;s)
        return s != &amp;quot;&amp;quot; &amp;amp;&amp;amp; s[0] == &#39;y&#39;
    })
    fmt.Printf(&amp;quot;Your number is %d.\n&amp;quot;, answer)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还实现了指定类型的search&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func SearchFloat64s(a []float64, x float64) int　　//SearchFloat64s 在float64s切片中搜索x并返回索引如Search函数所述. 返回可以插入x值的索引位置，如果x不存在，返回数组a的长度切片必须以升序排列
func SearchInts(a []int, x int) int  //SearchInts 在ints切片中搜索x并返回索引如Search函数所述. 返回可以插入x值的索引位置，如果x不存在，返回数组a的长度切片必须以升序排列
func SearchStrings(a []string, x string) int//SearchFloat64s 在strings切片中搜索x并返回索引如Search函数所述. 返回可以插入x值的索引位置，如果x不存在，返回数组a的长度切片必须以升序排列
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中需要注意的是，以上三种search查找方法，其对应的slice必须按照升序进行排序，否则会出现奇怪的结果．&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sort&amp;quot;
)

func main() {
    a := []string{&amp;quot;a&amp;quot;, &amp;quot;c&amp;quot;}
    i := sort.SearchStrings(a, &amp;quot;b&amp;quot;)
    fmt.Println(i) //1
    b := []string{&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;}
    i = sort.SearchStrings(b, &amp;quot;b&amp;quot;)
    fmt.Println(i) //1
    c := []string{&amp;quot;d&amp;quot;, &amp;quot;c&amp;quot;}
    i = sort.SearchStrings(c, &amp;quot;b&amp;quot;)
    fmt.Println(i) //0
    d := []string{&amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;b&amp;quot;}
    i = sort.SearchStrings(d, &amp;quot;b&amp;quot;)
    fmt.Println(i) //0，由于d不是以升序方式排列，所以出现奇怪的结果，这可以根据SearchStrings的定义进行解释．见下方．
}


func SearchStrings(a []string, x string) int {
return Search(len(a), func(i int) bool { return a[i] &amp;gt;= x })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由此可见，为了精确查找，必须对[]string　以升序方式进行排序。&lt;/p&gt;

&lt;h1 id=&#34;其他函数&#34;&gt;其他函数&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;func Sort(data Interface)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sort 对 data 进行排序。它调用一次 data.Len 来决定排序的长度 n，调用 data.Less 和 data.Swap 的开销为O(n*log(n))。此排序为不稳定排序。他根据不同形式决定使用不同的排序方式（插入排序，堆排序，快排）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Stable(data Interface)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stable对data进行排序，不过排序过程中，如果data中存在相等的元素，则他们原来的顺序不会改变，即如果有两个相等元素num,他们的初始index分别为i和j，并且i&amp;lt;j，则利用Stable对data进行排序后，i依然小于ｊ．直接利用sort进行排序则不能够保证这一点。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Math</title>
          <link>https://kingjcy.github.io/post/golang/go-math/</link>
          <pubDate>Fri, 01 Feb 2019 11:37:51 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-math/</guid>
          <description>&lt;p&gt;math包实现的就是数学函数计算。&lt;/p&gt;

&lt;h1 id=&#34;math&#34;&gt;math&lt;/h1&gt;

&lt;p&gt;提供了三角函数,幂次函数,特殊函数,类型转化函数等还有其他函数。&lt;/p&gt;

&lt;h2 id=&#34;三角函数&#34;&gt;三角函数&lt;/h2&gt;

&lt;p&gt;正弦函数，反正弦函数，双曲正弦，反双曲正弦&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;func Sin(x float64) float64&lt;/li&gt;
&lt;li&gt;func Asin(x float64) float64&lt;/li&gt;
&lt;li&gt;func Sinh(x float64) float64&lt;/li&gt;
&lt;li&gt;func Asinh(x float64) float64&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一次性返回 sin,cos&lt;/p&gt;

&lt;p&gt;func Sincos(x float64) (sin, cos float64)&lt;/p&gt;

&lt;p&gt;余弦函数，反余弦函数，双曲余弦，反双曲余弦&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;func Cos(x float64) float64&lt;/li&gt;
&lt;li&gt;func Acos(x float64) float64&lt;/li&gt;
&lt;li&gt;func Cosh(x float64) float64&lt;/li&gt;
&lt;li&gt;func Acosh(x float64) float64&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;正切函数，反正切函数，双曲正切，反双曲正切&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;func Tan(x float64) float64&lt;/li&gt;
&lt;li&gt;func Atan(x float64) float64 和 func Atan2(y, x float64) float64&lt;/li&gt;
&lt;li&gt;func Tanh(x float64) float64&lt;/li&gt;
&lt;li&gt;func Atanh(x float64) float64&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;幂次函数&#34;&gt;幂次函数&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;func Cbrt(x float64) float64 // 立方根函数&lt;/li&gt;
&lt;li&gt;func Pow(x, y float64) float64  // x 的幂函数&lt;/li&gt;
&lt;li&gt;func Pow10(e int) float64 // 10 根的幂函数&lt;/li&gt;
&lt;li&gt;func Sqrt(x float64) float64 // 平方根&lt;/li&gt;
&lt;li&gt;func Log(x float64) float64 // 对数函数&lt;/li&gt;
&lt;li&gt;func Log10(x float64) float64 // 10 为底的对数函数&lt;/li&gt;
&lt;li&gt;func Log2(x float64) float64  // 2 为底的对数函数&lt;/li&gt;
&lt;li&gt;func Log1p(x float64) float64 // log(1 + x)&lt;/li&gt;
&lt;li&gt;func Logb(x float64) float64 // 相当于 log2(x) 的绝对值&lt;/li&gt;
&lt;li&gt;func Ilogb(x float64) int // 相当于 log2(x) 的绝对值的整数部分&lt;/li&gt;
&lt;li&gt;func Exp(x float64) float64 // 指数函数&lt;/li&gt;
&lt;li&gt;func Exp2(x float64) float64 // 2 为底的指数函数&lt;/li&gt;
&lt;li&gt;func Expm1(x float64) float64 // Exp(x) - 1&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;特殊函数&#34;&gt;特殊函数&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;func Inf(sign int) float64  // 正无穷&lt;/li&gt;
&lt;li&gt;func IsInf(f float64, sign int) bool // 是否正无穷&lt;/li&gt;
&lt;li&gt;func NaN() float64 // 无穷值&lt;/li&gt;
&lt;li&gt;func IsNaN(f float64) (is bool) // 是否是无穷值&lt;/li&gt;
&lt;li&gt;func Hypot(p, q float64) float64 // 计算直角三角形的斜边长&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;类型转化函数&#34;&gt;类型转化函数&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;func Float32bits(f float32) uint32  // float32 和 unit32 的转换&lt;/li&gt;
&lt;li&gt;func Float32frombits(b uint32) float32 // uint32 和 float32 的转换&lt;/li&gt;
&lt;li&gt;func Float64bits(f float64) uint64 // float64 和 uint64 的转换&lt;/li&gt;
&lt;li&gt;func Float64frombits(b uint64) float64 // uint64 和 float64 的转换&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;其他函数&#34;&gt;其他函数&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;func Abs(x float64) float64 // 绝对值函数&lt;/li&gt;
&lt;li&gt;func Ceil(x float64) float64  // 向上取整&lt;/li&gt;
&lt;li&gt;func Floor(x float64) float64 // 向下取整&lt;/li&gt;
&lt;li&gt;func Mod(x, y float64) float64 // 取模&lt;/li&gt;
&lt;li&gt;func Modf(f float64) (int float64, frac float64) // 分解 f，以得到 f 的整数和小数部分&lt;/li&gt;
&lt;li&gt;func Frexp(f float64) (frac float64, exp int) // 分解 f，得到 f 的位数和指数&lt;/li&gt;
&lt;li&gt;func Max(x, y float64) float64  // 取大值&lt;/li&gt;
&lt;li&gt;func Min(x, y float64) float64  // 取小值&lt;/li&gt;
&lt;li&gt;func Dim(x, y float64) float64 // 复数的维数&lt;/li&gt;
&lt;li&gt;func J0(x float64) float64  // 0 阶贝塞尔函数&lt;/li&gt;
&lt;li&gt;func J1(x float64) float64  // 1 阶贝塞尔函数&lt;/li&gt;
&lt;li&gt;func Jn(n int, x float64) float64 // n 阶贝塞尔函数&lt;/li&gt;
&lt;li&gt;func Y0(x float64) float64  // 第二类贝塞尔函数 0 阶&lt;/li&gt;
&lt;li&gt;func Y1(x float64) float64  // 第二类贝塞尔函数 1 阶&lt;/li&gt;
&lt;li&gt;func Yn(n int, x float64) float64 // 第二类贝塞尔函数 n 阶&lt;/li&gt;
&lt;li&gt;func Erf(x float64) float64 // 误差函数&lt;/li&gt;
&lt;li&gt;func Erfc(x float64) float64 // 余补误差函数&lt;/li&gt;
&lt;li&gt;func Copysign(x, y float64) float64 // 以 y 的符号返回 x 值&lt;/li&gt;
&lt;li&gt;func Signbit(x float64) bool // 获取 x 的符号&lt;/li&gt;
&lt;li&gt;func Gamma(x float64) float64 // 伽玛函数&lt;/li&gt;
&lt;li&gt;func Lgamma(x float64) (lgamma float64, sign int) // 伽玛函数的自然对数&lt;/li&gt;
&lt;li&gt;func Ldexp(frac float64, exp int) float64 // value 乘以 2 的 exp 次幂&lt;/li&gt;
&lt;li&gt;func Nextafter(x, y float64) (r float64) // 返回参数 x 在参数 y 方向上可以表示的最接近的数值，若 x 等于 y，则返回 x&lt;/li&gt;
&lt;li&gt;func Nextafter32(x, y float32) (r float32) // 返回参数 x 在参数 y 方向上可以表示的最接近的数值，若 x 等于 y，则返回 x&lt;/li&gt;
&lt;li&gt;func Remainder(x, y float64) float64 // 取余运算&lt;/li&gt;
&lt;li&gt;func Trunc(x float64) float64 // 截取函数&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;math包中定义的常量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math&amp;quot;
)

func main() {

    fmt.Printf(&amp;quot;float64的最大值是:%.f\n&amp;quot;, math.MaxFloat64)
    fmt.Printf(&amp;quot;float64的最小值是:%.f\n&amp;quot;, math.SmallestNonzeroFloat64)
    fmt.Printf(&amp;quot;float32的最大值是:%.f\n&amp;quot;, math.MaxFloat32)
    fmt.Printf(&amp;quot;float32的最小值是:%.f\n&amp;quot;, math.SmallestNonzeroFloat32)
    fmt.Printf(&amp;quot;Int8的最大值是:%d\n&amp;quot;, math.MaxInt8)
    fmt.Printf(&amp;quot;Int8的最小值是:%d\n&amp;quot;, math.MinInt8)
    fmt.Printf(&amp;quot;Uint8的最大值是:%d\n&amp;quot;, math.MaxUint8)
    fmt.Printf(&amp;quot;Int16的最大值是:%d\n&amp;quot;, math.MaxInt16)
    fmt.Printf(&amp;quot;Int16的最小值是:%d\n&amp;quot;, math.MinInt16)
    fmt.Printf(&amp;quot;Uint16的最大值是:%d\n&amp;quot;, math.MaxUint16)
    fmt.Printf(&amp;quot;Int32的最大值是:%d\n&amp;quot;, math.MaxInt32)
    fmt.Printf(&amp;quot;Int32的最小值是:%d\n&amp;quot;, math.MinInt32)
    fmt.Printf(&amp;quot;Uint32的最大值是:%d\n&amp;quot;, math.MaxUint32)
    fmt.Printf(&amp;quot;Int64的最大值是:%d\n&amp;quot;, math.MaxInt64)
    fmt.Printf(&amp;quot;Int64的最小值是:%d\n&amp;quot;, math.MinInt64)
    //fmt.Println(&amp;quot;Uint64的最大值是:&amp;quot;, math.MaxUint64)
    fmt.Printf(&amp;quot;圆周率默认为:%.200f\n&amp;quot;, math.Pi)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math&amp;quot;
)

func main() {
    /*
        取绝对值,函数签名如下:
            func Abs(x float64) float64
    */
    fmt.Printf(&amp;quot;[-3.14]的绝对值为:[%.2f]\n&amp;quot;, math.Abs(-3.14))

    /*
        取x的y次方，函数签名如下:
            func Pow(x, y float64) float64
    */
    fmt.Printf(&amp;quot;[2]的16次方为:[%.f]\n&amp;quot;, math.Pow(2, 16))

    /*
        取余数，函数签名如下:
            func Pow10(n int) float64
    */
    fmt.Printf(&amp;quot;10的[3]次方为:[%.f]\n&amp;quot;, math.Pow10(3))

    /*
        取x的开平方，函数签名如下:
            func Sqrt(x float64) float64
    */
    fmt.Printf(&amp;quot;[64]的开平方为:[%.f]\n&amp;quot;, math.Sqrt(64))

    /*
        取x的开立方，函数签名如下:
            func Cbrt(x float64) float64
    */
    fmt.Printf(&amp;quot;[27]的开立方为:[%.f]\n&amp;quot;, math.Cbrt(27))

    /*
        向上取整，函数签名如下:
            func Ceil(x float64) float64
    */
    fmt.Printf(&amp;quot;[3.14]向上取整为:[%.f]\n&amp;quot;, math.Ceil(3.14))

    /*
        向下取整，函数签名如下:
            func Floor(x float64) float64
    */
    fmt.Printf(&amp;quot;[8.75]向下取整为:[%.f]\n&amp;quot;, math.Floor(8.75))

    /*
        取余数，函数签名如下:
            func Floor(x float64) float64
    */
    fmt.Printf(&amp;quot;[10/3]的余数为:[%.f]\n&amp;quot;, math.Mod(10, 3))

    /*
        分别取整数和小数部分,函数签名如下:
            func Modf(f float64) (int float64, frac float64)
    */
    Integer, Decimal := math.Modf(3.14159265358979)
    fmt.Printf(&amp;quot;[3.14159265358979]的整数部分为:[%.f],小数部分为:[%.14f]\n&amp;quot;, Integer, Decimal)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;rand&#34;&gt;rand&lt;/h1&gt;

&lt;p&gt;rand包实现了伪随机数生成器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import &amp;quot;math/rand&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;随机数从资源生成。包水平的函数都使用的默认的公共资源。该资源会在程序每次运行时都产生确定的序列。如果需要每次运行产生不同的序列，应使用Seed函数进行初始化。如果不调用sned函数，默认都是采用sned（1），默认资源可以安全的用于多go程并发。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/rand&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    rand.Seed(time.Now().UnixNano())
    for i := 0; i &amp;lt; 10; i++ {
        x := rand.Intn(100)
        fmt.Println(x)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上例子是打印10个100以内（0-99）的随机数字&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rand.Intn(n)。int代表整数，后面的n代表范围，其他类型类似&lt;/li&gt;
&lt;li&gt;rand.Seed 设置随机数种子&lt;/li&gt;
&lt;li&gt;time.Now().UnixNano() 种子，也就是随机因子，相同的种子产生的随机数是一样的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;自定义生成rand结构体-设置随机数种子&#34;&gt;自定义生成Rand结构体，设置随机数种子&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func NewSource(seed int64) Source
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用给定的种子创建一个伪随机资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func New(src Source) *Rand
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个使用src生产的随机数来生成其他各种分布的随机数值的*Rand。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/rand&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    source := rand.NewSource(time.Now().UnixNano()) // 使用当前的纳秒生成一个随机源，也就是随机种子
    ran := rand.New(source) // 生成一个rand
    fmt.Println(rand.Int())
    fmt.Println(rand.Int31())
    fmt.Println(rand.Intn(5))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;其它生成随机数的方法&#34;&gt;其它生成随机数的方法&lt;/h2&gt;

&lt;p&gt;Rand生成随机数当然不只这三个方法，还有其它生成随机数的方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Int63() int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个int64类型的非负的63位伪随机数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Uint32() uint32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个uint32类型的非负的32位伪随机数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Int31n(n int32) int32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个取值范围在[0,n)的伪随机int32值，如果n&amp;lt;=0会panic。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Int63n(n int64) int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个取值范围在[0, n)的伪随机int64值，如果n&amp;lt;=0会panic。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Float32() float32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个取值范围在[0.0, 1.0)的伪随机float32值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Float64() float64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个取值范围在[0.0, 1.0)的伪随机float64值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Perm(n int) []int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个有n个元素的，[0,n)范围内整数的伪随机排列的切片。&lt;/p&gt;

&lt;h2 id=&#34;crypto-rand&#34;&gt;crypto/rand&lt;/h2&gt;

&lt;p&gt;这个rand包实现了用于加解密的更安全的随机数生成器,主要应用场景：生成随机加密串。&lt;/p&gt;

&lt;p&gt;这边简单使用一个实例说明，具体可以看&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-crypto/&#34;&gt;crypto&lt;/a&gt;包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;crypto/rand&amp;quot;
    &amp;quot;encoding/base64&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/big&amp;quot;
)

func main() {
    //1、Int
    n, err := rand.Int(rand.Reader, big.NewInt(128))
    if err == nil {
        fmt.Println(&amp;quot;rand.Int：&amp;quot;, n, n.BitLen())
    }
    //2、Prime
    p, err := rand.Prime(rand.Reader, 5)
    if err == nil {
        fmt.Println(&amp;quot;rand.Prime：&amp;quot;, p)
    }
    //3、Read
    b := make([]byte, 32)
    m, err := rand.Read(b)
    if err == nil {
        fmt.Println(&amp;quot;rand.Read：&amp;quot;, b[:m])
        fmt.Println(&amp;quot;rand.Read：&amp;quot;, base64.URLEncoding.EncodeToString(b))
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rand.Int： 92 7
rand.Prime： 29
rand.Read： [207 47 241 208 190 84 109 134 86 106 87 223 111 113 203 155 44 118 71 20 186 62 66 130 244 98 97 184 8 179 6 230]
rand.Read： zy_x0L5UbYZWalffb3HLmyx2RxS6PkKC9GJhuAizBuY=
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;big&#34;&gt;big&lt;/h1&gt;

&lt;p&gt;大数处理，可以用golang的math/big包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/big&amp;quot;
)

func main() {
    //设置一个大于int64的数
    a := new(big.Int)
    a, ok := a.SetString(&amp;quot;9122322238215458478512545454878168716584545412154785452142499999&amp;quot;, 10)
    if !ok {
        panic(&amp;quot;error&amp;quot;)
    }
    //String方法可以转换成字符串输出
    fmt.Println(a.String())

    //大数相加
    b:=big.NewInt(2)
    b=b.Add(a,b) //  Mod 取模、Add 加、Sub 减、Mul 乘、Div 除
    fmt.Println(b.String())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;cmplx&#34;&gt;cmplx&lt;/h1&gt;

&lt;p&gt;cmplx 包为复数提供基本的常量和数学函数。&lt;/p&gt;

&lt;h1 id=&#34;bits&#34;&gt;bits&lt;/h1&gt;

&lt;p&gt;bits主要打包字节为预先声明的无符号整数类型实现位计数和操作函数。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 文件存储系统Fastdfs</title>
          <link>https://kingjcy.github.io/post/distributed/store/fs/fastfs/</link>
          <pubDate>Wed, 16 Jan 2019 20:32:58 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/fs/fastfs/</guid>
          <description>&lt;p&gt;fastdfs是一个开源的轻量级分布式文件系统，是纯C语言开发的。它对文件进行管理，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等，FastDFS 针对大量小文件存储有优势。&lt;/p&gt;

&lt;h1 id=&#34;安装-v5-08&#34;&gt;安装（v5.08）&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;172.32.148.127 client&lt;/p&gt;

&lt;p&gt;172.32.148.128 storage&lt;/p&gt;

&lt;p&gt;172.32.148.129 tracker storage&lt;/p&gt;

&lt;p&gt;172.32.148.130 storage&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;依赖libfastcommon安装&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;v5版本已经不需要单独安装libevent，但是需要安装项目中的libfastcommon，所有机器都要安装，下载地址&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/happyfish100/libfastcommon.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后用自带的脚本make.sh编译安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./make.sh
./make.sh install
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;fastdfs安装&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;依赖安装好就开始安装fastdfs，下载压缩包fastdfs-5.08.tar.gz，所有机器进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./make.sh
./make.sh install
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;配置文件与进程&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;fastdfs安装好后在/etc/fdfs/下面会有对应的配置文件，这个项目的配置文件注释还是比较全面的，对它进行修改&lt;/p&gt;

&lt;p&gt;1、tracker.conf&lt;/p&gt;

&lt;p&gt;监控进程使用的配置文件，主要是配置端口，日志数据存储路径，端口一般使用配置文件默认的22122，在172.32.148.129上操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# the tracker server port
port=22122

# the base path to store data and log files
base_path=/home/jcy/fastdfs/fastdfs_tracker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动监控进程，到对应的路径下看日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fdfs_trackerd /etc/fdfs/tracker.conf start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以ps看一下进程启动状况，也可以到日志看启动状况，也可以通过netstat来看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netstat -tupln|grep tracker
#可以看到如下：
tcp  0   0   0.0.0.0:22122   0.0.0.0:*   LISTEN   18559/fdfs_trackerd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、storage.conf&lt;/p&gt;

&lt;p&gt;存储进程使用的配置文件，主要是指定tracker服务的地址和端口，因为存储进程要定时给监控进程发送数据信息，同group内的storage进程会相互connect，来同步文件。这些进程在172.32.148.128，172.32.148.129，172.32.148.130上进行操作。同时它也有自己的group_name和开发的端口，还有对应的日志数据路径。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# the name of the group this storage server belongs to
#
# comment or remove this item for fetching from tracker server,
# in this case, use_storage_id must set to true in tracker.conf,
# and storage_ids.conf must be configed correctly.
group_name=group1

# the storage server port
port=23000

# the base path to store data and log files
base_path=/home/jcy/fastdfs/fastdfs_storage

# store_path#, based 0, if store_path0 not exists, it&#39;s value is base_path
# the paths must be exist
store_path0=/home/jcy/fastdfs/fastdfs_storage

# tracker_server can ocur more than once, and tracker_server format is
#  &amp;quot;host:port&amp;quot;, host can be hostname or ip address
tracker_server=172.32.148.129:22122
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动存储进程，查看对应的情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fdfs_storaged /etc/fdfs/storage.conf start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、client.conf&lt;/p&gt;

&lt;p&gt;客户端进行文件上传的测试,首先是对配置文件的tracker服务器地址进行配置，然后就是一些基本配置看注释就可以&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;测试文件上传&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在客户端机器172.32.148.127上进行操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[jcy@pdapp17 fastdfs]$ fdfs_upload_file /etc/fdfs/client.conf a.txt
group1/M00/00/00/rCCUgVh8lF-ATRZpAAAADw_r4o4559.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上传成功，到对应的storage进程指定的path下的data目录就可以找到对应的文件，文件会同时同步的同一个group下的所有storage上，完成备份。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;FastDFS服务端有三个角色：跟踪服务器（tracker server）、存储服务器（storage server）和客户端（client）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;tracker server：跟踪服务器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要做调度工作，起负载均衡的作用。在内存中记录集群中所有存储组和存储服务器的状态信息，是客户端和数据服务器交互的枢纽。相比GFS中的master更为精简，不记录文件索引信息，占用的内存量很少。&lt;/p&gt;

&lt;p&gt;Tracker是FastDFS的协调者，负责管理所有的storage server和group，每个storage在启动后会连接Tracker，告知自己所属的group等信息，并保持周期性的心跳，tracker根据storage的心跳信息，建立group==&amp;gt;[storage server list]的映射表。&lt;/p&gt;

&lt;p&gt;Tracker需要管理的元信息很少，会全部存储在内存中；另外tracker上的元信息都是由storage汇报的信息生成的，本身不需要持久化任何数据，这样使得tracker非常容易扩展，直接增加tracker机器即可扩展为tracker cluster来服务，cluster里每个tracker之间是完全对等的，所有的tracker都接受stroage的心跳信息，生成元数据信息来提供读写服务。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;storage server：存储服务器（又称：存储节点或数据服务器）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文件和文件属性（meta data）都保存到存储服务器上。Storage server直接利用OS的文件系统调用管理文件。&lt;/p&gt;

&lt;p&gt;Storage server（后简称storage）以组（卷，group或volume）为单位组织，一个group内包含多台storage机器，数据互为备份，存储空间以group内容量最小的storage为准，所以建议group内的多个storage尽量配置相同，以免造成存储空间的浪费。&lt;/p&gt;

&lt;p&gt;以group为单位组织存储能方便的进行应用隔离、负载均衡、副本数定制（group内storage server数量即为该group的副本数），比如将不同应用数据存到不同的group就能隔离应用数据，同时还可根据应用的访问特性来将应用分配到不同的group来做负载均衡；缺点是group的容量受单机存储容量的限制，同时当group内有机器坏掉时，数据恢复只能依赖group内地其他机器，使得恢复时间会很长。&lt;/p&gt;

&lt;p&gt;group内每个storage的存储依赖于本地文件系统，storage可配置多个数据存储目录，比如有10块磁盘，分别挂载在/data/disk1-/data/disk10，则可将这10个目录都配置为storage的数据存储目录。&lt;/p&gt;

&lt;p&gt;storage接受到写文件请求时，会根据配置好的规则（后面会介绍），选择其中一个存储目录来存储文件。为了避免单个目录下的文件数太多，在storage第一次启动时，会在每个数据存储目录里创建2级子目录，每级256个，总共65536个文件，新写的文件会以hash的方式被路由到其中某个子目录下，然后将文件数据直接作为一个本地文件存储到该目录中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;client：客户端&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;作为业务请求的发起方，通过专有接口，使用TCP/IP协议与跟踪器服务器或存储节点进行数据交互。FastDFS向使用者提供基本文件访问接口，比如upload、download、append、delete等，以客户端库的方式提供给用户使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;group&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;组， 也可称为卷。 同组内服务器上的文件是完全相同的 ，同一组内的storage server之间是对等的， 文件上传、 删除等操作可以在任意一台storage server上进行 。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;meta data&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文件相关属性，键值对（ Key Value Pair） 方式，如：width=1024,heigth=768 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/fastdfs&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;Tracker相当于FastDFS的大脑，不论是上传还是下载都是通过tracker来分配资源；客户端一般可以使用ngnix等静态服务器来调用或者做一部分的缓存；存储服务器内部分为卷（或者叫做组），卷于卷之间是平行的关系，可以根据资源的使用情况随时增加，卷内服务器文件相互同步备份，以达到容灾的目的。&lt;/p&gt;

&lt;h2 id=&#34;上传机制&#34;&gt;上传机制&lt;/h2&gt;

&lt;p&gt;首先客户端请求Tracker服务获取到存储服务器的ip地址和端口，然后客户端根据返回的IP地址和端口号请求上传文件，存储服务器接收到请求后生产文件，并且将文件内容写入磁盘并返回给客户端file_id、路径信息、文件名等信息，客户端保存相关信息上传完毕。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/upload&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;内部机制如下：&lt;/p&gt;

&lt;p&gt;1、选择tracker server&lt;/p&gt;

&lt;p&gt;当集群中不止一个tracker server时，由于tracker之间是完全对等的关系，客户端在upload文件时可以任意选择一个trakcer。 选择存储的group 当tracker接收到upload file的请求时，会为该文件分配一个可以存储该文件的group，支持如下选择group的规则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、Round robin，所有的group间轮询&lt;/li&gt;
&lt;li&gt;2、Specified group，指定某一个确定的group&lt;/li&gt;
&lt;li&gt;3、Load balance，剩余存储空间多多group优先&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、选择storage server&lt;/p&gt;

&lt;p&gt;当选定group后，tracker会在group内选择一个storage server给客户端，支持如下选择storage的规则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、Round robin，在group内的所有storage间轮询&lt;/li&gt;
&lt;li&gt;2、First server ordered by ip，按ip排序&lt;/li&gt;
&lt;li&gt;3、First server ordered by priority，按优先级排序（优先级在storage上配置）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、选择storage path&lt;/p&gt;

&lt;p&gt;当分配好storage server后，客户端将向storage发送写文件请求，storage将会为文件分配一个数据存储目录，支持如下规则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、Round robin，多个存储目录间轮询&lt;/li&gt;
&lt;li&gt;2、剩余存储空间最多的优先&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、生成Fileid&lt;/p&gt;

&lt;p&gt;选定存储目录之后，storage会为文件生一个Fileid，由storage server ip、文件创建时间、文件大小、文件crc32和一个随机数拼接而成，然后将这个二进制串进行base64编码，转换为可打印的字符串。 选择两级目录 当选定存储目录之后，storage会为文件分配一个fileid，每个存储目录下有两级256*256的子目录，storage会按文件fileid进行两次hash（猜测），路由到其中一个子目录，然后将文件以fileid为文件名存储到该子目录下。&lt;/p&gt;

&lt;p&gt;5、生成文件名&lt;/p&gt;

&lt;p&gt;当文件存储到某个子目录后，即认为该文件存储成功，接下来会为该文件生成一个文件名，文件名由group、存储目录、两级子目录、fileid、文件后缀名（由客户端指定，主要用于区分文件类型）拼接而成。&lt;/p&gt;

&lt;h2 id=&#34;下载机制&#34;&gt;下载机制&lt;/h2&gt;

&lt;p&gt;客户端带上文件名信息请求Tracker服务获取到存储服务器的ip地址和端口，然后客户端根据返回的IP地址和端口号请求下载文件，存储服务器接收到请求后返回文件给客户端。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/download&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;跟upload file一样，在download file时客户端可以选择任意tracker server。tracker发送download请求给某个tracker，必须带上文件名信息，tracke从文件名中解析出文件的group、大小、创建时间等信息，然后为该请求选择一个storage用来服务读请求。由于group内的文件同步时在后台异步进行的，所以有可能出现在读到时候，文件还没有同步到某些storage server上，为了尽量避免访问到这样的storage，tracker按照如下规则选择group内可读的storage。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、该文件上传到的源头storage - 源头storage只要存活着，肯定包含这个文件，源头的地址被编码在文件名中。&lt;/li&gt;
&lt;li&gt;2、文件创建时间戳==storage被同步到的时间戳 且(当前时间-文件创建时间戳) &amp;gt; 文件同步最大时间（如5分钟) - 文件创建后，认为经过最大同步时间后，肯定已经同步到其他storage了。&lt;/li&gt;
&lt;li&gt;3、文件创建时间戳 &amp;lt; storage被同步到的时间戳。 - 同步时间戳之前的文件确定已经同步了&lt;/li&gt;
&lt;li&gt;4、(当前时间-文件创建时间戳) &amp;gt; 同步延迟阀值（如一天）。 - 经过同步延迟阈值时间，认为文件肯定已经同步了。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Bufio</title>
          <link>https://kingjcy.github.io/post/golang/go-bufio/</link>
          <pubDate>Tue, 25 Dec 2018 14:27:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-bufio/</guid>
          <description>&lt;p&gt;bufio 包实现了缓存IO。它包装了 io.Reader 和 io.Writer 对象，创建了另外的Reader和Writer对象，它们也实现了 io.Reader 和 io.Writer 接口，不过它们是有缓存的。该包同时为文本I/O提供了一些便利操作。&lt;/p&gt;

&lt;h1 id=&#34;类型和方法&#34;&gt;类型和方法&lt;/h1&gt;

&lt;h2 id=&#34;reader-类型和方法&#34;&gt;Reader 类型和方法&lt;/h2&gt;

&lt;h3 id=&#34;类型&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;bufio.Reader 结构包装了一个 io.Reader 对象，提供缓存功能，同时实现了 io.Reader 接口。&lt;/p&gt;

&lt;p&gt;Reader 结构没有任何导出的字段，结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader struct {
    buf          []byte        // 缓存
    rd           io.Reader    // 底层的io.Reader
    // r:从buf中读走的字节（偏移）；w:buf中填充内容的偏移；
    // w - r 是buf中可被读的长度（缓存数据的大小），也是Buffered()方法的返回值
    r, w         int
    err          error        // 读过程中遇到的错误
    lastByte     int        // 最后一次读到的字节（ReadByte/UnreadByte)
    lastRuneSize int        // 最后一次读到的Rune的大小 (ReadRune/UnreadRune)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;bufio 包提供了两个实例化 bufio.Reader 对象的函数：NewReader 和 NewReaderSize。其中，NewReader 函数是调用 NewReaderSize 函数实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(rd io.Reader) *Reader {
    // 默认缓存大小：defaultBufSize=4096
    return NewReaderSize(rd, defaultBufSize)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下NewReaderSize的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReaderSize(rd io.Reader, size int) *Reader {
    // 已经是bufio.Reader类型，且缓存大小不小于 size，则直接返回
    b, ok := rd.(*Reader)
    if ok &amp;amp;&amp;amp; len(b.buf) &amp;gt;= size {
        return b
    }
    // 缓存大小不会小于 minReadBufferSize （16字节）
    if size &amp;lt; minReadBufferSize {
        size = minReadBufferSize
    }
    // 构造一个bufio.Reader实例
    return &amp;amp;Reader{
        buf:          make([]byte, size),
        rd:           rd,
        lastByte:     -1,
        lastRuneSize: -1,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见需要一个id.reader的实例来进行初始化，一般我们都是使用string或者[]byte的reader类型来创建。&lt;/p&gt;

&lt;h3 id=&#34;方法&#34;&gt;方法&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadSlice、ReadBytes、ReadString 和 ReadLine 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之所以将这几个方法放在一起，是因为他们有着类似的行为。事实上，后三个方法最终都是调用ReadSlice来实现的。所以，我们先来看看ReadSlice方法(感觉这一段直接看源码较好)。&lt;/p&gt;

&lt;p&gt;1.ReadSlice方法签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadSlice(delim byte) (line []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ReadSlice 从输入中读取，直到遇到第一个界定符（delim）为止，返回一个指向缓存中字节的 slice，在下次调用读操作（read）时，这些字节会无效。举例说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := bufio.NewReader(strings.NewReader(&amp;quot;http://studygolang.com. \nIt is the home of gophers&amp;quot;))
line, _ := reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
// 这里可以换上任意的 bufio 的 Read/Write 操作
n, _ := reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
fmt.Println(string(n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;the line:http://studygolang.com.

the line:It is the home of gophers
It is the home of gophers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从结果可以看出，第一次ReadSlice的结果（line），在第二次调用读操作后，内容发生了变化。也就是说，ReadSlice 返回的 []byte 是指向 Reader 中的 buffer ，而不是 copy 一份返回。正因为ReadSlice 返回的数据会被下次的 I/O 操作重写，因此许多的客户端会选择使用 ReadBytes 或者 ReadString 来代替。读者可以将上面代码中的 ReadSlice 改为 ReadBytes 或 ReadString ，看看结果有什么不同。&lt;/p&gt;

&lt;p&gt;注意，这里的界定符可以是任意的字符，可以将上面代码中的&amp;rsquo;\n&amp;rsquo;改为&amp;rsquo;m&amp;rsquo;试试。同时，返回的结果是包含界定符本身的，上例中，输出结果有一空行就是&amp;rsquo;\n&amp;rsquo;本身(line携带一个&amp;rsquo;\n&amp;rsquo;,printf又追加了一个&amp;rsquo;\n&amp;rsquo;)。&lt;/p&gt;

&lt;p&gt;如果 ReadSlice 在找到界定符之前遇到了 error ，它就会返回缓存中所有的数据和错误本身（经常是 io.EOF）。如果在找到界定符之前缓存已经满了，ReadSlice 会返回 bufio.ErrBufferFull 错误。当且仅当返回的结果（line）没有以界定符结束的时候，ReadSlice 返回err != nil，也就是说，如果ReadSlice 返回的结果 line 不是以界定符 delim 结尾，那么返回的 er r也一定不等于 nil（可能是bufio.ErrBufferFull或io.EOF）。 例子代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := bufio.NewReaderSize(strings.NewReader(&amp;quot;http://studygolang.com&amp;quot;),16)
line, err := reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;line:%s\terror:%s\n&amp;quot;, line, err)
line, err = reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;line:%s\terror:%s\n&amp;quot;, line, err)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line:http://studygola    error:bufio: buffer full
line:ng.com    error:EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.ReadBytes方法签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadBytes(delim byte) (line []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方法的参数和返回值类型与 ReadSlice 都一样。 ReadBytes 从输入中读取直到遇到界定符（delim）为止，返回的 slice 包含了从当前到界定符的内容 （包括界定符）。如果 ReadBytes 在遇到界定符之前就捕获到一个错误，它会返回遇到错误之前已经读取的数据，和这个捕获到的错误（经常是 io.EOF）。跟 ReadSlice 一样，如果 ReadBytes 返回的结果 line 不是以界定符 delim 结尾，那么返回的 err 也一定不等于 nil（可能是bufio.ErrBufferFull 或 io.EOF）。&lt;/p&gt;

&lt;p&gt;从这个说明可以看出，ReadBytes和ReadSlice功能和用法都很像，那他们有什么不同呢？&lt;/p&gt;

&lt;p&gt;在讲解ReadSlice时说到，它返回的 []byte 是指向 Reader 中的 buffer，而不是 copy 一份返回，也正因为如此，通常我们会使用 ReadBytes 或 ReadString。很显然，ReadBytes 返回的 []byte 不会是指向 Reader 中的 buffer，通过查看源码可以证实这一点。&lt;/p&gt;

&lt;p&gt;还是上面的例子，我们将 ReadSlice 改为 ReadBytes：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := bufio.NewReader(strings.NewReader(&amp;quot;http://studygolang.com. \nIt is the home of gophers&amp;quot;))
line, _ := reader.ReadBytes(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
// 这里可以换上任意的 bufio 的 Read/Write 操作
n, _ := reader.ReadBytes(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
fmt.Println(string(n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;the line:http://studygolang.com.

the line:http://studygolang.com.

It is the home of gophers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.ReadString方法&lt;/p&gt;

&lt;p&gt;看一下该方法的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadString(delim byte) (line string, err error) {
    bytes, err := b.ReadBytes(delim)
    return string(bytes), err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它调用了 ReadBytes 方法，并将结果的 []byte 转为 string 类型。&lt;/p&gt;

&lt;p&gt;4.ReadLine方法签名如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadLine() (line []byte, isPrefix bool, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ReadLine 是一个底层的原始行读取命令。许多调用者或许会使用 ReadBytes(&amp;rsquo;\n&amp;rsquo;) 或者 ReadString(&amp;rsquo;\n&amp;rsquo;) 来代替这个方法。&lt;/p&gt;

&lt;p&gt;ReadLine 尝试返回单独的行，不包括行尾的换行符。如果一行大于缓存，isPrefix 会被设置为 true，同时返回该行的开始部分（等于缓存大小的部分）。该行剩余的部分就会在下次调用的时候返回。当下次调用返回该行剩余部分时，isPrefix 将会是 false 。跟 ReadSlice 一样，返回的 line 只是 buffer 的引用，在下次执行IO操作时，line 会无效。可以将 ReadSlice 中的例子该为 ReadLine 试试。&lt;/p&gt;

&lt;p&gt;注意，返回值中，要么 line 不是 nil，要么 err 非 nil，两者不会同时非 nil。ReadLine 返回的文本不会包含行结尾（&amp;rdquo;\r\n&amp;rdquo;或者&amp;rdquo;\n&amp;rdquo;）。如果输入中没有行尾标识符，不会返回任何指示或者错误。&lt;/p&gt;

&lt;p&gt;个人建议可以这么实现读取一行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line, err := reader.ReadBytes(&#39;\n&#39;)
line = bytes.TrimRight(line, &amp;quot;\r\n&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样既读取了一行，也去掉了行尾结束符（当然，如果你希望留下行尾结束符，只用ReadBytes即可）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Peek 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从方法的名称可以猜到，该方法只是“窥探”一下 Reader 中没有读取的 n 个字节。好比栈数据结构中的取栈顶元素，但不出栈。&lt;/p&gt;

&lt;p&gt;方法的签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) Peek(n int) ([]byte, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同上面介绍的 ReadSlice一样，返回的 []byte 只是 buffer 中的引用，在下次IO操作后会无效，可见该方法（以及ReadSlice这样的，返回buffer引用的方法）对多 goroutine 是不安全的，也就是在多并发环境下，不能依赖其结果。&lt;/p&gt;

&lt;p&gt;我们通过例子来证明一下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;bufio&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;strings&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    reader := bufio.NewReaderSize(strings.NewReader(&amp;quot;http://studygolang.com.\t It is the home of gophers&amp;quot;), 14)
    go Peek(reader)
    go reader.ReadBytes(&#39;\t&#39;)
    time.Sleep(1e8)
}

func Peek(reader *bufio.Reader) {
    line, _ := reader.Peek(14)
    fmt.Printf(&amp;quot;%s\n&amp;quot;, line)
    // time.Sleep(1)
    fmt.Printf(&amp;quot;%s\n&amp;quot;, line)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygo
http://studygo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果和预期的一致。然而，这是由于目前的 goroutine 调度方式导致的结果。如果我们将例子中注释掉的 time.Sleep(1) 取消注释（这样调度其他 goroutine 执行），再次运行，得到的结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygo
ng.com.     It is
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，Reader 的 Peek 方法如果返回的 []byte 长度小于 n，这时返回的 err != nil ，用于解释为啥会小于 n。如果 n 大于 reader 的 buffer 长度，err 会是 ErrBufferFull。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Reader 的其他方法都是实现了 io 包中的接口，它们的使用方法在io包中都有介绍，在此不赘述。&lt;/p&gt;

&lt;p&gt;这些方法包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) Read(p []byte) (n int, err error)
func (b *Reader) ReadByte() (c byte, err error)
func (b *Reader) ReadRune() (r rune, size int, err error)
func (b *Reader) UnreadByte() error
func (b *Reader) UnreadRune() error
func (b *Reader) WriteTo(w io.Writer) (n int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你应该知道它们都是哪个接口的方法吧。&lt;/p&gt;

&lt;h2 id=&#34;scanner-类型和方法&#34;&gt;Scanner 类型和方法&lt;/h2&gt;

&lt;h3 id=&#34;类型-1&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;对于简单的读取一行，在 Reader 类型中，感觉没有让人特别满意的方法。于是，Go1.1增加了一个类型：Scanner。官方关于Go1.1增加该类型的说明如下：&lt;/p&gt;

&lt;p&gt;在 bufio 包中有多种方式获取文本输入，ReadBytes、ReadString 和独特的 ReadLine，对于简单的目的这些都有些过于复杂了。在 Go 1.1 中，添加了一个新类型，Scanner，以便更容易的处理如按行读取输入序列或空格分隔单词等，这类简单的任务。它终结了如输入一个很长的有问题的行这样的输入错误，并且提供了简单的默认行为：基于行的输入，每行都剔除分隔标识。这里的代码展示一次输入一行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scanner := bufio.NewScanner(os.Stdin)
for scanner.Scan() {
    fmt.Println(scanner.Text()) // Println will add back the final &#39;\n&#39;
}
if err := scanner.Err(); err != nil {
    fmt.Fprintln(os.Stderr, &amp;quot;reading standard input:&amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输入的行为可以通过一个函数控制，来控制输入的每个部分（参阅 SplitFunc 的文档），但是对于复杂的问题或持续传递错误的，可能还是需要原有接口。&lt;/p&gt;

&lt;p&gt;Scanner 类型和 Reader 类型一样，没有任何导出的字段，同时它也包装了一个 io.Reader 对象，但它没有实现 io.Reader 接口。&lt;/p&gt;

&lt;p&gt;Scanner 的结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Scanner struct {
    r            io.Reader // The reader provided by the client.
    split        SplitFunc // The function to split the tokens.
    maxTokenSize int       // Maximum size of a token; modified by tests.
    token        []byte    // Last token returned by split.
    buf          []byte    // Buffer used as argument to split.
    start        int       // First non-processed byte in buf.
    end          int       // End of data in buf.
    err          error     // Sticky error.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里 split、maxTokenSize 和 token 需要讲解一下。&lt;/p&gt;

&lt;h4 id=&#34;split&#34;&gt;split&lt;/h4&gt;

&lt;p&gt;split对应的类型是SplitFunc ，我们需要了解一下SplitFunc类型，定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SplitFunc func(data []byte, atEOF bool) (advance int, token []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SplitFunc 定义了 用于对输入进行分词的 split 函数的签名。参数 data 是还未处理的数据，atEOF 标识 Reader 是否还有更多数据（是否到了EOF）。返回值 advance 表示从输入中读取的字节数，token 表示下一个结果数据，err 则代表可能的错误。&lt;/p&gt;

&lt;p&gt;举例说明一下这里的 token 代表的意思：&lt;/p&gt;

&lt;p&gt;有数据 &amp;ldquo;studygolang\tpolaris\tgolangchina&amp;rdquo;，通过&amp;rdquo;\t&amp;rdquo;进行分词，那么会得到三个token，它们的内容分别是：studygolang、polaris 和 golangchina。而 SplitFunc 的功能是：进行分词，并返回未处理的数据中第一个 token。对于这个数据，就是返回 studygolang。
如果 data 中没有一个完整的 token，例如，在扫描行（scanning lines）时没有换行符，SplitFunc 会返回(0,nil,nil)通知 Scanner 读取更多数据到 slice 中。&lt;/p&gt;

&lt;p&gt;如果 err != nil，扫描停止，同时该错误会返回。&lt;/p&gt;

&lt;p&gt;如果参数 data 为空的 slice，除非 atEOF 为 true，否则该函数永远不会被调用。如果 atEOF 为 true，这时 data 可以非空，这时的数据是没有处理的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;SplitFunc 的实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 bufio 包中预定义了一些 split 函数，也就是说，在 Scanner 结构中的 split 字段，可以通过这些预定义的 split 赋值，同时 Scanner 类型的 Split 方法也可以接收这些预定义函数作为参数。所以，我们可以说，这些预定义 split 函数都是 SplitFunc 类型的实例。这些函数包括：ScanBytes、ScanRunes、ScanWords 和 ScanLines。（由于都是 SplitFunc 的实例，自然这些函数的签名都和 SplitFunc 一样）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ScanBytes 返回单个字节作为一个 token。&lt;/li&gt;
&lt;li&gt;ScanRunes 返回单个 UTF-8 编码的 rune 作为一个 token。返回的 rune 序列（token）和 range string类型 返回的序列是等价的，也就是说，对于无效的 UTF-8 编码会解释为 U+FFFD = &amp;ldquo;\xef\xbf\xbd&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;ScanWords 返回通过“空格”分词的单词。如：study golang，调用会返回study。注意，这里的“空格”是 unicode.IsSpace()，即包括：&amp;rsquo;\t&amp;rsquo;, &amp;lsquo;\n&amp;rsquo;, &amp;lsquo;\v&amp;rsquo;, &amp;lsquo;\f&amp;rsquo;, &amp;lsquo;\r&amp;rsquo;, &amp;lsquo; &amp;lsquo;, U+0085 (NEL), U+00A0 (NBSP)。&lt;/li&gt;
&lt;li&gt;ScanLines 返回一行文本，不包括行尾的换行符。这里的换行包括了Windows下的&amp;rdquo;\r\n&amp;rdquo;和Unix下的&amp;rdquo;\n&amp;rdquo;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般地，我们不会单独使用这些函数，而是提供给 Scanner 实例使用。&lt;/p&gt;

&lt;h4 id=&#34;maxtokensize&#34;&gt;maxTokenSize&lt;/h4&gt;

&lt;p&gt;maxTokenSize 字段 表示通过 split 分词后的一个 token 允许的最大长度。在该包中定义了一个常量 MaxScanTokenSize = 64 * 1024，这是允许的最大 token 长度（64k）。&lt;/p&gt;

&lt;h3 id=&#34;方法-1&#34;&gt;方法&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Scanner 没有导出任何字段，而它需要有外部的 io.Reader 对象，因此，我们不能直接实例化 Scanner 对象，必须通过 bufio 包提供的实例化函数来实例化。实例化函数签名以及内部实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewScanner(r io.Reader) *Scanner {
    return &amp;amp;Scanner{
        r:            r,
        split:        ScanLines,
        maxTokenSize: MaxScanTokenSize,
        buf:          make([]byte, 4096), // Plausible starting size; needn&#39;t be large.
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，返回的 Scanner 实例默认的 split 函数是 ScanLines。这边实例化也是需要一个id.reader的实例，所以我们一般也是使用string或者[]byte的实例来做创建参数使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Split 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;前面我们提到过可以通过 Split 方法为 Scanner 实例设置分词行为。由于 Scanner 实例的默认 split 总是 ScanLines，如果我们想要用其他的 split，可以通过 Split 方法做到。&lt;/p&gt;

&lt;p&gt;比如，我们想要统计一段英文有多少个单词（不排除重复），我们可以这么做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const input = &amp;quot;This is The Golang Standard Library.\nWelcome you!&amp;quot;
scanner := bufio.NewScanner(strings.NewReader(input))
scanner.Split(bufio.ScanWords)
count := 0
for scanner.Scan() {
    count++
}
if err := scanner.Err(); err != nil {
    fmt.Fprintln(os.Stderr, &amp;quot;reading input:&amp;quot;, err)
}
fmt.Println(count)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们实例化 Scanner 后，通过调用 scanner.Split(bufio.ScanWords) 来更改 split 函数。注意，我们应该在调用 Scan 方法之前调用 Split 方法。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Scan 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;该方法好比 iterator 中的 Next 方法，它用于将 Scanner 获取下一个 token，以便 Bytes 和 Text 方法可用。当扫描停止时，它返回false，这时候，要么是到了输入的末尾要么是遇到了一个错误。注意，当 Scan 返回 false 时，通过 Err 方法可以获取第一个遇到的错误（但如果错误是 io.EOF，Err 方法会返回 nil）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Bytes 和 Text 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这两个方法的行为一致，都是返回最近的 token，无非 Bytes 返回的是 []byte，Text 返回的是 string。该方法应该在 Scan 调用后调用，而且，下次调用 Scan 会覆盖这次的 token。比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scanner := bufio.NewScanner(strings.NewReader(&amp;quot;http://studygolang.com. \nIt is the home of gophers&amp;quot;))
if scanner.Scan() {
    scanner.Scan()
    fmt.Printf(&amp;quot;%s&amp;quot;, scanner.Text())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回的是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;It is the home of gophers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而不是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygolang.com.
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Err 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;前面已经提到，通过 Err 方法可以获取第一个遇到的错误（但如果错误是 io.EOF，Err 方法会返回 nil）。&lt;/p&gt;

&lt;h3 id=&#34;完整实例&#34;&gt;完整实例&lt;/h3&gt;

&lt;p&gt;我们经常会有这样的需求：读取文件中的数据，一次读取一行。在学习了 Reader 类型，我们可以使用它的 ReadBytes 或 ReadString来实现，甚至使用 ReadLine 来实现。然而，在 Go1.1 中，我们可以使用 Scanner 来做这件事，而且更简单好用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file, err := os.Create(&amp;quot;scanner.txt&amp;quot;)
if err != nil {
    panic(err)
}
defer file.Close()
file.WriteString(&amp;quot;http://studygolang.com.\nIt is the home of gophers.\nIf you are studying golang, welcome you!&amp;quot;)
// 将文件 offset 设置到文件开头
file.Seek(0, os.SEEK_SET)
scanner := bufio.NewScanner(file)
for scanner.Scan() {
    fmt.Println(scanner.Text())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygolang.com.
It is the home of gophers.
If you are studying golang, welcome you!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;writer-类型和方法&#34;&gt;Writer 类型和方法&lt;/h2&gt;

&lt;h3 id=&#34;类型-2&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;bufio.Writer 结构包装了一个 io.Writer 对象，提供缓存功能，同时实现了 io.Writer 接口。&lt;/p&gt;

&lt;p&gt;Writer 结构没有任何导出的字段，结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Writer struct {
    err error        // 写过程中遇到的错误
    buf []byte        // 缓存
    n   int            // 当前缓存中的字节数
    wr  io.Writer    // 底层的 io.Writer 对象
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相比 bufio.Reader, bufio.Writer 结构定义简单很多。&lt;/p&gt;

&lt;p&gt;注意：如果在写数据到 Writer 的时候出现了一个错误，不会再允许有数据被写进来了，并且所有随后的写操作都会返回该错误。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;和 Reader 类型一样，bufio 包提供了两个实例化 bufio.Writer 对象的函数：NewWriter 和 NewWriterSize。其中，NewWriter 函数是调用 NewWriterSize 函数实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewWriter(wr io.Writer) *Writer {
    // 默认缓存大小：defaultBufSize=4096
    return NewWriterSize(wr, defaultBufSize)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下 NewWriterSize 的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewWriterSize(wr io.Writer, size int) *Writer {
    // 已经是 bufio.Writer 类型，且缓存大小不小于 size，则直接返回
    b, ok := wr.(*Writer)
    if ok &amp;amp;&amp;amp; len(b.buf) &amp;gt;= size {
        return b
    }
    if size &amp;lt;= 0 {
        size = defaultBufSize
    }
    return &amp;amp;Writer{
        buf: make([]byte, size),
        wr:  w,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;方法-2&#34;&gt;方法&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Available 和 Buffered 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Available 方法获取缓存中还未使用的字节数（缓存大小 - 字段 n 的值）；&lt;/p&gt;

&lt;p&gt;Buffered 方法获取写入当前缓存中的字节数（字段 n 的值）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Flush 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;该方法将缓存中的所有数据写入底层的 io.Writer 对象中。使用 bufio.Writer 时，在所有的 Write 操作完成之后，应该调用 Flush 方法使得缓存都写入 io.Writer 对象中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Writer 类型其他方法是一些实际的写方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 实现了 io.ReaderFrom 接口
func (b *Writer) ReadFrom(r io.Reader) (n int64, err error)

// 实现了 io.Writer 接口
func (b *Writer) Write(p []byte) (nn int, err error)

// 实现了 io.ByteWriter 接口
func (b *Writer) WriteByte(c byte) error

// io 中没有该方法的接口，它用于写入单个 Unicode 码点，返回写入的字节数（码点占用的字节），内部实现会根据当前 rune 的范围调用 WriteByte 或 WriteString
func (b *Writer) WriteRune(r rune) (size int, err error)

// 写入字符串，如果返回写入的字节数比 len(s) 小，返回的error会解释原因
func (b *Writer) WriteString(s string) (int, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些写方法在缓存满了时会调用 Flush 方法。另外，这些写方法源码开始处，有这样的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if b.err != nil {
    return b.err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说，只要写的过程中遇到了错误，再次调用写操作会直接返回该错误。&lt;/p&gt;

&lt;h2 id=&#34;readwriter-类型和实例化&#34;&gt;ReadWriter 类型和实例化&lt;/h2&gt;

&lt;h3 id=&#34;类型-3&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;ReadWriter 结构存储了 bufio.Reader 和 bufio.Writer 类型的指针（内嵌），它实现了 io.ReadWriter 结构。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ReadWriter struct {
    *Reader
    *Writer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;实例&#34;&gt;实例&lt;/h3&gt;

&lt;p&gt;ReadWriter 的实例化可以跟普通结构类型一样，也可以通过调用 bufio.NewReadWriter 函数来实现：只是简单的实例化 ReadWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReadWriter(r *Reader, w *Writer) *ReadWriter {
    return &amp;amp;ReadWriter{r, w}
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Syscall</title>
          <link>https://kingjcy.github.io/post/golang/go-syscall/</link>
          <pubDate>Tue, 25 Dec 2018 14:04:07 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-syscall/</guid>
          <description>&lt;p&gt;标准库syscall给这些系统调用做了不错的封装，不少常用的系统调用已经可以像普通函数一样直接调用，但是大部分使用起来都比较陌生。&lt;/p&gt;

&lt;h1 id=&#34;syscall&#34;&gt;Syscall&lt;/h1&gt;

&lt;p&gt;Golang 的 syscall 库已经对常用系统调用进行了封装，我们只需要调用相应的函数，并传入相应的参数就可以等着执行完成，给我们返回需要的结果了。&lt;/p&gt;

&lt;p&gt;man 这个我们平日里经常用到的命令，除了提供各种命令的使用帮助，还提供了不少系统层面的资料，其中就有我们所需要的各个系统调用的具体描述。通过比对 man 里的资料与封装函数的外观，我们可以得到具体系统调用的对应实践方式。&lt;/p&gt;

&lt;p&gt;我们来做个尝试，在实现过程中来体会具体的实践方式。这里，我们选择使用 mmap 来实现数据的持久化存储作为示例。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;mmap&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;mmap 就是将文件映射进内存的基本原理，以及相比传统的文件读写方式的优劣势。&lt;/p&gt;

&lt;p&gt;然后，查看标准库对 mmap 这个系统调用的封装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Mmap(fd int, offset int64, length int, prot int, flags int) (data []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着，我们查看 man 对 mmap 的介绍 ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void *mmap(void *addr, size_t length, int prot, int flags,
                  int fd, off_t offset);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这下，两边就能够对应上了，让我们来了解一下各个参数的具体定义：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;fd：映射进内存的文件描述符&lt;/li&gt;
&lt;li&gt;offset：映射进内存的文件段的起始位置，在文件中的偏移量&lt;/li&gt;
&lt;li&gt;length：映射进内存的文件段的长度，必须是正整数&lt;/li&gt;
&lt;li&gt;prot：protection 的缩写，用来做权限控制，golang 标准库已有预定义的值&lt;/li&gt;
&lt;li&gt;flags：对 mmap 的一些行为进行控制，golang 标准库已有预定义的值&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;同样，返回值也可以对应得上，不过，在形式上进行了一些转变，需要进行理解和翻译：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;data：对应 *addr，返回映射进内存的文件段对应的数组，持久化数据就是使用这个数组&lt;/li&gt;
&lt;li&gt;err：对应这个函数的返回值 void ，返回值的含义，在 golang 已有对应的定义&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来试一下，根据这个文档写出实现代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    f, err := os.OpenFile(&amp;quot;mmap.bin&amp;quot;, os.O_RDWR|os.O_CREATE, 0644)
    if nil != err {
        log.Fatalln(err)
    }
    // extend file
    if _, err := f.WriteAt([]byte{byte(0)}, 1&amp;lt;&amp;lt;8); nil != err {
        log.Fatalln(err)
    }
    data, err := syscall.Mmap(int(f.Fd()), 0, 1&amp;lt;&amp;lt;8, syscall.PROT_WRITE, syscall.MAP_SHARED)
    if nil != err {
        log.Fatalln(err)
    }
    if err := f.Close(); nil != err {
        log.Fatalln(err)
    }
    for i, v := range []byte(&amp;quot;hello syscall&amp;quot;) {
        data[i] = v
    }
    if err := syscall.Munmap(data); nil != err {
        log.Fatalln(err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译并执行这段代码，会在当前目录生成 mmap.bin 文件，执行 hexdump -C mmap.bin 可以看到，文件里面已有我们写入的内容。&lt;/p&gt;

&lt;p&gt;其他的系统调用都可以参照这个方法来理解。&lt;/p&gt;

&lt;h1 id=&#34;封装方式&#34;&gt;封装方式&lt;/h1&gt;

&lt;p&gt;标准库syscall提供了 4 个通用的封装方式，供我们执行任意的系统调用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Syscall(trap, a1, a2, a3 uintptr) (r1, r2 uintptr, err Errno)
Syscall6(trap, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2 uintptr, err Errno)
RawSyscall(trap, a1, a2, a3 uintptr) (r1, r2 uintptr, err Errno)
RawSyscall6(trap, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2 uintptr, err Errno)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从外观观察，可以知道它们可以按支持的参数个数分成两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;供 4 个及 4 个以下参数的系统调用使用的 Syscall、RawSyscall&lt;/li&gt;
&lt;li&gt;供 6 个及 6 个以下参数的系统调用使用的 Syscall6、RawSyscall6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而从对我们来说更有意义的实现、功用的角度看，可以分为 Syscall、RawSyscall 两类。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus snmp Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/snmp_exporter/</link>
          <pubDate>Fri, 09 Nov 2018 14:29:56 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/snmp_exporter/</guid>
          <description>&lt;p&gt;Prometheus exporter for snmp server metrics.&lt;/p&gt;

&lt;h1 id=&#34;源码原理解析&#34;&gt;源码原理解析&lt;/h1&gt;

&lt;p&gt;1、解析配置文件到结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Config map[string]*Module
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、metrics中可以指定snmp对应的版本&lt;/p&gt;

&lt;p&gt;3、支持配置重载&lt;/p&gt;

&lt;p&gt;4、重置url：snmp的处理函数handler&lt;/p&gt;

&lt;p&gt;5、获取两个必要调用参数：target和module（默认是if_mib）&lt;/p&gt;

&lt;p&gt;6、生成结构体collector重写describe和collect&lt;/p&gt;

&lt;p&gt;7、colletcd中调用github.com/soniah/gosnmp这个snmp clinet lib来获取数据&lt;/p&gt;

&lt;p&gt;8、解析数据放入到对应配置文件中的指标进行赋值输出&lt;/p&gt;

&lt;p&gt;9、这边就是直接使用了snmp协议，所以还是要了解mib对应的东西，包括格式与语法&lt;/p&gt;

&lt;p&gt;下面就是看我们默认的if_mib采集是哪些数据，是否符合要求，是否要换去其他模版&lt;/p&gt;

&lt;p&gt;針對普通網絡設備的端口，MIB的相關定義是Interface組，主要管理如下信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ifIndex 端口索引號
ifDescr 端口描述
ifType 端口類型
ifMtu 最大傳輸包字節數
ifSpeed 端口速度
ifPhysAddress 物理地址
ifOperStatus 操作狀態
ifLastChange 上次狀態更新時間
ifInOctets 輸入字節數
*ifInUcastPkts 輸入非廣播包數
*ifInNUcastPkts 輸入廣播包數
*ifInDiscards 輸入包丟棄數
*ifInErrors 輸入包錯誤數
*ifInUnknownProtos 輸入未知協議包數
*ifOutOctets 輸出字節數
*ifOutUcastPkts 輸出非廣播包數
*ifOutNUcastPkts 輸出廣播包數
*ifOutDiscards 輸出包丟棄數
*ifOutErrors 輸出包錯誤數
ifOutQLen 輸出隊長 其中， 號標識的是與網絡流量有關的信息。
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;獲取CISCO2900端口1的上行總流量 snmpwalk -v 1 -c public 192.168.1.254 IF-MIB::ifInOctets.1 返回結果 IF-MIB::ifInOctets.1 = Counter32: 4861881&lt;/li&gt;
&lt;li&gt;五秒後再獲取一次 snmpwalk -v 1 -c public 192.168.1.254 IF-MIB::ifInOctets.1 返回結果 IF-MIB::ifInOctets.1 = Counter32: 4870486 3、計算結果 （後值48704863-前值4861881）/ 5＝1721b/s （應該是BYTE）&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;generator原理&#34;&gt;generator原理&lt;/h1&gt;

&lt;p&gt;将generator.yml转化为snmp.yml文件，通过解析generator.yml中配置的mib module也就是walk中的数组，在mibs中都有现成的指标定义，然后听过netsnmp解析成我们需要的snmp.yml文件进行采集&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控系列---- 监控总纲</title>
          <link>https://kingjcy.github.io/post/monitor/monitor/</link>
          <pubDate>Fri, 02 Nov 2018 19:51:38 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/monitor/</guid>
          <description>&lt;p&gt;几乎所有的系统（我们通常都是APM：应用系统监控）都可以通过是三个方面来构建三维一体立体化监控体系。&lt;/p&gt;

&lt;h1 id=&#34;总体架构&#34;&gt;总体架构&lt;/h1&gt;

&lt;p&gt;立体化监控分三个维度,三种相互辅助的完整监控体系。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Metrics：   可以用于服务告警&lt;/li&gt;
&lt;li&gt;Logging：   用于调试发现问题&lt;/li&gt;
&lt;li&gt;Tracing：   用于调试发现问题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/monitor&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;不同的指标表示不同维度的监控，构成一个完成的监控体系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/monitor1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;监控分类&#34;&gt;监控分类&lt;/h1&gt;

&lt;h2 id=&#34;metrics&#34;&gt;Metrics&lt;/h2&gt;

&lt;p&gt;metrics就是指标监控，通过定义的指标来表示资源的使用情况或者状态等。针对这一类监控，我们通常使用时序数据库来做图表面展示，我们常用的监控系统有&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/metrics/&#34;&gt;prometheus生态,zabbix&lt;/a&gt;等，目前来说时序更加符合监控的需求，无论重数据的存储到数据的使用上都是时序数据库有优势。&lt;/p&gt;

&lt;h2 id=&#34;logging&#34;&gt;Logging&lt;/h2&gt;

&lt;p&gt;log就是日志监控，采集日志，进行处理，存储，展示，重日志中获取到运行的信息，我们通常用的日志系统有&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/log-scheme/&#34;&gt;EFK生态&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;tracing&#34;&gt;Tracing&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/trace/trace/&#34;&gt;trace&lt;/a&gt;就是调用链监控，即一次完整的事务调用请求。比方说一个用户的下单请求，经过层层服务预处理，到支付服务成功，数据落库，成功返回，这就是一条完整的 Trace 。Trace 最大的特点就是它含有上下文环境，通常来说会由一个唯一的 ID 来进行标识。一个 Trace 内可能有多个不同的事务 (Transaction) 以及标志事件 (Event) 组成。我们通常使用的是[Skywalking]()，CNCF推荐的是[jaeger]()。&lt;/p&gt;

&lt;h2 id=&#34;对比结合&#34;&gt;对比结合&lt;/h2&gt;

&lt;p&gt;三种对比&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;metrics监控前期的搭建难度适中，后期维护比较简单，在出现问题的时候，灵敏度比较高，比较容易发现问题，发出告警，但是在排查问题的只有表面的数据，不能找到具体的根因。&lt;/li&gt;
&lt;li&gt;log监控前期的搭建难度比较低，现在搭建一套ELK已经很成熟了，后期维护比较高，因为日志的数据量都是巨大的，在出现问题的时候，灵敏度比较适中，不是太容易发现问题，发出告警，主要在排查问题的进行查看。&lt;/li&gt;
&lt;li&gt;trace监控前期的搭建难度很高，后期维护也比较适中，在出现问题的时候，灵敏度很低，不容易发现问题，发出告警，但是在排查问题的时候能找到具体的根因，主要是用于排查问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结合&lt;/p&gt;

&lt;p&gt;一般系统出问题的时候，都是metrics先发现问题，发出告警，然后我们去查看日志查看问题，查看trace来定位到具体的原因。&lt;/p&gt;

&lt;h1 id=&#34;监控分层&#34;&gt;监控分层&lt;/h1&gt;

&lt;p&gt;在不同的层面都可以使用上面三种监控类型进行监控，结合起来完成每一层完整的监控体系。&lt;/p&gt;

&lt;p&gt;从底到上分为：基础设施监控、系统层监控、应用层监控、业务层监控、端用户体验监控&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/monitor2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;端侧监控主要是用调用链监控，来监控一些调用延迟，错误等，比如听云。&lt;/li&gt;
&lt;li&gt;业务层监控主要是用metrics监控，日志监控，来监控注册登陆转化订单数据，这些业务的错误情况下还需要通过日志来进行排查。&lt;/li&gt;
&lt;li&gt;应用层其实也就是我们的服务层了，需要通过三位一体化的监控，形成完整的监控体系，来监控一些调用延迟错误。&lt;/li&gt;
&lt;li&gt;系统层就是底层基础设施的监控，一些系统的指标日志。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控系列---- trace</title>
          <link>https://kingjcy.github.io/post/monitor/trace/trace/</link>
          <pubDate>Mon, 13 Aug 2018 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/trace/trace/</guid>
          <description>&lt;p&gt;全链路监控系统 - APM（Application Performance Managemen）主要用于调用链路追踪，对每一次调用都做性能分析。&lt;/p&gt;

&lt;p&gt;当我们需要解决以下的问题的时候，我们就需要引入APM了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;上线发布后，如何确认服务一切正常？&lt;/li&gt;
&lt;li&gt;客户端收到了错误的提示，但是到底是哪个服务抛出的这个错误？&lt;/li&gt;
&lt;li&gt;程序性能有问题，但是具体是哪个环节成了性能的瓶颈？&lt;/li&gt;
&lt;li&gt;接口响应很慢，到底是网络问题还是代码问题？&lt;/li&gt;
&lt;li&gt;服务调用链路长，每个环节都可能是一个出问题的风险点？&lt;/li&gt;
&lt;li&gt;做技术优化，如何丈量我们的服务质量呢？&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;调用链&#34;&gt;调用链&lt;/h1&gt;

&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;

&lt;p&gt;调用链中的核心概念&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trace 一次分布式调用的链路&lt;/li&gt;
&lt;li&gt;Span 一次本地或者远程方法的调用&lt;/li&gt;
&lt;li&gt;Annotation 附加在 Span 上的日志信息&lt;/li&gt;
&lt;li&gt;Sampling 采样率（客户端按照比例将埋点信息提交给服务端）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来通过实例来具体展示以下对应的概念&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/trace&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上我们图示了一次分布式调用的全过程，图中有三个分布式服务 Service A、Service B、Service B，每个方法的调用链信息中涉及到如下三个标记：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;tid：tid 即为 trace id，代表该次调用的唯一 ID，在一次分布式调用中，所有方法的 tid 都相同。如上图中的 Tid:1。&lt;/li&gt;
&lt;li&gt;sid：sid 即为 span id， 代表的是一个本地/远程方法调用的唯一 ID。如上图中每个绿色框代表的是一次方法调用，每次调用都有自己的 sid。&lt;/li&gt;
&lt;li&gt;pid：pid 其实也是一个 span id，但是它代表的是当前方法的父级方法的 span id。如上图中第一方法调用由客户端发起，是没有 pid 的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在上图中所示的调用链中总共包含了 7 个方法（本地/远程）调用，依次如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用客户端发起调用请求后，首先请求进入到 A 服务。此时会产生调用链信息 tid:1, sid:1 。&lt;/li&gt;
&lt;li&gt;接着发生了一次远程调用 Tid:1, pid:1, sid:2，pid 为 1 代表父级方法的 span id 为 1 即为 sid = 1 的方法，同理本次 redis 远程调用的 span id 为 2。&lt;/li&gt;
&lt;li&gt;Redis 远程调用结束后发生了对 Service B 的远程调用 Tid:1, pid:1, sid:3，与方法 2 类似，不同的是本次方法调用的 span id 为 3。&lt;/li&gt;
&lt;li&gt;在 Service B 中，首先是一个本地方法调用 Tid:1, pid:3, sid:4，从 pid = 3 可以得出它的父级方法正是方法 3。&lt;/li&gt;
&lt;li&gt;接着发生了一次对 Mysql 的远程调用 Tid:1, pid:4, sid:5，pid = 4 代表父级方法为方法 4，span id 为 5。&lt;/li&gt;
&lt;li&gt;Mysql 远程调用结束后，Service B 对 Service C 进行了一次远程调用 Tid:1, pid:4, sid:6，同样通过 pid 和 tid 我们可以将本次方法调用与整个调用链关联起来。&lt;/li&gt;
&lt;li&gt;最后是 Service C 的一个本地方法调用 Tid:1, pid:6, sid:7，至此整个调用链到达最远端，这是本次分布式调用链最深处的一个方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;发展&#34;&gt;发展&lt;/h2&gt;

&lt;p&gt;2010 年 Google Dapper 问世，其实早在 2002 年 eBay 已经有了自己的调用链监控产品，它叫 CAL（Central Application Logging）。当时 eBay 中国研发中心的一位资深工程师作为 CAL 的核心维护人员，对 CAL 的方方面面都非常熟悉。&lt;/p&gt;

&lt;p&gt;后来他去了美团点评，在 2011 年的时候，他带领团队研发出了 CAT（Central Application Tracing），CAT 继承了 CAL 的优点，也增加了很多自己的特色功能，并且它已经在 GitHub 开源，也在美团点评经受了大流量，高并发应用的检验，是目前业界应用比较广泛和成熟的生产级别调用链监控产品。&lt;/p&gt;

&lt;p&gt;随后由于 Google Dapper 论文的发布，也伴随着互联网产品的迅速发展，各个大厂依据 Dapper 纷纷实现了自己的调用链监控产品。在 2012 年就诞生了 3 款产品，携程的 CTrace，韩国公司 Naver 的 PinPoint，Twitter 的 Zipkin。&lt;/p&gt;

&lt;p&gt;随后在 2014 年，阿里研发了 Eagleye，京东研发了 Hydra。接着诞生了调用链监控的标准规范 Open Tracing，面对各大厂的调用链监控产品，他们使用不兼容的 API 来实现各自的应用需求。尽管这些分布式追踪系统有着相似的 API 语法，但各种语言的开发人员依然很难将他们各自的系统（使用不同的语言和技术）和特定的分布式追踪系统进行整合，Open Tracing 希望可以解决这个问题，因此颁布了这套标准。于此也诞生了 Uber 的 Jaeger，国人吴晟做的 SkyWalking（现在已经捐赠给了 Apache）均实现了 Open Tracing 标准&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/trace1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;各大调用链产品分为了典型的三类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CAT 类：鼻祖 CAL，侵入式埋点，国内公司使用较广。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/trace/zipkin&#34;&gt;Zipkin&lt;/a&gt; 类：鼻祖 Google Dapper，侵入式埋点，国内使用不广(s)。&lt;/li&gt;
&lt;li&gt;PinPiont类：鼻祖 Google Dapper，非侵入式卖点，采用字节码增强技术。&lt;/li&gt;
&lt;li&gt;opentacing：&lt;a href=&#34;https://kingjcy.github.io/post/monitor/trace/jaeger&#34;&gt;jaeger&lt;/a&gt;，skywalking&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/trace2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;es&#34;&gt;ES&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/database/elasticsearch/&#34;&gt;ES&lt;/a&gt;数据库很重要，不管在日志还是在调用链都是存储数据的核心模块，也是提供聚合查询的基础。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus postgresql_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/postgres_exporter/</link>
          <pubDate>Thu, 09 Aug 2018 14:29:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/postgres_exporter/</guid>
          <description>&lt;p&gt;Prometheus exporter for PostgreSQL server metrics.&lt;/p&gt;

&lt;h1 id=&#34;源码原理解析&#34;&gt;源码原理解析&lt;/h1&gt;

&lt;p&gt;1、使用kingpin来解析启动参数，包含web.listen-address，web.telemetry-path，disable-default-metrics，extend.query-path，dumpmaps，version&lt;/p&gt;

&lt;p&gt;2、如果设置参数dumpmaps，则直接把默认查询的查询map结构输出，主要查询一些系统信息，最后结束探针不运行&lt;/p&gt;

&lt;p&gt;3、启动时设置环境变量DATA_SOURCE_NAME作为探测pg的地址链接，代码中通过获取环境变量的值来获取datasource&lt;/p&gt;

&lt;p&gt;4、创建一个exporter的结构体，并初始化，包含了默认采集指标&lt;/p&gt;

&lt;p&gt;5、针对可配的路由web.telemetry-path来做对应的数据处理，默认是/metrics，根目录则显示到可配置的目录下获取指标数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1)根据获取的dsn来连接数据库
2）检查版本是否需要更新，就是看pg的版本是否和我们目前支持的采集兼容，低于最低可探测版本则不可用，然后对可探测的对应的版本默认指标进行调整。
3）解析配置文件
4）根据默认语句和配置文件查询语句来进行查询，将查询结果放到对应的指标变量中去。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、启动监听地址和端口，可配置&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系统---- Thanos</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/</link>
          <pubDate>Fri, 13 Jul 2018 17:14:15 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/</guid>
          <description>&lt;p&gt;Thanos，一组通过跨集群联合、跨集群无限存储和全局查询为Prometheus 增加高可用性的组件。&lt;/p&gt;

&lt;h1 id=&#34;基本功能&#34;&gt;基本功能&lt;/h1&gt;

&lt;p&gt;prometheus单点能够支持百万的metrics，但是在规模越来越大的系统中，已经不能满足要求，需要集群的功能来处理更加庞大的数据，基于这个情况，thanos诞生了，thanos的主要功能：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;去重，单点问题，可以让prometheus高可用，实现多采集情况下的数据查询，query是无状态的，可以使用负载均衡&lt;/li&gt;
&lt;li&gt;聚合，实现不同prometheus的数据的聚合，匹配prometheus的hashmode功能，实现集群的方式&lt;/li&gt;
&lt;li&gt;数据备份，主要是基于s3的，相当于远程存储，我们没有使用，直接将数据写入到了kafka&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;基本组件&#34;&gt;基本组件&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Sidecar&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sidecar作为一个单独的进程和已有的Prometheus实例运行在一个server上，互不影响。Sidecar可以视为一个Proxy组件，所有对Prometheus的访问都通过Sidecar来代理进行。通过Sidecar还可以将采集到的数据直接备份到云端对象存储服务器。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Querier&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有的Sidecar与Querier直连，同时Querier实现了一套Prometheus官方的HTTP API从而保证对外提供与Prometheus一致的数据源接口，Grafana可以通过同一个查询接口请求不同集群的数据，Querier负责找到对应的集群并通过Sidecar获取数据。Querier本身无状态的也是水平可扩展的，因而可以实现高可部署，而且Querier可以实现对高可部署的Prometheus的数据进行合并从而保证多次查询结果的一致性，从而解决全局视图和高可用的问题。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Store&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Store实现了一套和Sidecar完全一致的API提供给Querier用于查询Sidecar备份到云端对象存储的数据。因为Sidecar在完成数据备份后，Prometheus会清理掉本地数据保证本地空间可用。所以当监控人员需要调取历史数据时只能去对象存储空间获取，而Store就提供了这样一个接口。Store Gateway只会缓存对象存储的基本信息，例如存储块的索引，从而保证实现快速查询的同时占用较少本地空间。&lt;/p&gt;

&lt;p&gt;store和sidecar都提供了相同gprc的api，给外部client进行查询，其实是一回事。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Comactor&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Compactor主要用于对采集到的数据进行压缩，实现将数据存储至对象存储时节省空间。单独使用，和集群没有什么关系。主要是将对象存储 Bucket 中的多个小 的相同的Block 合并成 大 Block&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;h2 id=&#34;sidecar&#34;&gt;sidecar&lt;/h2&gt;

&lt;p&gt;sidecar部署在prometheus机器上,直接使用二进制文件配置不同的启动参数来启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/promes/thanos-sidecar/thanos sidecar --log.level=debug --tsdb.path=/data --prometheus.url=http://localhost:9099
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;query&#34;&gt;query&lt;/h2&gt;

&lt;p&gt;query用于查询，单独部署，然后和prometheus一样使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/promes/thanos-query/thanos query --query.timeout=15s --store.response-timeout=15s --log.level=debug --store=10.243.53.96:19091 --store=10.243.53.100:19091 --store=10.243.53.101:19091 --store=10.243.53.186:19091
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sd&#34;&gt;sd&lt;/h2&gt;

&lt;p&gt;thanos有三种sd的方式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Static Flags&lt;/p&gt;

&lt;p&gt;最简单的就是在参数中配置列表，就是我们上面使用的方式&lt;/p&gt;

&lt;p&gt;&amp;ndash;store参数指定的是每个sidecar的grpc端口，query会根据&amp;ndash;store参数列表找到对应的prometheus进行查询，所有组件的端口都是有默认值的，如果需要修改则指定参数&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Interface&lt;/th&gt;
&lt;th&gt;Port&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sidecar&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10901&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sidecar&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10902&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Query&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10903&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Query&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10904&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Store&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10905&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Store&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10906&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Receive&lt;/td&gt;
&lt;td&gt;gRPC (store API)&lt;/td&gt;
&lt;td&gt;10907&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Receive&lt;/td&gt;
&lt;td&gt;HTTP (remote write API)&lt;/td&gt;
&lt;td&gt;10908&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Receive&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10909&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rule&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10910&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rule&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10911&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Compact&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10912&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;File SD&lt;/p&gt;

&lt;p&gt;&amp;ndash;store.sd-files=&lt;path&gt;和 &amp;ndash;store.sd-interval=&lt;5m&gt;来获取对应的prometheus列表&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;DNS SD&lt;/p&gt;

&lt;p&gt;&amp;ndash;store=dns+stores.thanos.mycompany.org:9090
&amp;ndash;store=dnssrv+_thanosstores._tcp.mycompany.org
&amp;ndash;store=dnssrvnoa+_thanosstores._tcp.mycompany.org&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;Thanos 在每一台 Prometheus 服务器上运行一个sidecar组件，并提供了一个用于处理 PromQL 查询的中央 Querier 组件，因而在所有服务器之间引入了一个中央查询层。这些组件构成了一个 Thanos 部署，并基于 memberlist gossip 协议实现组件间通信。Querier 可以水平扩展，因为它是无状态的，并且可充当智能逆向代理，将请求转发给sidecar，汇总它们的响应，并对 PromQL 查询进行评估。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实现细节&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;Thanos 通过使用后端的对象存储来解决数据保留问题。Prometheus 在将数据写入磁盘时，sidecar的 StoreAPI 组件会检测到，并将数据上传到对象存储器中。Store 组件还可以作为一个基于 gossip 协议的检索代理，让 Querier 组件与它进行通信以获取数据。&lt;/li&gt;
&lt;li&gt;我们使用基本的过滤器（基于时间范围和外部标签）过滤掉不会提供所需数据的 StoreAPI（叶子），然后执行剩余的查询。然后将来自不同来源的数据按照时间顺序追加的方式合并在一起。&lt;/li&gt;
&lt;li&gt;Querier 组件可以基于用户规模自动调整密度（例如 5 分钟、1 小时或 24 小时）&lt;/li&gt;
&lt;li&gt;StoreAPI 组件了解 Prometheus 的数据格式，因此它可以优化查询执行计划，并缓存数据块的特定索引，以对用户查询做出足够快的响应，避免了缓存大量数据的必要。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;我们通过为所有 Prometheus+ sidecar实例提供唯一的外部标签来解决多个边车试图将相同的数据块上传到对象存储的问题，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;First:

&amp;quot;cluster&amp;quot;: &amp;quot;prod1&amp;quot;

&amp;quot;replica&amp;quot;: &amp;quot;0&amp;quot;

Second:

&amp;quot;cluster&amp;quot;:&amp;quot;prod1&amp;quot;

&amp;quot;replica&amp;quot;: &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于标签集是唯一的，所以不会有什么问题。不过，如果指定了副本，查询层可以在运行时通过“replica”标签进行除重操作。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thanos 还提供了时间序列数据的压缩和降采样（downsample）存储。Prometheus 提供了一个内置的压缩​​模型，现有较小的数据块被重写为较大的数据块，并进行结构重组以提高查询性能。Thanos 在Compactor 组件（作为批次作业运行）中使用了相同的机制，并压缩对象存储数据。Płotka 说，Compactor 也对数据进行降采样，“目前降采样时间间隔不可配置，不过我们选择了一些合理的时间间隔——5 分钟和1 小时”。压缩也是其他时间序列数据库（如 InfluxDB 和 OpenTSDB ）的常见功能。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;遇到的问题&#34;&gt;遇到的问题&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;查询速度较慢，在数据量基本特别大的时候，查询会超时。&lt;/li&gt;
&lt;li&gt;thanos 目前还不能支持默认查询lookback时间，promehteus可以设置默认查询时间，thanos默认是根据规模自动调整的，目前发现有10m，20m等，这边可以暂时表达式加时间处理这个问题。&lt;/li&gt;
&lt;li&gt;sidecar的启动参数&amp;ndash;cluster-peers是什么作用&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;在prometheus的聚合和集群发展中，出现了很多的相同的项目，大部分都是使用了远程存储的概念，比如&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/&#34;&gt;cortex&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/&#34;&gt;M3DB&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;victoriametrics&lt;/a&gt;，thanos在调研落地的过程中，各方面还是相对做到比较好的，适合做为prometheus的扩展和聚合方案。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/thanos/thanos&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2019.9.9&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;victoriametrics&lt;/a&gt;在存储和查询上更加的优秀，目前比较推荐victoriametrics。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算CDCI系列---- helm</title>
          <link>https://kingjcy.github.io/post/cloud/paas/cdci/helm/</link>
          <pubDate>Wed, 11 Jul 2018 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/cdci/helm/</guid>
          <description>&lt;p&gt;Helm 是 Deis (&lt;a href=&#34;https://deis.com/&#34;&gt;https://deis.com/&lt;/a&gt;) 开发的一个用于 kubernetes 的包管理器。每个包称为一个 Chart，一个 Chart 是一个目录（一般情况下会将目录进行打包压缩，形成 name-version.tgz 格式的单一文件，方便传输和存储),可以将 Helm 看作 Kubernetes 下的 apt-get/yum。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;chart&#34;&gt;chart&lt;/h2&gt;

&lt;p&gt;chart 是描述相关的一组 Kubernetes 资源的文件集合。 chart 通过创建为特定目录树的文件，将它们打包到版本化的压缩包，然后进行部署。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Chart.yaml 是必须的，它记录了 chart 的一些信息: chart 版本和名字等&lt;/li&gt;
&lt;li&gt;templates 下是 kubernetes资源的模板

&lt;ul&gt;
&lt;li&gt;NOTES.txt 说明文件,helm install之后展示给用户看的内容&lt;/li&gt;
&lt;li&gt;deployment.yaml 创建k8s资源的yaml文件&lt;/li&gt;
&lt;li&gt;_helpers.tpl: 下划线开头的文件,可以被其他模板引用.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;values.yaml 存放了模板 中的变量的值&lt;/li&gt;
&lt;li&gt;charts: 依赖其他包的charts文件&lt;/li&gt;
&lt;li&gt;requirements.yaml: 依赖的charts&lt;/li&gt;
&lt;li&gt;README.md: 开发人员自己阅读的文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个最小的chart目录,只需要包含一个Chart.yaml,和templates目录下一个k8s资源文件&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/helm/chart1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;release&#34;&gt;Release&lt;/h2&gt;

&lt;p&gt;Release表示在 Kubernetes 集群上运行的 Chart 的一个实例。在同一个集群上，一个 Chart 可以安装很多次。每次安装都会创建一个新的 release。例如一个 MySQL Chart，如果想在服务器上运行两个数据库，就可以把这个 Chart 安装两次。每次安装都会生成自己的 Release，会有自己的 Release 名称。&lt;/p&gt;

&lt;h2 id=&#34;repository&#34;&gt;Repository&lt;/h2&gt;

&lt;p&gt;Repository用于发布和存储 Chart 的存储库。&lt;/p&gt;

&lt;h2 id=&#34;helm&#34;&gt;helm&lt;/h2&gt;

&lt;p&gt;helm就是一个命令行下客户端工具，主要用于kubernetes应用chart的创建/打包/发布已经创建和管理和远程Chart仓库。&lt;/p&gt;

&lt;h2 id=&#34;tiller&#34;&gt;Tiller&lt;/h2&gt;

&lt;p&gt;Tiller就是helm的服务端，部署于kubernetes内，Tiller接受helm的请求，并根据chart生成kubernetes部署文件（helm称为release），然后提交给 Kubernetes 创建应用。Tiller 还提供了 Release 的升级、删除、回滚等一系列功能。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/helm/helm&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;chart-install-过程&#34;&gt;Chart Install 过程&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Helm从指定的目录或者tgz文件中解析出Chart结构信息&lt;/li&gt;
&lt;li&gt;Helm将指定的Chart结构和Values信息通过gRPC传递给Tiller&lt;/li&gt;
&lt;li&gt;Tiller根据Chart和Values生成一个Release&lt;/li&gt;
&lt;li&gt;Tiller将Release发送给Kubernetes运行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;chart-update过程&#34;&gt;Chart Update过程&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Helm从指定的目录或者tgz文件中解析出Chart结构信息&lt;/li&gt;
&lt;li&gt;Helm将要更新的Release的名称和Chart结构，Values信息传递给Tiller&lt;/li&gt;
&lt;li&gt;Tiller生成Release并更新指定名称的Release的History&lt;/li&gt;
&lt;li&gt;Tiller将Release发送给Kubernetes运行&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;chart-rollback过程&#34;&gt;Chart Rollback过程&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;helm将会滚的release名称传递给tiller&lt;/li&gt;
&lt;li&gt;tiller根据release名称查找history&lt;/li&gt;
&lt;li&gt;tiller从history中获取到上一个release&lt;/li&gt;
&lt;li&gt;tiller将上一个release发送给kubernetes用于替换当前release&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以简单的看一下基本的交互：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/helm/helm1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;安装和使用&#34;&gt;安装和使用&lt;/h1&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;client 管理 charts，可在本地运行，一般运行在CI/CD Server上。而 server （tiller）运行在Kubernetes集群上，管理chart安装的release。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;客户端安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;到 Helm Release 下载二进制文件,根据使用 的操作系统不同下载不同的版本，这里以 Linux上V2.15.1 为例，解压后将可执行文件 helm 拷贝至 usr/local/ bin 目录下即可， 这样 Helm 客户端就在这台机器上安装完了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 在helm客户端主机上，一般为master主机
wget https://get.helm.sh/helm-v2.14.2-linux-amd64.tar.gz
tar xf helm-v2.14.2-linux-amd64.tar.gz
mv helm /usr/local/bin/
helm version
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;服务端安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;服务端安装就是Tiller服务的安装，我们需要将Tiller服务安装在k8s集群中，来监听来自 Helm client 的请求，安装 chart 到 Kubernetes 集群，并跟踪随后的发布通过与 Kubernetes 交互升级或卸载 chart。&lt;/p&gt;

&lt;p&gt;需要创建helm相关的角色，我们来看一下对应的资源配置清单clusterrole.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &amp;quot;true&amp;quot;
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
rules:
- apiGroups:
  - &#39;*&#39;
  resources:
  - &#39;*&#39;
  verbs:
  - &#39;*&#39;
- nonResourceURLs:
  - &#39;*&#39;
  verbs:
  - &#39;*&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建ClusterRole&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f clusterrole.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建ServiceAccount并使用ClusterRoleBinding将其与ClusterRole关联&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create serviceaccount -n kube-system tiller
$ kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化Helm，其实也就是在k8s上部署tiller&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm init --service-account tiller --skip-refresh
Creating /root/.helm
Creating /root/.helm/repository
Creating /root/.helm/repository/cache
Creating /root/.helm/repository/local
Creating /root/.helm/plugins
Creating /root/.helm/starters
Creating /root/.helm/cache/archive
Creating /root/.helm/repository/repositories.yaml
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com
Adding local repo with URL: http://127.0.0.1:8879/charts
$HELM_HOME has been configured at /root/.helm.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;helm init. Helm 默认会去 gcr.io 拉取 tiller 的镜像,有时镜像拉不下来，可以指定 tiller 的镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm init --tiller-image registry.cn-hangzhou.aliyuncs.com/softputer/tiller:v2.15.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;tiller默认被部署在k8s集群中的kube-system这个namespace下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pod -n kube-system -l app=helm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次helm version可以打印客户端和服务端的版本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.7.2&amp;quot;, GitCommit:&amp;quot;8478fb4fc723885b155c924d1c8c410b7a9444e6&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.7.2&amp;quot;, GitCommit:&amp;quot;8478fb4fc723885b155c924d1c8c410b7a9444e6&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里就安装好了。&lt;/p&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;直接创建release&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;先看一个实例安装redis：下面声明了一个版本名称为my-redis,并且指定该部署将使用持久化存储，大小为15G&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install --name my-redis \
--set persistence.enabled=true,persistence.size=15Gi stable/redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下安装的过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME:   my-influx
LAST DEPLOYED: Thu Mar 14 12:36:50 2019
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1beta1/Deployment
NAME                   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
my-influx-redis-slave  1        1        1           0          0s

==&amp;gt; v1beta2/StatefulSet
NAME                    DESIRED  CURRENT  AGE
my-influx-redis-master  1        0        0s

==&amp;gt; v1/Pod(related)
NAME                                    READY  STATUS             RESTARTS  AGE
my-influx-redis-slave-556f9894b7-wd5cf  0/1    ContainerCreating  0         0s
my-influx-redis-master-0                0/1    Pending            0         0s

==&amp;gt; v1/Secret
NAME             TYPE    DATA  AGE
my-influx-redis  Opaque  1     0s

==&amp;gt; v1/ConfigMap
NAME                    DATA  AGE
my-influx-redis         3     0s
my-influx-redis-health  3     0s

==&amp;gt; v1/Service
NAME                    TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE
my-influx-redis-master  ClusterIP  10.107.184.240  &amp;lt;none&amp;gt;       6379/TCP  0s
my-influx-redis-slave   ClusterIP  10.97.224.81    &amp;lt;none&amp;gt;       6379/TCP  0s


NOTES:
** Please be patient while the chart is being deployed **
Redis can be accessed via port 6379 on the following DNS names from within your cluster:

my-influx-redis-master.default.svc.cluster.local for read/write operations
my-influx-redis-slave.default.svc.cluster.local for read-only operations


To get your password run:

    export REDIS_PASSWORD=$(kubectl get secret --namespace default my-influx-redis -o jsonpath=&amp;quot;{.data.redis-password}&amp;quot; | base64 --decode)

To connect to your Redis server:

1. Run a Redis pod that you can use as a client:

   kubectl run --namespace default my-influx-redis-client --rm --tty -i --restart=&#39;Never&#39; \
    --env REDIS_PASSWORD=$REDIS_PASSWORD \
   --image docker.io/bitnami/redis:4.0.13 -- bash

2. Connect using the Redis CLI:
   redis-cli -h my-influx-redis-master -a $REDIS_PASSWORD
   redis-cli -h my-influx-redis-slave -a $REDIS_PASSWORD

To connect to your database from outside the cluster execute the following commands:

    kubectl port-forward --namespace default svc/my-influx-redis 6379:6379 &amp;amp;
    redis-cli -h 127.0.0.1 -p 6379 -a $REDIS_PASSWORD
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看release&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm ls
NAME         REVISION    UPDATED                     STATUS      CHART          APP VERSION    NAMESPACE
my-influx    1           Tue May 30 21:18:43 2017       DEPLOYED    redis-6.3.0    4.0.13         default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除release&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm del  my-influx
release &amp;quot;my-influx&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;创建chart来创建release&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Helm 提供了 create 指令建立一个 Chart 基本结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master mychart]# helm create mychart
Creating mychart
[root@master mychart]# ls
mychart
[root@master mychart]# tree mychart/
mychart/
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml                     # 部署相关资源
│   ├── _helpers.tpl                            # 模版助手
│   ├── ingress.yaml                            # ingress资源
│   ├── NOTES.txt                                   # chart的帮助文本，运行helm install展示给用户
│   ├── service.yaml                            # service端点
│   └── tests
│       └── test-connection.yaml
└── values.yaml

3 directories, 8 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上面我们已经对具体的文件做过了说明，这边在具体描述一下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;charts目录中是本chart依赖的chart，当前是空的&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Chart.yaml这个yaml文件用于描述Chart的基本信息，如名称版本等&lt;/p&gt;

&lt;p&gt;$ cat Chart.yaml
  apiVersion: v1
  description: A Helm chart for Kubernetes
  name: mychart
  version: 0.1.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;templates是Kubernetes manifest文件模板目录，模板使用chart配置的值生成Kubernetes manifest文件。模板文件使用的Go语言模板语法&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;templates/NOTES.txt 纯文本文件，可在其中填写chart的使用说明&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;value.yaml 是chart配置的默认值&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们一般只要编写模版文件和对应的赋值文件，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm -rf mychart/templates/*
# 我们首先创建一个名为 mychart/templates/configmap.yaml：

apiVersion: v1
kind: ConfigMap
metadata:
  name: mychart-configmap
data:
  myvalue: &amp;quot;Hello World&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用helm在Kubernetes上安装chart时，实际上是将chart的模板生成Kubernetes使用的manifest yaml文件。 在编写chart的过程中可以chart目录下使用helm install –dry-run –debug ./来验证模板和配置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install --dry-run --debug ./
[debug] Created tunnel using local port: &#39;22635&#39;

[debug] SERVER: &amp;quot;127.0.0.1:22635&amp;quot;

[debug] Original chart version: &amp;quot;&amp;quot;
[debug] CHART PATH: /root/helm/hello-svc

NAME:   foolish-zebra
REVISION: 1
......输出基于配置值和模板生成的yaml文件
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当我们设置完 Chart 后，就可以通过 helm 指令打包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm package example/
example-0.1.0.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过helm安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install ./example-0.1.0.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以不打包，直接安装，由于创建的yaml文件在template下，tiller读取此文件，会将其发送给kubernetes。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master mychart]# helm install ./mychart/
NAME:   enervated-dolphin
LAST DEPLOYED: Sun Jul 21 09:29:13 2019
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/ConfigMap
NAME               DATA  AGE
mychart-configmap  1     0s

[root@master mychart]# kubectl get cm mychart-configmap
NAME                DATA   AGE
mychart-configmap   1      2m6s
[root@master mychart]# kubectl describe cm mychart-configmap
Name:         mychart-configmap
Namespace:    default
Labels:       &amp;lt;none&amp;gt;
Annotations:  &amp;lt;none&amp;gt;

Data
====
myvalue:
----
this is my chart configmap
Events:  &amp;lt;none&amp;gt;


[root@master mychart]# helm get manifest enervated-dolphin

---
# Source: mychart/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mychart-configmap
data:
  myvalue: &amp;quot;this is my chart configmap&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看release&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm list
NAME            REVISION        UPDATED                         STATUS                      CHART           NAMESPACE
enervated-dolphin   1               Thu Dec 21 22:04:19 2017        DEPLOYED        enervated-dolphin-0.1.0 default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除release&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm delete enervated-dolphin
release &amp;quot;enervated-dolphin&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;添加模板调用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一、内置对象&lt;/p&gt;

&lt;p&gt;1、release&lt;/p&gt;

&lt;p&gt;修改下之前的configmap为如下内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.Release.Name}}-configmap
data:
  myvalue: &amp;quot;Hello World&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;模板指令 {{.Release.Name}} 将 release 名称注入模板。也就是将上面的enervated-dolphin的release的名称赋值给当前的{{.Release.Name}}&lt;/p&gt;

&lt;p&gt;Release 是可以在模板中访问的顶级对象之一。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Release：这个对象描述了 release 本身。它里面有几个对象：
Release.Name：release 名称
Release.Time：release 的时间
Release.Namespace：release 的 namespace（如果清单未覆盖）
Release.Service：release 服务的名称（始终是 Tiller）。
Release.Revision：此 release 的修订版本号。它从 1 开始，每 helm upgrade 一次增加一个。
Release.IsUpgrade：如果当前操作是升级或回滚，则将其设置为 true。
Release.IsInstall：如果当前操作是安装，则设置为 true。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、value&lt;/p&gt;

&lt;p&gt;Helm 模板提供的内置对象。四个内置对象之一是 Values。该对象提供对传入 chart 的值的访问。其内容来自四个来源：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chart 中的 values.yaml 文件&lt;/li&gt;
&lt;li&gt;如果这是一个子 chart，来自父 chart 的 values.yaml 文件&lt;/li&gt;
&lt;li&gt;value 文件通过 helm install 或 helm upgrade 的 - f 标志传入文件（helm install -f myvals.yaml ./mychart）&lt;/li&gt;
&lt;li&gt;通过 &amp;ndash;set（例如 helm install &amp;ndash;set foo=bar ./mychart）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改对应的yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 编辑values.yaml
domain: anchnet.com

# 在模版中引用
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.Release.Name}}-configmap
data:
  myvalue: &amp;quot;Hello World&amp;quot;
  domain: {{.Values.domain}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意我们在最后一行 {{ .Values.domain}} 获取 domain` 的值。还可以使用上面其他的几种方法来实现。&lt;/p&gt;

&lt;p&gt;3、chart&lt;/p&gt;

&lt;p&gt;Chart.yaml 文件的内容。任何数据 Chart.yaml 将在这里访问。例如 {{.Chart.Name}}-{{.Chart.Version}} 将打印出来 mychart-0.1.0。&lt;/p&gt;

&lt;p&gt;二、模版函数和管道&lt;/p&gt;

&lt;p&gt;1、模版函数&lt;/p&gt;

&lt;p&gt;修改配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.Release.Name}}-configmap
data:
  myvalue: &amp;quot;Hello World&amp;quot;
  drink: {{quote .Values.favorite.drink}}
  food: {{quote .Values.favorite.food}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;模板函数遵循语法 functionName arg1 arg2&amp;hellip;。在上面的代码片段中，quote .Values.favorite.drink 调用 quote 函数并将一个参数传递给它。&lt;/p&gt;

&lt;p&gt;2、管道&lt;/p&gt;

&lt;p&gt;修改配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: {{.Release.Name}}-configmap
data:
  myvalue: &amp;quot;Hello World&amp;quot;
  drink: {{.Values.favorite.drink | quote}}
  food: {{.Values.favorite.food | quote}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;没有调用 quote ARGUMENT，我们调换了顺序。我们使用管道（|）将 “参数” 发送给函数：.Values.favorite.drink | quote。&lt;/p&gt;

&lt;p&gt;三、模版语法&lt;/p&gt;

&lt;p&gt;1、使用 default 函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drink: {{.Values.favorite.drink | default &amp;quot;tea&amp;quot; | quote}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、运算符函数&lt;/p&gt;

&lt;p&gt;对于模板，运算符（eq，ne，lt，gt，and，or 等等）都是已实现的功能。在管道中，运算符可以用圆括号（( 和 )）分组。&lt;/p&gt;

&lt;p&gt;3、流程控制&lt;/p&gt;

&lt;p&gt;Helm 的模板语言提供了以下控制结构：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;if/else 用于创建条件块&lt;/li&gt;
&lt;li&gt;with 指定范围&lt;/li&gt;
&lt;li&gt;range，它提供了一个 “for each” 风格的循环&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此之外，它还提供了一些声明和使用命名模板段的操作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;define 在模板中声明一个新的命名模板&lt;/li&gt;
&lt;li&gt;template 导入一个命名模板&lt;/li&gt;
&lt;li&gt;block 声明了一种特殊的可填写模板区域&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;总计&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实说到底模版就是go模版的语法和使用功能。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基础命令&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;http://hub.kubeapps.com/&#34;&gt;http://hub.kubeapps.com/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;completion  # 为指定的shell生成自动完成脚本（bash或zsh）
create      # 创建一个具有给定名称的新 chart
delete      # 从 Kubernetes 删除指定名称的 release
dependency  # 管理 chart 的依赖关系
fetch       # 从存储库下载 chart 并（可选）将其解压缩到本地目录中
get         # 下载一个命名 release
help        # 列出所有帮助信息
history     # 获取 release 历史
home        # 显示 HELM_HOME 的位置
init        # 在客户端和服务器上初始化Helm
inspect     # 检查 chart 详细信息
install     # 安装 chart 存档
lint        # 对 chart 进行语法检查
list        # releases 列表
package     # 将 chart 目录打包成 chart 档案
plugin      # 添加列表或删除 helm 插件
repo        # 添加列表删除更新和索引 chart 存储库
reset       # 从集群中卸载 Tiller
rollback    # 将版本回滚到以前的版本
search      # 在 chart 存储库中搜索关键字
serve       # 启动本地http网络服务器
status      # 显示指定 release 的状态
template    # 本地渲染模板
test        # 测试一个 release
upgrade     # 升级一个 release
verify      # 验证给定路径上的 chart 是否已签名且有效
version     # 打印客户端/服务器版本信息
dep         # 分析 Chart 并下载依赖
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;仓库&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将制作好的charts包可以上传至helm仓库，可以放在自己的自建私有仓库，流入：kubeapps/Monocular/minior等，可以利用helm命令一键安装。&lt;/p&gt;

&lt;p&gt;上传至公有云公共仓库，例如国内的阿里目前创建的Apphub等，在现今的云原生生态当中，已经有很多成熟的开源软件被制作成了 Helm Charts，使得用户可以非常方便地下载和使用，比如 Nginx，Apache、Elasticsearch、Redis 等等。不过，在开放云原生应用中心 App hub（Helm Charts 中国站) 发布之前，国内用户一直都很难直接下载使用这些 Charts。而现在，AppHub 不仅为国内用户实时同步了官方 Helm Hub 里的所有应用，还自动替换了这些 Charts 里所有不可访问的镜像 URL（比如 gcr.io, quay.io 等），终于使得国内开发者通过 helm install “一键安装”应用成为了可能。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus mysqld_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/mysqld-exporter/</link>
          <pubDate>Mon, 09 Jul 2018 14:29:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/mysqld-exporter/</guid>
          <description>&lt;p&gt;mysql监控指标采集探针:Prometheus exporter for MySQL server metrics.&lt;/p&gt;

&lt;h1 id=&#34;源码原理解析&#34;&gt;源码原理解析&lt;/h1&gt;

&lt;p&gt;1、collector&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scraperFlags := map[collector.Scraper]*bool{}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记录需要采集的collector，是interface Scraper的实现的结构体，并且支持所有采集 启动项可配置。&lt;/p&gt;

&lt;p&gt;2、连接数据库重环境变量或者mysql的配置文件中获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DATA_SOURCE_NAME／my.cnf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后采集一下数据库状态，每个scrapers启动一个协程去调用接口中Scrape的实现&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- 容器日志采集方案</title>
          <link>https://kingjcy.github.io/post/monitor/log/collect/collect-scheme/</link>
          <pubDate>Sun, 08 Jul 2018 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/collect/collect-scheme/</guid>
          <description>&lt;p&gt;容器由于其特殊性，在日志采集上有着不同的解决方案，目前主要还是以探针采集为主。&lt;/p&gt;

&lt;h1 id=&#34;日志采集演进&#34;&gt;日志采集演进&lt;/h1&gt;

&lt;p&gt;容器日志采集方案一直不断的演进，纵览当前容器日志收集的场景，无非就是两种方式：一是直接采集Docker标准输出，容器内的服务将日志信息写到标准输出，这样通过Docker的log driver可以发送到相应的收集程序中；二是延续传统的日志写入方式，容器内的服务将日志直接写到普通文件中，通过Docker volume将日志文件映射到Host上，日志采集程序就可以收集它。&lt;/p&gt;

&lt;h2 id=&#34;docker-log-driver&#34;&gt;docker log driver&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;docker logs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;docker logs edc-k8s-demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，Docker的日志会发送到容器的标准输出设备（STDOUT）和标准错误设备（STDERR），其中STDOUT和STDERR实际上就是容器的控制台终端。如果想要持续看到新打印出的日志信息，那么可以加上 -f 参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker logs -f edc-k8s-demo
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Docker logging driver&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Docker还提供了其他的一些机制允许我们从运行的容器中提取日志，这些机制统称为 logging driver。&lt;/p&gt;

&lt;p&gt;对Docker而言，其默认的logging driver是json-file，如果在启动时没有特别指定，都会使用这个默认的logging driver。json-file会将我们在控制台通过docker logs命名看到的日志都保存在一个json文件中，我们可以在服务器Host上的容器目录中找到这个json文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;容器日志路径：/var/lib/docker/containers/&amp;lt;container-id&amp;gt;/&amp;lt;container-id&amp;gt;-json.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了json-file，Docker还支持以下多种logging dirver&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;none  No logs are available for the container and docker logs does not return any output.&lt;/li&gt;
&lt;li&gt;local Logs are stored in a custom format designed for minimal overhead.&lt;/li&gt;
&lt;li&gt;json-file The logs are formatted as JSON. The default logging driver for Docker.&lt;/li&gt;
&lt;li&gt;syslog    Writes logging messages to the syslog facility. The syslog daemon must be running on the host machine.&lt;/li&gt;
&lt;li&gt;journald  Writes log messages to journald. The journald daemon must be running on the host machine.&lt;/li&gt;
&lt;li&gt;gelf  Writes log messages to a Graylog Extended Log Format (GELF) endpoint such as Graylog or Logstash.&lt;/li&gt;
&lt;li&gt;fluentd   Writes log messages to fluentd (forward input). The fluentd daemon must be running on the host machine.&lt;/li&gt;
&lt;li&gt;awslogs   Writes log messages to Amazon CloudWatch Logs.&lt;/li&gt;
&lt;li&gt;splunk    Writes log messages to splunk using the HTTP Event Collector.&lt;/li&gt;
&lt;li&gt;etwlogs   Writes log messages as Event Tracing for Windows (ETW) events. Only available on Windows platforms.&lt;/li&gt;
&lt;li&gt;gcplogs   Writes log messages to Google Cloud Platform (GCP) Logging.&lt;/li&gt;
&lt;li&gt;logentries    Writes log messages to Rapid7 Logentries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以在容器启动时通过加上 &amp;ndash;log-driver 来指定使用哪个具体的 logging driver，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --log-driver=syslog ......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想要设置默认的logging driver，那么则需要修改Docker daemon的启动脚本，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;log-driver&amp;quot;: &amp;quot;json-file&amp;quot;,
  &amp;quot;log-opts&amp;quot;: {
    &amp;quot;labels&amp;quot;: &amp;quot;production_status&amp;quot;,
    &amp;quot;env&amp;quot;: &amp;quot;os,customer&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个logging driver都有一些自己特定的log-opt，使用时可以参考具体官方文档。&lt;/p&gt;

&lt;p&gt;可见，第一种方式足够简单，直接配置相关的Log Driver就可以，但是这种方式也有些劣势：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当主机的容器密度比较高的时候，对Docker Engine的压力比较大，毕竟容器标准输出都要通过Docker Engine来处理。&lt;/li&gt;
&lt;li&gt;尽管原则上，我们希望遵循一容器部署一个服务的原则，但是有时候特殊情况不可避免容器内有多个业务服务，这时候很难做到所有服务都向标准输出写日志，这就需要用到前面所说的第二种场景模式。&lt;/li&gt;
&lt;li&gt;虽然我们可以先选择很多种Log Driver，但是有些Log Driver会破坏Docker原生的体验，比如docker logs无法直接看到容器日志。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;docker-volume&#34;&gt;docker volume&lt;/h2&gt;

&lt;p&gt;通过对第一种方案的摸索，存在着很多的问题与不方便，所以目前我们大多数采集还是使用第二种方案，文件采集的方式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;第三方采集方案&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面都是将日志文件落到STDOUT和STDERR，我们采集都是基于这个，其实在我们应用编程的时候，完全可以将日志文件落到容器的对应的目录下，落盘然后使用第三方采集组件比如filebeat、fluentd等采集，统一管理。&lt;/p&gt;

&lt;h1 id=&#34;容器日志采集方案&#34;&gt;容器日志采集方案&lt;/h1&gt;

&lt;p&gt;根据上面的基本描述，容器日志采集有很多种方式，每种方式都用不同实现方案，适用于不同的场景。&lt;/p&gt;

&lt;h2 id=&#34;logdriver&#34;&gt;LogDriver&lt;/h2&gt;

&lt;p&gt;DockerEngine 本身具有 LogDriver 功能，可通过配置不同的 LogDriver 将容器的 stdout 通过 DockerEngine 写入到远端存储，以此达到日志采集的目的。这种方式的可定制化、灵活性、资源隔离性都很低，一般不建议在生产环境中使用，上面我们已经说明不使用的原因。&lt;/p&gt;

&lt;h2 id=&#34;http&#34;&gt;http&lt;/h2&gt;

&lt;p&gt;业务直写是在应用中集成日志采集的 SDK，通过 SDK 直接将日志发送到服务端。这种方式省去了落盘采集的逻辑，也不需要额外部署 Agent，对于系统的资源消耗最低，但由于业务和日志 SDK 强绑定，整体灵活性很低，一般只有日志量极大的场景中使用，这是一种特殊的场景，我们会在特殊情况下使用。&lt;/p&gt;

&lt;h2 id=&#34;deamonset模式&#34;&gt;deamonset模式&lt;/h2&gt;

&lt;p&gt;DaemonSet 方式在每个 node 节点上只运行一个日志 agent(&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat/&#34;&gt;filebeat&lt;/a&gt;,fluentd,flume,fluentbit)，采集这个节点上所有的日志。DaemonSet 相对资源占用要小很多，但扩展性、租户隔离性受限，比较适用于功能单一或业务不是很多的集群；&lt;/p&gt;

&lt;p&gt;正常规模的采集可以适应，日志分类明确、功能较单一的集群，大规模的集群采集速度就跟不上了，而且没有办法做到垂直扩展无上限。&lt;/p&gt;

&lt;p&gt;当然完整的方案还是有很多需要做的工作，比如如何做发现，如何动态变更，这就是一个完成的平台建设了，这些每个公司都有自己的建设，就不太好说了。&lt;/p&gt;

&lt;h2 id=&#34;sidecar模式&#34;&gt;sidecar模式&lt;/h2&gt;

&lt;p&gt;Sidecar 方式为每个 POD 单独部署日志 agent，这个 agent 只负责一个业务应用的日志采集。Sidecar 相对资源占用较多，但灵活性以及多租户隔离性较强，建议大型的 K8s 集群或作为 PaaS 平台为多个业务方服务的集群使用该方式。&lt;/p&gt;

&lt;p&gt;适用于大型、混合型、PAAS型集群的日志采集，是一种水平扩展消耗更多资源来增加采集速度的方案，但是方案就比较复杂。&lt;/p&gt;

&lt;p&gt;当然完整的方案还是有很多需要做的工作，比如如何做发现，如何动态变更，这就是一个完成的平台建设了，这些每个公司都有自己的建设，也就不太好说了。&lt;/p&gt;

&lt;h1 id=&#34;网络采集性能数据&#34;&gt;网络采集性能数据&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;有赞&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;重flume发展到自研rsyslog-hub和http服务&lt;/p&gt;

&lt;p&gt;17年平均每秒产生日志1.1万条，峰值1.5万条，每天的日志量约9亿条，占用空间2.4T左右&lt;/p&gt;

&lt;p&gt;19年每天都会产生百亿级别的日志量（据统计，平均每秒产生 50 万条日志，峰值每秒可达 80 万条）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;七牛云&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;自研logkit&lt;/p&gt;

&lt;p&gt;17年现在日均数据流入量超 250 TB，3650 亿条，其中最大的客户日均数据流入量超过 45 TB。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;b站&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;17年目前集群规模20台机器，接入业务200+，单日日志量10T+。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;阿里云&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;自研logtail（重内核都得到的优化和充分利用）&lt;/p&gt;

&lt;p&gt;速度达到160M/s&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Nginx</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/server/nginx/</link>
          <pubDate>Fri, 29 Jun 2018 16:55:15 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/server/nginx/</guid>
          <description>&lt;p&gt;nginx [engine x] is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server.关于&lt;a href=&#34;https://kingjcy.github.io/post/middleware/proxy/nginx&#34;&gt;nginx&lt;/a&gt;的介绍就不多谈了，这里主要聊下如何打造nginx集群的监控系统。&lt;/p&gt;

&lt;h1 id=&#34;监控演化&#34;&gt;监控演化&lt;/h1&gt;

&lt;p&gt;目前国内大多数互联网都是选择nginx构建web转发集群，那么如何构建nginx集群的监控系统？&lt;/p&gt;

&lt;p&gt;下面这个监控方案的历程，大体也说明了nginx的进化发展。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2011年左右&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当时并没有合适的开源工具，只能自写python脚本，一方面是通过nginx的status模块获取少量指标如处理请求数，连接数等等，另一方面是通过定期跑nginx日志生成监控数据，写入mysql进行存储，然后通过自研的监控系统展示相关监控指标。其中，指标的主要维度是域名。这个方案除了跑nginx日志消耗过多资源以及需要大量开发工作外，没啥大问题，就是有新的监控需求时很头疼，得改脚本。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2013年左右&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这时候阿里的tengine开始发力，开发各种插件并在阿里内部推广使用，其中就包括了更为强大的status模块。只是tengine魔改了大量nginx的内部实现，导致大多数模块与nginx并不兼容。于是就有了这个项目hnlq715/status-nginx-module，完美支持nginx，并成功重构了当时上百台nginx，日请求量几十亿的的监控系统，此时能满足绝大多数监控需求。指标的主要维度仍然是域名。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2015年左右&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这时候注意到了prometheus的出现，感叹于其强大，于是考虑基于prometheus实现nginx的监控。而此时，vozlt/nginx-module-vts已经使用很广，于是创建了这个项目hnlq715/nginx-vts-exporter，用于将vts的输出内容转化为prometheus的格式，便于prometheus抓取。基于grafana提供监控查询及UI展示，这时候算是步入真正的现代化监控系统，此时的指标维度已经细化到具体的URI。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2017年左右&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然而上一个方案还是有个短板，那就是nginx-module-vts对响应时间的处理太过粗糙，只给了一个平均值，无法对P99，P90或是P50给出直观的数据。于是基于lua内嵌在nginx里跑监控指标，demo在这hnlq715/nginx-prometheus-metrics，这种方式可以直接用prometheus的histogram类型统计响应时间等指标，并在prometheus层聚合。此时，才算是真正的现代化监控系统。指标维度可以完全自定义，且更加多维化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;基于prometheus聚合nginx的监控数据是非常好的选择。上述项目能够在一定程度上帮助实现，不用手写代码或是通过极少的lua代码实现一套现代化的nginx监控系统。当然，也可以聚合其他诸如redis、mysql、node等各类系统的监控数据到prometheus进行统一管理。&lt;/p&gt;

&lt;h1 id=&#34;监控&#34;&gt;监控&lt;/h1&gt;

&lt;h2 id=&#34;status&#34;&gt;status&lt;/h2&gt;

&lt;p&gt;原版的 NGINX 会在一个简单的状态页面上显示几个与服务器状态有关的基本指标，它们由你启用的 HTTP stub status module 所提供。&lt;/p&gt;

&lt;p&gt;你可以浏览状态页看到你的指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Active connections: 24 
server accepts handled requests
1156958 1156958 4491319
Reading: 0 Writing: 18 Waiting : 6 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下表是 Nginx 提供的监控参数及其简单释义。&lt;/p&gt;

&lt;p&gt;参数名称    参数描述&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Active connections  当前活跃的用户连接(包含Waiting状态)
accepts 接收到的用户连接总数
handled Nginx处理的用户连接总数
requests    用户请求总数
Reading 当前连接中Nginx读取请求首部的个数
Writing 当前连接中Nginx写返回给用户的个数
Waiting 当前没有请求的活跃用户连接数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些提供了我们简单的指标。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当用户请求连接Nginx服务器时，accepts计数器会加一。且当服务器处理该连接请求时，handled计数器同样会加一。一般而言，两者的值是相等的，除非达到了某些资源极限（如worker_connection的限制）。&lt;/li&gt;
&lt;li&gt;用户连接请求被处理，就会进入 active 状态。如果该连接没有其他 request，则进入 waiting 的子状态；如果有 request，nginx 会读取 request 的 header，计数器 request 加一，进入 reading 的子状态。 reading 状态持续时间非常短，header 被读取后就会进入 writing 状态。事实上，直到服务器将响应结果返回给用户之前，该连接会一直保持 writing 状态。所以说，writing 状态一般会被长时间占用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;监控需求&#34;&gt;监控需求&lt;/h2&gt;

&lt;p&gt;三类指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;基本活动指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Accepts（接受）、Handled（已处理）、Requests（请求数）是一直在增加的计数器。Active（活跃）、Waiting（等待）、Reading（读）、Writing（写）随着请求量而增减。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;提醒指标: 丢弃连接
被丢弃的连接数目等于 Accepts 和 Handled 之差（NGINX 中），或是可直接得到的标准指标（NGINX Plus 中）。在正常情况下，丢弃连接数应该是零。如果在每个单位时间内丢弃连接的速度开始上升，那么应该看看是否资源饱和了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;提醒指标: 每秒请求数
按固定时间间隔采样你的请求数据（开源 NGINX 的requests或者 NGINX Plus 中total） 会提供给你单位时间内（通常是分钟或秒）所接受的请求数量。监测这个指标可以查看进入的 Web 流量尖峰，无论是合法的还是恶意的，或者突然的下降，这通常都代表着出现了问题。每秒请求数若发生急剧变化可以提醒你的环境出现问题了，即使它不能告诉你确切问题的位置所在。请注意，所有的请求都同样计数，无论 URL 是什么。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;错误指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NGINX 错误指标告诉你服务器是否经常返回错误而不是正常工作。客户端错误返回4XX状态码，服务器端错误返回5XX状态码。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;提醒指标: 服务器错误率&lt;/p&gt;

&lt;p&gt;服务器错误率等于在单位时间（通常为一到五分钟）内5xx错误状态代码的总数除以状态码（1XX，2XX，3XX，4XX，5XX）的总数。如果你的错误率随着时间的推移开始攀升，调查可能的原因。如果突然增加，可能需要采取紧急行动，因为客户端可能收到错误信息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;收集错误度量
配置 NGINX 的日志模块将响应码写入访问日志&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;性能指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;提醒指标: 请求处理时间
请求处理时间指标记录了 NGINX 处理每个请求的时间，从读到客户端的第一个请求字节到完成请求。较长的响应时间说明问题在上游。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;

&lt;p&gt;目前prometheus还没有官方的exporter。目前有两种采集的办法&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;nginx-lua-prometheus&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个是以lua插件的形式暴露出一些基础连接信息&lt;/p&gt;

&lt;p&gt;下载地址：&lt;a href=&#34;https://github.com/knyar/nginx-lua-prometheus&#34;&gt;https://github.com/knyar/nginx-lua-prometheus&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;nginx-lua-prometheus测试&lt;/p&gt;

&lt;p&gt;解压源码包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf nginx-1.6.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开始进行编译。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test nginx-1.6.0]# pwd
/opt/nginx-1.6.0
[root@test nginx-1.6.0]# ./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-http_realip_module --add-module=./nginx-sticky-module-1.1 --add-module=./nginx_upstream_check_module-master --add-module=./nginx_upstream_hash-0.3.1 --add-module=./lua-nginx-module-0.9.10 --add-module=./nginx-concat-module
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看编译后安装的模块&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test sbin]# ./nginx -V
nginx version: nginx/1.6.0
built by gcc 4.1.2 20080704 (Red Hat 4.1.2-48)
TLS SNI support disabled
configure arguments: --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-http_realip_module --add-module=./nginx-sticky-module-1.1 --add-module=./nginx_upstream_check_module-master --add-module=./nginx_upstream_hash-0.3.1 --add-module=./lua-nginx-module-0.9.10 --add-module=./nginx-concat-module
[root@test sbin]# pwd
/usr/local/nginx/sbin
[root@test sbin]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后make &amp;amp; make install&lt;/p&gt;

&lt;p&gt;启动nginx&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test sbin]# ./nginx
[root@test sbin]# ps -ef |grep nginx
root     15585     1  0 15:45 ?        00:00:00 nginx: master process ./nginx
nobody   15586 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15587 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15588 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15589 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15590 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15591 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15592 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15593 15585  0 15:45 ?        00:00:00 nginx: worker process
root     15597  3959  0 15:45 pts/2    00:00:00 grep nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问&lt;/p&gt;

&lt;p&gt;开始安装对应监控模块&lt;/p&gt;

&lt;p&gt;下载最新的release版本nginx-lua-prometheus-0.1-20170610.tar.gz&lt;/p&gt;

&lt;p&gt;创建目录&lt;/p&gt;

&lt;p&gt;/usr/local/nginx/lua&lt;/p&gt;

&lt;p&gt;上传gz包，并解压&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test nginx-lua-prometheus-0.1-20170610]# pwd
/usr/local/nginx/lua/nginx-lua-prometheus-0.1-20170610
[root@test nginx-lua-prometheus-0.1-20170610]# ls
nginx-lua-prometheus-0.1-20170610.rockspec  prometheus.lua  prometheus_test.lua  README.md
[root@test nginx-lua-prometheus-0.1-20170610]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改nginx.conf&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lua_shared_dict prometheus_metrics 10M;
lua_package_path &amp;quot;/usr/local/nginx/lua/nginx-lua-prometheus-0.1-20170610/?.lua&amp;quot;;
init_by_lua &#39;
  prometheus = require(&amp;quot;prometheus&amp;quot;).init(&amp;quot;prometheus_metrics&amp;quot;)
  metric_requests = prometheus:counter(
    &amp;quot;nginx_http_requests_total&amp;quot;, &amp;quot;Number of HTTP requests&amp;quot;, {&amp;quot;host&amp;quot;, &amp;quot;status&amp;quot;})
  metric_latency = prometheus:histogram(
    &amp;quot;nginx_http_request_duration_seconds&amp;quot;, &amp;quot;HTTP request latency&amp;quot;, {&amp;quot;host&amp;quot;})
  metric_connections = prometheus:gauge(
    &amp;quot;nginx_http_connections&amp;quot;, &amp;quot;Number of HTTP connections&amp;quot;, {&amp;quot;state&amp;quot;})
&#39;;
log_by_lua &#39;
  local host = ngx.var.host:gsub(&amp;quot;^www.&amp;quot;, &amp;quot;&amp;quot;)
  metric_requests:inc(1, {host, ngx.var.status})
  metric_latency:observe(ngx.now() - ngx.req.start_time(), {host})
&#39;;

server {
  listen 9145;
  #allow 0.0.0.0/16;
   #deny all;
  location /metrics {
    content_by_lua &#39;
      metric_connections:set(ngx.var.connections_reading, {&amp;quot;reading&amp;quot;})
      metric_connections:set(ngx.var.connections_waiting, {&amp;quot;waiting&amp;quot;})
      metric_connections:set(ngx.var.connections_writing, {&amp;quot;writing&amp;quot;})
      prometheus:collect()
    &#39;;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新加载nginx&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test conf]# ../sbin/nginx -s reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问nginx的页面&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://10.19.250.191:9145/metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;nginx-vts-exporter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;目前的版本是nginx-vts-exporter-0.8.3.linux-amd64.tar.gz&lt;/p&gt;

&lt;p&gt;下载地址是：&lt;a href=&#34;https://github.com/hnlq715/nginx-vts-exporter/releases&#34;&gt;https://github.com/hnlq715/nginx-vts-exporter/releases&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;nginx-vts-exporter测试&lt;/p&gt;

&lt;p&gt;由于nginx-vts-exporter依赖于Nginx的nginx-module-vts模块，所以这里需要重新编译下Nginx。之后再安装这个exporter&lt;/p&gt;

&lt;p&gt;VTS安装步骤&lt;/p&gt;

&lt;p&gt;1、  下载nginx-module-vts&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://github.com/vozlt/nginx-module-vts/releases/tag/v0.1.15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解压后目录为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/nginx-module-vts-0.1.15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、  重新编译nginx&lt;/p&gt;

&lt;p&gt;说明 由于 nginx_upstream_check_module-master 模块有问题。&lt;/p&gt;

&lt;p&gt;所以编译的时候的配置语句为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./configure  --with-http_stub_status_module --with-http_ssl_module --with-http_realip_module --add-module=./nginx-sticky-module-1.1  --add-module=./lua-nginx-module-0.9.10  --add-module=./nginx-concat-module   --add-module=/opt/nginx-module-vts-0.1.15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、  开始安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make &amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、  修改配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http {
    vhost_traffic_status_zone;

    ...

server {
      listen 8088;
      location /status {
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、  重新加载配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test conf]# ../sbin/nginx -s reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问页面&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://10.19.250.191:8088/status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/server/20190315.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见，vts的本身监控就是比较全面的，就是为监控而生。&lt;/p&gt;

&lt;p&gt;安装启动探针&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test nginx-vts-exporter-0.8.3.linux-amd64]# nohup /opt/nginx-vts-exporter-0.8.3.linux-amd64/./nginx-vts-exporter  -nginx.scrape_uri=&amp;quot;http://10.19.250.191:8088/status/format/json&amp;quot; 2&amp;gt;&amp;amp;1 &amp;amp;

[root@test nginx-vts-exporter-0.8.3.linux-amd64]# nohup: appending output to `nohup.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置访问&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -XPUT http://10.27.136.227:9996/v1/agent/service/register   -d &#39;
{
    &amp;quot;id&amp;quot;: &amp;quot;prometheus-exporter11&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;promether-exporter&amp;quot;,
    &amp;quot;address&amp;quot;: &amp;quot;10.19.250.191&amp;quot;,
    &amp;quot;port&amp;quot;: 9913,
    &amp;quot;tags&amp;quot;: [
          &amp;quot;SNMON&amp;quot;,
                &amp;quot;NJXZ&amp;quot;,
                &amp;quot;DEV&amp;quot;,
                &amp;quot;10.19.250.191&amp;quot;,
                &amp;quot;nginx-9913&amp;quot;
    ],
    &amp;quot;checks&amp;quot;: [
        {
            &amp;quot;script&amp;quot;: &amp;quot;curl http://10.19.250.9913/metrics &amp;gt;/dev/null 2&amp;gt;&amp;amp;1&amp;quot;,
            &amp;quot;interval&amp;quot;: &amp;quot;10s&amp;quot;
        }
    ]
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://10.27.136.227:9099/targets
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看Metric&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://10.19.250.191:9913/metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;综合来说，vts的指标本来就比较全面，结合prometheus，更加匹配我们的监控需求，但是要重新编译nginx比较麻烦，最终还是使用vts。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- prometheus监控方案</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/monitor-scheme/</link>
          <pubDate>Wed, 13 Jun 2018 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/monitor-scheme/</guid>
          <description>&lt;p&gt;prometheus生态可以构建一个完整的监控平台，包括采集数据、分析存储数据、展示数据、告警等一系列操作，我们来看看他在原始的基础设施监控和新兴的容器监控中如何架构落地。&lt;/p&gt;

&lt;h1 id=&#34;infrastructure监控实现&#34;&gt;Infrastructure监控实现&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/infrastructure-base/&#34;&gt;KVM监控方案&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;k8s监控实现&#34;&gt;k8s监控实现&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s-base/&#34;&gt;容器监控方案&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Operator</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/</link>
          <pubDate>Tue, 12 Jun 2018 16:57:05 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/</guid>
          <description>&lt;p&gt;Prometheus-Operator是一套为了方便整合prometheus和kubernetes的开源方案，使用Prometheus-Operator可以非常简单的在kubernetes集群中部署Prometheus服务，用户能够使用简单的声明性配置来配置和管理Prometheus实例，这些配置将响应、创建、配置和管理Prometheus监控实例。&lt;/p&gt;

&lt;h1 id=&#34;operator&#34;&gt;operator&lt;/h1&gt;

&lt;p&gt;Operator是由CoreOS公司开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的一些专业知识，比如创建一个数据库的Operator，则必须对创建的数据库的各种运维方式非常了解，创建Operator的关键是CRD（自定义资源）的设计。&lt;/p&gt;

&lt;p&gt;CRD是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在YAML文件里定义的那些spec都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。&lt;/p&gt;

&lt;p&gt;Operator是将运维人员对软件操作的知识给代码化，同时利用 Kubernetes 强大的抽象来管理大规模的软件应用。目前CoreOS官方提供了几种Operator的实现，其中就包括我们今天的主角：Prometheus Operator，Operator的核心实现就是基于 Kubernetes 的以下两个概念：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;资源：对象的状态定义&lt;/li&gt;
&lt;li&gt;控制器：观测、分析和行动，以调节资源的分布&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然我们如果有对应的需求也完全可以自己去实现一个Operator，接下来我们就来给大家详细介绍下Prometheus-Operator的使用方法&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;基本架构&#34;&gt;基本架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/operator/operator.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、Operator： 根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心，也就是我们常用的控制器，可见operater让prometheus更加k8s。&lt;/p&gt;

&lt;p&gt;2、Prometheus：声明 Prometheus deployment 期望的状态，Operator 确保这个 deployment 运行时一直与定义保持一致。这边是一个资源类型，和下一个具体的prometheus是有区别的。&lt;/p&gt;

&lt;p&gt;3、Prometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。&lt;/p&gt;

&lt;p&gt;4、ServiceMonitor：声明指定监控的服务，描述了一组被 Prometheus 监控的目标列表。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。&lt;/p&gt;

&lt;p&gt;5、Service：简单的说就是 Prometheus 监控的对象。&lt;/p&gt;

&lt;p&gt;6、Alertmanager：定义 AlertManager deployment 期望的状态，Operator 确保这个 deployment 运行时一直与定义保持一致。&lt;/p&gt;

&lt;p&gt;这边涉及来operater定义的几种crd类型。&lt;/p&gt;

&lt;h2 id=&#34;crd&#34;&gt;CRD&lt;/h2&gt;

&lt;p&gt;Prometheus Operater 定义了如下的六类自定义资源（CRD）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Prometheus：部署prometheus
ServiceMonitor：服务发现拉去列表基于service
Alertmanager：部署alertmanager
PrometheusRule：告警规则
ThanosRuler：部署thanos
PodMonitor：服务发现拉去列表基于pod
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;prometheus&#34;&gt;Prometheus&lt;/h3&gt;

&lt;p&gt;Prometheus 自定义资源（CRD）声明了在 Kubernetes 集群中运行的 Prometheus 的期望设置。包含了副本数量，持久化存储，以及 Prometheus 实例发送警告到的 Alertmanagers等配置选项。&lt;/p&gt;

&lt;p&gt;每一个 Prometheus 资源，Operator 都会在相同 namespace 下部署成一个正确配置的 StatefulSet，Prometheus 的 Pod 都会挂载一个名为 &lt;prometheus-name&gt; 的 Secret，里面包含了 Prometheus 的配置。Operator 根据包含的 ServiceMonitor 生成配置，并且更新含有配置的 Secret。无论是对 ServiceMonitors 或者 Prometheus 的修改，都会持续不断的被按照前面的步骤更新。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Prometheus
metadata: # 略
spec:
  alerting:
    alertmanagers:
    - name: prometheus-prometheus-oper-alertmanager # 定义该 Prometheus 对接的 Alertmanager 集群的名字, 在 default 这个 namespace 中
      namespace: default
      pathPrefix: /
      port: web
  baseImage: quay.io/prometheus/prometheus
  replicas: 2 # 定义该 Proemtheus “集群”有两个副本，说是集群，其实 Prometheus 自身不带集群功能，这里只是起两个完全一样的 Prometheus 来避免单点故障
  ruleSelector: # 定义这个 Prometheus 需要使用带有 prometheus=k8s 且 role=alert-rules 标签的 PrometheusRule
    matchLabels:
      prometheus: k8s
      role: alert-rules
  serviceMonitorNamespaceSelector: {} # 定义这些 Prometheus 在哪些 namespace 里寻找 ServiceMonitor
  serviceMonitorSelector: # 定义这个 Prometheus 需要使用带有 k8s-app=node-exporter 标签的 ServiceMonitor，不声明则会全部选中
    matchLabels:
      k8s-app: node-exporter
  version: v2.10.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;servicemonitor&#34;&gt;ServiceMonitor&lt;/h3&gt;

&lt;p&gt;ServiceMonitor 自定义资源(CRD)能够声明如何监控一组动态服务的定义。它使用标签选择定义一组需要被监控的服务。主要通过Selector来依据 Labels 选取对应的Service的endpoints，并让 Prometheus Server 通过 Service 进行拉取指标。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: ServiceMonitor
metadata:
  labels:
    k8s-app: node-exporter # 这个 ServiceMonitor 对象带有 k8s-app=node-exporter 标签，因此会被 Prometheus 选中
  name: node-exporter
  namespace: default
spec:
  selector:
    matchLabels: # 定义需要监控的 Endpoints，带有 app=node-exporter 且 k8s-app=node-exporter标签的 Endpoints 会被选中
      app: node-exporter
      k8s-app: node-exporter
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    interval: 30s # 定义这些 Endpoints 需要每 30 秒抓取一次
    targetPort: 9100 # 定义这些 Endpoints 的指标端口为 9100
    scheme: https
  jobLabel: k8s-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spec 的 endpoints 部分用于配置需要收集 metrics 的 Endpoints 的端口和其他参数。endpoints（小写）是 ServiceMonitor CRD 中的一个字段，而 Endpoints（大写）是 Kubernetes 资源类型。&lt;/p&gt;

&lt;p&gt;Spec 下的 namespaceSelector 可以现在允许发现 Endpoints 对象的命名空间。要发现所有命名空间下的目标，namespaceSelector 必须为空。&lt;/p&gt;

&lt;h3 id=&#34;alertmanager&#34;&gt;Alertmanager&lt;/h3&gt;

&lt;p&gt;Alertmanager 自定义资源(CRD)声明在 Kubernetes 集群中运行的 Alertmanager 的期望设置。它也提供了配置副本集和持久化存储的选项。&lt;/p&gt;

&lt;p&gt;每一个 Alertmanager 资源，Operator 都会在相同 namespace 下部署成一个正确配置的 StatefulSet。Alertmanager pods 配置挂载一个名为 &lt;alertmanager-name&gt; 的 Secret， 使用 alertmanager.yaml key 对作为配置文件。&lt;/p&gt;

&lt;p&gt;当有两个或更多配置的副本时，Operator 可以高可用性模式运行Alertmanager实例。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Alertmanager #  一个 Alertmanager 对象
metadata:
  name: prometheus-prometheus-oper-alertmanager
spec:
  baseImage: quay.io/prometheus/alertmanager
  replicas: 3      # 定义该 Alertmanager 集群的节点数为 3
  version: v0.17.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;prometheusrule&#34;&gt;PrometheusRule&lt;/h3&gt;

&lt;p&gt;PrometheusRule CRD 声明一个或多个 Prometheus 实例需要的 Prometheus rule。&lt;/p&gt;

&lt;p&gt;Alerts 和 recording rules 可以保存并应用为 yaml 文件，可以被动态加载而不需要重启。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PrometheusRule
metadata:
  labels: # 定义该 PrometheusRule 的 label, 显然它会被 Prometheus 选中
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
spec:
  groups:
  - name: k8s.rules
    rules: # 定义了一组规则，其中只有一条报警规则，用来报警 kubelet 是不是挂了
    - alert: KubeletDown
      annotations:
        message: Kubelet has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job=&amp;quot;kubelet&amp;quot;} == 1)
      for: 15m
      labels:
        severity: critical
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;thanosruler&#34;&gt;ThanosRuler&lt;/h3&gt;

&lt;p&gt;ThanosRuler 自定义资源(CRD)声明在 Kubernetes 集群中运行的 thanos 的期望设置。它也提供了配置副本集和持久化存储的选项。&lt;/p&gt;

&lt;h3 id=&#34;podmonitor&#34;&gt;PodMonitor&lt;/h3&gt;

&lt;p&gt;直接对接pod。&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;h2 id=&#34;部署prometheus-operater&#34;&gt;部署prometheus-operater&lt;/h2&gt;

&lt;p&gt;operater安装直接使用yaml安装就好了，先clone项目&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/coreos/prometheus-operator.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在项目目录下有一个bundle.yaml定义了各种crd资源和operater的镜像启动配置清单，直接运行就好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:prometheus-operator chunyinjiang$ kubectl apply -f bundle.yaml
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
serviceaccount/prometheus-operator created
service/prometheus-operator created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见在default的namespace下创建了prometheus-operator的sa，service，deployment应用，还有授权role以及CRD。&lt;/p&gt;

&lt;p&gt;最新的版本官方将资源&lt;a href=&#34;https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus&#34;&gt;https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus&lt;/a&gt; 迁移到了独立的git仓库中：&lt;a href=&#34;https://github.com/coreos/kube-prometheus.git，&#34;&gt;https://github.com/coreos/kube-prometheus.git，&lt;/a&gt; 我们也可以直接使用这里面setup的yaml文件来部署prometheus-operater，这个项目中还有prometheus相关生态的部署yaml，可以参考使用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/coreos/kube-prometheus.git
cd manifests/setup
$ ls
00namespace-namespace.yaml                                         node-exporter-clusterRole.yaml
0prometheus-operator-0alertmanagerCustomResourceDefinition.yaml    node-exporter-daemonset.yaml
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后直接部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:setup chunyinjiang$ kubectl apply -f .
namespace/monitoring created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
service/prometheus-operator created
serviceaccount/prometheus-operator created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get all -n monitoring
NAME                                       READY   STATUS    RESTARTS   AGE
pod/prometheus-operator-6f98f66b89-dggn6   2/2     Running   0          20h

NAME                          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
service/prometheus-operator   ClusterIP   None         &amp;lt;none&amp;gt;        8443/TCP   20h

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/prometheus-operator   1/1     1            1           20h

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/prometheus-operator-6f98f66b89   1         1         1       20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看crd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get crd | grep monitoring
alertmanagers.monitoring.coreos.com     2020-06-19T11:34:06Z
podmonitors.monitoring.coreos.com       2020-06-19T11:34:06Z
prometheuses.monitoring.coreos.com      2020-06-19T11:34:06Z
prometheusrules.monitoring.coreos.com   2020-06-19T11:34:06Z
servicemonitors.monitoring.coreos.com   2020-06-19T11:34:07Z
thanosrulers.monitoring.coreos.com      2020-06-19T11:34:07Z
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;部署prometheus生态&#34;&gt;部署prometheus生态&lt;/h2&gt;

&lt;p&gt;直接使用kube-prometheus的yaml进行部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl apply -f .
alertmanager.monitoring.coreos.com/main created
secret/alertmanager-main created
service/alertmanager-main created
serviceaccount/alertmanager-main created
servicemonitor.monitoring.coreos.com/alertmanager created
secret/grafana-datasources created
configmap/grafana-dashboard-apiserver created
configmap/grafana-dashboard-cluster-total created
configmap/grafana-dashboard-controller-manager created
configmap/grafana-dashboard-k8s-resources-cluster created
configmap/grafana-dashboard-k8s-resources-namespace created
configmap/grafana-dashboard-k8s-resources-node created
configmap/grafana-dashboard-k8s-resources-pod created
configmap/grafana-dashboard-k8s-resources-workload created
configmap/grafana-dashboard-k8s-resources-workloads-namespace created
configmap/grafana-dashboard-kubelet created
configmap/grafana-dashboard-namespace-by-pod created
configmap/grafana-dashboard-namespace-by-workload created
configmap/grafana-dashboard-node-cluster-rsrc-use created
configmap/grafana-dashboard-node-rsrc-use created
configmap/grafana-dashboard-nodes created
configmap/grafana-dashboard-persistentvolumesusage created
configmap/grafana-dashboard-pod-total created
configmap/grafana-dashboard-prometheus-remote-write created
configmap/grafana-dashboard-prometheus created
configmap/grafana-dashboard-proxy created
configmap/grafana-dashboard-scheduler created
configmap/grafana-dashboard-statefulset created
configmap/grafana-dashboard-workload-total created
configmap/grafana-dashboards created
deployment.apps/grafana created
service/grafana created
serviceaccount/grafana created
servicemonitor.monitoring.coreos.com/grafana created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
deployment.apps/kube-state-metrics created
service/kube-state-metrics created
serviceaccount/kube-state-metrics created
servicemonitor.monitoring.coreos.com/kube-state-metrics created
clusterrole.rbac.authorization.k8s.io/node-exporter created
clusterrolebinding.rbac.authorization.k8s.io/node-exporter created
daemonset.apps/node-exporter created
service/node-exporter created
serviceaccount/node-exporter created
servicemonitor.monitoring.coreos.com/node-exporter created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io configured
clusterrole.rbac.authorization.k8s.io/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created
clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created
clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created
configmap/adapter-config created
deployment.apps/prometheus-adapter created
rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created
service/prometheus-adapter created
serviceaccount/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/prometheus-k8s created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus-operator created
prometheus.monitoring.coreos.com/k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s-config created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
prometheusrule.monitoring.coreos.com/prometheus-k8s-rules created
service/prometheus-k8s created
serviceaccount/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus created
servicemonitor.monitoring.coreos.com/kube-apiserver created
servicemonitor.monitoring.coreos.com/coredns created
servicemonitor.monitoring.coreos.com/kube-controller-manager created
servicemonitor.monitoring.coreos.com/kube-scheduler created
servicemonitor.monitoring.coreos.com/kubelet created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get all -n monitoring
NAME                                       READY   STATUS              RESTARTS   AGE
pod/alertmanager-main-0                    0/2     ContainerCreating   0          3m16s
pod/alertmanager-main-1                    0/2     ContainerCreating   0          3m16s
pod/alertmanager-main-2                    0/2     ContainerCreating   0          3m16s
pod/grafana-5c55845445-7tdhk               0/1     ContainerCreating   0          3m15s
pod/kube-state-metrics-957fd6c75-sqntg     0/3     ContainerCreating   0          3m14s
pod/node-exporter-tnftm                    0/2     ContainerCreating   0          3m14s
pod/prometheus-adapter-5cdcdf9c8d-xpxz4    1/1     Running             0          3m15s
pod/prometheus-k8s-0                       0/3     ContainerCreating   0          3m13s
pod/prometheus-k8s-1                       0/3     ContainerCreating   0          3m13s
pod/prometheus-operator-6f98f66b89-dggn6   2/2     Running             0          20h

NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
service/alertmanager-main       ClusterIP   10.106.202.8    &amp;lt;none&amp;gt;        9093/TCP                     3m17s
service/alertmanager-operated   ClusterIP   None            &amp;lt;none&amp;gt;        9093/TCP,9094/TCP,9094/UDP   3m17s
service/grafana                 ClusterIP   10.98.82.99     &amp;lt;none&amp;gt;        3000/TCP                     3m16s
service/kube-state-metrics      ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP,9443/TCP            3m16s
service/node-exporter           ClusterIP   None            &amp;lt;none&amp;gt;        9100/TCP                     3m15s
service/prometheus-adapter      ClusterIP   10.98.119.241   &amp;lt;none&amp;gt;        443/TCP                      3m15s
service/prometheus-k8s          ClusterIP   10.104.199.30   &amp;lt;none&amp;gt;        9090/TCP                     3m14s
service/prometheus-operated     ClusterIP   None            &amp;lt;none&amp;gt;        9090/TCP                     3m15s
service/prometheus-operator     ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP                     20h

NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/node-exporter   1         1         0       1            0           kubernetes.io/os=linux   3m15s

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana               0/1     1            0           3m16s
deployment.apps/kube-state-metrics    0/1     1            0           3m16s
deployment.apps/prometheus-adapter    1/1     1            1           3m15s
deployment.apps/prometheus-operator   1/1     1            1           20h

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-5c55845445               1         1         0       3m16s
replicaset.apps/kube-state-metrics-957fd6c75     1         1         0       3m16s
replicaset.apps/prometheus-adapter-5cdcdf9c8d    1         1         1       3m15s
replicaset.apps/prometheus-operator-6f98f66b89   1         1         1       20h

NAME                                 READY   AGE
statefulset.apps/alertmanager-main   0/3     3m17s
statefulset.apps/prometheus-k8s      0/2     3m15s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到资源正在创建，拉去镜像可能需要一点事件，其中 alertmanager 和 prometheus 是用 StatefulSet 控制器管理的。&lt;/p&gt;

&lt;p&gt;可以看到上面针对 grafana 和 prometheus 都创建了一个类型为 ClusterIP 的 Service，当然如果我们想要在外网访问这两个服务的话可以通过创建对应的 Ingress 对象或者使用 NodePort 类型的 Service，我们这里为了简单，直接使用 NodePort 类型的服务即可，编辑 grafana 和 prometheus-k8s 这两个 Service，将服务类型更改为 NodePort:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type: ClusterIp   --&amp;gt; NodePort

MacBook-Pro:manifests chunyinjiang$ kubectl edit svc prometheus-k8s -n monitoring
service/prometheus-k8s edited
MacBook-Pro:manifests chunyinjiang$ kubectl edit svc grafana -n monitoring
service/grafana edited
MacBook-Pro:manifests chunyinjiang$ kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.106.85.214   &amp;lt;none&amp;gt;        9093/TCP                     95m
alertmanager-operated   ClusterIP   None            &amp;lt;none&amp;gt;        9093/TCP,9094/TCP,9094/UDP   95m
grafana                 NodePort    10.106.80.1     &amp;lt;none&amp;gt;        3000:30267/TCP               95m
kube-state-metrics      ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP,9443/TCP            95m
node-exporter           ClusterIP   None            &amp;lt;none&amp;gt;        9100/TCP                     95m
prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      95m
prometheus-k8s          NodePort    10.103.177.44   &amp;lt;none&amp;gt;        9090:31174/TCP               95m
prometheus-operated     ClusterIP   None            &amp;lt;none&amp;gt;        9090/TCP                     95m
prometheus-operator     ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP                     22h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更改完成后，我们就可以通过NodeIP:NodePort去访问上面的两个服务了，比如查看 prometheus 的 targets 页面：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/operator/operater.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;至此基本的prometheus生态组件就部署好了，但是可以看到kube-controller-manager 和 kube-scheduler 这两个系统组件并没有taeget，这就和 ServiceMonitor 的定义有关系了，我们刚好研究一下ServiceMonitor&lt;/p&gt;

&lt;p&gt;我们查看ServiceMonitor这种crd的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get servicemonitors.monitoring.coreos.com -n monitoring
NAME                      AGE
alertmanager              101m
coredns                   101m
grafana                   101m
kube-apiserver            101m
kube-controller-manager   101m
kube-scheduler            101m
kube-state-metrics        101m
kubelet                   101m
node-exporter             101m
prometheus                101m
prometheus-operator       101m
MacBook-Pro:manifests chunyinjiang$ kubectl describe servicemonitors.monitoring.coreos.com kube-scheduler -n monitoring
Name:         kube-scheduler
Namespace:    monitoring
Labels:       k8s-app=kube-scheduler
Annotations:  API Version:  monitoring.coreos.com/v1
Kind:         ServiceMonitor
Metadata:
  Creation Timestamp:  2020-06-20T08:04:50Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:kubectl.kubernetes.io/last-applied-configuration:
        f:labels:
          .:
          f:k8s-app:
      f:spec:
        .:
        f:endpoints:
        f:jobLabel:
        f:namespaceSelector:
          .:
          f:matchNames:
        f:selector:
          .:
          f:matchLabels:
            .:
            f:k8s-app:
    Manager:         kubectl
    Operation:       Update
    Time:            2020-06-20T08:04:50Z
  Resource Version:  862846
  Self Link:         /apis/monitoring.coreos.com/v1/namespaces/monitoring/servicemonitors/kube-scheduler
  UID:               07132145-1db1-4847-a2a1-347cc014a80e
Spec:
  Endpoints:
    Interval:  30s
    Port:      http-metrics
  Job Label:   k8s-app
  Namespace Selector:
    Match Names:
      kube-system
  Selector:
    Match Labels:
      k8s-app:  kube-scheduler
Events:         &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到每个监控的应用都是使用ServiceMonitor部署了。我们再来看看对应的资源配置清单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-serviceMonitorKubeScheduler.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: kube-scheduler
  name: kube-scheduler
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s
    port: http-metrics
  jobLabel: k8s-app
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      k8s-app: kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是一个典型的 ServiceMonitor 资源文件的声明方式，上面我们通过selector.matchLabels在 kube-system 这个命名空间下面匹配具有k8s-app=kube-scheduler这样的 Service，我们来看看service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc -n kube-system -L k8s-app
NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE    K8S-APP
kube-dns         ClusterIP   10.96.0.10      &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP         11d    kube-dns
kubelet          ClusterIP   None            &amp;lt;none&amp;gt;        10250/TCP,10255/TCP,4194/TCP   26h    kubelet
metrics-server   ClusterIP   10.111.196.64   &amp;lt;none&amp;gt;        443/TCP                        2d6h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是我们系统中根本就没有对应的 Service，所以我们需要手动创建一个 Service：（prometheus-kubeSchedulerService.yaml）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-scheduler
  labels:
    k8s-app: kube-scheduler
spec:
  selector:
    component: kube-scheduler
  ports:
  - name: http-metrics
    port: 10251
    targetPort: 10251
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10251是kube-scheduler组件 metrics 数据所在的端口，10252是kube-controller-manager组件的监控数据所在端口。&lt;/p&gt;

&lt;p&gt;其中最重要的是上面 labels 和 selector 部分，labels 区域的配置必须和我们上面的 ServiceMonitor 对象中的 selector 保持一致，selector下面配置的是component=kube-scheduler，为什么会是这个 label 标签呢，我们可以去 describe 下 kube-scheduelr 这个 Pod：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pod kube-scheduler-minikube -n kube-system
Name:                 kube-scheduler-minikube
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 minikube/192.168.99.101
Start Time:           Sat, 20 Jun 2020 17:25:48 +0800
Labels:               component=kube-scheduler
                      tier=control-plane
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到这个 Pod 具有component=kube-scheduler和tier=control-plane这两个标签，而前面这个标签具有更唯一的特性，所以使用前面这个标签较好，这样上面创建的 Service 就可以和我们的 Pod 进行关联了，直接创建即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl apply -f prometheus-kubeSchedulerService.yaml
service/kube-scheduler created
MacBook-Pro:manifests chunyinjiang$ kubectl get svc -n kube-system -L k8s-app
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                        AGE    K8S-APP
kube-dns         ClusterIP   10.96.0.10       &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP         14d    kube-dns
kube-scheduler   ClusterIP   10.105.229.159   &amp;lt;none&amp;gt;        10251/TCP                      3m5s   kube-scheduler
kubelet          ClusterIP   None             &amp;lt;none&amp;gt;        10250/TCP,10255/TCP,4194/TCP   4d1h   kubelet
metrics-server   ClusterIP   10.111.196.64    &amp;lt;none&amp;gt;        443/TCP                        5d4h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 targets 下面 kube-scheduler 的状态，可以看到已经发现了，并且有数据了。&lt;/p&gt;

&lt;p&gt;kube-controller-manager 也是一样的操作。下面我们就可以通过自定义的grafana视图来进行监控了。&lt;/p&gt;

&lt;h2 id=&#34;部署详情&#34;&gt;部署详情&lt;/h2&gt;

&lt;h3 id=&#34;prometheus-1&#34;&gt;prometheus&lt;/h3&gt;

&lt;p&gt;prometheus的所有信息都能重prometheus的ui界面进行查看，主要查看status的状态，我们重容器中查看一下。&lt;/p&gt;

&lt;p&gt;上面我们知道使用有状态的statefulset部署两个prometheus，我们来看一下他们的具体情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get pod -n monitoring | grep prometheus-k8s
prometheus-k8s-0                       3/3     Running   17         3d1h
prometheus-k8s-1                       3/3     Running   17         3d1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个pod中有三个容器，可以使用 kubectl describe pod prometheus-k8s-0 -n monitoring来查看这三个容器分别是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus：quay.io/prometheus/prometheus:v2.17.2
prometheus-config-reloader：quay.io/coreos/prometheus-config-reloader:v0.39.0
rules-configmap-reloader：jimmidyson/configmap-reload:v0.3.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们到prometheus中看看，可以理解这个就是启动了prometheus的实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl exec -ti prometheus-k8s-0 -c prometheus -n monitoring -- sh
/prometheus $ ps -ef | grep prome
    1 1000     11:13 /bin/prometheus --web.console.templates=/etc/prometheus/consoles --web.console.libraries=/etc/prometheus/console_libraries --config.file=/etc/prometheus/config_out/prometheus.env.yaml --storage.tsdb.path=/prometheus --storage.tsdb.retention.time=24h --web.enable-lifecycle --storage.tsdb.no-lockfile --web.route-prefix=/
   52 1000      0:00 grep prome
/prometheus $ cat /etc/prometheus/config_out/prometheus.env.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在节点中可以看到&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以看到每个job就是监控的一个组件，也就是serviceMonitor。主要监听组件alertmanager，coredns，grafana，kube-apiserver，kube-controller-manager，kube-scheduler，kube-state-metrics，kubelet，node-exporter，prometheus，prometheus-operator。&lt;/li&gt;
&lt;li&gt;在配置文件中并没有使用hash的模式来分集群进行采集，这边两个prometheus节点是双采，解决来单点问题&lt;/li&gt;
&lt;li&gt;使用的是kubernetes_sd_configs的服务发现模式&lt;/li&gt;
&lt;li&gt;数据存储24h，存储在/prometheus目录下&lt;/li&gt;
&lt;li&gt;监听端口9090，我们可以使用nodeport或者ingress来对外暴露&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个简单的部署只适合简单的小集群的使用，使用大集群的监控还需要将数据进行分片采集，远程存储聚合等方案。&lt;/p&gt;

&lt;p&gt;我们再看看config的container是做什么的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti prometheus-k8s-0 -c prometheus-config-reloader -n monitoring -- sh
/ $ ps -ef
PID   USER     TIME  COMMAND
    1 1000      0:01 /bin/prometheus-config-reloader --log-format=logfmt --reload-url=http://localhost:9090/-/reload --config-file=/etc/prometheus/config/prometheus.yaml.gz --config-envsubst-file=/etc/prometheus/config_out/pro
$ kubectl exec -ti prometheus-k8s-0 -c rules-configmap-reloader -n monitoring -- sh
/ $ ps -ef
\PID   USER     TIME  COMMAND
    1 1000      0:00 /configmap-reload --webhook-url=http://localhost:9090/-/reload --volume-dir=/etc/prometheus/rules/prometheus-k8s-rulefiles-0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实就是对配置文件和rule文件进行热加载。&lt;/p&gt;

&lt;h3 id=&#34;grafana&#34;&gt;grafana&lt;/h3&gt;

&lt;p&gt;grafana也是直接在k8s中用deployment进行部署的，只有一个节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod -n monitoring | grep grafana
grafana-5c55845445-bnln8               1/1     Running   2          3d3h
MacBook-Pro:manifests chunyinjiang$ kubectl exec -ti grafana-5c55845445-bnln8 -n monitoring -- sh
/usr/share/grafana $ ps -ef | grep grafana
    1 nobody    4:36 grafana-server --homepath=/usr/share/grafana --config=/etc/grafana/grafana.ini --packaging=docker cfg:default.log.mode=console cfg:default.paths.data=/var/lib/grafana cfg:default.paths.logs=/var/log/grafana cfg:default.paths.plugins=/var/lib/grafana/plugins cfg:default.paths.provisioning=/etc/grafana/provisioning
   30 nobody    0:00 grep grafana
/usr/share/grafana $ cat /etc/grafana/grafana.ini
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到一些内容&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;监听端口3000，我们可以使用nodeport或者ingress来对外暴露&lt;/li&gt;
&lt;li&gt;没有使用mysql数据库，使用sqlite数据库&lt;/li&gt;
&lt;li&gt;直接通过域名访问prometheus：&lt;a href=&#34;http://prometheus-k8s.monitoring.svc:9090&#34;&gt;http://prometheus-k8s.monitoring.svc:9090&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;alertmanager-1&#34;&gt;alertmanager&lt;/h3&gt;

&lt;p&gt;alertmanager使用的也是statefulset的方式进行部署的，我们看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get pod -n monitoring | grep alert
alertmanager-main-0                    2/2     Running   6          3d3h
alertmanager-main-1                    2/2     Running   7          3d3h
alertmanager-main-2                    2/2     Running   5          3d3h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个pod也有两个container。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;alertmanager：quay.io/prometheus/alertmanager:v0.20.0
config-reloader：jimmidyson/configmap-reload:v0.3.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;config-reloader肯定就是配置加载，我们看看alertmanager的实例吧&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti alertmanager-main-0 -c alertmanager -n monitoring -- sh
/alertmanager $ ps
PID   USER     TIME  COMMAND
    1 1000      2:12 /bin/alertmanager --config.file=/etc/alertmanager/config/alertmanager.yaml --cluster.listen-address=[172.17.0.35]:9094 --storage.path=/alertmanager --data.retention=120h --web.listen-address=:9093 --web.ro
   21 1000      0:00 sh
   26 1000      0:00 ps
/alertmanager $ cat /etc/alertmanager/config/alertmanager.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;adapter&#34;&gt;adapter&lt;/h3&gt;

&lt;p&gt;kubernetes apiserver 提供了两种 api 用于监控指标相关的操作，&lt;/p&gt;

&lt;p&gt;k8s-prometheus-adapter是将prometheus的metrics数据格式转换成k8s API接口能识别的格式，同时通过apiservice扩展的模式（声明apiservice）注册到kube-apiserver来给k8s进行调用。&lt;/p&gt;

&lt;p&gt;查看adapter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get all -n monitoring | grep adapter
pod/prometheus-adapter-66b9c9dd58-6bdbm    1/1     Running   0          14h
service/prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      8d
deployment.apps/prometheus-adapter    1/1     1            1           8d
replicaset.apps/prometheus-adapter-5cdcdf9c8d    0         0         0       8d
replicaset.apps/prometheus-adapter-66b9c9dd58    1         1         1       14h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用deployment来部署了两个副本的k8s-prometheus-adapter，然后启动了一个service。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ps -ef | grep adapter | grep -v grep
dbus     23281 23263  0 00:13 ?        00:00:20 /adapter --cert-dir=/var/run/serving-cert --config=/etc/adapter/config.yaml --logtostderr=true --metrics-relist-interval=1m --prometheus-url=http://192.168.99.101:31174/ --secure-port=6443
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--lister-kubeconfig=&amp;lt;path-to-kubeconfig&amp;gt;: 指定通信的kubeconfig
--metrics-relist-interval=&amp;lt;duration&amp;gt;: 获取指标的间隔，应该大于prometheus的采集间隔时间。
--prometheus-url=&amp;lt;url&amp;gt;: 连接到Prometheus的URL。
--config=&amp;lt;yaml-file&amp;gt; (-c): 配置文件，主要是Prometheus指标和关联的Kubernetes资源，以及如何在自定义指标API中显示这些指标。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再来看看配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rules:
- seriesQuery: &#39;nginx_vts_server_requests_total&#39;
  seriesFilters: []
  resources:
    overrides:
      kubernetes_namespace:
        resource: namespace
      kubernetes_pod_name:
        resource: pod
  name:
    matches: &amp;quot;^(.*)_total&amp;quot;
    as: &amp;quot;${1}_per_second&amp;quot;
  metricsQuery: (sum(rate(&amp;lt;&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}[1m])) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个带参数的 Prometheus 查询，其中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;seriesQuery：查询 Prometheus 的语句，通过这个查询语句查询到的所有指标都可以用于 HPA
seriesFilters：查询到的指标可能会存在不需要的，可以通过它过滤掉。
resources：通过 seriesQuery 查询到的只是指标，如果需要查询某个 Pod 的指标，肯定要将它的名称和所在的命名空间作为指标的标签进行查询，resources 就是将指标的标签和 k8s 的资源类型关联起来，最常用的就是 pod 和 namespace。有两种添加标签的方式，一种是 overrides，另一种是 template。

    overrides：它会将指标中的标签和 k8s 资源关联起来。上面示例中就是将指标中的 pod 和 namespace 标签和 k8s 中的 pod 和 namespace 关联起来，因为 pod 和 namespace 都属于核心 api 组，所以不需要指定 api 组。当我们查询某个 pod 的指标时，它会自动将 pod 的名称和名称空间作为标签加入到查询条件中。比如 nginx: {group: &amp;quot;apps&amp;quot;, resource: &amp;quot;deployment&amp;quot;} 这么写表示的就是将指标中 nginx 这个标签和 apps 这个 api 组中的 deployment 资源关联起来；
    template：通过 go 模板的形式。比如template: &amp;quot;kube_&amp;lt;&amp;lt;.Group&amp;gt;&amp;gt;_&amp;lt;&amp;lt;.Resource&amp;gt;&amp;gt;&amp;quot; 这么写表示，假如 &amp;lt;&amp;lt;.Group&amp;gt;&amp;gt; 为 apps，&amp;lt;&amp;lt;.Resource&amp;gt;&amp;gt; 为 deployment，那么它就是将指标中 kube_apps_deployment 标签和 deployment 资源关联起来。

name：用来给指标重命名的，之所以要给指标重命名是因为有些指标是只增的，比如以 total 结尾的指标。这些指标拿来做 HPA 是没有意义的，我们一般计算它的速率，以速率作为值，那么此时的名称就不能以 total 结尾了，所以要进行重命名。

    matches：通过正则表达式来匹配指标名，可以进行分组
    as：默认值为 $1，也就是第一个分组。as 为空就是使用默认值的意思。

metricsQuery：这就是 Prometheus 的查询语句了，前面的 seriesQuery 查询是获得 HPA 指标。当我们要查某个指标的值时就要通过它指定的查询语句进行了。可以看到查询语句使用了速率和分组，这就是解决上面提到的只增指标的问题。

    Series：表示指标名称
    LabelMatchers：附加的标签，目前只有 pod 和 namespace 两种，因此我们要在之前使用 resources 进行关联
    GroupBy：就是 pod 名称，同样需要使用 resources 进行关联。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看一下项目的的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-adapter-configMap.yaml
apiVersion: v1
data:
  config.yaml: |-
    &amp;quot;resourceRules&amp;quot;:
      &amp;quot;cpu&amp;quot;:
        &amp;quot;containerLabel&amp;quot;: &amp;quot;container&amp;quot;
        &amp;quot;containerQuery&amp;quot;: &amp;quot;sum(irate(container_cpu_usage_seconds_total{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;,container!=\&amp;quot;POD\&amp;quot;,container!=\&amp;quot;\&amp;quot;,pod!=\&amp;quot;\&amp;quot;}[5m])) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;nodeQuery&amp;quot;: &amp;quot;sum(1 - irate(node_cpu_seconds_total{mode=\&amp;quot;idle\&amp;quot;}[5m]) * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;resources&amp;quot;:
          &amp;quot;overrides&amp;quot;:
            &amp;quot;namespace&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;namespace&amp;quot;
            &amp;quot;node&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;node&amp;quot;
            &amp;quot;pod&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;pod&amp;quot;
      &amp;quot;memory&amp;quot;:
        &amp;quot;containerLabel&amp;quot;: &amp;quot;container&amp;quot;
        &amp;quot;containerQuery&amp;quot;: &amp;quot;sum(container_memory_working_set_bytes{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;,container!=\&amp;quot;POD\&amp;quot;,container!=\&amp;quot;\&amp;quot;,pod!=\&amp;quot;\&amp;quot;}) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;nodeQuery&amp;quot;: &amp;quot;sum(node_memory_MemTotal_bytes{job=\&amp;quot;node-exporter\&amp;quot;,&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;} - node_memory_MemAvailable_bytes{job=\&amp;quot;node-exporter\&amp;quot;,&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;resources&amp;quot;:
          &amp;quot;overrides&amp;quot;:
            &amp;quot;instance&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;node&amp;quot;
            &amp;quot;namespace&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;namespace&amp;quot;
            &amp;quot;pod&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;pod&amp;quot;
      &amp;quot;window&amp;quot;: &amp;quot;5m&amp;quot;
kind: ConfigMap
metadata:
  name: adapter-config
  namespace: monitoring
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置了两个规则，可以到prometheus的界面查看一下是可以查到的，下面我们需要提供给k8s，我们就需要使用聚合api，先注册apiservice&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl api-versions  | grep metrics
metrics.k8s.io/v1beta1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们就可以重/apis/metrics.k8s.io/v1beta1这个URL来获取指标，这是给resource metrics 使用的,主要是提供核心指标，这边是指向k8s-prometheus-adapter，所以是prometheus的采集的指标。&lt;/p&gt;

&lt;p&gt;还有是自定义指标，只要是prometheus采集的指标，都可以在上面的配置文件配置，然后都可以通过这个接口查询到，这种情况其实一般使用custom.metrics.k8s.io api接口来操作，&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-autoscaler/#基于prometheus&#34;&gt;具体使用可以在hpa场景下查看&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get --raw=&amp;quot;/apis/metrics.k8s.io/v1beta1&amp;quot;
{
    &amp;quot;kind&amp;quot;:&amp;quot;APIResourceList&amp;quot;,
    &amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,
    &amp;quot;groupVersion&amp;quot;:&amp;quot;metrics.k8s.io/v1beta1&amp;quot;,
    &amp;quot;resources&amp;quot;:[
        {
            &amp;quot;name&amp;quot;:&amp;quot;nodes&amp;quot;,
            &amp;quot;singularName&amp;quot;:&amp;quot;&amp;quot;,
            &amp;quot;namespaced&amp;quot;:false,
            &amp;quot;kind&amp;quot;:&amp;quot;NodeMetrics&amp;quot;,
            &amp;quot;verbs&amp;quot;:[
                &amp;quot;get&amp;quot;,
                &amp;quot;list&amp;quot;
            ]
        },
        {
            &amp;quot;name&amp;quot;:&amp;quot;pods&amp;quot;,
            &amp;quot;singularName&amp;quot;:&amp;quot;&amp;quot;,
            &amp;quot;namespaced&amp;quot;:true,
            &amp;quot;kind&amp;quot;:&amp;quot;PodMetrics&amp;quot;,
            &amp;quot;verbs&amp;quot;:[
                &amp;quot;get&amp;quot;,
                &amp;quot;list&amp;quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看出来这个接口主要获取了核心资源指标，比如nodes，pods，可以具体去查看一下。&lt;/p&gt;

&lt;h3 id=&#34;其他组件&#34;&gt;其他组件&lt;/h3&gt;

&lt;p&gt;主要是给proemtheus的采集的探针&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kube-state-metrics主要是为了暴露集群的一些状态
node-exporter主要是获取主机信息
其他组件集成prometheus的库，通过端口直接暴露metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;servicemonitor-1&#34;&gt;serviceMonitor&lt;/h3&gt;

&lt;p&gt;将所有组件的监控通过serviceMonitor来暴露采集。&lt;/p&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;h2 id=&#34;自定义servicemonitor&#34;&gt;自定义servicemonitor&lt;/h2&gt;

&lt;p&gt;添加一个自定义监控的步骤&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;第一步建立一个 ServiceMonitor 对象，用于 Prometheus 添加监控项
第二步为 ServiceMonitor 对象关联 metrics 数据接口的一个 Service 对象
第三步确保 Service 对象可以正确获取到 metrics 数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们以监控etcd为实例&lt;/p&gt;

&lt;p&gt;etcd 集群一般情况下，为了安全都会开启 https 证书认证的方式，所以要想让 Prometheus 访问到 etcd 集群的监控数据，就需要提供相应的证书校验。查看证书&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pod etcd-minikube  -n kube-system
...
    Command:
      etcd
      --advertise-client-urls=https://192.168.99.101:2379
      --cert-file=/var/lib/minikube/certs/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/minikube/etcd
      --initial-advertise-peer-urls=https://192.168.99.101:2380
      --initial-cluster=minikube=https://192.168.99.101:2380
      --key-file=/var/lib/minikube/certs/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://192.168.99.101:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://192.168.99.101:2380
      --name=minikube
      --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/var/lib/minikube/certs/etcd/peer.key
      --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到证书都在/var/lib/minikube/certs/etcd/下面，我们也可以通过kubectl get pod etcd-minikube  -n kube-system -o yaml来获取对应的配置，我们再来看看这个目录下的证书&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -lrth /var/lib/minikube/certs/etcd/
total 32K
-rw------- 1 root root 1.7K Jun  9 01:38 ca.key
-rw-r--r-- 1 root root 1017 Jun  9 01:38 ca.crt
-rw------- 1 root root 1.7K Jun  9 01:38 server.key
-rw-r--r-- 1 root root 1.2K Jun  9 01:38 server.crt
-rw------- 1 root root 1.7K Jun  9 01:38 peer.key
-rw-r--r-- 1 root root 1.2K Jun  9 01:38 peer.crt
-rw------- 1 root root 1.7K Jun  9 01:38 healthcheck-client.key
-rw-r--r-- 1 root root 1.1K Jun  9 01:38 healthcheck-client.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们需要将证书放到secret中给promehteus使用验证，创建secret就需要把这些证书拉到本地来进行创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret generic etcd-certs -n monitoring --from-file=./healthcheck-client.crt --from-file=./healthcheck-client.key --from-file=./ca.crt
secret/etcd-certs created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将上面创建的 etcd-certs 对象配置到 prometheus 资源对象中，直接更新 prometheus 资源对象即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get prometheus -n monitoring
NAME   VERSION   REPLICAS   AGE
k8s    v2.17.2   2          3d22h
$ kubectl edit prometheus k8s -n monitoring
prometheus.monitoring.coreos.com/k8s edited
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要在spec中新增secret给prometheus使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nodeSelector:
  beta.kubernetes.io/os: linux
replicas: 2
secrets:
- etcd-certs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新完成后，我们就可以在 Prometheus 的 Pod 中获取到上面创建的 etcd 证书文件了，具体的路径我们可以进入 Pod 中查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -it prometheus-k8s-0 -c prometheus /bin/sh -n monitoring
/prometheus $ ls -lrth /etc/prometheus/secrets/etcd-certs/
total 0
lrwxrwxrwx    1 root     root          29 Jun 24 06:48 healthcheck-client.key -&amp;gt; ..data/healthcheck-client.key
lrwxrwxrwx    1 root     root          29 Jun 24 06:48 healthcheck-client.crt -&amp;gt; ..data/healthcheck-client.crt
lrwxrwxrwx    1 root     root          13 Jun 24 06:48 ca.crt -&amp;gt; ..data/ca.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面就是创建ServiceMonitor资源配置清单prometheus-serviceMonitorEtcd.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: etcd-k8s
  namespace: monitoring
  labels:
    k8s-app: etcd-k8s
spec:
  jobLabel: k8s-app
  endpoints:
  - port: port
    interval: 30s
    scheme: https
    tlsConfig:
      caFile: /etc/prometheus/secrets/etcd-certs/ca.crt
      certFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.crt
      keyFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.key
      insecureSkipVerify: true
  selector:
    matchLabels:
      k8s-app: etcd
  namespaceSelector:
    matchNames:
    - kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在 monitoring 命名空间下面创建了名为 etcd-k8s 的 ServiceMonitor 对象&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;匹配 kube-system 这个命名空间下面的具有 k8s-app=etcd 这个 label 标签的 Service&lt;/li&gt;
&lt;li&gt;jobLabel 表示用于检索 job 任务名称的标签&lt;/li&gt;
&lt;li&gt;和前面不太一样的地方是 endpoints 属性的写法，配置上访问 etcd 的相关证书，endpoints 属性下面可以配置很多抓取的参数，比如 relabel、proxyUrl，tlsConfig 表示用于配置抓取监控数据端点的 tls 认证，由于证书 serverName 和 etcd 中签发的可能不匹配，所以加上了 insecureSkipVerify=true&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后就是创建来这个资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f prometheus-serviceMonitorEtcd.yaml
servicemonitor.monitoring.coreos.com &amp;quot;etcd-k8s&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候promehteus的配置文件中就新增一个job为etcd的监控，target是etcd的service，但是现在还没有关联的对应的 Service 对象，所以需要我们去手动创建一个 Service 对象prometheus-etcdService.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: port
    port: 2379
    protocol: TCP

---
apiVersion: v1
kind: Endpoints
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
subsets:
- addresses:
  - ip: 10.151.30.57
    nodeName: etc-master
  ports:
  - name: port
    port: 2379
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们这里创建的 Service 没有采用前面通过 label 标签的形式去匹配 Pod 的做法，因为前面我们说过很多时候我们创建的 etcd 集群是独立于集群之外的，这种情况下面我们就需要自定义一个 Endpoints，要注意 metadata 区域的内容要和 Service 保持一致，Service 的 clusterIP 设置为 None&lt;/p&gt;

&lt;p&gt;Endpoints 的 subsets 中填写 etcd 集群的地址即可&lt;/p&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f prometheus-etcdService.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面讲解的是独立于k8s之外的监控访问，前提是需要把网络打通，也是可以直接使用endpoint进行配置的，当然在集群内的监控常规就是匹配的pod的label。&lt;/p&gt;

&lt;p&gt;比如我们把etcd运行在k8s上，我们创建的service就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-etcdService.yaml
apiVersion: v1
kind: Service
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
spec:
  selector:
    component: etcd
  ports:
  - name: port
    port: 2379
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体在上面的schduler讲解过，不多说了，到此一个完整的监控就新增好了。&lt;/p&gt;

&lt;h2 id=&#34;自定义告警规则&#34;&gt;自定义告警规则&lt;/h2&gt;

&lt;p&gt;我们首先查看prometheus部署的时候的alert的配置，可以在prometheus的ui界面的config下查到&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;alerting:
  alert_relabel_configs:
  - separator: ;
    regex: prometheus_replica
    replacement: $1
    action: labeldrop
  alertmanagers:
  - kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - monitoring
    scheme: http
    path_prefix: /
    timeout: 10s
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name]
      separator: ;
      regex: alertmanager-main
      replacement: $1
      action: keep
    - source_labels: [__meta_kubernetes_endpoint_port_name]
      separator: ;
      regex: web
      replacement: $1
      action: keep
rule_files:
- /etc/prometheus/rules/prometheus-k8s-rulefiles-0/*.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过角色为 endpoints 的 kubernetes 的服务发现机制来知道需要发送的alert的地址&lt;/li&gt;
&lt;li&gt;匹配的是服务名为 alertmanager-main，端口名为 web 的 Service 服务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来看一下operator部署的svc&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.106.85.214   &amp;lt;none&amp;gt;        9093/TCP                     4d
alertmanager-operated   ClusterIP   None            &amp;lt;none&amp;gt;        9093/TCP,9094/TCP,9094/UDP   4d
grafana                 NodePort    10.106.80.1     &amp;lt;none&amp;gt;        3000:30267/TCP               4d
kube-state-metrics      ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP,9443/TCP            4d
node-exporter           ClusterIP   None            &amp;lt;none&amp;gt;        9100/TCP                     4d
prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      4d
prometheus-k8s          NodePort    10.103.177.44   &amp;lt;none&amp;gt;        9090:31174/TCP               4d
prometheus-operated     ClusterIP   None            &amp;lt;none&amp;gt;        9090/TCP                     4d
prometheus-operator     ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP                     4d20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确实有一个alertmanager-main的svc，我们查看一下他的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe svc alertmanager-main -n monitoring
Name:              alertmanager-main
Namespace:         monitoring
Labels:            alertmanager=main
Annotations:       Selector:  alertmanager=main,app=alertmanager
Type:              ClusterIP
IP:                10.106.85.214
Port:              web  9093/TCP
TargetPort:        web/TCP
Endpoints:         172.17.0.22:9093,172.17.0.7:9093,172.17.0.9:9093
Session Affinity:  ClientIP
Events:            &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到服务名正是 alertmanager-main，Port 定义的名称也是 web，符合上面的规则，所以 Prometheus 和 AlertManager 组件就正确关联上了。我们就可以将告警发送到对应的alertmanaager了。&lt;/p&gt;

&lt;p&gt;再来看告警规则在/etc/prometheus/rules/prometheus-k8s-rulefiles-0/目录下面所有的 YAML 文件。我们在部署prometheus的时候有一个规则资源配置清单prometheus-rules.yaml，就是我们现在看到的告警规则。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
  namespace: monitoring
spec:
  groups:
  - name: k8s.rules
    rules:
    - expr: |
        sum(rate(container_cpu_usage_seconds_total{job=&amp;quot;kubelet&amp;quot;, image!=&amp;quot;&amp;quot;, container_name!=&amp;quot;&amp;quot;}[5m])) by (namespace)
      record: namespace:container_cpu_usage_seconds_total:sum_rate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PrometheusRule 的 name 为 prometheus-k8s-rules，namespace 为 monitoring，我们可以猜想到我们创建一个 PrometheusRule 资源对象后，会自动在上面的 prometheus-k8s-rulefiles-0 目录下面生成一个对应的&lt;namespace&gt;-&lt;name&gt;.yaml文件，所以如果以后我们需要自定义一个报警选项的话，只需要定义一个 PrometheusRule 资源对象即可。&lt;/p&gt;

&lt;p&gt;至于为什么 Prometheus 能够识别这个 PrometheusRule 资源对象呢？&lt;/p&gt;

&lt;p&gt;创建的 prometheus 这个资源对象里面有非常重要的一个属性 ruleSelector，用来匹配 rule 规则的过滤器，要求匹配具有 prometheus=k8s 和 role=alert-rules 标签的 PrometheusRule 资源对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruleSelector:
  matchLabels:
    prometheus: k8s
    role: alert-rules
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以自定义一个报警规则，还需要需要创建一个具有 prometheus=k8s 和 role=alert-rules 标签的 PrometheusRule 对象。&lt;/p&gt;

&lt;p&gt;我们以etcd为例来自定义一个告警：如果不可用的 etcd 数量超过了一半那么就触发报警&lt;/p&gt;

&lt;p&gt;创建文件 prometheus-etcdRules.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: etcd-rules
  namespace: monitoring
spec:
  groups:
  - name: etcd
    rules:
    - alert: EtcdClusterUnavailable
      annotations:
        summary: etcd cluster small
        description: If one more etcd peer goes down the cluster will be unavailable
      expr: |
        count(up{job=&amp;quot;etcd&amp;quot;} == 0) &amp;gt; (count(up{job=&amp;quot;etcd&amp;quot;}) / 2 - 1)
      for: 3m
      labels:
        severity: critical
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建好了，我们就可以看到在对应目录下生成了一份yaml告警规则文件。&lt;/p&gt;

&lt;h2 id=&#34;配置告警方式&#34;&gt;配置告警方式&lt;/h2&gt;

&lt;p&gt;我们可以通过 AlertManager 的配置文件去配置各种报警接收器，首先我们将 alertmanager-main 这个 Service 改为 NodePort 类型的 Service，和前面的修改是一样的操作，修改完成后我们可以在页面上的 status 路径下面查看 AlertManager 的配置信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  resolve_timeout: 5m
  http_config: {}
  smtp_hello: localhost
  smtp_require_tls: true
  pagerduty_url: https://events.pagerduty.com/v2/enqueue
  hipchat_api_url: https://api.hipchat.com/
  opsgenie_api_url: https://api.opsgenie.com/
  wechat_api_url: https://qyapi.weixin.qq.com/cgi-bin/
  victorops_api_url: https://alert.victorops.com/integrations/generic/20131114/alert/
route:
  receiver: Default
  group_by:
  - namespace
  routes:
  - receiver: Watchdog
    match:
      alertname: Watchdog
  - receiver: Critical
    match:
      severity: critical
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
inhibit_rules:
- source_match:
    severity: critical
  target_match_re:
    severity: warning|info
  equal:
  - namespace
  - alertname
- source_match:
    severity: warning
  target_match_re:
    severity: info
  equal:
  - namespace
  - alertname
receivers:
- name: Default
- name: Watchdog
- name: Critical
templates: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些配置信息实际上是来自于我们之前在prometheus-operator/contrib/kube-prometheus/manifests目录下面创建的 alertmanager-secret.yaml 文件，将文件中 alertmanager.yaml 对应的 value 值做一个 base64 解码，内容和上面查看的配置信息是一致的。&lt;/p&gt;

&lt;p&gt;果我们想要添加自己的接收器，或者模板消息，我们就可以更改这个文件，比如我们添加了两个接收器，默认的通过邮箱进行发送，对于 CoreDNSDown 这个报警我们通过 webhook 来进行发送。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  resolve_timeout: 5m
  smtp_smarthost: &#39;smtp.163.com:25&#39;
  smtp_from: &#39;ych_1024@163.com&#39;
  smtp_auth_username: &#39;ych_1024@163.com&#39;
  smtp_auth_password: &#39;&amp;lt;邮箱密码&amp;gt;&#39;
  smtp_hello: &#39;163.com&#39;
  smtp_require_tls: false
route:
  group_by: [&#39;job&#39;, &#39;severity&#39;]
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: default
  routes:
  - receiver: webhook
    match:
      alertname: CoreDNSDown
receivers:
- name: &#39;default&#39;
  email_configs:
  - to: &#39;517554016@qq.com&#39;
    send_resolved: true
- name: &#39;webhook&#39;
  webhook_configs:
  - url: &#39;http://dingtalk-hook.kube-ops:5000&#39;
    send_resolved: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上面文件保存为 alertmanager.yaml，然后使用这个文件创建一个 Secret 对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 先将之前的 secret 对象删除
$ kubectl delete secret alertmanager-main -n monitoring
secret &amp;quot;alertmanager-main&amp;quot; deleted
$ kubectl create secret generic alertmanager-main --from-file=alertmanager.yaml -n monitoring
secret &amp;quot;alertmanager-main&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以完成发送的配置了。&lt;/p&gt;

&lt;h2 id=&#34;自动发现配置&#34;&gt;自动发现配置&lt;/h2&gt;

&lt;p&gt;如果在我们的 Kubernetes 集群中有了很多的 Service/Pod，那么我们都需要一个一个的去建立一个对应的 ServiceMonitor 对象来进行监控吗？这样岂不是又变得麻烦起来了？&lt;/p&gt;

&lt;p&gt;我们可以通过添加额外的配置来进行服务发现进行自动监控。配置如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;kubernetes-service-endpoints&#39;
  kubernetes_sd_configs:
  - role: endpoints
  relabel_configs:
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
    action: replace
    target_label: __scheme__
    regex: (https?)
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
    action: replace
    target_label: __metrics_path__
    regex: (.+)
  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
    action: replace
    target_label: __address__
    regex: ([^:]+)(?::\d+)?;(\d+)
    replacement: $1:$2
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    action: replace
    target_label: kubernetes_name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上面文件直接保存为 prometheus-additional.yaml，然后通过这个文件创建一个对应的 Secret 对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret generic additional-configs --from-file=prometheus-additional.yaml -n monitoring
secret &amp;quot;additional-configs&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建完成后，会将上面配置信息进行 base64 编码后作为 prometheus-additional.yaml 这个 key 对应的值存在&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get secret additional-configs -n monitoring -o yaml
apiVersion: v1
data:
  prometheus-additional.yaml: LSBqb2JfbmFtZTogJ2t1YmVybmV0ZXMtc2VydmljZS1lbmRwb2ludHMnCiAga3ViZXJuZXRlc19zZF9jb25maWdzOgogIC0gcm9sZTogZW5kcG9pbnRzCiAgcmVsYWJlbF9jb25maWdzOgogIC0gc291cmNlX2xhYmVsczogW19fbWV0YV9rdWJlcm5ldGVzX3NlcnZpY2VfYW5ub3RhdGlvbl9wcm9tZXRoZXVzX2lvX3NjcmFwZV0KICAgIGFjdGlvbjoga2VlcAogICAgcmVnZXg6IHRydWUKICAtIHNvdXJjZV9sYWJlbHM6IFtfX21ldGFfa3ViZXJuZXRlc19zZXJ2aWNlX2Fubm90YXRpb25fcHJvbWV0aGV1c19pb19zY2hlbWVdCiAgICBhY3Rpb246IHJlcGxhY2UKICAgIHRhcmdldF9sYWJlbDogX19zY2hlbWVfXwogICAgcmVnZXg6IChodHRwcz8pCiAgLSBzb3VyY2VfbGFiZWxzOiBbX19tZXRhX2t1YmVybmV0ZXNfc2VydmljZV9hbm5vdGF0aW9uX3Byb21ldGhldXNfaW9fcGF0aF0KICAgIGFjdGlvbjogcmVwbGFjZQogICAgdGFyZ2V0X2xhYmVsOiBfX21ldHJpY3NfcGF0aF9fCiAgICByZWdleDogKC4rKQogIC0gc291cmNlX2xhYmVsczogW19fYWRkcmVzc19fLCBfX21ldGFfa3ViZXJuZXRlc19zZXJ2aWNlX2Fubm90YXRpb25fcHJvbWV0aGV1c19pb19wb3J0XQogICAgYWN0aW9uOiByZXBsYWNlCiAgICB0YXJnZXRfbGFiZWw6IF9fYWRkcmVzc19fCiAgICByZWdleDogKFteOl0rKSg/OjpcZCspPzsoXGQrKQogICAgcmVwbGFjZW1lbnQ6ICQxOiQyCiAgLSBhY3Rpb246IGxhYmVsbWFwCiAgICByZWdleDogX19tZXRhX2t1YmVybmV0ZXNfc2VydmljZV9sYWJlbF8oLispCiAgLSBzb3VyY2VfbGFiZWxzOiBbX19tZXRhX2t1YmVybmV0ZXNfbmFtZXNwYWNlXQogICAgYWN0aW9uOiByZXBsYWNlCiAgICB0YXJnZXRfbGFiZWw6IGt1YmVybmV0ZXNfbmFtZXNwYWNlCiAgLSBzb3VyY2VfbGFiZWxzOiBbX19tZXRhX2t1YmVybmV0ZXNfc2VydmljZV9uYW1lXQogICAgYWN0aW9uOiByZXBsYWNlCiAgICB0YXJnZXRfbGFiZWw6IGt1YmVybmV0ZXNfbmFtZQo=
kind: Secret
metadata:
  creationTimestamp: 2018-12-20T14:50:35Z
  name: additional-configs
  namespace: monitoring
  resourceVersion: &amp;quot;41814998&amp;quot;
  selfLink: /api/v1/namespaces/monitoring/secrets/additional-configs
  uid: 9bbe22c5-0466-11e9-a777-525400db4df7
type: Opaque
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们只需要在声明创建 prometheus 的资源对象文件中添加上这个额外的配置，其实就是通过secret将这段配置挂载到prometheus上去，作为prometheus的配置文件的一部分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  additionalScrapeConfigs:
    name: additional-configs
    key: prometheus-additional.yaml
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以自动发现service信息了，这边还有一个权限的问题，Prometheus 绑定了一个名为 prometheus-k8s 的 ServiceAccount 对象，而这个对象绑定的是一个名为 prometheus-k8s 的 ClusterRole，有对 Service 或者 Pod 的 list 权限，所以报错了，要解决这个问题，我们只需要添加上需要的权限即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - nodes
  - services
  - endpoints
  - pods
  - nodes/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - configmaps
  - nodes/metrics
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就有权限了，只需要我们在 Service 的annotation区域添加prometheus.io/scrape=true的声明，就会在服务创建的时候被自动发现。&lt;/p&gt;

&lt;h2 id=&#34;数据持久化&#34;&gt;数据持久化&lt;/h2&gt;

&lt;p&gt;查看生成的 Prometheus Pod 的挂载情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod prometheus-k8s-0 -n monitoring -o yaml
......
    volumeMounts:
    - mountPath: /etc/prometheus/config_out
      name: config-out
      readOnly: true
    - mountPath: /prometheus
      name: prometheus-k8s-db
......
  volumes:
......
  - emptyDir: {}
    name: prometheus-k8s-db
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到 Prometheus 的数据目录 /prometheus 实际上是通过 emptyDir 进行挂载的，我们知道 emptyDir 挂载的数据的生命周期和 Pod 生命周期一致的，所以如果 Pod 挂掉了，数据也就丢失了。&lt;/p&gt;

&lt;p&gt;对应线上的监控数据肯定需要做数据的持久化的，们的 Prometheus 最终是通过 Statefulset 控制器进行部署的，所以我们这里需要通过 storageclass 来做数据持久化，首先创建一个 StorageClass 对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: prometheus-data-db
provisioner: fuseim.pri/ifs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 provisioner=fuseim.pri/ifs，则是因为我们集群中使用的是 nfs 作为存储后端，将该文件保存为 prometheus-storageclass.yaml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f prometheus-storageclass.yaml
storageclass.storage.k8s.io &amp;quot;prometheus-data-db&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在 prometheus 的 CRD 资源对象中添加如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;storage:
  volumeClaimTemplate:
    spec:
      storageClassName: prometheus-data-db
      resources:
        requests:
          storage: 10Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意这里的 storageClassName 名字为上面我们创建的 StorageClass 对象名称，然后更新 prometheus 这个 CRD 资源。更新完成后会自动生成两个 PVC 和 PV 资源对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pvc -n monitoring
NAME                                 STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE
prometheus-k8s-db-prometheus-k8s-0   Bound     pvc-0cc03d41-047a-11e9-a777-525400db4df7   10Gi       RWO            prometheus-data-db   8m
prometheus-k8s-db-prometheus-k8s-1   Bound     pvc-1938de6b-047b-11e9-a777-525400db4df7   10Gi       RWO            prometheus-data-db   1m
$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                           STORAGECLASS         REASON    AGE
pvc-0cc03d41-047a-11e9-a777-525400db4df7   10Gi       RWO            Delete           Bound       monitoring/prometheus-k8s-db-prometheus-k8s-0   prometheus-data-db             2m
pvc-1938de6b-047b-11e9-a777-525400db4df7   10Gi       RWO            Delete           Bound       monitoring/prometheus-k8s-db-prometheus-k8s-1   prometheus-data-db             1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在我们再去看 Prometheus Pod 的数据目录就可以看到是关联到一个 PVC 对象上了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod prometheus-k8s-0 -n monitoring -o yaml
......
    volumeMounts:
    - mountPath: /etc/prometheus/config_out
      name: config-out
      readOnly: true
    - mountPath: /prometheus
      name: prometheus-k8s-db
......
  volumes:
......
  - name: prometheus-k8s-db
    persistentVolumeClaim:
      claimName: prometheus-k8s-db-prometheus-k8s-0
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在即使我们的 Pod 挂掉了，数据也不会丢失了。&lt;/p&gt;

&lt;h1 id=&#34;使用场景&#34;&gt;使用场景&lt;/h1&gt;

&lt;p&gt;什么时候prometheus使用的物理机部署的集群？&lt;/p&gt;

&lt;p&gt;1、取决于生产环境，比如要求prometheus不光需要监控k8s还需要监控kvm的机器&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;k8s的服务发现主要是通过定义serviceMonitor，或者service配置备注做自动发现，本质其实就是通过service来暴露指标，但是对于kvm没有这些机制，如何对kvm环境下的机器做服务发现就是一个问题&lt;/li&gt;
&lt;li&gt;k8s和kvm的网络打通也是一个问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、监控规模，数据量导致的稳定性&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当集群规模小，监控数据少的情况下部署单点的prometheus是够用的，但是如果监控规模扩展，数据量很大的时候，对资源，比如cpu和memory的要求比较高，对k8s来说是一个很重的应用，对于本身的稳定性也是一个很重要的考验。&lt;/li&gt;
&lt;li&gt;当规模达到一定的时候，需要分布式集群来处理，在k8s部署分布式集群也是有着很多的问题&lt;/li&gt;
&lt;li&gt;当规模扩大时候，分组也是一个很大的问题，单个prometheus的的采集分配也是问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以如果只是监控k8s使用operator部署k8s上都可以优化，但是如果加上kvm是很有必要在物理机上部署prometheus的监控的，需要设计完成的架构和实现方案。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Go Context</title>
          <link>https://kingjcy.github.io/post/golang/go-context/</link>
          <pubDate>Wed, 06 Jun 2018 11:02:37 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-context/</guid>
          <description>&lt;p&gt;控制并发有两种经典的方式，一种是WaitGroup，另外一种就是Context，当然还可以简单的直接用channel通知。&lt;/p&gt;

&lt;h1 id=&#34;waitgroup&#34;&gt;waitgroup&lt;/h1&gt;

&lt;p&gt;WaitGroup以前我们在并发的时候介绍过，它是一种控制并发的方式，它的这种方式是控制多个goroutine同时完成。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var wg sync.WaitGroup

    wg.Add(2)
    go func() {
        time.Sleep(2*time.Second)
        fmt.Println(&amp;quot;1号完成&amp;quot;)
        wg.Done()
    }()
    go func() {
        time.Sleep(2*time.Second)
        fmt.Println(&amp;quot;2号完成&amp;quot;)
        wg.Done()
    }()
    wg.Wait()
    fmt.Println(&amp;quot;好了，大家都干完了，放工&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个很简单的例子，一定要例子中的2个goroutine同时做完，才算是完成，先做好的就要等着其他未完成的，所有的goroutine要都全部完成才可以。&lt;/p&gt;

&lt;p&gt;这是一种控制并发的方式，这种尤其适用于好多个goroutine协同做一件事情的时候，因为每个goroutine做的都是这件事情的一部分，只有全部的goroutine都完成，这件事情才算是完成，这是等待的方式。&lt;/p&gt;

&lt;p&gt;这边说一下wg的传输，像上面这种事全局的，直接使用就好，但是很多时候wg的局部变量需要传输使用，这个时候需要传输地址&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sync&amp;quot;
    &amp;quot;time&amp;quot;
)

func dosomething(millisecs time.Duration, wg *sync.WaitGroup) {
    duration := millisecs * time.Millisecond
    time.Sleep(duration)
    fmt.Println(&amp;quot;Function in background, duration:&amp;quot;, duration)
    wg.Done()
}

func main() {
    var wg sync.WaitGroup
    wg.Add(4)
    go dosomething(200, &amp;amp;wg)
    go dosomething(400, &amp;amp;wg)
    go dosomething(150, &amp;amp;wg)
    go dosomething(600, &amp;amp;wg)

    wg.Wait()
    fmt.Println(&amp;quot;Done&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在实际的业务种，我们可能会有这么一种场景：需要我们主动的通知某一个goroutine结束。比如我们开启一个后台goroutine一直做事情，比如监控，现在不需要了，就需要通知这个监控goroutine结束，不然它会一直跑，就泄漏了，这个时候我们就需要使用chan了。&lt;/p&gt;

&lt;h1 id=&#34;chan通知&#34;&gt;chan通知&lt;/h1&gt;

&lt;p&gt;我们都知道一个goroutine启动后，我们是无法控制他的，大部分情况是等待它自己结束，那么如果这个goroutine是一个不会自己结束的后台goroutine呢？比如监控等，会一直运行的。&lt;/p&gt;

&lt;p&gt;这种情况，一种傻瓜式的办法是全局变量，其他地方通过修改这个变量完成结束通知，然后后台goroutine不停的检查这个变量，如果发现被通知关闭了，就自我结束。&lt;/p&gt;

&lt;p&gt;这种方式也可以，但是首先我们要保证这个变量在多线程下的安全，基于此，有一种更好的方式：chan + select 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    stop := make(chan bool)

    go func() {
        for {
            select {
            case &amp;lt;-stop:
                fmt.Println(&amp;quot;监控退出，停止了...&amp;quot;)
                return
            default:
                fmt.Println(&amp;quot;goroutine监控中...&amp;quot;)
                time.Sleep(2 * time.Second)
            }
        }
    }()

    time.Sleep(10 * time.Second)
    fmt.Println(&amp;quot;可以了，通知监控停止&amp;quot;)
    stop&amp;lt;- true
    //为了检测监控过是否停止，如果没有监控输出，就表示停止了
    time.Sleep(5 * time.Second)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例子中我们定义一个stop的chan，通知他结束后台goroutine。实现也非常简单，在后台goroutine中，使用select判断stop是否可以接收到值，如果可以接收到，就表示可以退出停止了；如果没有接收到，就会执行default里的监控逻辑，继续监控，只到收到stop的通知。&lt;/p&gt;

&lt;p&gt;有了以上的逻辑，我们就可以在其他goroutine种，给stop chan发送值了，例子中是在main goroutine中发送的，控制让这个监控的goroutine结束。&lt;/p&gt;

&lt;p&gt;发送了stop&amp;lt;- true结束的指令后，我这里使用time.Sleep(5 * time.Second)故意停顿5秒来检测我们结束监控goroutine是否成功。如果成功的话，不会再有goroutine监控中&amp;hellip;的输出了；如果没有成功，监控goroutine就会继续打印goroutine监控中&amp;hellip;输出。这其实是一种异步的思想，可以好好琢磨对比一下。&lt;/p&gt;

&lt;p&gt;这种chan+select的方式，是比较优雅的结束一个goroutine的方式，不过这种方式也有局限性，如果有很多goroutine都需要控制结束怎么办呢？如果这些goroutine又衍生了其他更多的goroutine怎么办呢？如果一层层的无穷尽的goroutine呢？这就非常复杂了，即使我们定义很多chan也很难解决这个问题，因为goroutine的关系链就导致了这种场景非常复杂。&lt;/p&gt;

&lt;h1 id=&#34;初识context&#34;&gt;初识Context&lt;/h1&gt;

&lt;p&gt;上面说的这种场景是存在的，比如一个网络请求Request，每个Request都需要开启一个goroutine做一些事情，这些goroutine又可能会开启其他的goroutine。所以我们需要一种可以跟踪goroutine的方案，才可以达到控制他们的目的，这就是Go语言为我们提供的Context，称之为上下文非常贴切，它就是goroutine的上下文。&lt;/p&gt;

&lt;p&gt;下面我们就使用Go Context重写上面的示例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    ctx, cancel := context.WithCancel(context.Background())
    go func(ctx context.Context) {
        for {
            select {
            case &amp;lt;-ctx.Done():
                fmt.Println(&amp;quot;监控退出，停止了...&amp;quot;)
                return
            default:
                fmt.Println(&amp;quot;goroutine监控中...&amp;quot;)
                time.Sleep(2 * time.Second)
            }
        }
    }(ctx)

    time.Sleep(10 * time.Second)
    fmt.Println(&amp;quot;可以了，通知监控停止&amp;quot;)
    cancel()
    //为了检测监控过是否停止，如果没有监控输出，就表示停止了
    time.Sleep(5 * time.Second)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重写比较简单，就是把原来的chan stop 换成Context，使用Context跟踪goroutine，以便进行控制，比如结束等。&lt;/p&gt;

&lt;p&gt;context.Background() 返回一个空的Context，这个空的Context一般用于整个Context树的根节点。然后我们使用context.WithCancel(parent)函数，创建一个可取消的子Context，然后当作参数传给goroutine使用，这样就可以使用这个子Context跟踪这个goroutine。&lt;/p&gt;

&lt;p&gt;在goroutine中，使用select调用&amp;lt;-ctx.Done()判断是否要结束，如果接受到值的话，就可以返回结束goroutine了；如果接收不到，就会继续进行监控。&lt;/p&gt;

&lt;p&gt;那么是如何发送结束指令的呢？这就是示例中的cancel函数啦，它是我们调用context.WithCancel(parent)函数生成子Context的时候返回的，第二个返回值就是这个取消函数，它是CancelFunc类型的。我们调用它就可以发出取消指令，然后我们的监控goroutine就会收到信号，就会返回结束。&lt;/p&gt;

&lt;h2 id=&#34;context控制多个goroutine&#34;&gt;Context控制多个goroutine&lt;/h2&gt;

&lt;p&gt;使用Context控制一个goroutine的例子如上，非常简单，下面我们看看控制多个goroutine的例子，其实也比较简单。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    ctx, cancel := context.WithCancel(context.Background())
    go watch(ctx,&amp;quot;【监控1】&amp;quot;)
    go watch(ctx,&amp;quot;【监控2】&amp;quot;)
    go watch(ctx,&amp;quot;【监控3】&amp;quot;)

    time.Sleep(10 * time.Second)
    fmt.Println(&amp;quot;可以了，通知监控停止&amp;quot;)
    cancel()
    //为了检测监控过是否停止，如果没有监控输出，就表示停止了
    time.Sleep(5 * time.Second)
}

func watch(ctx context.Context, name string) {
    for {
        select {
        case &amp;lt;-ctx.Done():
            fmt.Println(name,&amp;quot;监控退出，停止了...&amp;quot;)
            return
        default:
            fmt.Println(name,&amp;quot;goroutine监控中...&amp;quot;)
            time.Sleep(2 * time.Second)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例中启动了3个监控goroutine进行不断的监控，每一个都使用了Context进行跟踪，当我们使用cancel函数通知取消时，这3个goroutine都会被结束。这就是Context的控制能力，它就像一个控制器一样，按下开关后，所有基于这个Context或者衍生的子Context都会收到通知，这时就可以进行清理操作了，最终释放goroutine，这就优雅的解决了goroutine启动后不可控的问题。&lt;/p&gt;

&lt;h2 id=&#34;context接口&#34;&gt;Context接口&lt;/h2&gt;

&lt;p&gt;Context的接口定义的比较简洁，我们看下这个接口的方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Context interface {
    Deadline() (deadline time.Time, ok bool)

    Done() &amp;lt;-chan struct{}

    Err() error

    Value(key interface{}) interface{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个接口共有4个方法，了解这些方法的意思非常重要，这样我们才可以更好的使用他们。&lt;/p&gt;

&lt;p&gt;Deadline方法是获取设置的截止时间的意思，第一个返回式是截止时间，到了这个时间点，Context会自动发起取消请求；第二个返回值ok==false时表示没有设置截止时间，如果需要取消的话，需要调用取消函数进行取消。&lt;/p&gt;

&lt;p&gt;Done方法返回一个只读的chan，类型为struct{}，我们在goroutine中，如果该方法返回的chan可以读取，则意味着parent context已经发起了取消请求，我们通过Done方法收到这个信号后，就应该做清理操作，然后退出goroutine，释放资源。&lt;/p&gt;

&lt;p&gt;Err方法返回取消的错误原因，因为什么Context被取消。&lt;/p&gt;

&lt;p&gt;Value方法获取该Context上绑定的值，是一个键值对，所以要通过一个Key才可以获取对应的值，这个值一般是线程安全的。&lt;/p&gt;

&lt;p&gt;以上四个方法中常用的就是Done了，如果Context取消的时候，我们就可以得到一个关闭的chan，关闭的chan是可以读取的，所以只要可以读取的时候，就意味着收到Context取消的信号了，以下是这个方法的经典用法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  func Stream(ctx context.Context, out chan&amp;lt;- Value) error {
    for {
        v, err := DoSomething(ctx)
        if err != nil {
            return err
        }
        select {
        case &amp;lt;-ctx.Done():
            return ctx.Err()
        case out &amp;lt;- v:
        }
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Context接口并不需要我们实现，Go内置已经帮我们实现了2个，我们代码中最开始都是以这两个内置的作为最顶层的partent context，衍生出更多的子Context。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    background = new(emptyCtx)
    todo       = new(emptyCtx)
)

func Background() Context {
    return background
}

func TODO() Context {
    return todo
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个是Background，主要用于main函数、初始化以及测试代码中，作为Context这个树结构的最顶层的Context，也就是根Context。&lt;/p&gt;

&lt;p&gt;一个是TODO,它目前还不知道具体的使用场景，如果我们不知道该使用什么Context的时候，可以使用这个。&lt;/p&gt;

&lt;p&gt;他们两个本质上都是emptyCtx结构体类型，是一个不可取消，没有设置截止时间，没有携带任何值的Context。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type emptyCtx int

func (*emptyCtx) Deadline() (deadline time.Time, ok bool) {
    return
}

func (*emptyCtx) Done() &amp;lt;-chan struct{} {
    return nil
}

func (*emptyCtx) Err() error {
    return nil
}

func (*emptyCtx) Value(key interface{}) interface{} {
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是emptyCtx实现Context接口的方法，可以看到，这些方法什么都没做，返回的都是nil或者零值。&lt;/p&gt;

&lt;h2 id=&#34;context的继承衍生&#34;&gt;Context的继承衍生&lt;/h2&gt;

&lt;p&gt;有了如上的根Context，那么是如何衍生更多的子Context的呢？这就要靠context包为我们提供的With系列的函数了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func WithCancel(parent Context) (ctx Context, cancel CancelFunc)
func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc)
func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)
func WithValue(parent Context, key, val interface{}) Context
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这四个With函数，接收的都有一个partent参数，就是父Context，我们要基于这个父Context创建出子Context的意思，这种方式可以理解为子Context对父Context的继承，也可以理解为基于父Context的衍生。&lt;/p&gt;

&lt;p&gt;通过这些函数，就创建了一颗Context树，树的每个节点都可以有任意多个子节点，节点层级可以有任意多个。&lt;/p&gt;

&lt;p&gt;WithCancel函数，传递一个父Context作为参数，返回子Context，以及一个取消函数用来取消Context。&lt;/p&gt;

&lt;p&gt;WithDeadline函数，和WithCancel差不多，它会多传递一个截止时间参数，意味着到了这个时间点，会自动取消Context，当然我们也可以不等到这个时候，可以提前通过取消函数进行取消。&lt;/p&gt;

&lt;p&gt;WithTimeout和WithDeadline基本上一样，这个表示是超时自动取消，是多少时间后自动取消Context的意思。&lt;/p&gt;

&lt;p&gt;WithValue函数和取消Context无关，它是为了生成一个绑定了一个键值对数据的Context，这个绑定的数据可以通过Context.Value方法访问到，后面我们会专门讲。&lt;/p&gt;

&lt;p&gt;大家可能留意到，前三个函数都返回一个取消函数CancelFunc，这是一个函数类型，它的定义非常简单。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type CancelFunc func()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是取消函数的类型，该函数可以取消一个Context，以及这个节点Context下所有的所有的Context，不管有多少层级。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WithValue传递元数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过Context我们也可以传递一些必须的元数据，这些数据会附加在Context上以供使用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var key string=&amp;quot;name&amp;quot;

func main() {
    ctx, cancel := context.WithCancel(context.Background())
    //附加值
    valueCtx:=context.WithValue(ctx,key,&amp;quot;【监控1】&amp;quot;)
    go watch(valueCtx)
    time.Sleep(10 * time.Second)
    fmt.Println(&amp;quot;可以了，通知监控停止&amp;quot;)
    cancel()
    //为了检测监控过是否停止，如果没有监控输出，就表示停止了
    time.Sleep(5 * time.Second)
}

func watch(ctx context.Context) {
    for {
        select {
        case &amp;lt;-ctx.Done():
            //取出值
            fmt.Println(ctx.Value(key),&amp;quot;监控退出，停止了...&amp;quot;)
            return
        default:
            //取出值
            fmt.Println(ctx.Value(key),&amp;quot;goroutine监控中...&amp;quot;)
            time.Sleep(2 * time.Second)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在前面的例子，我们通过传递参数的方式，把name的值传递给监控函数。在这个例子里，我们实现一样的效果，但是通过的是Context的Value的方式。&lt;/p&gt;

&lt;p&gt;我们可以使用context.WithValue方法附加一对K-V的键值对，这里Key必须是等价性的，也就是具有可比性；Value值要是线程安全的。&lt;/p&gt;

&lt;p&gt;这样我们就生成了一个新的Context，这个新的Context带有这个键值对，在使用的时候，可以通过Value方法读取ctx.Value(key)。&lt;/p&gt;

&lt;p&gt;记住，使用WithValue传值，一般是必须的值，不要什么值都传递。&lt;/p&gt;

&lt;h2 id=&#34;context-使用原则&#34;&gt;Context 使用原则&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;不要把Context放在结构体中，要以参数的方式传递&lt;/li&gt;
&lt;li&gt;以Context作为参数的函数方法，应该把Context作为第一个参数，放在第一位。命名为ctx。 func DoSomething（ctx context.Context，arg Arg）error { // &amp;hellip; use ctx &amp;hellip; }&lt;/li&gt;
&lt;li&gt;给一个函数方法传递Context的时候，不要传递nil，如果不知道传递什么，就使用context.TODO&lt;/li&gt;
&lt;li&gt;Context的Value相关方法应该传递必须的数据，不要什么数据都使用这个传递，不要用它来传递一些可选的参数&lt;/li&gt;
&lt;li&gt;Context是线程安全的，可以放心的在多个goroutine中传递，相同的 Context 可以传递给在不同的goroutine；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;应用场景&#34;&gt;应用场景&lt;/h2&gt;

&lt;p&gt;在 Go http 包的 Server 中，每一个请求在都有一个对应的goroutine去处理。请求处理函数通常会启动额外的goroutine用来访问后端服务，比如数据库和 RPC 服务。用来处理一个请求的goroutine通常需要访问一些与请求特定的数据，比如终端用户的身份认证信息、验证相关的 token、请求的截止时间。当一个请求被取消或超时时，所有用来处理该请求的goroutine都应该迅速退出，然后系统才能释放这些goroutine占用的资源。&lt;/p&gt;

&lt;p&gt;注意：go1.6及之前版本请使用golang.org/x/net/context。go1.7及之后已移到标准库context。&lt;/p&gt;

&lt;h2 id=&#34;实战&#34;&gt;实战&lt;/h2&gt;

&lt;h3 id=&#34;withcancel-例子&#34;&gt;WithCancel 例子&lt;/h3&gt;

&lt;p&gt;WithCancel 以一个新的 Done channel 返回一个父 Context 的拷贝。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func WithCancel(parent Context) (ctx Context, cancel CancelFunc) {
    c := newCancelCtx(parent)
    propagateCancel(parent, &amp;amp;c)
    return &amp;amp;c, func() { c.cancel(true, Canceled) }
}

// newCancelCtx returns an initialized cancelCtx.
func newCancelCtx(parent Context) cancelCtx {
    return cancelCtx{
        Context: parent,
        done:    make(chan struct{}),
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此示例演示使用一个可取消的上下文，以防止 goroutine 泄漏。示例函数结束时，defer 调用 cancel 方法，gen goroutine 将返回而不泄漏。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;context&amp;quot;
    &amp;quot;fmt&amp;quot;
)

func main() {
    // gen generates integers in a separate goroutine and
    // sends them to the returned channel.
    // The callers of gen need to cancel the context once
    // they are done consuming generated integers not to leak
    // the internal goroutine started by gen.
    gen := func(ctx context.Context) &amp;lt;-chan int {
        dst := make(chan int)
        n := 1
        go func() {
            for {
                select {
                case &amp;lt;-ctx.Done():
                    return // returning not to leak the goroutine
                case dst &amp;lt;- n:
                    n++
                }
            }
        }()
        return dst
    }

    ctx, cancel := context.WithCancel(context.Background())
    defer cancel() // cancel when we are finished consuming integers

    for n := range gen(ctx) {
        fmt.Println(n)
        if n == 5 {
            break
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;withdeadline-例子&#34;&gt;WithDeadline 例子&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc) {
    if cur, ok := parent.Deadline(); ok &amp;amp;&amp;amp; cur.Before(deadline) {
        // The current deadline is already sooner than the new one.
        return WithCancel(parent)
    }
    c := &amp;amp;timerCtx{
        cancelCtx: newCancelCtx(parent),
        deadline:  deadline,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以清晰的看到，当派生出的子 Context 的deadline在父Context之后，直接返回了一个父Context的拷贝。故语义上等效为父。&lt;/p&gt;

&lt;p&gt;WithDeadline 的最后期限调整为不晚于 d 返回父上下文的副本。如果父母的截止日期已经早于 d，WithDeadline （父，d） 是在语义上等效为父。返回的上下文完成的通道关闭的最后期限期满后，返回的取消函数调用时，或当父上下文完成的通道关闭，以先发生者为准。&lt;/p&gt;

&lt;p&gt;看看官方例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;context&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    d := time.Now().Add(50 * time.Millisecond)
    ctx, cancel := context.WithDeadline(context.Background(), d)

    // Even though ctx will be expired, it is good practice to call its
    // cancelation function in any case. Failure to do so may keep the
    // context and its parent alive longer than necessary.
    defer cancel()

    select {
    case &amp;lt;-time.After(1 * time.Second):
        fmt.Println(&amp;quot;overslept&amp;quot;)
    case &amp;lt;-ctx.Done():
        fmt.Println(ctx.Err())
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;withtimeout-例子&#34;&gt;WithTimeout 例子&lt;/h3&gt;

&lt;p&gt;WithTimeout 返回 WithDeadline(parent, time.Now().Add(timeout))。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) {
    return WithDeadline(parent, time.Now().Add(timeout))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看看官方例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;context&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    // Pass a context with a timeout to tell a blocking function that it
    // should abandon its work after the timeout elapses.
    ctx, cancel := context.WithTimeout(context.Background(), 50*time.Millisecond)
    defer cancel()

    select {
    case &amp;lt;-time.After(1 * time.Second):
        fmt.Println(&amp;quot;overslept&amp;quot;)
    case &amp;lt;-ctx.Done():
        fmt.Println(ctx.Err()) // prints &amp;quot;context deadline exceeded&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;自己写的例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Requset(ctx context.Context,ip string,w http.ResponseWriter){
    c := make(chan string, 1024)

    go func(ip string){
        cfg := g.GlbServerCfg.Conf

        url := &amp;quot;http://&amp;quot; + ip + &amp;quot;:&amp;quot; + strconv.Itoa(cfg.PromesPort) + &amp;quot;/-/reload&amp;quot;
        log.Debug(&amp;quot;Reload %s&amp;quot;, ip)

        client := &amp;amp;http.Client{
            Timeout: time.Duration(cfg.TimeOut)*time.Second,
        }

        request, err := http.NewRequest(&amp;quot;POST&amp;quot;,url,nil)
        if err != nil {
            log.Error(&amp;quot;New Request Error &amp;quot;,err)
        }

        resp, err := client.Do(request)
        defer resp.Body.Close()

        //resp, err := http.Post(url, &amp;quot;&amp;quot;, nil)
        if err != nil || resp.StatusCode != 200 {
            log.Error(&amp;quot;Http Post Faild : %s, Status Code: %d&amp;quot;, err, resp.StatusCode)
            c &amp;lt;- string(ip + &amp;quot; Reload Faild\n&amp;quot;)
        }
        log.Debug(&amp;quot;Http Post Successd , Status Code: : %d&amp;quot;, resp.StatusCode)
        c &amp;lt;- string(ip + &amp;quot; Reload Success\n&amp;quot;)
    }(ip)

    for {
        select {
        case &amp;lt;-ctx.Done():
            w.Write([]byte(ip + &amp;quot; Reload TimeOut\n&amp;quot;))
            return
        case response := &amp;lt;-c:
            w.Write([]byte(response))
            return
        }
    }



}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;withvalue-例子&#34;&gt;WithValue 例子&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;func WithValue(parent Context, key, val interface{}) Context {
    if key == nil {
        panic(&amp;quot;nil key&amp;quot;)
    }
    if !reflect.TypeOf(key).Comparable() {
        panic(&amp;quot;key is not comparable&amp;quot;)
    }
    return &amp;amp;valueCtx{parent, key, val}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WithValue 返回的父与键关联的值在 val 的副本。&lt;/p&gt;

&lt;p&gt;使用上下文值仅为过渡进程和 Api 的请求范围的数据，而不是将可选参数传递给函数。&lt;/p&gt;

&lt;p&gt;提供的键必须是可比性和应该不是字符串类型或任何其他内置的类型以避免包使用的上下文之间的碰撞。WithValue 用户应该定义自己的键的类型。为了避免分配分配给接口 {} 时，上下文键经常有具体类型结构 {}。另外，导出的上下文关键变量静态类型应该是一个指针或接口。&lt;/p&gt;

&lt;p&gt;看看官方例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;context&amp;quot;
    &amp;quot;fmt&amp;quot;
)

func main() {
    type favContextKey string

    f := func(ctx context.Context, k favContextKey) {
        if v := ctx.Value(k); v != nil {
            fmt.Println(&amp;quot;found value:&amp;quot;, v)
            return
        }
        fmt.Println(&amp;quot;key not found:&amp;quot;, k)
    }

    k := favContextKey(&amp;quot;language&amp;quot;)
    ctx := context.WithValue(context.Background(), k, &amp;quot;Go&amp;quot;)

    f(ctx, k)
    f(ctx, favContextKey(&amp;quot;color&amp;quot;))
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算devops系列---- ops</title>
          <link>https://kingjcy.github.io/post/cloud/paas/ops/ops/</link>
          <pubDate>Mon, 04 Jun 2018 11:26:37 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/ops/ops/</guid>
          <description>&lt;p&gt;应用程序升级面临最大挑战是新旧业务切换，将软件从测试的最后阶段带到生产环境，同时要保证系统不间断提供服务。长期以来，业务升级渐渐形成了几个发布策略：蓝绿发布、金丝雀发布和滚动发布等灰度发布策略，目的是尽可能避免因发布导致的流量丢失或服务不可用问题。&lt;/p&gt;

&lt;h1 id=&#34;蓝绿发布&#34;&gt;蓝绿发布&lt;/h1&gt;

&lt;p&gt;项目逻辑上分为AB组，在项目系统时，首先把A组从负载均衡中摘除，进行新版本的部署。B组仍然继续提供服务。当A组升级完毕，负载均衡重新接入A组，再把B组从负载列表中摘除，进行新版本的部署。A组重新提供服务。&lt;/p&gt;

&lt;p&gt;在k8s中也是经常使用的蓝绿发布，应用版本 1 与版本 2 的后端 pod 都部署在环境中，通过控制流量切换来决定发布哪个版本。与滚动发布相比，蓝绿发布策略下的应用终态，是可以同时存在版本 1 和版本 2 两种 pod 的，我们可以通过 service 流量的切换来决定当前服务使用哪个版本的后端。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/ops/ops1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;特点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基础设施无改动，增大升级稳定性。&lt;/li&gt;
&lt;li&gt;发布策略简单；&lt;/li&gt;
&lt;li&gt;用户无感知，平滑过渡；&lt;/li&gt;
&lt;li&gt;升级/回滚速度快。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果出问题，影响范围较大；&lt;/li&gt;
&lt;li&gt;需要准备正常业务使用资源的两倍以上服务器，防止升级期间单组无法承载业务突发；&lt;/li&gt;
&lt;li&gt;短时间内浪费一定资源成本；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;蓝绿发布在早期物理服务器时代，还是比较昂贵的，由于云计算普及，成本也大大降低。使得蓝绿发布变的更加适合使用&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、通过go-demo-v1.yaml，go-demo-v2.yaml，service.yaml部署应用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go-demo-v1.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo-v1
spec:
  replicas: 4
  selector:
    matchLabels:
      app: go-demo
      version: v1
  template:
    metadata:
      labels:
        app: go-demo
        version: v1
    spec:
      containers:
      - name: go-demo
        image: registry.cn-hangzhou.aliyuncs.com/haoshuwei24/go-demo:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080

go-demo-v2.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo-v2
spec:
  replicas: 4
  selector:
    matchLabels:
      app: go-demo
      version: v2
  template:
    metadata:
      labels:
        app: go-demo
        version: v2
    spec:
      containers:
      - name: go-demo
        image: registry.cn-hangzhou.aliyuncs.com/haoshuwei24/go-demo:v2
        imagePullPolicy: Always
        ports:
        - containerPort: 8080

service.yaml
apiVersion: v1
kind: Service
metadata:
  name: go-demo
spec:
  ports:
  - port: 80
    targetPort: 8080
    name: go-demo
  selector:
    app: go-demo
    version: v1
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、部署以上 3 个资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f go-demo-v1.yaml -f go-demo-v2.yaml -f service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、访问服务可以看到目前只访问到版本 1 的服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while sleep 0.1; do curl http://172.19.8.137; done
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、修改 service.yaml 的 spec.selector 下 version=v2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: go-demo
spec:
  ports:
  - port: 80
    targetPort: 8080
    name: go-demo
  selector:
    app: go-demo
    version: v2
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、重新部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、重新访问服务可以看到很快切换到了版本 2 上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ [root@iZbp13u3z7d2tqx0cs6ovqZ blue-green]# while sleep 0.1; do curl http://172.19.8.137; done
Version: v2
Version: v2
Version: v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们刚才说到滚动升级有一个过程需要时间，即使回滚，它也需要一定的时间才能回滚完毕，在新版本应用有缺陷的情况下，蓝绿发布的策略可以快速在 v1 和 v2 两个版本之前切流量，所以这个切换流量的时间跟滚动升级相比就缩短了很多了，但蓝绿发布的缺点跟滚动发布相同的就是这个缺陷会影响到整体用户，服务要么百分百切换到版本 2 上，要么百分百切换到版本 1 上，这是个非 0 即 100 的操作，即使蓝绿发布策略可以大大缩短故障恢复时间，但在某些场景下也是不可接受的。 而且集群环境中同时存在两个版本的 pod 副本，资源占用的话相比滚动发布是 2 倍的。&lt;/p&gt;

&lt;h1 id=&#34;金丝雀发布&#34;&gt;金丝雀发布&lt;/h1&gt;

&lt;p&gt;金丝雀发布只升级部分服务，即让一部分用户继续用老版本，一部分用户开始用新版本，如果用户对新版本没什么意见，那么逐步扩大范围，把所有用户都迁移到新版本上面来。&lt;/p&gt;

&lt;p&gt;金丝雀部署是应用版本 1 和版本 2 同时部署在环境中，并且用户请求有可能会路由到版本 1 的后端，也可能会路由到版本 2 的后端，从而达到让一部分新用户访问到版本 2 的应用。 这种发布策略下，我们可以通过调整流量百分比来逐步控制应用向新的版本切换，它与蓝绿部署相比，不仅继承了蓝绿部署的优点，而且占用资源优于蓝绿部署所需要的 2 倍资源，在新版本有缺陷的情况下只影响少部分用户，把损失降到最低。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/ops/ops2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;特点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;保证整体系统稳定性，在初始灰度的时候就可以发现、调整问题，影响范围可控；&lt;/li&gt;
&lt;li&gt;新功能逐步评估性能，稳定性和健康状况，如果出问题影响范围很小，相对用户体验也少；&lt;/li&gt;
&lt;li&gt;用户无感知，平滑过渡。&lt;/li&gt;
&lt;li&gt;继承了蓝绿部署的优点，而且占用资源优于蓝绿部署所需要的 2 倍资源&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;发布周期相对来说要慢很多&lt;/li&gt;
&lt;li&gt;自动化要求高&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过 pod 的数量来控制流量比例。&lt;/p&gt;

&lt;p&gt;1、go-demo-v1.yaml 设定副本数为 9&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo-v1
spec:
  replicas: 9
  selector:
    matchLabels:
      app: go-demo
      version: v1
  template:
    metadata:
      labels:
        app: go-demo
        version: v1
    spec:
      containers:
      - name: go-demo
        image: registry.cn-hangzhou.aliyuncs.com/haoshuwei24/go-demo:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、go-demo-v2.yaml 设定副本数为 1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo-v2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: go-demo
      version: v2
  template:
    metadata:
      labels:
        app: go-demo
        version: v2
    spec:
      containers:
      - name: go-demo
        image: registry.cn-hangzhou.aliyuncs.com/haoshuwei24/go-demo:v2
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、通过service.yaml启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: go-demo
spec:
  ports:
  - port: 80
    targetPort: 8080
    name: go-demo
  selector:
    app: go-demo
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、部署以上 3 个资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f go-demo-v1.yaml -f go-demo-v2.yaml -f service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、访问服务可以看到基本上是 10% 的流量切换到版本 2 上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while sleep 0.1; do curl http://172.19.8.248; done
Version: v1
Version: v2
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 nginx ingress controller 来控制流量切换，这个方式要更精准。&lt;/p&gt;

&lt;p&gt;1、go-demo-v1.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo-v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: go-demo
      version: v1
  template:
    metadata:
      labels:
        app: go-demo
        version: v1
    spec:
      containers:
      - name: go-demo
        image: registry.cn-hangzhou.aliyuncs.com/haoshuwei24/go-demo:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、go-demo-v2.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo-v2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: go-demo
      version: v2
  template:
    metadata:
      labels:
        app: go-demo
        version: v2
    spec:
      containers:
      - name: go-demo
        image: registry.cn-hangzhou.aliyuncs.com/haoshuwei24/go-demo:v2
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、service-v1.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: go-demo-v1
spec:
  ports:
  - port: 80
    targetPort: 8080
    name: go-demo
  selector:
    app: go-demo
    version: v1
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、service-v2.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: go-demo-v2
spec:
  ports:
  - port: 80
    targetPort: 8080
    name: go-demo
  selector:
    app: go-demo
    version: v2
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、ingress.yaml， 设置 nginx.ingress.kubernetes.io/service-weight: | go-demo-v1: 100, go-demo-v2: 0， 版本1 - 100% 流量， 版本2 - 0% 流量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/service-weight: |
        go-demo-v1: 100, go-demo-v2: 0
  name: go-demo
  labels:
    app: go-demo
spec:
  rules:
    - host: go-demo.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: go-demo-v1
              servicePort: 80
          - path: /
            backend:
              serviceName: go-demo-v2
              servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、部署以上 4 个资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f go-demo-v1.yaml -f go-demo-v2.yaml -f service-v1.yaml -f service-v2.yaml -f nginx.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7、访问服务可以看到流量 100% 到版本 1 上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while sleep 0.1; do curl http://go-demo.example.com; done
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8、更新 ingress.yaml， 设置流量比为 50:50&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/service-weight: |
        go-demo-v1: 50, go-demo-v2: 50
  name: go-demo
  labels:
    app: go-demo
spec:
  rules:
    - host: go-demo.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: go-demo-v1
              servicePort: 80
          - path: /
            backend:
              serviceName: go-demo-v2
              servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;9、访问服务可以看到流量 50% 到版本 1 上， 50% 到版本 2 上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while sleep 0.1; do curl http://go-demo.example.com; done
Version: v2
Version: v1
Version: v1
Version: v1
Version: v2
Version: v2
Version: v1
Version: v1
Version: v2
Version: v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种比滚动升级更加容易控制，也是使用比较多的方式。&lt;/p&gt;

&lt;h1 id=&#34;滚动发布&#34;&gt;滚动发布&lt;/h1&gt;

&lt;p&gt;滚动发布是指每次只升级一个或多个服务，升级完成后加入生产环境，不断执行这个过程，直到集群中的全部旧版本升级新版本。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/ops/ops&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;特点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;比较简单，用户无感知，平滑过渡&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;部署时间慢，取决于每阶段更新时间；&lt;/li&gt;
&lt;li&gt;发布策略较复杂，不易回滚&lt;/li&gt;
&lt;li&gt;无法控制&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、通过go-demo-v1.yaml部署应用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: go-demo
  template:
    metadata:
      labels:
        app: go-demo
    spec:
      containers:
      - name: go-demo
        image: registry.cn-hangzhou.aliyuncs.com/haoshuwei24/go-demo:v1
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: go-demo
spec:
  ports:
  - port: 80
    targetPort: 8080
    name: go-demo
  selector:
    app: go-demo
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、部署版本v1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f go-demo-v1.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、查看 pod 运行状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get po
NAME                       READY   STATUS    RESTARTS   AGE
go-demo-78bc65c564-2rhxp   1/1     Running   0          19s
go-demo-78bc65c564-574z6   1/1     Running   0          19s
go-demo-78bc65c564-sgl2s   1/1     Running   0          19s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、访问应用服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while sleep 0.1; do curl http://172.19.15.25; done
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
Version: v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、更新 go-demo-v1.yaml 为 go-demo-v2.yaml 并更新镜像 tag&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
registry.cn-hangzhou.aliyuncs.com/haoshuwei24/go-demo:v2
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、部署版本 v2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f go-demo-v2.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7、可以查看 pod 会被新版本 pod 逐个替换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl get po -w
NAME                                READY   STATUS              RESTARTS   AGE
application-demo-8594ff4967-85jsg   1/1     Running             0          3m24s
application-demo-8594ff4967-d4sv8   1/1     Terminating         0          3m22s
application-demo-8594ff4967-w6lpz   0/1     Terminating         0          3m20s
application-demo-b98d94554-4mwqd    1/1     Running             0          3s
application-demo-b98d94554-ng9wx    0/1     ContainerCreating   0          1s
application-demo-b98d94554-pmc5g    1/1     Running             0          4s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8、访问服务会发现在应用滚动升级过程中，版本 v1 和 v2 都会被访问到，这个时间的长短取决于应用的启动速度&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while sleep 0.1; do curl http://172.19.15.25; done
Version: v1
Version: v2
Version: v1
Version: v1
Version: v2
Version: v1
Version: v1
Version: v2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见k8s默认使用的就是滚动升级的方式，直接使用apply就行，但是这种方式不可控，不能定制化，所以很多发布场景需要开发对应的控制器，比如阿里的kurise项目。&lt;/p&gt;

&lt;h1 id=&#34;小结&#34;&gt;小结&lt;/h1&gt;

&lt;p&gt;综上所述，三种方式均可以做到平滑式升级，在升级过程中服务仍然保持服务的连续性，升级对外界是无感知的。那生产上选择哪种部署方法最合适呢？这取决于哪种方法最适合你的业务和技术需求。如果你们运维自动化能力储备不够，肯定是越简单越好，建议蓝绿发布，如果业务对用户依赖很强，建议灰度发布。如果是K8S平台，滚动更新是现成的方案，建议先直接使用。&lt;/p&gt;

&lt;p&gt;蓝绿发布：两套环境交替升级，旧版本保留一定时间便于回滚。&lt;/p&gt;

&lt;p&gt;灰度发布：根据比例将老版本升级，例如80%用户访问是老版本，20%用户访问是新版本。&lt;/p&gt;

&lt;p&gt;滚动发布：按批次停止老版本实例，启动新版本实例。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Principle</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/</link>
          <pubDate>Sun, 13 May 2018 17:56:03 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/</guid>
          <description>&lt;p&gt;本篇文章主要是对prometheus的一些原理进行解析。&lt;/p&gt;

&lt;h1 id=&#34;启动流程解析&#34;&gt;启动流程解析&lt;/h1&gt;

&lt;p&gt;Prometheus 启动过程中，主要包含服务组件初始化，服务组件配置应用及启动各个服务组件三个部分，下面基于版本 v2.7.1，详细分析这三部分内容。&lt;/p&gt;

&lt;h2 id=&#34;服务组件初始化&#34;&gt;服务组件初始化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Storage组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prometheus的Storage组件是时序数据库，包含两个：localStorage和remoteStorage。localStorage当前版本指TSDB，用于对metrics的本地存储存储，remoteStorage用于metrics的远程存储，其中fanoutStorage作为localStorage和remoteStorage的读写代理服务器。&lt;/p&gt;

&lt;p&gt;初始化流程如下，代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;localStorage  = &amp;amp;tsdb.ReadyStorage{} //本地存储
remoteStorage = remote.NewStorage(log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;remote&amp;quot;), //远端存储 localStorage.StartTime, time.Duration(cfg.RemoteFlushDeadline))
fanoutStorage = storage.NewFanout(logger, localStorage, remoteStorage) //读写代理服务器
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;notifier 组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;notifier组件用于发送告警信息给AlertManager，通过方法notifier.NewManager完成初始化&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;notifierManager = notifier.NewManager(&amp;amp;cfg.notifier, log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;notifier&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;discoveryManagerScrape组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;discoveryManagerScrape组件用于服务发现，当前版本支持多种服务发现系统，比如kuberneters等，通过方法discovery.NewManager完成初始化。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;discoveryManagerScrape  = discovery.NewManager(ctxScrape, log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;discovery manager scrape&amp;quot;), discovery.Name(&amp;quot;scrape&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;discoveryManagerNotify组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;discoveryManagerNotify组件用于告警通知服务发现，比如AlertManager服务．也是通过方法discovery.NewManager完成初始化，不同的是，discoveryManagerNotify服务于notify，而discoveryManagerScrape服务于scrape。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;discoveryManagerNotify  = discovery.NewManager(ctxNotify, log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;discovery manager notify&amp;quot;), discovery.Name(&amp;quot;notify&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;scrapeManager组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrapeManager组件利用discoveryManagerScrape组件发现的targets，抓取对应targets的所有metrics，并将抓取的metrics存储到fanoutStorage中，通过方法scrape.NewManager完成初始化。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrapeManager = scrape.NewManager(log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;scrape manager&amp;quot;), fanoutStorage)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;queryEngine组件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;queryEngine组件用于rules查询和计算，通过方法promql.NewEngine完成初始化。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;opts = promql.EngineOpts{
    Logger:        log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;query engine&amp;quot;),
    Reg:           prometheus.DefaultRegisterer,
    MaxConcurrent: cfg.queryConcurrency,　　　　　　　//最大并发查询个数
    MaxSamples:    cfg.queryMaxSamples,
    Timeout:       time.Duration(cfg.queryTimeout),　//查询超时时间
}
queryEngine = promql.NewEngine(opts)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;ruleManager组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ruleManager组件通过方法rules.NewManager完成初始化．其中rules.NewManager的参数涉及多个组件：存储，queryEngine和notifier，整个流程包含rule计算和发送告警。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruleManager = rules.NewManager(&amp;amp;rules.ManagerOptions{
    Appendable:      fanoutStorage,                        //存储器
    TSDB:            localStorage,　　　　　　　　　　　　　　//本地时序数据库TSDB
    QueryFunc:       rules.EngineQueryFunc(queryEngine, fanoutStorage), //rules计算
    NotifyFunc:      sendAlerts(notifierManager, cfg.web.ExternalURL.String()),　//告警通知
    Context:         ctxRule,　//用于控制ruleManager组件的协程
    ExternalURL:     cfg.web.ExternalURL,　//通过Web对外开放的URL
    Registerer:      prometheus.DefaultRegisterer,
    Logger:          log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;rule manager&amp;quot;),
    OutageTolerance: time.Duration(cfg.outageTolerance), //当prometheus重启时，保持alert状态（https://ganeshvernekar.com/gsoc-2018/persist-for-state/）
    ForGracePeriod:  time.Duration(cfg.forGracePeriod),
    ResendDelay:     time.Duration(cfg.resendDelay),
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Web组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Web组件用于为Storage组件，queryEngine组件，scrapeManager组件， ruleManager组件和notifier 组件提供外部HTTP访问方式，也就是我们经常访问的prometheus的界面。&lt;/p&gt;

&lt;p&gt;初始化代码如下，代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfg.web.Context = ctxWeb
cfg.web.TSDB = localStorage.Get
cfg.web.Storage = fanoutStorage
cfg.web.QueryEngine = queryEngine
cfg.web.ScrapeManager = scrapeManager
cfg.web.RuleManager = ruleManager
cfg.web.Notifier = notifierManager

cfg.web.Version = &amp;amp;web.PrometheusVersion{
    Version:   version.Version,
    Revision:  version.Revision,
    Branch:    version.Branch,
    BuildUser: version.BuildUser,
    BuildDate: version.BuildDate,
    GoVersion: version.GoVersion,
}

cfg.web.Flags = map[string]string{}

// Depends on cfg.web.ScrapeManager so needs to be after cfg.web.ScrapeManager = scrapeManager
webHandler := web.New(log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;web&amp;quot;), &amp;amp;cfg.web)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;服务组件配置应用&#34;&gt;服务组件配置应用&lt;/h2&gt;

&lt;p&gt;除了服务组件ruleManager用的方法是Update，其他服务组件的在匿名函数中通过各自的ApplyConfig方法，实现配置的管理。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reloaders := []func(cfg *config.Config) error{
    remoteStorage.ApplyConfig, //存储配置
    webHandler.ApplyConfig,    //web配置
    notifierManager.ApplyConfig, //notifier配置
    scrapeManager.ApplyConfig,　　//scrapeManger配置
　　//从配置文件中提取Section:scrape_configs
    func(cfg *config.Config) error {
        c := make(map[string]sd_config.ServiceDiscoveryConfig)
        for _, v := range cfg.ScrapeConfigs {
            c[v.JobName] = v.ServiceDiscoveryConfig
        }
        return discoveryManagerScrape.ApplyConfig(c)
    },
    //从配置文件中提取Section:alerting
    func(cfg *config.Config) error {
        c := make(map[string]sd_config.ServiceDiscoveryConfig)
        for _, v := range cfg.AlertingConfig.AlertmanagerConfigs {
            // AlertmanagerConfigs doesn&#39;t hold an unique identifier so we use the config hash as the identifier.
            b, err := json.Marshal(v)
            if err != nil {
                return err
            }
            c[fmt.Sprintf(&amp;quot;%x&amp;quot;, md5.Sum(b))] = v.ServiceDiscoveryConfig
        }
        return discoveryManagerNotify.ApplyConfig(c)
    },
    //从配置文件中提取Section:rule_files
    func(cfg *config.Config) error {
        // Get all rule files matching the configuration paths.
        var files []string
        for _, pat := range cfg.RuleFiles {
            fs, err := filepath.Glob(pat)
            if err != nil {
                // The only error can be a bad pattern.
                return fmt.Errorf(&amp;quot;error retrieving rule files for %s: %s&amp;quot;, pat, err)
            }
            files = append(files, fs...)
        }
        return ruleManager.Update(time.Duration(cfg.GlobalConfig.EvaluationInterval), files)
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务组件remoteStorage，webHandler，notifierManager和ScrapeManager的ApplyConfig方法，参数cfg *config.Config中传递的配置文件，是整个文件prometheus.yml&lt;/p&gt;

&lt;p&gt;代码文件prometheus/scrape/manager.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg *config.Config) error {
   .......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务组件discoveryManagerScrape和discoveryManagerNotify的ApplyConfig方法，参数中传递的配置文件，是文件中的一个Section&lt;/p&gt;

&lt;p&gt;代码文件prometheus/discovery/manager.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg map[string]sd_config.ServiceDiscoveryConfig) error {
     ......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以，需要利用匿名函数提前处理下，取出对应的Section。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//从配置文件中提取Section:scrape_configs
func(cfg *config.Config) error {
    c := make(map[string]sd_config.ServiceDiscoveryConfig)
    for _, v := range cfg.ScrapeConfigs {
        c[v.JobName] = v.ServiceDiscoveryConfig
    }
    return discoveryManagerScrape.ApplyConfig(c)
},
//从配置文件中提取Section:alerting
func(cfg *config.Config) error {
    c := make(map[string]sd_config.ServiceDiscoveryConfig)
    for _, v := range cfg.AlertingConfig.AlertmanagerConfigs {
        // AlertmanagerConfigs doesn&#39;t hold an unique identifier so we use the config hash as the identifier.
        b, err := json.Marshal(v)
        if err != nil {
            return err
        }
        c[fmt.Sprintf(&amp;quot;%x&amp;quot;, md5.Sum(b))] = v.ServiceDiscoveryConfig
    }
    return discoveryManagerNotify.ApplyConfig(c)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务组件ruleManager，在匿名函数中提取出Section:rule_files&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//从配置文件中提取Section:rule_files
func(cfg *config.Config) error {
    // Get all rule files matching the configuration paths.
    var files []string
    for _, pat := range cfg.RuleFiles {
        fs, err := filepath.Glob(pat)
        if err != nil {
            // The only error can be a bad pattern.
            return fmt.Errorf(&amp;quot;error retrieving rule files for %s: %s&amp;quot;, pat, err)
        }
        files = append(files, fs...)
    }
    return ruleManager.Update(time.Duration(cfg.GlobalConfig.EvaluationInterval), files)
},
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;利用该组件内置的Update方法完成配置管理。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/rules/manager.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Update(interval time.Duration, files []string) error {
  .......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，通过reloadConfig方法，加载各个服务组件的配置项&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func reloadConfig(filename string, logger log.Logger, rls ...func(*config.Config) error) (err error) {
    level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Loading configuration file&amp;quot;, &amp;quot;filename&amp;quot;, filename)

    defer func() {
        if err == nil {
            configSuccess.Set(1)
            configSuccessTime.SetToCurrentTime()
        } else {
            configSuccess.Set(0)
        }
    }()

    conf, err := config.LoadFile(filename)
    if err != nil {
        return fmt.Errorf(&amp;quot;couldn&#39;t load configuration (--config.file=%q): %v&amp;quot;, filename, err)
    }

    failed := false
　　//通过一个for循环，加载各个服务组件的配置项
    for _, rl := range rls {
        if err := rl(conf); err != nil {
            level.Error(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Failed to apply configuration&amp;quot;, &amp;quot;err&amp;quot;, err)
            failed = true
        }
    }
    if failed {
        return fmt.Errorf(&amp;quot;one or more errors occurred while applying the new configuration (--config.file=%q)&amp;quot;, filename)
    }
    promql.SetDefaultEvaluationInterval(time.Duration(conf.GlobalConfig.EvaluationInterval))
    level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Completed loading of configuration file&amp;quot;, &amp;quot;filename&amp;quot;, filename)
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;服务组件启动&#34;&gt;服务组件启动&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里引用了github.com/oklog/oklog/pkg/group包，实例化一个对象g&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// &amp;quot;github.com/oklog/oklog/pkg/group&amp;quot;
var g group.Group
{
　　......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对象g中包含各个服务组件的入口，通过调用Add方法把把这些入口添加到对象g中，以组件scrapeManager为例。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    // Scrape manager.
　　//通过方法Add，把ScrapeManager组件添加到g中
    g.Add(
        func() error {
            // When the scrape manager receives a new targets list
            // it needs to read a valid config for each job.
            // It depends on the config being in sync with the discovery manager so
            // we wait until the config is fully loaded.
            &amp;lt;-reloadReady.C
　　　　　　　//ScrapeManager组件的启动函数
            err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
            level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape manager stopped&amp;quot;)
            return err
        },
        func(err error) {
            // Scrape manager needs to be stopped before closing the local TSDB
            // so that it doesn&#39;t try to write samples to a closed storage.
            level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Stopping scrape manager...&amp;quot;)
            scrapeManager.Stop()
        },
    )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;run&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过对象g，调用方法run，启动所有服务组件&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if err := g.Run(); err != nil {
    level.Error(logger).Log(&amp;quot;err&amp;quot;, err)
    os.Exit(1)
}
level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;See you next time!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，Prometheus的启动过程分析完成。&lt;/p&gt;

&lt;h1 id=&#34;内部结构&#34;&gt;内部结构&lt;/h1&gt;

&lt;p&gt;Prometheus的内部主要分为三大块，Retrieval是负责定时去暴露的目标页面上去抓取采样指标数据，Storage是负责将采样数据写磁盘，PromQL是Prometheus提供的查询语言模块。当然还有其他的一些组件，可以参考上面组件的初始化，比如一些web，notify等。&lt;/p&gt;

&lt;h2 id=&#34;retrieval&#34;&gt;Retrieval&lt;/h2&gt;

&lt;h3 id=&#34;采集实现&#34;&gt;采集实现&lt;/h3&gt;

&lt;p&gt;​Prometheus采集数据使用pull模式，通过HTTP协议去采集指标，只要应用系统能够提供HTTP接口就可以接入监控系统。&lt;/p&gt;

&lt;p&gt;​拉取目标称之为scrape，一个scrape一般对应一个进程，如下为scrape相关的配置。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;配置文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;scrape_interval:     15s
scrape_configs:
  - job_name: &#39;test_server_name&#39;
    static_configs:
    - targets: [&#39;localhost:8886&#39;]
      labels:
        project: &#39;test_server&#39;
        environment: &#39;test&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该配置描述：每15秒去拉取一次上报数据，拉取目标为localhost:8886。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;读取配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ScrapeConfig的结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ScrapeConfig struct {
   //  作业名称
   JobName string `yaml:&amp;quot;job_name&amp;quot;`
   // 同名lable，是否覆盖处理
   HonorLabels bool `yaml:&amp;quot;honor_labels,omitempty&amp;quot;`
   HonorTimestamps bool `yaml:&amp;quot;honor_timestamps&amp;quot;`
   // 采集目标url参数
   Params url.Values `yaml:&amp;quot;params,omitempty&amp;quot;`
   // 采集周期
   ScrapeInterval model.Duration `yaml:&amp;quot;scrape_interval,omitempty&amp;quot;`
   // 采集超时时间
   ScrapeTimeout model.Duration `yaml:&amp;quot;scrape_timeout,omitempty&amp;quot;`
   // 目标 URl path
   MetricsPath string `yaml:&amp;quot;metrics_path,omitempty&amp;quot;`
   Scheme string `yaml:&amp;quot;scheme,omitempty&amp;quot;`
   SampleLimit uint `yaml:&amp;quot;sample_limit,omitempty&amp;quot;`
   // 服务发现配置
   ServiceDiscoveryConfig sd_config.ServiceDiscoveryConfig `yaml:&amp;quot;,inline&amp;quot;`
   // 客户端http client 配置
   HTTPClientConfig       config_util.HTTPClientConfig     `yaml:&amp;quot;,inline&amp;quot;`
   // 目标重置规则
   RelabelConfigs []*relabel.Config `yaml:&amp;quot;relabel_configs,omitempty&amp;quot;`
   // 指标重置规则
   MetricRelabelConfigs []*relabel.Config `yaml:&amp;quot;metric_relabel_configs,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读取配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg *config.Config) error {
    m.mtxScrape.Lock()
    defer m.mtxScrape.Unlock()
    // 初始化map结构，用于保存配置
    c := make(map[string]*config.ScrapeConfig)
    for _, scfg := range cfg.ScrapeConfigs {
    // 配置读取维度
        c[scfg.JobName] = scfg
    }
    m.scrapeConfigs = c
    // 设置 所有时间序列和警告与外部通信时用的外部标签 external_labels
    if err := m.setJitterSeed(cfg.GlobalConfig.ExternalLabels); err != nil {
        return err
    }

    // 如果配置已经更改，清理历史配置，重新加载到池子中
    var failed bool
    for name, sp := range m.scrapePools {
    // 如果当前job不存在，则删除
        if cfg, ok := m.scrapeConfigs[name]; !ok {
            sp.stop()
            delete(m.scrapePools, name)
        } else if !reflect.DeepEqual(sp.config, cfg) {
      // 如果配置变更，重新启动reload，进行加载
            err := sp.reload(cfg)
            if err != nil {
                level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error reloading scrape pool&amp;quot;, &amp;quot;err&amp;quot;, err, &amp;quot;scrape_pool&amp;quot;, name)
                failed = true
            }
        }
    }
    // 失败 return
    if failed {
        return errors.New(&amp;quot;failed to apply the new configuration&amp;quot;)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus 中，将任意一个独立的数据源（target）称之为实例（instance）。包含相同类型的实例的集合称之为作业（job)，从读取配置中，我们也能看到，以job为key。所以注意job在业务侧的使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Scrape Manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;​
添加Scrape Manager 到 run.Group启动。reloadReady.C的作用是当Manager接收到一组数据采集目标(target)的时候，他需要为每个job读取有效的配置。因此这里等待所有配置加载完成，进行下一步。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;g.Add(
            func() error {
        // 当所有配置都准备好
                &amp;lt;-reloadReady.C
                // 启动scrapeManager
                err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
                level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape manager stopped&amp;quot;)
                return err
            },
            func(err error) {
        // 失败处理
                level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Stopping scrape manager...&amp;quot;)
                scrapeManager.Stop()
            },
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;加载Targets&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加载targets，如果targets更新，会触发重新加载，reloader的加载发生在后台，所以并不会影响target的更新，(配置文件中配置的target是依赖discoveryManagerScrape.ApplyConfig&amp;copy;进行加载的，后面分析target服务发现的时候详细分析)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Run(tsets &amp;lt;-chan map[string][]*targetgroup.Group) error {
    go m.reloader()
    for {
        select {
    // 触发重新加载目标。添加新增
        case ts := &amp;lt;-tsets:
            m.updateTsets(ts)

            select {
      // 关闭 Scrape Manager 处理信号
            case m.triggerReload &amp;lt;- struct{}{}:
            default:
            }

        case &amp;lt;-m.graceShut:
            return nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;scrape pool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;顺着Run继续阅读，reload为每一组tatget生成一个对应的scrape pool管理targets集合，scrapePool结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type scrapePool struct {
   appendable Appendable
   logger     log.Logger
     // 读写锁
   mtx    sync.RWMutex
   // Scrape 配置
   config *config.ScrapeConfig
   // http client
   client *http.Client

   // 正在运行的target
   activeTargets  map[uint64]*Target
   // 无效的target
   droppedTargets []*Target
   // 所有运行的loop
   loops          map[uint64]loop
   // 取消
   cancel         context.CancelFunc
     // 创建loop
   newLoop func(scrapeLoopOptions) loop
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;执行reload&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;m.reloade的流程也很简单，setName指我们配置中的job，如果scrapePools不存在该job，则添加，添加前也是先校验该job的配置是否存在，不存在则报错，创建scrape pool。总结看就是为每个job创建与之对应的scrape pool&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) reload() {
   //加锁
   m.mtxScrape.Lock()
   var wg sync.WaitGroup
   for setName, groups := range m.targetSets {
       //检查该scrape是否存在scrapePools，不存在则创建
      if _, ok := m.scrapePools[setName]; !ok {
         //读取该scrape的配置
         scrapeConfig, ok := m.scrapeConfigs[setName]
         if !ok {
            // 未读取到该scrape的配置打印错误
            level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error reloading target set&amp;quot;, &amp;quot;err&amp;quot;, &amp;quot;invalid config id:&amp;quot;+setName)
            // 跳出
            continue
         }
         // 创建该scrape的scrape pool
         sp, err := newScrapePool(scrapeConfig, m.append, m.jitterSeed, log.With(m.logger, &amp;quot;scrape_pool&amp;quot;, setName))
         if err != nil {
            level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error creating new scrape pool&amp;quot;, &amp;quot;err&amp;quot;, err, &amp;quot;scrape_pool&amp;quot;, setName)
            continue
         }
         // 保存
         m.scrapePools[setName] = sp
      }

      wg.Add(1)
            // 并行运行，提升性能。
      go func(sp *scrapePool, groups []*targetgroup.Group) {
         sp.Sync(groups)
         wg.Done()
      }(m.scrapePools[setName], groups)

   }
   // 释放锁
   m.mtxScrape.Unlock()
   // 阻塞，等待所有pool运行完毕
   wg.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;创建scrape pool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrape pool利用newLoop去为该job下的所有target生成对应的loop：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newScrapePool(cfg *config.ScrapeConfig, app Appendable, jitterSeed uint64, logger log.Logger) (*scrapePool, error) {
    targetScrapePools.Inc()
    if logger == nil {
        logger = log.NewNopLogger()
    }
  // 创建http client，用于执行数据抓取
    client, err := config_util.NewClientFromConfig(cfg.HTTPClientConfig, cfg.JobName)
    if err != nil {
        targetScrapePoolsFailed.Inc()
        return nil, errors.Wrap(err, &amp;quot;error creating HTTP client&amp;quot;)
    }
    // 设置buffers
    buffers := pool.New(1e3, 100e6, 3, func(sz int) interface{} { return make([]byte, 0, sz) })
  // 设置scrapePool的一些基础属性
    ctx, cancel := context.WithCancel(context.Background())
    sp := &amp;amp;scrapePool{
        cancel:        cancel,
        appendable:    app,
        config:        cfg,
        client:        client,
        activeTargets: map[uint64]*Target{},
        loops:         map[uint64]loop{},
        logger:        logger,
    }
  // newLoop用于生层loop，主要处理对应的target，可以理解为，每个target对应一个loop。
    sp.newLoop = func(opts scrapeLoopOptions) loop {
        // Update the targets retrieval function for metadata to a new scrape cache.
        cache := newScrapeCache()
        opts.target.setMetadataStore(cache)

        return newScrapeLoop(
            ctx,
            opts.scraper,
            log.With(logger, &amp;quot;target&amp;quot;, opts.target),
            buffers,
            func(l labels.Labels) labels.Labels {
                return mutateSampleLabels(l, opts.target, opts.honorLabels, opts.mrc)
            },
            func(l labels.Labels) labels.Labels { return mutateReportSampleLabels(l, opts.target) },
            func() storage.Appender {
                app, err := app.Appender()
                if err != nil {
                    panic(err)
                }
                return appender(app, opts.limit)
            },
            cache,
            jitterSeed,
            opts.honorTimestamps,
        )
    }

    return sp, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;group转化为target&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrape pool创建完成后，则通过sp.Sync执行，使用该job对应的pool遍历Group，使其转换为target&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go func(sp *scrapePool, groups []*targetgroup.Group) {
   sp.Sync(groups)
   wg.Done()
}(m.scrapePools[setName], groups)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sync函数解读如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sp *scrapePool) Sync(tgs []*targetgroup.Group) {
   start := time.Now()

   var all []*Target
   // 加锁
   sp.mtx.Lock()
   sp.droppedTargets = []*Target{}
   // 遍历所有Group
   for _, tg := range tgs {
        // 转化对应 targets
      targets, err := targetsFromGroup(tg, sp.config)
      if err != nil {
         level.Error(sp.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;creating targets failed&amp;quot;, &amp;quot;err&amp;quot;, err)
         continue
      }
      // 将所有有效targets添加到all，等待处理
      for _, t := range targets {
         // 检查该target的lable是否有效
         if t.Labels().Len() &amp;gt; 0 {
            // 添加到all队列中
            all = append(all, t)
         } else if t.DiscoveredLabels().Len() &amp;gt; 0 {
            // 记录无效target
            sp.droppedTargets = append(sp.droppedTargets, t)
         }
      }
   }
   // 解锁
   sp.mtx.Unlock()
   // 处理all队列，执行scarape同步操作
   sp.sync(all)

   targetSyncIntervalLength.WithLabelValues(sp.config.JobName).Observe(
      time.Since(start).Seconds(),
   )
   targetScrapePoolSyncsCounter.WithLabelValues(sp.config.JobName).Inc()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;生成loop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在sync最后，调用了当前scrape pool的sync去处理all队列中的target，添加新的target，删除失效的target。实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sp *scrapePool) sync(targets []*Target) {
  // 加锁
    sp.mtx.Lock()
    defer sp.mtx.Unlock()

    var (
    // target 标记
        uniqueTargets   = map[uint64]struct{}{}
    // 采集周期
        interval        = time.Duration(sp.config.ScrapeInterval)
    // 采集超时时间
        timeout         = time.Duration(sp.config.ScrapeTimeout)
        limit           = int(sp.config.SampleLimit)
    // 重复lable是否覆盖
        honorLabels     = sp.config.HonorLabels
        honorTimestamps = sp.config.HonorTimestamps
        mrc             = sp.config.MetricRelabelConfigs
    )
    // 遍历all队列中的所有target
    for _, t := range targets {
    // 赋值，避免range的坑
        t := t
    // 生成对应的hash（对该hash算法感兴趣可以看下这里的源码）
        hash := t.hash()
    // 标记
        uniqueTargets[hash] = struct{}{}
        // 判断该taget是否已经在运行了。如果没有则运行该target对应的loop，将该loop加入activeTargets中
        if _, ok := sp.activeTargets[hash]; !ok {
            s := &amp;amp;targetScraper{Target: t, client: sp.client, timeout: timeout}
            l := sp.newLoop(scrapeLoopOptions{
                target:          t,
                scraper:         s,
                limit:           limit,
                honorLabels:     honorLabels,
                honorTimestamps: honorTimestamps,
                mrc:             mrc,
            })

            sp.activeTargets[hash] = t
            sp.loops[hash] = l
            // 启动该loop
            go l.run(interval, timeout, nil)
        } else {
      // 该target对应的loop已经运行，设置最新的标签信息
            sp.activeTargets[hash].SetDiscoveredLabels(t.DiscoveredLabels())
        }
    }

    var wg sync.WaitGroup

  // 停止并且移除无效的targets与对应的loops
  // 遍历activeTargets正在执行的Target
    for hash := range sp.activeTargets {
    // 检查该hash对应的标记是否存在，放过不存在执行清除逻辑
        if _, ok := uniqueTargets[hash]; !ok {
            wg.Add(1)
      // 异步清除
            go func(l loop) {
                // 停止该loop
                l.stop()
                // 执行完成
                wg.Done()
            }(sp.loops[hash])
            // 从loops中删除该hash对应的loop
            delete(sp.loops, hash)
      // 从activeTargets中删除该hash对应的target
            delete(sp.activeTargets, hash)
        }
    }
  // 等待所有执行完成
    wg.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;运行loop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrape pool对应的sync的实现中可以看到，如果该target没有运行，则启动该target对应的loop，执行l.run，通过一个goroutine来执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sl *scrapeLoop) run(interval, timeout time.Duration, errc chan&amp;lt;- error) {
  // 偏移量相关设置
    select {
    case &amp;lt;-time.After(sl.scraper.offset(interval, sl.jitterSeed)):
        // Continue after a scraping offset.
    case &amp;lt;-sl.scrapeCtx.Done():
        close(sl.stopped)
        return
    }

    var last time.Time
    // 根据interval设置定时器
    ticker := time.NewTicker(interval)
    defer ticker.Stop()

mainLoop:
    for {
        select {
        case &amp;lt;-sl.ctx.Done():
            close(sl.stopped)
            return
        case &amp;lt;-sl.scrapeCtx.Done():
            break mainLoop
        default:
        }

        var (
            start             = time.Now()
            scrapeCtx, cancel = context.WithTimeout(sl.ctx, timeout)
        )

        // 记录第一次
        if !last.IsZero() {
            targetIntervalLength.WithLabelValues(interval.String()).Observe(
                time.Since(last).Seconds(),
            )
        }
        // 根据上次拉取数据的大小，设置buffer空间
        b := sl.buffers.Get(sl.lastScrapeSize).([]byte)
        buf := bytes.NewBuffer(b)
        // 读取数据，设置到buffer中
        contentType, scrapeErr := sl.scraper.scrape(scrapeCtx, buf)
    // 取消，结束scrape
        cancel()

        if scrapeErr == nil {
            b = buf.Bytes()
            if len(b) &amp;gt; 0 {
        // 记录本次Scrape大小
                sl.lastScrapeSize = len(b)
            }
        } else {
      // 错误处理
            level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape failed&amp;quot;, &amp;quot;err&amp;quot;, scrapeErr.Error())
            if errc != nil {
                errc &amp;lt;- scrapeErr
            }
        }

        // 生成数据，存储指标
        total, added, seriesAdded, appErr := sl.append(b, contentType, start)
        if appErr != nil {
            level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;append failed&amp;quot;, &amp;quot;err&amp;quot;, appErr)

            if _, _, _, err := sl.append([]byte{}, &amp;quot;&amp;quot;, start); err != nil {
                level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;append failed&amp;quot;, &amp;quot;err&amp;quot;, err)
            }
        }
        // 对象复用
        sl.buffers.Put(b)

        if scrapeErr == nil {
            scrapeErr = appErr
        }
        // 上报指标，进行统计
        if err := sl.report(start, time.Since(start), total, added, seriesAdded, scrapeErr); err != nil {
            level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;appending scrape report failed&amp;quot;, &amp;quot;err&amp;quot;, err)
        }
    // 重置时间位置
        last = start

        select {
        case &amp;lt;-sl.ctx.Done():
            close(sl.stopped)
            return
        case &amp;lt;-sl.scrapeCtx.Done():
            break mainLoop
        case &amp;lt;-ticker.C:
        }
    }

    close(sl.stopped)

    sl.endOfRunStaleness(last, ticker, interval)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;拉取数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;依赖scrape实现数据的抓取，使用GET方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *targetScraper) scrape(ctx context.Context, w io.Writer) (string, error) {
    if s.req == nil {
    // 新建Http Request
        req, err := http.NewRequest(&amp;quot;GET&amp;quot;, s.URL().String(), nil)
        if err != nil {
            return &amp;quot;&amp;quot;, err
        }
    // 设置请求头
        req.Header.Add(&amp;quot;Accept&amp;quot;, acceptHeader)
        req.Header.Add(&amp;quot;Accept-Encoding&amp;quot;, &amp;quot;gzip&amp;quot;)
        req.Header.Set(&amp;quot;User-Agent&amp;quot;, userAgentHeader)
        req.Header.Set(&amp;quot;X-Prometheus-Scrape-Timeout-Seconds&amp;quot;, fmt.Sprintf(&amp;quot;%f&amp;quot;, s.timeout.Seconds()))

        s.req = req
    }
    // 发起请求
    resp, err := s.client.Do(s.req.WithContext(ctx))
    if err != nil {
        return &amp;quot;&amp;quot;, err
    }
    defer func() {
        io.Copy(ioutil.Discard, resp.Body)
        resp.Body.Close()
    }()
    // 错误处理
    if resp.StatusCode != http.StatusOK {
        return &amp;quot;&amp;quot;, errors.Errorf(&amp;quot;server returned HTTP status %s&amp;quot;, resp.Status)
    }
    // 检查Content-Encoding
    if resp.Header.Get(&amp;quot;Content-Encoding&amp;quot;) != &amp;quot;gzip&amp;quot; {
    // copy buffer到w
        _, err = io.Copy(w, resp.Body)
        if err != nil {
            return &amp;quot;&amp;quot;, err
        }
        return resp.Header.Get(&amp;quot;Content-Type&amp;quot;), nil
    }
    if s.gzipr == nil {
        s.buf = bufio.NewReader(resp.Body)
        s.gzipr, err = gzip.NewReader(s.buf)
        if err != nil {
            return &amp;quot;&amp;quot;, err
        }
    } else {
        s.buf.Reset(resp.Body)
        if err = s.gzipr.Reset(s.buf); err != nil {
            return &amp;quot;&amp;quot;, err
        }
    }

    _, err = io.Copy(w, s.gzipr)
    s.gzipr.Close()
    if err != nil {
        return &amp;quot;&amp;quot;, err
    }
    return resp.Header.Get(&amp;quot;Content-Type&amp;quot;), nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每一个job有一个与之对应的scrape pool，每一个target有一个与之对应的loop，每个loop内部执 Http Get请求拉取数据。通过一些控制参数，控制采集周期以及结束等逻辑。&lt;/p&gt;

&lt;h3 id=&#34;数据规范&#34;&gt;数据规范&lt;/h3&gt;

&lt;h4 id=&#34;数据模型&#34;&gt;数据模型&lt;/h4&gt;

&lt;p&gt;Prometheus与其他主流时序数据库一样，在数据模型定义上，也会包含metric name、一个或多个labels（同tags）以及metric value&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/datamodel.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图是所有数据点分布的一个简单视图，横轴是时间，纵轴是时间线，区域内每个点就是数据点。Prometheus每次接收数据，收到的是图中区域内纵向的一条线。这个表述很形象，在同一时刻，每条时间线只会产生一个数据点，但同时会有多条时间线产生数据，把这些数据点连在一起，就是一条竖线。这个特征很重要，影响数据写入和压缩的优化策略。&lt;/p&gt;

&lt;h3 id=&#34;探针数据&#34;&gt;探针数据&lt;/h3&gt;

&lt;p&gt;都体现在client_golang的库中，直接去&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/&#34;&gt;client_golang&lt;/a&gt;文章中参考。&lt;/p&gt;

&lt;h2 id=&#34;promql&#34;&gt;PromQL&lt;/h2&gt;

&lt;p&gt;PromQL 是 Prom 中的查询语言，提供了简洁的、贴近自然语言的语法实现时序数据的分析计算。&lt;/p&gt;

&lt;h3 id=&#34;原理&#34;&gt;原理&lt;/h3&gt;

&lt;p&gt;PromQL 表达式输入是一段文本，Prom 会解析这段文本，将它转化为一个结构化的语法树对象，进而实现相应的数据计算逻辑。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(avg_over_time(go_goroutines{job=&amp;quot;prometheus&amp;quot;}[5m])) by (instance)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述表达式可以从外往内分解为三层：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(…) by (instance)：序列纵向分组合并序列（包含相同的 instance 会分配到一组）
avg_over_time(…)
go_goroutines{job=&amp;quot;prometheus&amp;quot;}[5m]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;时间点对象MatrixSelector 对象，是获取时序数据的基础结构&lt;/li&gt;
&lt;li&gt;获取时间段里面的数据，通过iterator 是序列筛选结果的顺序访问接口，获取某个时间点往前的一段历史数据，这是一个二维矩阵 (matrix)，进而由外层函数将这段历史数据汇总成一个 vector&lt;/li&gt;
&lt;li&gt;实现对一段数据的汇总，然后求平均值&lt;/li&gt;
&lt;li&gt;最后来看关键字（keyword）sum 的实现，这里注意 sum 不是函数（Function）而是关键字。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;sum(avg_over_time(go_goroutines{job=&amp;ldquo;prometheus&amp;rdquo;}[5m])) by (instance) 计算过程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/promql.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;PromQL 有三个很简单的原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;任意 PromQL 返回的结果都不是原始数据，即使查询一个具体的 Metric（如 go_goroutines），结果也不是原始数据&lt;/li&gt;
&lt;li&gt;任意 Metrics 经过 Function 计算后会丢失&lt;code&gt;__name__&lt;/code&gt;Label&lt;/li&gt;
&lt;li&gt;子序列间具备完全相同的 Label/Value 键值对（可以有不同的&lt;code&gt;__name__&lt;/code&gt;）才能进行代数运算&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;storage&#34;&gt;storage&lt;/h2&gt;

&lt;h3 id=&#34;源码解读&#34;&gt;源码解读&lt;/h3&gt;

&lt;p&gt;真正存储指标的是storage.Appender，在scrape与storage之间有一层缓存。缓存主要的作用是过滤错误的指标。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type scrapeCache struct {
  iter uint64                           // scrape批次
    successfulCount int                   // 成功保存的元数据数
    series map[string]*cacheEntry         // 缓存解析的相关数据
    droppedSeries map[string]*uint64      // 缓存无效指标
    seriesCur  map[uint64]labels.Labels     // 本次采集指标
    seriesPrev map[uint64]labels.Labels   // 上次采集指标

    metaMtx  sync.Mutex
    metadata map[string]*metaEntry
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建scrapeCache，调用newScrapeLoop，初始化scrapeLoop，会判断scrapeCache是否为空，如果为nil，调用newScrapeCache对cache进行初始化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if cache == nil {
        cache = newScrapeCache()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;newScrapeCache()如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newScrapeCache() *scrapeCache {
    return &amp;amp;scrapeCache{
        series:        map[string]*cacheEntry{},
        droppedSeries: map[string]*uint64{},
        seriesCur:     map[uint64]labels.Labels{},
        seriesPrev:    map[uint64]labels.Labels{},
        metadata:      map[string]*metaEntry{},
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;scrapeCache 方法介绍，这里简介各个fun的作用，详细代码不做注解。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 根据met信息，获取对应的cacheEntry
func (c *scrapeCache) get(met string) (*cacheEntry, bool)
// 根据met创建cacheEntry节点
func (c *scrapeCache) addRef(met string, ref uint64, lset labels.Labels, hash uint64)
// 添加无效指标，met作为key
func (c *scrapeCache) addDropped(met string)
// 根据met，检查该指标是否有效
func (c *scrapeCache) getDropped(met string) bool
// 添加当前采集指标
func (c *scrapeCache) trackStaleness(hash uint64, lset labels.Labels)
// 检查指标状态
func (c *scrapeCache) forEachStale(f func(labels.Labels) bool)
// 缓存清理
func (c *scrapeCache) iterDone(flushCache bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;存储过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;分析scrapeLoop.append是如何实现存储数据的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sl *scrapeLoop)  append(b []byte, contentType string, ts time.Time) (total, added, seriesAdded int, err error) {
    var (
    // 获取指标存储组件
        app            = sl.appender()
    // 获取解析组件
        p              = textparse.New(b, contentType)
        defTime        = timestamp.FromTime(ts)
        numOutOfOrder  = 0
        numDuplicates  = 0
        numOutOfBounds = 0
    )
    var sampleLimitErr error

loop:
    for {
        var et textparse.Entry
    // 开始遍历，遍历到EOF(字节流尾部)，终止遍历
        if et, err = p.Next(); err != nil {
            if err == io.EOF {
                err = nil
            }
            break
        }
    // 以下Entry类型跳过
        switch et {
        case textparse.EntryType:
            sl.cache.setType(p.Type())
            continue
        case textparse.EntryHelp:
            sl.cache.setHelp(p.Help())
            continue
        case textparse.EntryUnit:
            sl.cache.setUnit(p.Unit())
            continue
        case textparse.EntryComment:
            continue
        default:
        }
        total++

        t := defTime
    // 获取指标label，时间戳（如果设置了），当前样本值
        met, tp, v := p.Series()
    // 如果设置了honorTimestamps，时间戳设置为nil
        if !sl.honorTimestamps {
            tp = nil
        }
    // 如果时间戳不为空，更新当前t
        if tp != nil {
            t = *tp
        }
        // 检查该指标值是否有效，无效则直接跳过当前处理
        if sl.cache.getDropped(yoloString(met)) {
            continue
        }
    // 根据当前met获取对应的cacheEntry结构
        ce, ok := sl.cache.get(yoloString(met))
    // 如果从缓存中获取，则执行指标的存储操作
        if ok {
      // 指标存储
            switch err = app.AddFast(ce.lset, ce.ref, t, v); err {
            case nil:
        // 如果不带时间戳
                if tp == nil {
          // 存储该不带时间戳的指标到seriesCur中。
                    sl.cache.trackStaleness(ce.hash, ce.lset)
                }
       // 未找到错误，重置ok为false，执行!ok逻辑
            case storage.ErrNotFound:
                ok = false
      // 乱序样本
            case storage.ErrOutOfOrderSample:
        // 乱序样本错误记录，并上报
                numOutOfOrder++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of order sample&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfOrder.Inc()
                continue
      // 重复样本
            case storage.ErrDuplicateSampleForTimestamp:
        // 重复样本错误记录，并上报
                numDuplicates++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Duplicate sample for timestamp&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleDuplicate.Inc()
                continue
      // 存储越界
            case storage.ErrOutOfBounds:
        // 存储越界错误记录，并上报
                numOutOfBounds++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of bounds metric&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfBounds.Inc()
                continue
      // 超出样本限制错误
            case errSampleLimit:
        // 如果我们达到上限也要继续解析输出，所以我们要上报正确的样本总量
                sampleLimitErr = err
                added++
                continue
      // 未知情况，终止loop
            default:
                break loop
            }
        }
    // 在缓存中未查找到，
        if !ok {
            var lset labels.Labels
            // 生成mets
            mets := p.Metric(&amp;amp;lset)
      // 生成hash值
            hash := lset.Hash()

            // 根据配置重置label set
            lset = sl.sampleMutator(lset)

            // 如果label set为空，则表明该mets为非法指标
            if lset == nil {
        // 添加mets到无效指标字典中
                sl.cache.addDropped(mets)
                continue
            }

            var ref uint64
      // 存储指标
            ref, err = app.Add(lset, t, v)

      // 错误处理同上，不重复描述
            switch err {
            case nil:
      // 乱序样本
            case storage.ErrOutOfOrderSample:
                err = nil
                numOutOfOrder++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of order sample&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfOrder.Inc()
                continue
      // 重复样本
            case storage.ErrDuplicateSampleForTimestamp:
                err = nil
                numDuplicates++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Duplicate sample for timestamp&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleDuplicate.Inc()
                continue
      // 存储越界
            case storage.ErrOutOfBounds:
                err = nil
                numOutOfBounds++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of bounds metric&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfBounds.Inc()
                continue
      // 样本限制
            case errSampleLimit:
                sampleLimitErr = err
                added++
                continue
            default:
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;unexpected error&amp;quot;, &amp;quot;series&amp;quot;, string(met), &amp;quot;err&amp;quot;, err)
                break loop
            }
            if tp == nil {
                // 存储该不带时间戳的指标到seriesCur中。
                sl.cache.trackStaleness(hash, lset)
            }
        // 缓存该指标到series中
            sl.cache.addRef(mets, ref, lset, hash)
            seriesAdded++
        }
        added++
    }
  // 错误相关处理，不做分析。
    if sampleLimitErr != nil {
        if err == nil {
            err = sampleLimitErr
        }
        targetScrapeSampleLimit.Inc()
    }
    if numOutOfOrder &amp;gt; 0 {
        level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Error on ingesting out-of-order samples&amp;quot;, &amp;quot;num_dropped&amp;quot;, numOutOfOrder)
    }
    if numDuplicates &amp;gt; 0 {
        level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Error on ingesting samples with different value but same timestamp&amp;quot;, &amp;quot;num_dropped&amp;quot;, numDuplicates)
    }
    if numOutOfBounds &amp;gt; 0 {
        level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Error on ingesting samples that are too old or are too far into the future&amp;quot;, &amp;quot;num_dropped&amp;quot;, numOutOfBounds)
    }
    if err == nil {
    // 指标状态检查。
        sl.cache.forEachStale(func(lset labels.Labels) bool {
            // 标记存储中的过期指标
            _, err = app.Add(lset, defTime, math.Float64frombits(value.StaleNaN))
            switch err {
      // 以下错误不做处理
            case storage.ErrOutOfOrderSample, storage.ErrDuplicateSampleForTimestamp:
                err = nil
            }
            return err == nil
        })
    }
    if err != nil {
    // 出现错误，存储组件进行回滚
        app.Rollback()
        return total, added, seriesAdded, err
    }
  // 存储提交
    if err := app.Commit(); err != nil {
        return total, added, seriesAdded, err
    }

    // 执行缓存清理相关工作
    sl.cache.iterDone(len(b) &amp;gt; 0)

    return total, added, seriesAdded, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;整个存储逻辑都围绕着过滤无效指标进行。特殊点在于存储的时候指标分为有时间戳与无时间戳两种情况。&lt;/p&gt;

&lt;p&gt;1、有时间戳&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;解析指标数据通过Series()&lt;/li&gt;
&lt;li&gt;利用getDropped判断指标是否有效，无效则跳出处理&lt;/li&gt;
&lt;li&gt;通过get查找对应cacheEntry，如果找到利用app.AddFast直接存储样本值。如果未找到，使用sampleMutator进行解析重置，判断lset是否为空，为空则使用addDropped添加到无效字典中，跳出当前处理，如果有效则使用app.Add存储指标。(可以看到，通过get找到使用AddFast存储，未找到使用Add存储，感兴趣可以看下两个fun实现的区别)&lt;/li&gt;
&lt;li&gt;通过forEachStale检查指标是否过期。&lt;/li&gt;
&lt;li&gt;app.Add标记过期指标&lt;/li&gt;
&lt;li&gt;调用iterDone进行相关缓存清理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、无时间戳&lt;/p&gt;

&lt;p&gt;每次存储后，如果不带时间戳都会调用trackStaleness，存储指标到seriesCur中&lt;/p&gt;

&lt;p&gt;这里seriesCur与seriesPrev的作用就是处理指标label是否过期的。forEachStale实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *scrapeCache) forEachStale(f func(labels.Labels) bool) {
    for h, lset := range c.seriesPrev {
        if _, ok := c.seriesCur[h]; !ok {
            if !f(lset) {
                break
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果seriesPrev中的指标(label)存在于seriesPrev，则不处理，如果不存在，则说明过期。其中在iterDone中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 交换seriesPrev与seriesCur
c.seriesPrev, c.seriesCur = c.seriesCur, c.seriesPrev

// 清空当前指标缓存列表
for k := range c.seriesCur {
    delete(c.seriesCur, k)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以，每次存储处理后，都会交换seriesPrev与seriesCur，然后清空seriesCur。下次存储在做比较。如果命中过期规则，则标记该样本值为StaleNaN。&lt;/p&gt;

&lt;h3 id=&#34;local-storage&#34;&gt;local storage&lt;/h3&gt;

&lt;h4 id=&#34;v2&#34;&gt;v2&lt;/h4&gt;

&lt;p&gt;Prometheus 1.0版本的TSDB（V2存储引擎）使用了G家的LevelDB来做索引(PromSQL重度依赖LevelDB)，并且使用了和Facebook Gorilla一样的压缩算法，能够将16个字节的数据点压缩到平均1.37个字节。&lt;/p&gt;

&lt;p&gt;V2存储引擎对于大量的采样数据有自己的存储层，Prometheus为每个时序数据创建一个本地文件，以1024byte大小的chunk来组织。写到head chunk，写满1KB，就再生成新的块，完成的块，是不可再变更的 , 根据配置文件的设置，有一部份chunk会被保留在内存里，按照LRU算法，定期将块写进磁盘文件内。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;缺陷&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;文件数会随着时间线的数量同比增长，慢慢会耗尽inode。&lt;/li&gt;
&lt;li&gt;即便使用了Chunk写优化，若一次写入涉及的时间线过多，IOPS要求还是会很高。&lt;/li&gt;
&lt;li&gt;每个文件不可能会时刻保持open状态，一次查询可能需要重新打开大量文件，增大查询延迟。&lt;/li&gt;
&lt;li&gt;数据回收需要从大量文件扫描和重写数据，耗时较长。&lt;/li&gt;
&lt;li&gt;数据需要在内存中积累一定时间以Chunk写，V2会采用定时写Checkpoint的机制来尽量保证内存中数据不丢失。但通常记录Checkpoint的时间大于能承受的数据丢失的时间窗口，并且在节点恢复时从checkpoint restore数据的时间也会很长。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;优化策略&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Chunk写，热数据内存缓存&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prometheus一次性接收到的数据是一条竖线，包含很多的数据点，但是这些数据点属于不同的时间线。而当前的设计是一条时间线对应一个独立的文件，所以每次写入都会需要向很多不同的文件写入极少量的数据。针对这个问题，V2存储引擎的优化策略是Chunk写，针对单个时间线的写入必须是批量写，那就需要数据在时间线维度累积一定时间后才能凑到一定量的数据点。Chunk写策略带来的好处除了批量写外，还能优化热数据查询效率以及数据压缩率。V2存储引擎使用了和Facebook Gorilla一样的压缩算法，能够将16个字节的数据点压缩到平均1.37个字节，节省12倍内存和空间。Chunk写就要求数据一定要在服务器内存里积累一定的时间，即热数据基本都在内存中，查询效率很高。&lt;/p&gt;

&lt;h4 id=&#34;v3&#34;&gt;v3&lt;/h4&gt;

&lt;p&gt;Prometheus 2.0版本引入了全新的V3存储引擎，提供了更高的写入和查询性能。&lt;/p&gt;

&lt;p&gt;V3引擎完全重新设计，但是也延续了v2的一些优化策略，也来解决V2引擎中存在的这些问题。V3引擎可以看做是一个简单版、针对时序数据场景优化过后的LSM，可以带着LSM的设计思想来理解，先看一下V3引擎中数据的文件目录结构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/data.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;data目录下存放所有的数据，data目录的下一级目录是以&amp;rsquo;b-&amp;lsquo;为前缀，顺序自增的ID为后缀的目录，代表Block。每个Block下有chunks、index和meta.json，chunks目录下存放chunk的数据。这个chunk和V2的chunk是一个概念，唯一的不同是一个chunk内会包含很多的时间线，而不再只是一条。index是这个block下对chunk的索引，可以支持根据某个label快速定位到时间线以及数据所在的chunk。meta.json是一个简单的关于block数据和状态的一个描述文件。要理解V3引擎的设计思想，只需要搞明白几个问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk文件的存储格式？&lt;/li&gt;
&lt;li&gt;index的存储格式，如何实现快速查找？&lt;/li&gt;
&lt;li&gt;为何最后一个block没有chunk目录但有一个wal目录？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/block.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Prometheus将数据按时间维度切分为多个block，每个block被认为是独立的一个数据库，覆盖不同的时间范围的数据，完全没有交叉。每个Block下chunk内的数据dump到文件后即不可再修改，只有最近的一个block允许接收新数据。最新的block内数据写入会先写到一个内存的结构，为了保证数据不丢失，会先写一份WAL（write ahead log）。&lt;/p&gt;

&lt;p&gt;V3完全借鉴了LSM的设计思想，针对时序数据特征做了一些优化，带来很多好处：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当查询一个时间范围的数据时，可快速排除无关的block。每个block有独立的index，能够有效解决V2内遇到的『无效时间线 Series Churn』的问题。&lt;/li&gt;
&lt;li&gt;内存数据dump到chunk file，可高效采用大块数据顺序写，对SSD和HDD都很友好。&lt;/li&gt;
&lt;li&gt;和V2一样，最近的数据在内存内，最近的数据也是最热的数据，在内存可支持最高效的查询。&lt;/li&gt;
&lt;li&gt;老数据的回收变得非常简单和高效，只需要删除少量目录。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/block1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;V3内block以两个小时的跨度来切割，这个时间跨度不能太大，也不能太小。太大的话若内存中要保留两个小时数据，则内存占用会比较大。太小的话会导致太多的block，查询时需要对更多的文件做查询。所以两个小时是一个综合考虑后决定的值，但是当查询大跨度时间范围时，仍不可避免需要跨多个文件，例如查询一周时间跨度需要84个文件。V3也是采用了LSM一样的compaction策略来做查询优化，把小的block合并为大的block，compaction期间也可做其他一些事，例如删除过期数据或重构chunk数据以支持更高效的查询。InfluxDB也有多种不同的compaction策略，在不同的时刻使用。&lt;/p&gt;

&lt;p&gt;prometheus重2.0版本开始使用了V3引擎，V3没有和V2一样采用LevelDB，在已经持久化的Block，Index已经固定下来，不可修改。而对于最新的还在写数据的block，V3则会把所有的索引全部hold在内存，维护一个内存结构，等到这个block被关闭，再持久化到文件。这样做会比较简单一点，内存里维护时间线到ID的映射以及label到ID列表的映射，查询效率会很高。而且Prometheus对Label的基数会有一个假设：『a real-world dataset of ~4.4 million series with about 12 labels each has less than 5,000 unique labels』，这个全部保存在内存也是一个很小的量级，完全没有问题。InfluxDB采用的是类似的策略，而其他一些TSDB则直接使用ElasticSearch作为索引引擎。&lt;/p&gt;

&lt;p&gt;针对时序数据这种写多读少的场景，类LSM的存储引擎还是有不少优势的。有些TSDB直接基于开源的LSM引擎分布式数据库例如Hbase或Cassandra，也有自己基于LevelDB/RocksDB研发，或者再像InfluxDB和Prometheus一样纯自研，因为时序数据这一特定场景还是可以做更多的优化，例如索引、compaction策略等。Prometheus V3引擎的设计思想和InfluxDB真的很像，优化思路高度一致，后续在有新的需求的出现后，会有更多变化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prometheus将Timeseries数据按2小时一个block进行存储。每个block由一个目录组成，该目录里包含：一个或者多个chunk文件（保存timeseries数据）、一个metadata文件、一个index文件（通过metric name和labels查找timeseries数据在chunk文件的位置）。最新写入的数据保存在内存block中，达到2小时后写入磁盘。为了防止程序崩溃导致数据丢失，实现了WAL（write-ahead-log）机制，将timeseries原始数据追加写入log中进行持久化。删除timeseries时，删除条目会记录在独立的tombstone文件中，而不是立即从chunk文件删除。启动时会以写入日志(WAL)的方式来实现重播，从而恢复数据。&lt;/p&gt;

&lt;p&gt;这些2小时的block会在后台压缩成更大的block，数据压缩合并成更高level的block文件后删除低level的block文件。这个和leveldb、rocksdb等LSM树的思路一致。&lt;/p&gt;

&lt;p&gt;这些设计和Gorilla的设计高度相似，所以Prometheus几乎就是等于一个缓存TSDB。它本地存储的特点决定了它不能用于long-term数据存储，只能用于短期窗口的timeseries数据保存和查询，并且不具有高可用性（宕机会导致历史数据无法读取）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;具体形式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、磁盘文件结构&lt;/p&gt;

&lt;p&gt;内存中的block&lt;/p&gt;

&lt;p&gt;内存中的block数据未刷盘时，block目录下面主要保存wal文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./data/01BKGV7JBM69T2G1BGBGM6KB12
./data/01BKGV7JBM69T2G1BGBGM6KB12/meta.json
./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000002
./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;持久化的block&lt;/p&gt;

&lt;p&gt;持久化的block目录下wal文件被删除，timeseries数据保存在chunk文件里。index用于索引timeseries在wal文件里的位置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./data/01BKGV7JC0RY8A6MACW02A2PJD
./data/01BKGV7JC0RY8A6MACW02A2PJD/meta.json
./data/01BKGV7JC0RY8A6MACW02A2PJD/index
./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks
./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks/000001
./data/01BKGV7JC0RY8A6MACW02A2PJD/tombstones
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、mmap&lt;/p&gt;

&lt;p&gt;使用mmap读取压缩合并后的大文件（不占用太多句柄），建立进程虚拟地址和文件偏移的映射关系，只有在查询读取对应的位置时才将数据真正读到物理内存。绕过文件系统page cache，减少了一次数据拷贝。查询结束后，对应内存由Linux系统根据内存压力情况自动进行回收，在回收之前可用于下一次查询命中。因此使用mmap自动管理查询所需的的内存缓存，具有管理简单，处理高效的优势。
从这里也可以看出，它并不是完全基于内存的TSDB，和Gorilla的区别在于查询历史数据需要读取磁盘文件。&lt;/p&gt;

&lt;p&gt;3、Compaction&lt;/p&gt;

&lt;p&gt;Compaction主要操作包括合并block、删除过期数据、重构chunk数据。其中合并多个block成为更大的block，可以有效减少block个数，当查询覆盖的时间范围较长时，避免需要合并很多block的查询结果。
为提高删除效率，删除时序数据时，会记录删除的位置，只有block所有数据都需要删除时，才将block整个目录删除。因此block合并的大小也需要进行限制，避免保留了过多已删除空间（额外的空间占用）。比较好的方法是根据数据保留时长，按百分比（如10%）计算block的最大时长。&lt;/p&gt;

&lt;p&gt;4、Inverted Index&lt;/p&gt;

&lt;p&gt;Inverted Index（倒排索引）基于其内容的子集提供数据项的快速查找。简而言之，我可以查看所有标签为app=“nginx”的数据，而不必遍历每一个timeseries，并检查是否包含该标签。
为此，每个时间序列key被分配一个唯一的ID，通过它可以在恒定的时间内检索，在这种情况下，ID就是正向索引。
举个栗子：如ID为9,10,29的series包含label app=&amp;ldquo;nginx&amp;rdquo;，则lable &amp;ldquo;nginx&amp;rdquo;的倒排索引为[9,10,29]用于快速查询包含该label的series。&lt;/p&gt;

&lt;p&gt;5、存储配置&lt;/p&gt;

&lt;p&gt;对于本地存储，prometheus提供了一些配置项，主要包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--storage.tsdb.path: 存储数据的目录，默认为data/，如果要挂外部存储，可以指定该目录
--storage.tsdb.retention.time: 数据过期清理时间，默认保存15天
--storage.tsdb.retention.size: 实验性质，声明数据块的最大值，不包括wal文件，如512MB
--storage.tsdb.retention: 已被废弃，改为使用storage.tsdb.retention.time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus将所有当前使用的块保留在内存中。此外，它将最新使用的块保留在内存中，最大内存可以通过storage.local.memory-chunks标志配置。&lt;/p&gt;

&lt;p&gt;监测当前使用的内存量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_chunks
process_resident_memory_bytes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监测当前使用的存储指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_series: 时间序列持有的内存当前块数量
prometheus_local_storage_memory_chunks: 在内存中持久块的当前数量


prometheus_local_storage_chunks_to_persist: 当前仍然需要持久化到磁盘的的内存块数量
prometheus_local_storage_persistence_urgency_score: 紧急程度分数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、性能&lt;/p&gt;

&lt;p&gt;在文章Writing a Time Series Database from Scratch里，作者给出了benchmark测试结果为Macbook Pro上写入达到2000万每秒。这个数据比Gorilla论文中的目标7亿次写入每分钟（1000千多万每秒）提供了更高的单机性能。&lt;/p&gt;

&lt;h3 id=&#34;remote-storage&#34;&gt;remote storage&lt;/h3&gt;

&lt;p&gt;Prometheus 的设计者非常看重监控系统自身的稳定性，所以 Prometheus 仅仅依赖了本地文件系统，而这就决定了 Prometheus 自身并不适合存储长期数据。本地存储的优势就是运维简单,缺点就是无法海量的metrics持久化和数据存在丢失的风险，我们在实际使用过程中，出现过几次wal文件损坏，无法再写入的问题。&lt;/p&gt;

&lt;p&gt;所以 Prometheus 提供了 remote read 和 remote write 的接口，让用户自己去实现对接，prometheus以两种方式与远程存储系统集成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prometheus可以以标准格式将其提取的样本写入远程URL。&lt;/li&gt;
&lt;li&gt;Prometheus可以以标准格式从远程URL读取（返回）样本数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/20190315.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Adapter 是一个中间组件，Prometheus 与 Adapter 之间通过由 Prometheus 定义的标准格式发送和接收数据，Adapter 与外部存储系统之间的通信可以自定义，目前 Prometheus 和 Adapter 之间通过 grpc 通信。Prometheus 将 samples 发送到 Adapter。为了提高效率，samples 会在队列中先缓存，再打包发送给 Adapter，所以一个读请求中包含了 start_timestamp，end_timestamp 和 label_matchers，response 则包含所有 match 到的 time series，也就是说，Prometheus 仅通过 Adapter 来获取时间序列，进一步的处理都在 Prometheus 中完成。&lt;/p&gt;

&lt;p&gt;remote read 和 remote write 的配置还没有稳定，我们从代码中来一探究竟，HTTPClientConfig 可以用来配置 HTTP 相关的 auth 信息，proxy 方式，以及 tls。WriteRelabelConfigs 用在发送过程中对 timeseries 进行 relabel。QueueConfig 定义了发送队列的 batch size，queue 数量，发送失败时的重试次数与等待时间等参数。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;write&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prometheus 默认定义了 1000 个 queue，batch size 为 100，预期可以达到 1M samples/s 的发送速率。Prometheus 输出了一些 queue 相关的指标，例如 failed_samples_total, dropped_samples_total，如果这两个指标的 rate 大于 0，就需要说明 Remote Storage 出现了问题导致发送失败，或者队列满了导致 samples 被丢弃掉。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReadRecent 如果为 false，Prometheus 会在处理查询时比较本地存储中最早的数据的 timestamp 与 query 的 start timestamp，如果发现需要的数据都在本地存储中，则会跳过对 Remote Storage 的查询。&lt;/p&gt;

&lt;p&gt;社区中支持prometheus远程读写的方案&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AppOptics: write
Chronix: write
Cortex: read and write
CrateDB: read and write
Elasticsearch: write
Gnocchi: write
Graphite: write
InfluxDB: read and write
OpenTSDB: write
PostgreSQL/TimescaleDB: read and write
SignalFx: write
clickhouse: read and write
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前远程存储使用最多的是influxdb（收费），opentsdb（依赖hbase），m3db（不稳定）,VM(很优秀的存储查询性能)。&lt;/p&gt;

&lt;h2 id=&#34;服务发现&#34;&gt;服务发现&lt;/h2&gt;

&lt;p&gt;Prometheus目前支持以下平台的动态发现能力：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;容器编排系统：kubernetes&lt;/li&gt;
&lt;li&gt;云平台：EC2、Azure、OpenStack&lt;/li&gt;
&lt;li&gt;服务发现：DNS、Zookeeper、Consul 等。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;加载配置
​
ServiceDiscoveryConfig配置结构如下：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type ServiceDiscoveryConfig struct {
    // 静态服务发现配置
    StaticConfigs []*targetgroup.Group `yaml:&amp;quot;static_configs,omitempty&amp;quot;`
    // DNS服务发现配置
    DNSSDConfigs []*dns.SDConfig `yaml:&amp;quot;dns_sd_configs,omitempty&amp;quot;`
    // 配置文件服务发现配置
    FileSDConfigs []*file.SDConfig `yaml:&amp;quot;file_sd_configs,omitempty&amp;quot;`
    // Consul服务发现配置
    ConsulSDConfigs []*consul.SDConfig `yaml:&amp;quot;consul_sd_configs,omitempty&amp;quot;`
    // zookeeper Serverset 服务发现配置
    ServersetSDConfigs []*zookeeper.ServersetSDConfig `yaml:&amp;quot;serverset_sd_configs,omitempty&amp;quot;`
    // zookeeper Nerve 服务发现配置
    NerveSDConfigs []*zookeeper.NerveSDConfig `yaml:&amp;quot;nerve_sd_configs,omitempty&amp;quot;`
    // 根据Marathon API 服务发现配置
    MarathonSDConfigs []*marathon.SDConfig `yaml:&amp;quot;marathon_sd_configs,omitempty&amp;quot;`
    // 根据Kubernetes API 服务发现配置
    KubernetesSDConfigs []*kubernetes.SDConfig `yaml:&amp;quot;kubernetes_sd_configs,omitempty&amp;quot;`
    // GCE 服务发现配置
    GCESDConfigs []*gce.SDConfig `yaml:&amp;quot;gce_sd_configs,omitempty&amp;quot;`
    // EC2服务发现配置
    EC2SDConfigs []*ec2.SDConfig `yaml:&amp;quot;ec2_sd_configs,omitempty&amp;quot;`
    // Openstack 服务发现配置
    OpenstackSDConfigs []*openstack.SDConfig `yaml:&amp;quot;openstack_sd_configs,omitempty&amp;quot;`
    // Azure 服务发现配置
    AzureSDConfigs []*azure.SDConfig `yaml:&amp;quot;azure_sd_configs,omitempty&amp;quot;`
    // Triton 服务发现配置
    TritonSDConfigs []*triton.SDConfig `yaml:&amp;quot;triton_sd_configs,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​
在Prometheus初始化过程中，通过执行discoveryManagerScrape.ApplyConfig进行服务发现相关配置的加载。&lt;/p&gt;

&lt;p&gt;移除目前正在运行的providers，根据新的provided 配置，启动新的providers。。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg map[string]sd_config.ServiceDiscoveryConfig) error {
    // 加锁
    m.mtx.Lock()
  // 函数结束后 解锁
    defer m.mtx.Unlock()
  // 遍历已存在target
    for pk := range m.targets {
        if _, ok := cfg[pk.setName]; !ok {
      // 删除标签
            discoveredTargets.DeleteLabelValues(m.name, pk.setName)
        }
    }
  // 取消所有Discoverer
    m.cancelDiscoverers()
    for name, scfg := range cfg {
    // 根据scfg，注册服务发现实例
        m.registerProviders(scfg, name)
    // 设置标签
        discoveredTargets.WithLabelValues(m.name, name).Set(0)
    }
    for _, prov := range m.providers {
    // 启动服务发现实例
        m.startProvider(m.ctx, prov)
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;注册Providers&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中m.registerProviders的主要作用就是根据cfg（配置）注册所有provider实例，保存在m.providers&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) registerProviders(cfg sd_config.ServiceDiscoveryConfig, setName string) {
    // 标签
    var added bool
  // 加载Providers的add方法
    add := func(cfg interface{}, newDiscoverer func() (Discoverer, error)) {
    // 读取cfg类型
        t := reflect.TypeOf(cfg).String()
        for _, p := range m.providers {
      // 检查该cfg是否加载过
            if reflect.DeepEqual(cfg, p.config) {
        // 如果加载过，记录该job
                p.subs = append(p.subs, setName)
        // 变更标签状态
                added = true
        // 跳出
                return
            }
        }
        // 创建一个Discoverer实例
        d, err := newDiscoverer()
        if err != nil {
            level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Cannot create service discovery&amp;quot;, &amp;quot;err&amp;quot;, err, &amp;quot;type&amp;quot;, t)
            failedConfigs.WithLabelValues(m.name).Inc()
            return
        }
        // 创建一个provider
        provider := provider{
      // 生成provider名称
            name:   fmt.Sprintf(&amp;quot;%s/%d&amp;quot;, t, len(m.providers)),
      // 关联对应的Discoverer实例（比如DNS、zk等）
            d:      d,
      // 关联配置
            config: cfg,
      // 关联job
            subs:   []string{setName},
        }
    // 添加该provider到m.providers队列中
        m.providers = append(m.providers, &amp;amp;provider)
        // 更新标签
        added = true
    }
    // 遍历DNS配置，生成该Discoverer
    for _, c := range cfg.DNSSDConfigs {
        add(c, func() (Discoverer, error) {
            return dns.NewDiscovery(*c, log.With(m.logger, &amp;quot;discovery&amp;quot;, &amp;quot;dns&amp;quot;)), nil
        })
    }
  .
  .
  .
  .
  .
  .
  // 类似配置遍历省略，感兴趣可以阅读源码查看
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;启动Provider&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在ApplyConfig，执行m.startProvider(m.ctx, prov)启动provider。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) startProvider(ctx context.Context, p *provider) {
    level.Info(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Starting provider&amp;quot;, &amp;quot;provider&amp;quot;, p.name, &amp;quot;subs&amp;quot;, fmt.Sprintf(&amp;quot;%v&amp;quot;, p.subs))
    ctx, cancel := context.WithCancel(ctx)
  // 记录发现的服务
    updates := make(chan []*targetgroup.Group)
    // 添加取消方法
    m.discoverCancel = append(m.discoverCancel, cancel)
    // 执行run  每个服务发现都有自己的run方法。
    go p.d.Run(ctx, updates)
  // 更新发现的服务
    go m.updater(ctx, p, updates)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里分析DNS 服务发现对应的Run方法。需要标注下，DNS对应的Discovery其实是refresh中的Discovery的Run实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d.Discovery = refresh.NewDiscovery(
        logger,
        &amp;quot;dns&amp;quot;,
        time.Duration(conf.RefreshInterval),
        d.refresh,
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (d *Discovery) Run(ctx context.Context, ch chan&amp;lt;- []*targetgroup.Group) {
  // 首次进入，执行更新
    tgs, err := d.refresh(ctx)
    if err != nil {
        level.Error(d.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Unable to refresh target groups&amp;quot;, &amp;quot;err&amp;quot;, err.Error())
    } else {
        select {
        case ch &amp;lt;- tgs:
        case &amp;lt;-ctx.Done():
            return
        }
    }
    // 创建定时器
    ticker := time.NewTicker(d.interval)
    defer ticker.Stop()

    for {
        select {
        case &amp;lt;-ticker.C:
      // 定时执行更新，如果发现变化，通过ch发出更新信息
            tgs, err := d.refresh(ctx)
            if err != nil {
                level.Error(d.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Unable to refresh target groups&amp;quot;, &amp;quot;err&amp;quot;, err.Error())
                continue
            }

            select {
      // 发送 变化的targets
            case ch &amp;lt;- tgs:
            case &amp;lt;-ctx.Done():
                return
            }
        case &amp;lt;-ctx.Done():
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;更新服务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当服务发现变化的targets时，通过updates chan进行更新。最终更新Discovery Manager的targets。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) updater(ctx context.Context, p *provider, updates chan []*targetgroup.Group) {
   for {

      select {
      case &amp;lt;-ctx.Done():
         return
      // 接收updates数据
      case tgs, ok := &amp;lt;-updates:
         receivedUpdates.WithLabelValues(m.name).Inc()
         if !ok {
            level.Debug(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;discoverer channel closed&amp;quot;, &amp;quot;provider&amp;quot;, p.name)
            return
         }
                 // 更新targets
         for _, s := range p.subs {
            m.updateGroup(poolKey{setName: s, provider: p.name}, tgs)
         }

         select {
         // 发送更新通知
         case m.triggerSend &amp;lt;- struct{}{}:
         default:
         }
      }
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;启动discovery manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加载完配置，并且完成注册、启动、更新操作后，开始执行discoveryManagerScrape.Run方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Run() error {
  // 后台处理
    go m.sender()
    for range m.ctx.Done() {
        m.cancelDiscoverers()
        return m.ctx.Err()
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定时执行，当接收到服务发现的更新通知，通过m.allGroups()同步服务快照信息到scrapeManager&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) sender() {
  // 创建定时器
    ticker := time.NewTicker(m.updatert)
    defer ticker.Stop()

    for {
        select {
        case &amp;lt;-m.ctx.Done():
            return
        case &amp;lt;-ticker.C:
            select {
      // 检测到更新
            case &amp;lt;-m.triggerSend:
                sentUpdates.WithLabelValues(m.name).Inc()
                select {
        // 通过allGroups同步服务快照信息到scrapeManager
                case m.syncCh &amp;lt;- m.allGroups():
                default:
                    delayedUpdates.WithLabelValues(m.name).Inc()
                    level.Debug(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;discovery receiver&#39;s channel was full so will retry the next cycle&amp;quot;)
                    select {
                    case m.triggerSend &amp;lt;- struct{}{}:
                    default:
                    }
                }
            default:
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;关联ScrapeManager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关联&lt;/p&gt;

&lt;p&gt;在ScrapeManager在启动的时候会关联discoveryManagerScrape.SyncCh()。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func() error {
                &amp;lt;-reloadReady.C
                // 关联 discoveryManagerScrape 的 syncCh
                err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
                level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape manager stopped&amp;quot;)
                return err
            },
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新&lt;/p&gt;

&lt;p&gt;更新ScrapeManager的targets&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Run(tsets &amp;lt;-chan map[string][]*targetgroup.Group) error {
    go m.reloader()
    for {
        select {
    // 收到更新targets
        case ts := &amp;lt;-tsets:
            m.updateTsets(ts)

            select {
            case m.triggerReload &amp;lt;- struct{}{}:
            default:
            }

        case &amp;lt;-m.graceShut:
            return nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行更新&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) updateTsets(tsets map[string][]*targetgroup.Group) {
    m.mtxScrape.Lock()
  // 替换新的 tagets
    m.targetSets = tsets
    m.mtxScrape.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;discoveryManager在加载配置的时候，顺便完成provider的注册、启动、以及discovery的自更新通知操作。discoveryManager与ScrapeManager通过discoveryManager的syncCh通道来关联同步。&lt;/p&gt;

&lt;p&gt;整个服务发现的流程很值得学习，尤其是discoveryManager支持多种服务发现的扩展配置的相关设设计很值得学习。&lt;/p&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;Prometheus有两个比较著名的扩展版一个是cortex，另一个是thanos。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos&lt;/a&gt;在github的简介是『Highly available Prometheus setup with long term storage capabilities』，它基于Prometheus的最大改进是底层存储可扩展支持对象存储，例如AWS的S3，使得单机容量可扩展。这个得益于Prometheus 2.0中V3引擎的特性，持久化的Chunk文件是immutable的，所以能够很容易迁移到对象存储上。从它的设计文档里可以看出，它引入了一个Sidecar节点，与Prometheus server结对部署，主要作用将本地数据backup到远端的对象存储。当然数据被切割到本地和对象存储内后，为了支持统一的查询接口，又引入了Store层。Store层支持标准查询接口，屏蔽了底层是对象存储的细节，同时做了一些查询优化例如对Index的缓存。thanos中包含多个类型的节点，包括Prometheus Server、Sidecar、Store node、Rule node、compactor和query layer，其中只有query layer能水平扩展，因为其是无状态的。也就是说，单个实例的Prometheus其写入能力还是会有瓶颈，cotex相比它则在scalability上改进了更多。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/&#34;&gt;cortex&lt;/a&gt;在github的简介是『A multitenant, horizontally scalable Prometheus as a Service』，几个关键词：多租户、水平扩展及服务化。&lt;/p&gt;

&lt;p&gt;现在还可以使用远程存储聚合的方式来实现集群，比如做的比较好的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Mq Compare</title>
          <link>https://kingjcy.github.io/post/middleware/mq/mq-compare/</link>
          <pubDate>Sat, 21 Apr 2018 09:54:56 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/mq-compare/</guid>
          <description>&lt;p&gt;消息队列（MQ）是一种不同应用程序之间(跨进程)的通信方法，用于上下游应用程序之间传递消息。&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;消息队列（MQ）我们拆分来看：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;消息：应用程序通过写入和检索出入列队的数据（消息）来通信。&lt;/li&gt;
&lt;li&gt;队列：除去了接收和发送应用程序同时执行的要求。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样就实现了上游与下游之间的解耦，上游向MQ发送消息，下游从MQ接收消息，上游下游互不依赖，它们只依赖MQ。因为有队列的存在，MQ可在上下游之间进行缓冲，把上游信息先缓存起来，下游根据自己的能力从MQ中拉去信息，起到削峰的作用。&lt;/p&gt;

&lt;p&gt;所有的MQ的基本逻辑架构都是如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/mq&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们在设计和实现mq的时候主要要考虑以下的因素&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/mq2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;主要的使用场景&lt;/p&gt;

&lt;p&gt;可以使用MQ的场景有很多，最常用是业务解耦/最终一致性/广播/错峰削峰流控等。反之，如果需要强一致性，关注业务逻辑的处理结果，则RPC显得更为合适。主要是用来解决异步处理的耗时操作，否则就会增加系统的负载。&lt;/p&gt;

&lt;h1 id=&#34;对比&#34;&gt;对比&lt;/h1&gt;

&lt;p&gt;目前mq的相关产品可以说是百花齐放，有&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/activemq/&#34;&gt;activemq&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/kafka/&#34;&gt;kafka&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/nsq/&#34;&gt;nsq&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/rabbitmq/&#34;&gt;rabbitmq&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/rocketmq/&#34;&gt;rocketmq&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/emq/&#34;&gt;emq&lt;/a&gt;等，我们对其在支持的功能结合上面设计mq的因素来做一个简单的对比(图片来自网络)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/20180421.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/mq3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见目前比较性能好，使用比较多的就是kafka和阿里云的rocketmq。&lt;/p&gt;

&lt;p&gt;这边提一下redis，首先Redis的设计是用来做缓存的，但是由于它自身的某种特性使得他可以用来做消息队列(Redis的List数据结构比较适合做MQ)。它有几个阻塞式的API可以使用，正是这些阻塞式的API让他有做消息队列的能力。 另外做消息队列的其他特性，例如FIFO也很容易实现，只需要一个list对象从头取数据，从尾部塞数据即可实现。 Redis能做消息队列得益于它的list对象blpop brpop接口以及Pub/Sub(发布/订阅)的某些接口。他们都是阻塞版的，所以可以用来做消息队列。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算K8s系列---- K8s Rpm Build</title>
          <link>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-rpm-build/</link>
          <pubDate>Wed, 11 Apr 2018 20:20:00 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-rpm-build/</guid>
          <description>&lt;p&gt;记录rpm的打包过程&lt;/p&gt;

&lt;p&gt;1、执行docker-build.sh&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@opt8 rpm]# ./docker-build.sh
Sending build context to Docker daemon 2.076 GBGB
Step 1/8 : FROM fedora:24
 ---&amp;gt; 971e0f0a8b71
Step 2/8 : MAINTAINER Devan Goodwin &amp;lt;dgoodwin@redhat.com&amp;gt;
 ---&amp;gt; Running in 5a061ac48960
 ---&amp;gt; bdbb08c2f308
Removing intermediate container 5a061ac48960
Step 3/8 : RUN dnf install -y rpm-build rpmdevtools createrepo &amp;amp;&amp;amp; dnf clean all
 ---&amp;gt; Running in 0578e2b9ce55



^@^@Last metadata expiration check: 0:00:28 ago on Wed Mar 14 02:31:49 2018.
Dependencies resolved.
================================================================================
 Package                    Arch   Version                        Repository
                                                                           Size
================================================================================
Installing:
 binutils                   x86_64 2.26.1-1.fc24                  updates 5.9 M
 bzip2                      x86_64 1.0.6-21.fc24                  updates  57 k
 cpio                       x86_64 2.12-3.fc24                    fedora  261 k
 cpp                        x86_64 6.1.1-2.fc24                   fedora  9.0 M
 createrepo                 noarch 0.10.3-3.fc21                  fedora   94 k
 dnf-plugins-core           noarch 0.1.21-5.fc24                  updates  16 k
 dwz                        x86_64 0.12-2.fc24                    fedora  107 k
 elfutils                   x86_64 0.166-2.fc24                   fedora  299 k
 fakeroot                   x86_64 1.20.2-4.fc24                  fedora  101 k
 fakeroot-libs              x86_64 1.20.2-4.fc24                  fedora   35 k
 file                       x86_64 5.25-6.fc24                    fedora   68 k
 findutils                  x86_64 1:4.6.0-7.fc24                 updates 521 k
 fpc-srpm-macros            noarch 1.0-1.fc24                     fedora  7.8 k
 gc                         x86_64 7.4.2-6.fc24                   fedora  104 k
 gcc                        x86_64 6.1.1-2.fc24                   fedora   20 M
 gcc-gdb-plugin             x86_64 6.1.1-2.fc24                   fedora   78 k
 gdb                        x86_64 7.11.1-86.fc24                 updates 3.3 M
 ghc-srpm-macros            noarch 1.4.2-4.fc24                   fedora  8.7 k
 glibc-devel                x86_64 2.23.1-7.fc24                  fedora  934 k
 glibc-headers              x86_64 2.23.1-7.fc24                  fedora  500 k
 gnat-srpm-macros           noarch 4-1.fc24                       fedora  9.0 k
 go-srpm-macros             noarch 2-6.fc24                       fedora  8.5 k
 groff-base                 x86_64 1.22.3-8.fc24                  fedora  1.0 M
 guile                      x86_64 5:2.0.13-1.fc24                updates 3.5 M
 isl                        x86_64 0.14-5.fc24                    fedora  482 k
 kernel-headers             x86_64 4.11.12-100.fc24               updates 1.1 M
 libatomic_ops              x86_64 7.4.2-9.fc24                   fedora   32 k
 libbabeltrace              x86_64 1.2.4-4.fc24                   fedora  151 k
 libgomp                    x86_64 6.1.1-2.fc24                   fedora  183 k
 libipt                     x86_64 1.4.4-2.fc24                   fedora   38 k
 libmpc                     x86_64 1.0.2-5.fc24                   fedora   54 k
 libtool-ltdl               x86_64 2.4.6-13.fc24                  updates  54 k
 mpfr                       x86_64 3.1.5-1.fc24                   updates 215 k
 ocaml-srpm-macros          noarch 2-4.fc24                       fedora  8.0 k
 patch                      x86_64 2.7.5-3.fc24                   fedora  125 k
 perl                       x86_64 4:5.22.4-372.fc24              updates 6.0 M
 perl-Carp                  noarch 1.38-2.fc24                    fedora   28 k
 perl-Encode                x86_64 3:2.84-11.fc24                 updates 1.5 M
 perl-Errno                 x86_64 1.23-372.fc24                  updates  65 k
 perl-Exporter              noarch 5.72-349.fc24                  fedora   33 k
 perl-Fedora-VSP            noarch 0.001-2.fc24                   fedora   23 k
 perl-File-Path             noarch 2.12-3.fc24                    updates  33 k
 perl-File-Temp             noarch 0.23.04-347.fc24               fedora   61 k
 perl-Getopt-Long           noarch 2.49.1-1.fc24                  updates  61 k
 perl-HTTP-Tiny             noarch 0.056-5.fc24                   updates  53 k
 perl-IO                    x86_64 1.35-372.fc24                  updates 130 k
 perl-MIME-Base64           x86_64 3.15-349.fc24                  fedora   29 k
 perl-PathTools             x86_64 3.62-3.fc24                    updates  88 k
 perl-Pod-Escapes           noarch 1:1.07-349.fc24                fedora   20 k
 perl-Pod-Perldoc           noarch 3.26-1.fc24                    updates  86 k
 perl-Pod-Simple            noarch 1:3.35-1.fc24                  updates 211 k
 perl-Pod-Usage             noarch 4:1.69-1.fc24                  fedora   33 k
 perl-Scalar-List-Utils     x86_64 3:1.46-1.fc24                  updates  64 k
 perl-Socket                x86_64 3:2.024-1.fc24                 updates  56 k
 perl-Term-ANSIColor        noarch 4.03-347.fc24                  fedora   45 k
 perl-Term-Cap              noarch 1.17-2.fc24                    fedora   22 k
 perl-Text-ParseWords       noarch 3.30-347.fc24                  fedora   17 k
 perl-Text-Tabs+Wrap        noarch 2013.0523-347.fc24             fedora   23 k
 perl-Time-Local            noarch 1:1.250-1.fc24                 updates  30 k
 perl-Unicode-Normalize     x86_64 1.25-2.fc24                    fedora   80 k
 perl-constant              noarch 1.33-348.fc24                  fedora   24 k
 perl-generators            noarch 1.10-1.fc24                    updates  16 k
 perl-libs                  x86_64 4:5.22.4-372.fc24              updates 1.4 M
 perl-macros                x86_64 4:5.22.4-372.fc24              updates  61 k
 perl-parent                noarch 1:0.234-4.fc24                 fedora   18 k
 perl-podlators             noarch 4.09-1.fc24                    updates 114 k
 perl-srpm-macros           noarch 1-18.fc24                      fedora  9.8 k
 perl-threads               x86_64 1:2.02-3.fc24                  fedora   58 k
 perl-threads-shared        x86_64 1.48-347.fc24                  fedora   44 k
 pyliblzma                  x86_64 0.5.3-15.fc24                  fedora   53 k
 python                     x86_64 2.7.13-2.fc24                  updates  96 k
 python-deltarpm            x86_64 3.6-15.fc24                    fedora   37 k
 python-libs                x86_64 2.7.13-2.fc24                  updates 6.2 M
 python-libxml2             x86_64 2.9.3-3.fc24                   fedora  226 k
 python-pip                 noarch 8.0.2-1.fc24                   fedora  1.7 M
 python-pycurl              x86_64 7.43.0-2.fc24                  fedora  204 k
 python-six                 noarch 1.10.0-2.fc24                  fedora   34 k
 python-srpm-macros         noarch 3-7.fc24                       fedora  8.1 k
 python-urlgrabber          noarch 3.10.1-8.fc24                  fedora  112 k
 python2-iniparse           noarch 0.4-19.fc24                    fedora   45 k
 python2-pygpgme            x86_64 0.3-18.fc24                    updates  90 k
 python2-setuptools         noarch 20.1.1-1.fc24                  fedora  417 k
 python3-dnf-plugins-core   noarch 0.1.21-5.fc24                  updates  69 k
 pyxattr                    x86_64 0.5.3-7.fc24                   fedora   33 k
 redhat-rpm-config          noarch 42-2.fc24                      updates  59 k
 rpm-build                  x86_64 4.13.0.1-1.fc24                updates 138 k
 rpm-python                 x86_64 4.13.0.1-1.fc24                updates 101 k
 rpmdevtools                noarch 8.9-1.fc24                     updates 105 k
 tar                        x86_64 2:1.28-8.fc24                  updates 944 k
 unzip                      x86_64 6.0-31.fc24                    updates 184 k
 xemacs-filesystem          noarch 21.5.34-17.20160603hga561e02bb626.fc24
                                                                  updates  21 k
 xz                         x86_64 5.2.2-2.fc24                   fedora  148 k
 yum                        noarch 3.4.3-509.fc24                 fedora  1.2 M
 yum-metadata-parser        x86_64 1.1.4-16.fc24                  fedora   39 k
 zip                        x86_64 3.0-16.fc24                    fedora  272 k
Upgrading:
 rpm                        x86_64 4.13.0.1-1.fc24                updates 514 k
 rpm-build-libs             x86_64 4.13.0.1-1.fc24                updates 117 k
 rpm-libs                   x86_64 4.13.0.1-1.fc24                updates 300 k
 rpm-plugin-selinux         x86_64 4.13.0.1-1.fc24                updates  53 k
 rpm-plugin-systemd-inhibit x86_64 4.13.0.1-1.fc24                updates  53 k
 rpm-python3                x86_64 4.13.0.1-1.fc24                updates 101 k

Transaction Summary
================================================================================
Install  95 Packages
Upgrade   6 Packages

Total download size: 72 M
Downloading Packages:
^@^@^@^@/usr/share/locale/fi/LC_MESSAGES/rpm.mo: No such file or directory
cannot reconstruct rpm from disk files
rpm-4.13.0.1-1.fc24.x86_64: Delta RPM rebuild failed
--------------------------------------------------------------------------------
Total                                           322 kB/s |  72 MB     03:49
Delta RPMs reduced 72.2 MB of updates to 72.1 MB (0.1% saved)
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Installing  : perl-Exporter-5.72-349.fc24.noarch                        1/107
  Installing  : perl-libs-4:5.22.4-372.fc24.x86_64                        2/107
  Installing  : perl-Carp-1.38-2.fc24.noarch                              3/107
  Installing  : python-libs-2.7.13-2.fc24.x86_64                          4/107
  Installing  : python-pip-8.0.2-1.fc24.noarch                            5/107
  Installing  : python2-setuptools-20.1.1-1.fc24.noarch                   6/107
  Installing  : python-2.7.13-2.fc24.x86_64                               7/107
  Upgrading   : rpm-plugin-selinux-4.13.0.1-1.fc24.x86_64                 8/107
  Upgrading   : rpm-libs-4.13.0.1-1.fc24.x86_64                           9/107
  Upgrading   : rpm-4.13.0.1-1.fc24.x86_64                               10/107
  Installing  : perl-Scalar-List-Utils-3:1.46-1.fc24.x86_64              11/107
  Upgrading   : rpm-build-libs-4.13.0.1-1.fc24.x86_64                    12/107
  Installing  : mpfr-3.1.5-1.fc24.x86_64                                 13/107
  Installing  : libmpc-1.0.2-5.fc24.x86_64                               14/107
  Installing  : rpm-python-4.13.0.1-1.fc24.x86_64                        15/107
  Installing  : pyliblzma-0.5.3-15.fc24.x86_64                           16/107
  Installing  : yum-metadata-parser-1.1.4-16.fc24.x86_64                 17/107
  Installing  : perl-Text-ParseWords-3.30-347.fc24.noarch                18/107
  Installing  : unzip-6.0-31.fc24.x86_64                                 19/107
  Installing  : binutils-2.26.1-1.fc24.x86_64                            20/107
  Installing  : findutils-1:4.6.0-7.fc24.x86_64                          21/107
  Installing  : file-5.25-6.fc24.x86_64                                  22/107
  Installing  : cpio-2.12-3.fc24.x86_64                                  23/107
  Installing  : zip-3.0-16.fc24.x86_64                                   24/107
  Installing  : cpp-6.1.1-2.fc24.x86_64                                  25/107
  Upgrading   : rpm-python3-4.13.0.1-1.fc24.x86_64                       26/107
  Installing  : python-deltarpm-3.6-15.fc24.x86_64                       27/107
  Installing  : pyxattr-0.5.3-7.fc24.x86_64                              28/107
  Installing  : python-pycurl-7.43.0-2.fc24.x86_64                       29/107
  Installing  : python-urlgrabber-3.10.1-8.fc24.noarch                   30/107
  Installing  : python-six-1.10.0-2.fc24.noarch                          31/107
  Installing  : python2-iniparse-0.4-19.fc24.noarch                      32/107
  Installing  : python-libxml2-2.9.3-3.fc24.x86_64                       33/107
  Installing  : python2-pygpgme-0.3-18.fc24.x86_64                       34/107
  Installing  : yum-3.4.3-509.fc24.noarch                                35/107
  Installing  : perl-Term-ANSIColor-4.03-347.fc24.noarch                 36/107
  Installing  : perl-Fedora-VSP-0.001-2.fc24.noarch                      37/107
  Installing  : perl-macros-4:5.22.4-372.fc24.x86_64                     38/107
  Installing  : perl-Errno-1.23-372.fc24.x86_64                          39/107
  Installing  : perl-Socket-3:2.024-1.fc24.x86_64                        40/107
  Installing  : perl-constant-1.33-348.fc24.noarch                       41/107
  Installing  : perl-parent-1:0.234-4.fc24.noarch                        42/107
  Installing  : perl-Text-Tabs+Wrap-2013.0523-347.fc24.noarch            43/107
  Installing  : perl-Unicode-Normalize-1.25-2.fc24.x86_64                44/107
  Installing  : perl-threads-shared-1.48-347.fc24.x86_64                 45/107
  Installing  : perl-threads-1:2.02-3.fc24.x86_64                        46/107
  Installing  : perl-File-Path-2.12-3.fc24.noarch                        47/107
  Installing  : perl-PathTools-3.62-3.fc24.x86_64                        48/107
  Installing  : perl-IO-1.35-372.fc24.x86_64                             49/107
  Installing  : perl-4:5.22.4-372.fc24.x86_64                            50/107
  Installing  : perl-File-Temp-0.23.04-347.fc24.noarch                   51/107
  Installing  : perl-MIME-Base64-3.15-349.fc24.x86_64                    52/107
  Installing  : perl-generators-1.10-1.fc24.noarch                       53/107
  Installing  : perl-Term-Cap-1.17-2.fc24.noarch                         54/107
  Installing  : perl-Pod-Escapes-1:1.07-349.fc24.noarch                  55/107
  Installing  : perl-Time-Local-1:1.250-1.fc24.noarch                    56/107
  Installing  : perl-HTTP-Tiny-0.056-5.fc24.noarch                       57/107
  Installing  : kernel-headers-4.11.12-100.fc24.x86_64                   58/107
  Installing  : glibc-headers-2.23.1-7.fc24.x86_64                       59/107
  Installing  : glibc-devel-2.23.1-7.fc24.x86_64                         60/107
  Installing  : isl-0.14-5.fc24.x86_64                                   61/107
  Installing  : libgomp-6.1.1-2.fc24.x86_64                              62/107
  Installing  : gcc-6.1.1-2.fc24.x86_64                                  63/107
  Installing  : python3-dnf-plugins-core-0.1.21-5.fc24.noarch            64/107
  Installing  : libtool-ltdl-2.4.6-13.fc24.x86_64                        65/107
  Installing  : groff-base-1.22.3-8.fc24.x86_64                          66/107
  Installing  : perl-Encode-3:2.84-11.fc24.x86_64                        67/107
  Installing  : perl-Pod-Simple-1:3.35-1.fc24.noarch                     68/107
  Installing  : perl-Getopt-Long-2.49.1-1.fc24.noarch                    69/107
  Installing  : perl-podlators-4.09-1.fc24.noarch                        70/107
  Installing  : perl-Pod-Perldoc-3.26-1.fc24.noarch                      71/107
  Installing  : perl-Pod-Usage-4:1.69-1.fc24.noarch                      72/107
  Installing  : tar-2:1.28-8.fc24.x86_64                                 73/107
  Installing  : python-srpm-macros-3-7.fc24.noarch                       74/107
  Installing  : perl-srpm-macros-1-18.fc24.noarch                        75/107
  Installing  : ocaml-srpm-macros-2-4.fc24.noarch                        76/107
  Installing  : go-srpm-macros-2-6.fc24.noarch                           77/107
  Installing  : gnat-srpm-macros-4-1.fc24.noarch                         78/107
  Installing  : ghc-srpm-macros-1.4.2-4.fc24.noarch                      79/107
  Installing  : fpc-srpm-macros-1.0-1.fc24.noarch                        80/107
  Installing  : dwz-0.12-2.fc24.x86_64                                   81/107
  Installing  : redhat-rpm-config-42-2.fc24.noarch                       82/107
  Installing  : libatomic_ops-7.4.2-9.fc24.x86_64                        83/107
  Installing  : gc-7.4.2-6.fc24.x86_64                                   84/107
  Installing  : guile-5:2.0.13-1.fc24.x86_64                             85/107
  Installing  : libipt-1.4.4-2.fc24.x86_64                               86/107
  Installing  : libbabeltrace-1.2.4-4.fc24.x86_64                        87/107^@
  Installing  : gdb-7.11.1-86.fc24.x86_64                                88/107
  Installing  : bzip2-1.0.6-21.fc24.x86_64                               89/107
  Installing  : xemacs-filesystem-21.5.34-17.20160603hga561e02bb626.f    90/107
  Installing  : fakeroot-libs-1.20.2-4.fc24.x86_64                       91/107
  Installing  : fakeroot-1.20.2-4.fc24.x86_64                            92/107
  Installing  : elfutils-0.166-2.fc24.x86_64                             93/107
  Installing  : xz-5.2.2-2.fc24.x86_64                                   94/107
  Installing  : patch-2.7.5-3.fc24.x86_64                                95/107
  Installing  : rpm-build-4.13.0.1-1.fc24.x86_64                         96/107
  Installing  : rpmdevtools-8.9-1.fc24.noarch                            97/107
  Installing  : dnf-plugins-core-0.1.21-5.fc24.noarch                    98/107
  Installing  : gcc-gdb-plugin-6.1.1-2.fc24.x86_64                       99/107
  Installing  : createrepo-0.10.3-3.fc21.noarch                         100/107
  Upgrading   : rpm-plugin-systemd-inhibit-4.13.0.1-1.fc24.x86_64       101/107
  Cleanup     : rpm-python3-4.13.0-0.rc1.27.fc24.x86_64                 102/107
  Cleanup     : rpm-build-libs-4.13.0-0.rc1.27.fc24.x86_64              103/107
/sbin/ldconfig: Cannot lstat /lib64/librpmbuild.so.7.0.0: No such file or directory
/sbin/ldconfig: Cannot lstat /lib64/librpmsign.so.7.0.0: No such file or directory
  Cleanup     : rpm-plugin-systemd-inhibit-4.13.0-0.rc1.27.fc24.x86_6   104/107
  Cleanup     : rpm-plugin-selinux-4.13.0-0.rc1.27.fc24.x86_64          105/107
  Cleanup     : rpm-libs-4.13.0-0.rc1.27.fc24.x86_64                    106/107
/sbin/ldconfig: Cannot lstat /lib64/librpm.so.7.0.0: No such file or directory
/sbin/ldconfig: Cannot lstat /lib64/librpmbuild.so.7.0.0: No such file or directory
/sbin/ldconfig: Cannot lstat /lib64/librpmio.so.7.0.0: No such file or directory
/sbin/ldconfig: Cannot lstat /lib64/librpmsign.so.7.0.0: No such file or directory
  Cleanup     : rpm-4.13.0-0.rc1.27.fc24.x86_64                         107/107
  Verifying   : createrepo-0.10.3-3.fc21.noarch                           1/107
  Verifying   : pyliblzma-0.5.3-15.fc24.x86_64                            2/107
  Verifying   : python-deltarpm-3.6-15.fc24.x86_64                        3/107
  Verifying   : yum-3.4.3-509.fc24.noarch                                 4/107
  Verifying   : yum-metadata-parser-1.1.4-16.fc24.x86_64                  5/107
  Verifying   : cpio-2.12-3.fc24.x86_64                                   6/107
  Verifying   : python-urlgrabber-3.10.1-8.fc24.noarch                    7/107
  Verifying   : python2-iniparse-0.4-19.fc24.noarch                       8/107
  Verifying   : pyxattr-0.5.3-7.fc24.x86_64                               9/107
  Verifying   : python-pycurl-7.43.0-2.fc24.x86_64                       10/107
  Verifying   : python-six-1.10.0-2.fc24.noarch                          11/107
  Verifying   : rpm-build-4.13.0.1-1.fc24.x86_64                         12/107
  Verifying   : file-5.25-6.fc24.x86_64                                  13/107
  Verifying   : patch-2.7.5-3.fc24.x86_64                                14/107
  Verifying   : xz-5.2.2-2.fc24.x86_64                                   15/107
  Verifying   : rpm-python-4.13.0.1-1.fc24.x86_64                        16/107
  Verifying   : rpmdevtools-8.9-1.fc24.noarch                            17/107
  Verifying   : fakeroot-1.20.2-4.fc24.x86_64                            18/107
  Verifying   : perl-File-Temp-0.23.04-347.fc24.noarch                   19/107
  Verifying   : perl-Carp-1.38-2.fc24.noarch                             20/107
  Verifying   : perl-Exporter-5.72-349.fc24.noarch                       21/107
  Verifying   : perl-constant-1.33-348.fc24.noarch                       22/107
  Verifying   : perl-parent-1:0.234-4.fc24.noarch                        23/107
  Verifying   : elfutils-0.166-2.fc24.x86_64                             24/107
  Verifying   : python-libxml2-2.9.3-3.fc24.x86_64                       25/107
  Verifying   : python-2.7.13-2.fc24.x86_64                              26/107
  Verifying   : python-libs-2.7.13-2.fc24.x86_64                         27/107
  Verifying   : python-pip-8.0.2-1.fc24.noarch                           28/107
  Verifying   : python2-setuptools-20.1.1-1.fc24.noarch                  29/107
  Verifying   : python2-pygpgme-0.3-18.fc24.x86_64                       30/107
  Verifying   : perl-4:5.22.4-372.fc24.x86_64                            31/107
  Verifying   : perl-libs-4:5.22.4-372.fc24.x86_64                       32/107
  Verifying   : perl-Text-Tabs+Wrap-2013.0523-347.fc24.noarch            33/107
  Verifying   : perl-Unicode-Normalize-1.25-2.fc24.x86_64                34/107
  Verifying   : perl-threads-1:2.02-3.fc24.x86_64                        35/107
  Verifying   : perl-threads-shared-1.48-347.fc24.x86_64                 36/107
  Verifying   : perl-Errno-1.23-372.fc24.x86_64                          37/107
  Verifying   : perl-File-Path-2.12-3.fc24.noarch                        38/107
  Verifying   : perl-IO-1.35-372.fc24.x86_64                             39/107
  Verifying   : perl-PathTools-3.62-3.fc24.x86_64                        40/107
  Verifying   : perl-Scalar-List-Utils-3:1.46-1.fc24.x86_64              41/107
  Verifying   : fakeroot-libs-1.20.2-4.fc24.x86_64                       42/107
  Verifying   : findutils-1:4.6.0-7.fc24.x86_64                          43/107
  Verifying   : perl-Getopt-Long-2.49.1-1.fc24.noarch                    44/107
  Verifying   : perl-Pod-Usage-4:1.69-1.fc24.noarch                      45/107
  Verifying   : perl-Text-ParseWords-3.30-347.fc24.noarch                46/107
  Verifying   : xemacs-filesystem-21.5.34-17.20160603hga561e02bb626.f    47/107
  Verifying   : binutils-2.26.1-1.fc24.x86_64                            48/107
  Verifying   : bzip2-1.0.6-21.fc24.x86_64                               49/107
  Verifying   : gdb-7.11.1-86.fc24.x86_64                                50/107
  Verifying   : gc-7.4.2-6.fc24.x86_64                                   51/107
  Verifying   : libbabeltrace-1.2.4-4.fc24.x86_64                        52/107
  Verifying   : libipt-1.4.4-2.fc24.x86_64                               53/107
  Verifying   : libatomic_ops-7.4.2-9.fc24.x86_64                        54/107
  Verifying   : perl-generators-1.10-1.fc24.noarch                       55/107
  Verifying   : perl-Fedora-VSP-0.001-2.fc24.noarch                      56/107
  Verifying   : redhat-rpm-config-42-2.fc24.noarch                       57/107
  Verifying   : dwz-0.12-2.fc24.x86_64                                   58/107
  Verifying   : fpc-srpm-macros-1.0-1.fc24.noarch                        59/107
  Verifying   : ghc-srpm-macros-1.4.2-4.fc24.noarch                      60/107
  Verifying   : gnat-srpm-macros-4-1.fc24.noarch                         61/107
  Verifying   : go-srpm-macros-2-6.fc24.noarch                           62/107
  Verifying   : ocaml-srpm-macros-2-4.fc24.noarch                        63/107
  Verifying   : perl-srpm-macros-1-18.fc24.noarch                        64/107
  Verifying   : python-srpm-macros-3-7.fc24.noarch                       65/107
  Verifying   : zip-3.0-16.fc24.x86_64                                   66/107
  Verifying   : tar-2:1.28-8.fc24.x86_64                                 67/107
  Verifying   : unzip-6.0-31.fc24.x86_64                                 68/107
  Verifying   : perl-Socket-3:2.024-1.fc24.x86_64                        69/107
  Verifying   : perl-macros-4:5.22.4-372.fc24.x86_64                     70/107
  Verifying   : guile-5:2.0.13-1.fc24.x86_64                             71/107
  Verifying   : perl-Pod-Perldoc-3.26-1.fc24.noarch                      72/107
  Verifying   : groff-base-1.22.3-8.fc24.x86_64                          73/107
  Verifying   : perl-podlators-4.09-1.fc24.noarch                        74/107
  Verifying   : perl-Term-ANSIColor-4.03-347.fc24.noarch                 75/107
  Verifying   : perl-Term-Cap-1.17-2.fc24.noarch                         76/107
  Verifying   : perl-Encode-3:2.84-11.fc24.x86_64                        77/107
  Verifying   : perl-MIME-Base64-3.15-349.fc24.x86_64                    78/107
  Verifying   : perl-Pod-Simple-1:3.35-1.fc24.noarch                     79/107
  Verifying   : perl-Pod-Escapes-1:1.07-349.fc24.noarch                  80/107
  Verifying   : perl-HTTP-Tiny-0.056-5.fc24.noarch                       81/107
  Verifying   : perl-Time-Local-1:1.250-1.fc24.noarch                    82/107
  Verifying   : libtool-ltdl-2.4.6-13.fc24.x86_64                        83/107
  Verifying   : dnf-plugins-core-0.1.21-5.fc24.noarch                    84/107
  Verifying   : python3-dnf-plugins-core-0.1.21-5.fc24.noarch            85/107
  Verifying   : gcc-gdb-plugin-6.1.1-2.fc24.x86_64                       86/107
  Verifying   : gcc-6.1.1-2.fc24.x86_64                                  87/107
  Verifying   : cpp-6.1.1-2.fc24.x86_64                                  88/107
  Verifying   : libmpc-1.0.2-5.fc24.x86_64                               89/107
  Verifying   : libgomp-6.1.1-2.fc24.x86_64                              90/107
  Verifying   : mpfr-3.1.5-1.fc24.x86_64                                 91/107
  Verifying   : isl-0.14-5.fc24.x86_64                                   92/107
  Verifying   : glibc-devel-2.23.1-7.fc24.x86_64                         93/107
  Verifying   : glibc-headers-2.23.1-7.fc24.x86_64                       94/107
  Verifying   : kernel-headers-4.11.12-100.fc24.x86_64                   95/107
  Verifying   : rpm-4.13.0.1-1.fc24.x86_64                               96/107
  Verifying   : rpm-python3-4.13.0.1-1.fc24.x86_64                       97/107
  Verifying   : rpm-libs-4.13.0.1-1.fc24.x86_64                          98/107
  Verifying   : rpm-plugin-selinux-4.13.0.1-1.fc24.x86_64                99/107
  Verifying   : rpm-plugin-systemd-inhibit-4.13.0.1-1.fc24.x86_64       100/107
  Verifying   : rpm-build-libs-4.13.0.1-1.fc24.x86_64                   101/107
  Verifying   : rpm-build-libs-4.13.0-0.rc1.27.fc24.x86_64              102/107
  Verifying   : rpm-libs-4.13.0-0.rc1.27.fc24.x86_64                    103/107
  Verifying   : rpm-plugin-selinux-4.13.0-0.rc1.27.fc24.x86_64          104/107
  Verifying   : rpm-plugin-systemd-inhibit-4.13.0-0.rc1.27.fc24.x86_6   105/107
  Verifying   : rpm-python3-4.13.0-0.rc1.27.fc24.x86_64                 106/107
  Verifying   : rpm-4.13.0-0.rc1.27.fc24.x86_64                         107/107

Installed:
  binutils.x86_64 2.26.1-1.fc24
  bzip2.x86_64 1.0.6-21.fc24
  cpio.x86_64 2.12-3.fc24
  cpp.x86_64 6.1.1-2.fc24
  createrepo.noarch 0.10.3-3.fc21
  dnf-plugins-core.noarch 0.1.21-5.fc24
  dwz.x86_64 0.12-2.fc24
  elfutils.x86_64 0.166-2.fc24
  fakeroot.x86_64 1.20.2-4.fc24
  fakeroot-libs.x86_64 1.20.2-4.fc24
  file.x86_64 5.25-6.fc24
  findutils.x86_64 1:4.6.0-7.fc24
  fpc-srpm-macros.noarch 1.0-1.fc24
  gc.x86_64 7.4.2-6.fc24
  gcc.x86_64 6.1.1-2.fc24
  gcc-gdb-plugin.x86_64 6.1.1-2.fc24
  gdb.x86_64 7.11.1-86.fc24
  ghc-srpm-macros.noarch 1.4.2-4.fc24
  glibc-devel.x86_64 2.23.1-7.fc24
  glibc-headers.x86_64 2.23.1-7.fc24
  gnat-srpm-macros.noarch 4-1.fc24
  go-srpm-macros.noarch 2-6.fc24
  groff-base.x86_64 1.22.3-8.fc24
  guile.x86_64 5:2.0.13-1.fc24
  isl.x86_64 0.14-5.fc24
  kernel-headers.x86_64 4.11.12-100.fc24
  libatomic_ops.x86_64 7.4.2-9.fc24
  libbabeltrace.x86_64 1.2.4-4.fc24
  libgomp.x86_64 6.1.1-2.fc24
  libipt.x86_64 1.4.4-2.fc24
  libmpc.x86_64 1.0.2-5.fc24
  libtool-ltdl.x86_64 2.4.6-13.fc24
  mpfr.x86_64 3.1.5-1.fc24
  ocaml-srpm-macros.noarch 2-4.fc24
  patch.x86_64 2.7.5-3.fc24
  perl.x86_64 4:5.22.4-372.fc24
  perl-Carp.noarch 1.38-2.fc24
  perl-Encode.x86_64 3:2.84-11.fc24
  perl-Errno.x86_64 1.23-372.fc24
  perl-Exporter.noarch 5.72-349.fc24
  perl-Fedora-VSP.noarch 0.001-2.fc24
  perl-File-Path.noarch 2.12-3.fc24
  perl-File-Temp.noarch 0.23.04-347.fc24
  perl-Getopt-Long.noarch 2.49.1-1.fc24
  perl-HTTP-Tiny.noarch 0.056-5.fc24
  perl-IO.x86_64 1.35-372.fc24
  perl-MIME-Base64.x86_64 3.15-349.fc24
  perl-PathTools.x86_64 3.62-3.fc24
  perl-Pod-Escapes.noarch 1:1.07-349.fc24
  perl-Pod-Perldoc.noarch 3.26-1.fc24
  perl-Pod-Simple.noarch 1:3.35-1.fc24
  perl-Pod-Usage.noarch 4:1.69-1.fc24
  perl-Scalar-List-Utils.x86_64 3:1.46-1.fc24
  perl-Socket.x86_64 3:2.024-1.fc24
  perl-Term-ANSIColor.noarch 4.03-347.fc24
  perl-Term-Cap.noarch 1.17-2.fc24
  perl-Text-ParseWords.noarch 3.30-347.fc24
  perl-Text-Tabs+Wrap.noarch 2013.0523-347.fc24
  perl-Time-Local.noarch 1:1.250-1.fc24
  perl-Unicode-Normalize.x86_64 1.25-2.fc24
  perl-constant.noarch 1.33-348.fc24
  perl-generators.noarch 1.10-1.fc24
  perl-libs.x86_64 4:5.22.4-372.fc24
  perl-macros.x86_64 4:5.22.4-372.fc24
  perl-parent.noarch 1:0.234-4.fc24
  perl-podlators.noarch 4.09-1.fc24
  perl-srpm-macros.noarch 1-18.fc24
  perl-threads.x86_64 1:2.02-3.fc24
  perl-threads-shared.x86_64 1.48-347.fc24
  pyliblzma.x86_64 0.5.3-15.fc24
  python.x86_64 2.7.13-2.fc24
  python-deltarpm.x86_64 3.6-15.fc24
  python-libs.x86_64 2.7.13-2.fc24
  python-libxml2.x86_64 2.9.3-3.fc24
  python-pip.noarch 8.0.2-1.fc24
  python-pycurl.x86_64 7.43.0-2.fc24
  python-six.noarch 1.10.0-2.fc24
  python-srpm-macros.noarch 3-7.fc24
  python-urlgrabber.noarch 3.10.1-8.fc24
  python2-iniparse.noarch 0.4-19.fc24
  python2-pygpgme.x86_64 0.3-18.fc24
  python2-setuptools.noarch 20.1.1-1.fc24
  python3-dnf-plugins-core.noarch 0.1.21-5.fc24
  pyxattr.x86_64 0.5.3-7.fc24
  redhat-rpm-config.noarch 42-2.fc24
  rpm-build.x86_64 4.13.0.1-1.fc24
  rpm-python.x86_64 4.13.0.1-1.fc24
  rpmdevtools.noarch 8.9-1.fc24
  tar.x86_64 2:1.28-8.fc24
  unzip.x86_64 6.0-31.fc24
  xemacs-filesystem.noarch 21.5.34-17.20160603hga561e02bb626.fc24
  xz.x86_64 5.2.2-2.fc24
  yum.noarch 3.4.3-509.fc24
  yum-metadata-parser.x86_64 1.1.4-16.fc24
  zip.x86_64 3.0-16.fc24

Upgraded:
  rpm.x86_64 4.13.0.1-1.fc24
  rpm-build-libs.x86_64 4.13.0.1-1.fc24
  rpm-libs.x86_64 4.13.0.1-1.fc24
  rpm-plugin-selinux.x86_64 4.13.0.1-1.fc24
  rpm-plugin-systemd-inhibit.x86_64 4.13.0.1-1.fc24
  rpm-python3.x86_64 4.13.0.1-1.fc24

Complete!
20 files removed
 ---&amp;gt; 295a2b248a98
Removing intermediate container 0578e2b9ce55
Step 4/8 : RUN rpmdev-setuptree
 ---&amp;gt; Running in aec87b56defa

 ---&amp;gt; f43a229b7230
Removing intermediate container aec87b56defa
Step 5/8 : USER root
 ---&amp;gt; Running in 7549fcc85980
 ---&amp;gt; a6e5dbebfb6d
Removing intermediate container 7549fcc85980
Step 6/8 : ADD entry.sh /root/
 ---&amp;gt; 3484f1f7b28b
Removing intermediate container e4c31edbf1b7
Step 7/8 : COPY ./ /root/rpmbuild/SPECS
^@^@^@ ---&amp;gt; b61e9f1b4dae
Removing intermediate container d0d131b9086e
Step 8/8 : ENTRYPOINT /root/entry.sh
 ---&amp;gt; Running in 9f47fb92dabf
 ---&amp;gt; ce9655e630be
Removing intermediate container 9f47fb92dabf
Successfully built ce9655e630be
Cleaning output directory...


Building RPM&#39;s for amd64.....
Building target platforms: x86_64
Building for target x86_64
warning: bogus date in %changelog: Mon Jun 30 2017 Mike Danese &amp;lt;mikedanese@google.com&amp;gt; - 1.7.0
Executing(%prep): /bin/sh -e /var/tmp/rpm-tmp.AhtePd
+ umask 022
+ cd /root/rpmbuild/BUILD
+ ln -s 10-kubeadm-post-1.8.conf /root/rpmbuild/SOURCES/x86_64/10-kubeadm.conf
+ cp -p /root/rpmbuild/SOURCES/x86_64/kubelet /root/rpmbuild/BUILD/
+ cp -p /root/rpmbuild/SOURCES/x86_64/kubelet.service /root/rpmbuild/BUILD/
+ cp -p /root/rpmbuild/SOURCES/x86_64/kubectl /root/rpmbuild/BUILD/
+ cp -p /root/rpmbuild/SOURCES/x86_64/kubeadm /root/rpmbuild/BUILD/
+ cp -p /root/rpmbuild/SOURCES/x86_64/10-kubeadm.conf /root/rpmbuild/BUILD/
+ cd /root/rpmbuild/BUILD
+ /usr/bin/mkdir -p cni-plugins
+ cd cni-plugins
+ /usr/bin/gzip -dc /root/rpmbuild/SOURCES/x86_64/cni-amd64-0799f5732f2a11b329d9e3d51b9c8f2e3759f2ff.tar.gz
+ /usr/bin/tar -xvvof -
drwxr-xr-x root/root         0 2017-03-22 20:04 bin/
-rwxr-xr-x root/root   4000236 2017-03-22 20:04 bin/ptp
-rwxr-xr-x root/root   3170507 2017-03-22 20:04 bin/loopback
-rwxr-xr-x root/root   2909669 2017-03-22 20:03 bin/tuning
-rwxr-xr-x root/root   2733314 2017-03-22 20:04 bin/noop
-rwxr-xr-x root/root   3102946 2017-03-22 20:04 bin/host-local
-rwxr-xr-x root/root   3609358 2017-03-22 20:04 bin/ipvlan
-rwxr-xr-x root/root   4026452 2017-03-22 20:04 bin/bridge
-rwxr-xr-x root/root   9636499 2017-03-22 20:04 bin/dhcp
-rwxr-xr-x root/root   2910884 2017-03-22 20:03 bin/flannel
-rwxr-xr-x root/root   2901956 2017-03-22 20:03 bin/cnitool
-rwxr-xr-x root/root   3640336 2017-03-22 20:04 bin/macvlan
+ STATUS=0
+ &#39;[&#39; 0 -ne 0 &#39;]&#39;
+ /usr/bin/chmod -Rf a+rX,u+w,g-w,o-w .
+ exit 0
Executing(%install): /bin/sh -e /var/tmp/rpm-tmp.Bwbwj3
+ umask 022
+ cd /root/rpmbuild/BUILD
+ &#39;[&#39; /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64 &#39;!=&#39; / &#39;]&#39;
+ rm -rf /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64
++ dirname /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64
+ mkdir -p /root/rpmbuild/BUILDROOT
+ mkdir /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64
+ cd cni-plugins
+ cd /root/rpmbuild/BUILD
+ install -m 755 -d /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/usr/bin
+ install -m 755 -d /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/etc/systemd/system/
+ install -m 755 -d /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/etc/systemd/system/kubelet.service.d/
+ install -m 755 -d /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/etc/cni/net.d/
+ install -m 755 -d /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/etc/kubernetes/manifests/
+ install -m 755 -d /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/var/lib/kubelet/
+ install -p -m 755 -t /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/usr/bin/ kubelet
+ install -p -m 755 -t /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/usr/bin/ kubectl
+ install -p -m 755 -t /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/usr/bin/ kubeadm
^@+ install -p -m 755 -t /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/etc/systemd/system/ kubelet.service
+ install -p -m 755 -t /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/etc/systemd/system/kubelet.service.d/ 10-kubeadm.conf
+ install -m 755 -d /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/opt/cni/bin
+ mv cni-plugins/bin/ /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64/opt/cni/
+ &#39;[&#39; &#39;%{buildarch}&#39; = noarch &#39;]&#39;
+ QA_CHECK_RPATHS=1
+ case &amp;quot;${QA_CHECK_RPATHS:-}&amp;quot; in
+ /usr/lib/rpm/check-rpaths
+ /usr/lib/rpm/check-buildroot
+ /usr/lib/rpm/brp-compress
+ /usr/lib/rpm/brp-strip /usr/bin/strip
+ /usr/lib/rpm/brp-strip-comment-note /usr/bin/strip /usr/bin/objdump
+ /usr/lib/rpm/brp-strip-static-archive /usr/bin/strip
+ /usr/lib/rpm/brp-python-bytecompile /usr/bin/python 1
+ /usr/lib/rpm/brp-python-hardlink
+ /usr/lib/rpm/redhat/brp-java-repack-jars
Processing files: kubelet-1.8.8-0.x86_64
Provides: kubelet = 1.8.8-0 kubelet(x86-64) = 1.8.8-0
Requires(rpmlib): rpmlib(CompressedFileNames) &amp;lt;= 3.0.4-1 rpmlib(FileDigests) &amp;lt;= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) &amp;lt;= 4.0-1
Requires: libc.so.6()(64bit) libc.so.6(GLIBC_2.2.5)(64bit) libdl.so.2()(64bit) libdl.so.2(GLIBC_2.2.5)(64bit) libpthread.so.0()(64bit) libpthread.so.0(GLIBC_2.2.5)(64bit) libpthread.so.0(GLIBC_2.3.2)(64bit) rtld(GNU_HASH)
Processing files: kubernetes-cni-0.5.1-0.x86_64
Provides: kubernetes-cni = 0.5.1-0 kubernetes-cni(x86-64) = 0.5.1-0
Requires(rpmlib): rpmlib(CompressedFileNames) &amp;lt;= 3.0.4-1 rpmlib(FileDigests) &amp;lt;= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) &amp;lt;= 4.0-1
Processing files: kubectl-1.8.8-0.x86_64
Provides: kubectl = 1.8.8-0 kubectl(x86-64) = 1.8.8-0
Requires(rpmlib): rpmlib(CompressedFileNames) &amp;lt;= 3.0.4-1 rpmlib(FileDigests) &amp;lt;= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) &amp;lt;= 4.0-1
Processing files: kubeadm-1.8.8-0.x86_64
Provides: kubeadm = 1.8.8-0 kubeadm(x86-64) = 1.8.8-0
Requires(rpmlib): rpmlib(CompressedFileNames) &amp;lt;= 3.0.4-1 rpmlib(FileDigests) &amp;lt;= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) &amp;lt;= 4.0-1
Checking for unpackaged file(s): /usr/lib/rpm/check-files /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64
Wrote: /root/rpmbuild/RPMS/x86_64/kubelet-1.8.8-0.x86_64.rpm
Wrote: /root/rpmbuild/RPMS/x86_64/kubernetes-cni-0.5.1-0.x86_64.rpm
Wrote: /root/rpmbuild/RPMS/x86_64/kubectl-1.8.8-0.x86_64.rpm
Wrote: /root/rpmbuild/RPMS/x86_64/kubeadm-1.8.8-0.x86_64.rpm
Executing(%clean): /bin/sh -e /var/tmp/rpm-tmp.Fcm7SF
+ umask 022
+ cd /root/rpmbuild/BUILD
+ cd cni-plugins
+ /usr/bin/rm -rf /root/rpmbuild/BUILDROOT/kubelet-1.8.8-0.x86_64
+ exit 0
Spawning worker 0 with 4 pkgs
Workers Finished
Saving Primary metadata
Saving file lists metadata
Saving other metadata
Generating sqlite DBs
Sqlite DBs complete

----------------------------------------

RPMs written to:
kubeadm-1.8.8-0.x86_64.rpm  kubectl-1.8.8-0.x86_64.rpm  kubelet-1.8.8-0.x86_64.rpm  kubernetes-cni-0.5.1-0.x86_64.rpm  repodata

Yum repodata written to:
09954ce8a2bfc9330aa1596d76c2c764ef65bb2023f23a667cf193b53e1e28e2-primary.xml.gz      78e0b66f5f1973d9e2776d3a0404d1ed608ac8596bef947617c4396d01fedcde-filelists.sqlite.bz2
17f049e21c69cd6aeba18158ff48e3b7adfe32e1710ba0ca99c90daf4ce7c54d-primary.sqlite.bz2  b2f1de02474133b9f68ccecc9b95b279c3dc23d3343515359a475b5b6fe68a91-other.xml.gz
30e6372ee67fc74b5bf60c7f13ba45ae6e08ba8d3db80f90329e3ae871c2079a-other.sqlite.bz2    repomd.xml
37f4dc5b66b39f06337d21ac6f0b540f7c27f2f5b42deb4f6a149cda3df3fd82-filelists.xml.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@opt8 rpm]# ll
总用量 2027784
-rw-r--r--. 1 root root      1016 3月  10 07:32 10-kubeadm-post-1.8.conf
-rw-r--r--. 1 root root       856 3月  10 07:32 10-kubeadm-pre-1.8.conf
-rwxr-xr-x. 1 root root  54237747 3月  10 16:22 apiextensions-apiserver
-rwxr-xr-x. 1 root root 110880768 3月  10 16:22 cloud-controller-manager
-rw-r--r--. 1 root root  14082201 3月  10 18:13 cni-amd64-0799f5732f2a11b329d9e3d51b9c8f2e3759f2ff.tar.gz
-rwxr-xr-x. 1 root root   6740450 3月  10 16:22 conversion-gen
-rwxr-xr-x. 1 root root   6742107 3月  10 16:22 deepcopy-gen
-rwxr-xr-x. 1 root root   6718271 3月  10 16:22 defaulter-gen
-rwxr-xr-x. 1 root root       408 3月  10 07:32 docker-build.sh
-rw-r--r--. 1 root root       244 3月  10 07:32 Dockerfile
-rwxr-xr-x. 1 root root       988 3月  10 11:09 entry.sh
-rwxr-xr-x. 1 root root  52011243 3月  10 16:22 gendocs
-rwxr-xr-x. 1 root root 140484555 3月  10 16:22 genfeddocs
-rwxr-xr-x. 1 root root 156373023 3月  10 16:22 genkubedocs
-rwxr-xr-x. 1 root root 164402757 3月  10 16:22 genman
-rwxr-xr-x. 1 root root   4974927 3月  10 16:22 genswaggertypedocs
-rwxr-xr-x. 1 root root  51952109 3月  10 16:22 genyaml
-rwxr-xr-x. 1 root root  10084277 3月  10 16:22 ginkgo
-rwxr-xr-x. 1 root root  38472295 3月  10 16:22 gke-certificates-controller
-rwxr-xr-x. 1 root root   2649945 3月  10 16:22 go-bindata
-rwxr-xr-x. 1 root root 235923770 3月  10 16:22 hyperkube
-rwxr-xr-x. 1 root root 137715128 3月  10 16:22 kubeadm
-rwxr-xr-x. 1 root root  53087245 3月  10 16:22 kube-aggregator
-rwxr-xr-x. 1 root root 193591708 3月  10 16:23 kube-apiserver
-rwxr-xr-x. 1 root root 129514813 3月  10 16:23 kube-controller-manager
-rwxr-xr-x. 1 root root  51900572 3月  10 16:23 kubectl
-rwxr-xr-x. 1 root root  55276524 3月  10 16:23 kubefed
-rwxr-xr-x. 1 root root 138920515 3月  10 16:23 kubelet
-rw-r--r--. 1 root root       221 3月  10 07:32 kubelet.service
-rw-r--r--. 1 root root      7248 3月  10 16:32 kubelet.spec
-rwxr-xr-x. 1 root root 137851393 3月  10 16:23 kubemark
-rwxr-xr-x. 1 root root  47428667 3月  10 16:23 kube-proxy
-rwxr-xr-x. 1 root root  53075318 3月  10 16:23 kube-scheduler
-rwxr-xr-x. 1 root root   6210494 3月  10 16:23 linkcheck
-rwxr-xr-x. 1 root root  12325899 3月  10 16:23 openapi-gen
drwxr-xr-x. 3 root root        19 3月  13 22:47 output
-rwxr-xr-x. 1 root root   2724358 3月  10 16:24 teststale
[root@opt8 rpm]# cd output/
[root@opt8 output]# ll
总用量 4
drwxr-xr-x. 3 root root 4096 3月  13 22:47 x86_64
[root@opt8 output]# cd x86_64/
[root@opt8 x86_64]# l
总用量 47112
-rw-r--r--. 1 root root 15862502 3月  13 22:47 kubeadm-1.8.8-0.x86_64.rpm
-rw-r--r--. 1 root root  7722922 3月  13 22:47 kubectl-1.8.8-0.x86_64.rpm
-rw-r--r--. 1 root root 16844318 3月  13 22:47 kubelet-1.8.8-0.x86_64.rpm
-rw-r--r--. 1 root root  7800954 3月  13 22:47 kubernetes-cni-0.5.1-0.x86_64.rpm
drwxr-xr-x. 2 root root     4096 3月  13 22:47 repodata
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算devops系列---- dev</title>
          <link>https://kingjcy.github.io/post/cloud/paas/ops/dev/</link>
          <pubDate>Wed, 04 Apr 2018 11:26:37 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/ops/dev/</guid>
          <description>&lt;p&gt;开发相关事项。&lt;/p&gt;

&lt;h1 id=&#34;k8s本身开发&#34;&gt;k8s本身开发&lt;/h1&gt;

&lt;p&gt;基本上都是扩展开发，比如CNI，CSI，CRI，调度框架，api等，最好不要在源码上修改。&lt;/p&gt;

&lt;h1 id=&#34;云原生应用的开发&#34;&gt;云原生应用的开发&lt;/h1&gt;

&lt;h2 id=&#34;云原生应用开发基本方式&#34;&gt;云原生应用开发基本方式&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;全云端：开发效率低&lt;/li&gt;
&lt;li&gt;全本地：机器需求大&lt;/li&gt;
&lt;li&gt;云端+本地：本地开发，同步的云端，目前比较好的开发方式&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;社区开发模式&#34;&gt;社区开发模式&lt;/h2&gt;

&lt;p&gt;下面是社区中Kubernetes开源爱好者的分享内容，我觉得是对Kubernetes在DevOps中应用的很好的形式值得大家借鉴。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;根据环境（比如开发、测试、生产）划分namespace，也可以根据项目来划分&lt;/li&gt;
&lt;li&gt;再为每个用户划分一个namespace、创建一个serviceaccount和kubeconfig文件，不同namespace间的资源隔离，目前不隔离网络，不同namespace间的服务可以互相访问&lt;/li&gt;
&lt;li&gt;创建yaml模板，降低编写Kubernetes yaml文件编写难度&lt;/li&gt;
&lt;li&gt;在kubectl命令上再封装一层，增加用户身份设置和环境初始化操作，简化kubectl命令和常用功能&lt;/li&gt;
&lt;li&gt;管理员通过dashboard查看不同namespace的状态，也可以使用它来使操作更便捷&lt;/li&gt;
&lt;li&gt;所有应用的日志统一收集到ElasticSearch中，统一日志访问入口&lt;/li&gt;
&lt;li&gt;可以通过Grafana查看所有namespace中的应用的状态和kubernetes集群本身的状态&lt;/li&gt;
&lt;li&gt;需要持久化的数据保存在分布式存储中，例如GlusterFS或Ceph中&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Rabbitmq</title>
          <link>https://kingjcy.github.io/post/middleware/mq/rabbitmq/</link>
          <pubDate>Tue, 20 Mar 2018 19:27:58 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/rabbitmq/</guid>
          <description>&lt;p&gt;RabbitMQ是一个在AMQP（Advanced Message Queuing Protocol ）基础上实现的，由Erlang开发，可复用的企业消息系统。它可以用于大型软件系统各个模块之间的高效通信，支持高并发，支持可扩展。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;1、Broker:消息队列服务器实体&lt;/p&gt;

&lt;p&gt;2、messages（消息）：每个消息都有一个路由键(routing key)的属性。就是一个简单的字符串。&lt;/p&gt;

&lt;p&gt;3、connection：应用程序与broker的网络连接。&lt;/p&gt;

&lt;p&gt;4、channel:几乎所有的操作都在channel中进行，channel是进行消息读写的通道。客户端可建立多个channel，每个channel代表一个会话任务。&lt;/p&gt;

&lt;p&gt;5、exchange（交换机）：消息交换机，它指定消息按什么规则，路由到哪个队列。&lt;/p&gt;

&lt;p&gt;6、Routing Key：路由关键字，exchange根据这个关键字进行消息投递&lt;/p&gt;

&lt;p&gt;7、binding（绑定）：它的作用就是把exchange和queue按照路由规则绑定起来。一个绑定就是基于路由键将交换机和队列连接起来的路由规则，所以交换机不过就是一个由绑定构成的路由表。&lt;/p&gt;

&lt;p&gt;比如一个具有路由键“key1”的消息要发送到两个队列，queueA和queueB。要做到这点就要建立两个绑定，每个绑定连接一个交换机和一个队列。两者都是由路由键“key1”触发，这种情况，交换机会复制一份消息并把它们分别发送到两个队列中。&lt;/p&gt;

&lt;p&gt;8、queues（队列）：消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。&lt;/p&gt;

&lt;p&gt;9、vhost：虚拟主机，一个broker里可以开设多个vhost，用作不同用户的权限分离。&lt;/p&gt;

&lt;p&gt;10、producer：消息生产者，就是投递消息的程序。&lt;/p&gt;

&lt;p&gt;11、consumer：消息消费者，就是接受消息的程序。&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;先看rabbitmq的一个使用组件流程图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/rabbitmq/20180320.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;AMQP模型中，消息在producer中产生，发送到MQ的exchange上，exchange根据配置的路由方式发到相应的Queue上，Queue又将消息发送给consumer，消息从queue到consumer有push和pull两种方式。&lt;/p&gt;

&lt;p&gt;整理大概如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;客户端连接到消息队列服务器，打开一个channel。&lt;/li&gt;
&lt;li&gt;客户端声明一个exchange，并设置相关属性。&lt;/li&gt;
&lt;li&gt;客户端声明一个queue，并设置相关属性。&lt;/li&gt;
&lt;li&gt;客户端使用routing key，在exchange和queue之间建立好绑定关系。&lt;/li&gt;
&lt;li&gt;客户端投递消息到exchange。&lt;/li&gt;
&lt;li&gt;exchange接收到消息后，就根据消息的key和已经设置的binding，进行消息路由，将消息投递到一个或多个队列里。exchange也有几个类型，完全根据key进行投递的叫做Direct交换机，例如，绑定时设置了routing key为”abc”，那么客户端提交的消息，只有设置了key为”abc”的才会投递到队列。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;rabbitmq的核心就是exchange构成的路由规则和queues组成的队列，下面我们核心讲解一下这两个&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;exchange（交换机）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;交换机基本有3种类型：direct，topic，fanout。&lt;/p&gt;

&lt;p&gt;为什么不创建一种交换机来处理所有类型的路由规则？因为每种规则匹配时的CPU开销是不同的，所以根据不同需求选择合适交换机。&lt;/p&gt;

&lt;p&gt;举例：一个&amp;rdquo;topic&amp;rdquo;类型的交换机会将消息的路由键与类似“dog.*”的模式进行匹配。一个“direct”类型的交换机会将路由键与“dogs”进行比较。匹配末端通配符比直接比较消耗更多的cpu,所以如果用不到“topic”类型交换机带来的灵活性，就通过“direct”类型交换机获得更高的处理效率。&lt;/p&gt;

&lt;p&gt;1、Direct交换机：转发消息到routingKey指定队列（完全匹配，单播）。&lt;/p&gt;

&lt;p&gt;routingKey与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发routingkey标记为dog的消息，不会转发dog.puppy，也不会转发dog.guard等。&lt;/p&gt;

&lt;p&gt;2、Topic交换机：按规则转发消息（最灵活，组播）&lt;/p&gt;

&lt;p&gt;Topic类型交换机通过模式匹配分配消息的routing-key属性。将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。&lt;/p&gt;

&lt;p&gt;它将routing-key和binding-key的字符串切分成单词。这些单词之间用点隔开。它同样也会识别两个通配符：符号“#”和符号“*”。#匹配0个或多个单词，*匹配不多不少一个单词。&lt;/p&gt;

&lt;p&gt;例如，binding key:*.stock.#匹配routing key: usd.stock和eur.stock.db，但是不匹配stock.nana。&lt;/p&gt;

&lt;p&gt;例如，“audit.#”能够匹配到“audit.irs.corporate”，但是“audit.*”只会匹配到“audit.irs”。&lt;/p&gt;

&lt;p&gt;3、Fanout交换机：转发消息到所有绑定队列（最快，广播）&lt;/p&gt;

&lt;p&gt;fanout交换机不处理路由键，简单的将队列绑定到交换机上，每个发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。&lt;/p&gt;

&lt;p&gt;很像子网广播，每台子网内的主机都获得了一份复制的消息。Fanout交换机转发消息是最快的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果没有队列绑定在交换机上，则发送到该交换机上的消息会丢失。&lt;/p&gt;

&lt;p&gt;一个交换机可以绑定多个队列，一个队列可以被多个交换机绑定。&lt;/p&gt;

&lt;p&gt;因为交换机是命名实体，声明一个已经存在的交换机，但是试图赋予不同类型是会导致错误。客户端需要删除这个已经存在的交换机，然后重新声明并且赋予新的类型。&lt;/p&gt;

&lt;p&gt;交换机的属性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;持久性：如果启用，交换机将会在server重启前都有效。&lt;/li&gt;
&lt;li&gt;自动删除：如果启用，那么交换机将会在其绑定的队列都被删掉之后删除自身。&lt;/li&gt;
&lt;li&gt;惰性:如果没有声明交换机，那么在执行到使用的时候会导致异常，并不会主动声明。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;queues（队列）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;队列的属性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;持久性：如果启用，队列将在Server服务重启前都有效。&lt;/li&gt;
&lt;li&gt;自动删除：如果启用，那么队列将会在所有的消费者停止使用之后自动删除自身。&lt;/li&gt;
&lt;li&gt;惰性：如果没有声明队列，那么在执行到使用的时候会导致异常，并不会主动声明。&lt;/li&gt;
&lt;li&gt;排他性：如果启用，队列只能被声明它的消费者使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;部署使用&#34;&gt;部署使用&lt;/h1&gt;

&lt;p&gt;1、启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmq-server &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、队列重置（清空队列、用户等）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl stop_app
rabbitmqctl reset
rabbitmqctl stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、关闭&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、列举出所有用户&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl list_users
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、列举出所有队列&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl list_queues
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、添加用户&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl add_user user_name user_passwd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7、设置用户角色为管理员&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl set_user_tags user administrator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8、权限设置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl set_permissions -p / user &amp;quot;.*&amp;quot; &amp;quot;.*&amp;quot; &amp;quot;.*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用户和权限设置&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;添加用户：rabbitmqctl add_user username password&lt;/li&gt;
&lt;li&gt;分配角色：rabbitmqctl set_user_tags username administrator&lt;/li&gt;
&lt;li&gt;新增虚拟主机：rabbitmqctl add_vhost  vhost_name&lt;/li&gt;
&lt;li&gt;将新虚拟主机授权给新用户：rabbitmqctl set_permissions -p vhost_name username “.*” “.*” “.*”(后面三个”*”代表用户拥有配置、写、读全部权限)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;角色说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;超级管理员(administrator)
可登陆管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。&lt;/li&gt;
&lt;li&gt;监控者(monitoring)
可登陆管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等)&lt;/li&gt;
&lt;li&gt;策略制定者(policymaker)
可登陆管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。&lt;/li&gt;
&lt;li&gt;普通管理者(management)
仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。&lt;/li&gt;
&lt;li&gt;其他
无法登陆管理控制台，通常就是普通的生产者和消费者。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;9、查看状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10、安装RabbitMQWeb管理插件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmq-plugins enable rabbitmq_management 
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Activemq</title>
          <link>https://kingjcy.github.io/post/middleware/mq/activemq/</link>
          <pubDate>Mon, 19 Mar 2018 19:54:48 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/activemq/</guid>
          <description>&lt;p&gt;ActiveMQ是Apache软件基金下的一个开源软件，它遵循JMS规范（Java Message Service），是消息驱动中间件软件（MOM）。它为企业消息传递提供高可用，出色性能，可扩展，稳定和安全保障。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;1、Broker，消息代理，表示消息队列服务器实体，接受客户端连接，提供消息通信的核心服务。&lt;/p&gt;

&lt;p&gt;2、Producer，消息生产者，业务的发起方，负责生产消息并传输给 Broker 。&lt;/p&gt;

&lt;p&gt;3、Consumer，消息消费者，业务的处理方，负责从 Broker 获取消息并进行业务逻辑处理。&lt;/p&gt;

&lt;p&gt;4、Topic，主题，发布订阅模式下的消息统一汇集地，不同生产者向 Topic 发送消息，由 Broker 分发到不同的订阅者，实现消息的广播。&lt;/p&gt;

&lt;p&gt;5、Queue，队列，点对点模式下特定生产者向特定队列发送消息，消费者订阅特定队列接收消息并进行业务逻辑处理。&lt;/p&gt;

&lt;p&gt;6、Message，消息体，根据不同通信协议定义的固定格式进行编码的数据包，来封装业务 数据，实现消息的传输。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;主要特点&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、多种语言和协议编写客户端。语言: Java, C, C++, C#, Ruby, Perl, Python, PHP。应用协议: OpenWire,Stomp REST,WSNotification,XMPP,AMQP&lt;/p&gt;

&lt;p&gt;2、完全支持JMS1.1和J2EE 1.4规范 (持久化,XA消息,事务)&lt;/p&gt;

&lt;p&gt;3、对Spring的支持,ActiveMQ可以很容易内嵌到使用Spring的系统里面去,而且也支持Spring2.0的特性&lt;/p&gt;

&lt;p&gt;4、通过了常见J2EE服务器(如 Geronimo,JBoss 4,GlassFish,WebLogic)的测试,其中通过JCA 1.5resource adaptors的配置,可以让ActiveMQ可以自动的部署到任何兼容J2EE 1.4 商业服务器上&lt;/p&gt;

&lt;p&gt;5、支持多种传送协议:in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA&lt;/p&gt;

&lt;p&gt;6、支持通过JDBC和journal提供高速的消息持久化&lt;/p&gt;

&lt;p&gt;7、从设计上保证了高性能的集群,客户端-服务器,点对点&lt;/p&gt;

&lt;p&gt;8、支持Ajax&lt;/p&gt;

&lt;p&gt;9、支持与Axis的整合&lt;/p&gt;

&lt;p&gt;10、可以很容易得调用内嵌JMS provider,进行测试&lt;/p&gt;

&lt;p&gt;因为java编写可见对java使用还是很友好的。&lt;/p&gt;

&lt;h1 id=&#34;activemq的安装&#34;&gt;ActiveMQ的安装&lt;/h1&gt;

&lt;p&gt;进入&lt;a href=&#34;http://activemq.apache.org/&#34;&gt;http://activemq.apache.org/&lt;/a&gt; 下载ActiveMQ。&lt;/p&gt;

&lt;p&gt;安装环境：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、需要jdk
2、安装Linux系统。生产环境都是Linux系统。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装步骤&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;第一步： 把ActiveMQ 的压缩包上传到Linux系统。
第二步：解压缩。
第三步：启动。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用bin目录下的activemq命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;启动：[root@localhost bin]# ./activemq start

关闭：[root@localhost bin]# ./activemq stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost bin]# ./activemq status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入管理后台：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://192.168.25.168:8161/admin

用户名：admin
密码：admin
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;activemq类别及开发流程&#34;&gt;ActiveMQ类别及开发流程&lt;/h1&gt;

&lt;p&gt;1、Point-to-Point (点对点)消息模式开发流程 ：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;生产者（producer）开发流程（ProducerTool.java）： 

&lt;ul&gt;
&lt;li&gt;创建Connection： 根据url，user和password创建一个jms Connection。 &lt;/li&gt;
&lt;li&gt;创建Session： 在connection的基础上创建一个session，同时设置是否支持事务和ACKNOWLEDGE标识。 &lt;/li&gt;
&lt;li&gt;创建Destination对象： 需指定其对应的主题（subject）名称，producer和consumer将根据subject来发送/接收对应的消息。 &lt;/li&gt;
&lt;li&gt;创建MessageProducer： 根据Destination创建MessageProducer对象，同时设置其持久模式。 &lt;/li&gt;
&lt;li&gt;发送消息到队列（Queue）： 封装TextMessage消息，使用MessageProducer的send方法将消息发送出去。 &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;消费者（consumer）开发流程（ConsumerTool.java）： 

&lt;ul&gt;
&lt;li&gt;实现MessageListener接口： 消费者类必须实现MessageListener接口，然后在onMessage()方法中监听消息的到达并处理。 &lt;/li&gt;
&lt;li&gt;创建Connection： 根据url，user和password创建一个jms Connection，如果是durable模式，还需要给connection设置一个clientId。 
- 创建Session和Destination： 与ProducerTool.java中的流程类似，不再赘述。 &lt;/li&gt;
&lt;li&gt;创建replyProducer【可选】：可以用来将消息处理结果发送给producer。 &lt;/li&gt;
&lt;li&gt;创建MessageConsumer：  根据Destination创建MessageConsumer对象。 &lt;/li&gt;
&lt;li&gt;消费message：  在onMessage()方法中接收producer发送过来的消息进行处理，并可以通过replyProducer反馈信息给producer &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、当然还是支持发布／订阅的广播模式。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Producer&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建ConnectionFactory对象，需要指定服务端ip及端口号。&lt;/li&gt;
&lt;li&gt;使用ConnectionFactory对象创建一个Connection对象。&lt;/li&gt;
&lt;li&gt;开启连接，调用Connection对象的start方法。&lt;/li&gt;
&lt;li&gt;使用Connection对象创建一个Session对象。&lt;/li&gt;
&lt;li&gt;使用Session对象创建一个Destination对象（topic、queue），此处创建一个Topic对象。&lt;/li&gt;
&lt;li&gt;使用Session对象创建一个Producer对象。&lt;/li&gt;
&lt;li&gt;创建一个Message对象，创建一个TextMessage对象。&lt;/li&gt;
&lt;li&gt;使用Producer对象发送消息。&lt;/li&gt;
&lt;li&gt;关闭资源。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Consumer消费者：接收消息。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建一个ConnectionFactory对象。&lt;/li&gt;
&lt;li&gt;从ConnectionFactory对象中获得一个Connection对象。&lt;/li&gt;
&lt;li&gt;开启连接。调用Connection对象的start方法。&lt;/li&gt;
&lt;li&gt;使用Connection对象创建一个Session对象。&lt;/li&gt;
&lt;li&gt;使用Session对象创建一个Destination对象。和发送端保持一致topic，并且话题的名称一致。&lt;/li&gt;
&lt;li&gt;使用Session对象创建一个Consumer对象。&lt;/li&gt;
&lt;li&gt;接收消息。&lt;/li&gt;
&lt;li&gt;打印消息。&lt;/li&gt;
&lt;li&gt;关闭资源&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基本就是这两种使用方式。我们可以使用golang来编写生产者和消费者，使用现成的库，然后当着一个mq来传递数据即可。这也是我们最常使用，跨语言进行使用。使得业务和技术实现解耦，也是我们所说的中间件的概念。这样可以跨语言来使用优秀的开源产品，只要知道原理，使用方法，就可以直接使用。其他的mq比如kafka，也是这么个使用道理。所以要理解架构才是道理。&lt;/p&gt;

&lt;h1 id=&#34;性能&#34;&gt;性能&lt;/h1&gt;

&lt;p&gt;ActiveMQ，在赛扬（2.40GHz）机器上能够达到2000/s，消息大小为1-2k。好一些的服务器可以达到2万以上/秒。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Node Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/node-exporter/</link>
          <pubDate>Mon, 19 Mar 2018 16:51:51 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/node-exporter/</guid>
          <description>&lt;p&gt;node_exporter 主要用于 LINUX 系统监控, 用 Golang 编写，是我们最常用于监控服务器资源的探针。&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;1、二进制&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/prometheus/node_exporter/releases/download/v0.14.0/node_exporter-0.14.0.linux-amd64.tar.gz&#34;&gt;https://github.com/prometheus/node_exporter/releases/download/v0.14.0/node_exporter-0.14.0.linux-amd64.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;我们可以直接使用 ./node_exporter -h 查看运行选项，./node_exporter 运行 Node Exporter&lt;/p&gt;

&lt;p&gt;2、docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -p 9100:9100 \
  -v &amp;quot;/proc:/host/proc:ro&amp;quot; \
  -v &amp;quot;/sys:/host/sys:ro&amp;quot; \
  -v &amp;quot;/:/rootfs:ro&amp;quot; \
  --net=&amp;quot;host&amp;quot; \
  quay.io/prometheus/node-exporter \
    -collector.procfs /host/proc \
    -collector.sysfs /host/sys \
    -collector.filesystem.ignored-mount-points &amp;quot;^/(sys|proc|dev|host|etc)($|/)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;源码原理解析&#34;&gt;源码原理解析&lt;/h1&gt;

&lt;p&gt;node探针进程启动的时候，会调用collector的package，就会初始化所有的collect注册到&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;factories      = make(map[string]func() (Collector, error))
collectorState = make(map[string]*bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个中，根据初始化中传递的参数，确定采集是否开启，处理函数是什么&lt;/p&gt;

&lt;p&gt;然后会创建一个结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type nodeCollector struct {
    Collectors map[string]Collector
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将开启采集的数据，创建对应的结构体，其实也是一个实现interface的结构，最后存储在这个结构中&lt;/p&gt;

&lt;p&gt;然后将这些collector进行注册，启动监听端口和一些其他设置&lt;/p&gt;

&lt;p&gt;然后就是调用client_golang中的describe和collect的重写函数&lt;/p&gt;

&lt;p&gt;在collect中启动多协程进行每个类型的处理&lt;/p&gt;

&lt;p&gt;调用update函数实现数据赋值与采集&lt;/p&gt;

&lt;h1 id=&#34;开启默认关闭的采集&#34;&gt;开启默认关闭的采集&lt;/h1&gt;

&lt;p&gt;我们可以使用 &amp;ndash;collectors.enabled 运行参数指定 node_exporter 收集的功能模块, 如果不指定，将使用默认模块。&lt;/p&gt;

&lt;p&gt;比如开启ntp采集&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./node_exporter --collector.ntp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出相关指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# HELP node_ntp_leap NTPD leap second indicator, 2 bits.
# TYPE node_ntp_leap gauge
node_ntp_leap 3
# HELP node_ntp_offset_seconds ClockOffset between NTP and local clock.
# TYPE node_ntp_offset_seconds gauge
node_ntp_offset_seconds -1.6239e-05
# HELP node_ntp_reference_timestamp_seconds NTPD ReferenceTime, UNIX timestamp.
# TYPE node_ntp_reference_timestamp_seconds gauge
node_ntp_reference_timestamp_seconds 0
# HELP node_ntp_root_delay_seconds NTPD RootDelay.
# TYPE node_ntp_root_delay_seconds gauge
node_ntp_root_delay_seconds 0
# HELP node_ntp_root_dispersion_seconds NTPD RootDispersion.
# TYPE node_ntp_root_dispersion_seconds gauge
node_ntp_root_dispersion_seconds 0.060363769
# HELP node_ntp_rtt_seconds RTT to NTPD.
# TYPE node_ntp_rtt_seconds gauge
node_ntp_rtt_seconds 8.6646e-05
# HELP node_ntp_sanity NTPD sanity according to RFC5905 heuristics and configured limits.
# TYPE node_ntp_sanity gauge
node_ntp_sanity 0
# HELP node_ntp_stratum NTPD stratum.
# TYPE node_ntp_stratum gauge
node_ntp_stratum 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;计算表达式&#34;&gt;计算表达式&lt;/h1&gt;

&lt;p&gt;收集到 node_exporter 的数据后，我们可以使用 PromQL 进行一些业务查询和监控，下面是一些比较常见的查询&lt;/p&gt;

&lt;p&gt;CPU 各 mode 占比率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;avg by (instance, mode) (irate(node_cpu_seconds_total{instance=&amp;quot;xxx&amp;quot;}[5m])) * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cpu的使用率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;avg(irate(node_cpu_seconds_total{ip=&amp;quot;$host&amp;quot;}[1m]))by (mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node_cpu_seconds_total采集出来的是cpu使用时间的累加，在linux机器上cpu的使用就是这样记录的，所以可以使用速率的方式来求使用率，GPU貌似不是这样记录的。并不是递增的。因为可能是多核的，所以要使用avg平均出所有核。&lt;/p&gt;

&lt;p&gt;pod的cpu使用率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(rate(container_cpu_usage_seconds_total{image!=&amp;quot;&amp;quot;, container_name!=&amp;quot;POD&amp;quot;}[5m])) by (pod_name,sn_pod_ip,container_name,namespace) / sum(container_spec_cpu_quota{image!=&amp;quot;&amp;quot;, container_name!=&amp;quot;POD&amp;quot;}/container_spec_cpu_period{image!=&amp;quot;&amp;quot;, container_name!=&amp;quot;POD&amp;quot;}) by (pod_name,sn_pod_ip,container_name,namespace)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;机器平均负载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;node_load1{instance=&amp;quot;xxx&amp;quot;} // 1分钟负载
node_load5{instance=&amp;quot;xxx&amp;quot;} // 5分钟负载
node_load15{instance=&amp;quot;xxx&amp;quot;} // 15分钟负载
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内存使用率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;100 - ((node_memory_MemFree{instance=&amp;quot;xxx&amp;quot;}+node_memory_Cached{instance=&amp;quot;xxx&amp;quot;}+node_memory_Buffers{instance=&amp;quot;xxx&amp;quot;})/node_memory_MemTotal) * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;磁盘使用率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;100 - node_filesystem_free{instance=&amp;quot;xxx&amp;quot;,fstype!~&amp;quot;rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs|udev|none|devpts|sysfs|debugfs|fuse.*&amp;quot;} / node_filesystem_size{instance=&amp;quot;xxx&amp;quot;,fstype!~&amp;quot;rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs|udev|none|devpts|sysfs|debugfs|fuse.*&amp;quot;} * 100
或者你也可以直接使用 {fstype=&amp;quot;xxx&amp;quot;} 来指定想查看的磁盘信息
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;网络 IO&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 上行带宽
sum by (instance) (irate(node_network_receive_bytes{instance=&amp;quot;xxx&amp;quot;,device!~&amp;quot;bond.*?|lo&amp;quot;}[5m])/128)

// 下行带宽
sum by (instance) (irate(node_network_transmit_bytes{instance=&amp;quot;xxx&amp;quot;,device!~&amp;quot;bond.*?|lo&amp;quot;}[5m])/128)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;网卡出/入包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 入包量
sum by (instance) (rate(node_network_receive_bytes{instance=&amp;quot;xxx&amp;quot;,device!=&amp;quot;lo&amp;quot;}[5m]))

// 出包量
sum by (instance) (rate(node_network_transmit_bytes{instance=&amp;quot;xxx&amp;quot;,device!=&amp;quot;lo&amp;quot;}[5m]))
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 微服务</title>
          <link>https://kingjcy.github.io/post/architecture/microservices/microservices/</link>
          <pubDate>Mon, 05 Mar 2018 19:11:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/microservices/microservices/</guid>
          <description>&lt;p&gt;微服务其实就是服务化的一种概念，由过去单体架构演变成分布式系统的一个产物。&lt;/p&gt;

&lt;h1 id=&#34;微服务架构演变由来&#34;&gt;微服务架构演变由来&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/architecture-change&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;微服务&#34;&gt;微服务&lt;/h1&gt;

&lt;p&gt;微服务是一种软件架构思想，它将一个大且聚合的业务项目拆解为多个小且独立的业务模块，模块即服务，各服务间使用高效的协议（protobuf、JSON 等）相互调用即是 RPC。这种拆分代码库的方式有以下特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个服务应作为小规模的、独立的业务模块在运行，类似 Unix 的 Do one thing and do it well，这就是微服务参考了unix的设计哲学，包括独立开发，自动化测试（细致的错误检查和处理）和（分布式）部署，不影响其他服务。同时也加快了开发交付周期，降低代码耦合度导致的沟通成本。&lt;/li&gt;
&lt;li&gt;从依赖库到依赖服务，增加了开发者选择的自由（语言，框架，库），提高了复用效率，只要符合服务 API 契约，开发人员可以自由选择开发技术。这就意味着开发人员可以采用新技术编写或重构服务，由于服务相对较小，所以这并不会对整体应用造成太大影响。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见微服务也是分布式系统，但是微服务注重的注册中心，和服务发现，可以实现简单的扩缩容的方式，这也是微服务的一大重大特点。也就是服务治理，其实后面出现的各大框架也是着重解决这个问题。&lt;/p&gt;

&lt;p&gt;为什么要服务化？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;随着模块越来越多，相互调用会产生很多的冗余，所以需要服务化架构（SOA）。&lt;/li&gt;
&lt;li&gt;解耦，解决冲突与臃肿。比如sql在专门的sql服务运行，统一检测修改。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;第一代微服务-服务化&#34;&gt;第一代微服务&amp;mdash;服务化&lt;/h2&gt;

&lt;p&gt;微服务落地目前存在的主要困难有如下几方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务间通信：单体应用拆分为分布式系统后，进程间的通讯机制和故障处理措施变的更加复杂。&lt;/li&gt;
&lt;li&gt;分布式事务：系统微服务化后，一个看似简单的功能，内部可能需要调用多个服务并操作多个数据库实现，服务调用的分布式事务问题变的非常突出。&lt;/li&gt;
&lt;li&gt;大规模部署：微服务数量众多，其测试、部署、监控等都变的更加困难。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;随着RPC框架的成熟，第一个问题已经逐渐得到解决。例如springcloud可以非常好的支持restful调用，dubbo可以支持多种通讯协议，包括现在k8s上istio都志在解决服务间通信。&lt;/p&gt;

&lt;p&gt;对于第三个问题，随着docker、devops技术的发展以及各公有云paas平台自动化运维工具的推出，微服务的测试、部署与运维会变得越来越容易。&lt;/p&gt;

&lt;p&gt;而对于第二个问题，现在还没有通用方案很好的解决微服务产生的事务问题。分布式事务已经成为微服务落地最大的阻碍，也是最具挑战性的一个技术难题,目前&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-event/&#34;&gt;主要解决方案&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;当然微服务架构的还有很多缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;微服务强调了服务大小，但实际上这并没有一个统一的标准：业务逻辑应该按照什么规则划分为微服务，这本身就是一个经验工程。有些开发者主张 10-100 行代码就应该建立一个微服务。虽然建立小型服务是微服务架构崇尚的，但要记住，微服务是达到目的的手段，而不是目标。微服务的目标是充分分解应用程序，以促进敏捷开发和持续集成部署。&lt;/li&gt;
&lt;li&gt;微服务的分布式特点带来的复杂性：开发人员需要基于 RPC 或者消息实现微服务之间的调用和通信，而这就使得服务之间的发现、服务调用链的跟踪和质量问题变得的相当棘手。微服务的精髓之一就是服务之间传递的是可序列化消息，而不是对象和引用，这个思想是和 DCOM 及 EJB 完全相反的。只有数据，不包含逻辑；这个设计的好处不用我多说也很好理解，参考 CSP&lt;/li&gt;
&lt;li&gt;分区的数据库体系和分布式事务：更新多个业务实体的业务交易相当普遍，不同服务可能拥有不同的数据库。CAP 原理的约束，使得我们不得不放弃传统的强一致性，而转而追求最终一致性，这个对开发人员来说是一个挑战。&lt;/li&gt;
&lt;li&gt;测试挑战：传统的单体WEB应用只需测试单一的 REST API 即可，而对微服务进行测试，需要启动它依赖的所有其他服务。这种复杂性不可低估。&lt;/li&gt;
&lt;li&gt;跨多个服务的更改：比如在传统单体应用中，若有 A、B、C 三个服务需要更改，A 依赖 B，B 依赖 C。我们只需更改相应的模块，然后一次性部署即可。但是在微服务架构中，我们需要仔细规划和协调每个服务的变更部署。我们需要先更新 C，然后更新 B，最后更新 A。&lt;/li&gt;
&lt;li&gt;部署复杂：微服务由不同的大量服务构成。每种服务可能拥有自己的配置、应用实例数量以及基础服务地址。这里就需要不同的配置、部署、扩展和监控组件。此外，我们还需要服务发现机制，以便服务可以发现与其通信的其他服务的地址。因此，成功部署微服务应用需要开发人员有更好地部署策略和高度自动化的水平。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总的来说（问题和挑战）：API Gateway、服务间调用、服务发现、服务容错、服务部署、数据调用。其实大部分也是我们一开始的说的微服务落地的三大难题。&lt;/p&gt;

&lt;p&gt;不过，现在很多微服务的框架（比如 Spring Cloud、Dubbo）已经很好的解决了上面的问题。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Spring Cloud&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Spring Cloud为开发者提供了快速构建分布式系统的通用模型的工具（包括配置管理、服务发现、熔断器、智能路由、微代理、控制总线、一次性令牌、全局锁、领导选举、分布式会话、集群状态等）&lt;/p&gt;

&lt;p&gt;其实主要解决了部署的问题，他就是一个很好的部署工具，目前也开始拥抱docker了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Dubbo&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Dubbo是一个阿里巴巴开源出来的一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案，其实主要解决的服务间通信的问题。&lt;/p&gt;

&lt;p&gt;在这个时候组织架构就发生了很大的变化，基本都是要加上一层网关或者企业总线，也就是&lt;strong&gt;前台-网关（ESB）-后台&lt;/strong&gt;的结构。&lt;/p&gt;

&lt;h3 id=&#34;soa&#34;&gt;SOA&lt;/h3&gt;

&lt;p&gt;SOA是什么？SOA全英文是Service-Oriented Architecture，中文意思是面向服务架构，是一种思想，一种方法论，一种分布式的服务架构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/20180305.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实就是上面实现的架构思想，抽象共享解耦。&lt;/p&gt;

&lt;h2 id=&#34;下一代微服务-去中心化&#34;&gt;下一代微服务&amp;ndash;去中心化&lt;/h2&gt;

&lt;h3 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h3&gt;

&lt;p&gt;随着注册中心的出现，任何调用都走网关，网关的瓶颈有到来，于是出现了下一代的微服务架构service mesh，主要落地的项目就是istio，还有一些其他的项目，主要是去中心化的设计。&lt;/p&gt;

&lt;p&gt;在云原生模型里，一个应用可以由数百个服务组成，每个服务可能有数千个实例，而每个实例可能会持续地发生变化。这种情况下，服务间通信不仅异常复杂，而且也是运行时行为的基础。管理好服务间通信对于保证端到端的性能和可靠性来说是无疑是非常重要的。种种复杂局面便催生了服务间通信层的出现，这个层既不会与应用程序的代码耦合，又能捕捉到底层环境高度动态的特点，让业务开发者只关注自己的业务代码，并将应用云化后带来的诸多问题以不侵入业务代码的方式提供给开发者。&lt;/p&gt;

&lt;p&gt;这个服务间通信层就是 Service Mesh，它可以提供安全、快速、可靠的服务间通讯（service-to-service）。&lt;/p&gt;

&lt;p&gt;Service Mesh，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给 Service Mesh 就可以了。&lt;/p&gt;

&lt;p&gt;微服务的各种框架也有一些。但这些框架大多是编程语言层面来解决的，需要用户的业务代码中集成框架的类库，语言的选择也受限。这种方案很难作为单独的产品或者服务给用户使用，升级更新也受限于应用本身的更新与迭代。直到 Service Mesh 的概念的提出。Service Mesh 貌似也没有比较契合的翻译（有的译做服务齿合层，有的翻译做服务网格），这个概念就是试图在网络层抽象出一层，来统一接管一些微服务治理的功能。这样就可以做到跨语言，无侵入，独立升级。其中前一段时间 Google，IBM，Lyft 联合开源的&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-istio/&#34;&gt;istio&lt;/a&gt;就是这样一个工具，先看下它的功能简介：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;智能路由以及负载均衡&lt;/li&gt;
&lt;li&gt;跨语言以及平台&lt;/li&gt;
&lt;li&gt;全范围（Fleet-wide）策略执行&lt;/li&gt;
&lt;li&gt;深度监控和报告&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;主要做了什么&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可视化 其实本质上微服务治理的许多技术点都包含可视化要求，比如监控和链路追踪，比如服务依赖&lt;/li&gt;
&lt;li&gt;弹性（Resiliency 或者应该叫柔性，因为弹性很容易想到 scale） 就是网络层可以不那么生硬，比如超时控制，重试策略，错误注入，熔断，延迟注入都属于这个范围。&lt;/li&gt;
&lt;li&gt;效率（Efficiency） 网络层可以帮应用层多做一些事情，提升效率。比如卸载 TLS，协议转换兼容&lt;/li&gt;
&lt;li&gt;流量控制 比如根据一定规则分发流量到不同的 Service 后端，但对调用方来说是透明的。&lt;/li&gt;
&lt;li&gt;安全保护 在网络层对流量加密/解密，增加安全认证机制，而对应用透明。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前流行的 Service Mesh 开源软件还有 Linkerd、Envoy，而最近 Buoyant（开源 Linkerd 的公司）又发布了基于 Kubernetes 的 Service Mesh 开源项目 Conduit。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linkerd（&lt;a href=&#34;https://github.com/linkerd/linkerd）：第一代&#34;&gt;https://github.com/linkerd/linkerd）：第一代&lt;/a&gt; Service Mesh，2016 年 1 月 15 日首发布，业界第一个 Service Mesh 项目，由 Buoyant 创业小公司开发（前 Twitter 工程师），2017 年 7 月 11 日，宣布和 Istio 集成，成为 Istio 的数据面板。&lt;/li&gt;
&lt;li&gt;Envoy（&lt;a href=&#34;https://github.com/envoyproxy/envoy）：第一代&#34;&gt;https://github.com/envoyproxy/envoy）：第一代&lt;/a&gt; Service Mesh，2016 年 9 月 13 日首发布，由 Matt Klein 个人开发（Lyft 工程师），之后默默发展，版本较稳定。&lt;/li&gt;
&lt;li&gt;Conduit（&lt;a href=&#34;https://github.com/runconduit/conduit）：第二代&#34;&gt;https://github.com/runconduit/conduit）：第二代&lt;/a&gt; Service Mesh，2017 年 12 月 5 日首发布，由 Buoyant 公司开发（借鉴 Istio 整体架构，部分进行了优化），对抗 Istio 压力山大，也期待 Buoyant 公司的毅力。&lt;/li&gt;
&lt;li&gt;nginMesh（&lt;a href=&#34;https://github.com/nginmesh/nginmesh）：2017&#34;&gt;https://github.com/nginmesh/nginmesh）：2017&lt;/a&gt; 年 9 月首发布，由 Nginx 开发，定位是作为 Istio 的服务代理，也就是替代 Envoy，思路跟 Linkerd 之前和 Istio 集成很相似，极度低调，GitHub 上的 star 也只有不到 100。&lt;/li&gt;
&lt;li&gt;Kong（&lt;a href=&#34;https://github.com/Kong/kong）：比&#34;&gt;https://github.com/Kong/kong）：比&lt;/a&gt; nginMesh 更加低调，默默发展中。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/go-micro/&#34;&gt;go-micro&lt;/a&gt;是基于Go语言实现的插件化RPC微服务框架，与go-kit，kite等微服务框架相比，它具有易上手、部署简单、工具插件化等优点。go-micro框架提供了服务发现、负载均衡、同步传输、异步通信以及事件驱动等机制，它尝试去简化分布式系统间的通信，让我们可以专注于自身业务逻辑的开发。所以对于新手而言，go-micro是个不错的微服务实践的开始。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前，市面上有许多 Service Mesh 的实现。我们这里挑选 4 种当前最主流的 Service Mesh，对其诸多方面( 包括功能特性、支持平台、是否付费等 )进行横向对比，用以说明 Istio 所存在的优势和不足。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/servicemesh&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关于微服务和服务网格的区别，我的一些理解：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;service mesh主要是解决了微服务框架中三大难题之一的服务间通信的问题，类似于上一代服务架构dubbo的作用，是在容器docker和k8s的原生基础上实现的服务间通信的方案，所以比java系的dubbo方案更加友好。&lt;/li&gt;
&lt;li&gt;service mesh在上一代的SOA架构上做了升级，设计的是去中性化的通信架构，解决的网关或者总线的瓶颈问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;k8s和微服务&#34;&gt;k8s和微服务&lt;/h3&gt;

&lt;p&gt;k8s在docker的基础上实现了管理，促使了下一代的微服务得以落地，主要解决了微服务三大难题的部署问题。类似于springcloud，springcloud目前也开始拥抱k8s了，可见k8s是一种趋势。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/k8s-ecology&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;随着docker、容器的日渐成熟，容器编排的问题就凸显出来，大量的容器怎么去管理，怎么调度，怎么启停都成了迫切需要解决的问题。有需求就有人去解决，ApacheMesos、kubernetes、docker swarm陆续登场，大有三足鼎立之势，而随着各自的发展，到了2017年下半年，google的亲儿子kubernetes的呼声越来越高，社区也更加活跃、成熟。2017年底，docker swarm和ApacheMesos陆续宣称支持kubernetes，预示着容器编排大战的结束，kubernetes已然成为容器编排领域的事实标准。&lt;/p&gt;

&lt;p&gt;服务编排框架的成熟，使得容器的管理越来越方便、高效，容器带来的好处也随之凸显：提升资源利用率节省成本、更高效的持续集成，持续交付、解放运维、快速扩缩容，应对突发流量&amp;hellip;&lt;/p&gt;

&lt;p&gt;服务编排框架的成熟也让微服务的概念得以落地，同时也催生了java界微服务化的方案，像SpringBoot，SpringCloud。然而服务编排一定是对微服务的编排吗？也就是我们容器里运行的一定是微服务吗？不是的，我们可以运行任何服务，我们现有的业务可以不做任何改造就运行到容器中，让kubernetes去管理、调度。至于微服务呢，只是有了kubernetes，让微服务变得容易管理了。让我们有条件把服务拆分的足够小，足够简单。再也不用担心运维管理的复杂了。了解了docker，服务编排，微服务的关系，我们在看看他们在企业的落地情况。&lt;/p&gt;

&lt;p&gt;k8s实现了容器编排工具，能够满足微服务架构拆分落地的需求，包括了管理和运维。k8s算是一个工具，而微服务更多的是个一个概念，架构，可以使用k8s实现，Istio就是微服务使用k8s实现的一个实现。&lt;/p&gt;

&lt;h4 id=&#34;serverless&#34;&gt;Serverless&lt;/h4&gt;

&lt;p&gt;Serverless被翻译为“无服务器架构”，重应用，轻服务器，k8s其实就是这么一个概念。&lt;/p&gt;

&lt;h1 id=&#34;互联网架构的服务化&#34;&gt;互联网架构的服务化&lt;/h1&gt;

&lt;h2 id=&#34;为什么服务化&#34;&gt;为什么服务化&lt;/h2&gt;

&lt;p&gt;在服务化之前，互联网的高可用架构大致是这样一个架构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/server.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户端是浏览器browser，APP客户端&lt;/li&gt;
&lt;li&gt;后端入口是高可用的nginx集群，用于做反向代理&lt;/li&gt;
&lt;li&gt;中间核心是高可用的web-server集群，研发工程师主要编码工作就是在这一层&lt;/li&gt;
&lt;li&gt;后端存储是高可用的db集群，数据存储在这一层&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;痛点：代码冗余&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;比如各个业务线都是自己通过DAO写SQL访问user库来存取用户数据，这无形中就导致了代码的拷贝。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/server1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;痛点：复杂性扩散&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;随着并发量的越来越高，用户数据的访问数据库成了瓶颈，需要加入缓存来降低数据库的读压力，于是架构中引入了缓存，由于没有统一的服务层，各个业务线都需要关注缓存的引入导致的复杂性,这个复杂性是典型的“业务无关”的复杂性，业务方需要被迫升级。&lt;/p&gt;

&lt;p&gt;随着数据量的越来越大，数据库需要进行水平拆分，于是架构中又引入了分库分表，由于没有统一的服务层，各个业务线都需要关注分库分表的引入导致的复杂性,这个复杂性也是典型的“业务无关”的复杂性，业务方需要被迫升级。&lt;/p&gt;

&lt;p&gt;包括bug的修改，发现一个bug，多个地方都需要修改。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;痛点：库的复用与耦合&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;服务化并不是唯一的解决上述两痛点的方法，抽象出统一的“库”是最先容易想到的解决,的方法。抽象出一个user.so，负责整个用户数据的存取，从而避免代码的拷贝。至于复杂性，也只有user.so这一个地方需要关注了。&lt;/p&gt;

&lt;p&gt;但是解决了旧的问题，会引入新的问题，库的版本维护与业务线之间代码的耦合，服务化在这个时候各用各自的实例可以实现解耦。&lt;/p&gt;

&lt;p&gt;SQL质量得不到保障，业务相互影响，本质上SQL语句还是各个业务线拼装的，资深的工程师写出高质量的SQL没啥问题，经验没有这么丰富的工程师可能会写出一些低效的SQL，假如业务线A写了一个全表扫描的SQL，导致数据库的CPU100%，影响的不只是一个业务线，而是所有的业务线都会受影响。&lt;/p&gt;

&lt;h2 id=&#34;服务化&#34;&gt;服务化&lt;/h2&gt;

&lt;p&gt;为了解决上面的诸多问题，互联网高可用分层架构演进的过程中，引入了“服务层”，其实就是对db的服务封装。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/server2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;好处：调用直接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有服务层之前：业务方访问用户数据，需要通过DAO拼装SQL访问&lt;/p&gt;

&lt;p&gt;有服务层之后：业务方通过RPC访问用户数据，就像调用一个本地函数一样，非常之爽&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;好处：复用性，防止代码拷贝，屏蔽底层，统一优化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;升级一处升级，bug修改一处修改。&lt;/p&gt;

&lt;p&gt;在有了服务层之后，只有服务层需要专注关注底层的复杂性了，向上游屏蔽了细节。&lt;/p&gt;

&lt;p&gt;有了服务层之后，所有的SQL都是服务层提供的，业务线不能再为所欲为了。底层服务对于稳定性的要求更好的话，可以由更资深的工程师维护，而不是像原来SQL难以收口，难以控制。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;这里只是一个简单的数据库的封装，给数据库新增的一个服务层，通过本地调用函数的方式来获取数据，具体的数据库优化交给数据库专业的人来处理。其实实际业务中还可以服务化的组件有很多，比如会员订单等子业务和序列化、反序列化、网络框架、连接池、收发线程、超时处理、状态机等非业务，最后其实就是我们常说的SOA的架构。&lt;/p&gt;

&lt;h1 id=&#34;微服务的粒度&#34;&gt;微服务的粒度&lt;/h1&gt;

&lt;p&gt;大家也都认可，随着数据量、流量、业务复杂度的提升，服务化架构是架构演进中的必由之路，但是微服务架构多“微”才合适？&lt;/p&gt;

&lt;p&gt;不同粒度的服务化各有优缺点总的来说，细粒度拆分的优点有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务都能够独立部署&lt;/li&gt;
&lt;li&gt;扩容和缩容方便，有利于提高资源利用率&lt;/li&gt;
&lt;li&gt;拆得越细，耦合相对会减小&lt;/li&gt;
&lt;li&gt;拆得越细，容错相对会更好，一个服务出问题不影响其他服务&lt;/li&gt;
&lt;li&gt;扩展性更好&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;细粒度拆分的不足也很明显：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;拆得越细，系统越复杂&lt;/li&gt;
&lt;li&gt;系统之间的依赖关系也更复杂&lt;/li&gt;
&lt;li&gt;运维复杂度提升&lt;/li&gt;
&lt;li&gt;监控更加复杂&lt;/li&gt;
&lt;li&gt;出问题时定位问题更难&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;个人觉得，以“子业务系统”粒度作为微服务的单位是比较合适的。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Ioutil</title>
          <link>https://kingjcy.github.io/post/golang/go-ioutil/</link>
          <pubDate>Sat, 13 Jan 2018 11:04:07 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-ioutil/</guid>
          <description>&lt;p&gt;ioutil主要是提供了一些常用、方便的IO操作函数。&lt;/p&gt;

&lt;h1 id=&#34;ioutil&#34;&gt;ioutil&lt;/h1&gt;

&lt;p&gt;ioutil针对reader和writer这两个接口封装的基础操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Discard 是一个 io.Writer 接口，调用它的 Write 方法将不做任何事情
// 并且始终成功返回。
var Discard io.Writer = devNull(0)

// ReadAll 读取 r 中的所有数据，返回读取的数据和遇到的错误。
// 如果读取成功，则 err 返回 nil，而不是 EOF，因为 ReadAll 定义为读取
// 所有数据，所以不会把 EOF 当做错误处理。
func ReadAll(r io.Reader) ([]byte, error)

// ReadFile 读取文件中的所有数据，返回读取的数据和遇到的错误。
// 如果读取成功，则 err 返回 nil，而不是 EOF
func ReadFile(filename string) ([]byte, error)

// WriteFile 向文件中写入数据，写入前会清空文件。
// 如果文件不存在，则会以指定的权限创建该文件。
// 返回遇到的错误。
func WriteFile(filename string, data []byte, perm os.FileMode) error

// ReadDir 读取指定目录中的所有目录和文件（不包括子目录）。
// 返回读取到的文件信息列表和遇到的错误，列表是经过排序的。
func ReadDir(dirname string) ([]os.FileInfo, error)

// NopCloser 将 r 包装为一个 ReadCloser 类型，但 Close 方法不做任何事情。
func NopCloser(r io.Reader) io.ReadCloser

// TempFile 在 dir 目录中创建一个以 prefix 为前缀的临时文件，并将其以读
// 写模式打开。返回创建的文件对象和遇到的错误。
// 如果 dir 为空，则在默认的临时目录中创建文件（参见 os.TempDir），多次
// 调用会创建不同的临时文件，调用者可以通过 f.Name() 获取文件的完整路径。
// 调用本函数所创建的临时文件，应该由调用者自己删除。
func TempFile(dir, prefix string) (f *os.File, err error)

// TempDir 功能同 TempFile，只不过创建的是目录，返回目录的完整路径。
func TempDir(dir, prefix string) (name string, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;示例&#34;&gt;示例&lt;/h1&gt;

&lt;h2 id=&#34;读取目录&#34;&gt;读取目录&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    rd, err := ioutil.ReadDir(&amp;quot;/&amp;quot;)
    fmt.Println(err)
    for _, fi := range rd {
        if fi.IsDir() {
            fmt.Printf(&amp;quot;[%s]\n&amp;quot;, fi.Name())

        } else {
            fmt.Println(fi.Name())
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;临时目录-临时文件&#34;&gt;临时目录、临时文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    // 创建临时目录
    dir, err := ioutil.TempDir(&amp;quot;&amp;quot;, &amp;quot;Test&amp;quot;)
    if err != nil {
        fmt.Println(err)
    }
    defer os.Remove(dir) // 用完删除
    fmt.Printf(&amp;quot;%s\n&amp;quot;, dir)

    // 创建临时文件
    f, err := ioutil.TempFile(dir, &amp;quot;Test&amp;quot;)
    if err != nil {
        fmt.Println(err)
    }
    defer os.Remove(f.Name()) // 用完删除
    fmt.Printf(&amp;quot;%s\n&amp;quot;, f.Name())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;读取文件&#34;&gt;读取文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;io/ioutil&amp;quot;
)

func main() {
    b, err := ioutil.ReadFile(&amp;quot;test.log&amp;quot;)
    if err != nil {
        fmt.Print(err)
    }
    fmt.Println(b)
    str := string(b)
    fmt.Println(str)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;写文件&#34;&gt;写文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
   &amp;quot;io/ioutil&amp;quot;
)

func check(e error) {
   if e != nil {
       panic(e)
   }
}

func main() {

   d1 := []byte(&amp;quot;hello\ngo\n&amp;quot;)
   err := ioutil.WriteFile(&amp;quot;test.txt&amp;quot;, d1, 0644)
   check(err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读取文件和写文件内容还可以使用&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/#文件io&#34;&gt;os包&lt;/a&gt;来处理。一般也是使用os标准库来处理。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Bytes</title>
          <link>https://kingjcy.github.io/post/golang/go-bytes/</link>
          <pubDate>Mon, 25 Dec 2017 14:28:17 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-bytes/</guid>
          <description>&lt;p&gt;该包定义了一些操作 byte slice 的便利操作。因为字符串可以表示为 []byte，因此，bytes 包定义的函数、方法等和 strings 包很类似，所以讲解时会和 strings 包类似甚至可以直接参考。&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;h2 id=&#34;是否存在某个子-slice&#34;&gt;是否存在某个子 slice&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 子 slice subslice 在 b 中，返回 true
func Contains(b, subslice []byte) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数的内部调用了 bytes.Index 函数（在后面会讲解）:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Contains(b, subslice []byte) bool {
    return Index(b, subslice) != -1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;题外：对比 strings.Contains 你会发现，一个判断 &amp;gt;=0，一个判断 != -1，可见库不是一个人写的，没有做到一致性。&lt;/p&gt;

&lt;h2 id=&#34;byte-出现次数&#34;&gt;[]byte 出现次数&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// slice sep 在 s 中出现的次数（无重叠）
func Count(s, sep []byte) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和 strings 实现不同，此包中的 Count 核心代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;count := 0
c := sep[0]
i := 0
t := s[:len(s)-n+1]
for i &amp;lt; len(t) {
    // 判断 sep 第一个字节是否在 t[i:] 中
    // 如果在，则比较之后相应的字节
    if t[i] != c {
        o := IndexByte(t[i:], c)
        if o &amp;lt; 0 {
            break
        }
        i += o
    }
    // 执行到这里表示 sep[0] == t[i]
    if n == 1 || Equal(s[i:i+n], sep) {
        count++
        i += n
        continue
    }
    i++
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;runes-类型转换&#34;&gt;Runes 类型转换&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 将 []byte 转换为 []rune
func Runes(s []byte) []rune
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数将 []byte 转换为 []rune ，适用于汉字等多字节字符，示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b:=[]byte(&amp;quot;你好，世界&amp;quot;)
for k,v:=range b{
    fmt.Printf(&amp;quot;%d:%s |&amp;quot;,k,string(v))
}
r:=bytes.Runes(b)
for k,v:=range r{
    fmt.Printf(&amp;quot;%d:%s|&amp;quot;,k,string(v))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0:ä |1:½ |2:  |3:å |4:¥ |5:½ |6:ï |7:¼ |8:  |9:ä |10:¸ |11:  |12:ç |13:  |14: |
0:你|1:好|2:，|3:世|4:界|
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;其它函数&#34;&gt;其它函数&lt;/h2&gt;

&lt;p&gt;其它大部分函数、方法与 strings 包下的函数、方法类似，只是数据源从 string 变为了 []byte ，请参考 strings 包的用法。&lt;/p&gt;

&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;

&lt;h2 id=&#34;reader-类型&#34;&gt;Reader 类型&lt;/h2&gt;

&lt;p&gt;看到名字就能猜到，这是实现了 io 包中的接口。其实这就是缓存io，对缓存中的[]byte进行读写操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader struct {
    s        []byte
    i        int64 // 当前读取下标
    prevRune int   // 前一个字符的下标，也可能 &amp;lt; 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bytes 包下的 Reader 类型实现了 io 包下的 Reader, ReaderAt, RuneReader, RuneScanner, ByteReader, ByteScanner, ReadSeeker, Seeker, WriterTo 等多个接口。主要用于 Read 数据。&lt;/p&gt;

&lt;p&gt;我们需要在通过 bytes.NewReader 方法来初始化 bytes.Reader 类型的对象。初始化时传入 []byte 类型的数据。NewReader 函数签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(b []byte) *Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果直接声明该对象了，可以通过 Reset 方法重新写入数据，示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x:=[]byte(&amp;quot;你好，世界&amp;quot;)

r1:=bytes.NewReader(x)
d1:=make([]byte,len(x))
n,_:=r1.Read(d1)
fmt.Println(n,string(d1))

r2:=bytes.Reader{}
r2.Reset(x)
d2:=make([]byte,len(x))
n,_=r2.Read(d2)
fmt.Println(n,string(d2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;15 你好，世界
15 你好，世界
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reader 包含了 8 个读取相关的方法，实现了前面提到的 io 包下的 9 个接口（ReadSeeker 接口内嵌 Reader 和 Seeker 两个接口）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 读取数据至 b
func (r *Reader) Read(b []byte) (n int, err error)
// 读取一个字节
func (r *Reader) ReadByte() (byte, error)
// 读取一个字符
func (r *Reader) ReadRune() (ch rune, size int, err error)
// 读取数据至 w
func (r *Reader) WriteTo(w io.Writer) (n int64, err error)
// 进度下标指向前一个字节，如果 r.i &amp;lt;= 0 返回错误。
func (r *Reader) UnreadByte()
// 进度下标指向前一个字符，如果 r.i &amp;lt;= 0 返回错误，且只能在每次 ReadRune 方法后使用一次，否则返回错误。
func (r *Reader) UnreadRune()
// 读取 r.s[off:] 的数据至b，该方法忽略进度下标 i，不使用也不修改。
func (r *Reader) ReadAt(b []byte, off int64) (n int, err error)
// 根据 whence 的值，修改并返回进度下标 i ，当 whence == 0 ，进度下标修改为 off，当 whence == 1 ，进度下标修改为 i+off，当 whence == 2 ，进度下标修改为 len[s]+off.
// off 可以为负数，whence 的只能为 0，1，2，当 whence 为其他值或计算后的进度下标越界，则返回错误。
func (r *Reader) Seek(offset int64, whence int) (int64, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x := []byte(&amp;quot;你好，世界&amp;quot;)
r1 := bytes.NewReader(x)

ch, size, _ := r1.ReadRune()
fmt.Println(size, string(ch))
_ = r1.UnreadRune()
ch, size, _ = r1.ReadRune()
fmt.Println(size, string(ch))
_ = r1.UnreadRune()

by, _ := r1.ReadByte()
fmt.Println(by)
_ = r1.UnreadByte()
by, _ = r1.ReadByte()
fmt.Println(by)
_ = r1.UnreadByte()

d1 := make([]byte, 6)
n, _ := r1.Read(d1)
fmt.Println(n, string(d1))

d2 := make([]byte, 6)
n, _ = r1.ReadAt(d2, 0)
fmt.Println(n, string(d2))

w1 := &amp;amp;bytes.Buffer{}
_, _ = r1.Seek(0, 0)
_, _ = r1.WriteTo(w1)
fmt.Println(w1.String())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3 你
3 你
228
228
6 你好
6 你好
你好，世界
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;buffer-类型&#34;&gt;Buffer 类型&lt;/h2&gt;

&lt;p&gt;buffer类型也实现了缓存io，也是对[]byte进行读写操作，提供了多种实现化函数对象，个人感觉是对string，byte的reader的综合使用实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Buffer struct {
    buf      []byte
    off      int
    lastRead readOp
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上一个示例的最后，我们使用了 bytes.Buffer 类型，该类型实现了 io 包下的 ByteScanner, ByteWriter, ReadWriter, Reader, ReaderFrom, RuneReader, RuneScanner, StringWriter, Writer, WriterTo 等接口，可以方便的进行读写操作。&lt;/p&gt;

&lt;p&gt;对象可读取数据为 buf[off : len(buf)], off 表示进度下标，lastRead 表示最后读取的一个字符所占字节数，方便 Unread* 相关操作。&lt;/p&gt;

&lt;p&gt;Buffer 可以通过 3 中方法初始化对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := bytes.NewBufferString(&amp;quot;Hello World&amp;quot;)
b := bytes.NewBuffer([]byte(&amp;quot;Hello World&amp;quot;))
c := bytes.Buffer{}

fmt.Println(a)
fmt.Println(b)
fmt.Println(c)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Hello World
Hello World
{[] 0 0}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Buffer 包含了 21 个读写相关的方法，大部分同名方法的用法与前面讲的类似，这里只讲演示其中的 3 个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 读取到字节 delim 后，以字节数组的形式返回该字节及前面读取到的字节。如果遍历 b.buf 也找不到匹配的字节，则返回错误(一般是 EOF)
func (b *Buffer) ReadBytes(delim byte) (line []byte, err error)
// 读取到字节 delim 后，以字符串的形式返回该字节及前面读取到的字节。如果遍历 b.buf 也找不到匹配的字节，则返回错误(一般是 EOF)
func (b *Buffer) ReadString(delim byte) (line string, err error)
// 截断 b.buf , 舍弃 b.off+n 之后的数据。n == 0 时，调用 Reset 方法重置该对象，当 n 越界时（n &amp;lt; 0 || n &amp;gt; b.Len() ）方法会触发 panic.
func (b *Buffer) Truncate(n int)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := bytes.NewBufferString(&amp;quot;Good Night&amp;quot;)

x, err := a.ReadBytes(&#39;t&#39;)
if err != nil {
    fmt.Println(&amp;quot;delim:t err:&amp;quot;, err)
} else {
    fmt.Println(string(x))
}

a.Truncate(0)
a.WriteString(&amp;quot;Good Night&amp;quot;)
fmt.Println(a.Len())
a.Truncate(5)
fmt.Println(a.Len())
y, err := a.ReadString(&#39;N&#39;)
if err != nil {
    fmt.Println(&amp;quot;delim:N err:&amp;quot;, err)
} else {
    fmt.Println(y)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Good Night
10
5
delim:N err: EOF
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Archive</title>
          <link>https://kingjcy.github.io/post/golang/go-archive/</link>
          <pubDate>Mon, 25 Dec 2017 14:26:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-archive/</guid>
          <description>&lt;p&gt;archive就是使用tar和zip两种方式对文档进行归档，压缩看compress包。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;tar和zip有什么不同&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;二者使用平台不同，对于 Windows 平台而言，最常用的格式是 zip 和 rar，国内大多数是用 rar，国外大多数是用 zip。而对于类 Unix 平台而言，常用的格式是 tar 和 tar.gz，zip 比较少一些，rar 则几乎没有。&lt;/p&gt;

&lt;p&gt;zip 格式是开放且免费的，所以广泛使用在 Windows、Linux、MacOS 平台，要说 zip 有什么缺点的话，就是它的压缩率并不是很高，不如 rar及 tar.gz 等格式。&lt;/p&gt;

&lt;p&gt;严格的说，tar 只是一种打包格式，并不对文件进行压缩，主要是为了便于文件的管理，所以打包后的文档大小一般远远大于 zip 和 tar.gz，但这种格式也有很明显的优点，例如打包速度非常快，打包时 CPU 占用率也很低，因为不需要压缩嘛。&lt;/p&gt;

&lt;h1 id=&#34;archive-tar&#34;&gt;archive/tar&lt;/h1&gt;

&lt;h2 id=&#34;单个文件操作&#34;&gt;单个文件操作&lt;/h2&gt;

&lt;p&gt;这个非常简单，就是读取一个文件，进行打包及解包操作即可。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;单个文件打包&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从 /etc/passwd 下复制了一个 passwd 文件到当前目录下，用来做压缩测试。什么文件都是可以的，自己随意写一个也行。这里的示例主要为了说明 tar ，没有处理路径，所以过程全部假设是在当前目录下执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp /etc/passwd .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于文件的打包直接查看示例代码，已经在示例代码中做了详细的注释。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;os&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;archive/tar&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
)

func main() {
    // 准备打包的源文件
    var srcFile = &amp;quot;passwd&amp;quot;
    // 打包后的文件
    var desFile = fmt.Sprintf(&amp;quot;%s.tar&amp;quot;,srcFile)

    // 需要注意文件的打开即关闭的顺序，因为 defer 是后入先出，所以关闭顺序很重要
    // 第一次写这个示例的时候就没注意，导致写完的 tar 包不完整

    // ###### 第 1 步，先准备好一个 tar.Writer 结构，然后再向里面写入内容。 ######
    // 创建一个文件，用来保存打包后的 passwd.tar 文件
    fw, err := os.Create(desFile)
    ErrPrintln(err)
    defer fw.Close()

    // 通过 fw 创建一个 tar.Writer
    tw := tar.NewWriter(fw)
    // 这里不要忘记关闭，如果不能成功关闭会造成 tar 包不完整
    // 所以这里在关闭的同时进行判断，可以清楚的知道是否成功关闭
    defer func() {
        if err := tw.Close(); err != nil {
            ErrPrintln(err)
        }
    }()

    // ###### 第 2 步，处理文件信息，也就是 tar.Header 相关的 ######
    // tar 包共有两部分内容：文件信息和文件数据
    // 通过 Stat 获取 FileInfo，然后通过 FileInfoHeader 得到 hdr tar.*Header
    fi, err := os.Stat(srcFile)
    ErrPrintln(err)
    hdr, err := tar.FileInfoHeader(fi, &amp;quot;&amp;quot;)
    // 将 tar 的文件信息 hdr 写入到 tw
    err = tw.WriteHeader(hdr)
    ErrPrintln(err)

    // 将文件数据写入
    // 打开准备写入的文件
    fr, err := os.Open(srcFile)
    ErrPrintln(err)
    defer fr.Close()

    written, err := io.Copy(tw, fr)
    ErrPrintln(err)

    log.Printf(&amp;quot;共写入了 %d 个字符的数据\n&amp;quot;,written)
}

// 定义一个用来打印的函数，少写点代码，因为要处理很多次的 err
// 后面其他示例还会继续使用这个函数，就不单独再写，望看到此函数了解
func ErrPrintln(err error)  {
    if err != nil {
        log.Println(err)
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;单个文件解包&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个也很简单，基本上将上面过程反过来，只需要处理 tar.Reader 即可，详细的描述见示例。&lt;/p&gt;

&lt;p&gt;这里就用刚刚打包的 passwd.tar 文件做示例，如果怕结果看不出效果，可以将之前用的 passwd 源文件删除。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;os&amp;quot;
    &amp;quot;archive/tar&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
)

func main() {

    var srcFile = &amp;quot;passwd.tar&amp;quot;

    // 将 tar 包打开
    fr, err := os.Open(srcFile)
    ErrPrintln(err)
    defer fr.Close()

    // 通过 fr 创建一个 tar.*Reader 结构，然后将 tr 遍历，并将数据保存到磁盘中
    tr := tar.NewReader(fr)

    for hdr, err := tr.Next(); err != io.EOF; hdr, err = tr.Next(){
        // 处理 err ！= nil 的情况
        ErrPrintln(err)
        // 获取文件信息
        fi := hdr.FileInfo()

        // 创建一个空文件，用来写入解包后的数据
        fw, err := os.Create(fi.Name())
        ErrPrintln(err)

        // 将 tr 写入到 fw
        n, err := io.Copy(fw, tr)
        ErrPrintln(err)
        log.Printf(&amp;quot;解包： %s 到 %s ，共处理了 %d 个字符的数据。&amp;quot;, srcFile,fi.Name(),n)

        // 设置文件权限，这样可以保证和原始文件权限相同，如果不设置，会根据当前系统的 umask 来设置。
        os.Chmod(fi.Name(),fi.Mode().Perm())

        // 注意，因为是在循环中，所以就没有使用 defer 关闭文件
        // 如果想使用 defer 的话，可以将文件写入的步骤单独封装在一个函数中即可
        fw.Close()
    }
}

func ErrPrintln(err error){
    if err != nil {
        log.Fatalln(err)
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;操作整个目录&#34;&gt;操作整个目录&lt;/h2&gt;

&lt;p&gt;我们实际中 tar 很少会去打包单个文件，一般都是打包整个目录，并且打包的时候通过 gzip 或者 bzip2 压缩。&lt;/p&gt;

&lt;p&gt;如果要打包整个目录，可以通过递归的方式来实现。这里只演示了 gzip 方式压缩，这个实现非常简单，只需要在 fw 和 tw 之前加上一层压缩即可，详情见示例代码。&lt;/p&gt;

&lt;p&gt;为了测试打包整个目录，复制了一个 log 目录到当前路径下。什么目录和文件都可以，只是因为这个里面内容比较多，就拿这个来做测试了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;打包压缩&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;archive/tar&amp;quot;
    &amp;quot;compress/gzip&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;path/filepath&amp;quot;
    &amp;quot;strings&amp;quot;
)

func main() {
    // 修改日志格式，显示出错代码的所在行，方便调试，实际项目中一般不记录这个。

    var src = &amp;quot;apt&amp;quot;
    var dst = fmt.Sprintf(&amp;quot;%s.tar.gz&amp;quot;, src)

    // 将步骤写入了一个函数中，这样处理错误方便一些
    if err := Tar(src, dst); err != nil {
        log.Fatalln(err)
    }
}

func Tar(src, dst string) (err error) {
    // 创建文件
    fw, err := os.Create(dst)
    if err != nil {
        return
    }
    defer fw.Close()

    // 将 tar 包使用 gzip 压缩，其实添加压缩功能很简单，
    // 只需要在 fw 和 tw 之前加上一层压缩就行了，和 Linux 的管道的感觉类似
    gw := gzip.NewWriter(fw)
    defer gw.Close()

    // 创建 Tar.Writer 结构
    tw := tar.NewWriter(gw)
    // 如果需要启用 gzip 将上面代码注释，换成下面的

    defer tw.Close()

    // 下面就该开始处理数据了，这里的思路就是递归处理目录及目录下的所有文件和目录
    // 这里可以自己写个递归来处理，不过 Golang 提供了 filepath.Walk 函数，可以很方便的做这个事情
    // 直接将这个函数的处理结果返回就行，需要传给它一个源文件或目录，它就可以自己去处理
    // 我们就只需要去实现我们自己的 打包逻辑即可，不需要再去路径相关的事情
    return filepath.Walk(src, func(fileName string, fi os.FileInfo, err error) error {
        // 因为这个闭包会返回个 error ，所以先要处理一下这个
        if err != nil {
            return err
        }

        // 这里就不需要我们自己再 os.Stat 了，它已经做好了，我们直接使用 fi 即可
        hdr, err := tar.FileInfoHeader(fi, &amp;quot;&amp;quot;)
        if err != nil {
            return err
        }
        // 这里需要处理下 hdr 中的 Name，因为默认文件的名字是不带路径的，
        // 打包之后所有文件就会堆在一起，这样就破坏了原本的目录结果
        // 例如： 将原本 hdr.Name 的 syslog 替换程 log/syslog
        // 这个其实也很简单，回调函数的 fileName 字段给我们返回来的就是完整路径的 log/syslog
        // strings.TrimPrefix 将 fileName 的最左侧的 / 去掉，
        // 熟悉 Linux 的都知道为什么要去掉这个
        hdr.Name = strings.TrimPrefix(fileName, string(filepath.Separator))

        // 写入文件信息
        if err := tw.WriteHeader(hdr); err != nil {
            return err
        }

        // 判断下文件是否是标准文件，如果不是就不处理了，
        // 如： 目录，这里就只记录了文件信息，不会执行下面的 copy
        if !fi.Mode().IsRegular() {
            return nil
        }

        // 打开文件
        fr, err := os.Open(fileName)
        defer fr.Close()
        if err != nil {
            return err
        }

        // copy 文件数据到 tw
        n, err := io.Copy(tw, fr)
        if err != nil {
            return err
        }

        // 记录下过程，这个可以不记录，这个看需要，这样可以看到打包的过程
        log.Printf(&amp;quot;成功打包 %s ，共写入了 %d 字节的数据\n&amp;quot;, fileName, n)

        return nil
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打包及压缩就搞定了，不过这个代码现在我还发现有个问题，就是不能处理软链接&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;解包解压&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个过程基本就是把压缩的过程返回来，多了些创建目录的操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;archive/tar&amp;quot;
    &amp;quot;compress/gzip&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;path/filepath&amp;quot;
)

func main() {
    var dst = &amp;quot;&amp;quot; // 不写就是解压到当前目录
    var src = &amp;quot;log.tar.gz&amp;quot;

    UnTar(dst, src)
}

func UnTar(dst, src string) (err error) {
    // 打开准备解压的 tar 包
    fr, err := os.Open(src)
    if err != nil {
        return
    }
    defer fr.Close()

    // 将打开的文件先解压
    gr, err := gzip.NewReader(fr)
    if err != nil {
        return
    }
    defer gr.Close()

    // 通过 gr 创建 tar.Reader
    tr := tar.NewReader(gr)

    // 现在已经获得了 tar.Reader 结构了，只需要循环里面的数据写入文件就可以了
    for {
        hdr, err := tr.Next()

        switch {
        case err == io.EOF:
            return nil
        case err != nil:
            return err
        case hdr == nil:
            continue
        }

        // 处理下保存路径，将要保存的目录加上 header 中的 Name
        // 这个变量保存的有可能是目录，有可能是文件，所以就叫 FileDir 了……
        dstFileDir := filepath.Join(dst, hdr.Name)

        // 根据 header 的 Typeflag 字段，判断文件的类型
        switch hdr.Typeflag {
        case tar.TypeDir: // 如果是目录时候，创建目录
            // 判断下目录是否存在，不存在就创建
            if b := ExistDir(dstFileDir); !b {
                // 使用 MkdirAll 不使用 Mkdir ，就类似 Linux 终端下的 mkdir -p，
                // 可以递归创建每一级目录
                if err := os.MkdirAll(dstFileDir, 0775); err != nil {
                    return err
                }
            }
        case tar.TypeReg: // 如果是文件就写入到磁盘
            // 创建一个可以读写的文件，权限就使用 header 中记录的权限
            // 因为操作系统的 FileMode 是 int32 类型的，hdr 中的是 int64，所以转换下
            file, err := os.OpenFile(dstFileDir, os.O_CREATE|os.O_RDWR, os.FileMode(hdr.Mode))
            if err != nil {
                return err
            }
            n, err := io.Copy(file, tr)
            if err != nil {
                return err
            }
            // 将解压结果输出显示
            fmt.Printf(&amp;quot;成功解压： %s , 共处理了 %d 个字符\n&amp;quot;, dstFileDir, n)

            // 不要忘记关闭打开的文件，因为它是在 for 循环中，不能使用 defer
            // 如果想使用 defer 就放在一个单独的函数中
            file.Close()
        }
    }

    return nil
}

// 判断目录是否存在
func ExistDir(dirname string) bool {
    fi, err := os.Stat(dirname)
    return (err == nil || os.IsExist(err)) &amp;amp;&amp;amp; fi.IsDir()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;archive-zip&#34;&gt;archive/zip&lt;/h1&gt;

&lt;h2 id=&#34;压缩&#34;&gt;压缩&lt;/h2&gt;

&lt;p&gt;和 tar 的过程很像，只有些小的差别，详情见示例代码。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;archive/zip&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;path/filepath&amp;quot;
    &amp;quot;strings&amp;quot;
)

func main() {
    // 源档案（准备压缩的文件或目录）
    var src = &amp;quot;log&amp;quot;
    // 目标文件，压缩后的文件
    var dst = &amp;quot;log.zip&amp;quot;

    if err := Zip(dst, src); err != nil {
        log.Fatalln(err)
    }
}

func Zip(dst, src string) (err error) {
    // 创建准备写入的文件
    fw, err := os.Create(dst)
    defer fw.Close()
    if err != nil {
        return err
    }

    // 通过 fw 来创建 zip.Write
    zw := zip.NewWriter(fw)
    defer func() {
        // 检测一下是否成功关闭
        if err := zw.Close(); err != nil {
            log.Fatalln(err)
        }
    }()

    // 下面来将文件写入 zw ，因为有可能会有很多个目录及文件，所以递归处理
    return filepath.Walk(src, func(path string, fi os.FileInfo, errBack error) (err error) {
        if errBack != nil {
            return errBack
        }

        // 通过文件信息，创建 zip 的文件信息
        fh, err := zip.FileInfoHeader(fi)
        if err != nil {
            return
        }

        // 替换文件信息中的文件名
        fh.Name = strings.TrimPrefix(path, string(filepath.Separator))

        // 这步开始没有加，会发现解压的时候说它不是个目录
        if fi.IsDir() {
            fh.Name += &amp;quot;/&amp;quot;
        }

        // 写入文件信息，并返回一个 Write 结构
        w, err := zw.CreateHeader(fh)
        if err != nil {
            return
        }

        // 检测，如果不是标准文件就只写入头信息，不写入文件数据到 w
        // 如目录，也没有数据需要写
        if !fh.Mode().IsRegular() {
            return nil
        }

        // 打开要压缩的文件
        fr, err := os.Open(path)
        defer fr.Close()
        if err != nil {
            return
        }

        // 将打开的文件 Copy 到 w
        n, err := io.Copy(w, fr)
        if err != nil {
            return
        }
        // 输出压缩的内容
        fmt.Printf(&amp;quot;成功压缩文件： %s, 共写入了 %d 个字符的数据\n&amp;quot;, path, n)

        return nil
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;解压缩&#34;&gt;解压缩&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;archive/zip&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;path/filepath&amp;quot;
)

func main() {
    // 压缩包
    var src = &amp;quot;log.zip&amp;quot;
    // 解压后保存的位置，为空表示当前目录
    var dst = &amp;quot;&amp;quot;

    if err := UnZip(dst, src); err != nil {
        log.Fatalln(err)
    }
}

func UnZip(dst, src string) (err error) {
    // 打开压缩文件，这个 zip 包有个方便的 ReadCloser 类型
    // 这个里面有个方便的 OpenReader 函数，可以比 tar 的时候省去一个打开文件的步骤
    zr, err := zip.OpenReader(src)
    defer zr.Close()
    if err != nil {
        return
    }

    // 如果解压后不是放在当前目录就按照保存目录去创建目录
    if dst != &amp;quot;&amp;quot; {
        if err := os.MkdirAll(dst, 0755); err != nil {
            return err
        }
    }

    // 遍历 zr ，将文件写入到磁盘
    for _, file := range zr.File {
        path := filepath.Join(dst, file.Name)

        // 如果是目录，就创建目录
        if file.FileInfo().IsDir() {
            if err := os.MkdirAll(path, file.Mode()); err != nil {
                return err
            }
            // 因为是目录，跳过当前循环，因为后面都是文件的处理
            continue
        }

        // 获取到 Reader
        fr, err := file.Open()
        if err != nil {
            return err
        }

        // 创建要写出的文件对应的 Write
        fw, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR|os.O_TRUNC, file.Mode())
        if err != nil {
            return err
        }

        n, err := io.Copy(fw, fr)
        if err != nil {
            return err
        }

        // 将解压的结果输出
        fmt.Printf(&amp;quot;成功解压 %s ，共写入了 %d 个字符的数据\n&amp;quot;, path, n)

        // 因为是在循环中，无法使用 defer ，直接放在最后
        // 不过这样也有问题，当出现 err 的时候就不会执行这个了，
        // 可以把它单独放在一个函数中，这里是个实验，就这样了
        fw.Close()
        fr.Close()
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- zabbix源码阅读</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbixcode/</link>
          <pubDate>Sat, 25 Nov 2017 09:52:47 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbixcode/</guid>
          <description>&lt;p&gt;阅读源码，解析基本原理。&lt;/p&gt;

&lt;h1 id=&#34;流程&#34;&gt;流程&lt;/h1&gt;

&lt;p&gt;一个监控系统运行的大概的流程是这样的：&lt;/p&gt;

&lt;p&gt;agentd需要安装到被监控的主机上，它负责定期收集各项数据，并发送到zabbix server端，zabbix server将数据存储到数据库中，zabbix web根据数据在前端进行展现和绘图。这里agentd收集数据分为主动和被动两种模式：&lt;/p&gt;

&lt;p&gt;主动：agent请求server获取主动的监控项列表，并主动将监控项内需要检测的数据提交给server/proxy&lt;/p&gt;

&lt;p&gt;被动：server向agent请求获取监控项的数据，agent返回数据。&lt;/p&gt;

&lt;h1 id=&#34;主动监测&#34;&gt;主动监测&lt;/h1&gt;

&lt;p&gt;通信过程如下：&lt;/p&gt;

&lt;p&gt;zabbix首先向ServerActive配置的IP请求获取active items，获取并提交active tiems数据值server或者proxy。很多人会提出疑问：zabbix多久获取一次active items？它会根据配置文件中的RefreshActiveChecks的频率进行，如果获取失败，那么将会在60秒之后重试。分两个部分：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/zabbix/z1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1.获取ACTIVE ITEMS列表&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Agent打开TCP连接（主动检测变成Agent打开）&lt;/li&gt;
&lt;li&gt;Agent请求items检测列表&lt;/li&gt;
&lt;li&gt;Server返回items列表&lt;/li&gt;
&lt;li&gt;Agent 处理响应&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;li&gt;Agent开始收集数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2.主动检测提交数据过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Agent建立TCP连接&lt;/li&gt;
&lt;li&gt;Agent提交items列表收集的数据&lt;/li&gt;
&lt;li&gt;Server处理数据，并返回响应状态&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;被动监测&#34;&gt;被动监测&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/zabbix/z2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通信过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server打开一个TCP连接&lt;/li&gt;
&lt;li&gt;Server发送请求agent.ping\n&lt;/li&gt;
&lt;li&gt;Agent接收到请求并且响应&lt;code&gt;&amp;lt;HEADER&amp;gt;&amp;lt;DATALEN&amp;gt;1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Server处理接收到的数据1&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里，有人可以看出来，被动模式每次都需要打开一个tcp连接，这样当监控项越来越多时，就会出现server端性能问题了。&lt;/p&gt;

&lt;p&gt;比如not supported items通信过程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server打开一个TCP连接&lt;/li&gt;
&lt;li&gt;Server发送请求&lt;code&gt;vfs.fs.size[ no]\n&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Agent接收请求并且返回响应数据 &lt;code&gt;&amp;lt;HEADER&amp;gt;&amp;lt;DATALEN&amp;gt;ZBX_NOTSUPPORTED\0Cannot obtain filesystem information: [2] No such file or directory&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Server接收并处理数据, 将item的状态改为“ not supported ”&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有人会问，那实际监控中是用主动的还是被动的呢？这里主要涉及两个地方：&lt;/p&gt;

&lt;p&gt;1、新建监控项目时，选择的是zabbix代理还是zabbix端点代理程式（主动式），前者是被动模式，后者是主动模式。&lt;/p&gt;

&lt;p&gt;2、agentd配置文件中StartAgents参数的设置，如果为0，表示禁止被动模式，否则开启。一般建议不要设置为0，因为监控项目很多时，可以部分使用主动，部分使用被动模式。&lt;/p&gt;

&lt;h1 id=&#34;常用的监控架构平台&#34;&gt;常用的监控架构平台&lt;/h1&gt;

&lt;p&gt;1、server-agentd模式：&lt;/p&gt;

&lt;p&gt;这个是最简单的架构了，常用于监控主机比较少的情况下。&lt;/p&gt;

&lt;p&gt;2、server-proxy-agentd模式：&lt;/p&gt;

&lt;p&gt;这个常用于比较多的机器，使用proxy进行分布式监控，有效的减轻server端的压力。&lt;/p&gt;

&lt;h1 id=&#34;组件解析&#34;&gt;组件解析&lt;/h1&gt;

&lt;h2 id=&#34;agent&#34;&gt;Agent&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;入口函数：zabbix_agentd.c:MAIN_ZABBIX_ENTRY&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;采集线程：stats.c: ZBX_THREAD_ENTRY(collector_thread, args)，采集数据&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监听线程：listener.c: ZBX_THREAD_ENTRY(listener_thread, args)，监听端口（根据加密格式）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;发送线程：active.c:ZBX_THREAD_ENTRY(active_checks_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.发送报文函数: active.c:send_buffer，消息体为消息头+json格式的消息体，根据加密配置，分为不加密，cert加密和psk加密。Json的编码可以在这个函数里看。
2.加密可以使用openssl的库，主要实现在tls.c:zbx_tls_connect函数中。
3.消息头的编码：comms.c:zbx_tcp_send_ext，包括” ZBXD”+1字节flag+32位json消息长度+32位0x00，在发送json体的时候，使用了zlib的compress函数进行压缩，对端接收的时候使用uncompress进行了解压缩。
4.1字节flag有以下取值：

        {
        ZBX_TCP_PROTOCOL(0x01)
        ZBX_TCP_PROTOCOL |ZBX_TCP_COMPRESS (0x03)
        0x00
        }
        当flag&amp;amp; ZBX_TCP_COMPRESS!=0时，发送报文需要对消息体进行compress压缩，接收报文需要对消息体进行uncompress解压缩
        #define ZBX_TCP_PROTOCOL        0x01
        #define ZBX_TCP_COMPRESS        0x02
        当flag==0时，报文没有消息头，只有json消息体

5.消息长度

        发送报文时，如果加密，消息体最长16K
        #define ZBX_TLS_MAX_REC_LEN 16384
        如果不加密，没有限制，写json串时动态申请内存
        接收报文时，最大长度128M，根据接收的消息长度循环动态申请内存
        #define ZBX_MAX_RECV_DATA_SIZE  (128 * ZBX_MEBIBYTE)

6.json编码中request的类型

        #define ZBX_PROTO_VALUE_GET_ACTIVE_CHECKS   &amp;quot;active checks&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_CONFIG        &amp;quot;proxy config&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_HEARTBEAT     &amp;quot;proxy heartbeat&amp;quot;
        #define ZBX_PROTO_VALUE_SENDER_DATA     &amp;quot;sender data&amp;quot;
        #define ZBX_PROTO_VALUE_AGENT_DATA      &amp;quot;agent data&amp;quot;
        #define ZBX_PROTO_VALUE_COMMAND         &amp;quot;command&amp;quot;
        #define ZBX_PROTO_VALUE_JAVA_GATEWAY_INTERNAL   &amp;quot;java gateway internal&amp;quot;
        #define ZBX_PROTO_VALUE_JAVA_GATEWAY_JMX    &amp;quot;java gateway jmx&amp;quot;
        #define ZBX_PROTO_VALUE_GET_QUEUE       &amp;quot;queue.get&amp;quot;
        #define ZBX_PROTO_VALUE_GET_STATUS      &amp;quot;status.get&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_DATA      &amp;quot;proxy data&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_TASKS     &amp;quot;proxy tasks&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;proxy&#34;&gt;Proxy&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;入口函数：zabbix_proxy.c:MAIN_ZABBIX_ENTRY&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置同步线程：proxyconfig.c: ZBX_THREAD_ENTRY(proxyconfig_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.主要处理函数：proxyconfig.c: process_configuration_sync
2.发送request为#define ZBX_PROTO_VALUE_PROXY_CONFIG        &amp;quot;proxy config&amp;quot;的配置同步请求消息，消息头和消息体的格式和agent消息格式一样，见agent段落的第3节
3.接收对端的配置同步响应消息，并解析消息体中的json段
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;心跳线程：heartbeat.c:ZBX_THREAD_ENTRY(heart_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;发送request为#define ZBX_PROTO_VALUE_PROXY_HEARTBEAT       &amp;quot;proxy heartbeat&amp;quot;的心跳消息
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;发送线程：datasender.c: ZBX_THREAD_ENTRY(datasender_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.主要处理函数：datasender.c: proxy_data_sender
2.发送request为#define ZBX_PROTO_VALUE_PROXY_DATA      &amp;quot;proxy data&amp;quot;的消息，消息头和消息体的格式和agent消息格式一样，见agent段落的第3节
3.发送的消息体包括下面4个类型的数据，数据源主要从db中获取
    #define ZBX_DATASENDER_AVAILABILITY     0x0001
    #define ZBX_DATASENDER_HISTORY          0x0002
    #define ZBX_DATASENDER_DISCOVERY        0x0004
    #define ZBX_DATASENDER_AUTOREGISTRATION     0x0008
4.从数据库中获取remotetasks，zbx_tm_get_remote_tasks，根据获取的task组织json消息体zbx_tm_json_serialize_tasks
5.接收对端的响应消息，解析消息体中的json段，并更新db中的task数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;poller线程：poller.c: ZBX_THREAD_ENTRY(poller_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.poller.c: get_values，从队列中获取数据项串并解析substitute_simple_macros，根据接口类型(snmp,java等)获取数值get_values_snmp，get_values_java
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;trapper线程：trapper.c:ZBX_THREAD_ENTRY(trapper_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.解析各类响应消息并对应处理：trapper.c:process_trap
2.消息体格式分为json格式，ZBX_GET_ACTIVE_CHECKS开头格式，xml格式，host:key:value格式
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pinger线程：pinger.c:ZBX_THREAD_ENTRY(pinger_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.从snmp或者java接口中获取数据
2.Icmp.c:process_ping，写数据到zbx_get_thread_id()i.pinger文件中
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;housekeeper_thread线程：housekeeper.c:ZBX_THREAD_ENTRY(pinger_thread, args)
    1.连接数据库删除历史数据&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;discoverer线程：httppoller.c:ZBX_THREAD_ENTRY(httppoller_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.数据库操作，获取新主机
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dbsyncer线程：dbsyncer.c:ZBX_THREAD_ENTRY(dbsyncer_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.同步数据库和内存
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;snmptrapper线程：snmptrapper.c: ZBX_THREAD_ENTRY(snmptrapper_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.读取snmptrapper文件中的数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;selfmon线程：selfmon.c: ZBX_THREAD_ENTRY(selfmon_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.收集selfmon统计数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vmware线程：selfmon.c: ZBX_THREAD_ENTRY(vmware_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.收集vmware统计数据，使用soap协议
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;sever-proxy的交互&#34;&gt;Sever: proxy的交互&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;入口函数Proxypoll.c:ZBX_THREAD_ENTRY(proxypoller_thread, args)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;主要处理函数process_proxy，发送报文send_data_to_proxy，接收报文recv_data_from_proxy，回proxy响应zbx_send_proxy_data_response，报文格式仍然为json格式，同agent的第3部分&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>UML</title>
          <link>https://kingjcy.github.io/post/architecture/map/uml/</link>
          <pubDate>Wed, 08 Nov 2017 11:40:49 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/map/uml/</guid>
          <description>&lt;p&gt;UML（Unified Modeling Language）是一种统一建模语言，为面向对象开发系统的产品进行说明、可视化、和编制文档的一种标准语言。下面将对UML的九种图的基本概念进行介绍以及各个图的使用场景。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念　　&lt;/h1&gt;

&lt;p&gt;如下图所示，UML图分为用例视图、设计视图、进程视图、实现视图和拓扑视图，又可以静动分为静态视图和动态视图。&lt;/p&gt;

&lt;p&gt;静态图分为：用例图，类图，对象图，包图，构件图，部署图。&lt;/p&gt;

&lt;p&gt;动态图分为：状态图，活动图，协作图，序列图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;用例图-usecase-diagrams&#34;&gt;用例图（UseCase Diagrams）&lt;/h2&gt;

&lt;p&gt;用例图主要回答了两个问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、是谁用软件。
2、软件的功能。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从用户的角度描述了系统的功能，并指出各个功能的执行者，强调用户的使用者，系统为执行者完成哪些功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;类图-class-diagrams&#34;&gt;类图（Class Diagrams）&lt;/h2&gt;

&lt;p&gt;用户根据用例图抽象成类，描述类的内部结构和类与类之间的关系，是一种静态结构图。 在UML类图中，常见的有以下几种关系: 泛化（Generalization）, 实现（Realization），关联（Association)，聚合（Aggregation），组合(Composition)，依赖(Dependency)。&lt;/p&gt;

&lt;p&gt;各种关系的强弱顺序： 泛化 = 实现 &amp;gt; 组合 &amp;gt; 聚合 &amp;gt; 关联 &amp;gt; 依赖&lt;/p&gt;

&lt;p&gt;1.泛化&lt;/p&gt;

&lt;p&gt;泛化关系：是一种继承关系，表示一般与特殊的关系，它指定了子类如何继承父类的所有特征和行为。例如：老虎是动物的一种，即有老虎的特性也有动物的共性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.实现&lt;/p&gt;

&lt;p&gt;实现关系：是一种类与接口的关系，表示类是接口所有特征和行为的实现。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3.关联&lt;/p&gt;

&lt;p&gt;关联关系：是一种拥有的关系，它使一个类知道另一个类的属性和方法；如：老师与学生，丈夫与妻子关联可以是双向的，也可以是单向的。双向的关联可以有两个箭头或者没有箭头，单向的关联有一个箭头。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.共享聚合　&lt;/p&gt;

&lt;p&gt;聚合关系：是整体与部分的关系，且部分可以离开整体而单独存在。如车和轮胎是整体和部分的关系，轮胎离开车仍然可以存在。&lt;/p&gt;

&lt;p&gt;聚合关系是关联关系的一种，是强的关联关系；关联和聚合在语法上无法区分，必须考察具体的逻辑关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;5.组合集合&lt;/p&gt;

&lt;p&gt;组合关系：是整体与部分的关系，但部分不能离开整体而单独存在。如公司和部门是整体和部分的关系，没有公司就不存在部门。&lt;/p&gt;

&lt;p&gt;组合关系是关联关系的一种，是比聚合关系还要强的关系，它要求普通的聚合关系中代表整体的对象负责代表部分的对象的生命周期。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml6.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;6.依赖　　&lt;/p&gt;

&lt;p&gt;依赖关系：是一种使用的关系，即一个类的实现需要另一个类的协助，所以要尽量不使用双向的互相依赖.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;7 各种类图关系&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml8.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;对象图-object-diagrams&#34;&gt;对象图（Object Diagrams）&lt;/h2&gt;

&lt;p&gt;描述的是参与交互的各个对象在交互过程中某一时刻的状态。对象图可以被看作是类图在某一时刻的实例。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml9.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;状态图-statechart-diagrams&#34;&gt;状态图（Statechart Diagrams）&lt;/h2&gt;

&lt;p&gt;一种由状态、变迁、事件和活动组成的状态机，用来描述类的对象所有可能的状态以及时间发生时状态的转移条件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml10.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;活动图-activity-diagrams&#34;&gt;活动图（Activity Diagrams）：&lt;/h2&gt;

&lt;p&gt;状态图的一种特殊情况，这些状态大都处于活动状态。本质是一种流程图，它描述了活动到活动的控制流。　　　　&lt;/p&gt;

&lt;p&gt;交互图强调的是对象到对象的控制流，而活动图则强调的是从活动到活动的控制流。&lt;/p&gt;

&lt;p&gt;活动图是一种表述过程基理、业务过程以及工作流的技术。它可以用来对业务过程、工作流建模，也可以对用例实现甚至是程序实现来建模。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml11.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1.带泳道的活动图&lt;/p&gt;

&lt;p&gt;泳道表明每个活动是由哪些人或哪些部门负责完成。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml12.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.带对象流的活动图&lt;/p&gt;

&lt;p&gt;用活动图描述某个对象时，可以把涉及到的对象放置在活动图中，并用一个依赖将其连接到进行创建、修改和撤销的动作状态或者活动状态上，对象的这种使用方法就构成了对象流。对象流用带有箭头的虚线表示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml13.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;序列图-时序图-sequence-diagrams&#34;&gt;序列图-时序图（Sequence Diagrams）&lt;/h2&gt;

&lt;p&gt;交互图的一种，描述了对象之间消息发送的先后顺序，强调时间顺序。&lt;/p&gt;

&lt;p&gt;序列图的主要用途是把用例表达的需求，转化为进一步、更加正式层次的精细表达。用例常常被细化为一个或者更多的序列图。同时序列图更有效地描述如何分配各个类的职责以及各类具有相应职责的原因。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml14.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;消息用从一个对象的生命线到另一个对象生命线的箭头表示。箭头以时间顺序在图中从上到下排列。&lt;/p&gt;

&lt;p&gt;序列图中涉及的元素：&lt;/p&gt;

&lt;p&gt;1.角色&lt;/p&gt;

&lt;p&gt;系统角色，可以是人、及其甚至其他的系统或者子系统&lt;/p&gt;

&lt;p&gt;2.对象&lt;/p&gt;

&lt;p&gt;对象包括三种命名方式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  第一种方式包括对象名和类名；

  第二中方式只显示类名不显示对象名，即表示他是一个匿名对象；

  第三种方式只显示对象名不显示类。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.生命线&lt;/p&gt;

&lt;p&gt;生命线在顺序图中表示为从对象图标向下延伸的一条虚线，表示对象存在的时间。&lt;/p&gt;

&lt;p&gt;生命线名称可带下划线。当使用下划线时，意味着序列图中的生命线代表一个类的特定实例。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml15.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.控制焦点&lt;/p&gt;

&lt;p&gt;控制焦点是顺序图中表示时间段的符号，在这个时间段内对象将执行相应的操作。用小矩形表示&lt;/p&gt;

&lt;p&gt;5.同步消息&lt;/p&gt;

&lt;p&gt;同步等待消息&lt;/p&gt;

&lt;p&gt;消息的发送者把控制传递给消息的接收者，然后停止活动，等待消息的接收者放弃或者返回控制。用来表示同步的意义。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml16.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;6.异步消息&lt;/p&gt;

&lt;p&gt;异步发送消息，不需等待&lt;/p&gt;

&lt;p&gt;消息发送者通过消息把信号传递给消息的接收者，然后继续自己的活动，不等待接受者返回消息或者控制。异步消息的接收者和发送者是并发工作的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml17.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;7.注释&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml18.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;8.约束&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml19.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;9.组合　　&lt;/p&gt;

&lt;p&gt;组合片段用来解决交互执行的条件及方式。它允许在序列图中直接表示逻辑组件，用于通过指定条件或子进程的应用区域，为任何生命线的任何部分定义特殊条件和子进程。常用的组合片段有：抉择、选项、循环、并行。&lt;/p&gt;

&lt;h2 id=&#34;协作图-collaboration-diagrams&#34;&gt;协作图（Collaboration Diagrams）&lt;/h2&gt;

&lt;p&gt;交互图的一种，描述了收发消息的对象的组织关系，强调对象之间的合作关系。时序图按照时间顺序布图，而写作图按照空间结构布图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml20.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;构件图-component-diagrams&#34;&gt;构件图（Component Diagrams）：&lt;/h2&gt;

&lt;p&gt;构件图是用来表示系统中构件与构件之间，类或接口与构件之间的关系图。其中，构建图之间的关系表现为依赖关系，定义的类或接口与类之间的关系表现为依赖关系或实现关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml21.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署图-deployment-diagrams&#34;&gt;部署图（Deployment Diagrams）：&lt;/h2&gt;

&lt;p&gt;描述了系统运行时进行处理的结点以及在结点上活动的构件的配置。强调了物理设备以及之间的连接关系。&lt;/p&gt;

&lt;p&gt;部署模型的目的：&lt;/p&gt;

&lt;p&gt;描述一个具体应用的主要部署结构，通过对各种硬件，在硬件中的软件以及各种连接协议的显示，可以很好的描述系统是如何部署的；平衡系统运行时的计算资源分布；可以通过连接描述组织的硬件网络结构或者是嵌入式系统等具有多种硬件和软件相关的系统运行模型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml22.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>云计算平台系列---- rancher</title>
          <link>https://kingjcy.github.io/post/cloud/paas/platform/racher/</link>
          <pubDate>Mon, 17 Jul 2017 20:12:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/cloud/paas/platform/racher/</guid>
          <description>&lt;p&gt;Rancher是一个开源的企业级容器管理平台,Rancher提供了在生产环境中使用的管理Docker和Kubernetes的全栈化容器部署与管理平台。&lt;/p&gt;

&lt;h1 id=&#34;rancher&#34;&gt;rancher&lt;/h1&gt;

&lt;p&gt;Rancher 1.x 最初是为了支持多种容器编排引擎而构建的，其中包括自己的容器编排引擎 Cattle。但随着 Kubernetes 在市场上的兴起，Rancher 2.x 已经完全转向了 Kubernetes。Rancher 2.x 可以部署和管理在任何地方运行的 Kubernetes 集群。&lt;/p&gt;

&lt;p&gt;Rancher 通过支持集群的身份验证和基于角色的访问控制（RBAC），使系统管理员能够从一个位置控制全部集群的访问。Rancher 可以对集群及其资源进行详细的监控和并在需要时发送告警，也可以将容器日志发送给外部日志系统，并通过应用商店与 Helm 集成。如果您具有外部 CI/CD 流水线系统，则可以将其与 Rancher 对接，如果没有，Rancher 也提供了简单易用的流水线来帮助您自动部署和升级工作负载。除此之外，Rancher 还有很多开箱即用的功能来帮助您更好的管理集群和业务应用，例如多集群应用，全局 DNS，服务网格，安全扫描，集群模版和基于 OPA 的策略管理等功能。&lt;/p&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;h3 id=&#34;单节点安装&#34;&gt;单节点安装&lt;/h3&gt;

&lt;p&gt;单节点安装只适用于测试和 demo 环境，而且单节点安装和高可用集群安装之间不能进行数据迁移，所以我们推荐从一开始就使用高可用集群（k8s）安装的方式安装 Rancher。&lt;/p&gt;

&lt;p&gt;docker安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接使用镜像rancher来运行rancher server服务&lt;/p&gt;

&lt;p&gt;我们看一下容器中的应用进程的情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                      NAMES
9a1278c071cf        rancher/rancher     &amp;quot;entrypoint.sh&amp;quot;     2 days ago          Up 25 hours         0.0.0.0:80-&amp;gt;80/tcp, 0.0.0.0:443-&amp;gt;443/tcp   thirsty_elbakyan
MacBook-Pro:~ chunyinjiang$ docker exec -ti 9a1278c071cf -- sh
OCI runtime exec failed: exec failed: container_linux.go:348: starting container process caused &amp;quot;exec: \&amp;quot;--\&amp;quot;: executable file not found in $PATH&amp;quot;: unknown
$ docker exec -ti 9a1278c071cf /bin/bash
root@9a1278c071cf:/var/lib/rancher# ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 Aug06 ?        00:00:03 tini -- rancher --http-listen-port=80 --https-listen-port=443 --audit-log-path=/var/log/auditlog/rancher-api-audit.log --audit-level=0 --audit-log-maxage=10 --audit-log-maxbackup=
root         7     1  0 Aug06 ?        00:05:59 rancher --http-listen-port=80 --https-listen-port=443 --audit-log-path=/var/log/auditlog/rancher-api-audit.log --audit-level=0 --audit-log-maxage=10 --audit-log-maxbackup=10 --aud
root        16     7  4 Aug06 ?        00:38:52 etcd --data-dir=management-state/etcd
root        28     7 38 Aug06 ?        05:27:37 k3s server --no-deploy=traefik --no-deploy=coredns --no-deploy=servicelb --no-deploy=metrics-server --no-deploy=local-storage --disable-agent --datastore-endpoint=http://localhost
root        44     7  0 Aug06 ?        00:00:06 channelserver --config-key=k3s --url https://releases.rancher.com/kontainer-driver-metadata/release-v2.4/data.json --url=/var/lib/rancher-data/driver-metadata/data.json --refresh-
root        81     7  0 Aug06 ?        00:00:03 telemetry client --url https://localhost:443/v3 --token-key telemetry:mcg9bn8r7nwh6gxb4gmwqzwn5xvwb2s2b7m5552sjrv6btkmkxtrnh
root       370     0  1 00:47 pts/0    00:00:00 /bin/bash
root       386   370  0 00:47 pts/0    00:00:00 ps -ef
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到在新版中rancher-server已经直接是rancher了，这是一个小的变化。&lt;/p&gt;

&lt;p&gt;1、登录 Rancher 界面并配置初始设置&lt;/p&gt;

&lt;p&gt;您需要先登录 Rancher，然后再开始使用 Rancher。登录以后，您需要完成一些一次性的配置。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;打开浏览器，输入主机的 IP 地址：https://&lt;SERVER_IP&gt;，请使用真实的主机 IP 地址替换 &lt;SERVER_IP&gt; 。&lt;/li&gt;
&lt;li&gt;首次登录时，请按照页面提示设置登录密码。&lt;/li&gt;
&lt;li&gt;设置 Rancher Server URL。URL 既可以是一个 IP 地址，也可以是一个主机名称。请确保您在集群内添加的每个节点都可以连接到这个 URL。如果您使用的是主机名称，请保证主机名称可以被节点的 DNS 服务器成功解析。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、创建业务集群&lt;/p&gt;

&lt;p&gt;完成安装和登录 Rancher 的步骤之后，在 Rancher 中创建第一个 Kubernetes 集群。您可以使用自定义集群选项，使用的任意 Linux 主机（云主机、虚拟机或裸金属服务器）创建集群。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;访问集群页面，单击添加集群。&lt;/li&gt;
&lt;li&gt;选择自定义选项。&lt;/li&gt;
&lt;li&gt;输入集群名称。&lt;/li&gt;
&lt;li&gt;跳过集群角色和集群选项。&lt;/li&gt;
&lt;li&gt;单击下一步。&lt;/li&gt;
&lt;li&gt;勾选主机选项 - 角色选择中的所有角色： Etcd、 Control 和 Worker。可选： Rancher 会自动探查用于 Rancher 通信和集群通信的 IP 地址。您可以通过主机选项 &amp;gt; 显示高级选项中的公网地址和内网地址指定 IP 地址。&lt;/li&gt;
&lt;li&gt;跳过主机标签参数，因为对快速入门来说，这部分的参数不太重要。&lt;/li&gt;
&lt;li&gt;复制代码框中的命令。登录您的 Linux 主机，打开命令行工具，粘贴命令，单击回车键运命令。&lt;/li&gt;
&lt;li&gt;运行完成后，回到 Rancher 界面，单击完成。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;到这边就在 Rancher 中创建了一个 Kubernetes 集群。&lt;/p&gt;

&lt;p&gt;3、部署工作负载&lt;/p&gt;

&lt;p&gt;k8s集群部署好，就可以创建工作负载（workload）。工作负载（workload）即 Kubernetes 对一组 Pod 的抽象模型，用于描述业务的运行载体，包括 Deployment、Statefulset、Daemonset、Job、CronJob 等多种类型。&lt;/p&gt;

&lt;p&gt;我们在 Rancher Server 中部署带有 Ingress 的工作负载是一个“Hello-World”应用。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;访问集群页面，选择您刚刚创建的集群，进入集群页面。&lt;/li&gt;
&lt;li&gt;从集群页面的主菜单中选择项目/命名空间。&lt;/li&gt;
&lt;li&gt;打开 项目：Default。&lt;/li&gt;
&lt;li&gt;单击资源 &amp;gt; 工作负载。如果您使用的是 v2.3.0 之前的版本，请单击 工作负载 &amp;gt; 工作负载。&lt;/li&gt;
&lt;li&gt;单击部署。&lt;/li&gt;
&lt;li&gt;输入工作负载的名称。&lt;/li&gt;
&lt;li&gt;在Docker 镜像一栏，输入rancher/hello-world，请注意区分大小写字母。&lt;/li&gt;
&lt;li&gt;余下的选项保持默认配置即可。&lt;/li&gt;
&lt;li&gt;单击运行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后就开始部署对应的工作负载。这个过程可能需要几分钟完成。当工作负载部署完成后，它的状态将变为Active，可以从项目的工作负载页面查看工作负载当前的状态。&lt;/p&gt;

&lt;p&gt;4、暴露服务&lt;/p&gt;

&lt;p&gt;上述步骤帮助您完成了工作负载的部署，现在需要将服务暴露出来，让其他服务可以通过网络连接和调用这个工作负载。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;访问集群页面，选择您刚刚创建的集群，进入集群页面。&lt;/li&gt;
&lt;li&gt;从集群页面的主菜单中选择项目/命名空间。&lt;/li&gt;
&lt;li&gt;打开 项目 &amp;gt; Default。&lt;/li&gt;
&lt;li&gt;单击资源 &amp;gt; 工作负载 &amp;gt; 负载均衡。如果您使用的是 v2.3.0 之前的版本，请单击 工作负载 &amp;gt; 负载均衡。&lt;/li&gt;
&lt;li&gt;单击添加 Ingress&lt;/li&gt;
&lt;li&gt;输入 Ingress 负载均衡的名称，如 “hello”。&lt;/li&gt;
&lt;li&gt;在目标一栏，从下拉菜单选择您服务的名称。&lt;/li&gt;
&lt;li&gt;在端口一栏输入 80。&lt;/li&gt;
&lt;li&gt;余下的选项保持默认配置即可，单击保存。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个时候这个工作负载分配到了一个xip.io地址，已经暴露出去了。可能需要 1~2 分钟完成服务关联。&lt;/p&gt;

&lt;h3 id=&#34;高可用集群安装&#34;&gt;高可用集群安装&lt;/h3&gt;

&lt;p&gt;分开部署 Rancher 与下游集群&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/rancher/rancher2.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以单点，也可以使用负载均衡来做rancher的高可用，所以我们建议将 Rancher Server 安装在高可用的 Kubernetes 集群上，主要是因为它可以保护 Rancher Server 的数据。在高可用安装中，负载均衡器充当客户端的单点入口，并在集群中的多台服务器之间分配网络流量，这有助于防止任何一台服务器成为单点故障。我们不建议在单个 Docker 容器中安装 Rancher，因为如果该节点发生故障，则其他节点上将没有可用的集群数据副本，并且您可能会丢失 Rancher Server 上的数据。&lt;/p&gt;

&lt;p&gt;我们建议使用Helm (Kubernetes 包管理器)在专用的 Kubernetes 集群上安装 Rancher。因为通过在多个节点上运行 Rancher 可以提高可用性，这个安装方式叫做“高可用 Kubernetes 安装”。&lt;/p&gt;

&lt;p&gt;高可用的流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rancher 的 DNS 应该解析为 4 层负载均衡器&lt;/li&gt;
&lt;li&gt;负载均衡器应将端口 TCP/80 和 TCP/443 流量转发到 Kubernetes 集群中的所有 3 个节点。&lt;/li&gt;
&lt;li&gt;Ingress 控制器会将 HTTP 重定向到 HTTPS，并在端口 TCP/443 上终止 SSL/TLS。&lt;/li&gt;
&lt;li&gt;Ingress 控制器会将流量转发到 Rancher deployment 中 Pod 上的端口 TCP/80。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在上面我们多次提到专用的k8s集群，其实就是rancher自身推出的也是经过CNCF认证的轻量级的k8s集群：k3s。这个可以说都是k8s集群，但是现在还是很少用人使用部署业务和k8s还是有差距的，目前也只是用于安装rancher，用于高可用的实现。&lt;/p&gt;

&lt;h4 id=&#34;k3s集群&#34;&gt;k3s集群&lt;/h4&gt;

&lt;p&gt;K3s 也是 Rancher 发布的经过完全认证的 Kubernetes 发行版，但比 RKE 更新。我们建议在 K3s 上安装 Rancher，因为 K3s 易于使用且更轻量，全部组件都打包在了一个二进制文件里。并且这个二进制文件小于 100 MB。注意：如果在 RKE 集群上安装了 Rancher 之后，目前没有办法将这个高可用迁移到 K3s 集群上。&lt;/p&gt;

&lt;p&gt;1、部署要求&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2 个 Linux 节点，通常是虚拟机，您可以自行选择的基础设施提供商，例如 Amazon EC2，阿里云，腾讯云或者 vShpere 等。&lt;/li&gt;
&lt;li&gt;1 个外置数据库，用于存储集群数据。我们建议使用 MySQL。&lt;/li&gt;
&lt;li&gt;1 个负载均衡器，用于将流量转发到这两个节点。&lt;/li&gt;
&lt;li&gt;一条 DNS 记录，用于将 URL 指向负载均衡器。这将成为 Rancher Server 的 URL，下游集群需要可以访问到这个地址。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、安装&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;连接到您准备运行 Rancher Server 的 Linux 节点。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在 Linux 节点上，运行以下命令以启动 K3s Server 并将其连接到外部数据库：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -sfL https://get.k3s.io | sh -s - server \
  --datastore-endpoint=&amp;quot;mysql://username:password@tcp(hostname:3306)/database-name&amp;quot;

//国内用户，可以使用以下方法加速安装：
curl -sfL https://docs.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - server \
--datastore-endpoint=&amp;quot;mysql://username:password@tcp(hostname:3306)/database-name&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;确认 K3s 是否创建成功，在任一 K3s Server 节点上运行以下命令，如下表示k3s集群已经成功&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo k3s kubectl get nodes
NAME               STATUS   ROLES    AGE    VERSION
ip-172-31-60-194   Ready    master   44m    v1.17.2+k3s1
ip-172-31-63-88    Ready    master   6m8s   v1.17.2+k3s1

sudo k3s kubectl get pods --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;保存并使用 kubeconfig 文件，在每个 Rancher Server 节点上安装 K3s 时，会在节点上/etc/rancher/k3s/k3s.yaml位置创建一个kubeconfig文件。该文件包含用于完全访问集群的凭据，您应该将此文件保存在安全的位置，要使用此kubeconfig文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;安装 Kubernetes 命令行工具kubectl。&lt;/li&gt;
&lt;li&gt;将文件/etc/rancher/k3s/k3s.yaml复制并保存到本地计算机上的~/.kube/config文件中。&lt;/li&gt;
&lt;li&gt;在这个 kubeconfig 文件中，server参数为 localhost。您需要手动更改这个地址为负载均衡器的 DNS，并且指定端口 6443。（Kubernetes API Server 的端口为 6443，Rancher Server 的端口为 80 和 443。）以下是一个示例k3s.yaml：&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;现在可以使用kubectl来管理您的 K3s 集群。如果您有多个 kubeconfig 文件，可以在使用kubectl时通过传递文件路径来指定要使用的 kubeconfig 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --kubeconfig ~/.kube/config/k3s.yaml get pods --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;已确认可以使用kubectl访问集群，并且 K3s 集群正在正确运行。现在，可以在集群上安装 Rancher Server 了。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;rke集群&#34;&gt;RKE集群&lt;/h4&gt;

&lt;p&gt;除了上面说的k3s，我们还可以将rancher运行在真正的k8s上实现高可用，前面我们也说过rancher安装k8s集群的流程，这边就使用到了RKE。RKE是一个用Golang编写的Kubernetes安装程序，rancher就是使用这个工具完成了k8s集群的部署和管理的，一般我们使用RKE部署的k8s集群就叫RKE集群。&lt;/p&gt;

&lt;p&gt;1、部署要求&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3 个 Linux 节点，通常是虚拟机，您可以自行选择的基础设施提供商，例如 Amazon EC2，阿里云，腾讯云或者 vShpere。&lt;/li&gt;
&lt;li&gt;1 个负载均衡器，用于将流量转发到这三个节点。&lt;/li&gt;
&lt;li&gt;一条 DNS 记录，用于将 URL 指向负载均衡器。这将成为 Rancher Server 的 URL，下游集群需要可以访问到这个地址。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、部署&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;那种REK，国内用户，可以导航到 &lt;a href=&#34;http://mirror.cnrancher.com&#34;&gt;http://mirror.cnrancher.com&lt;/a&gt; 下载所需资源&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建 RKE 配置文件rancher-cluster.yml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nodes:
  - address: 165.227.114.63
    internal_address: 172.16.22.12
    user: ubuntu
    role: [controlplane, worker, etcd]
  - address: 165.227.116.167
    internal_address: 172.16.32.37
    user: ubuntu
    role: [controlplane, worker, etcd]
  - address: 165.227.127.226
    internal_address: 172.16.42.73
    user: ubuntu
    role: [controlplane, worker, etcd]

services:
  etcd:
    snapshot: true
    creation: 6h
    retention: 24h

# 当使用外部 TLS 终止，并且使用 ingress-nginx v0.22或以上版本时，必须。
ingress:
  provider: nginx
  options:
    use-forwarded-headers: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;address: 公用 DNS 或 IP 地址&lt;/li&gt;
&lt;li&gt;user: 可以运行 docker 命令的用户&lt;/li&gt;
&lt;li&gt;role: 分配给节点的 Kubernetes 角色列表&lt;/li&gt;
&lt;li&gt;internal_address: 内部集群流量的专用 DNS 或 IP 地址&lt;/li&gt;
&lt;li&gt;ssh_key_path: 用于对节点进行身份验证的 SSH 私钥的路径（默认为~/.ssh/id_rsa）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;运行 RKE&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rke up --config ./rancher-cluster.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完成后，它应该以这样一行结束： Finished building Kubernetes cluster successfully.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;测试集群&lt;/p&gt;

&lt;p&gt;当您运行 rke up 时，RKE 应该已经创建了一个名为kube_config_rancher-cluster.yml的kubeconfig文件。该文件具有kubectl和helm的凭据。如果您已经安装了kubectl，您需要将kubeconfig文件放置在kubectl可以访问的位置。kubeconfig文件包含使用kubectl访问集群所必需的凭证。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes

NAME                          STATUS    ROLES                      AGE       VERSION
165.227.114.63                Ready     controlplane,etcd,worker   11m       v1.13.5
165.227.116.167               Ready     controlplane,etcd,worker   11m       v1.13.5
165.227.127.226               Ready     controlplane,etcd,worker   11m       v1.13.5
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;保存文件&lt;/p&gt;

&lt;p&gt;将以下文件的副本保存在安全的位置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rancher-cluster.yml: RKE 集群配置文件。
kube_config_rancher-cluster.yml: 集群的Kubeconfig 文件，此文件包含用于访问集群的凭据。
rancher-cluster.rkestate: Kubernetes 集群状态文件，此文件包含用于完全访问集群的凭据。
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;安装rancher&#34;&gt;安装rancher&lt;/h4&gt;

&lt;p&gt;1、添加 Helm Chart 仓库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm repo add rancher-&amp;lt;CHART_REPO&amp;gt; https://releases.rancher.com/server-charts/&amp;lt;CHART_REPO&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、为 Rancher 创建 Namespace&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create namespace cattle-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、设置ssl&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 安装 CustomResourceDefinition 资源

kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.0/cert-manager.crds.yaml

# **重要：**
# 如果您正在运行 Kubernetes v1.15 或更低版本，
# 则需要在上方的 kubectl apply 命令中添加`--validate=false`标志，
# 否则您将在 cert-manager 的 CustomResourceDefinition 资源中收到与
# x-kubernetes-preserve-unknown-fields 字段有关的验证错误。
# 这是一个良性错误，是由于 kubectl 执行资源验证的方式造成的。

# 为 cert-manager 创建命名空间

kubectl create namespace cert-manager

# 添加 Jetstack Helm 仓库

helm repo add jetstack https://charts.jetstack.io

# 更新本地 Helm chart 仓库缓存

helm repo update

# 安装 cert-manager Helm chart

helm install \
 cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --version v0.15.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、安装rancher&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install rancher rancher-&amp;lt;CHART_REPO&amp;gt;/rancher \
 --namespace cattle-system \
 --set hostname=rancher.my.org
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;Rancher Server 由认证代理（Authentication Proxy）、Rancher API Server、集群控制器（Cluster Controller）、etcd 节点和集群 Agent（Cluster Agent） 组成。除了集群 Agent 以外，其他组件都部署在 Rancher Server 中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/rancher/rancher.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图中描述的是用户通过 Rancher Server 管控 Rancher 部署的 Kubernetes 集群（RKE 集群）和托管的 Kubernetes 集群的（EKS）集群的流程。以用户下发指令为例，指令的流动路径如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先，用户通过 Rancher UI（即 Rancher 控制台） Rancher 命令行工具（Rancher CLI）输入指令；直接调用 Rancher API 接口也可以达到相同的效果。&lt;/li&gt;
&lt;li&gt;用户通过 Rancher 的代理认证后，指令会进一步下发到 Rancher Server 。&lt;/li&gt;
&lt;li&gt;与此同时，Rancher Server 也会执行容灾备份，将数据备份到 etcd 节点。&lt;/li&gt;
&lt;li&gt;然后 Rancher Server 把指令传递给集群控制器。集群控制器把指令传递到下游集群的 Agent，最终通过 Agent 把指令下发到指定的集群中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Agent和k8s的交互流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/cloud/rancher/rancher1.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;p&gt;Rancher 支持的集群种类繁多：云供应商托管的 Kubernetes 集群、RKE 集群、基础设施提供商创建节点并创建 Kubernetes 集群、在自定义节点上创建 Kubernetes 集群、导入现有 Kubernetes 集群和导入 K3s 集群，都是 Rancher 支持的创建集群的方式。&lt;/p&gt;

&lt;h2 id=&#34;创建正常的k8s集群&#34;&gt;创建正常的k8s集群&lt;/h2&gt;

&lt;p&gt;创建k8s集群也就是rancher通过RKE来部署一个自定义的K8s集群&lt;/p&gt;

&lt;p&gt;1、在集群页面中，单击添加集群。&lt;/p&gt;

&lt;p&gt;2、选择Custom。&lt;/p&gt;

&lt;p&gt;3、输入集群名称。&lt;/p&gt;

&lt;p&gt;4、通过成员角色来设置用户访问集群的权限。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;点击添加成员将需要访问这个集群的用户添加到成员中。&lt;/li&gt;
&lt;li&gt;在角色下拉菜单中选择每个用户的权限。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5、使用集群选项设置 Kubernetes 的版本，网络插件以及是否要启用项目网络隔离。要查看更多集群选项，请单击显示高级选项。&lt;/p&gt;

&lt;p&gt;6、点击 下一步 。&lt;/p&gt;

&lt;p&gt;7、从节点角色中，选择需要的集群节点角色。&lt;/p&gt;

&lt;p&gt;8、可选：点击显示高级选项以指定注册节点时要使用的 IP 地址、重写节点的主机名或添加标签或污点到节点上。&lt;/p&gt;

&lt;p&gt;9、复制屏幕上显示的命令到剪贴板。使用 shell 工具登录到您的 Linux 主机，如 PuTTy 等。运行复制到剪贴板的命令。如果要将特定主机专用于特定节点角色，请重复步骤 7-10。根据需要多次重复这些步骤。&lt;/p&gt;

&lt;p&gt;10、当您完成在 Linux 主机上运行该命令时，点击 完成。&lt;/p&gt;

&lt;p&gt;这个时候一个集群就部署完成了，下面rancher完成下列工作&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;集群已创建并进入为 Provisioning 的状态。Rancher 正在启动您的集群。&lt;/li&gt;
&lt;li&gt;您可以在集群的状态更新为 Active 后访问它。&lt;/li&gt;
&lt;li&gt;Rancher 为活动的集群分配了两个项目，即 Default（包含命名空间 default）和 System（包含命名空间 cattle-system，ingress-nginx，kube-public 和 kube-system，如果存在）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;导入现有的k8s集群&#34;&gt;导入现有的k8s集群&lt;/h2&gt;

&lt;p&gt;1、在 集群 页, 点击 添加。&lt;/p&gt;

&lt;p&gt;2、选择 导入。&lt;/p&gt;

&lt;p&gt;3、输入 集群名称。&lt;/p&gt;

&lt;p&gt;4、通过成员角色来设置用户访问集群的权限。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- 点击添加成员将需要访问这个集群的用户添加到成员中。
- 在角色下拉菜单中选择每个用户的权限。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、单击 创建。&lt;/p&gt;

&lt;p&gt;6、这里显示了需要集群管理员特权的先决条件 (请参阅上面的先决条件)的提示，其中包括了达到该先决条件的示例命令。&lt;/p&gt;

&lt;p&gt;7、将kubectl命令复制到剪贴板，并在有着指向您要导入的集群的 kubeconfig 的节点上运行它。如果您不确定它是否正确配置，在运行 Rancher 中显示的命令之前，运行kubectl get nodes来验证一下。&lt;/p&gt;

&lt;p&gt;8、如果您正在使用自签名证书，您将收到certificate signed by unknown authority消息。要解决这个验证问题，请把 Rancher 中显示的curl开头的命令复制到剪贴板中。并在有着指向您要导入的集群的 kubeconfig 的节点上运行它。&lt;/p&gt;

&lt;p&gt;9、在节点上运行完命令后, 单击 完成。&lt;/p&gt;

&lt;p&gt;到这里就导入完成，后面还要等一会，rancher自动完如下工作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;您的集群创建成功并进入到Pending（等待中）的状态。Rancher 正在向您的集群部署资源。&lt;/li&gt;
&lt;li&gt;在集群状态变为Active（激活）状态后，您将可以开始访问您的集群。&lt;/li&gt;
&lt;li&gt;在Active的集群中，默认会有两个项目：Default项目（包括default命名空间）和System项目（包括cattle-system、ingress-nginx、kube-public 和 kube-system）。&lt;/li&gt;
&lt;li&gt;不能重新导入当前在 Rancher 设置中处于激活状态的集群.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;部署workload&#34;&gt;部署workload&lt;/h2&gt;

&lt;h2 id=&#34;基础运维&#34;&gt;基础运维&lt;/h2&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus入门</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus/</link>
          <pubDate>Thu, 29 Jun 2017 16:31:54 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus/</guid>
          <description>&lt;p&gt;Prometheus，它最早是借鉴了 Google 的 Borgmon 系统，完全是开源的，也是CNCF 下继 K8S 之后第二个项目。它们的开发人员都是原 Google 的 SRE，通过 HTTP 的方式来做数据收集，对其最深远的应该是其被设计成一个 self sustained 的系统，也就是说它是完全独立的系统，不需要外部依赖。&lt;/p&gt;

&lt;h1 id=&#34;时序数据库的发展&#34;&gt;时序数据库的发展&lt;/h1&gt;

&lt;h2 id=&#34;时序数据&#34;&gt;时序数据&lt;/h2&gt;

&lt;p&gt;时序数据的种类：常规和不规则。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;开发人员比较常见和熟悉的是常规时间序列，它只在规定的时间间隔内进行测量，如每10秒钟一次，通常会发生在传感器中，定期读取数据。常规时间序列代表了一些基本的原始事件流或分发。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不规则时间序列则对应离散事件，主要是针对API，例如股票交易。如果要以1分钟间隔计算API的平均响应时间，可以聚合各个请求以生成常规时间序列。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;关系型数据库和nosql&#34;&gt;关系型数据库和nosql&lt;/h2&gt;

&lt;p&gt;使用mysql或者分布式数据库cassandra等，数据频繁插入操作，数据量很大，查询困难，还需要不停的进行分区分表，在应用级获取的时候需要要大量的代码控制，所以需要一个时序数据库。&lt;/p&gt;

&lt;p&gt;nosql可以很好的处理的大规模数据的处理查询，但是缺乏规范的sql，现在虽然每种nosql都得到了广泛的应用，但是其实都是缓存数据库的思想，每中nosql都要有自己学习的成本，当然这个并不是使用时序数据库的理由，相反，缓存数据库在很多场景下都是得到的重用，但是针对一些特殊场景，比如以时间为主轴的数据，观察变化趋势的，优化后的时序数据库则拥有了更好的数据存储处理查询能力&lt;/p&gt;

&lt;p&gt;时间序列数据跟关系型数据库有太多不同，但是很多公司并不想放弃关系型数据库。于是就产生了一些特殊的用法，比如：用 MySQL 的 VividCortex, 用 Postgres 的 TimescaleDB；当然，还有人依赖K-V、NoSQL数据库或者列式数据库的，比如：OpenTSDB的HBase，而Druid则是一个不折不扣的列式存储系统；更多人觉得特殊的问题需要特殊的解决方法，于是很多时间序列数据库从头写起，不依赖任何现有的数据库, 比如： Graphite，InfluxDB。&lt;/p&gt;

&lt;p&gt;时序数据库基本上是基于缓存（nosql思想）的基础上处理大规模的数据，并且在一些场景，比如以时间为主轴的数据变化趋势：自动驾驶，交易，监控等行业，就需要时序数据库进行大规模的数据处理，用于跟踪历史数据。&lt;/p&gt;

&lt;p&gt;现在生活中时序的场景很多很多，所以时序数据库很受需要，已经成为发展最快的一种数据库。&lt;/p&gt;

&lt;p&gt;下面我们来全面对比一下关系数据库和时序数据库&lt;/p&gt;

&lt;p&gt;时序数据库&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;数据写入&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;时间是一个主坐标轴，数据通常按照时间顺序抵达&lt;/li&gt;
&lt;li&gt;大多数测量是在观察后的几秒或几分钟内写入的，抵达的数据几乎总是作为新条目被记录&lt;/li&gt;
&lt;li&gt;95％到99％的操作是写入，有时更高&lt;/li&gt;
&lt;li&gt;更新几乎没有&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据读取&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;随机位置的单个测量读取、删除操作几乎没有&lt;/li&gt;
&lt;li&gt;读取和删除是批量的，从某时间点开始的一段时间内&lt;/li&gt;
&lt;li&gt;时间段内读取的数据有可能非常巨大&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据存储&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据结构简单，价值随时间推移迅速降低&lt;/li&gt;
&lt;li&gt;通过压缩、移动、删除等手段降低存储成本&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而关系数据库主要应对的数据特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据写入：大多数操作都是DML操作，插入、更新、删除等；&lt;/li&gt;
&lt;li&gt;数据读取：读取逻辑一般都比较复杂；&lt;/li&gt;
&lt;li&gt;数据存储：很少压缩，一般也不设置数据生命周期管理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;针对这些特点，致使我们使用时序数据库，我们来看一下需要使用时序数据库的主要的特点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基本上都是插入，没有更新的需求。&lt;/li&gt;
&lt;li&gt;数据基本上都有时间属性，随着时间的推移不断产生新的数据。&lt;/li&gt;
&lt;li&gt;数据量大，每秒钟需要写入千万、上亿条数据&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为什么要使用时序数据库？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;因为数据量大，并且大部分都是写入的要求，并且要求性能特别高，并且有时间属性。这类数据使用时序数据库的特殊处理方式（以缓存为基础，以时间为主轴来存储数据），比较快捷高效&lt;/li&gt;
&lt;li&gt;数据重复性特别大，使用压缩来降低存储成本。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;时序数据库&#34;&gt;时序数据库&lt;/h2&gt;

&lt;h3 id=&#34;基本概念&#34;&gt;基本概念&lt;/h3&gt;

&lt;p&gt;一些基本概念(不同的时序数据库称呼略有不同)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Metric:  度量，相当于关系型数据库中的 table。&lt;/li&gt;
&lt;li&gt;Data point:  数据点，相当于关系型数据库中的 row。&lt;/li&gt;
&lt;li&gt;Timestamp：时间戳，代表数据点产生的时间。&lt;/li&gt;
&lt;li&gt;Field:  度量下的不同字段。比如位置这个度量具有经度和纬度两个 field。一般情况下存放的是随时间戳而变化的数据。&lt;/li&gt;
&lt;li&gt;Tag:  标签。一般存放的是不随时间戳变化的信息。timestamp 加上所有的 tags 可以视为 table 的 primary key。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如采集有关风的数据，度量为 Wind，每条数据都有时间戳timestamp，两个字段 field：direction(风向)、speed(风速)，两个tag：sensor(传感器编号)、city(城市)。&lt;/p&gt;

&lt;p&gt;业务方常见需求&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取最新状态，查询最近的数据(例如传感器最新的状态)&lt;/li&gt;
&lt;li&gt;展示区间统计，指定时间范围，查询统计信息，例如平均值，最大值，最小值，计数等。。。&lt;/li&gt;
&lt;li&gt;获取异常数据，根据指定条件，筛选异常数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;常见业务场景&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;监控软件系统： 虚拟机、容器、服务、应用&lt;/li&gt;
&lt;li&gt;监控物理系统： 水文监控、制造业工厂中的设备监控、国家安全相关的数据监控、通讯监控、传感器数据、血糖仪、血压变化、心率等&lt;/li&gt;
&lt;li&gt;资产跟踪应用： 汽车、卡车、物理容器、运货托盘&lt;/li&gt;
&lt;li&gt;金融交易系统： 传统证券、新兴的加密数字货币&lt;/li&gt;
&lt;li&gt;事件应用程序： 跟踪用户、客户的交互数据&lt;/li&gt;
&lt;li&gt;商业智能工具： 跟踪关键指标和业务的总体健康情况&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在互联网行业中，也有着非常多的时序数据，例如用户访问网站的行为轨迹，应用程序产生的日志数据等等。&lt;/p&gt;

&lt;h3 id=&#34;主流时序数据库&#34;&gt;主流时序数据库&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;influxdb，opentsdb，Graphite，prometheus，HiTSDB，LinDB
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;InfluxDB：很多公司都在用，包括饿了么有部分监控系统也是用的InfluxDB。其优点在于支持多维和多字段，存储也根据TSDB的特点做了优化，不过开源的部分并不支持。很多公司自己做集群化，但大多基于指标名来，这样就会有单指的热点问题。现在饿了么也是类似的做法，但热点问题很严重，大的指标已经用了最好的服务器，可查询性能还是不够理想，如果做成按Series Sharding，那成本还是有一点高；&lt;/li&gt;
&lt;li&gt;Graphite：根据指标写入及查询，计算函数很多，但很难支持多维，包括机房或多集群的查询。原来饿了么把业务层的监控指标存储在Graphite中，并工作的很好，不过多活之后基本已经很难满足一些需求了，由于其存储结构的特点，很占IO，根据目前线上的数据写放大差不多几十倍以上；&lt;/li&gt;
&lt;li&gt;OpenTSDB：基于HBase，优点在于存储层不用自己考虑，做好查询聚合就可以，也会存在HBase的热点问题等。在以前公司也用基于HBase实现的TSDB来解决OpenTSDB的一些问题， 如热点、部分查询聚合下放到HBase等，目的是优化其查询性能，但依赖HBase/HDFS还是很重；&lt;/li&gt;
&lt;li&gt;HiTSDB：阿里提供的TSDB，存储也是用HBase，在数据结构及Index上面做了很多优化，具体没有研究。&lt;/li&gt;
&lt;li&gt;LinDB：饿了么轻量级分布式时序数据库，基础组件如下

&lt;ul&gt;
&lt;li&gt;LinProxy主要做一些SQL的解析及一些中间结合的再聚合计算，如果不是跨集群，LinProxy可以不需要，对于单集群的每个节点都内嵌了一个LinProxy来提供查询服务；&lt;/li&gt;
&lt;li&gt;LinDB Client主要用于数据的写入，也有一些查询的API；&lt;/li&gt;
&lt;li&gt;LinStorage的每个节点组成一个集群，节点之间进行复制，并有副本的Leader节点提供读写服务，这点设计主要是参考Kafka的设计，可以把LinDB理解成类Kafka的数据写入复制+底层时间序列的存储层；&lt;/li&gt;
&lt;li&gt;LinMaster主要负责database、shard、replica的分配，所以LinStorage存储的调度及MetaData（目前存储Zookeeper中）的管理；由于LinStorage Node都是对等的，所以我们基于Zookeeper在集群的节点选一个成为Master，每个Node把自身的状态以心跳的方式上报到Master上，Master根据这些状态进行调度，如果Master挂了，自动再选一个Master出来，这个过程基本对整个服务是无损的，所以用户基本无感知。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;prometheus&#34;&gt;prometheus&lt;/h1&gt;

&lt;h2 id=&#34;安装编译&#34;&gt;安装编译&lt;/h2&gt;

&lt;p&gt;可以通过源码编译也可以通过下载二进制包，还可以通过docker启动，如果是源码编译很简单，clone下代码make build一下就行，会产生二进制文件prometheus，&lt;/p&gt;

&lt;p&gt;下载tar包二进制文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar xvfz prometheus-*.tar.gz
cd prometheus-*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./prometheus --config.file=prometheus.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用启动参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--storage.tsdb.path指定的路径存储文件，默认为./data
--web.listen-address=0.0.0.0:9090 指定监听的ip和端口
--config.file=/opt/prometheus-2.4.2.linux-amd64-k8s/prometheus.yml 指定启动的配置文件
--storage.tsdb.retention=10d 指定数据存储时间
--log.level=info 指定日志等级
--query.max-concurrency=2000 指定查询并发数量
--web.max-connections=4096 指定连接数
--web.read-timeout=40s 界面查询超时时间
--query.timeout=40s 指定查询超时时间
--query.lookback-delta=3600s 查询最长多少时间范围内的点
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;全部启动参数：2.4.2版本的详细说明，随着升级会有对应的变化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@promessitapp05 k8s-prometheus-2.4.2.linux-amd64-k8s]# ./prometheus -h
usage: prometheus [&amp;lt;flags&amp;gt;]

The Prometheus monitoring server

Flags:
  -h, --help                     Show context-sensitive help (also try --help-long and --help-man).
      --version                  Show application version.
      --config.file=&amp;quot;prometheus.yml&amp;quot;
                                 Prometheus configuration file path.
      --web.listen-address=&amp;quot;0.0.0.0:9090&amp;quot;
                                 Address to listen on for UI, API, and telemetry.
      --web.read-timeout=5m      Maximum duration before timing out read of the request, and closing idle connections.
      --web.max-connections=512  Maximum number of simultaneous connections.
      --web.external-url=&amp;lt;URL&amp;gt;   The URL under which Prometheus is externally reachable (for example, if Prometheus is served via a reverse proxy). Used for generating relative and
                                 absolute links back to Prometheus itself. If the URL has a path portion, it will be used to prefix all HTTP endpoints served by Prometheus. If
                                 omitted, relevant URL components will be derived automatically.
      --web.route-prefix=&amp;lt;path&amp;gt;  Prefix for the internal routes of web endpoints. Defaults to path of --web.external-url.
      --web.user-assets=&amp;lt;path&amp;gt;   Path to static asset directory, available at /user.
      --web.enable-lifecycle     Enable shutdown and reload via HTTP request.
      --web.enable-admin-api     Enable API endpoints for admin control actions.
      --web.console.templates=&amp;quot;consoles&amp;quot;
                                 Path to the console template directory, available at /consoles.
      --web.console.libraries=&amp;quot;console_libraries&amp;quot;
                                 Path to the console library directory.
      --storage.tsdb.path=&amp;quot;data/&amp;quot;
                                 Base path for metrics storage.
      --storage.tsdb.retention=15d
                                 How long to retain samples in storage.
      --storage.tsdb.no-lockfile
                                 Do not create lockfile in data directory.
      --storage.remote.flush-deadline=&amp;lt;duration&amp;gt;
                                 How long to wait flushing sample on shutdown or config reload.
      --storage.remote.read-sample-limit=5e7
                                 Maximum overall number of samples to return via the remote read interface, in a single query. 0 means no limit.
      --rules.alert.for-outage-tolerance=1h
                                 Max time to tolerate prometheus outage for restoring &#39;for&#39; state of alert.
      --rules.alert.for-grace-period=10m
                                 Minimum duration between alert and restored &#39;for&#39; state. This is maintained only for alerts with configured &#39;for&#39; time greater than grace period.
      --rules.alert.resend-delay=1m
                                 Minimum amount of time to wait before resending an alert to Alertmanager.
      --alertmanager.notification-queue-capacity=10000
                                 The capacity of the queue for pending Alertmanager notifications.
      --alertmanager.timeout=10s
                                 Timeout for sending alerts to Alertmanager.
      --query.lookback-delta=5m  The delta difference allowed for retrieving metrics during expression evaluations.就是查询当前时间前多长时间的数据中最新的一个数据，当配置较小的时候，可能采集间隔过大而获取不到数据。
      --query.timeout=2m         Maximum time a query may take before being aborted.
      --query.max-concurrency=20
                                 Maximum number of queries executed concurrently.
      --log.level=info           Only log messages with the given severity or above. One of: [debug, info, warn, error]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d--name=prometheus     --publish=9090:9090-v /etc/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml     -v /var/prometheus/storage:/prometheus     prom/prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;p&gt;1、就是上面的二进制或者docker直接启动&lt;/p&gt;

&lt;p&gt;2、k8s部署&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;直接使用这个项目中的yaml文件&lt;a href=&#34;https://github.com/giantswarm/prometheus&#34;&gt;https://github.com/giantswarm/prometheus&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Prometheus Operator部署&lt;/p&gt;

&lt;p&gt;具体可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/&#34;&gt;Prometheus Operator&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;配置文件&#34;&gt;配置文件&lt;/h2&gt;

&lt;p&gt;通常的配置文件如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# my global config全局配置
global:
  scrape_interval:     15s # By default, scrape targets every 15 seconds.采集频率
  evaluation_interval: 15s # By default, scrape targets every 15 seconds.规则计算的频率
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  # 给全局指标增加一个label
  external_labels:
      monitor: &#39;codelab-monitor&#39;

# Load and evaluate rules in this file every &#39;evaluation_interval&#39; seconds.
# 告警规则文件
rule_files:
  # - &amp;quot;first.rules&amp;quot;
  # - &amp;quot;second.rules&amp;quot;
  - &amp;quot;alert.rules&amp;quot;
  # - &amp;quot;record.rules&amp;quot;


#lertmanager configuration
# altermanager服务器的配置，所有的地址都要配置
alerting:
  alertmanagers:
  - static_configs:
    - targets: [&#39;10.242.182.161:9093&#39;,&#39;10.242.182.166:9093&#39;]


# A scrape configuration containing exactly one endpoint to scrape:
# Here it&#39;s Prometheus itself.
# 采集配置
scrape_configs:
  # The job name is added as a label `job=&amp;lt;job_name&amp;gt;` to any timeseries scraped from this config.
  # job的名字
  - job_name: &#39;windows-test&#39;

    # Override the global default and scrape targets from this job every 5 seconds.
    # 每个job可以单独设置采集频率，但是这个不能在label中设置，也就是说只能一个job一个采集频率，不能一个target一个采集频率
    scrape_interval: 1s

    # metrics_path defaults to &#39;/metrics&#39;，
    # 可以设置采集路经,默认是metrics，这个参数可以在label中设置
    metrics_path: /probe

    # Optional HTTP URL parameters.
    # params:
    #  [ &amp;lt;string&amp;gt;: [&amp;lt;string&amp;gt;, ...] ]
    # target的URL的请求参数，比如http://10.27.241.4:10260/metrics?all，就是k/v结构
    params:
        all: [&amp;quot;&amp;quot;]

    # 这边还有一个match的使用方法
    # 只采集job是node_exporter_1的数据。
    params:
      match[]:
        - &#39;{job=~&amp;quot;node_exporter_1&amp;quot;}&#39;

    # scheme defaults to &#39;http&#39;.
    # 可以设置http的方式，默认http，这个参数也可以在label中设置
    scheme： http

    # 静态target的配置，也可以使用其他的服务发现，但是都是job统一级别的
    static_configs:
      - targets: [&#39;192.168.3.1:9090&#39;,&#39;192.168.3.120:9090&#39;]
      # 可以直接设置采集数据的标签
        labels:
            appid : &#39;mycat&#39;



    # Sets the `Authorization` header on every scrape request with the
    # configured username and password.
    # password and password_file are mutually exclusive.
    # basic_auth:
    #  [ username: &amp;lt;string&amp;gt; ]
    #  [ password: &amp;lt;secret&amp;gt; ]
    #  [ password_file: &amp;lt;string&amp;gt; ]
    # 访问https的时候可以带上用户名和密码

    basic_auth:
      username: &amp;quot;admin&amp;quot;
      password: &amp;quot;Pwd123456&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面是默认的使用方式，使用的是static_configs直接静态配置ip，也可以使用一些服务发现来动态更新IP。&lt;/p&gt;

&lt;h3 id=&#34;服务发现&#34;&gt;服务发现&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;static_configs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;static_configs直接静态配置ip&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;文件服务发现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;file_sd_config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;node&#39;
file_sd_configs:
  - files:
    - /opt/promes/harbor-prometheus-2.4.2.linux-amd64/discoveries/node/discovery.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是使用了json文件的服务发现，可以把对应的target和label写入json文件，这边就可以使用一些模版生产工具（consul-template）来生成对应的json文件&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kubernetes_sd_configs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要参考&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s/#使用k8s的服务发现&#34;&gt;k8s监控方案中的prometheus in k8s的配置文件解析&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;这里我们主要关注在kubernetes下的采集目标发现的配置，Prometheus支持通过kubernetes的Rest API动态发现采集的目标Target信息，包括kubernetes下的node,service,pod,endpoints等信息。&lt;/p&gt;

&lt;p&gt;kubernets_sd_config下面role类型中的任何一个都能在发现目标上配置：&lt;/p&gt;

&lt;p&gt;节点node&lt;/p&gt;

&lt;p&gt;这个node角色发现带有地址的每一个集群节点一个目标，都指向Kublelet的HTTP端口。这个目标地址默认为Kubernetes节点对象的第一个现有地址，地址类型为NodeInernalIP, NodeExternalIP, NodeLegacyHostIP和NodeHostName。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_node_name: 节点对象的名称
__meta_kubernetes_node_label_&amp;lt;labelname&amp;gt;: 节点对象的每个标签
__meta_kubernetes_node_annotation_&amp;lt;annotationname&amp;gt;: 节点对象的每个注释
_meta_kubernetes_node_address&amp;lt;address_type&amp;gt;: 如果存在，每一个节点对象类型的第一个地址
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，对于节点的instance标签，将会被设置成从API服务中获取的节点名称。&lt;/p&gt;

&lt;p&gt;服务service&lt;/p&gt;

&lt;p&gt;对于每个服务每个服务端口，service角色发现一个目标。对于一个服务的黑盒监控是通常有用的。这个地址被设置成这个服务的Kubernetes DNS域名, 以及各自的服务端口。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_namespace: 服务对象的命名空间
__meta_kubernetes_service_name: 服务对象的名称
__meta_kubernetes_service_label_&amp;lt;labelname&amp;gt;: 服务对象的标签。
__meta_kubernetes_service_annotation_&amp;lt;annotationname&amp;gt;: 服务对象的注释
__meta_kubernetes_service_port_name: 目标服务端口的名称
__meta_kubernetes_service_port_number: 目标服务端口的数量
__meta_kubernetes_service_port_portocol: 目标服务端口的协议
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note：这里Service中同样标注了 prometheus.io/scrape: ‘true’从而确保prometheus会采集数据。&lt;/p&gt;

&lt;p&gt;pod&lt;/p&gt;

&lt;p&gt;pod角色发现所有的pods，并暴露它们的容器作为目标。对于每一个容器的声明端口，单个目标被生成。 如果一个容器没有指定端口，每个容器的无端口目标都是通过relabeling手动添加端口而创建的。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_namespace: pod对象的命名空间
__meta_kubernetes_pod_name: pod对象的名称
__meta_kubernetes_pod_ip: pod对象的IP地址
__meta_kubernetes_pod_label_&amp;lt;labelname&amp;gt;: pod对象的标签
__meta_kubernetes_pod_annotation_&amp;lt;annotationname&amp;gt;: pod对象的注释
__meta_kubernetes_pod_container_name: 目标地址的容器名称
__meta_kubernetes_pod_container_port_name: 容器端口名称
__meta_kubernetes_pod_container_port_number: 容器端口的数量
__meta_kubernetes_pod_container_port_protocol: 容器端口的协议
__meta_kubernetes_pod_ready: 设置pod ready状态为true或者false
__meta_kubernetes_pod_node_name: pod调度的node名称
__meta_kubernetes_pod_host_ip: 节点对象的主机IP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;endpoints端点&lt;/p&gt;

&lt;p&gt;endpoints角色发现来自于一个服务的列表端点目标。对于每一个终端地址，一个目标被一个port发现。如果这个终端被写入到pod中，这个节点的所有其他容器端口，未绑定到端点的端口，也会被目标发现。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_namespace: 端点对象的命名空间
__meta_kubernetes_endpoints_name: 端点对象的名称
对于直接从端点列表中获取的所有目标，下面的标签将会被附加上。
__meta_kubernetes_endpoint_ready: endpoint ready状态设置为true或者false。
__meta_kubernetes_endpoint_port_name: 端点的端口名称
__meta_kubernetes_endpoint_port_protocol: 端点的端口协议

如果端点属于一个服务，这个角色的所有标签：服务发现被附加上。
对于在pod中的所有目标，这个角色的所有表掐你：pod发现被附加上
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于Kuberntes发现，看看下面的配置选项：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# The information to access the Kubernetes API.

# The API server addresses. If left empty, Prometheus is assumed to run inside
# of the cluster and will discover API servers automatically and use the pod&#39;s
# CA certificate and bearer token file at /var/run/secrets/kubernetes.io/serviceaccount/.
[ api_server: &amp;lt;host&amp;gt; ]

# The Kubernetes role of entities that should be discovered.
role: &amp;lt;role&amp;gt;

# Optional authentication information used to authenticate to the API server.
# Note that `basic_auth`, `bearer_token` and `bearer_token_file` options are
# mutually exclusive.

# Optional HTTP basic authentication information.
basic_auth:
  [ username: &amp;lt;string&amp;gt; ]
  [ password: &amp;lt;string&amp;gt; ]

# Optional bearer token authentication information.
[ bearer_token: &amp;lt;string&amp;gt; ]

# Optional bearer token file authentication information.
[ bearer_token_file: &amp;lt;filename&amp;gt; ]

# TLS configuration.
tls_config:
  [ &amp;lt;tls_config&amp;gt; ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;role&amp;gt;&lt;/code&gt;必须是endpoints, service, pod或者node。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;consul服务发现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;consul_sd_configs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;TEST_NEW_1&#39;
    scrape_interval:     30s
    consul_sd_configs:
      - server: &#39;192.47.178.100:9996&#39;
        services: [&#39;node_exporter_1&#39;]
    relabel_configs:
    - source_labels: [&#39;__meta_consul_service&#39;]
      regex:         &#39;(.*)&#39;
      target_label:  &#39;job&#39;
      replacement:   &#39;PROMES_$1&#39;
    - source_labels: [&#39;__meta_consul_node&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;instance&#39;
      replacement:   &#39;$4&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;appId&#39;
      replacement:   &#39;$1&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;ldc&#39;
      replacement:   &#39;$2&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;env&#39;
      replacement:   &#39;$3&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;ip&#39;
      replacement:   &#39;$4&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;softType&#39;
      replacement:   &#39;$5&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;software&#39;
      replacement:   &#39;$6&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;exporter&#39;
      replacement:   &#39;$7&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;exporterVersion&#39;
      replacement:   &#39;$8&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由上面的配置可见，配置consul的服务器的地址和对应的services的名字就可以匹配到api注册需要采集对应的配置。&lt;/p&gt;

&lt;p&gt;consul服务发现中支持一下内部使用的metadata：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_consul_address: the address of the target
__meta_consul_dc: the datacenter name for the target
__meta_consul_tagged_address_&amp;lt;key&amp;gt;: each node tagged address key value of the target
__meta_consul_metadata_&amp;lt;key&amp;gt;: each node metadata key value of the target
__meta_consul_node: the node name defined for the target
__meta_consul_service_address: the service address of the target
__meta_consul_service_id: the service ID of the target
__meta_consul_service_metadata_&amp;lt;key&amp;gt;: each service metadata key value of the target
__meta_consul_service_port: the service port of the target
__meta_consul_service: the name of the service the target belongs to
__meta_consul_tags: the list of tags of the target joined by the tag separator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过注册tags的编号来替换对应内部专门使用的变量的值，来完成label的注册。&lt;/p&gt;

&lt;p&gt;这种注册和服务发现的模式，需要一直去请求consul的api，当数据量大的时候，会出现超时现象的性能瓶颈，影响采集的动态更新，小规模使用比较好，services还有自检的功能，但是大规模，直接使用k/v结构存储注册数据，作为数据来来使用，然后使用第三方模版工具(consul-template)生成json文件，来完成动态更新，etcd+confd也是类似的模式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他还有很多服务发现，没有用过，先不做说明。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;prometheus的relabeling机制&#34;&gt;Prometheus的Relabeling机制&lt;/h3&gt;

&lt;p&gt;在Prometheus所有的Target实例中，都包含一些默认的Metadata标签信息。可以通过Prometheus UI的Targets页面中查看这些实例的Metadata标签的内容：&lt;/p&gt;

&lt;p&gt;默认情况下，当Prometheus加载Target实例完成后，这些Target时候都会包含一些默认的标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__address__：当前Target实例的访问地址&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;
__scheme__：采集目标服务访问地址的HTTP Scheme，HTTP或者HTTPS
__metrics_path__：采集目标服务访问地址的访问路径
__param_&amp;lt;name&amp;gt;：采集任务目标服务的中包含的请求参数
__name__是特定的label标签，代表了metric name。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这些标签将会告诉Prometheus如何从该Target实例中获取监控数据。除了这些默认的标签以外，我们还可以为Target添加自定义的标签，也就是我们平常使用的label&lt;/p&gt;

&lt;p&gt;一般来说，Target以&lt;code&gt;__&lt;/code&gt;作为前置的标签是作为系统内部使用的，因此这些标签不会被写入到样本数据中。不过这里有一些例外，例如，我们会发现所有通过Prometheus采集的样本数据中都会包含一个名为instance的标签，该标签的内容对应到Target实例的&lt;code&gt;__address__&lt;/code&gt;。 这里实际上是发生了一次标签的重写处理。&lt;/p&gt;

&lt;p&gt;这种发生在采集样本数据之前，对Target实例的标签进行重写的机制在Prometheus被称为Relabeling。&lt;/p&gt;

&lt;p&gt;Promtheus允许用户在采集任务设置中通过relabel_configs来添加自定义的Relabeling过程。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;relabel_config&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;relabel_config的作用就是将时间序列中 label 的值做一个替换，具体的替换规则有配置决定，默认 job 的值是 job_name，&lt;code&gt;__address__&lt;/code&gt;的值为&lt;code&gt;&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;，instance的值默认就是 &lt;code&gt;__address__，__param_&amp;lt;name&amp;gt;&lt;/code&gt;的值就是请求url中&lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt;的值 &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;blackbox&#39;
metrics_path: /probe
params:
  module: [http_2xx]  # Look for a HTTP 200 response.
static_configs:
  - targets: [&amp;quot;https://test.com/api/projects&amp;quot;]
relabel_configs:
  - source_labels: [__address__]
    target_label: __param_target
  - source_labels: [__param_target]
    target_label: instance
  - target_label: __address__
    replacement: 10.243.129.101:9115  # The blackbox exporter&#39;s real hostname:port.
basic_auth:
  username: &amp;quot;admin&amp;quot;
  password: &amp;quot;Pwd123456&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这个配置的意思就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__param_target = __address__ ，&amp;lt;- https://test.com/api/projects
instance = __param_target &amp;lt;- https://test.com/api/projects
__address__ = 10.243.129.101:9115。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus最后是根据&lt;code&gt;__address__&lt;/code&gt;来作为采集的地址来拉去数据的。可以看出默认情况下，targets将地址给了&lt;code&gt;__address__&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;具体规则做一个简单的说明，其实就是relabel_action所决定的&lt;/p&gt;

&lt;p&gt;&lt;relabel_action&gt; determines the relabeling action to take:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;replace: Match regex against the concatenated source_labels. Then, set target_label to replacement, with match group references (${1}, ${2}, &amp;hellip;) in replacement substituted by their value. If regex does not match, no replacement takes place.&lt;/li&gt;
&lt;li&gt;keep: Drop targets for which regex does not match the concatenated source_labels.&lt;/li&gt;
&lt;li&gt;drop: Drop targets for which regex matches the concatenated source_labels.&lt;/li&gt;
&lt;li&gt;hashmod: Set target_label to the modulus of a hash of the concatenated source_labels.&lt;/li&gt;
&lt;li&gt;labelmap: Match regex against all label names. Then copy the values of the matching labels to label names given by replacement with match group references (${1}, ${2}, &amp;hellip;) in replacement substituted by their value.&lt;/li&gt;
&lt;li&gt;labeldrop: Match regex against all label names. Any label that matches will be removed from the set of labels.&lt;/li&gt;
&lt;li&gt;labelkeep: Match regex against all label names. Any label that does not match will be removed from the set of labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;重新贴标签的工作如下（对应每行数据）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;定义源标签列表。&lt;/li&gt;
&lt;li&gt;对于每个目标，这些标签的值与分隔符连接。&lt;/li&gt;
&lt;li&gt;正则表达式与结果字符串匹配。&lt;/li&gt;
&lt;li&gt;基于这些匹配的新值被分配给另一个标签。&lt;/li&gt;
&lt;li&gt;可以为每个刮擦配置定义多个重新标记规则。简单的将两个标签压成一个，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例看起来如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;relabel_configs:
- source_labels: [&#39;label_a&#39;, &#39;label_b&#39;]
  separator:     &#39;;&#39;
  regex:         &#39;(.*);(.*)&#39;
  replacement:   &#39;${1}-${2}&#39;
  target_label:  &#39;label_c&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这条规则用标签集转换目标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;job&amp;quot;: &amp;quot;job1&amp;quot;,
  &amp;quot;label_a&amp;quot;: &amp;quot;foo&amp;quot;,
  &amp;quot;label_b&amp;quot;: &amp;quot;bar&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;成为标签集的目标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;job&amp;quot;: &amp;quot;job1&amp;quot;,
  &amp;quot;label_a&amp;quot;: &amp;quot;foo&amp;quot;,
  &amp;quot;label_b&amp;quot;: &amp;quot;bar&amp;quot;,
  &amp;quot;label_c&amp;quot;: &amp;quot;foo-bar&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;separator&lt;/p&gt;

&lt;p&gt;意思是如果有多个&lt;code&gt;source_label([__address__,jod])&lt;/code&gt;的时候用separator去连接几个值&lt;/p&gt;

&lt;p&gt;regex&lt;/p&gt;

&lt;p&gt;意思是符合这个正则表达式的source_label会被赋值给replacement再赋值给target_label&lt;/p&gt;

&lt;p&gt;也可以在采集的时候drop掉某些label&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#如下是删除一个原来的标签
- action: labeldrop
  regex: job
- action: labeldrop
  regex: soft.*
- action: labeldrop
  regex: exporter.*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以在采集的时候不采集一类指标符合正则表达式，使用的是一个新的域标签metric_relabel_configs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metric_relabel_configs:
- source_labels: [ __name__ ]
  regex: &#39;go.*&#39;
  action: drop
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;hashmod&#34;&gt;hashmod&lt;/h3&gt;

&lt;p&gt;hashmod是基于服务发现的基础中的一种分布式集群的实现方式，多个prometheus实例来平均分配采集任务，完成prometheus的水平扩展。&lt;/p&gt;

&lt;p&gt;可以结合lb来负载均衡，也可以来指定ip去采集对应的数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: ibmmq
metrics_path: /metrics
params:
  module: [ibm-mq]
file_sd_configs:
  - files:
    - /opt/prometheus/discoveries/discovery.json
relabel_configs:
- source_labels: [__address__]
  modulus:       3    # 0 slaves
  target_label:  __tmp_hash
  action:        hashmod
- source_labels: [__tmp_hash]
  regex:         ^2$  # This is the 2nd slave
  action:        keep
- source_labels: [__address__]
  target_label: __param_target
- source_labels: [__param_target]
  target_label: instance
- target_label: __address__
  replacement: 10.47.247.214:9115
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当relabel_config设置为hashmod时，Promtheus会根据modulus的值作为系数，计算source_labels值的hash值。&lt;/p&gt;

&lt;p&gt;根据当前Target实例&lt;strong&gt;address&lt;/strong&gt;的值以4作为系数，这样每个Target实例都会包含一个新的标签tmp_hash，并且该值的范围在1~4之间。&lt;/p&gt;

&lt;p&gt;如果relabel的操作只是为了产生一个临时变量，以作为下一个relabel操作的输入，那么我们可以使用__tmp作为标签名的前缀，通过该前缀定义的标签就不会写入到Target或者采集到的样本的标签中。&lt;/p&gt;

&lt;p&gt;上面的可以理解为&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置的第一个 souce_labels 是对同一个任务抓取目标的 LabelSet 进行预处理，具体而言就是将抓取目标地址进行 hashmod, 并将 hashmod 的值存到一个自定义字段 __tmp_hash 中。&lt;/li&gt;
&lt;li&gt;配置的第二个 souce_labels 对预处理后的抓取目标进行筛选，只选取 __tmp_hash 值满足正则匹配的，例子中 hashmod != 2 将全部被忽略。&lt;/li&gt;
&lt;li&gt;通过以上两步，就非常容易对相同 job 的抓取目标进行散列，从而抓取命中的部分。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以采用 hashmod 配置，使用同样的配置列表，将抓取目标散列到不同的 Prometheus server 中去, 从而很好实现 Prometheus 数据收集的水平扩展。&lt;/p&gt;

&lt;h3 id=&#34;远程读写&#34;&gt;远程读写&lt;/h3&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#remote_read:
#  - url: &amp;quot;http://localhost:7201/api/v1/prom/remote/read&amp;quot;
    # To test reading even when local Prometheus has the data
#    read_recent: true
#remote_write:
#  - url: &amp;quot;http://localhost:7201/api/v1/prom/remote/write&amp;quot;
#  - url: &amp;quot;http://10.47.178.80:9268/write&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是m3db的远程读写的配置，prometheus采集的数据就会直接发生到prometheus的apadter中，然后通过调用m3db的接口，将数据存储在m3db中，查询也直接在m3db中查询数据。&lt;/p&gt;

&lt;p&gt;远程读写是prometheus的一个扩展功能，prometheus自身主要是做时序数据库，关于存储提供了一个&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/#remote-storage&#34;&gt;可扩展性的方案&lt;/a&gt;，可以自己实现，目前已经有很多项目支持prometheus远程存储，比如&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/&#34;&gt;m3db&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/&#34;&gt;cortex&lt;/a&gt;,&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM&lt;/a&gt;等，目前VM在这一块做的还是比较好的。&lt;/p&gt;

&lt;h3 id=&#34;支持密钥文件校验-也可以跳过密钥校验&#34;&gt;支持密钥文件校验，也可以跳过密钥校验&lt;/h3&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: k8s-etcd
file_sd_configs:
  - files:
    - /opt/prometheus/discoveries/discovery-etcd.json
scheme: https
tls_config:
  ca_file: /opt/prometheus/ssl/etcd-ca.pem
  cert_file: /opt/prometheus/ssl/etcd.pem
  key_file:  /opt/k8s-prometheus/ssl/etcd-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;带着etcd的密钥证书去验证采集。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: k8s-other
file_sd_configs:
  - files:
    - /opt/prometheus-2.4.2.linux-amd64/discoveries/discovery-k8s.json
tls_config:
  insecure_skip_verify: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以直接跳过验证，前提是跳过验证能拉到数据。&lt;/p&gt;

&lt;h3 id=&#34;prometheus支持yml文件的服务发现实现路径重新设置&#34;&gt;prometheus支持yml文件的服务发现实现路径重新设置&lt;/h3&gt;

&lt;p&gt;I achieved this by using file_sd_config option. All targets are described in separate file(s), which can be either in YML or JSON format.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus.yml:

scrape_configs:
  - job_name: &#39;dummy&#39;  # This will be overridden in targets.yml
    file_sd_configs:
      - files:
        - targets.yml



targets.yml:

- targets: [&#39;host1:9999&#39;]
  labels:
    job: my_job
    __metrics_path__: /path1

- targets: [&#39;host2:9999&#39;]
  labels:
    job: my_job  # can belong to the same job
    __metrics_path__: /path2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;reload&#34;&gt;reload&lt;/h3&gt;

&lt;p&gt;Prometheus can reload its configuration at runtime.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kill -HUP pid
curl -X POST http://IP/-/reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus可以在运行时重新加载它的配置。 如果新配置格式不正确，则更改将不会应用。 通过向Prometheus进程发送SIGHUP或向/-/reload端点发送HTTP POST请求（启用&amp;ndash;web.enable-lifecycle标志时）来触发配置reload。 这也将重新加载任何配置的规则文件。&lt;/p&gt;

&lt;p&gt;我个人更倾向于采用 curl -X POST 的方式，因为每次 reload 过后， pid 会改变，使用 kill 方式需要找到当前进程号。
从 2.0 开始，hot reload 功能是默认关闭的，如需开启，需要在启动 Prometheus 的时候，添加 &amp;ndash;web.enable-lifecycle 参数。&lt;/p&gt;

&lt;h2 id=&#34;高级特性&#34;&gt;高级特性&lt;/h2&gt;

&lt;h3 id=&#34;prometheus分布式&#34;&gt;prometheus分布式&lt;/h3&gt;

&lt;p&gt;1、目前prometheus处理百万级的数据是完全没有问题的，也就是一千个服务器，一千个指标，以10S的频率去采集完全没有问题的，如果量级上去了，可以分业务进行多个prometheus进行采集使用，如果需要聚合，就需要使用prometheus的联邦集群，如果已经分业务但是量级还是不够，就是需要分group采集，然后聚合，其实也是分布式的概念。&lt;/p&gt;

&lt;p&gt;2、&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos&lt;/a&gt;+hashmod实现分布式采集聚合查询。&lt;/p&gt;

&lt;p&gt;3、自己的想法，想开发一个类似于redis cluster的分片的集群，使用raft算法，目前并没有相关的实现方案。&lt;/p&gt;

&lt;p&gt;4、使用远程读写，比如目前性能比较优秀的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;federation-联合&#34;&gt;FEDERATION(联合)&lt;/h3&gt;

&lt;p&gt;Federation允许一个Prometheus从另一个Prometheus中拉取某些指定的时序数据，Federation是Prometheus提供的扩展机制，允许Prometheus从一个节点扩展到多个节点，实际使用中一般会扩展成树状的层级结构。下面是Prometheus官方文档中对federation的配置示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;federate&#39;
  scrape_interval: 15s

  honor_labels: true
  metrics_path: &#39;/federate&#39;

  params:
    &#39;match[]&#39;:
      - &#39;{job=&amp;quot;prometheus&amp;quot;}&#39;
      - &#39;{__name__=~&amp;quot;job:.*&amp;quot;}&#39;

  static_configs:
    - targets:
      - &#39;source-prometheus-1:9090&#39;
      - &#39;source-prometheus-2:9090&#39;
      - &#39;source-prometheus-3:9090&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段配置所属的Prometheus将从source-prometheus-1 ~ 3这3个Prometheus的/federate端点拉取监控数据。 match[]参数指定了只拉取带有job=”prometheus标签的指标或者名称以job开头的指标。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;federation的使用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、物理使用&lt;/p&gt;

&lt;p&gt;就是上面使用方式，将几个prometheus的数据聚合到一个prometheus中，往往就是使用几个性能差的机器来采集部分数据，然后使用性能好的来聚合，也缓解了探针连接和拉去的压力。&lt;/p&gt;

&lt;p&gt;2、k8s使用federation&lt;/p&gt;

&lt;p&gt;要实现对Kubernetes集群的监控，因为Kubernetes的rbac机制以及证书认证，当然是把Prometheus部署在Kubernetes集群上最方便。可是很多监控系统是以k8s集群外部的Prometheus为主的，grafana和告警都是使用这个外部的Prometheus，如果还需要在Kubernetes集群内部部署一个Prometheus的话一定要把它连通外部的Prometheus联合起来，好在Prometheus支持Federation。&lt;/p&gt;

&lt;p&gt;前面已经介绍了将使用Prometheus federation的形式，k8s集群外部的Prometheus从k8s集群中Prometheus拉取监控数据，外部的Prometheus才是监控数据的存储。 k8s集群中部署Prometheus的数据存储层可以简单的使用emptyDir,数据只保留24小时(或更短时间)即可，部署在k8s集群上的这个Prometheus实例即使发生故障也可以放心的让它在集群节点中漂移。&lt;/p&gt;

&lt;p&gt;federation也只能在数据量不是太大的情况下使用，如果数据量太大，聚合到prometheus中单实例还是有着各种瓶颈，并不适合后期的聚合查询使用。&lt;/p&gt;

&lt;h3 id=&#34;prometheus高可用&#34;&gt;prometheus高可用&lt;/h3&gt;

&lt;p&gt;目前prometheus解决单点故障还是使用的是多份一致数据，启动多个prometheus对同一个数据进行采集，保留多分数据，但是数据是一致的，时序数据对一致性要求不高，可以容忍数据的部分丢失，对外是一个service。&lt;/p&gt;

&lt;h3 id=&#34;adapter&#34;&gt;adapter&lt;/h3&gt;

&lt;p&gt;adapter就是一个适配器，通用的功能就是为了适配，在prometheus中有很多需要使用的地方，在&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/#remote-storage&#34;&gt;远程存储&lt;/a&gt;中是一种使用方式，可以将数据转化到其他数据库适配的格式发送到对应的数据库中，还可以转换适配其他一些应用，还有我们使用的&lt;a href=&#34;https://github.com/DirectXMan12/k8s-prometheus-adapter&#34;&gt;k8s-prometheus-adapter&lt;/a&gt;也是一种&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-autoscaler/#基于prometheus&#34;&gt;方式&lt;/a&gt;，用于k8s重prometheus拉去指标。&lt;/p&gt;

&lt;h2 id=&#34;监控方案选择&#34;&gt;监控方案选择&lt;/h2&gt;

&lt;p&gt;一直纠结于选择Prometheus还是Open-falcon。这两者都是非常棒的新一代监控解决方案，后者是小米公司开源的，目前包括小米、金山云、美团、京东金融、赶集网等都在使用Open-Falcon，最大区别在于前者采用的是pull的方式获取数据，后者使用push的方式，暂且不说这两种方式的优缺点。简单说下我喜欢Prometheus的原因：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;开箱即用，部署运维非常方便&lt;/li&gt;
&lt;li&gt;prometheus的社区非常活跃&lt;/li&gt;
&lt;li&gt;自带服务发现功能&lt;/li&gt;
&lt;li&gt;简单的文本存储格式，进行二次开发非常方便。&lt;/li&gt;
&lt;li&gt;最重要的一点，他的报警插件我非常喜欢，带有分组、报警抑制、静默提醒机制。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里并没有贬低open-falcon的意思，还是那句老话适合自己的才是最好的。&lt;/p&gt;

&lt;h2 id=&#34;prometheus二次开发项目&#34;&gt;prometheus二次开发项目&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/yunlzheng/prometheus-pusher&#34;&gt;prometheus改造&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;使用总结&#34;&gt;使用总结&lt;/h2&gt;

&lt;p&gt;1、 正确关闭Prometheus有助于降低启动延迟的风险。那你怎么做的？&lt;/p&gt;

&lt;p&gt;如果没有干净地关闭普罗米修斯理论上应该能够在启动时正常恢复，但是它可能需要更长的时间，或者你可能会在软件堆栈的某处遇到一个模糊的错误，这会导致问题。因此，最好让普罗米修斯自己一个个关闭对应程序，直接使用kill pid，不要加-9，然后等待停止所需的时间，这通常不会花费太多时间。&lt;/p&gt;

&lt;p&gt;2、 prometheus只支持数值，可以为正可以为负，字符串只能作为标签。&lt;/p&gt;

&lt;p&gt;在Prometheus的世界里面，所有的数值都是64bit的。每条时间序列里面记录的其实就是64bit timestamp(时间戳) + 64bit value(采样值)。&lt;/p&gt;

&lt;p&gt;3、Prometheus有着非常高效的时间序列数据存储方法，每个采样数据仅仅占用3.5byte左右空间，上百万条时间序列，30秒间隔，保留60天，大概花了200多G（引用官方PPT）&lt;/p&gt;

&lt;p&gt;我们实际环境中，Node Exporter 有 251 个测量点，Prometheus 服务本身有 775 个测量点。每一千个时间序列大约需要 1M 内存。每条数据占用了1K的空间，可见加了很多标签在里面，数据量还是很可观的。&lt;/p&gt;

&lt;p&gt;4、metrics&lt;/p&gt;

&lt;p&gt;指标名称只能由ASCII字符、数字、下划线以及冒号组成并必须符合正则表达式&lt;code&gt;[a-zA-Z:][a-zA-Z0-9_:]*&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;标签的名称只能由ASCII字符、数字以及下划线组成并满足正则表达式&lt;code&gt;[a-zA-Z_][a-zA-Z0-9_]*&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;其中以&lt;code&gt;__&lt;/code&gt;作为前缀的标签，是系统保留的关键字，只能在系统内部使用。标签的值则可以包含任何Unicode编码的字符。在Prometheus的底层实现中指标名称实际上是以&lt;code&gt;__name__=&amp;lt;metric name&amp;gt;&lt;/code&gt;的形式保存在数据库中的，因此以下两种方式均表示的同一条time-series：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;api_http_requests_total{method=&amp;quot;POST&amp;quot;, handler=&amp;quot;/messages&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等同于：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{__name__=&amp;quot;api_http_requests_total&amp;quot;，method=&amp;quot;POST&amp;quot;, handler=&amp;quot;/messages&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pro将所有数据保存为timeseries data，用metric name和label区分，label是在metric name上的更细维度的划分，其中的每一个实例是由一个float64和timestamp组成，只不过timestamp是隐式加上去的，有时候不会显示出来，如下面所示(数据来源于pro暴露的监控数据，访问&lt;a href=&#34;http://localhost:9090/metrics&#34;&gt;http://localhost:9090/metrics&lt;/a&gt; 可得），其中go_gc_duration_seconds是metrics name,quantile=&amp;ldquo;0.5&amp;rdquo;是key-value pair的label，而后面的值是float64 value。
pro为了方便client library的使用提供了四种数据类型： Counter, Gauge, Histogram, Summary, 简单理解就是Counter对数据只增不减，Gauage可增可减，Histogram,Summary提供跟多的统计信息。下面的实例中注释部分# TYPE go_gc_duration_seconds summary 标识出这是一个summary对象。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile=&amp;quot;0.5&amp;quot;} 0.000107458
go_gc_duration_seconds{quantile=&amp;quot;0.75&amp;quot;} 0.000200112
go_gc_duration_seconds{quantile=&amp;quot;1&amp;quot;} 0.000299278
go_gc_duration_seconds_sum 0.002341738
go_gc_duration_seconds_count 18
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 107
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在我们的使用场景中，大部分监控使用Counter来记录，例如接口请求次数、消息队列数量、重试操作次数等。比较推荐多使用Counter类型采集，因为Counter类型不会在两次采集间隔中间丢失信息。&lt;/p&gt;

&lt;p&gt;一小部分使用Gauge，如在线人数、协议流量、包大小等。Gauge模式比较适合记录无规律变化的数据，而且两次采集之间可能会丢失某些数值变化的情况。随着时间周期的粒度变大，丢失关键变化的情况也会增多。&lt;/p&gt;

&lt;p&gt;还有一小部分使用Histogram和Summary，用于统计平均延迟、请求延迟占比和分布率。另外针对Historgram，不论是打点还是查询对服务器的CPU消耗比较高，通过查询时查询结果的返回耗时会有十分直观的感受。&lt;/p&gt;

&lt;p&gt;5、PromQL&lt;/p&gt;

&lt;p&gt;直接通过类似于PromQL表达式httprequeststotal查询时间序列时，返回值中只会包含该时间序列中的最新的一个样本值，这样的返回结果我们称之为瞬时向量。而相应的这样的表达式称之为瞬时向量表达式。&lt;/p&gt;

&lt;p&gt;而如果我们想过去一段时间范围内的样本数据时，我们则需要使用区间向量表达式。区间向量表达式和瞬时向量表达式之间的差异在于在区间向量表达式中我们需要定义时间选择的范围，时间范围通过时间范围选择器[]进行定义。例如，通过以下表达式可以选择最近5分钟内的所有样本数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http_request_total{}[5m]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对比&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http_request_total{} # 瞬时向量表达式，选择当前最新的数据
http_request_total{}[5m] # 区间向量表达式，选择以当前时间为基准，5分钟内的数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、http api&lt;/p&gt;

&lt;p&gt;Prometheus API使用了JSON格式的响应内容。 当API调用成功后将会返回2xx的HTTP状态码。&lt;/p&gt;

&lt;p&gt;反之，当API调用失败时可能返回以下几种不同的HTTP状态码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;404 Bad Request：当参数错误或者缺失时。

422 Unprocessable Entity 当表达式无法执行时。

503 Service Unavailiable 当请求超时或者被中断时。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所有的API请求均使用以下的JSON格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;status&amp;quot;: &amp;quot;success&amp;quot; | &amp;quot;error&amp;quot;,
  &amp;quot;data&amp;quot;: &amp;lt;data&amp;gt;,
​
  // Only set if status is &amp;quot;error&amp;quot;. The data field may still hold
  // additional data.
  &amp;quot;errorType&amp;quot;: &amp;quot;&amp;lt;string&amp;gt;&amp;quot;,
  &amp;quot;error&amp;quot;: &amp;quot;&amp;lt;string&amp;gt;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;瞬时数据查询&lt;/p&gt;

&lt;p&gt;通过使用QUERY API我们可以查询PromQL在特定时间点下的计算结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /api/v1/query
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;URL请求参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query=：PromQL表达式。

time=：用于指定用于计算PromQL的时间戳。可选参数，默认情况下使用当前系统时间。

timeout=：超时设置。可选参数，默认情况下使用-query,timeout的全局设置。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如使用以下表达式查询表达式up在时间点2015-07-01T20:10:51.781Z的计算结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;http://localhost:9090/api/v1/query?query=up&amp;amp;time=2015-07-01T20:10:51.781Z&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;区间数据查询&lt;/p&gt;

&lt;p&gt;使用QUERY_RANGE API我们则可以直接查询PromQL表达式在一段时间返回内的计算结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /api/v1/query_range
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;URL请求参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query=: PromQL表达式。

start=: 起始时间。

end=: 结束时间。

step=: 查询步长。

timeout=: 超时设置。可选参数，默认情况下使用-query,timeout的全局设置。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当使用QUERY_RANGE API查询PromQL表达式时，返回结果一定是一个区间向量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;resultType&amp;quot;: &amp;quot;matrix&amp;quot;,
  &amp;quot;result&amp;quot;: &amp;lt;value&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要注意的是，在QUERY_RANGE API中PromQL只能使用瞬时向量选择器类型的表达式。&lt;/p&gt;

&lt;p&gt;7、sum&lt;/p&gt;

&lt;p&gt;sum_over_time(range-vector): 范围向量内每个度量指标的求和值。&lt;/p&gt;

&lt;p&gt;sum不能用于时间范围的求和，只能用于不同维度之间的求和&lt;/p&gt;

&lt;p&gt;8、编码方式和压缩比&lt;/p&gt;

&lt;p&gt;prometheus目前提供了三种算法(主要是为了压缩数据)用于块的编码,可以通过-storage.local.chunk-encoding-version进行配置.参数的有效值为0,1,2.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk-encoding为0时,采用的是一种叫做delta encoding的算法.早期的prometheus存储层用的就是该实现.&lt;/li&gt;
&lt;li&gt;chunk-encoding为1时,是一种改进型的double-delta encoding算法,目前的额prometheus默认使用该编码方式.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这两种编码方式对每个块使用固定的字节长度,这样有利于随机读取.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk-encoding为2时,使用的则是可变长的编码方式.这种编码比起上面两种方式,特点在于牺牲压缩速度换取了压缩率.facebook的时间序列数据库Beringei采用的编码方式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面展示了压缩同样大小的数据对比(文档说样本很大,但没说具体多少):&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;编码类型&lt;/th&gt;
&lt;th&gt;压缩后样本大小&lt;/th&gt;
&lt;th&gt;所用时间&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3.3bytes&lt;/td&gt;
&lt;td&gt;2.9s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1.3bytes&lt;/td&gt;
&lt;td&gt;4.9s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;测试:&lt;/p&gt;

&lt;p&gt;官方给出在生产环境中,每个样本加上索引信息后的大小一般为3-4bytes,我们可以做下测试看看实际的样本有多大,因为数据文件是经过处理后写入磁盘的,所以没办法查看单个样本的大小,只能采集一段时间的数据后计算.&lt;/p&gt;

&lt;p&gt;测试的监控目标的有两个,一个是prometheus本身的信息,一个是node-exporter输出的硬件数据,我们的分别访问host:port/metrics获取采集到的数据内容.在这个例子中,每进行一次采集,prometheus server就会取回145756 bytes的数据.(即访问两个/metrics接口返回的数据相加)&lt;/p&gt;

&lt;p&gt;五次测试得出的结果为:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;用时&lt;/th&gt;
&lt;th&gt;抓取频率&lt;/th&gt;
&lt;th&gt;数据变化量(bytes)&lt;/th&gt;
&lt;th&gt;原始大小(bytes)&lt;/th&gt;
&lt;th&gt;压缩率&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;第一次&lt;/td&gt;
&lt;td&gt;10min&lt;/td&gt;
&lt;td&gt;5s +1003520&lt;/td&gt;
&lt;td&gt;17490720&lt;/td&gt;
&lt;td&gt;94%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第二次&lt;/td&gt;
&lt;td&gt;20min&lt;/td&gt;
&lt;td&gt;5s +1597440&lt;/td&gt;
&lt;td&gt;34981440&lt;/td&gt;
&lt;td&gt;95%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第三次&lt;/td&gt;
&lt;td&gt;155min&lt;/td&gt;
&lt;td&gt;5s +4243456&lt;/td&gt;
&lt;td&gt;271106160&lt;/td&gt;
&lt;td&gt;98%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第四次&lt;/td&gt;
&lt;td&gt;10min&lt;/td&gt;
&lt;td&gt;1s +1658880&lt;/td&gt;
&lt;td&gt;17490720&lt;/td&gt;
&lt;td&gt;90%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第五次&lt;/td&gt;
&lt;td&gt;20min&lt;/td&gt;
&lt;td&gt;1s +3481600&lt;/td&gt;
&lt;td&gt;34981440&lt;/td&gt;
&lt;td&gt;90%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;按照抓取频率5s,压缩率90%进行粗略估算.&lt;/p&gt;

&lt;p&gt;假设检测的数据为系统的硬件指标,即node-exporter的输出(145756个字节),且集群中有10台机器,那么24个小时的数据量将不超过200m.假设监控数据保留1个月,那么大概需要6-7G左右的空间&lt;/p&gt;

&lt;p&gt;9、内存使用&lt;/p&gt;

&lt;p&gt;prometheus在内存里保存了最近使用的chunks，具体chunks的最大个数可以通过storage.local.memory-chunks来设定，默认值为1048576，即1048576个chunk，大小为1G。 除了采用的数据，prometheus还需要对数据进行各种运算，因此整体内存开销肯定会比配置的local.memory-chunks大小要来的大，因此官方建议要预留3倍的local.memory-chunks的内存大小。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;As a rule of thumb, you should have at least three times more RAM available than needed by the memory chunks alone
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过server的metrics去查看prometheus_local_storage_memory_chunks以及process_resident_memory_byte两个指标值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.prometheus_local_storage_memory_chunks

    The current number of chunks in memory, excluding cloned chunks 目前内存中暴露的chunks的个数

2.process_resident_memory_byte

    Resident memory size in bytes 驻存在内存的数据大小

3.prometheus_local_storage_persistence_urgency_score 介于0-1之间，当该值小于等于0.7时，prometheus离开rushed模式。 当大于0.8的时候，进入rushed模式

4.prometheus_local_storage_rushed_mode 1表示进入了rushed mode，0表示没有。进入了rushed模式的话，prometheus会利用storage.local.series-sync-strategy以及storage.local.checkpoint-interval的配置加速chunks的持久化。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监测当前使用的内存量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_chunks
process_resident_memory_bytes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监测当前使用的存储指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_series: 时间序列持有的内存当前块数量
prometheus_local_storage_memory_chunks: 在内存中持久块的当前数量


prometheus_local_storage_chunks_to_persist: 当前仍然需要持久化到磁盘的的内存块数量
prometheus_local_storage_persistence_urgency_score: 紧急程度分数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10、prometheus的target采用的是长连接的方式，会和target的机器端口一直保持连接。&lt;/p&gt;

&lt;p&gt;11、一般我们可以使用prometheus_egine_query_duration_seconds来评估prometheus整体的响应时间，如果响应过慢，可能是promql使用不当造成的，比如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大量使用join来组合指标或者增加label&lt;/li&gt;
&lt;li&gt;大范围时间查询，step很小，导致数据量很大&lt;/li&gt;
&lt;li&gt;rate时，range duration要大于step，否则会丢失数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;12、wal中文件太多，句柄不够用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level=error ts=2019-07-05T02:29:56.706Z caller=main.go:717 err=&amp;quot;opening storage failed: read WAL: open WAL segments: open segment:00020174 in dir:/data/wal: open /data/wal/00020174: too many open files&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;wal中文件太多，句柄不够用，需要打开句柄，句柄不够用可能导致压缩block出错，报错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level=error ts=2019-07-05T01:58:01.826Z caller=main.go:717 err=&amp;quot;opening storage failed: block dir: \&amp;quot;/data/01DEN382CDGHQR91QKNDHT77M8\&amp;quot;: open /data/01DEN382CDGHQR91QKNDHT77M8/meta.json: no such file or directory&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以需要在机器使用之前设置一下参数&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;锁定内存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;logMessage &amp;quot;lock mem&amp;quot;
echo &amp;quot;esadmin hard memlock unlimited&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf
echo &amp;quot;esadmin soft memlock unlimited&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;修改最大文件描述数&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;logMessage &amp;quot;file description &amp;quot;
echo &amp;quot;esadmin soft nofile 65536&amp;quot;  &amp;gt;&amp;gt;/etc/security/limits.conf
echo &amp;quot;esadmin hard nofile 131072&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf

#修改最大线程数
logMessage &amp;quot;max thread size &amp;quot;
echo &amp;quot;esadmin soft nproc 2048 &amp;quot;&amp;gt;&amp;gt; /etc/security/limits.conf
echo &amp;quot;esadmin hard nproc 4096 &amp;quot;&amp;gt;&amp;gt; /etc/security/limits.conf
echo &amp;quot;esadmin soft nproc 2048 &amp;quot;&amp;gt;&amp;gt; /etc/security/limits.d/90-nproc.conf

#修改内存映射区域最大数
logMessage &amp;quot;max mem count &amp;quot;
echo &amp;quot;vm.max_map_count=655360&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf
sysctl -p
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;函数与常用表达式&#34;&gt;函数与常用表达式&lt;/h3&gt;

&lt;h4 id=&#34;操作符&#34;&gt;操作符&lt;/h4&gt;

&lt;p&gt;或&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;up{exporterName=~&amp;quot;etcd|etcd-event&amp;quot; ,cluster_name=~&amp;quot;k8s_xingang_02&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正则匹配,全量配置.*&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;up{exporterName=~&amp;quot;etcd.*&amp;quot; ,cluster_name=~&amp;quot;k8s_xingang_02&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;函数&#34;&gt;函数&lt;/h4&gt;

&lt;p&gt;PromQL 有三个很简单的原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;任意 PromQL 返回的结果都不是原始数据，即使查询一个具体的 Metric（如 go_goroutines），结果也不是原始数据&lt;/li&gt;
&lt;li&gt;任意 Metrics 经过 Function 计算后会丢失 &lt;code&gt;__name__ Label&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;子序列间具备完全相同的 Label/Value 键值对（可以有不同的 &lt;code&gt;__name__&lt;/code&gt;）才能进行代数运算&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;rate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;(last值-first值)/时间差s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;irate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;(last值-last前一个值)/时间戳差值
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以cpu的使用率常用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;irate(node_cpu_seconds_total{mode=&amp;quot;idle&amp;quot;,ip=~&amp;quot;$ip&amp;quot;}[2m]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;avg&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;avg 同一时间的多条数据的平均值&lt;/li&gt;
&lt;li&gt;avg_over_time(range-vector): 范围向量内同一个度量指标不同时间的多条数据的平均值。&lt;/li&gt;
&lt;li&gt;同理的还有max，min等&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;相减&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边有一个两个指标相减的问题，必须是统一维度的才能相互计算，不能直接用指标value计算，可以对指标进行sum，max，rate等计算后进行加减乘除&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;increase()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;increase(v range-vector)函数，  度量指标：last值-first值,increase的返回值类型只能是counters，主要作用是增加图表和数据的可读性，使用rate记录规则的使用率，以便持续跟踪数据样本值的变化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;idelta()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;idelta(v range-vector)函数，输入一个范围向量，返回key: value = 度量指标： 每最后两个样本值差值。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;label_replace&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;label_replace给指标的label新生成一个指标名的指标&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;label_replace(v instant-vector, dst_label string, replacement string, src_label string, regex string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将正则表达式与标签值src_label匹配。如果匹配，则返回时间序列，标签值dst_label被替换的扩展替换。$1替换为第一个匹配子组，$2替换为第二个等。如果正则表达式不匹配，则时间序列不会更改。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;label_replace(redis_remote_replication_dest_repl_offset{},&amp;quot;destldcId&amp;quot;,&amp;quot;$1&amp;quot;, &amp;quot;ldcId&amp;quot;, &amp;quot;(.*)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;by&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当指标中的label发生变化的时候，哪怕是同一个指标名，在promethes也是两个数据，如果将变化的两条数据衔接起来，这个时候就使用by，by就是按着制订的维度来获取指标，可以摒弃不一样的label，这样就能是一条数据了，这样就可以使得时序图连接起来，例如ntp的client变更&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(ntp_offset{ip=~&amp;quot;$ip&amp;quot;})by(ip)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;by还可以用于表格的聚合，对于相同label的数据可以聚合在一个表格中的一条数据，所以用by获取到不通指标数据中的相同的label，就可以实现不同value的展示，但是label一样，就是一条数据。&lt;/p&gt;

&lt;p&gt;也可以sum不加by的数据可用和任何数据聚合，其实也就是聚合后少的标签可用和多的标签进行聚合。&lt;/p&gt;

&lt;p&gt;还可以使用or，当两个数据是对立的时候，一个出现另一个就不会出来。这样也能使得数据出来&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;topk(5, rate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, rate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval]))
topk(5, rate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval]))
topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m])) or topk(5, irate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m]))
topk(5, irate(redis_command_call_duration_seconds_count{ softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m])) or topk(5, irate(redis_commands_total{softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m]))
sum by (cmd)( rate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or sum by (cmd) (irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or sum by (cmd) (irate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval]))
redis_memory_fragmentation_ratio{ip=&amp;quot;$ip&amp;quot;}  or redis_memory_used_rss_bytes{ip=&amp;quot;$ip&amp;quot;} / redis_memory_used_bytes{ip=&amp;quot;$ip&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;原理解析&#34;&gt;原理解析&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle&#34;&gt;prometheus原理解析&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Nsq</title>
          <link>https://kingjcy.github.io/post/middleware/mq/nsq/</link>
          <pubDate>Mon, 19 Jun 2017 20:21:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/nsq/</guid>
          <description>&lt;p&gt;NSQ是一个基于Go语言的分布式实时消息平台，它基于MIT开源协议发布，由bitly公司开源出来的一款简单易用的消息中间件。可用于大规模系统中的实时消息服务，并且每天能够处理数亿(十亿)级别的消息，其设计目标是为在分布式环境下运行的去中心化服务提供一个强大的基础架构。&lt;/p&gt;

&lt;p&gt;NSQ具有分布式、去中心化的拓扑结构，该结构具有无单点故障、故障容错、高可用性以及能够保证消息的可靠传递的特征。NSQ非常容易配置和部署，且具有最大的灵活性，支持众多消息协议。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;NSQ 由 3 个守护进程组成:&lt;/p&gt;

&lt;p&gt;1、nsqd 是接收、队列和传送消息到客户端的守护进程。&lt;/p&gt;

&lt;p&gt;nsqd守护进程是NSQ的核心部分，它是一个单独的监听某个端口进来的消息的二进制程序。每个nsqd节点都独立运行，不共享任何状态。当一个节点启动时，它会同时开启tcp和http服务，两个服务都可以提供给生产者和消费者，向一组nsqlookupd节点进行注册操作，http服务还提供给nsqadmin获取该nsqd本地topic和channel信息；。&lt;/p&gt;

&lt;p&gt;客户端可以发布消息到nsqd守护进程上，或者从nsqd守护进程上读取消息。通常，消息发布者会向一个单一的local nsqd发布消息，消费者从连接了的一组nsqd节点的topic上远程读取消息。如果你不关心动态添加节点功能，你可以直接运行standalone模式。&lt;/p&gt;

&lt;p&gt;2、nsqlookupd 是管理的拓扑信息，并提供了最终一致发现服务的守护进程。&lt;/p&gt;

&lt;p&gt;nsqlookupd服务器像consul或etcd那样工作，只是它被设计得没有协调和强一致性能力。每个nsqlookupd都作为nsqd节点注册信息的短暂数据存储区。消费者连接这些节点去检测需要从哪个nsqd节点上读取消息。&lt;/p&gt;

&lt;p&gt;nsqlookupd服务同时开启tcp和http两个监听服务，nsqd会作为客户端，连上nsqlookupd的tcp服务，并上报自己的topic和channel信息，以及通过心跳机制判断nsqd状态；还有个http服务提供给nsqadmin获取集群信息；&lt;/p&gt;

&lt;p&gt;3、nsqadmin 是一个 Web UI 来实时监控集群和执行各种管理任务。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;术语&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;topic消息&lt;/p&gt;

&lt;p&gt;topic 是 NSQ 消息发布的 逻辑关键词 ，可以理解为人为定义的一种消息类型。当程序初次发布带 topic 的消息时,如果 topic 不存在,则会在 nsqd中创建。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;producer消息的生产者/发布者&lt;/p&gt;

&lt;p&gt;producer 通过 HTTP API 将消息发布到 nsqd 的指定 topic ，一般有 pub/mpub 两种方式， pub 发布一个消息， mpub 一个往返发布多个消息。&lt;/p&gt;

&lt;p&gt;producer 也可以通过 nsqd客户端 的 TCP接口 将消息发布给 nsqd 的指定 topic 。&lt;/p&gt;

&lt;p&gt;当生产者 producer 初次发布带 topic 的消息给 nsqd 时,如果 topic 不存在，则会在 nsqd 中创建 topic 。&lt;/p&gt;

&lt;p&gt;生产者会同时连上NSQ集群中所有nsqd节点，当然这些节点的地址是在Writer初始化时，通过外界传递进去；当发布消息时，writer会随机选择一个nsqd节点发布某个topic的消息；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;channel消息传递的通道&lt;/p&gt;

&lt;p&gt;当生产者每次发布消息的时候,消息会采用多播的方式被拷贝到各个 channel 中, channel 起到队列的作用。&lt;/p&gt;

&lt;p&gt;channel 与 consumer(消费者) 相关，是消费者之间的负载均衡,消费者通过这个特殊的channel读取消息。&lt;/p&gt;

&lt;p&gt;在 consumer 想单独获取某个 topic 的消息时，可以 subscribe(订阅)一个自己单独命名的 nsqd中还不存在的 channel, nsqd会为这个 consumer创建其命名的 channel&lt;/p&gt;

&lt;p&gt;Channel 会将消息进行排列，如果没有 consumer读取消息，消息首先会在内存中排队，当量太大时就会被保存到磁盘中。可以在配置中配置具体参数。&lt;/p&gt;

&lt;p&gt;一个 channel 一般会有多个 consumer 连接。假设所有已连接的 consumer 处于准备接收消息的状态，每个消息将被传递到一个随机的 consumer。&lt;/p&gt;

&lt;p&gt;Go语言中的channel是表达队列的一种自然方式，因此一个NSQ的topic/channel，其核心就是一个存放消息指针的Go-channel缓冲区。缓冲区的大小由 &amp;ndash;mem-queue-size 配置参数确定。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;consumer消息的消费者&lt;/p&gt;

&lt;p&gt;consumer 通过 TCPsubscribe 自己需要的 channel&lt;/p&gt;

&lt;p&gt;topic 和 channel 都没有预先配置。 topic 由第一次发布消息到命名 topic 的 producer 创建 或 第一次通过 subscribe 订阅一个命名 topic 的 consumer 来创建。 channel 被 consumer 第一次 subscribe 订阅到指定的 channel 创建。&lt;/p&gt;

&lt;p&gt;多个 consumersubscribe一个 channel，假设所有已连接的客户端处于准备接收消息的状态，每个消息将被传递到一个 随机 的 consumer。&lt;/p&gt;

&lt;p&gt;NSQ 支持延时消息， consumer 在配置的延时时间后才能接受相关消息。&lt;/p&gt;

&lt;p&gt;Channel在 consumer 退出后并不会删除，这点需要特别注意。&lt;/p&gt;

&lt;p&gt;消费者也会同时连上NSQ集群中所有nsqd节点，reader首先会连上nsqlookupd，获取集群中topic的所有producer，然后通过tcp连上所有producer节点，并在本地用tornado轮询每个连接，当某个连接有可读事件时，即有消息达到，处理即可；&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;h2 id=&#34;架构&#34;&gt;架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/nsq/20170619.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;NSQ推荐通过 nsqd 实例使用协同定位 producer，这意味着即使面对网络分区，消息也会被保存在本地，直到它们被一个 consumer读取。更重要的是， producer不必去发现其他的 nsqd节点，他们总是可以向本地 nsqd实例发布消息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一个 producer向它的本地 nsqd发送消息，要做到这点，首先要先打开一个连接( NSQ 提供 HTTP API 和 TCP 客户端 等2种方式连接到 nsqd)，然后发送一个包含 topic和消息主体的发布命令(pub/mpub/publish)，在这种情况下，我们将消息发布到 topic上，消息会采用多播的方式被拷贝到各个 channel中, 然后通过多个 channel以分散到我们不同需求的 consumer中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;channel起到队列的作用。 多个 producer产生的 topic消息在每一个连接 topic的 channel上进行排队。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个 channel的消息都会进行排队，直到一个 consumer把他们消费，如果此队列超出了内存限制，消息将会被写入到磁盘中。 nsqd节点首先会向 nsqlookup 广播他们的位置信息，一旦它们注册成功， consumer将会从 nsqlookup 服务器节点上发现所有包含事件 topic的 nsqd节点。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个 consumer向每个 nsqd主机进行订阅操作，用于表明 consumer已经准备好接受消息了。这里我们不需要一个完整的连通图，但我们必须要保证每个单独的 nsqd实例拥有足够的消费者去消费它们的消息，否则 channel会被队列堆着。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;设计&#34;&gt;设计&lt;/h2&gt;

&lt;p&gt;1、分布式方案&lt;/p&gt;

&lt;p&gt;nsqd随意起， nsqlookup使用备份的方式，nsqlookupd的高可用是通过同时运行多个实例， 多个实例之间保持互备实现的。
一个client只会同时对一个nsqd建立连接， 所以一旦一个nsqd连接， 那么就不会对其他的topic建立连接
只有再一个nsqd坏掉的时候，才会重新选择nsqd。&lt;/p&gt;

&lt;p&gt;2、高可用&lt;/p&gt;

&lt;p&gt;高可用(无单点问题) writer和reader是直接连上各个nsqd节点，因此即使nsqlookupd挂了，也不影响线上正常使用；即使某个nsqd节点挂了，writer发布消息时，发现节点挂了，可以选择其他节点(当然，这是客户端负责的)，单个节点挂了对reader无影响；&lt;/p&gt;

&lt;p&gt;3、高性能&lt;/p&gt;

&lt;p&gt;writer在发布消息时，是随机发布到集群中nsqd节点，因此在一定程序上达到负载均衡；reader同时监听着集群中所有nsqd节点，无论哪个节点有消息，都会投递到reader上；&lt;/p&gt;

&lt;p&gt;4、高可扩展&lt;/p&gt;

&lt;p&gt;当向集群中添加节点时，首先reader会通过nsqlookupd发现新的节点加入，并自动连接；因为writer连接的nsqd节点的地址是初始化时设置的，因此增加节点时，只需要在初始化writer时，添加新节点的地址即可；&lt;/p&gt;

&lt;p&gt;5、client选择nsqd的原则&lt;/p&gt;

&lt;p&gt;nsq保证消息能够正常至少传输一次的方式是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;client表明已经可以接受消息&lt;/li&gt;
&lt;li&gt;nsqd将消息发送出去， 同时将这个消息进行本地存储&lt;/li&gt;
&lt;li&gt;client如果回复FIN 表示成功接受， 如果回复REQ， 表明需要重发， 如果没有回复， 则认为超时了， 进行重发&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以， 当nsqd异常关闭的时候， 没有来得及保存到本地的消息可能会丢失, 解决办法是讲同样的消息发送到两个nsqd中&lt;/p&gt;

&lt;p&gt;由于消息至少会被发送一次， 则意味着消息可能会被发送多次， 客户端需要能够确定收到消息所执行的操作是幂等的，即收到一次与收到多次的影响一致&lt;/p&gt;

&lt;p&gt;6、保证消息不丢失&lt;/p&gt;

&lt;p&gt;nsdlookup 如何路由请求&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;channels&amp;quot;: [ &amp;quot;nsq_to_file&amp;quot;, &amp;quot;c&amp;quot; ], &amp;quot;producers&amp;quot;: [ { &amp;quot;remote_address&amp;quot;: &amp;quot;127.0.0.1:58148&amp;quot;, &amp;quot;hostname&amp;quot;: &amp;quot;safedev01v.add.corp.qihoo.net&amp;quot;, &amp;quot;broadcast_address&amp;quot;: &amp;quot;safedev01v.add.corp.qihoo.net&amp;quot;, &amp;quot;tcp_port&amp;quot;: 4150, &amp;quot;http_port&amp;quot;: 4151, &amp;quot;version&amp;quot;: &amp;quot;0.3.6&amp;quot; }, { &amp;quot;remote_address&amp;quot;: &amp;quot;10.16.59.85:39652&amp;quot;, &amp;quot;hostname&amp;quot;: &amp;quot;safedev02v.add.corp.qihoo.net&amp;quot;, &amp;quot;broadcast_address&amp;quot;: &amp;quot;safedev02v.add.corp.qihoo.net&amp;quot;, &amp;quot;tcp_port&amp;quot;: 4150, &amp;quot;http_port&amp;quot;: 4151, &amp;quot;version&amp;quot;: &amp;quot;0.3.7&amp;quot; } ] }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7、细节&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;可以设置内存的使用大小， 但是并不建议将内存设置太小， 毕竟持久化是为了保证unclean关闭nsqd时，消息不会丢失&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;nsq-chan 的信息就是保存在go-chan中的，&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;8、每一个topic包含三个协程：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;router： 从go-chan中读取新发布的消息，并讲消息保存在一个队列（ram or rom）中，&lt;/li&gt;
&lt;li&gt;messagePump&lt;/li&gt;
&lt;li&gt;DiskQueue 讲内存中的消息写入到磁盘，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果一个topic没有订阅者（客户端），则该topic的内容就不会被diskqueue写入到磁盘中， 而是由DummyBackendQueue直接将消息丢弃掉&lt;/p&gt;

&lt;p&gt;9、nsqd中减小GC的优化方案&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;避免[]byte转换string&lt;/li&gt;
&lt;li&gt;重用缓存或者对象&lt;/li&gt;
&lt;li&gt;预先分配slice的内存， 并且知道每个item的大小&lt;/li&gt;
&lt;li&gt;避免使用interface{} 和封装的类型， &amp;gt;like a struct for a “multiple value” go-chan).&lt;/li&gt;
&lt;li&gt;避免使用defer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体的源码解析在&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/nsq-principle/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;部署使用&#34;&gt;部署使用&lt;/h1&gt;

&lt;p&gt;1、下载有现成的二进制文件。&lt;/p&gt;

&lt;p&gt;2、首先启动 nsdlookupd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nsqlookupd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端通过查询 nsdlookupd 来发现指定topic的生产者，并且 nsqd 节点广播 topic 和通道 channel 信息&lt;/p&gt;

&lt;p&gt;该服务运行后有两个端口：TCP 接口，nsqd 用它来广播；HTTP 接口，客户端用它来发现和管理。&lt;/p&gt;

&lt;p&gt;在生产环境中，为了高可用，最好部署三个nsqlookupd服务。&lt;/p&gt;

&lt;p&gt;3、部署nsqd&lt;/p&gt;

&lt;p&gt;先创建 nsqd 的数据路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir /tmp/nsqdata1 /tmp/nsqdata2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行两个测试的 nsqd 实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nsqd --lookupd-tcp-address=127.0.0.1:4160 -broadcast-address=127.0.0.1 -tcp-address=127.0.0.1:4150 -http-address=0.0.0.0:4151 -data-path=/tmp/nsqdata1
nsqd --lookupd-tcp-address=127.0.0.1:4160 -broadcast-address=127.0.0.1 -tcp-address=127.0.0.1:4152 -http-address=0.0.0.0:4153 -data-path=/tmp/nsqdata2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;nsqd 可以独立运行，不过通常它是由 nsdlookupd 实例所在集群配置的(它在这能声明 topics 和 channels ，以便大家能找到)&lt;/p&gt;

&lt;p&gt;服务启动后有两个端口：一个给客户端(TCP)，另一个是 HTTP API。还能够开启HTTPS。&lt;/p&gt;

&lt;p&gt;同一台服务器启动多个 nsqd ，要注意端口和数据路径必须不同，包括： –lookupd-tcp-address 、 -tcp-address 、 –data-path&lt;/p&gt;

&lt;p&gt;删除 topic 、channel 需要 HTTP API 调用。&lt;/p&gt;

&lt;p&gt;4、启动 nsqadmin 前端Web监控&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nsqadmin --lookupd-http-address=localhost:4161
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;nsqadmin 是一套 WEB UI ，用来汇集集群的实时统计，并执行不同的管理任务。&lt;/p&gt;

&lt;p&gt;运行后，能够通过4171端口查看并管理 topic 和 channel 。&lt;/p&gt;

&lt;p&gt;nsqadmin 通常只需要运行一个。&lt;/p&gt;

&lt;h2 id=&#34;使用实例&#34;&gt;使用实例&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;2个Producer 1个Consumer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;produce1() 发布publish &amp;quot;x&amp;quot;,&amp;quot;y&amp;quot; 到 topic &amp;quot;test&amp;quot;
produce2() 发布publish &amp;quot;z&amp;quot; 到 topic &amp;quot;test&amp;quot;
consumer1() 订阅subscribe channel &amp;quot;sensor01&amp;quot; of topic &amp;quot;test&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package test

import (
        &amp;quot;log&amp;quot;
        &amp;quot;time&amp;quot;
        &amp;quot;testing&amp;quot;
        &amp;quot;strconv&amp;quot;

        &amp;quot;github.com/nsqio/go-nsq&amp;quot;
)

func TestNSQ1(t *testing.T) {
       NSQDsAddrs := []string{&amp;quot;127.0.0.1:4150&amp;quot;, &amp;quot;127.0.0.1:4152&amp;quot;}
       go consumer1(NSQDsAddrs)
       go produce1()
       go produce2()
       time.Sleep(30 * time.Second)
}

func produce1() {
        cfg := nsq.NewConfig()
        nsqdAddr := &amp;quot;127.0.0.1:4150&amp;quot;
        producer, err := nsq.NewProducer(nsqdAddr, cfg)
        if err != nil {
                log.Fatal(err)
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;x&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;y&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
}

func produce2() {
        cfg := nsq.NewConfig()
        nsqdAddr := &amp;quot;127.0.0.1:4152&amp;quot;
        producer, err := nsq.NewProducer(nsqdAddr, cfg)
        if err != nil {
                log.Fatal(err)
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;z&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
}

func consumer1(NSQDsAddrs []string) {
        cfg := nsq.NewConfig()
        consumer, err := nsq.NewConsumer(&amp;quot;test&amp;quot;, &amp;quot;sensor01&amp;quot;, cfg)
        if err != nil {
                log.Fatal(err)
        }
        consumer.AddHandler(nsq.HandlerFunc(
                func(message *nsq.Message) error {
                        log.Println(string(message.Body) + &amp;quot; C1&amp;quot;)
                        return nil
                }))
        if err := consumer.ConnectToNSQDs(NSQDsAddrs); err != nil {
                log.Fatal(err, &amp;quot; C1&amp;quot;)
        }
        &amp;lt;-consumer.StopChan
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试结果&lt;/p&gt;

&lt;p&gt;x,y,z 都被 consumer1 接收了。注意到接收时间， x,y 几乎同时被接收，它们都由 producer1 发布，而 z 由 producer2 发布，中间间隔10秒。测试了很多次都是10秒,偶尔是15秒或20秒。查看了ConnectToNSQDs()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// ConnectToNSQDs takes multiple nsqd addresses to connect directly to.
//
// It is recommended to use ConnectToNSQLookupd so that topics are discovered
// automatically.  This method is useful when you want to connect to local instance.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Consumer 每隔 x 秒，向 nsqlookud 进行http轮询，用来更新自己的 nsqd 地址目录,当一个 producer 的 channel 一直没有数据时，则会轮询到下一个 producer&lt;/p&gt;

&lt;p&gt;可见go的客户端代码库就是github.com/nsqio/go-nsq。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;1个Producer 3个Consumer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;produce3() 发布publish &amp;quot;x&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;z&amp;quot; 到 topic &amp;quot;test&amp;quot;
consumer1() 订阅subscribe channel &amp;quot;sensor01&amp;quot; of topic &amp;quot;test&amp;quot;
consumer2() 订阅subscribe channel &amp;quot;sensor01&amp;quot; of topic &amp;quot;test&amp;quot;
consumer3() 订阅subscribe channel &amp;quot;sensor02&amp;quot; of topic &amp;quot;test&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package test

import (
        &amp;quot;log&amp;quot;
        &amp;quot;time&amp;quot;
        &amp;quot;testing&amp;quot;
        &amp;quot;strconv&amp;quot;

        &amp;quot;github.com/nsqio/go-nsq&amp;quot;
)

func TestNSQ2(t *testing.T) {
        NSQDsAddrs := []string{&amp;quot;127.0.0.1:4150&amp;quot;}
        go consumer1(NSQDsAddrs)
        go consumer2(NSQDsAddrs)
        go consumer3(NSQDsAddrs)
        go produce3()
        time.Sleep(5 * time.Second)
}

func produce3() {
        cfg := nsq.NewConfig()
        nsqdAddr := &amp;quot;127.0.0.1:4150&amp;quot;
        producer, err := nsq.NewProducer(nsqdAddr, cfg)
        if err != nil {
                log.Fatal(err)
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;x&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;y&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;z&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
}

func consumer1(NSQDsAddrs []string) {
        cfg := nsq.NewConfig()
        consumer, err := nsq.NewConsumer(&amp;quot;test&amp;quot;, &amp;quot;sensor01&amp;quot;, cfg)
        if err != nil {
                log.Fatal(err)
        }
        consumer.AddHandler(nsq.HandlerFunc(
                func(message *nsq.Message) error {
                        log.Println(string(message.Body) + &amp;quot; C1&amp;quot;)
                        return nil
                }))
        if err := consumer.ConnectToNSQDs(NSQDsAddrs); err != nil {
                log.Fatal(err, &amp;quot; C1&amp;quot;)
        }
        &amp;lt;-consumer.StopChan
}

func consumer2(NSQDsAddrs []string) {
        cfg := nsq.NewConfig()
        consumer, err := nsq.NewConsumer(&amp;quot;test&amp;quot;, &amp;quot;sensor01&amp;quot;, cfg)
        if err != nil {
                log.Fatal(err)
        }
        consumer.AddHandler(nsq.HandlerFunc(
                func(message *nsq.Message) error {
                        log.Println(string(message.Body) + &amp;quot; C2&amp;quot;)
                        return nil
                }))
        if err := consumer.ConnectToNSQDs(NSQDsAddrs); err != nil {
                log.Fatal(err, &amp;quot; C2&amp;quot;)
        }
        &amp;lt;-consumer.StopChan
}

func consumer3(NSQDsAddrs []string) {
        cfg := nsq.NewConfig()
        consumer, err := nsq.NewConsumer(&amp;quot;test&amp;quot;, &amp;quot;sensor02&amp;quot;, cfg)
        if err != nil {
                log.Fatal(err)
        }
        consumer.AddHandler(nsq.HandlerFunc(
                func(message *nsq.Message) error {
                        log.Println(string(message.Body) + &amp;quot; C3&amp;quot;)
                        return nil
                }))
        if err := consumer.ConnectToNSQDs(NSQDsAddrs); err != nil {
               log.Fatal(err, &amp;quot; C3&amp;quot;)
        }
        &amp;lt;-consumer.StopChan
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;consumer1 接收到了 y
consumer2 接收到了 x,z
consumer3 接收到了 x,y,z
channelsensor01 中的消息被随机的分到了 consumer1 和 consumer2
consumer3 单独占有 channelsensor02，接收了其中的所有消息
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用细节&#34;&gt;使用细节&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;nsqd启动时，端口和数据存放要不同&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;消息发送必须指定具体的某个nsqd；而消费则可以通过lookupd获取再重定向&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;消费者接受数据时，要设置 config.MaxInFlight&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;channel在消费者退出后并不会删除，需要特别注意。如果紧紧是想利用nsq作为消息广播，不考虑离线数据保存，不妨考虑nats。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;channel的名字，有很多限制，基本ASSCI字符+数字，以及点号”.”,下划线”_”。中文（其他非英语文字应该也不行）、以及空格、冒号”:”、横线”-“等都不得出现&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 并发安全</title>
          <link>https://kingjcy.github.io/post/architecture/concurrencesafe/</link>
          <pubDate>Sun, 09 Apr 2017 19:25:10 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/concurrencesafe/</guid>
          <description>&lt;p&gt;并发安全，就是多个并发体在同一段时间内访问同一个共享数据，共享数据能被正确处理。&lt;/p&gt;

&lt;h1 id=&#34;并发不安全&#34;&gt;并发不安全&lt;/h1&gt;

&lt;p&gt;最典型的案例:卖票超售&lt;/p&gt;

&lt;p&gt;设想有一家电影院，有两个售票窗口，售票员售票时候先看一下当前剩余票数是否大于0，如果大于0则售出票。&lt;/p&gt;

&lt;p&gt;此时票数剩下一张票，两个售票窗口同时来了顾客，两个售票人都看了一下剩余票数还有一张，不约而同地收下顾客的钱，余票还剩一张，但是却售出了两张票，就会出现致命的问题。&lt;/p&gt;

&lt;h1 id=&#34;如何做到并发安全&#34;&gt;如何做到并发安全&lt;/h1&gt;

&lt;p&gt;目前最最主流的办法就是加锁就行操作，其实售票的整个操作同时间内只能一个人进行，在我看来归根到底加锁其实就是让查询和售票两个步骤原子化，只能一块执行，不能被其他程序中断，让这步操作变成串行化。下面就介绍一下使查询和售票原子化的常见程序操作：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;锁的做法就是每次进入这段变量共享的程序片段，都要先获取一下锁，如果获取成功则可以继续执行，如果获取失败则阻塞，直到其他并发体把锁给释放，程序得到执行调度才可以执行下去。&lt;/p&gt;

&lt;p&gt;锁本质上就是让并发体创建一个程序临界区，临界区一次只能进去一个并发体，伪代码示意如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lock()
totalNum = getTotalNum()
if totalNum &amp;gt; 0
    # 则售出一张票
    totalNum = totalNum - 1
else
    failedToSold()
unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读锁与写锁&lt;/p&gt;

&lt;p&gt;读锁也叫共享锁，写锁也叫排它锁，锁的概念被发明了之后，人们就想着如果我很多个并发体大部分时间都是读，如果就把变量读取的时候也要建立临界区，那就有点太大题小做了。于是人们发明了读锁，一个临界区如果加上了读锁，其他并发体执行到相同的临界区都可以加上读锁，执行下去，但不能加上写锁。这样就保证了可以多个并发体并发读取而又不会互相干扰。&lt;/p&gt;

&lt;p&gt;在golang中也是提供了mutex的锁机制。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;队列&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;队列也是解决并发不安全的做法。多个并发体去获取队列里的元素，然后进行处理，这种做法和上锁其实大同小异，本质都是把并发的操作串行化，同一个数据同一个时刻只能交给一个并发体去处理,伪代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 第一个获取到队列的元素就可以进行下去
isCanSold = canSoldList.pop()
totalNum = getTotalNum()
if totalNum &amp;gt; 0
    # 则售出一张票
    totalNum = totalNum - 1
else
    failedToSold()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在golang中也提供了队列机制，也就是Goroutine 通过 channel 进行安全读写共享变量。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;CAS&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CAS（compare and swap），先比对，然后再进行交换，和数据库里的乐观锁的做法很相似。&lt;/p&gt;

&lt;p&gt;乐观锁&lt;/p&gt;

&lt;p&gt;数据库里的乐观锁并不是真的使用了锁的机制，而是一种程序的实现思路。
乐观锁的想法是，每次拿取数据再去修改的时候很乐观，认为其他人不会去修改这个数据，表另外维护一个额外版本号的字段。
查数据的时候记录下该数据的版本号，如果成功修改的话，会修改该数据的版本号，如果修改的时候版本号和查询的时候版本号不一致，则认为数据已经被修改过，会重新尝试查询再次操作。&lt;/p&gt;

&lt;p&gt;设我们表有一个user表，除了必要的字段，还有一个字段version，表如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;id  username    money   version
1   a   10  100
2   b   20  100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候我们需要修改a的余额-10元，执行事务语句如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while
    select @money = money, @version = version from user where username = a;
    if @money &amp;lt; 10
        print(&#39;余额成功&#39;)
        break
    # 扣费前的预操作
    paied()
    # 实行扣费
    update user set money = money - 10, version = version + 1 where username = a and version = @version
    # 影响条数等于1，证明执行成功
    if @@ROWCOUNT == 1
        print(&#39;扣费成功&#39;)
        break
    else
        rollback
        print(&#39;扣费失败，重新进行尝试&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;乐观锁的做法就是使用版本的形式，每次写数据的时候会比对一下最开始的版本号，如果不同则证明有问题。&lt;/p&gt;

&lt;p&gt;CAS的做法也是一样的，在代码里面的实现稍有一点不同，由于SQL每条语句都是原子性，查询对应版本号的数据再更新的这个条件是原子性的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;update user set money = money - 10, version = version + 1 where username = a and version = @version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是在代码里面两条查询和赋值两个语句不是原子性的，需要有特定的函数让cpu底层把两个操作变成一个原子操作，在go里面有atomic包支持实现，是这样实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    user := getUserByName(A)
    version := user.version
    paied()
    if atomic.CompareAndSwapInt32(&amp;amp;user.version, version, version + 1) {
        user.money -= 10
    } else {
        rollback()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;atomic.CompareAndSwapInt32需要依次传入要比较变量的地址，旧变量的值，修改后变量的值，函数会判断旧变量的值是否与现在变量的地址是否相同，相同则把新变量的值写入到该变量。
CAS的好处是不需要程序去创建临界区，而是让CPU去把两个指令变成原子性操作，性能更好，但是如果变量会被频繁更改的话，重试的次数变多反而会使得效率不如加锁高。&lt;/p&gt;

&lt;p&gt;在golang中也提供了CAS机制，也就是Goroutine 通过 atomic进行安全读写共享变量。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Zabbix基本使用</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix/</link>
          <pubDate>Sat, 04 Mar 2017 17:54:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix/</guid>
          <description>&lt;p&gt;zabbix是目前各大互联网公司使用最广泛的开源监控之一,其历史最早可追溯到1998年,在业内拥有各种成熟的解决方案.&lt;/p&gt;

&lt;h1 id=&#34;网站可用性&#34;&gt;网站可用性&lt;/h1&gt;

&lt;p&gt;在软件系统的高可靠性（也称为可用性，英文描述为HA，High Available）里有个衡量其可靠性的标准——X个9，这个X是代表数字3~5。X个9表示在软件系统1年时间的使用过程中，系统可以正常使用时间与总时间（1年）之比，我们通过下面的计算来感受下X个9在不同级别的可靠性差异。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1个9：(1-90%)*365=36.5天，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是36.5天
2个9：(1-99%)*365=3.65天 ， 表示该软件系统在连续运行1年时间里最多可能的业务中断时间是3.65天
3个9：(1-99.9%)*365*24=8.76小时，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是8.76小时。
4个9：(1-99.99%)*365*24=0.876小时=52.6分钟，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是52.6分钟。
5个9：(1-99.999%)*365*24*60=5.26分钟，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是5.26分钟。
6个9：(1-99.9999%)*365*24*60*60=31秒， 示该软件系统在连续运行1年时间里最多可能的业务中断时间是31秒
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前能达到4个9就很好了。&lt;/p&gt;

&lt;h1 id=&#34;组件&#34;&gt;组件&lt;/h1&gt;

&lt;p&gt;zabbix属于CS架构,Server端基于C语言编写,相比其他语言具有一定的性能优势(在数据量不大的情况下!).Web管理端则使用了PHP. 而其client端有各种流行语言的库实现,方便使用其API&lt;/p&gt;

&lt;p&gt;在数据的存储方面,zabbix使用了关系性数据库,包括SQLite,MySQL,PostgreSQL,Oracle,DB2&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;yum安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;zabbix的安装比较繁琐,但也不算困难(主要是因为网上提供的资料足够多)&lt;/p&gt;

&lt;p&gt;我们需要一种关系型关系型数据库,目前提供的选择有MySQL,SQLite, PostgreSQL,Oracle,DB2&lt;/p&gt;

&lt;p&gt;接下来需要安装PHP的运行环境,Web服务器可是使用Apache或者Nginx都可以.&lt;/p&gt;

&lt;p&gt;最后一步是安装zabbix服务.&lt;/p&gt;

&lt;p&gt;完整的安装教程可以参考:&lt;a href=&#34;http://support.supermap.com.cn/DataWarehouse/WebDocHelp/icm/Appdix/Zabbix_server/Zabbix_Installation.htm&#34;&gt;zabbix安装指南&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;主要步骤&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;配置zabbix官方yum源，还有base和epel源&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install zabbix-server-mysql zabbix-get
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;初始化database&lt;/p&gt;

&lt;p&gt;导入zabbix-server-mysql包中的create.sql来初始化数据库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm -ql zabbix-server-mysql
mysql -uroot -p -Dzabbix &amp;lt; create.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以查看表了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置服务端配置文件并启动&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装web&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install httpd php php-mysql php-mbstring php-gd php-bamath php-ladp php-xml
yum install zabbix-web-mysql zabbix-web
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后访问　　&lt;a href=&#34;http://zabbix-web-ip/zabbix/setup.php进行zabbix初始化&#34;&gt;http://zabbix-web-ip/zabbix/setup.php进行zabbix初始化&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装zabbix-agent&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install zabbix-agent zabbix-sender
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置客户端端配置文件并启动&lt;/p&gt;

&lt;p&gt;服务端快速安装脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#clsn

#设置解析 注意：网络条件较好时，可以不用自建yum源
# echo &#39;10.0.0.1 mirrors.aliyuncs.com mirrors.aliyun.com repo.zabbix.com&#39; &amp;gt;&amp;gt; /etc/hosts

#安装zabbix源、aliyun YUM源
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
rpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm

#安装zabbix
yum install -y zabbix-server-mysql zabbix-web-mysql

#安装启动 mariadb数据库
yum install -y  mariadb-server
systemctl start mariadb.service

#创建数据库
mysql -e &#39;create database zabbix character set utf8 collate utf8_bin;&#39;
mysql -e &#39;grant all privileges on zabbix.* to zabbix@localhost identified by &amp;quot;zabbix&amp;quot;;&#39;

#导入数据
zcat /usr/share/doc/zabbix-server-mysql-3.0.13/create.sql.gz|mysql -uzabbix -pzabbix zabbix

#配置zabbixserver连接mysql
sed -i.ori &#39;115a DBPassword=zabbix&#39; /etc/zabbix/zabbix_server.conf

#添加时区
sed -i.ori &#39;18a php_value date.timezone  Asia/Shanghai&#39; /etc/httpd/conf.d/zabbix.conf

#解决中文乱码
yum -y install wqy-microhei-fonts
\cp /usr/share/fonts/wqy-microhei/wqy-microhei.ttc /usr/share/fonts/dejavu/DejaVuSans.ttf

#启动服务
systemctl start zabbix-server
systemctl start httpd

#写入开机自启动
chmod +x /etc/rc.d/rc.local
cat &amp;gt;&amp;gt;/etc/rc.d/rc.local&amp;lt;&amp;lt;EOF
systemctl start mariadb.service
systemctl start httpd
systemctl start zabbix-server
EOF

#输出信息
echo &amp;quot;浏览器访问 http://`hostname -I|awk &#39;{print $1}&#39;`/zabbix&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端快速部署脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#clsn

#设置解析
echo &#39;10.0.0.1 mirrors.aliyuncs.com mirrors.aliyun.com repo.zabbix.com&#39; &amp;gt;&amp;gt; /etc/hosts

#安装zabbix源、aliyu nYUM源
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
rpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm

#安装zabbix客户端
yum install zabbix-agent -y
sed -i.ori &#39;s#Server=127.0.0.1#Server=172.16.1.61#&#39; /etc/zabbix/zabbix_agentd.conf
systemctl start  zabbix-agent.service

#写入开机自启动
chmod +x /etc/rc.d/rc.local
cat &amp;gt;&amp;gt;/etc/rc.d/rc.local&amp;lt;&amp;lt;EOF
systemctl start  zabbix-agent.service
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;编译安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;系统环境&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    OS:         centos7.5
    software：  zabbix 4.0 LTS
    DBSever:    MariaDB-10.2.15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一、需要先把数据库装上，这里用到的是mariadb 二进制包安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、下载二进制包，
     官网的下载路径：
  wget http://mirrors.neusoft.edu.cn/mariadb//mariadb-10.2.15/bintar-linux-x86_64/mariadb-10.2.15-linux-x86_64.tar.gz

2、添加组和用户
  [root@node2 ~]# groupadd -r -g 306 mysql
  [root@node2 ~]# useradd -g mysql -u 306 -r mysql

3、解压mariadb二进制包到/usr/local下去
   [root@node2 ~]# tar xf mariadb-10.2.15-linux-x86_64.tar.gz -C /usr/local/

4、进入到/usr/local下面创建mysql的软连接
   [root@node2 ~]# cd /usr/local/
   [root@node2 /usr/local]# ln -s mariadb-10.2.15-linux-x86_64/ mysql

5、修改mysql的相对应的属主和属组权限
    [root@node2 /usr/local]# chown -R root.mysql mysql/

6、创建数据文件的存放路径，并修改所属组的权限为mysql
     [root@node2 ~]#   cd /app/
     [root@node2 /app]# mkdir mydata
     [root@node2 ]#  chown -R mysql.mysql  /app

7、初始化数据库，指定好数据文件的存放路径和用户
       [root@node2 ]# cd /usr/local/mysql/
       [root@node2 /usr/local/mysql/]# scripts/mysql_install_db --datadir=/app/mydata --user=mysql

8、拷贝mariadb的启动脚本到/etc/rc.d/init.d下命名为mysqld
       [root@node2 /usr/local/mysql/]# cp support-files/mysql.server /etc/rc.d/init.d/mysqld

9、把mysqld设置为开机启动
       [root@node2 /usr/local/mysql/]# chkconfig --add mysqld

10、创建mariadb的配置文件存放路径，并拷贝模版文件到这个目录下命名为my.cnf
      [root@node2 /usr/local/mysql/]# mkdir /etc/mysql
      [root@node2 /usr/local/mysql/]#cp support-files/my-large.cnf /etc/mysql/my.cnf

11、配置系统环境变量，重读配置文件让它生效
      [root@node2 /usr/local/mysql/]# vim /etc/profile.d/mysql.sh
      [root@node2 /usr/local/mysql/]#export PATH=/usr/local/mysql/bin:$PATH
      [root@node2 /usr/local/mysql/]# . /etc/profile.d/mysql.sh

12、修改mariadb的配置文件需要增加几条内容
      [root@node2 /usr/local/mysql/]# vim /etc/mysql/my.cnf
          lower_case_table_names = 1
          character-set-server = utf8
          datadir = /app/mydata
          innodb_file_per_table = on
          skip_name_resolve = o

13、启动数据库服务
      [root@node2 /usr/local/mysql/]#  service mysqld start

14、查看mariadb的服务端口是否正常监听
    [root@node2 /app]#ss -tnl
    State      Recv-Q Send-Q       Local Address:Port                      Peer Address:Port
    LISTEN     0      128                      *:52874                                *:*
    LISTEN     0      128                      *:11211                                *:*
    LISTEN     0      128                      *:111                                  *:*
    LISTEN     0      128                      *:22                                   *:*
    LISTEN     0      128              127.0.0.1:631                                  *:*
    LISTEN     0      100              127.0.0.1:25                                   *:*
    LISTEN     0      80                      :::3306                                :::*

15、数据库的安全初始操作，设置完之后就可以先创建zabbix相关的库和用户
    [root@node2 /app]#mysql_secure_installation
    [root@node2 /app]#mysql -uroot -p
16、创建zabbix库
    MariaDB [(none)]&amp;gt; create database zabbix character set utf8 collate utf8_bin;
17、给zabbix库授权并指定用户
    MariaDB [(none)]&amp;gt; grant all privileges on zabbix.* to zabbix@&#39;192.168.137.%&#39; identified by &#39;123456&#39;;

18、在另一台主机上测试用zabbix用是否能正常登陆数据库
    [root@node7 ~]#mysql -uzabbix -p123456 -h192.168.137.54
    Welcome to the MariaDB monitor.  Commands end with ; or \g.
    Your MariaDB connection id is 12
    Server version: 10.2.15-MariaDB-log MariaDB Server

    Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

    Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.

    MariaDB [(none)]&amp;gt; show databases;
    +--------------------+
    | Database           |
    +--------------------+
    | information_schema |
    | zabbix             |
    +--------------------+
    2 rows in set (0.00 sec)
    MariaDB [(none)]&amp;gt;
19、在zabbix server主机上导入zabbix自带的三个表，路径在/root/zabbix-4.0.1/database/mysql下后缀为.sql的三个文件
    [root@node6 ~/zabbix-4.0.1]#ls -l database/mysql/
    total 5816
    -rw-r--r-- 1 1001 1001 3795433 Oct 30 01:36 data.sql
    -rw-r--r-- 1 1001 1001 1978341 Oct 30 01:36 images.sql
    -rw-r--r-- 1 root root   15323 Nov 26 22:44 Makefile
    -rw-r--r-- 1 1001 1001     392 Oct 30 01:36 Makefile.am
    -rw-r--r-- 1 1001 1001   15711 Oct 30 01:36 Makefile.in
    -rw-r--r-- 1 1001 1001  140265 Oct 30 01:36 schema.sql

20、导入sql文件是有先后顺序的，先导schema.sql、images.sql、data.sql.
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456 zabbix &amp;lt; schema.sql
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456 zabbix &amp;lt; images.sql
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456 zabbix &amp;lt; data.sql

21、进到数据库里面查看zabbix库是否导入成功
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456
    MariaDB [(none)]&amp;gt; use zabbix
    MariaDB [zabbix]&amp;gt; show tables;
    +----------------------------+
    | Tables_in_zabbix           |
    +----------------------------+
    | acknowledges               |
    | actions                    |
    | alerts                     |
    | application_discovery      |
    | application_prototype      |
    | application_template       |
    | applications               |
    | auditlog                   |
    | auditlog_details           |
    | autoreg_host               |
    | conditions                 |
    | config                     |
    | corr_condition             |
    | corr_condition_group       |
    .......
    | users                      |
    | users_groups               |
    | usrgrp                     |
    | valuemaps                  |
    | widget                     |
    | widget_field               |
    +----------------------------+
    144 rows in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;二、编译zabbix&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、安装编译环境所需要的依赖包组
    [root@node6 ~]#yum install gcc  libxml2-devel libevent-devel net-snmp net-snmp-devel  curl  curl-devel php  php-bcmath  php-mbstring mariadb mariadb-devel –y

    还需要安装一些php的依赖包后续在网页端安装zabbix时需要用到所以先提前安装好
    [root@node6 ~]#yum install php-gettext php-session php-ctype php-xmlreader php-xmlwrer php-xml php-net-socket php-gd php-mysql -y

2、安装jdk环境，装的是jdk-8u191-linux-x64.rpm的包，要不后面编译时会报Java找不到。
    [root@node6 ~]#yum -y install jdk-8u191-linux-x64.rpm

3、创建zabbix用户
    [root@node6 ~]#useradd zabbix -s /sbin/nologin

4、下载zabbix的源码包
    [root@node6 ~]#wget http://192.168.137.53/yum/zabbix/zabbix-4.0.1.tar.gz

5、解压源码包，并进入到解压后的目录里去
    [root@node6 ~]#tar xf zabbix-4.0.1.tar.gz
    [root@node6 ~]#cd zabbix-4.0.1/
    [root@node6 ~/zabbix-4.0.1]#

6、开始编译安装zabbix
    [root@node6 ~/zabbix-4.0.1./configure  \
    --prefix=/usr/local/zabbix  \
    --enable-server  \
    --enable-agent  \
    --with-mysql   \
    --with-net-snmp  \
    --with-libcurl  \
    --with-libxml2  \
    --enable-java

7、执行make install
    [root@node6 ~/zabbix-4.0.1]#make -j 2 &amp;amp;&amp;amp; make install

8、拷贝启动脚本文件到/etc/init.d目录下
    [root@node6 ~/zabbix-4.0.1]#cp misc/init.d/fedora/core/* /etc/init.d/

9、拷贝过去的脚本需要修改下目录路径，server和agent都需要改
    [root@node6 ~/zabbix-4.0.1]#vim /etc/init.d/zabbix_server
        22         BASEDIR=/usr/local
    改成：
        22         BASEDIR=/usr/local/zabbix

    agent启动脚本修改也是一样
        [root@node6 ~/zabbix-4.0.1vim /etc/init.d/zabbix_agentd
        22         BASEDIR=/usr/local
    改成：
        22         BASEDIR=/usr/local/zabbix

10、创建zabbix的日志存放路径和修改/usr/local/zabbix的所属主为zabbix
    [root@node6 ~/zabbix-4.0.1]#mkdir /var/log/zabbix
    [root@node6 ~/zabbix-4.0.1]#chown -R zabbix.zabbix /var/log/zabbix
    [root@node6 ~/zabbix-4.0.1]#ll /var/log/zabbix/ -d
    drwxr-xr-x 2 zabbix zabbix 6 Nov 27 09:17 /var/log/zabbix/
    [root@node6 ~]#chown -R zabbix.zabbix /usr/local/zabbix/
    [root@node6 ~]#ll -d /usr/local/zabbix/
    drwxr-xr-x 7 zabbix zabbix 64 Nov 26 22:45 /usr/local/zabbix/

11、修改配置文件
    [root@node6 ~/zabbix-4.0.1]#vim /usr/local/zabbix/etc/zabbix_server.conf
    ListenPort=10051   启用监听端口，不过默认也是启用的。

    LogFile=/var/log/zabbix/zabbix_server.log    修改日志存放路径，默认是在/tmp下

    LogFileSize=5   开启日志滚动，单位为MB、达到指定值之后就生成新的日志文件。
    DebugLevel=4   日志级别等级，4为debug，利于排除错误，排错之后可以改成3级别的。
    PidFile=/usr/local/zabbix/zabbix_server.pid   zabbix pid文件路径默认为tmp下需要改成安装目录，并且安装目录的所属组要改成zabbix用户
    # SocketDir=/tmp
    User=zabbix                    启动的用户默认也是zabbix,如果要改成root的话 还需要修改一项
    # AllowRoot=0                  需要改成1才能使用root来启动，默认0的话是被禁止用root启动，不过最好别用root
    SocketDir=/usr/local/zabbix   socket 文件存放路径默认在/tmp下
    DBHost=192.168.137.54          数据库地址必须要填
    DBName=zabbix                  数据库名称
    DBUser=zabbix                  数据库连接用户
    DBPassword=123456              数据库连接密码，建议在生产中密码不要太简单了。
    DBPort=3306                    数据库端口，其实也不用开默认就是3306

12、启动zabbix、并查看端口是否正常监听
    [root@node6 ~/zabbix-4.0.1]#service zabbix_server start
    Reloading systemd:                                         [  OK  ]
    Starting zabbix_server (via systemctl):                    [  OK  ]
    [root@node6 ~/zabbix-4.0.1]#ss -tnl
    State       Recv-Q Send-Q          Local Address:Port                Peer Address:Port
    LISTEN      0      128                         *:10051                    *:*
    LISTEN      0      128                         *:111                      *:*
    LISTEN      0      128                         *:22                       *:*
    LISTEN      0      100                 127.0.0.1:25                       *:*

13、装前端展示端
    [root@node6 ~/zabbix-4.0.1]#yum -y install httpd

14、在httpd的默认工作目录下创建一个zabbix目录
    [root@node6 ~/zabbix-4.0.1]#mkdir /var/www/html/zabbix

15、从zabbix解压包里面把php的所有文件拷贝到/var/www/html/zabbix目录下
    [root@node6 ~/zabbix-4.0.1]#cp -a frontends/php/* /var/www/html/zabbix/

16、启动httpd、查看端口是否正常监听
    [root@node6 ~]#systemctl start httpd
    [root@node6 ~]#ss -tnl
    State       Recv-Q Send-Q          Local Address:Port                         Peer Address:Port
    LISTEN      0      128                         *:10051                                   *:*
    LISTEN      0      128                         *:111                                     *:*
    LISTEN      0      128                         *:22                                      *:*
    LISTEN      0      100                 127.0.0.1:25                                      *:*
    LISTEN      0      128                        :::111                                    :::*
    LISTEN      0      128                        :::80                                     :::*

17、通过网页来安装zabbix
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;zabbix使用&#34;&gt;zabbix使用&lt;/h1&gt;

&lt;p&gt;zabbix的使用基本上都是在界面完成操作的，比较简单，基本使用流程&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;添加需要监控的主机&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;为监控的主机添加监控项，也就是key，zabbix自身带有很多设定好的监控项，直接选择就好，比如cpu，内存，都是界面操作&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监控项中可以直接输入参数，来获取指定的数据&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监控项可以自定义key，主要设定key，和执行的脚本命令command，可见zabbix都是通过执行命令来获取监控数据的&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix有对应的告警机制，也就是触发器，设置触发器也就是表达式，达到阈值，就会产生事件，然后可以通过各种通信方式发送，都是支持界面操作。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;zabbix对比promethes&#34;&gt;zabbix对比promethes&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;zabbix采集数据只能通过脚本命令，比较局限，基本都是物理机上的一些命令，所以zabbix比较适合物理机的监控，prometheus不但能够监控物理机，更适合云环境（频繁变动），比如k8s，&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据存储在mysql等关系型数据库中，存储有限，而且很难扩展监控维度，prometheus则是一个时序数据库，还可以远程存储，更适合&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix监控界面不够实时，相比于grafana也是一点都不美观，而且定制化特别难，而grafana则是得到公认的可编辑可扩展美观软件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix集群规模有限，上线为10000个节点，但是promtheus监控节点可以有更大的规模，速度也快。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix已经发展比较成熟，确实在管理界面上比较完善。但是prometheus比较灵活。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;zabbix的报文协议&#34;&gt;zabbix的报文协议&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;cmppingloss[&lt;target&gt;,&lt;packets&gt;,&lt;interval&gt;,&lt;size&gt;,&lt;timeout&gt;] 目标服务器，包数量，包发送间隔，包大小，超时&lt;/li&gt;
&lt;li&gt;value是string，一般是出错信息&lt;/li&gt;
&lt;li&gt;redis.cpunu.discovery这个是一个做发现的配置，最后生成了如下的配置可以舍去  ￼&lt;/li&gt;
&lt;li&gt;state表示在key不支持，或者是监控数据的过程中出错时候会出来&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;zabbix配置文件&#34;&gt;zabbix配置文件&lt;/h1&gt;

&lt;p&gt;zabbix的配置文件一般有三种：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zabbixserver的配置文件zabbix_server.conf

zabbixproxy的配置文件zabbix_proxy.conf

zabbix_agentd的配置文件zabbix_agentd.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.zabbixserver的配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NodeID=0 #分布式节点id号，0代表是独立服务器，默认是被注释掉的，不强制配置
ListenPort=10051 #zabbix server的端口，默认是10051，可以自行修改，
范围是1024-32767 ，一般默认即可
SourceIP=  #连接的源ip地址，默认为空，默认即可
LogFile=/tmp/zabbix_server.log #日志文件的存放位置
LogFileSize=1 #日志文件的大小，单位为MB，当设置为0时，表示不仅行日志轮询，
默认设置为1，默认即可
DebugLevel=3 #指定调试级别，默认即可
PidFile=/tmp/zabbix_server.pid #pid文件的存放位置
DBHost=localhost #数据库主机名，当设置为localhost时，连接mysql通过sock
DBName=zabbix #指定存放zabbix数据数据库的名字
DBUser=zabbix #指定连接数据库的用户名
DBPassword=123456 #用户连接数据库需要的密码
DBSocket=/var/lib/mysql/mysql.sock #前文主机设置为localhost，用户
连接数据库所用的sock位置，
DBPort=3306 #数据库的端口号，当用sock连接时，无关紧要，当通过网络连接时需设置
StartPollers=5 #默认即可
StartIPMIPollers=0 #使用IPMI协议时，用到的参数
StartTrappers=5 #打开的进程数，
StartPingers=1 同上
StartDiscoverers=1
StartHTTPPollers=1
JavaGateway=127.0.0.1 #JavaGateway的ip地址或主机名
JavaGatewayPort=10052 #JavaGateway的端口号
StartJavaPollers=5 #开启连接javagatey的进程数
SNMPTrapperFile=/tmp/zabbix_traps.tmp
StartSNMPTrapper=0 #如果设置为1，snmp trapper进程就会开启
ListenIP=0.0.0.0 #监听来自trapper的ip地址
ListenIP=127.0.0.1
HousekeepingFrequency=1 #zabbix执行Housekeeping的频率，单位为hours
MaxHousekeeperDelete=500 #每次最多删除历史数据的行
SenderFrequency=30 #zabbix试图发送未发送的警报的时间，单位为秒
CacheSize=8M #缓存的大小
CacheUpdateFrequency=60#执行更新缓存配置的时间，单位为秒数
StartDBSyncers=4
HistoryCacheSize=8M
TrendCacheSize=4M
HistoryTextCacheSize=16M
NodeNoEvents=0
NodeNoHistory=0
Timeout=3
TrapperTimeout=300
UnreachablePeriod=45
UnavailableDelay=60
UnreachableDelay=15
AlertScriptsPath=/usr/local/zabbix/shell #脚本的存放路径
FpingLocation=/usr/local/sbin/fping #fping指令的绝对路径
SSHKeyLocation=
LogSlowQueries=0
TmpDir=/tmp
Include=/usr/local/etc/zabbix_server.general.conf
Include=/usr/local/etc/zabbix_server.conf.d/ #子配置文件路径
StartProxyPollers=1 #在zabbix proxy被动模式下用此参数
ProxyConfigFrequency=3600#同上
ProxyDataFrequency=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ListenPort=10051 #监听端口



LogFile=/opt/zabbix/logs/zabbix_server.log

LogFileSize=1024

DebugLevel=3

PidFile=/var/run/zabbix/zabbix_server.pid

#mysql 数据库配置
DBHost=10.243.51.107
DBName=zabbix
DBUser=zabbix
DBPassword=zabbix@suning
DBPort=3306


StartPollers=500

StartIPMIPollers=1

StartPollersUnreachable=100

StartTrappers=100

StartPingers=50

StartDiscoverers=10

StartHTTPPollers=10

StartTimers=10










SNMPTrapperFile=/opt/zabbix/zabbix_traps.tmp

StartSNMPTrapper=1

# 监听地址
ListenIP=0.0.0.0

CacheSize=8G

CacheUpdateFrequency=3600

StartDBSyncers=50

HistoryCacheSize=2G

TrendCacheSize=2G


ValueCacheSize=10G


Timeout=25
TrapperTimeout=120
UnreachablePeriod=300
UnavailableDelay=60
UnreachableDelay=60
AlertScriptsPath=/opt/zabbix/alertscripts
ExternalScripts=/opt/zabbix/externalscripts
FpingLocation=/usr/sbin/fping


LogSlowQueries=10

StartProxyPollers=100
ProxyConfigFrequency=3600
ProxyDataFrequency=30
AllowRoot=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.zabbixagentd的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PidFile=/tmp/zabbix_agentd.pid #pid文件的存放位置
LogFile=/tmp/zabbix_agentd.log #日志文件的位置
LogFileSize=1 #当日志文件达到多大时进行轮询操作
DebugLevel=3 #日志信息级别
SourceIP= #连接的源ip地址，默认为空，即可
EnableRemoteCommands=0 #是否允许zabbix server端的远程指令，
0表示不允许，
1表示允许
LogRemoteCommands=0 #是否开启日志记录shell命令作为警告 0表示不允许，1表示允许
Server=127.0.0.1 #zabbix server的ip地址或主机名，可同时列出多个，需要用逗号隔开
ListenPort=10050 #zabbix agent监听的端口
ListenIP=0.0.0.0 #zabbix agent监听的ip地址
StartAgents=3 #zabbix agent开启进程数
ServerActive=127.0.0.1 #开启主动检查
Hostname=Zabbix server#在zabbix server前端配置时指定的主机名要相同，最重要的配置
RefreshActiveChecks=120 #主动检查刷新的时间，单位为秒数
BufferSend=5 #数据缓冲的时间
BufferSize=100 #zabbix agent数据缓冲区的大小，当达到该值便会发送所有的数据到zabbix server
MaxLinesPerSecond=100 #zabbix agent发送给zabbix server最大的数据行
AllowRoot=0 #是否允许zabbix agent 以root用户运行
Timeout=3 #设定处理超时的时间
Include=/usr/local/etc/zabbix_agentd.userparams.conf
Include=/usr/local/etc/zabbix_agentd.conf.d/ #包含子配置文件的路径
UnsafeUserParameters=0 #是否允许所有字符参数的传递
UserParameter= #指定用户自定义参数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PidFile=/var/run/zabbix/zabbix_agentd.pid

LogFile=/var/log/zabbix/zabbix_agentd.log

LogFileSize=0
Server=10.243.51.50

# 推送指标连接的服务器，格式如下addr:port
ServerActive=10.243.51.48

Hostname=10.243.51.50

HostMetadataItem=system.uname
Timeout=15

AllowRoot=1

Include=/etc/zabbix/zabbix_agentd.d/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.zabbixproxy的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Server=192.168.70.133 #指定zabbix server的ip地址或主机名
Hostname=zabbix-proxy-1.35 #定义监控代理的主机名，需和zabbix server前端配置时指定的节点名相同
LogFile=/tmp/zabbix_proxy.log #指定日志文件的位置
PidFile=/tmp/zabbix_proxy.pid #pid文件的位置
DBName=zabbix_proxy #数据库名
DBUser=zabbix #连接数据库的用户
DBPassword=123456#连接数据库用户的密码
ConfigFrequency=60 #zabbix proxy从zabbix server取得配置数据的频率
DataSenderFrequency=60 #zabbix proxy发送监控到的数据给zabbix server的频率
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#连接server的地址
Server=10.243.51.48

ServerPort=10052

Hostname=10.243.51.48

#启动监听的地址和端口
ListenPort=10051
ListenIP=0.0.0.0

LogFile=/opt/zabbix/logs/zabbix_proxy.log

LogFileSize=1024

DebugLevel=3

PidFile=/var/run/zabbix/zabbix_proxy.pid

#自带数据库
DBHost=localhost
DBName=zabbix_proxy
DBUser=zabbix
DBPassword=zabbix@suning
DBSocket=/opt/mysql/run/mysqld.sock
DBPort=3306



ProxyOfflineBuffer=1


ConfigFrequency=3600

DataSenderFrequency=20


StartPollers=300

StartIPMIPollers=10

StartPollersUnreachable=100

StartTrappers=100

StartPingers=20

StartDiscoverers=50

StartHTTPPollers=100




StartVMwareCollectors=10

VMwareFrequency=60

VMwareCacheSize=256M

CacheSize=8G

StartDBSyncers=10

HistoryCacheSize=2G

HistoryTextCacheSize=2G

Timeout=30

TrapperTimeout=300

UnreachablePeriod=300

UnavailableDelay=60

UnreachableDelay=15

ExternalScripts=/opt/zabbix/externalscripts

FpingLocation=/usr/sbin/fping

LogSlowQueries=0


AllowRoot=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;官网配置文件：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_proxy&#34;&gt;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_proxy&lt;/a&gt;
&lt;a href=&#34;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_server&#34;&gt;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_server&lt;/a&gt;
&lt;a href=&#34;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_agentd&#34;&gt;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_agentd&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>服务发现系列---- Etcd</title>
          <link>https://kingjcy.github.io/post/middleware/serverdiscovery/etcd/</link>
          <pubDate>Tue, 14 Feb 2017 15:32:07 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/serverdiscovery/etcd/</guid>
          <description>&lt;p&gt;ETCD是coreOS开源的用于共享配置和服务发现的分布式，一致性的KV存储系统。是一款类似于zk有望取代复杂的zk的用go语言开发的存储系统。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;etcd有着几方面的优势：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一致性协议： ETCD使用&lt;a href=&#34;https://kingjcy.github.io/post/algorithm/raft/&#34;&gt;Raft&lt;/a&gt;协议， ZK使用ZAB（类PAXOS协议），前者容易理解，方便工程实现；&lt;/li&gt;
&lt;li&gt;运维方面：ETCD方便运维，ZK难以运维；&lt;/li&gt;
&lt;li&gt;项目活跃度：ETCD社区与开发活跃，ZK已经很臃肿维护困难；&lt;/li&gt;
&lt;li&gt;API：ETCD提供HTTP+JSON, gRPC接口，跨平台跨语言，ZK需要使用其客户端；&lt;/li&gt;
&lt;li&gt;访问安全方面：ETCD支持HTTPS访问，ZK在这方面缺失；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;etcd的使用场景和zk相似&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置管理&lt;/li&gt;
&lt;li&gt;服务注册与发现&lt;/li&gt;
&lt;li&gt;分布式队列&lt;/li&gt;
&lt;li&gt;分布式锁&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;h2 id=&#34;单点部署&#34;&gt;单点部署&lt;/h2&gt;

&lt;p&gt;安装比较简单，直接去开源的github上去下在压缩包，然后解压就有对应的可执行文件，可以将可执行文件etcd，etcdctl复制到/usr/bin下面使用&lt;/p&gt;

&lt;h2 id=&#34;集群部署&#34;&gt;集群部署&lt;/h2&gt;

&lt;p&gt;静态部署用命令直接启动&lt;/p&gt;

&lt;p&gt;node1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcd -name niub1 -debug \
-initial-advertise-peer-urls http://node1-ip:2380 \
-listen-peer-urls http://node1-ip:2380 \
-listen-client-urls http://node1-ip:2379,http://127.0.0.1:2379 \
-advertise-client-urls http://node1-ip:2379 \
-initial-cluster-token etcd-cluster-1 \
-initial-cluster niub1=http://node1-ip:2380,niub2=http://node2-ip:2380,niub3=http://node3-ip:2380 \
-initial-cluster-state new  &amp;gt;&amp;gt; ./etcd.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcd -name niub2 -debug \
-initial-advertise-peer-urls http://node2-ip:2380 \
-listen-peer-urls http://node2-ip:2380 \
-listen-client-urls http://node2-ip:2379,http://127.0.0.1:2379 \
-advertise-client-urls http://node2-ip:2379 \
-initial-cluster-token etcd-cluster-1 \
-initial-cluster niub1=http://node1-ip:2380,niub2=http://node2-ip:2380,niub3=http://node3-ip:2380 \
-initial-cluster-state new  &amp;gt;&amp;gt; ./etcd.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcd -name niub3 -debug \
-initial-advertise-peer-urls http://node3-ip:2380 \
-listen-peer-urls http://node3-ip:2380 \
-listen-client-urls http://node3-ip:2379,http://127.0.0.1:2379 \
-advertise-client-urls http://node3-ip:2379 \
-initial-cluster-token etcd-cluster-1 \
-initial-cluster niub1=http://node1-ip:2380,niub2=http://node2-ip:2380,niub3=http://node3-ip:2380 \
-initial-cluster-state new  &amp;gt;&amp;gt; ./etcd.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下基本参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ETCD_NAME –节点名称
ETCD_DATA_DIR -指定节点的数据存储目录
ETCD_LISTEN_PEER_URLS -监听URL，用于与其他节点通讯
ETCD_LISTEN_CLIENT_URLS –暴露自己的同时最好新增一个127.0.0.1的监听地址，便于etcdctl调用，当然用0.0.0.0也是可以的

ETCD_INITIAL_ADVERTISE_PEER_URLS -  告知集群其他节点url
ETCD_INITIAL_CLUSTER -  告知集群其他节点url.
ETCD_INITIAL_CLUSTER_STATE -  静态模式部署 new
ETCD_INITIAL_CLUSTER_TOKEN -集群的识别码
ETCD_ADVERTISE_CLIENT_URLS -告知客户端url, 也就是服务的url,不能包含127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以检查一下对集群情况了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcdctl member list
curl http://10.10.0.14:2379/v2/members
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;两种方式都能返回三个节点的相关情况，也可以使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcdctl cluster-health
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样etcd的集群就搭建成功了。&lt;/p&gt;

&lt;p&gt;正常会将其加入到系统服务中，首先创建设置配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /etc/etcd/etcd.conf

# [member]
ETCD_NAME=&amp;quot;etcd-2&amp;quot;
ETCD_DATA_DIR=&amp;quot;/data/etcd/&amp;quot;
#ETCD_WAL_DIR=&amp;quot;&amp;quot;
#ETCD_SNAPSHOT_COUNT=&amp;quot;10000&amp;quot;
#ETCD_HEARTBEAT_INTERVAL=&amp;quot;100&amp;quot;
#ETCD_ELECTION_TIMEOUT=&amp;quot;1000&amp;quot;
#ETCD_LISTEN_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
#ETCD_LISTEN_CLIENT_URLS=&amp;quot;http://localhost:2379&amp;quot;
ETCD_LISTEN_PEER_URLS=&amp;quot;http://0.0.0.0:7001&amp;quot;
ETCD_LISTEN_CLIENT_URLS=&amp;quot;http://0.0.0.0:4001&amp;quot;
#ETCD_MAX_SNAPSHOTS=&amp;quot;5&amp;quot;
#ETCD_MAX_WALS=&amp;quot;5&amp;quot;
#ETCD_CORS=&amp;quot;&amp;quot;
#
#[cluster]
#ETCD_INITIAL_ADVERTISE_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
ETCD_INITIAL_ADVERTISE_PEER_URLS=&amp;quot;http://172.32.148.128:7001&amp;quot;
# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &amp;quot;test=http://...&amp;quot;
#ETCD_INITIAL_CLUSTER=&amp;quot;default=http://localhost:2380&amp;quot;
ETCD_INITIAL_CLUSTER=&amp;quot;etcd-1=http://172.32.148.127:7001,etcd-2=http://172.32.148.128:7001,etcd-3=http://172.32.148.129:7001,etcd-4=http://172.32.148.130:7001&amp;quot;
ETCD_INITIAL_CLUSTER_STATE=&amp;quot;new&amp;quot;
#ETCD_INITIAL_CLUSTER_TOKEN=&amp;quot;etcd-cluster&amp;quot;
#ETCD_ADVERTISE_CLIENT_URLS=&amp;quot;http://localhost:2379&amp;quot;
ETCD_ADVERTISE_CLIENT_URLS=&amp;quot;http://172.32.148.128:4001&amp;quot;
#ETCD_DISCOVERY=&amp;quot;&amp;quot;
#ETCD_DISCOVERY_SRV=&amp;quot;&amp;quot;
#ETCD_DISCOVERY_FALLBACK=&amp;quot;proxy&amp;quot;
#ETCD_DISCOVERY_PROXY=&amp;quot;&amp;quot;
#ETCD_STRICT_RECONFIG_CHECK=&amp;quot;false&amp;quot;
#ETCD_AUTO_COMPACTION_RETENTION=&amp;quot;0&amp;quot;
.......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后增加开机启动配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /uusr/lib/systemd/system/etcd.service

[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=root
# set GOMAXPROCS to number of processors
#ExecStart=/bin/bash -c &amp;quot;GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\&amp;quot;${
ETCD_NAME}\&amp;quot; --data-dir=\&amp;quot;${
ETCD_DATA_DIR}\&amp;quot; --listen-client-urls=\&amp;quot;${
ETCD_LISTEN_CLIENT_URLS}\&amp;quot;&amp;quot;


ExecStart=/bin/bash -c &amp;quot;GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\&amp;quot;${
ETCD_NAME}\&amp;quot; --data-dir=\&amp;quot;${
ETCD_DATA_DIR}\&amp;quot; --listen-client-urls=\&amp;quot;${
ETCD_LISTEN_CLIENT_URLS}\&amp;quot; --listen-peer-urls=\&amp;quot;${
ETCD_LISTEN_PEER_URLS}\&amp;quot; --advertise-client-urls=\&amp;quot;${
ETCD_ADVERTISE_CLIENT_URLS}\&amp;quot; --initial-advertise-peer-urls=\&amp;quot;${
ETCD_INITIAL_ADVERTISE_PEER_URLS}\&amp;quot; --initial-cluster=\&amp;quot;${
ETCD_INITIAL_CLUSTER}\&amp;quot; --initial-cluster-state=\&amp;quot;${
ETCD_INITIAL_CLUSTER_STATE}\&amp;quot;&amp;quot;


Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然我们可以使用别人的rpm包来安装，就有现成的配置文件，我们在上面修改就行了。&lt;/p&gt;

&lt;p&gt;下面我们来来系统启动etcd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable etcd.service
systemctl start etcd.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以检查集群了，有一些需要主要的地方，一个就是服务的用户要有对应目录的权限。&lt;/p&gt;

&lt;h2 id=&#34;k8s部署&#34;&gt;k8s部署&lt;/h2&gt;

&lt;p&gt;一般部署在每个master节点上，组成一个集群。&lt;/p&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;p&gt;直接参考官方的md文件。就是正常的key/value类型的数据库的使用方法，类似于redis的使用。比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;写数据
#etcdctl put foo bar
#etcdctl put fool bar1 --lease=1234abcd
读数据
#etcdctl get foo
#etcdctl get foo --rev=3
删除数据
#etcdctl del foo
Watch机制
#etcdctl watch foo
租约TTL
#etcdctl lease grant 10
#etcdctl put –lease=3269xxx foo bar
#etcdctl lease revoke 3269xxx
#etcdctl lease keep-alive 3269xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;etcd碎片整理（defragmentation)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ etcdctl defrag Finished defragmenting etcd member[127.0.0.1:2379]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对每个节点的defrag时间需要错开，不能同时进行。&lt;/p&gt;

&lt;h2 id=&#34;client&#34;&gt;client&lt;/h2&gt;

&lt;p&gt;etcd/clientv3 is the official Go etcd client for v3.&lt;/p&gt;

&lt;p&gt;基本实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;time&amp;quot;
    &amp;quot;github.com/coreos/etcd/clientv3&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;context&amp;quot;
    &amp;quot;github.com/coreos/etcd/mvcc/mvccpb&amp;quot;
    &amp;quot;ketang/netWork/0604_Socket/Tool&amp;quot;
)

var (
    dialTimeout = 5 * time.Second
    requestTimeout = 2 * time.Second
    endPoints = []string{&amp;quot;127.0.0.1:2379&amp;quot;} //etcd 默认接受数据的端口2379
)
//添加 删除 查找 前缀 延时

var etcd *clientv3.Client
func main()  {
    fmt.Println(Tool.GetLocalIp())
    var err error
    etcd, err =clientv3.New(clientv3.Config{
        Endpoints:endPoints,
        DialTimeout:dialTimeout,


    })
    if err != nil {
        fmt.Println(err)
    }

    //添加
    err = putValue(&amp;quot;a&amp;quot;, &amp;quot;abc&amp;quot;)
    fmt.Println(err)

    //查找
    result := getValue(&amp;quot;a&amp;quot;)
    fmt.Println(result)

    //删除
    cnt := delValue(&amp;quot;a&amp;quot;)
    fmt.Println(&amp;quot;delete:&amp;quot;, cnt)


    err = putValue(&amp;quot;b1&amp;quot;, &amp;quot;abc1&amp;quot;)
    err = putValue(&amp;quot;b2&amp;quot;, &amp;quot;abc2&amp;quot;)
    err = putValue(&amp;quot;b3&amp;quot;, &amp;quot;abc3&amp;quot;)

    //按前缀查找
    result = getValueWIthPrefix(&amp;quot;b&amp;quot;)
    fmt.Println(result)
    for _,item := range result {
        fmt.Println(string(item.Key),string(item.Value))
    }

    //按前缀删除
    cnt2 := delValueWithPrefix(&amp;quot;b&amp;quot;)
    fmt.Println(&amp;quot;批量删除：&amp;quot;, cnt2)



    //事务处理
    putValue(&amp;quot;user1&amp;quot;, &amp;quot;zhangsan&amp;quot;)
    _,err = etcd.Txn(context.TODO()).
        If(clientv3.Compare(clientv3.Value(&amp;quot;user1&amp;quot;),&amp;quot;=&amp;quot;, &amp;quot;zhangsan&amp;quot;)).
        Then(clientv3.OpPut(&amp;quot;user1&amp;quot;, &amp;quot;zhangsan&amp;quot;)).
        Else(clientv3.OpPut(&amp;quot;user1&amp;quot;, &amp;quot;lisi&amp;quot;)).Commit()

    fmt.Println(err)
    result = getValue(&amp;quot;user1&amp;quot;)
    fmt.Println(&amp;quot;user1:&amp;quot;, string(result[0].Value))


    //lease 设置有效时间
    resp, err:= etcd.Grant(context.TODO(), 1)
    _,err = etcd.Put(context.TODO(), &amp;quot;username&amp;quot;,&amp;quot;wangwu&amp;quot;,clientv3.WithLease(resp.ID))

    time.Sleep(3 * time.Second)

    v := getValue(&amp;quot;username&amp;quot;)
    fmt.Println(&amp;quot;lease:&amp;quot;,v)


    //watch监听的使用
    putValue(&amp;quot;w&amp;quot;, &amp;quot;hello&amp;quot;)
    go func() {
        rch := etcd.Watch(context.Background(),&amp;quot;w&amp;quot;)
        for wresp := range  rch {
            for _,ev := range wresp.Events {
                fmt.Printf(&amp;quot;watch&amp;gt;&amp;gt;w  %s %q %q\n&amp;quot;, ev.Type,ev.Kv, ev.Kv)
            }
        }
    }()

    putValue(&amp;quot;w&amp;quot;, &amp;quot;hello world!&amp;quot;)



    //监听某个key在一定范围内 value的变化
    //putValue(&amp;quot;fo0&amp;quot;, &amp;quot;a&amp;quot;)
    go func() {
        //监听范围 [fo0-fo3)
        rch := etcd.Watch(context.Background(), &amp;quot;fo0&amp;quot;, clientv3.WithRange(&amp;quot;fo3&amp;quot;))

        for wresp := range  rch {
            for _,ev := range wresp.Events {
                fmt.Printf(&amp;quot;watch range  --   %s %q %q\n&amp;quot;, ev.Type,ev.Kv, ev.Kv)
            }
        }
    }()

    putValue(&amp;quot;fo0&amp;quot;, &amp;quot;b&amp;quot;)
    putValue(&amp;quot;fo1&amp;quot;, &amp;quot;b&amp;quot;)
    putValue(&amp;quot;fo2&amp;quot;, &amp;quot;c&amp;quot;)
    putValue(&amp;quot;fo2.5&amp;quot;, &amp;quot;c&amp;quot;)
    putValue(&amp;quot;fo3&amp;quot;, &amp;quot;c&amp;quot;)


    time.Sleep(10 * time.Second)

}


//添加键值对
func putValue(key, value string)  error  {
    _, err := etcd.Put(context.TODO(),key, value)
    return err
}

//查询
func getValue(key string) []*mvccpb.KeyValue  {
    resp, err := etcd.Get(context.TODO(), key)
    if err != nil {
        return  nil
    } else {
        return resp.Kvs
    }
}

// 返回删除了几条数据
func delValue(key string) int64  {
    res,err := etcd.Delete(context.TODO(),key)
    if err != nil {
        return 0
    } else {
        return res.Deleted
    }

}


//按照前缀删除
func delValueWithPrefix(prefix string) int64  {
    res,err := etcd.Delete(context.TODO(),prefix,clientv3.WithPrefix())
    if err != nil {
        fmt.Println(err)
        return 0
    } else {
        return res.Deleted
    }
}

func getValueWIthPrefix(prefix string) []*mvccpb.KeyValue {
    resp, err := etcd.Get(context.TODO(), prefix, clientv3.WithPrefix())
    if err != nil {
        return  nil
    } else {
        return resp.Kvs
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用总结&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conn.(clientv3.Client).Close()

cannot call pointer method on conn.(clientv3.Client)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要使用指针&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conn.(*clientv3.Client).Close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;主要使用的raft协议实现，可以查看&lt;a href=&#34;https://kingjcy.github.io/post/algorithm/raft/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>服务发现系列---- Consul</title>
          <link>https://kingjcy.github.io/post/middleware/serverdiscovery/consul/</link>
          <pubDate>Sun, 12 Feb 2017 16:04:21 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/serverdiscovery/consul/</guid>
          <description>&lt;p&gt;Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对。&lt;/p&gt;

&lt;h1 id=&#34;consul&#34;&gt;consul&lt;/h1&gt;

&lt;p&gt;Consul是一个服务发现和注册的工具，其具有分布式、高扩展性能特点,主要包含如下功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务发现： 支持 http 和 dns 两种协议的服务注册和发现方式。&lt;/li&gt;
&lt;li&gt;监控检查： 支持多种方式的健康检查。&lt;/li&gt;
&lt;li&gt;Key/Value存储： 支持通过HTTP API实现分布式KV数据存储。&lt;/li&gt;
&lt;li&gt;多数据中心支持：支持任意数量数据中心。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Consul 的使用场景&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实例的注册与配置共享&lt;/li&gt;
&lt;li&gt;与 confd 服务集成，动态生成 nginx 和 haproxy 配置文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;优势&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用 Raft 算法来保证一致性, 比复杂的 Paxos 算法更直接. zookeeper 采用的是 Paxos, consul,etcd 使用的则是 Raft.&lt;/li&gt;
&lt;li&gt;支持多数据中心，内外网的服务采用不同的端口进行监听。 多数据中心集群可以避免单数据中心的单点故障,而其部署则需要考虑网络延迟, 分片等情况等. zookeeper 和 etcd 均不提供多数据中心功能的支持.&lt;/li&gt;
&lt;li&gt;支持健康检查. etcd 不提供此功能.&lt;/li&gt;
&lt;li&gt;支持 http 和 dns 协议接口. zookeeper 的集成较为复杂, etcd 只支持 http 协议.&lt;/li&gt;
&lt;li&gt;官方提供web管理界面, etcd 无此功能.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;角色&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;client: 客户端, 无状态, 将 HTTP 和 DNS 接口请求转发给局域网内的服务端集群.&lt;/li&gt;
&lt;li&gt;server: 服务端, 保存配置信息, 高可用集群, 在局域网内与本地客户端通讯, 通过广域网与其他数据中心通讯. 每个数据中心的 server 数量推荐为 3 个或是 5 个.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;consul安装&#34;&gt;Consul安装&lt;/h1&gt;

&lt;p&gt;下载并解压&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## 下载
wget  https://releases.hashicorp.com/consul/1.0.0/consul_1.0.0_linux_amd64.zip?_ga=2.31706621.2141899075.1510636997-716462484.1510636997
## 解压
unzip consul_1.0.0_linux_amd64.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以UI形式后台启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./consul agent -server -ui -bootstrap-expect 1 -data-dir /tmp/consul &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用脚本，正常使用的时候是需要完善参数的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup /opt/consul_1.0.2/consul agent -server -bootstrap-expect 1 -data-dir /opt/consul_1.0.2/data -ui  -http-port=9996 -bind=10.47.178.81 -client 0.0.0.0 -config-dir /opt/consul_1.0.2/consul.d -enable-script-checks  &amp;gt;/opt/consul_1.0.2/logs/start.log  2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看启动状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@iZ2ze74 home]# ./consul members
Node     Address   Status  Type    Build  Protocol  DC   Segment
iZ2ze74  172.17.120.102:8301  alive   server  1.0.0  2         dc1  &amp;lt;all&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Address：节点地址&lt;/li&gt;
&lt;li&gt;Status：alive表示节点健康&lt;/li&gt;
&lt;li&gt;Type：server运行状态是server状态&lt;/li&gt;
&lt;li&gt;DC：dc1表示该节点属于DataCenter1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;查看节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl 127.0.0.1:8500/v1/catalog/nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;h2 id=&#34;注册&#34;&gt;注册&lt;/h2&gt;

&lt;p&gt;consul注册注册service 的方式有多种&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;静态注册&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;创建文件夹consul.d&lt;/p&gt;

&lt;p&gt;添加如下test.json：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;service&amp;quot;:{
    &amp;quot;id&amp;quot;: &amp;quot;node&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;prometheus-node&amp;quot;,
    &amp;quot;address&amp;quot;: &amp;quot;127.0.0.1&amp;quot;,
    &amp;quot;port&amp;quot;: 9100,
    &amp;quot;tags&amp;quot;: [&amp;quot;prometheus-target&amp;quot;],
    &amp;quot;checks&amp;quot;: [
        {
            &amp;quot;http&amp;quot;: &amp;quot;http://127.0.0.1:9100/metrics&amp;quot;,
            &amp;quot;interval&amp;quot;: &amp;quot;15s&amp;quot;
        }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在consul启动命令中，指定配置路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-config-dir=consul.d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动后查看Prometheus 和consul 界面，可以看到target是否引入。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;使用http Api 的方式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;curl -X PUT -d &#39;{&amp;quot;service&amp;quot;:{&amp;quot;id&amp;quot;:&amp;quot;node&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;prometheus-node&amp;quot;,&amp;quot;address&amp;quot;:&amp;quot;127.0.0.1&amp;quot;,&amp;quot;port&amp;quot;:9100,&amp;quot;tags&amp;quot;:[&amp;quot;prometheus-target&amp;quot;],&amp;quot;checks&amp;quot;:[{&amp;quot;http&amp;quot;:&amp;quot;http://127.0.0.1:9100/metrics&amp;quot;,&amp;quot;interval&amp;quot;:&amp;quot;15s&amp;quot;}]}}&#39; http://127.0.0.1:8500/v1/agent/service/register
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;还可以使用各语言版本的sdk:&lt;a href=&#34;https://www.consul.io/api/libraries-and-sdks.html&#34;&gt;https://www.consul.io/api/libraries-and-sdks.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我这里使用JAVA 版本的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;com.orbitz.consul&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;consul-client&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;1.0.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
使用如下：

public class ConsulTest {
  Consul client;

  /**
   * 初始化.
   */
  @Before
  public void init() {

    client = Consul.builder().withHostAndPort(HostAndPort.fromParts(&amp;quot;xx.xx.xx.xx&amp;quot;, 8500)).build();
    //  catalogClient = client.catalogClient();
  }

  @Test
  public void queryAll() {
    Map&amp;lt;String, Service&amp;gt; services = client.agentClient().getServices();
    for (Map.Entry&amp;lt;String, Service&amp;gt; entry : services.entrySet()) {
      System.out.println(&amp;quot;key:&amp;quot; + entry.getKey());
      System.out.println(&amp;quot;value:&amp;quot; + entry.getValue().toString());
    }

  }


  @Test
  public void testDelete() {
    client.agentClient().deregister(&amp;quot;etcd&amp;quot;);
  }


  @Test
  public void testAdd1() {
    String serviceName = &amp;quot;prometheus-etcd&amp;quot;;
    String serviceId = &amp;quot;etcd&amp;quot;;
    Registration.RegCheck single = Registration.RegCheck.http(&amp;quot;http://127.0.0.1:2379/metrics&amp;quot;, 20);
    Registration reg = ImmutableRegistration.builder()
        .check(single)
        .addTags(&amp;quot;prometheus-target&amp;quot;)
        .address(&amp;quot;127.0.0.1&amp;quot;)
        .port(2379)
        .name(serviceName)
        .id(serviceId)
        .build();
    client.agentClient().register(reg);
  }



}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;发现&#34;&gt;发现&lt;/h2&gt;

&lt;p&gt;查询注册的服务&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;通过DNS api&lt;/p&gt;

&lt;p&gt;dig @127.0.0.1 -p 8600 web.service.consul(dns域名)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通过http api&lt;/p&gt;

&lt;p&gt;curl &lt;a href=&#34;http://localhost:8500/v1/catalog/service/web&#34;&gt;http://localhost:8500/v1/catalog/service/web&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有一个connect模块，直接连接，docker和k8s中使用的比较多。&lt;/p&gt;

&lt;h2 id=&#34;健康检查&#34;&gt;健康检查&lt;/h2&gt;

&lt;p&gt;通过调用http api来获取监控状况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl &#39;http://localhost:8500/v1/health/service/web?passing&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;更新consul配置文件&#34;&gt;更新consul配置文件&lt;/h2&gt;

&lt;p&gt;1、可以通过更改配置文件并将SIGHUP代理文件发送到代理来更新服务定义。这使您可以在不停机或不可用于服务查询的情况下更新服务。现在还支持consul reload&lt;/p&gt;

&lt;p&gt;2、HTTP API可用于动态添加，删除和修改服务。&lt;/p&gt;

&lt;h2 id=&#34;常用命令&#34;&gt;常用命令&lt;/h2&gt;

&lt;p&gt;consul&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;agent：运行一个consul agent&lt;/li&gt;
&lt;li&gt;join：将agent加入到consul cluster&lt;/li&gt;
&lt;li&gt;members：列出consul cluster集群中的members&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;常用选项option：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-data-dir

作用：指定agent储存状态的数据目录
这是所有agent都必须的
对于server尤其重要，因为他们必须持久化集群的状态
-config-dir

作用：指定service的配置文件和检查定义所在的位置
通常会指定为&amp;quot;某一个路径/consul.d&amp;quot;（通常情况下，.d表示一系列配置文件存放的目录）
-config-file

作用：指定一个要装载的配置文件
该选项可以配置多次，进而配置多个配置文件（后边的会合并前边的，相同的值覆盖）
-dev

作用：创建一个开发环境下的server节点
该参数配置下，不会有任何持久化操作，即不会有任何数据写入到磁盘
这种模式不能用于生产环境（因为第二条）
-bootstrap-expect

作用：该命令通知consul server我们现在准备加入的server节点个数，该参数是为了延迟日志复制的启动直到我们指定数量的server节点成功的加入后启动。
-node

作用：指定节点在集群中的名称
该名称在集群中必须是唯一的（默认采用机器的host）
推荐：直接采用机器的IP
-bind

作用：指明节点的IP地址
-server

作用：指定节点为server
每个数据中心（DC）的server数推荐为3或5（理想的是，最多不要超过5）
所有的server都采用raft一致性算法来确保事务的一致性和线性化，事务修改了集群的状态，且集群的状态保存在每一台server上保证可用性
server也是与其他DC交互的门面（gateway）
-client

作用：指定节点为client
若不指定为-server，其实就是-client
-join

作用：将节点加入到集群
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;prometheus原生就支持consul的服务发现，可以直接获取consul上的配置，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrape_configs:
  # The job name is added as a label `job=&amp;lt;job_name&amp;gt;` to any timeseries scraped from this config.
  - job_name: &#39;prometheus&#39;

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    # metrics_path defaults to &#39;/metrics&#39;
    # scheme defaults to &#39;http&#39;.

    static_configs:
      - targets: [&#39;localhost:9090&#39;]

  - job_name: &#39;security&#39;
    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    metrics_path: &#39;/prometheus&#39;
    # scheme defaults to &#39;http&#39;.

    static_configs:
      - targets: [&#39;10.94.20.33:80&#39;]

  - job_name: &#39;overwritten-default&#39;
    consul_sd_configs:
    - server:   &#39;10.110.200.29:8500&#39;
      services: [&#39;lookup&#39;, &#39;security&#39;, &#39;workflow&#39;]

    relabel_configs:
    - source_labels: [&#39;__metrics_path__&#39;]
      regex:         &#39;/metrics&#39;
      target_label:  __metrics_path__
      replacement:   &#39;/prometheus&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里简单说明下上面的配置意义，在scrape_configs下，定义了3个job_name&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;job_name: &amp;lsquo;prometheus&amp;rsquo;是监听prometheus服务本身；&lt;/li&gt;
&lt;li&gt;job_name: &amp;lsquo;security&amp;rsquo;是按固定IP:PORT的方式监听微服务 ；&lt;/li&gt;
&lt;li&gt;job_name: &amp;lsquo;overwritten-default&amp;rsquo;就是一个监听consul的任务，在consul_sd_configs下，server是consul服务器的访问地址，services是微服务名的数组，如果什么都不填，则默认取consul上注册的所有微服务。relabel_configs是修改默认配置的规则，这里由于使用了springboot和promethues整合，暴露的metrics是通过/promethues路径访问的，而promethues默认的metrics访问路径（即metrics_path配置项）是/metrics，需要修改。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;consul-template&#34;&gt;consul-template&lt;/h2&gt;

&lt;p&gt;consul-template可以启动多个程序用于生成不同的模版json文件.&lt;/p&gt;

&lt;p&gt;consul-template的两种使用方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;直接启动作为进程使用，配置文件中间有一个时间设定，多长时间更新一次&lt;/li&gt;
&lt;li&gt;使用crontab来定时拉去一次，使用启动参数-once&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用consul+consul-template可以构建一套基于文件服务发现的动态注册和配置生成的服务发现功能。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过consul的api向consul上注册prometheus的采集信息，使用k/v的模式&lt;/li&gt;
&lt;li&gt;consul-template设置定时功能，命令行拉去consul上的配置，按着模版的形式生成json文件&lt;/li&gt;
&lt;li&gt;prometheus使用fd的服务发现模式读取json文件拉去采集信息&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;什么时候使用k/v模式？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用consul的services注册job的服务信息，然后使用consul-template动态生成prometheus的配置文件。然后prometheus通过查询consul中注册的信息正则匹配来完成prometheus的采集操作，在规模请求很小的时候，service完全没有问题
但是这样当job量很大的时候，比如有20组job，一组job130的target的的时候，就会出现consul请求瓶颈。&lt;/p&gt;

&lt;p&gt;所以在规模扩大的时候使用consul的k/v格式进行注册，直接通过IP：port作为key，对应的label作为vaule，然后使用consul-template动态生成discovery的json文件，然后prometheus使用file sd来发现这个json文件，相当于将对应的json的内容写到了prometheus的配置文件中去，这个时候五分钟consul-template动态生成一次，不会每次都去请求，这样consul的压力就几乎没有了，经过测试可以达到5000个target，prometheus的shard极限，对consul依旧没有什么压力，现在主要瓶颈在于json文件大小，filesd的压力，可以继续优化成多个文件。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;raft算法&#34;&gt;raft算法&lt;/h2&gt;

&lt;p&gt;Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选人（Candidate）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Leader：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。&lt;/li&gt;
&lt;li&gt;Follower：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。&lt;/li&gt;
&lt;li&gt;Candidate：Leader选举过程中的临时角色。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Raft实现了和Paxos相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等,核心就是Leader选举，可以&lt;a href=&#34;https://kingjcy.github.io/post/algorithm/raft/&#34;&gt;详细了解raft&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Raft 使用心跳（heartbeat）触发Leader选举。当服务器启动时，初始化为Follower。Leader向所有Followers周期性发送heartbeat。如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举。&lt;/p&gt;

&lt;p&gt;Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC 。结果有以下三种情况：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;赢得了多数的选票，成功选举为Leader；&lt;/li&gt;
&lt;li&gt;收到了Leader的消息，表示有其它服务器已经抢先当选了Leader；&lt;/li&gt;
&lt;li&gt;没有服务器赢得多数的选票，Leader选举失败，等待选举时间超时后发起下一次选举。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>服务发现系列---- Sd</title>
          <link>https://kingjcy.github.io/post/middleware/serverdiscovery/sd/</link>
          <pubDate>Sun, 12 Feb 2017 16:04:18 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/serverdiscovery/sd/</guid>
          <description>&lt;p&gt;服务发现就是程序如何通过一个标志来获取服务列表，并且这个服务列表是能够随着服务的状态而动态变更，最终得以调用到相应的服务。&lt;/p&gt;

&lt;p&gt;服务发现是在分布式系统规模越来越大的情况下，服务治理的必然产物，不然服务的配置调用将难以维护。&lt;/p&gt;

&lt;h1 id=&#34;服务发现&#34;&gt;服务发现&lt;/h1&gt;

&lt;p&gt;服务发现可以分为注册和解析两个部分。&lt;/p&gt;

&lt;h2 id=&#34;服务注册&#34;&gt;服务注册&lt;/h2&gt;

&lt;p&gt;存储的信息是域名和ip的对应映射关系，存储的解析信息至少包括正在运行的服务的主机ip和端口信息。&lt;/p&gt;

&lt;p&gt;注册的方式有两种&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;上报，每个服务启动后主动将自己的域名和ip信息上报存储下来&lt;/li&gt;
&lt;li&gt;监听，服务监听集群中服务的创建，并且获取相关信息存储下来，比如coreDNS&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;解析发现&#34;&gt;解析发现&lt;/h2&gt;

&lt;p&gt;服务发现主要存在有两种模式，客户端模式与服务端模式，两者的本质区别在于，客户端是否保存服务列表信息。&lt;/p&gt;

&lt;h3 id=&#34;客户端模式&#34;&gt;客户端模式&lt;/h3&gt;

&lt;p&gt;在客户端模式下，如果要进行微服务调用，首先要进行的是到服务注册中心获取服务列表，然后再根据调用端本地的负载均衡策略，进行服务调用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;客户端(自身有服务注册中心--可以获取服务列表)------&amp;gt;调用服务
   |
   |
   |
服务注册中心--服务列表
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们用图可以看出来&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/sd/sd&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;只需要周期性获取列表，在调用服务时可以直接调用少了一个RT。但需要在每个客户端维护获取列表的逻辑&lt;/li&gt;
&lt;li&gt;可用性高，即使注册中心出现故障也能正常工作&lt;/li&gt;
&lt;li&gt;服务上下线对调用方有影响（会出现短暂调用失败）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;大部分服务发现的实现都采取了客户端模式&lt;/p&gt;

&lt;h3 id=&#34;服务端模式&#34;&gt;服务端模式&lt;/h3&gt;

&lt;p&gt;在服务端模式下，调用方直接向服务注册中心进行请求，服务注册中心再通过自身负载均衡策略，对微服务进行调用。这个模式下，调用方不需要在自身节点维护服务发现逻辑以及服务注册信息，这个模式相对来说比较类似DNS模式。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;客户端(自身没有服务注册中心)------&amp;gt;服务注册中心(在服务端)------&amp;gt;调用服务
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/sd/sd1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;简单，不需要在客户端维护获取服务列表的逻辑&lt;/li&gt;
&lt;li&gt;可用性由路由器中间件决定，路由中间件故障则所有服务不可用，同时，由于所有调度以及存储都由中间件服务器完成，中间件服务器可能会面临过高的负载&lt;/li&gt;
&lt;li&gt;服务上下线调用方无感知&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;框架&#34;&gt;框架&lt;/h2&gt;

&lt;p&gt;目前服务发现框架：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;consul---------go语言编写，常用，简单，无依赖，集成了http/DNS library
etcd-----------go语言编写，常用，简单，无依赖，集成了Client Binging／http
Zookeeper------java语言编写，常用，简单，依赖jvm，集成了Client Binging
Eureka---------java
smartstsck-----ruby
nsq------------go
serf-----------go
spotify--------DNS
skydns---------go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前三个都是比较常用和通用的，其他都是自己造轮子，适合特定场景。&lt;/p&gt;

&lt;h3 id=&#34;eureka&#34;&gt;Eureka&lt;/h3&gt;

&lt;p&gt;eureka是netflix用于服务注册和发现的框架。在这个框架中，分为server和client两种角色。server负责保存服务的注册信息，同时server之间也可以彼此相互注册，client则需要向特定的server进行注册。&lt;/p&gt;

&lt;p&gt;client/server通过RESTful Api向server进行服务注册，并且定期调用renew接口来更新服务的注册状态，若server在60s内没有收到服务的renew信息，则该服务就会被标志为下线。而如果服务需要主动下线的话，向server调用cancel就可以了。&lt;/p&gt;

&lt;h3 id=&#34;consul&#34;&gt;Consul&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/middleware/serverdiscovery/consul/&#34;&gt;Consul&lt;/a&gt;是强一致性的数据存储，使用Raft形成动态集群。它提供分级键/值存储方式，不仅可以存储数据，而且可以用于注册器件事各种任务，从发送数据改变通知到运行健康检查和自定义命令，具体如何取决于它们的输出。&lt;/p&gt;

&lt;p&gt;与Zookeeper和etcd不一样，Consul内嵌实现了服务发现系统，所以这样就不需要构建自己的系统或使用第三方系统。这一发现系统除了上述提到的特性之外，还包括节点健康检查和运行在其上的服务。&lt;/p&gt;

&lt;p&gt;Zookeeper和etcd只提供原始的键/值队存储，要求应用程序开发人员构建他们自己的系统提供服务发现功能。而Consul提供了一个内置的服务发现的框架。客户只需要注册服务并通过DNS或HTTP接口执行服务发现。其他两个工具需要一个亲手制作的解决方案或借助于第三方工具。&lt;/p&gt;

&lt;p&gt;Consul为多种数据中心提供了开箱即用的原生支持，其中的gossip系统不仅可以工作在同一集群内部的各个节点，而且还可以跨数据中心工作。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;模版Consul-template&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;confd可以像和etcd搭配一样用于Consul，不过Consul有自己的模板服务，其更适配Consul。&lt;/p&gt;

&lt;p&gt;通过从Consul获得的信息，Consul-template是一个非常方便的创建文件的途径，还有一个额外的好处就是在文件更新后可以运行任意命令，正如confd，Consul-template也可以使用Go模板格式。&lt;/p&gt;

&lt;h3 id=&#34;zookeeper&#34;&gt;Zookeeper&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/middleware/serverdiscovery/zookeeper/&#34;&gt;Zookeeper&lt;/a&gt;是这种类型的项目中历史最悠久的之一，它起源于Hadoop，帮助在Hadoop集群中维护各种组件。它非常成熟、可靠，被许多大公司（YouTube、eBay、雅虎等）使用。其数据存储的格式类似于文件系统，如果运行在一个服务器集群中，Zookeper将跨所有节点共享配置状态，每个集群选举一个领袖，客户端可以连接到任何一台服务器获取数据。&lt;/p&gt;

&lt;p&gt;Zookeeper的主要优势是其成熟、健壮以及丰富的特性，然而，它也有自己的缺点，其中采用Java开发以及复杂性是罪魁祸首。尽管Java在许多方面非常伟大，然后对于这种类型的工作还是太沉重了，Zookeeper使用Java以及相当数量的依赖使其对于资源竞争非常饥渴。因为上述的这些问题，Zookeeper变得非常复杂，维护它需要比我们期望从这种类型的应用程序中获得的收益更多的知识。这部分地是由于丰富的特性反而将其从优势转变为累赘。应用程序的特性功能越多，就会有越大的可能性不需要这些特性，因此，我们最终将会为这些不需要的特性付出复杂度方面的代价。&lt;/p&gt;

&lt;p&gt;Zookeeper为其他项目相当大的改进铺平了道路，“大数据玩家“在使用它，因为没有更好的选择。今天，Zookeeper已经老态龙钟了，我们有了更好的选择。&lt;/p&gt;

&lt;h3 id=&#34;etcd&#34;&gt;etcd&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/middleware/serverdiscovery/etcd/&#34;&gt;etcd&lt;/a&gt;是一个采用HTTP协议的健/值对存储系统，它是一个分布式和功能层次配置系统，可用于构建服务发现系统。其很容易部署、安装和使用，提供了可靠的数据持久化特性。它是安全的并且文档也十分齐全。&lt;/p&gt;

&lt;p&gt;etcd比Zookeeper是比更好的选择，因为它很简单，然而，它需要搭配一些第三方工具才可以提供服务发现功能。比如Confd&lt;/p&gt;

&lt;p&gt;Confd是一个轻量级的配置管理工具，常见的用法是通过使用存储在etcd、consul和其他一些数据登记处的数据保持配置文件的最新状态，它也可以用来在配置文件改变时重新加载应用程序。换句话说，我们可以用存储在etcd（或者其他注册中心）的信息来重新配置所有服务。&lt;/p&gt;

&lt;h3 id=&#34;对比&#34;&gt;对比&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;功能\组件&lt;/th&gt;
&lt;th&gt;Zookeeper&lt;/th&gt;
&lt;th&gt;etcd&lt;/th&gt;
&lt;th&gt;Consul&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;产生时间&lt;/td&gt;
&lt;td&gt;长&lt;/td&gt;
&lt;td&gt;短&lt;/td&gt;
&lt;td&gt;短&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;原生语言&lt;/td&gt;
&lt;td&gt;JAVA&lt;/td&gt;
&lt;td&gt;Go&lt;/td&gt;
&lt;td&gt;Go&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;算法&lt;/td&gt;
&lt;td&gt;Paxos&lt;/td&gt;
&lt;td&gt;Raft&lt;/td&gt;
&lt;td&gt;Raft&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;多数据中心&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;健康检查&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;web管理界面&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;http协议&lt;/td&gt;
&lt;td&gt;较为复杂&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DNS协议&lt;/td&gt;
&lt;td&gt;较为复杂&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;对比的情况下，我们在k8s的集群中可以使用etcd做服务发现，但是在常规情况下，consul更加的全面直接的做服务发现，比如自带服务发现功能，支持多数据中心，直接界面友好等。&lt;/p&gt;

&lt;h2 id=&#34;高可用&#34;&gt;高可用&lt;/h2&gt;

&lt;p&gt;在多注册中心（server）的情况下，单个server在接收到服务的注册/更新信息的时候，它还会将这些信息同步给同样为server的peer(replicate to peer)，为了避免广播风暴，这些信息只会传递一次，也就是说，接收到的server，不会再同步给自身的peer。&lt;/p&gt;

&lt;p&gt;服务注册完成之后，当client需要进行服务调用的时候，就可以向server获取当前的服务列表，再根据服务列表中的ip地址以及端口号进行调用了。&lt;/p&gt;

&lt;p&gt;Consul是目前较为流行的一个服务发现以及配置工具，Consul能够承担包括服务注册与发现、健康检查（health check)以及键值对存储等，同时，Consul还支持多个数据中心。&lt;/p&gt;

&lt;p&gt;我们可以通过Consul的Restful Api（&lt;code&gt;curl -request PUT http://consul/v1//agent/service/register&lt;/code&gt;） 向Consul Agent注册服务信息，提交服务的端口号，ip地址，以及健康检查的方式。随后，这个Client就按照配置中的周期以及方式执行健康检查，当健康检查失败的时候，就会像Server Agent发送服务不可用的消息，这个服务就会被Consul标记为不可用了。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Compress</title>
          <link>https://kingjcy.github.io/post/golang/go-compress/</link>
          <pubDate>Sun, 29 Jan 2017 10:39:28 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-compress/</guid>
          <description>&lt;p&gt;archive一般用于打包，compress一般用于压缩。&lt;/p&gt;

&lt;h1 id=&#34;bzip2&#34;&gt;bzip2&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;bzip2的简介&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;bzip2包实现bzip2的解压缩，bzip2是对单个文件进行压缩，可以先进行tar归档，然后进行压缩。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;bzip2的使用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;go标准库中提供了一个对bzip2压缩包进行读取的操作，但是并没有提供进行bzip2压缩操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;compress/bzip2&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;fmt&amp;quot;
)

func main() {
    fz, err := os.Open(&amp;quot;1.go.bz2&amp;quot;)
    if err != nil {
        log.Fatal(err)
    }
    w := bzip2.NewReader(fz)
    buf := make([]byte, 1024 * 100)
    for {
        n, err := w.Read(buf)
        if n == 0 || err != nil {
            break
        }
        fmt.Println(string(buf[:n]))
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;deflate&#34;&gt;deflate&lt;/h1&gt;

&lt;p&gt;DEFLATE是同时使用了LZ77算法与哈夫曼编码（Huffman Coding）的一个无损数据压缩算法。它最初是由菲尔·卡茨（Phil Katz）为他的PKZIP软件第二版所定义的，后来被RFC 1951标准化。很多压缩方式都是在这个基础上封装开发的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    NoCompression = 0        // 不压缩
    BestSpeed          = 1        // 最快速度压缩
    BestCompression     = 9   // 最佳压缩比压缩
    DefaultCompression = -1  // 默认压缩
)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;NewReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(r io.Reader) io.ReadCloser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r deflate压缩文件的文件标识符
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;解压后的ReadCloser数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;从r读取deflate压缩数据，返回一个解压过的io.ReadCloser，使用后需要调用关闭该io.ReadCloser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;bytes&amp;quot;
    &amp;quot;compress/flate&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;os&amp;quot;
)

func main() {
    // 一个缓存区压缩的内容
    buf := bytes.NewBuffer(nil)

    // 创建一个flate.Writer
    flateWrite, err := flate.NewWriter(buf, flate.BestCompression)
    if err != nil {
        log.Fatalln(err)
    }
    defer flateWrite.Close()
    // 写入待压缩内容
    flateWrite.Write([]byte(&amp;quot;compress/flate\n&amp;quot;))
    flateWrite.Flush()
    fmt.Printf(&amp;quot;压缩后的内容：%s\n&amp;quot;, buf)

    // 解压刚压缩的内容
    flateReader := flate.NewReader(buf)
    defer flateWrite.Close()
    // 输出
    fmt.Print(&amp;quot;解压后的内容：&amp;quot;)
    io.Copy(os.Stdout, flateReader)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;NewReaderDict&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewReaderDict(r io.Reader, dict []byte) io.ReadCloser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r deflate压缩的数据
dict 解压数据时预设的字典，和NewWriteDict函数里得dict相同
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;解压后ReadCloser数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;从r读取deflate压缩数据，使用预设得dict字典压缩数据，返回一个压缩过得io.ReadCloser，使用后需要调用者关闭该io.ReadCloser。主要用来读取NewWriteDict压缩的数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;demo：
package main

import (
    &amp;quot;bytes&amp;quot;
    &amp;quot;compress/flate&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;os&amp;quot;
)

func main() {
    // 一个缓冲区存储压缩的内容
    buf := bytes.NewBuffer(nil)

    // 创建一个flate.Write
    flateWrite, err := flate.NewWriterDict(buf, flate.BestCompression, []byte(&amp;quot;key&amp;quot;))
    if err != nil {
        log.Fatalln(err)
    }
    defer flateWrite.Close()
    // 写入待压缩内容
    flateWrite.Write([]byte(&amp;quot;compress/flate\n&amp;quot;))
    flateWrite.Flush()
    fmt.Println(buf)

    // 解压刚压缩的内容
    flateReader := flate.NewReaderDict(buf, []byte(&amp;quot;key&amp;quot;))
    defer flateReader.Close()
    // 输出
    io.Copy(os.Stdout, flateReader)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;NewWrite&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewWrite(w io.Write, level int) (*Write, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1）w 表示输出数据的Write
2）level 表示压缩级别
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1）*Write 基于压缩级别新生成的压缩数据的Writer
2）error 表示该函数的错误信息
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;该函数返回一个压缩级别为level的新的压缩用的Writer，压缩级别的范围时1（BestSpeed）to 9（BestCompression）。压缩效果越好的意味着压缩速度越慢。0（NoCompression）表示不做任何压缩；仅仅只需要添加必要的deflate信息，-1（DefaultCompression）表示用默认的压缩级别。如果压缩级别在-1~9的范围内，error返回nil，否则将返回非nil的错误信息。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;bytes&amp;quot;
    &amp;quot;compress/flate&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;fmt&amp;quot;
)

func main() {
    // 一个缓冲区压缩的内容
    buf := bytes.NewBuffer(nil)

    // 创建一个flate.Writer，压缩级别最好
    flateWrite, err := flate.NewWriter(buf, flate.BestCompression)
    if err != nil {
        log.Fatalln(err)
    }
    defer flateWrite.Close()
    // 写入待压缩内容
    flateWrite.Write([]byte(&amp;quot;compress/flate\n&amp;quot;))
    flateWrite.Flush()
    fmt.Println(buf)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;NewWriteDict&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewWriteDict(w io.Writer, level int, dict []byte) (*Writer, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1）w 代表输出数据的Writer
2）level 代表压缩级别
3）dict 代表压缩预设字典
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1）*Writer 基于压缩级别和预设字典新生成的压缩数据的Writer
2）error 该函数的错误信息
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;该函数和NewWriter差不多，只不过使用了预设字典进行初始化Writer。使用该Writer压缩的数据只能被使用相同字典初始化的Reader解压。可以实现基于密码的解压缩。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;bytes&amp;quot;
    &amp;quot;compress/flate&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;fmt&amp;quot;
)

func main() {
    // 一个缓冲区存储压缩的内容
    buf := bytes.NewBuffer(nil)

    // 创建一个flate.Writer，压缩级别最好
    flateWriter, err := flate.NewWriterDict(buf, flate.BestCompression, []byte(&amp;quot;key&amp;quot;))
    if err != nil {
        log.Fatalln(err)
    }
    defer flateWriter.Close()
    // 写入待压缩内容
    flateWriter.Write([]byte(&amp;quot;compress/flate\n&amp;quot;))
    flateWriter.Flush()
    fmt.Println(buf)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;InternalError Error&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;func (e InternalError) Error() string&lt;/p&gt;

&lt;p&gt;返回值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;表示flate数据自身的错误信息
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InternalError其实是一个string，他实现了error接口，用于很方便的返回flate数据自身的错误信息
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadError Error&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (e *ReadError) Error() string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;表示flate读取拷贝数据时的错误信息
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ReadError其实是一个struct，他实现了error接口，用于很方便的返回flate读取拷贝数据时的错误信息
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;WriteError Error&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (e *WriteError) Error() string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;表示flate输出数据的错误信息
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WriteError是一个struct，他实现了error接口，用于很方便的返回flate输出数据的错误信息
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;close&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (w *Writer) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;返回一个error，没有错误时返回nil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;刷新缓冲并关闭w
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Flush&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (w *Writer) Flush() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;返回一个error，没有错误时该error为nil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Flush将缓存中的压缩数据刷新到下层的io.writer中。它主要用在压缩的网络协议中，目的时确保远程读取器有足够的数据重建一个数据包。Flush是阻塞的，直到缓冲中的数据都被写入到下层io.writer中才返回。如果下层io.writer返回一个error，那么Flush也会返回该error。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在zlib库的术语中，Flush等同于Z_SYNC_FLUSH。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Reset&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (w *Writer) Reset(dst io.Writer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1）dst 重置时将为作w的下层io.Writer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reset会丢弃现在w的状态，这相当于把dst、w的级别和字典作为参数，重新调用NewWriter或者NewWriterDict函数一样。
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Write&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (w *Writer) Write(data []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1）data 代表要写入的字节数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1）n 写入的字节数
2）err 错误信息，无错误返回nil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Write向w写入数据，最终会将压缩格数的数据写入到w的下层io.Writer中
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;snappy&#34;&gt;snappy&lt;/h1&gt;

&lt;p&gt;google 自家的snappy 压缩优点是非常高的速度和合理的压缩率。压缩率比gzip 小，CPU 占用小。&lt;/p&gt;

&lt;p&gt;下面是对几个简单的字符串做snappy 压缩前后对比：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;github.com/golang/snappy&amp;quot;
    &amp;quot;io/ioutil&amp;quot;
)

var (
    textMap = map[string]string{
        &amp;quot;a&amp;quot;: `1234567890-=qwertyuiop[]\&#39;;lkjhgfdsazxcvbnm,./`,
        &amp;quot;b&amp;quot;: `1234567890-=qwertyuiop[]\&#39;;lkjhgfdsazxcvbnm,./1234567890-=qwertyuiop[]\&#39;;lkjhgfdsazxcvbnm,./1234567890-=qwertyuiop[]\&#39;;lkjhgfdsazxcvbnm,./1234567890-=qwertyuiop[]\&#39;;lkjhgfdsazxcvbnm,./`,
        &amp;quot;c&amp;quot;: `浕浉浄浀浂洉洡洣浐洘泚浌洼洽派洿浃浇浈浊测浍济浏浑浒浓浔泿洱涏洀洁洂洃洄洅洆洇洈洊洋洌洎洏洐洑洒洓洔洕洗洠洙洚洛洝洞洟洢洤津洦洧洨洩洪洫洬洭洮洲洳洴洵洶洷洸洹洺活涎`,
        &amp;quot;d&amp;quot;: `浕浉浄浀浂洉洡洣浐洘泚浌洼洽派洿浃浇浈浊测浍济浏浑浒浓浔泿洱涏洀洁洂洃洄洅洆洇洈洊洋洌洎洏洐洑洒洓洔洕洗洠洙洚洛洝洞洟洢洤津洦洧洨洩洪洫洬洭洮洲洳洴洵洶洷洸洹洺活涎浕浉浄浀浂洉洡洣浐洘泚浌洼洽派洿浃浇浈浊测浍济浏浑浒浓浔泿洱涏洀洁洂洃洄洅洆洇洈洊洋洌洎洏洐洑洒洓洔洕洗洠洙洚洛洝洞洟洢洤津洦洧洨洩洪洫洬洭洮洲洳洴洵洶洷洸洹洺活涎浕浉浄浀浂洉洡洣浐洘泚浌洼洽派洿浃浇浈浊测浍济浏浑浒浓浔泿洱涏洀洁洂洃洄洅洆洇洈洊洋洌洎洏洐洑洒洓洔洕洗洠洙洚洛洝洞洟洢洤津洦洧洨洩洪洫洬洭洮洲洳洴洵洶洷洸洹洺活涎`,
    }
    imgSrc = []string{
        &amp;quot;1.jpg&amp;quot;, &amp;quot;2.jpg&amp;quot;, &amp;quot;3.jpg&amp;quot;, &amp;quot;4.jpg&amp;quot;,
    }
)

func main() {

    for k, v := range textMap {
        got := snappy.Encode(nil, []byte(v))
        fmt.Println(&amp;quot;k:&amp;quot;, k, &amp;quot;len:&amp;quot;, len(v), len(got))
    }

    fmt.Println(&amp;quot;snappy jpg&amp;quot;)
    for _, v := range imgSrc {
        buf, err := ioutil.ReadFile(v)
        if err == nil {
            got := snappy.Encode(nil, buf)
            fmt.Println(&amp;quot;k:&amp;quot;, v, &amp;quot;len:&amp;quot;, len(buf), len(got))
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;k: a len: 46 48
k: b len: 184 58
k: c len: 246 250
k: d len: 738 274
snappy jpg
k: 1.jpg len: 302829 282525
k: 2.jpg len: 89109 89051
k: 3.jpg len: 124463 123194
k: 4.jpg len: 420886 368608
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果字符串包含重复字符多压缩才看到效果，对jpg 图片的压缩率不大。&lt;/p&gt;

&lt;p&gt;对一个实际使用的数据库是否使用snappy 做对比，用户和文章都是10万，文章内容较简单。&lt;/p&gt;

&lt;p&gt;使用snappy 压缩前：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用时 4m32.916312692s
数据库占用空间 176,209,920 字节（磁盘上的 172 MB）
使用snappy 压缩后：

用时 4m6.750271414s
数据库占用空间 159,424,512 字节（磁盘上的 150.9 MB）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从使用时间上看，此例压缩使用的CPU 时间小于数据压缩后省下来的数据存储IO 占用的时间。因为文章数据较短、内容简单，压缩效果不明显。&lt;/p&gt;

&lt;h1 id=&#34;compress-gzip&#34;&gt;compress/gzip&lt;/h1&gt;

&lt;p&gt;golang系统自带的包里边compress/gzip就可以实现在代码中实现gzip的功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Example_writerReader() {
    var buf bytes.Buffer
    zw := gzip.NewWriter(&amp;amp;buf)

    // Setting the Header fields is optional.
    zw.Name = &amp;quot;a-new-hope.txt&amp;quot;
    zw.Comment = &amp;quot;an epic space opera by George Lucas&amp;quot;
    zw.ModTime = time.Date(1977, time.May, 25, 0, 0, 0, 0, time.UTC)

    _, err := zw.Write([]byte(&amp;quot;A long time ago in a galaxy far, far away...&amp;quot;))
    if err != nil {
        log.Fatal(err)
    }

    if err := zw.Close(); err != nil {
        log.Fatal(err)
    }

    zr, err := gzip.NewReader(&amp;amp;buf)
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf(&amp;quot;Name: %s\nComment: %s\nModTime: %s\n\n&amp;quot;, zr.Name, zr.Comment, zr.ModTime.UTC())

    if _, err := io.Copy(os.Stdout, zr); err != nil {
        log.Fatal(err)
    }

    if err := zr.Close(); err != nil {
        log.Fatal(err)
    }

    // Output:
    // Name: a-new-hope.txt
    // Comment: an epic space opera by George Lucas
    // ModTime: 1977-05-25 00:00:00 +0000 UTC
    //
    // A long time ago in a galaxy far, far away...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;方法说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewWriter(w io.Writer) *Writer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewWriter返回一个Writer接口，调用Writer的write方法把数据压缩后写入w。并由调用者负责关闭接口，有另外一个创建Writer的接口func NewWriterLevel(w io.Writer, level int) (*Writer, error) 其使用方法和NewWriter一样，就是增加了一个压缩级别。压缩级别和gzipgzip的压缩级别定义一致。具体可以看每天回顾linux命令（gzip） 中对压缩效率的介绍。&lt;/p&gt;

&lt;p&gt;Writer接口方法介绍&lt;/p&gt;

&lt;p&gt;1、func (z *Writer) Close() error&lt;/p&gt;

&lt;p&gt;关闭Writer接口，但是不关闭创建接口是引入的io.Writer。&lt;/p&gt;

&lt;p&gt;2、func (z *Writer) Flush() error&lt;/p&gt;

&lt;p&gt;把缓存中的数据刷到初始化时候传入的io.Writer中。注意，这个操作是个阻塞操作。&lt;/p&gt;

&lt;p&gt;3、func (z *Writer) Reset(w io.Writer)&lt;/p&gt;

&lt;p&gt;刷新Writer的状态为初始状态，并更替其内部的io.Writer&lt;/p&gt;

&lt;p&gt;4、func (z *Writer) Write(p []byte) (int, error)&lt;/p&gt;

&lt;p&gt;把p的内容压缩后写入接口实例内部的io.Writer中。支持多次写入，后面写入的拼接在先写入的后面。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(r io.Reader) (*Reader, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewReader创建一个新的Reader接口，并且将Reader的内容赋值为r，压缩数据存储在r中，并由调用者负责关闭接口。&lt;/p&gt;

&lt;p&gt;Reader接口方法介绍&lt;/p&gt;

&lt;p&gt;1、func (z *Reader) Close() error&lt;/p&gt;

&lt;p&gt;关闭Reader接口，但是不关闭创建接口是引入的io.Reader。&lt;/p&gt;

&lt;p&gt;2、func (z *Reader) Multistream(ok bool)&lt;/p&gt;

&lt;p&gt;设置Reader接口是否支持多流。&lt;/p&gt;

&lt;p&gt;3、func (z *Reader) Read(p []byte) (n int, err error)&lt;/p&gt;

&lt;p&gt;将io.Reader内容部分的内容解压后复制到p。&lt;/p&gt;

&lt;p&gt;4、func (z *Reader) Reset(r io.Reader) error&lt;/p&gt;

&lt;p&gt;刷新Reader的状态为初始状态，并更替其内部的io.Reader&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;文件压缩实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;定义解压缩文件接口CompressFile和DeCompressFile：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package gziptest

import (
    &amp;quot;compress/gzip&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;os&amp;quot;
)

//压缩文件Src到Dst

func CompressFile(Dst string, Src string) error {
    newfile, err := os.Create(Dst)
    if err != nil {
        return err
    }
    defer newfile.Close()

    file, err := os.Open(Src)
    if err != nil {
        return err
    }

    zw := gzip.NewWriter(newfile)

    filestat, err := file.Stat()
    if err != nil {
        return nil
    }

    zw.Name = filestat.Name()
    zw.ModTime = filestat.ModTime()
    _, err = io.Copy(zw, file)
    if err != nil {
        return nil
    }

    zw.Flush()
    if err := zw.Close(); err != nil {
        return nil
    }
    return nil
}

//解压文件Src到Dst
func DeCompressFile(Dst string, Src string) error {
    file, err := os.Open(Src)
    if err != nil {
        panic(err)
    }
    defer file.Close()

    newfile, err := os.Create(Dst)
    if err != nil {
        panic(err)
    }
    defer newfile.Close()

    zr, err := gzip.NewReader(file)
    if err != nil {
        panic(err)
    }

    filestat, err := file.Stat()
    if err != nil {
        panic(err)
    }

    zr.Name = filestat.Name()
    zr.ModTime = filestat.ModTime()
    _, err = io.Copy(newfile, zr)
    if err != nil {
        panic(err)
    }

    if err := zr.Close(); err != nil {
        panic(err)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;单元测试用例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gziptest_test.go

package gziptest

import (
    &amp;quot;os&amp;quot;
    &amp;quot;testing&amp;quot;
)

func TestCompressFile(t *testing.T) {
    pwd, _ := os.Getwd()
    newfile, err := os.Create(pwd + &amp;quot;/test.txt&amp;quot;)
    if err != nil {
        t.Fatal(err)
    }
    newfile.Write([]byte(&amp;quot;hello world!!!!&amp;quot;))
    newfile.Close()

    err = CompressFile(pwd+&amp;quot;/test.gz&amp;quot;, pwd+&amp;quot;/test.txt&amp;quot;)
    if err != nil {
        t.Fatal(err)
    }
}

func TestDeCompressFile(t *testing.T) {
    pwd, _ := os.Getwd()

    err := DeCompressFile(pwd+&amp;quot;/test2.txt&amp;quot;, pwd+&amp;quot;/test.gz&amp;quot;)
    if err != nil {
        t.Fatal(err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:/Go/bin/go.exe test -v [D:/go/src/gziptest]
=== RUN   TestCompressFile
--- PASS: TestCompressFile (0.00s)
=== RUN   TestDeCompressFile
--- PASS: TestDeCompressFile (0.00s)
PASS
ok      gziptest    2.351s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同级目录下增加了三个文件：&lt;/p&gt;

&lt;p&gt;其中test.txt和test2.txt内容为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hello world!!!!
1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test.gz内容为text.txt&lt;/p&gt;

&lt;h1 id=&#34;zlib&#34;&gt;zlib&lt;/h1&gt;

&lt;p&gt;在python的时候就习惯使用zlib进行网页压缩。 golang下同样使用zlib进行压缩解压缩。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Writer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;写入func NewWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewWriter(w io.Writer) *Writer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只能传递 []byte类型数据.   NewWriterLevel 可以传递压缩的等级.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;bytes&amp;quot;
    &amp;quot;compress/zlib&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
)

func main() {
    var in bytes.Buffer
    b := []byte(`xiorui.cc`)
    w := zlib.NewWriter(&amp;amp;in)
    w.Write(b)
    w.Close()

    var out bytes.Buffer
    r, _ := zlib.NewReader(&amp;amp;in)
    io.Copy(&amp;amp;out, r)
    fmt.Println(out.String())

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接口&lt;/p&gt;

&lt;p&gt;1.func (*Writer) Write&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (z *Writer) Write(p []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.func (*Writer) Close&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (z *Writer) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Reader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(r io.Reader) (io.ReadCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;设置压缩等级，并压缩数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewWriterLevel(w io.Writer, level int) (io.WriteCloser, os.Error)

func NewWriterLevel(w io.Writer, level int) (io.WriteCloser, os.Error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是几个级别.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
        NoCompression = 0
        BestSpeed     = 1

        BestCompression    = 9
        DefaultCompression = -1
)

const (
    NoCompression      = flate.NoCompression
    BestSpeed          = flate.BestSpeed
    BestCompression    = flate.BestCompression
    DefaultCompression = flate.DefaultCompression
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Goang zlib压缩的效率和性能都还可以, 毕竟rsync也在用这个压缩算法。  其实zlib和gzip对比，貌似用gzip的多一点。 比如nginx的gzip压缩. 以前看过国外一个帖子，是说zlib比gzip更适合那种速度跟压缩效果均衡的场景&lt;/p&gt;

&lt;h1 id=&#34;lzw&#34;&gt;lzw&lt;/h1&gt;

&lt;p&gt;软件包 lzw 实现了 Le Welpel-Ziv-Welch 压缩数据格式，在TA Welch 的“A Technique for High-Performance Data Compression（一种用于高性能数据压缩的技术）
特别是，它实现了 GIF 和 PDF 文件格式所使用的 LZW，这意味着可变宽度代码可达 12 位，前两个非文字代码是一个清晰的代码和一个 EOF 代码。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;压缩&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewWriter(w io.Writer, order Order, litWidth int) io.WriterCloser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1）w  输出压缩数据的io.Wrier
2）order lzw数据流的位顺序，有LSB和MSB
3）litWidth  字面码的位数，必须在[2,8]范围内，一般位8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;一个io.WriteCloser，可以将压缩的数据写入其下层的w，调用者使用后要将其关闭
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能说明&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NewWriter创建一个新的io.WriterCloser。它将数据压缩后写入w。调用者要在写入结束之后调用返回io.WriterCloser的Close函数关闭。litWidth是字面码的位数，必须在[2,8]范围内，一般为8。
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;解压&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(r io.Reader, order Order, litWidth int) io.ReadCloser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1）r    待解压的数据
2）order lzw数据流的位顺序，有LSB和MSB
3）litWidth  字面码的位数，必须在[2,8]范围内，一般为8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;一个解压过的io.ReadCloser，调用者使用后要将其关闭
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NewReader创建一个新的io.ReadCloser。它从r读取并解压数据。调用者要在读取完之后调用返回io.ReadCloser的Close函数关闭。litWidth是字面码的为数，必须在[2,8]范围内，一般为8.
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;bytes&amp;quot;
    &amp;quot;compress/lzw&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;os&amp;quot;
)

func main() {
    // 一个缓冲区存储压缩的内容
    buf := bytes.NewBuffer(nil)

    w := lzw.NewWriter(buf, lzw.LSB, 8)
    // 写入待压缩内容
    w.Write([]byte(&amp;quot;compress/flate\n&amp;quot;))
    w.Close()
    fmt.Println(buf)

    // 解压
    r := lzw.NewReader(buf, lzw.LSB, 8)
    defer r.Close()
    io.Copy(os.Stdout, r)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;压缩算法的比较&#34;&gt;压缩算法的比较&lt;/h1&gt;

&lt;p&gt;以下是Google几年前发布的一组测试数据（数据有些老了，有人近期做过测试的话希望能共享出来）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Algorithm   % remaining Encoding    Decoding
GZIP            13.4%   21 MB/s 118 MB/s
LZO             20.5%   135 MB/s    410 MB/s
Zippy/Snappy    22.2%   172 MB/s    409 MB/s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GZIP的压缩率最高，但是其实CPU密集型的，对CPU的消耗比其他算法要多，压缩和解压速度也慢；&lt;/li&gt;
&lt;li&gt;LZO的压缩率居中，比GZIP要低一些，但是压缩和解压速度明显要比GZIP快很多，其中解压速度快的更多；&lt;/li&gt;
&lt;li&gt;Zippy/Snappy的压缩率最低，而压缩和解压速度要稍微比LZO要快一些&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- DesignPatterns</title>
          <link>https://kingjcy.github.io/post/golang/designpatterns/</link>
          <pubDate>Sat, 28 Jan 2017 16:33:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/designpatterns/</guid>
          <description>&lt;p&gt;设计模式其实和语言关系不大，但是在项目工程的设计中有着很大的作用，这边使用golang实现相关的设计模式，也算是对过去看过用过的设计模式的回顾和总结。&lt;/p&gt;

&lt;h1 id=&#34;设计模式的六大原则&#34;&gt;设计模式的六大原则&lt;/h1&gt;

&lt;p&gt;1、开闭原则（Open Close Principle）&lt;/p&gt;

&lt;p&gt;开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。 所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。&lt;/p&gt;

&lt;p&gt;2、里氏代换原则（Liskov Substitution Principle）&lt;/p&gt;

&lt;p&gt;里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何 基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受 到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。 实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽 象化的具体步骤的规范。&lt;/p&gt;

&lt;p&gt;3、依赖倒转原则（Dependence Inversion Principle）&lt;/p&gt;

&lt;p&gt;这个是开闭原则的基础，具体内容：针对接口编程，依赖于抽象而不依赖于具体。&lt;/p&gt;

&lt;p&gt;4、接口隔离原则（Interface Segregation Principle）&lt;/p&gt;

&lt;p&gt;这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。还是一个降低类之间的耦合度的意思，从这儿我们看出， 其实设计模式就是一个软件的设计思想，从大型软件架构出发，为了升级和维护方便。所以上文中多次出现：降低依赖，降低耦合。&lt;/p&gt;

&lt;p&gt;5、迪米特法则（最少知道原则）（Demeter Principle）&lt;/p&gt;

&lt;p&gt;为什么叫最少知道原则，就是说：一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。&lt;/p&gt;

&lt;p&gt;类之间耦合越松，越有利于复用，一个处于弱耦合的类被修改，不会对有关系的类造成波及。&lt;/p&gt;

&lt;p&gt;6、合成复用原则（Composite Reuse Principle）&lt;/p&gt;

&lt;p&gt;原则是尽量使用合成/聚合的方式，而不是使用继承。&lt;/p&gt;

&lt;p&gt;合成是强拥有关系，体现了部分和整体的关系。&lt;/p&gt;

&lt;p&gt;聚合是弱拥有关系，体现了个体和群体的关系。&lt;/p&gt;

&lt;p&gt;优先使用合成／聚合原则有利于后面的类封装，使得类和继承保持较小规模。&lt;/p&gt;

&lt;p&gt;上面的六大规则在我们编程设计过程中整体上体现：解耦，抽象，封装，可复用可扩展。&lt;/p&gt;

&lt;h1 id=&#34;设计模式&#34;&gt;设计模式&lt;/h1&gt;

&lt;p&gt;正常23种设计模式，下面分的更加详细。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;创建型模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;抽象工厂模式：提供一个接口用于创建相关对象的家族；&lt;/li&gt;
&lt;li&gt;Builder模式：使用简单的对象来构建复杂的对象；&lt;/li&gt;
&lt;li&gt;工厂方法模式：一个创建产品对象的工厂接口，将实际创建工作推迟到子类当中；&lt;/li&gt;
&lt;li&gt;对象池模式：实例化并维护一组相同类型的对象实例；&lt;/li&gt;
&lt;li&gt;单例模式：限制类的实例，保证一个类只有一个实例。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;结构模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;适配器模式：适配另一个不兼容的接口来一起工作；&lt;/li&gt;
&lt;li&gt;桥接模式：将抽象化(Abstraction)与实现化(Implementation)脱耦，使得二者可以独立地变化；&lt;/li&gt;
&lt;li&gt;合成模式：将对象组织到树中，用来描述树的关系；&lt;/li&gt;
&lt;li&gt;装饰模式：给一个静态或动态对象添加行为；&lt;/li&gt;
&lt;li&gt;门面（Facade）模式：为子系统中的各类（或结构与方法）提供一个简明一致的界面，隐藏子系统的复杂性，使子系统更加容易使用；&lt;/li&gt;
&lt;li&gt;Flyweight模式：运用共享技术有效地支持大量细粒度的对象；&lt;/li&gt;
&lt;li&gt;MVC模式：是模型(model)－视图(view)－控制器(controller)的缩写，将一个应用程序划分成三个相互关联的部分，用一种业务逻辑、数据、界面显示分离的方法组织代码，将业务逻辑聚集到一个部件里，在改进和个性化定制界面及用户交互的同时，不需要重新编写业务逻辑。&lt;/li&gt;
&lt;li&gt;代理模式：为其他对象提供一种代理以控制对这个对象的访问。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;行为模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;责任链模式：使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系；&lt;/li&gt;
&lt;li&gt;命令模式：就是客户端发布一个命令（也就是“请求”），而这个命令已经被封装成一个对象。即这个命令对象的内部可能已经指定了该命令具体由谁负责执行；&lt;/li&gt;
&lt;li&gt;中介（Mediator）模式：用一个中介对象来封装一系列关于对象交互行为；&lt;/li&gt;
&lt;li&gt;观察者模式：对象间的一种一对多的依赖关系，以便一个对象的状态发生变化时，所有依赖于它的对象都得到通知并自动刷新；&lt;/li&gt;
&lt;li&gt;注册（Registry）模式：跟踪给定类的所有子类；&lt;/li&gt;
&lt;li&gt;状态模式：基于一个对象的内部状态，给相同对象提供多种行为；&lt;/li&gt;
&lt;li&gt;策略模式：定义一系列算法，并将每一个算法封装起来，而且使它们可以相互替换；&lt;/li&gt;
&lt;li&gt;模板（Template）模式：定义一个操作中算法的框架，而将一些步骤延迟到子类中。模板方法模式使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤；&lt;/li&gt;
&lt;li&gt;访问者模式：表示一个作用于某对象结构中的各元素的操作，它使开发者可以在不改变各元素类的前提下定义作用于这些元素的新操作。&lt;/li&gt;
&lt;li&gt;同步模式&lt;/li&gt;
&lt;li&gt;条件变量：利用线程间共享的全局变量进行同步的一种机制，主要包括两个动作：一个线程等待”条件变量的条件成立”而挂起；另一个线程使”条件成立”（给出条件成立信号）；&lt;/li&gt;
&lt;li&gt;Lock/Mutex：执行互斥限制资源获得独占访问；&lt;/li&gt;
&lt;li&gt;监视器模式：互斥锁和条件变量的组合模式；&lt;/li&gt;
&lt;li&gt;读写锁定模式：它把对共享资源的访问者划分成读者和写者，读者只对共享资源进行读访问，写者则需要对共享资源进行写操作；&lt;/li&gt;
&lt;li&gt;Semaphore：负责协调各个线程，以保证它们能够正确、合理地使用公共资源。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;并行模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Bounded Parallelism：完成大量资源限制的独立任务；&lt;/li&gt;
&lt;li&gt;广播（Broadcast）：把一个消息同时传输到所有接收端；&lt;/li&gt;
&lt;li&gt;协同（Coroutines）：允许在特定地方暂停和继续执行的子程序；&lt;/li&gt;
&lt;li&gt;生成器：一次性生成一系列值；&lt;/li&gt;
&lt;li&gt;Reactor模式：在事件驱动的应用中，将一个或多个客户的服务请求分离（demultiplex）和调度（dispatch）给应用程序。同步、有序地处理同时接收的多个服务请求。&lt;/li&gt;
&lt;li&gt;并行（Parallelism）：完成大量的独立任务；&lt;/li&gt;
&lt;li&gt;生产者消费者：从任务执行中分离任务；&lt;/li&gt;
&lt;li&gt;调度器（Scheduler）：协调任务步骤。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;消息传递模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;扇入（Fan-In）：该模块直接调用上级模块的个数，像漏斗型一样去工作；&lt;/li&gt;
&lt;li&gt;扇出（Fan-Out）：该模块直接调用的下级模块的个数；&lt;/li&gt;
&lt;li&gt;Futures &amp;amp; Promises：扮演一个占位角色，对未知的结果用于同步；&lt;/li&gt;
&lt;li&gt;Publish/Subscribe：将信息传递给订阅者；&lt;/li&gt;
&lt;li&gt;Push &amp;amp; Pull：把一个管道上的消息分发给多人。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;稳定模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Bulkheads：实施故障遏制原则，例如防止级联故障；&lt;/li&gt;
&lt;li&gt;断路器（Circuit-Breaker）模式：当请求有可能失败时，停止流动的请求；&lt;/li&gt;
&lt;li&gt;截止日期（Deadline）：一旦响应变缓，允许客户端停止一个正在等待的响应；&lt;/li&gt;
&lt;li&gt;Fail-Fast机制：集合的一种错误检测机制。当多个线程对集合进行结构上的改变操作时，有可能会产生fail-fast机制；&lt;/li&gt;
&lt;li&gt;Handshaking：如果一个组件的不能访问请求被拒绝，询问是否还能承担更多负载；&lt;/li&gt;
&lt;li&gt;稳定状态（Steady-State）：为每一个服务积累一个资源，其它服务必须回收这些资源；&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;剖析模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Timing Functions：包装和执行日志的函数；&lt;/li&gt;
&lt;li&gt;Functional Options：允许给默认值创建clean API和惯用重载；&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;反模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;级联故障：一个系统的某部分出现错误，与之有关的上下级也随之出现故障，导致多米诺效应。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;具体讲解&#34;&gt;具体讲解&lt;/h1&gt;

&lt;p&gt;具体讲解一下重要的设计模式&lt;/p&gt;

&lt;h2 id=&#34;策略模式-strategy&#34;&gt;策略模式(Strategy)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;策略模式： 将算法或操作抽象成实现共同接口、可以被替换的类，实现逻辑和具体算法的解耦。

将各种行为抽象成算法，封装算法为对象；
算法实现共同接口，调用者调用时不考虑算法具体实现，调用接口方法即可；
调用者可随时替换此算法对象；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;多个方法择一使用，且他们会被随时替换；
方法没有共性，使用继承会有大量重写，使用接口会有大量重复使用；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;两个算法： 冒泡排序和快速排序；

抽象冒泡排序和快速排序为算法对象，实现算法接口，拥有 used() 被使用方法；
计算器计算时不用理会是什么算法，调用 used() 即可；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;观察者模式-observer&#34;&gt;观察者模式(Observer)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;观察者模式：主题主动向观察者推送变化，解决观察者对主题对象的依赖。

观察者实现被通知接口，并在主题上注册，主题只保存观察者的引用，不关心观察者的实现；
在主题有变化时调用观察者的通知接口来通知已注册的观察者；
通知方式有推（主题变化时将变化数据推送给观察者）和拉（主题只告知变化，观察者主动来拉取数据）；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;一个主题，多个观察者，主题的任何变动，观察者都要第一时刻得到；
观察者获取主题变化困难，定时不及时，轮询消耗大；
观察者可以随时停止关注某主题；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;张三和李四是记者，他们需要及时了解城市发生的新闻；
张三和李四在电视台注册了自己的信息；
城市发生了新闻，电视台遍历注册信息，通知了张三和李四；
李四退休了，在电视台注销了自己的信息；
城市又发生了新闻，电视台只通知了张三；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;装饰者模式-decorator&#34;&gt;装饰者模式(Decorator)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;装饰者模式：包装一个对象，在被装饰对象的基础上添加功能；

装饰者与被装饰对象拥有同一个超类，装饰者拥有被装饰对象的所有外部接口，可被调用，外界无法感知调用的是装饰者还是被装饰者；
装饰者需要被装饰者作为参数传入，并在装饰者内部，在被装饰者实现的基础上添加或修改某些功能后，提供同被装饰者一样的接口；
装饰者也可被另一个装饰者装饰，即嵌套装饰；
装饰者是一群包装类，由于装饰的复杂性，会多出很多个装饰者小类；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;对象需要动态地添加和修改功能；
功能改变后不影响原对象的使用；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;在商店内，花作为被装饰者对象、红丝带和盒子作为花的装饰者；
花、红丝带、盒子有共同的超类“商品”，他们都能被卖掉；
我们可以在红丝带装饰过花后，再用盒子再包装一次；
包装后的花，顾客买时也不会受到任何影响；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;工厂模式-factory&#34;&gt;工厂模式(Factory)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;工厂模式： 顾名思义，工厂模式是对象的生产器，解耦用户对具体对象的依赖。

实现依赖倒置，让用户通过一个产品工厂依赖产品的抽象，而不是一个具体的产品；
简单工厂模式：接收参数并根据参数创建对应类，将对象的实例化和具体使用解耦；
抽象工厂模式：将工厂抽象出多个生产接口，不同类型的工厂调用生产接口时，生产不同类型的对象；
简单工厂常配合抽象工厂一起使用；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;根据不同条件需求不同的对象；
对象实例化的代码经常需要修改；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;简单工厂：向鞋厂内传入不同的类型（布制），鞋厂会生产出不同类型的鞋子（布鞋）；
抽象工厂：有两座鞋厂：李宁鞋厂、Adidas鞋厂，他们能生产对应各自品牌的鞋子；
搭配使用：向不同的抽象工厂（李宁）传入不同的类型（运动类型），会生产出对应品牌对应类型的鞋子（李宁运动鞋）；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;单例模式-singleton&#34;&gt;单例模式(Singleton)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;单例模式：保证同一个类全局只有一个实例对象;

在第一次实例化后会使用静态变量保存实例，后续全局使用此静态变量；
一般将构造方法私有化，构造方法添加 final 关键字无法被重写，添加一个类静态方法用于返回此实例；
在多线程时应该考虑并发问题，防止两次调用都被判定为实例未初始化而重复初始化对象；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;全局共享同一个实例对象（数据库连接等）；
某一处对此对象的更新全局可见；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;利用 Go 中包的可见性规则来隐藏对象的实例化权限；
使用包变量保存实例对象，获取实例时判断是否已实例化，如为nil，实例化对象并返回，如有值，直接返回值；
待用锁实现 Go routine 并发时的问题；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;命令模式-command&#34;&gt;命令模式(Command)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;命令模式:将一个命令封装成对象，解耦命令的发起者和执行者。

命令对象实现命令接口（excute[、undo]），命令发起者实例化命令对象，并传递此对象，并不关心此对象由谁执行；
命令执行者只负责调用命令对象的执行方法即可，不关心对象是由谁生成的；
与策略模式不同之处：策略模式是通过不同的算法做同一件事情（例如排序），而命令模式则是通过不同的命令做不同的事情；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;命令发起者与执行者无法直接接触；
命令需要撤销功能，却不易保存命令执行状态信息时；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;指挥官创建了一个“从树下跑到草地上”的命令；
命令被分配给张三执行，张三作为军人，接到命令后不管命令的具体内容，而是直接调用命令的执行接口执行；
指挥官发布了撤销指令，张三又从草地上跑到了树下；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;适配器模式-adapter&#34;&gt;适配器模式(Adapter)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;适配器模式：包装对象提供一个接口，以适配调用者。

适配器通过一个中间对象，封装目标接口以适应调用者调用；
调用者调用此适配器，以达到调用目标接口的目的；
适配器模式与装饰者模式的不同之处：适配器模式不改变接口的功能，而装饰者会添加或修改原接口功能；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;提供的接口与调用者调用的其他的接口都不一致；
为一个特殊接口修改调用者的调用方式得不偿失；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;张三是个正常人，他能通过说话直接地表达自己；
李四是个聋哑人，他没法直接表达自己，但他会写字；
笔记本作为一个适配器，用笔记本“包装”了李四之后，当李四需要表达自己想法时，调用笔记本的“表达”功能，笔记本再调用李四“写字”的方法；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;外观模式-facade&#34;&gt;外观模式(Facade)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;外观模式：通过封装多个复杂的接口来提供一个简化接口来实现一个复杂功能。

外观模式是通过封装多个接口来将接口简单化；
外观模式不会改变原有的多个复杂的单一接口，这些接口依然能被单独调用，只是提供了一个额外的接口；
外观模式与适配器模式的不同之处：外观模式是整合多个接口并添加一个简化接口，适配器是适配一个接口；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;实现某一功能需要调用多个复杂接口；
经常需要实现此功能；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;正常的冲咖啡步骤是：磨咖啡豆、烧开水、倒开水搅拌咖啡。
我们经常需要直接冲咖啡，而不是使用单一步骤，每次喝咖啡时调用三个方法很麻烦；
封装三个接口，额外提供一个 “冲咖啡” 的方法，需要喝咖啡时只需要调用一次冲咖啡方法即可；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;小结&#34;&gt;小结&lt;/h2&gt;

&lt;p&gt;《Head First 设计模式》这书真心不错，例子很轻松，给人很多时间和空间来思考，同时介绍模式时使用结合故事，层层深入的方法，让人印象很深刻，推荐。&lt;/p&gt;

&lt;p&gt;书中详细介绍了 14 个基础设计模式，还有 9 个简化版，就自己查资料结合自己的理解来总结了。&lt;/p&gt;

&lt;h1 id=&#34;实践&#34;&gt;实践&lt;/h1&gt;

&lt;h2 id=&#34;工厂模式-解耦和面向对象&#34;&gt;工厂模式&amp;ndash;解耦和面向对象&lt;/h2&gt;

&lt;p&gt;WIKI:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In class-based programming, the factory method pattern is a creational pattern that uses factory methods to deal with the problem of creating objects without having to specify the exact class of the object that will be created.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;百度百科：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;工厂模式是我们最常用的实例化对象模式了，是用工厂方法代替new操作的一种模式。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单工厂模式是通过传递不同的参数生成不同的实例，缺点就是扩展不同的类别时需要修改代码。&lt;/p&gt;

&lt;p&gt;工厂方法模式为每一个product提供一个工程类，通过不同工厂创建不同实例。&lt;/p&gt;

&lt;p&gt;实现实例&lt;/p&gt;

&lt;p&gt;1.首先，我们定义一个计算的接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package calc

type CalcSuper interface {
    SetData(data ...interface{})
    CalcOperate() float64
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.接下来，我们实现这个类的两个子类，分别是加法和减法&lt;/p&gt;

&lt;p&gt;加法，就是用两个数来相加&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package calc

import &amp;quot;fmt&amp;quot;

type Add struct {
    Num1 float64
    Num2 float64
}

func NewAdd() *Add {
    instance := new(Add)
    return instance
}

func (a *Add) SetData(data ...interface{}) {
    if len(data) != 2 {
        fmt.Println(&amp;quot;error,Need two parameters &amp;quot;)
        return
    }
    if _, ok := data[0].(float64); !ok {
        fmt.Println(&amp;quot;error,Need float64 parameters &amp;quot;)
        return
    }
    if _, ok := data[1].(float64); !ok {
        fmt.Println(&amp;quot;error,Need float64 parameters &amp;quot;)
        return
    }
    a.Num1 = data[0].(float64)
    a.Num2 = data[1].(float64)
}

func (a Add) CalcOperate() float64 {
    return a.Num1 + a.Num2
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;减法，就是把两个数相减，我感觉我好冷。。。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package calc

import &amp;quot;fmt&amp;quot;

type Subtraction struct {
    Num1 float64
    Num2 float64
}

func NewSubtraction() *Subtraction {
    instance := new(Subtraction)
    return instance
}

func (a *Subtraction) SetData(data ...interface{}) {
    if len(data) != 2 {
        fmt.Println(&amp;quot;error,Need two parameters &amp;quot;)
        return
    }
    if _, ok := data[0].(float64); !ok {
        fmt.Println(&amp;quot;error,Need float64 parameters &amp;quot;)
        return
    }
    if _, ok := data[1].(float64); !ok {
        fmt.Println(&amp;quot;error,Need float64 parameters &amp;quot;)
        return
    }
    a.Num1 = data[0].(float64)
    a.Num2 = data[1].(float64)
}

func (a Subtraction) CalcOperate() float64 {
    return a.Num1 - a.Num2
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.下面到了大功告成的时候了，定义简易工厂，来实例化这两个类&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package calc

type CalcFactory struct {
}

func NewCalcFactory() *CalcFactory {
    instance := new(CalcFactory)
    return instance
}

func (f CalcFactory) CreateOperate(opType string) CalcSuper {
    var op CalcSuper
    switch opType {
    case &amp;quot;+&amp;quot;:
        op = NewAdd()
    case &amp;quot;-&amp;quot;:
        op = NewSubtraction()
    default:
        panic(&amp;quot;error ! dont has this operate&amp;quot;)
    }
    return op
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个简易工厂，我们只传入相应的运算方式，如“＋”，“－”，用来创建相关的运算策略。它会返回一个运算接口的实例，当我们得到这个实例，就能调用里面的方法进行运算了。&lt;/p&gt;

&lt;p&gt;4.测试&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 简易工厂模式 project main.go
package main

import (
    . &amp;quot;calc&amp;quot;
    &amp;quot;fmt&amp;quot;
)

func main() {
    factory := NewCalcFactory()

    op := factory.CreateOperate(&amp;quot;+&amp;quot;)
    op.SetData(1.0, 2.0)
    fmt.Println(op.CalcOperate())

    op = factory.CreateOperate(&amp;quot;-&amp;quot;)
    op.SetData(1.0, 2.0)
    fmt.Println(op.CalcOperate())
    /*
        输出：3
        -1
    */
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;策略模式strategy-定义一系列算法-将每一个算法封装起来-并让它们可以相互替换-策略模式让算法独立于使用它的客户而变化&#34;&gt;策略模式Strategy：定义一系列算法，将每一个算法封装起来，并让它们可以相互替换。策略模式让算法独立于使用它的客户而变化。&lt;/h2&gt;

&lt;p&gt;将多种算法进行封装，只暴露给外界固定数目的通用算法接口，这样就可以在后续的工作中在算法内部进行维护和扩展，避免更改用户端的代码实现，从而减小代码维护的成本，降低模块间的耦合程度。&lt;/p&gt;

&lt;p&gt;代码实现&lt;/p&gt;

&lt;p&gt;下面我们就开始以代码的形式来展示一下策略模式吧，代码很简单，我们用一个加减乘除法来模拟。&lt;/p&gt;

&lt;p&gt;首先，我们看到的将会是策略接口和一系列的策略，这些策略不要依赖高层模块的实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package strategy
/**
 * 策略接口
 */
type Strategier interface {
    Compute(num1, num2 int) int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很简单的一个接口，定义了一个方法Compute，接受两个参数，返回一个int类型的值，很容易理解，我们要实现的策略将会将两个参数的计算值返回。
接下来，我们来看一个我们实现的策略，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package strategy
import &amp;quot;fmt&amp;quot;

type Division struct {}

func (p Division) Compute(num1, num2 int) int {
    defer func() {
        if f := recover(); f != nil {
            fmt.Println(f)
            return
        }
    }()

    if num2 == 0 {
        panic(&amp;quot;num2 must not be 0!&amp;quot;)
    }

    return num1 / num2
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为什么要拿除法作为代表呢？因为除法特殊嘛，被除数不能为0，其他的加减乘基本都是一行代码搞定，除法我们需要判断被除数是否为0，如果是0则直接抛出异常。
ok，基本的策略定义好了，我们还需要一个工厂方法，根据不用的type来返回不同的策略，这个type我们准备从命令好输入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewStrategy(t string) (res Strategier) {
    switch t {
        case &amp;quot;s&amp;quot;: // 减法
            res = Subtraction{}
        case &amp;quot;m&amp;quot;: // 乘法
            res = Multiplication{}
        case &amp;quot;d&amp;quot;: // 除法
            res = Division{}
        case &amp;quot;a&amp;quot;: // 加法
            fallthrough
        default:
            res = Addition{}
    }

    return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个工厂方法会根据不用的类型来返回不同的策略实现，当然，哪天我们需要新增新的策略，我们只需要在这个函数中增加对应的类型判断就ok。&lt;/p&gt;

&lt;p&gt;现在策略貌似已经完成了，接下来我们来看看主流程代码，一个Computer，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package compute

import (
    &amp;quot;fmt&amp;quot;
    s &amp;quot;../strategy&amp;quot;
)

type Computer struct {
    Num1, Num2 int
    strate s.Strategier
}

func (p *Computer) SetStrategy(strate s.Strategier) {
    p.strate = strate
}

func (p Computer) Do() int {
    defer func() {
        if f := recover(); f != nil {
            fmt.Println(f)
        }
    }()

    if p.strate == nil {
        panic(&amp;quot;Strategier is null&amp;quot;)
    }

    return p.strate.Compute(p.Num1, p.Num2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个Computer中有三个参数，Num1和Num2当然是我们要操作的数了，strate是我们要设置的策略，可能是上面介绍的Division，也有可能是其他的，在main函数中我们会调用SetStrategy方法来设置要使用的策略，Do方法会执行运算，最后返回运算的结果，可以看到在Do中我们将计算的功能委托给了Strategier。&lt;/p&gt;

&lt;p&gt;貌似一切准备就绪，我们就来编写main的代码吧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;flag&amp;quot;
    c &amp;quot;./computer&amp;quot;
    s &amp;quot;./strategy&amp;quot;
)

var stra *string = flag.String(&amp;quot;type&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;input the strategy&amp;quot;)
var num1 *int = flag.Int(&amp;quot;num1&amp;quot;, 1, &amp;quot;input num1&amp;quot;)
var num2 *int = flag.Int(&amp;quot;num2&amp;quot;, 1, &amp;quot;input num2&amp;quot;)

func init() {
    flag.Parse()
}

func main() {
    com := c.Computer{Num1: *num1, Num2: *num2}
    strate := s.NewStrategy(*stra)

    com.SetStrategy(strate)
    fmt.Println(com.Do())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先我们要从命令行读取要使用的策略类型和两个操作数，在main函数中，我们初始化Computer这个结构体，并将输入的操作数赋值给Computer的Num1和Num2，接下来我们根据策略类型通过调用NewStrategy函数来获取一个策略，并调用Computer的SetStrategy方法给Computer设置上面获取到的策略，最后执行Do方法计算结果，最后打印。&lt;/p&gt;

&lt;p&gt;感觉策略就是对工厂的上一层封装，只保留对外的一个基本接口和数据结构&lt;/p&gt;

&lt;h2 id=&#34;单一职责&#34;&gt;单一职责&lt;/h2&gt;

&lt;p&gt;单例模式：保证一个类仅有一个实例，并提供一个访问它的全局访问点&lt;/p&gt;

&lt;p&gt;这个解释足够简单。说白了就是假如我们希望我们在我们的系统中该类仅仅存在1个或0个该类的实例。虽然单例模式很简单，但是熟悉java的同学可能了解，单例模式有很多写法,懒汉式、饿汉式、双重锁。。。 这么多形式，难道有什么目的？确实，不过他们的目的很明确，就是保证在一种特殊情况下的单例-并发。&lt;/p&gt;

&lt;p&gt;ok，既然了解了单例模式，那下面我们就开始用代码描述一下单例模式。首先是最简单的单例，这里我们并不去考虑并发的情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package manager
import (
    &amp;quot;fmt&amp;quot;
)

var m *Manager

func GetInstance() *Manager {
    if m == nil {
        m = &amp;amp;Manager {}
    }
    return m
}

type Manager struct {}

func (p Manager) Manage() {
    fmt.Println(&amp;quot;manage...&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是一个最简单的单例了，对于Manager结构体，我们提供了一个GetInstance函数去获取它的实例，这个函数中首先去判断m变量是否为空，如果为空才去赋值一个Manager的指针类型的值，一个小小的判断，就保证了我们在第第二次调用GetInstance的时候直接返回m，而不是重新获取Manager的实例，进而保证了唯一实例。&lt;/p&gt;

&lt;p&gt;上面的代码确实简单，也实现了最简单的单例模式，不过大家有没有考虑到并发这一点，在并发的情况下，这里是不是还可以正常工作呢？ 来，先跟着下面的思路走一走，来看看问题出现在哪。&lt;/p&gt;

&lt;p&gt;现在我们是在并发的情况下去调用的 GetInstance函数，现在恰好第一个goroutine执行到m = &amp;amp;Manager {}这句话之前，第二个goroutine也来获取实例了，第二个goroutine去判断m是不是nil,因为m = &amp;amp;Manager{}还没有来得及执行，所以m肯定是nil，现在出现的问题就是if中的语句可能会执行两遍！&lt;/p&gt;

&lt;p&gt;在上面介绍的这种情形中，因为m = &amp;amp;Manager{}可能会执行多次，所以我们写的单例失效了，这个时候我们就该考虑为我们的单例加锁啦。&lt;/p&gt;

&lt;p&gt;这个时候我们就需要引入go的锁机制-sync.Mutex了,修改我们的代码，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package manager
import (
    &amp;quot;sync&amp;quot;
    &amp;quot;fmt&amp;quot;
)

var m *Manager
var lock *sync.Mutex = &amp;amp;sync.Mutex {}

func GetInstance() *Manager {
    lock.Lock()
    defer lock.Unlock()
    if m == nil {
        m = &amp;amp;Manager {}
    }
    return m
}

type Manager struct {}

func (p Manager) Manage() {
    fmt.Println(&amp;quot;manage...&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码做了简单的修改了，引入了锁的机制，在GetInstance函数中，每次调用我们都会上一把锁，保证只有一个goroutine执行它，这个时候并发的问题就解决了。不过现在不管什么情况下都会上一把锁，而且加锁的代价是很大的，有没有办法继续对我们的代码进行进一步的优化呢？ 熟悉java的同学可能早就想到了双重的概念，没错，在go中我们也可以使用双重锁机制来提高效率。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package manager
import (
    &amp;quot;sync&amp;quot;
    &amp;quot;fmt&amp;quot;
)

var m *Manager
var lock *sync.Mutex = &amp;amp;sync.Mutex {}

func GetInstance() *Manager {
    if m == nil {
        lock.Lock()
        defer lock.Unlock()
        if m == nil {
            m = &amp;amp;Manager {}
        }
    }

    return m
}

type Manager struct {}

func (p Manager) Manage() {
    fmt.Println(&amp;quot;manage...&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码只是稍作修改而已，不过我们用了两个判断，而且我们将同步锁放在了条件判断之后，这样做就避免了每次调用都加锁，提高了代码的执行效率。&lt;/p&gt;

&lt;p&gt;这获取就是很完美的单例代码了，不过还没完，在go中我们还有更优雅的方式去实现。单例的目的是啥？保证实例化的代码只执行一次，在go中就中这么一种机制来保证代码只执行一次，而且不需要我们手工去加锁解锁。对，就是我们的sync.Once，它有一个Do方法，在它中的函数go会只保证仅仅调用一次！再次修改我们的代码，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package manager
import (
    &amp;quot;sync&amp;quot;
    &amp;quot;fmt&amp;quot;
)

var m *Manager
var once sync.Once

func GetInstance() *Manager {
    once.Do(func() {
        m = &amp;amp;Manager {}
    })
    return m
}

type Manager struct {}

func (p Manager) Manage() {
    fmt.Println(&amp;quot;manage...&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码更简单了，而且有没有发现-漂亮了！Once.Do方法的参数是一个函数，这里我们给的是一个匿名函数，在这个函数中我们做的工作很简单，就是去赋值m变量，而且go能保证这个函数中的代码仅仅执行一次！&lt;/p&gt;

&lt;p&gt;ok，到现在单例模式我们就介绍完了，内容并不多，因为单例模式太简单而且太常见了。我们用单例的目的是为了保证在整个系统中存在唯一的实例，我们加锁的目的是为了在并发的环境中单例依旧好用。不过虽然单例简单，我们还是不能任性的用，因为这样做实例会一直存在内存中，一些我们用的不是那么频繁的东西使用了单例是不是就造成了内存的浪费？大家在用单例的时候还是要多思考思考，这个模块适不适合用单例！&lt;/p&gt;

&lt;h2 id=&#34;开放封闭原则-对于扩展开放-对于修改封闭-尽量不修改-新增-在设计发生变化的时候-就要考虑抽象来应对未来的变化&#34;&gt;开放封闭原则：对于扩展开放，对于修改封闭，尽量不修改，新增，在设计发生变化的时候，就要考虑抽象来应对未来的变化&lt;/h2&gt;

&lt;p&gt;实现开放封闭的核心思想就是对抽象编程，而不对具体编程，因为抽象相对稳定。让类依赖于固定的抽象，所以对修改就是封闭的；而通过面向对象的继承和多态机制，可以实现对抽象体的继承，通过覆写其方法来改变固有行为，实现新的扩展方法，所以对于扩展就是开放的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  对于违反这一原则的类，必须通过重构来进行改善。常用于实现的设计模式主要有Template Method模式和Strategy 模式。而封装变化，是实现这一原则的重要手段，将经常变化的状态封装为一个类。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以银行业务员为例&lt;/p&gt;

&lt;p&gt;没有实现OCP的设计：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class BankProcess

    {  //存款 

       public void Deposite(){}

        //取款

        public void Withdraw(){ }

        //转账

        public void Transfer(){}

    }

    public class BankStaff

    {

        private BankProcess bankpro = new BankProcess();

        public void BankHandle(Client client)

        {

            switch (client.Type)

            {  //存款

                case &amp;quot;deposite&amp;quot;:

                    bankpro.Deposite();

                    break;

                    //取款

                case &amp;quot;withdraw&amp;quot;:

                    bankpro.Withdraw();

                    break;

                    //转账

                case &amp;quot;transfer&amp;quot;:

                    bankpro.Transfer();

                    break;

            }

        }

    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种设计显然是存在问题的，目前设计中就只有存款，取款和转账三个功能，将来如果业务增加了，比如增加申购基金功能，理财功能等，就必须要修改BankProcess业务类。我们分析上述设计就不能发现把不能业务封装在一个类里面，违反单一职责原则，而有新的需求发生，必须修改现有代码则违反了开放封闭原则。&lt;/p&gt;

&lt;p&gt;从开放封闭的角度来分析，在银行系统中最可能扩展的就是业务功能的增加或变更。对业务流程应该作为扩展的部分来实现。当有新的功能时，不需要再对现有业务进行重新梳理，然后再对系统做大的修改。&lt;/p&gt;

&lt;p&gt;如何才能实现耦合度和灵活性兼得呢？&lt;/p&gt;

&lt;p&gt;那就是抽象，将业务功能抽象为接口，当业务员依赖于固定的抽象时，对修改就是封闭的，而通过继承和多态继承，从抽象体中扩展出新的实现，就是对扩展的开放。&lt;/p&gt;

&lt;p&gt;以下是符合OCP的设计：&lt;/p&gt;

&lt;p&gt;首先声明一个业务处理接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public  interface IBankProcess{  void Process();}

public class DepositProcess : IBankProcess

    {

        public void Process()

        { //办理存款业务

            Console.WriteLine(&amp;quot;Process Deposit&amp;quot;);

        }

}

public class WithDrawProcess : IBankProcess

    {

        public void Process()

        { //办理取款业务

            Console.WriteLine(&amp;quot;Process WithDraw&amp;quot;);

        }

}

public class TransferProcess : IBankProcess

    {

        public void Process()

        { //办理转账业务

            Console.WriteLine(&amp;quot;Process Transfer&amp;quot;);

        }

    }

public class BankStaff

    {

        private IBankProcess bankpro = null;

        public void BankHandle(Client client)

        {

            switch (client.Type)

            {   //存款

                case &amp;quot;Deposit&amp;quot;:

                    userProc = new DepositUser();

                    break;

                    //转账

                case &amp;quot;Transfer&amp;quot;:

                    userProc = new TransferUser();

                    break;

                    //取款

                case &amp;quot;WithDraw&amp;quot;:

                    userProc = new WithDrawUser();

                    break;

            }

            userProc.Process();

        }

    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样当业务变更时，只需要修改对应的业务实现类就可以，其他不相干的业务就不必修改。当业务增加，只需要增加业务的实现就可以了。&lt;/p&gt;

&lt;p&gt;设计建议：&lt;/p&gt;

&lt;p&gt;开放封闭原则，是最为重要的设计原则，Liskov替换原则和合成/聚合复用原则为开放封闭原则提供保证。&lt;/p&gt;

&lt;p&gt;可以通过Template Method模式和Strategy模式进行重构，实现对修改封闭，对扩展开放的设计思路。&lt;/p&gt;

&lt;p&gt;封装变化，是实现开放封闭原则的重要手段，对于经常发生变化的状态，一般将其封装为一个抽象，例如银行业务中IBankProcess接口。&lt;/p&gt;

&lt;p&gt;拒绝滥用抽象，只将经常变化的部分进行抽象。&lt;/p&gt;

&lt;h2 id=&#34;依赖倒转原则&#34;&gt;依赖倒转原则&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;抽象不应该依赖于细节，细节应该依赖于抽象    
针对接口编程，不要对实现编程
高层不应该依赖于底层模块，两个都应该依赖于抽象
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心其实就是针对接口编程&lt;/p&gt;

&lt;h2 id=&#34;装饰模式&#34;&gt;装饰模式&lt;/h2&gt;

&lt;p&gt;装饰模式 （decorator）就是动态的给一个对象添加一些额外的职责，就增加功能来说。装饰模式比生成子类更加的灵活。&lt;/p&gt;

&lt;p&gt;装饰模式使用对象组合的方式动态改变或增加对象行为。&lt;/p&gt;

&lt;p&gt;Go语言借助于匿名组合和非入侵式接口可以很方便实现装饰模式。&lt;/p&gt;

&lt;p&gt;使用匿名组合，在装饰器中不必显式定义转调原对象方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package decorator

import (
    &amp;quot;fmt&amp;quot;
)

type person struct {
    Name string
}

func (p *person) show() {
    if p == nil {
        return
    }
    fmt.Println(&amp;quot;姓名：&amp;quot;, p.Name)
}

type AbsstractPerson interface {
    show()
}
type Decorator struct {
    AbsstractPerson
}

func (d *Decorator) SetDecorator(component AbsstractPerson) {
    if d == nil {
        return
    }
    d.AbsstractPerson = component
}

func (d *Decorator) show() {
    if d == nil {
        return
    }
    if d.AbsstractPerson != nil {
        d.AbsstractPerson.show()
    }
}

type TShirts struct {
    Decorator
}

func (t *TShirts) show() {
    if t == nil {
        return
    }
    t.Decorator.show()
    fmt.Println(&amp;quot;T恤&amp;quot;)
}

type BigTrouser struct {
    Decorator
}

func (b *BigTrouser) show() {
    if b == nil {
        return
    }
    b.Decorator.show()
    fmt.Println(&amp;quot;大裤衩&amp;quot;)
}

type Sneakers struct {
    Decorator
}

func (b *Sneakers) show() {
    if b == nil {
        return
    }
    b.Decorator.show()
    fmt.Println(&amp;quot;破球鞋&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;代理模式&#34;&gt;代理模式&lt;/h2&gt;

&lt;p&gt;Proxy 代理模式：为其他对象提供一种代理，以控制对这个对象的访问。&lt;/p&gt;

&lt;p&gt;代理模式用于延迟处理操作或者在进行实际操作前后进行其它处理。一般都是在结构体外新增一个结构体和这个结构体进行组合，来达到操作或者控制访问。&lt;/p&gt;

&lt;p&gt;代理模式的常见用法有&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;虚代理
COW代理
远程代理
保护代理
Cache 代理
防火墙代理
同步代理
智能指引
等。。。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package proxy

import (
    &amp;quot;fmt&amp;quot;
)

type GiveGift interface {
    giveDolls()
    giveFlowers()
    giveChocolate()
}

type Girl struct {
    name string
}

func (g *Girl) Name() string {
    if g == nil {
        return &amp;quot;&amp;quot;
    }
    return g.name
}

func (g *Girl) SetName(name string) {
    if g == nil {
        return
    }
    g.name = name
}

type Pursuit struct {
    girl Girl
}

func (p *Pursuit) giveDolls() {
    if p == nil {
        return
    }
    fmt.Println(p.girl.name, &amp;quot;送你洋娃娃&amp;quot;)
}

func (p *Pursuit) giveFlowers() {
    if p == nil {
        return
    }
    fmt.Println(p.girl.name, &amp;quot;送你玫瑰花&amp;quot;)
}

func (p *Pursuit) giveChocolate() {
    if p == nil {
        return
    }
    fmt.Println(p.girl.name, &amp;quot;送你巧克力&amp;quot;)
}

type Proxy struct {
    p Pursuit
}

func (p *Proxy) giveDolls() {
    if p == nil {
        return
    }
    p.p.giveDolls()
}

func (p *Proxy) giveFlowers() {
    if p == nil {
        return
    }
    p.p.giveFlowers()
}

func (p *Proxy) giveChocolate() {
    if p == nil {
        return
    }
    p.p.giveChocolate()
}

func NewProxy(mm Girl) *Proxy {
    gg := Pursuit{mm}
    return &amp;amp;Proxy{gg}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;工厂办法模式&#34;&gt;工厂办法模式&lt;/h2&gt;

&lt;p&gt;工厂方法模式使用子类的方式延迟生成对象到子类中实现。&lt;/p&gt;

&lt;p&gt;Go中不存在继承 所以使用匿名组合来实现&lt;/p&gt;

&lt;p&gt;简单工厂模式是通过传递不同的参数生成不同的实例，缺点就是扩展不同的类别时需要修改代码。&lt;/p&gt;

&lt;p&gt;工厂方法模式为每一个product提供一个工程类，通过不同工厂创建不同实例。&lt;/p&gt;

&lt;p&gt;简单工厂和工厂模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;简单工厂定义的是静态函数，
一个函数处理所有的产品创建，工厂模式将创建对象过程抽象为一个类组，
有抽象类，有对应产品的创建类，创建的过程有创建类来完成，
工厂模式主要使用的是依赖反转原则
（高层模块不依赖底层模块，统一依赖抽象层，抽象层不依赖细节层，细节层依赖抽象层），
解决简单工厂的缺少开放-封闭原则
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package factorymethod

//Operator 是被封装的实际类接口
type Operator interface {
    SetA(int)
    SetB(int)
    Result() int
}

//OperatorFactory 是工厂接口
type OperatorFactory interface {
    Create() Operator
}

//OperatorBase 是Operator 接口实现的基类，封装公用方法
type OperatorBase struct {
    a, b int
}

//SetA 设置 A
func (o *OperatorBase) SetA(a int) {
    o.a = a
}

//SetB 设置 B
func (o *OperatorBase) SetB(b int) {
    o.b = b
}

//PlusOperatorFactory 是 PlusOperator 的工厂类
type PlusOperatorFactory struct{}

func (PlusOperatorFactory) Create() Operator {
    return &amp;amp;PlusOperator{
        OperatorBase: &amp;amp;OperatorBase{},
    }
}

//PlusOperator Operator 的实际加法实现
type PlusOperator struct {
    *OperatorBase
}

//Result 获取结果
func (o PlusOperator) Result() int {
    return o.a + o.b
}

//MinusOperatorFactory 是 MinusOperator 的工厂类
type MinusOperatorFactory struct{}

func (MinusOperatorFactory) Create() Operator {
    return &amp;amp;MinusOperator{
        OperatorBase: &amp;amp;OperatorBase{},
    }
}

//MinusOperator Operator 的实际减法实现
type MinusOperator struct {
    *OperatorBase
}

//Result 获取结果
func (o MinusOperator) Result() int {
    return o.a - o.b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;原型模式&#34;&gt;原型模式&lt;/h2&gt;

&lt;p&gt;原型模式使对象能复制自身，并且暴露到接口中，使客户端面向接口编程时，不知道接口实际对象的情况下生成新的对象。&lt;/p&gt;

&lt;p&gt;原型模式配合原型管理器使用，使得客户端在不知道具体类的情况下，通过接口管理器得到新的实例，并且包含部分预设定配置。&lt;/p&gt;

&lt;p&gt;注意浅复制、深复制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package prototype

import (
    &amp;quot;fmt&amp;quot;
)

type Resume struct {
    name     string
    sex      string
    age      string
    timeArea string
    company  string
}

func (r *Resume) setPersonalInfo(name, sex, age string) {
    if r == nil {
        return
    }
    r.name = name
    r.age = age
    r.sex = sex
}

func (r *Resume) setWorkExperience(timeArea, company string) {
    if r == nil {
        return
    }
    r.company = company
    r.timeArea = timeArea
}

func (r *Resume) display() {
    if r == nil {
        return
    }
    fmt.Println(&amp;quot;个人信息：&amp;quot;, r.name, r.sex, r.age)
    fmt.Println(&amp;quot;工作经历：&amp;quot;, r.timeArea, r.company)
}

func (r *Resume) clone() *Resume {
    if r == nil {
        return nil
    }
    new_obj := (*r)
    return &amp;amp;new_obj
}

func NewResume() *Resume {
    return &amp;amp;Resume{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;模版方法模式&#34;&gt;模版方法模式&lt;/h2&gt;

&lt;p&gt;模版方法模式使用继承机制，把通用步骤和通用方法放到父类中，把具体实现延迟到子类中实现。使得实现符合开闭原则。&lt;/p&gt;

&lt;p&gt;如实例代码中通用步骤在父类中实现（准备、下载、保存、收尾）下载和保存的具体实现留到子类中，并且提供 保存方法的默认实现。&lt;/p&gt;

&lt;p&gt;因为Golang不提供继承机制，需要使用匿名组合模拟实现继承。&lt;/p&gt;

&lt;p&gt;此处需要注意：因为父类需要调用子类方法，所以子类需要匿名组合父类的同时，父类需要持有子类的引用。&lt;/p&gt;

&lt;p&gt;Template Methed模板方法：&lt;/p&gt;

&lt;p&gt;定义一个操作中的算法的骨架，而将一些具体步骤延迟到子类中。&lt;/p&gt;

&lt;p&gt;模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。&lt;/p&gt;

&lt;p&gt;与建造者：一个是行为型模式，一个是创建型模式&lt;/p&gt;

&lt;p&gt;模版方法其实就是将不变的行为抽象为一个方法，具体实现在子类中。然后直接子类结构体直接调用这个方法，就可以实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package template

import (
    &amp;quot;fmt&amp;quot;
)

type getfood interface {
    first()
    secend()
    three()
}

type template struct {
    g getfood
}

func (b *template) getsomefood() {
    if b == nil {
        return
    }
    b.g.first()
    b.g.secend()
    b.g.three()
}

type bingA struct {
    template
}

func NewBingA() *bingA {
    b := bingA{}
    return &amp;amp;b
}

func (b *bingA) first() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;打开冰箱&amp;quot;)
}

func (b *bingA) secend() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;拿出东西&amp;quot;)
}

func (b *bingA) three() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;关闭冰箱&amp;quot;)
}

type Guo struct {
    template
}

func NewGuo() *Guo {
    b := Guo{}
    return &amp;amp;b
}

func (b *Guo) first() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;打开锅&amp;quot;)
}

func (b *Guo) secend() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;拿出东西锅&amp;quot;)
}

func (b *Guo) three() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;关闭锅&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;迪米特法则&#34;&gt;迪米特法则&lt;/h2&gt;

&lt;p&gt;最少知道原则，就是说：一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。&lt;/p&gt;

&lt;p&gt;类之间耦合越松，越有利于复用，一个处于弱耦合的类被修改，不会对有关系的类造成波及。&lt;/p&gt;

&lt;h2 id=&#34;外观模式&#34;&gt;外观模式&lt;/h2&gt;

&lt;p&gt;API 为facade 模块的外观接口，大部分代码使用此接口简化对facade类的访问。&lt;/p&gt;

&lt;p&gt;facade模块同时暴露了a和b 两个Module 的NewXXX和interface，其它代码如果需要使用细节功能时可以直接调用。&lt;/p&gt;

&lt;p&gt;为子系统中的一组接口提供一个一致的界面，此模式定义了一个高层接口，
        这个接口使得这一子系统更加容易使用（投资：基金，股票，房产）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package facade

import &amp;quot;fmt&amp;quot;

func NewAPI() API {
    return &amp;amp;apiImpl{
        a: NewAModuleAPI(),
        b: NewBModuleAPI(),
    }
}

//API is facade interface of facade package
type API interface {
    Test() string
}

//facade implement
type apiImpl struct {
    a AModuleAPI
    b BModuleAPI
}

func (a *apiImpl) Test() string {
    aRet := a.a.TestA()
    bRet := a.b.TestB()
    return fmt.Sprintf(&amp;quot;%s\n%s&amp;quot;, aRet, bRet)
}

//NewAModuleAPI return new AModuleAPI
func NewAModuleAPI() AModuleAPI {
    return &amp;amp;aModuleImpl{}
}

//AModuleAPI ...
type AModuleAPI interface {
    TestA() string
}

type aModuleImpl struct{}

func (*aModuleImpl) TestA() string {
    return &amp;quot;A module running&amp;quot;
}

//NewBModuleAPI return new BModuleAPI
func NewBModuleAPI() BModuleAPI {
    return &amp;amp;bModuleImpl{}
}

//BModuleAPI ...
type BModuleAPI interface {
    TestB() string
}

type bModuleImpl struct{}

func (*bModuleImpl) TestB() string {
    return &amp;quot;B module running&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;建造者模式&#34;&gt;建造者模式&lt;/h2&gt;

&lt;p&gt;Builder 生成器模式：（建造者模式）将一个复杂对象的构建与它表示分离，使得同样的构建过程可以创建不同的表示&lt;/p&gt;

&lt;p&gt;个人想法：建造者的建造流程是在指挥者中，指挥者在用户通知他现在具体的建造者是谁后，建造出对应的产品，建造者中实现了产品的建造细节&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package builder

import (
    &amp;quot;fmt&amp;quot;
)

type IBuilder interface {
    head()
    body()
    foot()
    hand()
}
type Thin struct {
}

func (t *Thin) head() {
    fmt.Println(&amp;quot;我的头很瘦&amp;quot;)
}

func (t *Thin) body() {
    fmt.Println(&amp;quot;我的身体很瘦&amp;quot;)
}
func (t *Thin) foot() {
    fmt.Println(&amp;quot;我的脚很瘦&amp;quot;)
}
func (t *Thin) hand() {
    fmt.Println(&amp;quot;我的身体手很瘦&amp;quot;)
}

type Fat struct {
}

func (t *Fat) head() {
    fmt.Println(&amp;quot;我的头很胖&amp;quot;)
}

func (t *Fat) body() {
    fmt.Println(&amp;quot;我的身体很胖&amp;quot;)
}
func (t *Fat) foot() {
    fmt.Println(&amp;quot;我的脚很胖&amp;quot;)
}
func (t *Fat) hand() {
    fmt.Println(&amp;quot;我的身体手很胖&amp;quot;)
}

type Director struct {
    person IBuilder
}

func (d *Director) CreatePerson() {
    if d == nil {
        return
    }
    d.person.head()
    d.person.body()
    d.person.foot()
    d.person.hand()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;观察者模式&#34;&gt;观察者模式&lt;/h2&gt;

&lt;p&gt;观察者模式用于触发联动。&lt;/p&gt;

&lt;p&gt;一个对象的改变会触发其它观察者的相关动作，而此对象无需关心连动对象的具体实现。&lt;/p&gt;

&lt;p&gt;Observer 观察者模式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    定义了一种一对多的依赖关系，让多个观察者对象同时监听某一个主题对象。
    这个主题对象在状态发生改变时，会通知所有观察者对象，使它们能够自动更新自己。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个系统分成多个系统互相合作，需要维护多个系统的一致性的时候，，不能为了一致性而使其紧密耦合，这样不利于扩展，重用，维护。而观察者有主题subject和观察者observer，一个subject可以有依赖于
他的任意数目的observers，一旦subject发送改变，所有的observer都可以得到通知。subject不需要关心observers的实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package observer

import (
    &amp;quot;fmt&amp;quot;
)

type Subject interface {
    Notify()
    State() int
    SetState(int)
    AddCallFunc(*update)
    RemoveCallFunc(*update)
}

type update func(int)

type SubjectA struct {
    state int
    call  []*update
}

func (s *SubjectA) Notify() {
    if s == nil {
        return
    }
    for _, c := range s.call {
        (*c)(s.state)
    }
}

func (s *SubjectA) State() int {
    if s == nil {
        return 0
    }
    return s.state
}

func (s *SubjectA) SetState(i int) {
    if s == nil {
        return
    }
    s.state = i
}
func (s *SubjectA) AddCallFunc(f *update) {
    if s == nil {
        return
    }
    for _, c := range s.call {
        if c == f {
            return
        }
    }

    s.call = append(s.call, f)
}

func (s *SubjectA) RemoveCallFunc(f *update) {
    if s == nil {
        return
    }
    for i, c := range s.call {
        if c == f {
            s.call = append(s.call[:i], s.call[i+1:]...)
        }
    }
}

func NewSubjectA(s int) *SubjectA {
    return &amp;amp;SubjectA{s, []*update{}}
}

type Observer interface {
    Update(int)
}

type ObserverA struct {
    s     Subject
    state int
}

func (o *ObserverA) Update(s int) {
    if o == nil {
        return
    }
    fmt.Println(&amp;quot;ObserverA&amp;quot;)
    fmt.Println(s)
    fmt.Println(o)
}
func NewObserverA(sa Subject, s int) *ObserverA {
    return &amp;amp;ObserverA{sa, s}
}

type ObserverB struct {
    s     Subject
    state int
}

func (o *ObserverB) Update(s int) {
    if o == nil {
        return
    }
    fmt.Println(&amp;quot;ObserverB&amp;quot;)
    fmt.Println(s)
    fmt.Println(o)
}
func NewObserverB(sa Subject, s int) *ObserverB {
    return &amp;amp;ObserverB{sa, s}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;抽象工厂模式&#34;&gt;抽象工厂模式&lt;/h2&gt;

&lt;p&gt;Abstract Factory 抽象工厂模式：提供一个创建一系列相关或者相互依赖对象的接口，而无需指定他们具体的类。&lt;/p&gt;

&lt;p&gt;工厂模式和抽象工厂模式：感觉抽象工厂可以叫集团模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    工厂模式下，是一个工厂下，对产品的每一个具体生成分配不同的流水线；
    集团模式：在集团下，有不同的工厂，可以生成不同的产品，每个工厂生产出来的同一个型号产品具体细节是不一样
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数据库（interface）&amp;mdash;-选择哪个数据库，创建对数据库的表进行操作实例&lt;/p&gt;

&lt;p&gt;iuser（interface）&amp;mdash;-不同实例对表的操作&lt;/p&gt;

&lt;p&gt;idepartment（interface）&amp;mdash;&amp;ndash;不同实例对表的操作&lt;/p&gt;

&lt;p&gt;其实就是返回对应接口的实现结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package abstractfactory

import (
    &amp;quot;fmt&amp;quot;
)

type User struct {
    id   int
    name string
}

func (u *User) Id() int {
    if u == nil {
        return -1
    }
    return u.id
}

func (u *User) SetId(id int) {
    if u == nil {
        return
    }
    u.id = id
}

func (u *User) Name() string {
    if u == nil {
        return &amp;quot;&amp;quot;
    }
    return u.name
}

func (u *User) SetName(name string) {
    if u == nil {
        return
    }
    u.name = name
}

type Department struct {
    id   int
    name string
}

func (d *Department) Id() int {
    if d == nil {
        return -1
    }
    return d.id
}
func (d *Department) SetId(id int) {
    if d == nil {
        return
    }
    d.id = id
}
func (d *Department) Name() string {
    if d == nil {
        return &amp;quot;&amp;quot;
    }
    return d.name
}
func (d *Department) SetName(name string) {
    if d == nil {
        return
    }
    d.name = name
}

type IUser interface {
    insert(*User)
    getUser(int) *User
}

type SqlServerUser struct {
}

func (s *SqlServerUser) insert(u *User) {
    if s == nil {
        return
    }
    fmt.Println(&amp;quot;往SqlServer的User表中插入一条User&amp;quot;, u)
}

func (s *SqlServerUser) getUser(id int) (u *User) {
    if s == nil {
        return nil
    }
    u = &amp;amp;User{id, &amp;quot;hclacS&amp;quot;}
    fmt.Println(&amp;quot;从SqlServer的User表中获取一条User&amp;quot;, *u)
    return
}

type AccessUser struct {
}

func (s *AccessUser) insert(u *User) {
    if s == nil {
        return
    }
    fmt.Println(&amp;quot;往AccessUser的User表中插入一条User&amp;quot;, *u)
}

func (s *AccessUser) getUser(id int) (u *User) {
    if s == nil {
        return nil
    }
    u = &amp;amp;User{id, &amp;quot;hclacA&amp;quot;}
    fmt.Println(&amp;quot;从AccessUser的User表中获取一条User&amp;quot;, *u)
    return
}

type IDepartment interface {
    insert(*Department)
    getDepartment(int) *Department
}

type SqlServerDepartment struct {
}

func (s *SqlServerDepartment) insert(d *Department) {
    if s == nil {
        return
    }
    fmt.Println(&amp;quot;往SqlServer的Department表中插入一条Department&amp;quot;, *d)
}

func (s *SqlServerDepartment) getDepartment(id int) (u *Department) {
    if s == nil {
        return nil
    }
    u = &amp;amp;Department{id, &amp;quot;hclacDS&amp;quot;}
    fmt.Println(&amp;quot;从SqlServer的Department表中获取一条Department&amp;quot;, *u)
    return
}

type AccessDepartment struct {
}

func (s *AccessDepartment) insert(u *Department) {
    if s == nil {
        return
    }
    fmt.Println(&amp;quot;往AccessDepartment的Department表中插入一条Department&amp;quot;, *u)
}

func (s *AccessDepartment) getDepartment(id int) (u *Department) {
    if s == nil {
        return nil
    }
    u = &amp;amp;Department{id, &amp;quot;hclacDA&amp;quot;}
    fmt.Println(&amp;quot;从AccessDepartment的Department表中获取一条Department&amp;quot;, *u)
    return
}

type Ifactory interface {
    createUser() IUser
    createDepartment() IDepartment
}

type SqlServerFactory struct {
}

func (s *SqlServerFactory) createUser() IUser {
    if s == nil {
        return nil
    }
    u := &amp;amp;SqlServerUser{}
    return u
}

func (s *SqlServerFactory) createDepartment() IDepartment {
    if s == nil {
        return nil
    }
    u := &amp;amp;SqlServerDepartment{}
    return u
}

type AccessFactory struct {
}

func (s *AccessFactory) createUser() IUser {
    if s == nil {
        return nil
    }
    u := &amp;amp;AccessUser{}
    return u
}

func (s *AccessFactory) createDepartment() IDepartment {
    if s == nil {
        return nil
    }
    u := &amp;amp;AccessDepartment{}
    return u
}

type DataAccess struct {
    db string
}

func (d *DataAccess) createUser(db string) IUser {
    if d == nil {
        return nil
    }

    var u IUser

    if db == &amp;quot;sqlserver&amp;quot; {
        u = new(SqlServerUser)
    } else if db == &amp;quot;access&amp;quot; {
        u = new(AccessUser)
    }
    return u
}

func (d *DataAccess) createDepartment(db string) IDepartment {
    if d == nil {
        return nil
    }

    var u IDepartment

    if db == &amp;quot;sqlserver&amp;quot; {
        u = new(SqlServerDepartment)
    } else if db == &amp;quot;access&amp;quot; {
        u = new(AccessDepartment)
    }
    return u
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;状态模式&#34;&gt;状态模式&lt;/h2&gt;

&lt;p&gt;方法实现过长是坏味道。&lt;/p&gt;

&lt;p&gt;State 状态模式：当一个对象的内在状态改变时，允许改变其行为，这个对象看起来像是改变了其类&lt;/p&gt;

&lt;p&gt;策略模式是用在对多个做同样事情（统一接口）的类对象的选择上，而状态模式是：将对某个事情的处理过程抽象成接口和实现类的形式，由context保存一份state，在state实现类处理事情时，修改状态传递给context，由context继续传递到下一个状态处理中。&lt;/p&gt;

&lt;p&gt;状态模式用于分离状态和行为。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package state

import (
    &amp;quot;fmt&amp;quot;
)

// 工作类 --context
type Work struct {
    hour  int
    state State
}

func (w *Work) Hour() int {
    if w == nil {
        return -1
    }
    return w.hour
}

func (w *Work) State() State {
    if w == nil {
        return nil
    }
    return w.state
}

func (w *Work) SetHour(h int) {
    if w == nil {
        return
    }
    w.hour = h
}

func (w *Work) SetState(s State) {
    if w == nil {
        return
    }
    w.state = s
}

func (w *Work) writeProgram() {
    if w == nil {
        return
    }
    w.state.writeProgram(w)
}

func NewWork() *Work {
    state := new(moringState)
    return &amp;amp;Work{state: state}
}

type State interface {
    writeProgram(w *Work)
}

// 上午时分状态类
type moringState struct {
}

func (m *moringState) writeProgram(w *Work) {
    if w.Hour() &amp;lt; 12 {
        fmt.Println(&amp;quot;现在是上午时分&amp;quot;, w.Hour())
    } else {
        w.SetState(new(NoonState))
        w.writeProgram()
    }
}

// 中午时分状态类
type NoonState struct {
}

func (m *NoonState) writeProgram(w *Work) {
    if w.Hour() &amp;lt; 13 {
        fmt.Println(&amp;quot;现在是中午时分&amp;quot;, w.Hour())
    } else {
        w.SetState(new(AfternoonState))
        w.writeProgram()
    }
}

// 下午时分状态类
type AfternoonState struct {
}

func (m *AfternoonState) writeProgram(w *Work) {
    if w.Hour() &amp;lt; 17 {
        fmt.Println(&amp;quot;现在是下午时分&amp;quot;, w.Hour())
    } else {
        w.SetState(new(EveningState))
        w.writeProgram()
    }
}

// 晚上时分状态类
type EveningState struct {
}

func (m *EveningState) writeProgram(w *Work) {
    if w.Hour() &amp;lt; 21 {
        fmt.Println(&amp;quot;现在是晚上时分&amp;quot;, w.Hour())
    } else {
        fmt.Println(&amp;quot;现在开始睡觉&amp;quot;, w.Hour())
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;适配器模式&#34;&gt;适配器模式&lt;/h2&gt;

&lt;p&gt;Adapter 适配器模式：将一个类的接口转换成客户端希望的另一个接口。
        适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作&lt;/p&gt;

&lt;p&gt;代理和适配器：代理和代理的对象接口一致，客户端不知道代理对象，而适配器是客户端想要适配器的接口，适配器对象的接口和客户端想要的不一样，适配器将适配器对象的接口封装一下，改成客户端想要的接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package adapter

import (
    &amp;quot;fmt&amp;quot;
)

type Player interface {
    attack()
    defense()
}

type Forwards struct {
    name string
}

func (f *Forwards) attack() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在进攻&amp;quot;)
}
func (f *Forwards) defense() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在防守&amp;quot;)
}

func NewForwards(name string) Player {
    return &amp;amp;Forwards{name}
}

type Centers struct {
    name string
}

func (f *Centers) attack() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在进攻&amp;quot;)
}
func (f *Centers) defense() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在防守&amp;quot;)
}

func NewCenter(name string) Player {
    return &amp;amp;Centers{name}
}

type ForeignCenter struct {
    name string
}

func (f *ForeignCenter) attack(what string) {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在进攻&amp;quot;)
}
func (f *ForeignCenter) defense() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在防守&amp;quot;)
}

type Translator struct {
    f ForeignCenter
}

// 这是用户想要的接口
func (t *Translator) attack() {
    if t == nil {
        return
    }
    t.f.attack(&amp;quot;进攻&amp;quot;)
}
func (t *Translator) defense() {
    if t == nil {
        return
    }
    t.f.defense()
}

func NewTranslator(name string) Player {
    return &amp;amp;Translator{ForeignCenter{name}}
}









package adapter

//Target 是适配的目标接口
type Target interface {
    Request() string
}

//Adaptee 是被适配的目标接口
type Adaptee interface {
    SpecificRequest() string
}

//NewAdaptee 是被适配接口的工厂函数
func NewAdaptee() Adaptee {
    return &amp;amp;adapteeImpl{}
}

//AdapteeImpl 是被适配的目标类
type adapteeImpl struct{}

//SpecificRequest 是目标类的一个方法
func (*adapteeImpl) SpecificRequest() string {
    return &amp;quot;adaptee method&amp;quot;
}

//NewAdapter 是Adapter的工厂函数
func NewAdapter(adaptee Adaptee) Target {
    return &amp;amp;adapter{
        Adaptee: adaptee,
    }
}

//Adapter 是转换Adaptee为Target接口的适配器
type adapter struct {
    Adaptee
}

//Request 实现Target接口
func (a *adapter) Request() string {
    return a.SpecificRequest()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;备忘录模式&#34;&gt;备忘录模式&lt;/h2&gt;

&lt;p&gt;Memento 备忘录模式：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可以将该对象恢复到原先保存的状态&lt;/p&gt;

&lt;p&gt;将某个类的状态（某些状态，具体有该类决定）保存在另外一个类中（代码级别：提供一个函数能够将状态保存起来，返回出去），保存好状态的类对象是管理类的成员，原来的类需要恢复时，再从管理类中获取原来的状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package memento

import (
    &amp;quot;fmt&amp;quot;
)

type GameRole struct {
    vit int
    atk int
    def int
}

func (g *GameRole) StateDisplay() {
    if g == nil {
        return
    }
    fmt.Println(&amp;quot;角色当前状态：&amp;quot;)
    fmt.Println(&amp;quot;体力：&amp;quot;, g.vit)
    fmt.Println(&amp;quot;攻击：&amp;quot;, g.atk)
    fmt.Println(&amp;quot;防御：&amp;quot;, g.def)
    fmt.Println(&amp;quot;============&amp;quot;)
}

func (g *GameRole) GetInitState() {
    if g == nil {
        return
    }
    g.vit = 100
    g.atk = 100
    g.def = 100
}

func (g *GameRole) Fight() {
    if g == nil {
        return
    }
    g.vit = 0
    g.atk = 0
    g.def = 0
}
func (g *GameRole) SaveState() RoleStateMemento {
    if g == nil {
        return RoleStateMemento{}
    }
    return RoleStateMemento{*g}
}

func (g *GameRole) RecoveryState(r RoleStateMemento) {
    if g == nil {
        return
    }
    g.vit = r.vit
    g.atk = r.atk
    g.def = r.def
}

type RoleStateMemento struct {
    GameRole
}

type RoleStateCaretaker struct {
    memento RoleStateMemento
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;备忘录模式用于保存程序内部状态到外部，又不希望暴露内部状态的情形。&lt;/p&gt;

&lt;p&gt;程序内部状态使用窄接口船体给外部进行存储，从而不暴露程序实现细节。&lt;/p&gt;

&lt;p&gt;备忘录模式同时可以离线保存内部状态，如保存到数据库，文件等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package memento

import &amp;quot;fmt&amp;quot;

type Memento interface{}

type Game struct {
    hp, mp int
}

type gameMemento struct {
    hp, mp int
}

func (g *Game) Play(mpDelta, hpDelta int) {
    g.mp += mpDelta
    g.hp += hpDelta
}

func (g *Game) Save() Memento {
    return &amp;amp;gameMemento{
        hp: g.hp,
        mp: g.mp,
    }
}

func (g *Game) Load(m Memento) {
    gm := m.(*gameMemento)
    g.mp = gm.mp
    g.hp = gm.hp
}

func (g *Game) Status() {
    fmt.Printf(&amp;quot;Current HP:%d, MP:%d\n&amp;quot;, g.hp, g.mp)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;组合模式&#34;&gt;组合模式&lt;/h2&gt;

&lt;p&gt;Composite 组合模式：将对象组合成树形结构，以表示“部分-整体”的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性&lt;/p&gt;

&lt;p&gt;组合模式统一对象和对象集，使得使用相同接口使用对象和对象集。&lt;/p&gt;

&lt;p&gt;组合模式常用于树状结构，用于统一叶子节点和树节点的访问，并且可以用于应用某一操作到所有子节点。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package composite

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;strings&amp;quot;
)

// 公司管理接口
type Company interface {
    add(Company)
    remove(Company)
    display(int)
    lineOfDuty()
}

type RealCompany struct {
    name string
}

// 具体公司
type ConcreateCompany struct {
    RealCompany
    list []Company
}

func NewConcreateCompany(name string) *ConcreateCompany {
    return &amp;amp;ConcreateCompany{RealCompany{name}, []Company{}}
}

func (c *ConcreateCompany) add(newc Company) {
    if c == nil {
        return
    }
    c.list = append(c.list, newc)
}

func (c *ConcreateCompany) remove(delc Company) {
    if c == nil {
        return
    }
    for i, val := range c.list {
        if val == delc {
            c.list = append(c.list[:i], c.list[i+1:]...)
            return
        }
    }
    return
}

func (c *ConcreateCompany) display(depth int) {
    if c == nil {
        return
    }
    fmt.Println(strings.Repeat(&amp;quot;-&amp;quot;, depth), &amp;quot; &amp;quot;, c.name)
    for _, val := range c.list {
        val.display(depth + 2)
    }
}

func (c *ConcreateCompany) lineOfDuty() {
    if c == nil {
        return
    }

    for _, val := range c.list {
        val.lineOfDuty()
    }
}

// 人力资源部门
type HRDepartment struct {
    RealCompany
}

func NewHRDepartment(name string) *HRDepartment {
    return &amp;amp;HRDepartment{RealCompany{name}}
}

func (h *HRDepartment) add(c Company)    {}
func (h *HRDepartment) remove(c Company) {}

func (h *HRDepartment) display(depth int) {
    if h == nil {
        return
    }
    fmt.Println(strings.Repeat(&amp;quot;-&amp;quot;, depth), &amp;quot; &amp;quot;, h.name)
}

func (h *HRDepartment) lineOfDuty() {
    if h == nil {
        return
    }
    fmt.Println(h.name, &amp;quot;员工招聘培训管理&amp;quot;)
}

// 财务部门
type FinanceDepartment struct {
    RealCompany
}

func NewFinanceDepartment(name string) *FinanceDepartment {
    return &amp;amp;FinanceDepartment{RealCompany{name}}
}

func (h *FinanceDepartment) add(c Company)    {}
func (h *FinanceDepartment) remove(c Company) {}

func (h *FinanceDepartment) display(depth int) {
    if h == nil {
        return
    }
    fmt.Println(strings.Repeat(&amp;quot;-&amp;quot;, depth), &amp;quot; &amp;quot;, h.name)
}

func (h *FinanceDepartment) lineOfDuty() {
    if h == nil {
        return
    }
    fmt.Println(h.name, &amp;quot;公司财务收支管理&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;迭代器模式&#34;&gt;迭代器模式&lt;/h2&gt;

&lt;p&gt;Iterator 迭代器模式：提供一种方法顺序访问一个聚合对象中的各个元素，而又不暴露该对象的内部表示。比如下面只是使用next，而不会暴露具体的东西，就像range。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package iterator

//&amp;quot;fmt&amp;quot;

type Book struct {
    name string
}

type Iterator interface {
    first() interface{}
    next() interface{}
}

type BookGroup struct {
    books []Book
}

func (b *BookGroup) add(newb Book) {
    if b == nil {
        return
    }
    b.books = append(b.books, newb)
}

func (b *BookGroup) createIterator() *BookIterator {
    if b == nil {
        return nil
    }
    return &amp;amp;BookIterator{b, 0}
}

type BookIterator struct {
    g     *BookGroup
    index int
}

func (b *BookIterator) first() interface{} {
    if b == nil {
        return nil
    }
    if len(b.g.books) &amp;gt; 0 {
        b.index = 0
        return b.g.books[b.index]
    }
    return nil
}

func (b *BookIterator) next() interface{} {
    if b == nil {
        return nil
    }
    if len(b.g.books) &amp;gt; b.index+1 {
        b.index++
        return b.g.books[b.index]
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;单例模式&#34;&gt;单例模式&lt;/h2&gt;

&lt;p&gt;Singleton 单例：保证一个类仅有一个实例，并提供一个访问它的全局访问点&lt;/p&gt;

&lt;p&gt;用Go实现时，巧妙使用包级别的变量声明规则：小写字母的包级别变量是不对外开放的，创建实例时，用同步库sync.Once来保证全局只有一个对象实例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package singleton

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sync&amp;quot;
)

// 全局实例者
type singleton struct {
    data int
}

// 定义一个包级别的private实例变量
var sin *singleton

// 同步Once,保证每次调用时，只有第一次生效
var once sync.Once

// 获取实例对象函数
func GetSingleton() *singleton {
    once.Do(func() {
        sin = &amp;amp;singleton{12}
    })
    fmt.Println(&amp;quot;实例对象的信息和地址&amp;quot;, sin, &amp;amp;sin)
    return sin
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用懒惰模式的单例模式，使用双重检查加锁保证线程安全&lt;/p&gt;

&lt;h2 id=&#34;桥接模式&#34;&gt;桥接模式&lt;/h2&gt;

&lt;p&gt;Bridge 桥接模式：将抽象部分与它的实现部分分离，使它们都可以独立地变化&lt;/p&gt;

&lt;p&gt;桥接模式是聚合／合成规则的一种使用。&lt;/p&gt;

&lt;p&gt;桥接模式分离抽象部分和实现部分。使得两部分独立扩展。&lt;/p&gt;

&lt;p&gt;桥接模式类似于策略模式，区别在于策略模式封装一系列算法使得算法可以互相替换。&lt;/p&gt;

&lt;p&gt;策略模式使抽象部分和实现部分分离，可以独立变化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package bridge

import (
    &amp;quot;fmt&amp;quot;
)

type Phone struct {
    soft ISoftware
    name string
}

func (p *Phone) setSoft(soft ISoftware) {
    if p == nil {
        return
    }
    p.soft = soft
}

func (p *Phone) Run() {
    if p == nil {
        return
    }
    fmt.Println(p.name)
    p.soft.Run()
}

type PhoneA struct {
    Phone
}

func NewPhoneA(name string) *PhoneA {
    return &amp;amp;PhoneA{Phone{name: name}}
}

type PhoneB struct {
    Phone
}

func NewPhoneB(name string) *PhoneB {
    return &amp;amp;PhoneB{Phone{name: name}}
}

type ISoftware interface {
    Run()
}

type TSoftware struct {
    ISoftware
}

type Software struct {
    name string
}

type SoftwareA struct {
    Software
}

func (s *SoftwareA) Run() {
    if s == nil {
        return
    }
    fmt.Println(s.name)
}

type SoftwareB struct {
    Software
}

/*func (s *SoftwareB) Run() {
    if s == nil {
        return
    }
    fmt.Println(s.name)
}*/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;命令模式&#34;&gt;命令模式&lt;/h2&gt;

&lt;p&gt;命令模式本质是把某个对象的方法调用封装到对象中，方便传递、存储、调用。&lt;/p&gt;

&lt;p&gt;示例中把主板单中的启动(start)方法和重启(reboot)方法封装为命令对象，再传递到主机(box)对象中。于两个按钮进行绑定：&lt;/p&gt;

&lt;p&gt;第一个机箱(box1)设置按钮1(buttion1) 为开机按钮2(buttion2)为重启。
第二个机箱(box1)设置按钮2(buttion2) 为开机按钮1(buttion1)为重启。&lt;/p&gt;

&lt;p&gt;从而得到配置灵活性。&lt;/p&gt;

&lt;p&gt;除了配置灵活外，使用命令模式还可以用作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;批处理
任务队列
undo, redo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等把具体命令封装到对象中使用的场合&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package command

import &amp;quot;fmt&amp;quot;

type Command interface {
    Execute()
}

type StartCommand struct {
    mb *MotherBoard
}

func NewStartCommand(mb *MotherBoard) *StartCommand {
    return &amp;amp;StartCommand{
        mb: mb,
    }
}

func (c *StartCommand) Execute() {
    c.mb.Start()
}

type RebootCommand struct {
    mb *MotherBoard
}

func NewRebootCommand(mb *MotherBoard) *RebootCommand {
    return &amp;amp;RebootCommand{
        mb: mb,
    }
}

func (c *RebootCommand) Execute() {
    c.mb.Reboot()
}

type MotherBoard struct{}

func (*MotherBoard) Start() {
    fmt.Print(&amp;quot;system starting\n&amp;quot;)
}

func (*MotherBoard) Reboot() {
    fmt.Print(&amp;quot;system rebooting\n&amp;quot;)
}

type Box struct {
    buttion1 Command
    buttion2 Command
}

func NewBox(buttion1, buttion2 Command) *Box {
    return &amp;amp;Box{
        buttion1: buttion1,
        buttion2: buttion2,
    }
}

func (b *Box) PressButtion1() {
    b.buttion1.Execute()
}

func (b *Box) PressButtion2() {
    b.buttion2.Execute()
}







package command

import (
    &amp;quot;fmt&amp;quot;
)

// 命令接口 -- 可以保存在请求队形中，方便请求队形处理命令，具体对命令的执行体在实现这个接口的类型结构体中保存着
type Command interface {
    Run()
}

// 请求队形，保存命令列表，在ExecuteCommand函数中遍历执行命令
type Invoker struct {
    comlist []Command
}

// 添加命令
func (i *Invoker) AddCommand(c Command) {
    if i == nil {
        return
    }
    i.comlist = append(i.comlist, c)
}

// 执行命令
func (i *Invoker) ExecuteCommand() {
    if i == nil {
        return
    }
    for _, val := range i.comlist {
        val.Run()
    }
}

func NewInvoker() *Invoker {
    return &amp;amp;Invoker{[]Command{}}
}

// 具体命令,实现Command接口，保存一个对该命令如何处理的执行体
type ConcreteCommandA struct {
    receiver ReceiverA
}

func (c *ConcreteCommandA) SetReceiver(r ReceiverA) {
    if c == nil {
        return
    }
    c.receiver = r
}

// 具体命令的执行体
func (c *ConcreteCommandA) Run() {
    if c == nil {
        return
    }
    c.receiver.Execute()
}

func NewConcreteCommandA() *ConcreteCommandA {
    return &amp;amp;ConcreteCommandA{}
}

// 针对ConcreteCommand，如何处理该命令
type ReceiverA struct {
}

func (r *ReceiverA) Execute() {
    if r == nil {
        return
    }
    fmt.Println(&amp;quot;针对ConcreteCommandA，如何处理该命令&amp;quot;)
}

func NewReceiverA() *ReceiverA {
    return &amp;amp;ReceiverA{}
}

//////////////////////////////////////////////////////////

// 具体命令,实现Command接口，保存一个对该命令如何处理的执行体
type ConcreteCommandB struct {
    receiver ReceiverB
}

func (c *ConcreteCommandB) SetReceiver(r ReceiverB) {
    if c == nil {
        return
    }
    c.receiver = r
}

// 具体命令的执行体
func (c *ConcreteCommandB) Run() {
    if c == nil {
        return
    }
    c.receiver.Execute()
}

func NewConcreteCommandB() *ConcreteCommandB {
    return &amp;amp;ConcreteCommandB{}
}

// 针对ConcreteCommandB，如何处理该命令
type ReceiverB struct {
}

func (r *ReceiverB) Execute() {
    if r == nil {
        return
    }
    fmt.Println(&amp;quot;针对ConcreteCommandB，如何处理该命令&amp;quot;)
}

func NewReceiverB() *ReceiverB {
    return &amp;amp;ReceiverB{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;职责链模式&#34;&gt;职责链模式&lt;/h2&gt;

&lt;p&gt;Chain Of Responsibility 职责链模式：使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package chainofresponsibility

import (
    &amp;quot;fmt&amp;quot;
)

const (
    constHandler = iota
    constHandlerA
    constHandlerB
)

// 处理请求接口
type IHandler interface {
    SetSuccessor(IHandler)
    HandleRequest(int) int
}

// 实现处理请求的接口的基本结构体类型
type Handler struct {
    successor IHandler // 继承者
}

func (h *Handler) SetSuccessor(i IHandler) {
    if h == nil {
        return
    }
    h.successor = i
}

// 具体处理结构体，这里简单处理int类型的请求，判断是否在[1-10]之间，是：处理，否：交给successor处理
type ConcreteHandlerA struct {
    Handler
}

func (c *ConcreteHandlerA) HandleRequest(req int) int {
    if c == nil {
        return constHandler
    }
    if req &amp;gt; 0 &amp;amp;&amp;amp; req &amp;lt; 11 {
        fmt.Println(&amp;quot;ConcreteHandlerA可以处理这个请求&amp;quot;)
        return constHandlerA
    } else if c.successor != nil {
        return c.successor.HandleRequest(req)
    }
    return constHandler
}

func NewConcreteHandlerA() *ConcreteHandlerA {
    return &amp;amp;ConcreteHandlerA{}
}

// 具体处理结构体，这里简单处理int类型的请求，判断是否在[11-20]之间，是：处理，否：交给successor处理
type ConcreteHandlerB struct {
    Handler
}

func (c *ConcreteHandlerB) HandleRequest(req int) int {
    if c == nil {
        return constHandler
    }
    if req &amp;gt; 10 &amp;amp;&amp;amp; req &amp;lt; 21 {
        fmt.Println(&amp;quot;ConcreteHandlerB可以处理这个请求&amp;quot;)
        return constHandlerB
    } else if c.successor != nil {
        return c.successor.HandleRequest(req)
    }
    return constHandler
}

func NewConcreteHandlerB() *ConcreteHandlerB {
    return &amp;amp;ConcreteHandlerB{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;中介者模式&#34;&gt;中介者模式&lt;/h2&gt;

&lt;p&gt;Mediator 中介者模式：用一个中介对象来封装一系列的对象交互。中介这使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。&lt;/p&gt;

&lt;p&gt;每个对象都有一个中介者对象，发生变化时，通知中介者，由中介者判断通知其他的对象。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package Mediator

import (
    &amp;quot;fmt&amp;quot;
)

// 中介者接口
type IMediator interface {
    Send(string, IColleague)
}

// 实现中介者接口的基本类型
type Mediator struct {
}

// 具体的中介者
type ConcreteMediator struct {
    Mediator
    colleagues []IColleague
}

func (m *ConcreteMediator) AddColleague(c IColleague) {
    if m == nil {
        return
    }
    m.colleagues = append(m.colleagues, c)
}

func (m *ConcreteMediator) Send(message string, c IColleague) {
    if m == nil {
        return
    }
    for _, val := range m.colleagues {
        if c == val {
            continue
        }
        val.Notify(message)
    }
}

func NewConcreteMediator() *ConcreteMediator {
    return &amp;amp;ConcreteMediator{}
}

// 合作者接口
type IColleague interface {
    Send(string)
    Notify(string)
}

// 实现合作者接口的基本类型
type Colleague struct {
    mediator IMediator
}

// 具体合作者对象A
type ConcreteColleageA struct {
    Colleague
}

func (c *ConcreteColleageA) Notify(message string) {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;ConcreteColleageA get message:&amp;quot;, message)
}

func (c *ConcreteColleageA) Send(message string) {
    if c == nil {
        return
    }
    c.mediator.Send(message, c)

}
func NewConcreteColleageA(mediator IMediator) *ConcreteColleageA {
    return &amp;amp;ConcreteColleageA{Colleague{mediator}}
}

// 具体合作者对象B
type ConcreteColleageB struct {
    Colleague
}

func (c *ConcreteColleageB) Notify(message string) {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;ConcreteColleageB get message:&amp;quot;, message)
}
func (c *ConcreteColleageB) Send(message string) {
    if c == nil {
        return
    }
    c.mediator.Send(message, c)

}
func NewConcreteColleageB(mediator IMediator) *ConcreteColleageB {
    return &amp;amp;ConcreteColleageB{Colleague{mediator}}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;享元模式&#34;&gt;享元模式&lt;/h2&gt;

&lt;p&gt;Flyweight 享元模式：运用共享技术有效地支持大量细粒度的对象&lt;/p&gt;

&lt;p&gt;主要思想是共享，将可以共享的部分放在对象内部，不可以共享的部分放在外边，享元工厂创建几个享元对象就可以了，这样不同的外部状态，可以针对同一个对象，给人感觉是操作多个对象，通过参数的形式对同一个对象的操作，像是对多个对象的操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package flyweight

import (
    &amp;quot;fmt&amp;quot;
)

// 享元对象接口
type IFlyweight interface {
    Operation(int) //来自外部的状态
}

// 共享对象
type ConcreteFlyweight struct {
    name string
}

func (c *ConcreteFlyweight) Operation(outState int) {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;共享对象响应外部状态&amp;quot;, outState)
}

// 不共享对象
type UnsharedConcreteFlyweight struct {
    name string
}

func (c *UnsharedConcreteFlyweight) Operation(outState int) {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;不共享对象响应外部状态&amp;quot;, outState)
}

// 享元工厂对象
type FlyweightFactory struct {
    flyweights map[string]IFlyweight
}

func (f *FlyweightFactory) Flyweight(name string) IFlyweight {
    if f == nil {
        return nil
    }
    if name == &amp;quot;u&amp;quot; {
        return &amp;amp;UnsharedConcreteFlyweight{&amp;quot;u&amp;quot;}
    } else if _, ok := f.flyweights[name]; !ok {
        f.flyweights[name] = &amp;amp;ConcreteFlyweight{name}
    }
    return f.flyweights[name]
}

func NewFlyweightFactory() *FlyweightFactory {
    ff := FlyweightFactory{make(map[string]IFlyweight)}
    ff.flyweights[&amp;quot;a&amp;quot;] = &amp;amp;ConcreteFlyweight{&amp;quot;a&amp;quot;}
    ff.flyweights[&amp;quot;b&amp;quot;] = &amp;amp;ConcreteFlyweight{&amp;quot;b&amp;quot;}
    return &amp;amp;ff
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;享元模式从对象中剥离出不发生改变且多个实例需要的重复数据，独立出一个享元，使多个对象共享，从而节省内存以及减少对象数量。&lt;/p&gt;

&lt;h2 id=&#34;解释器模式&#34;&gt;解释器模式&lt;/h2&gt;

&lt;p&gt;Interpreter 解释器模式：给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package interpreter

import (
    &amp;quot;fmt&amp;quot;
)

type Context struct {
    text string
}

// 抽象表达式
type IAbstractExpression interface {
    Interpret(*Context)
}

// 终结符表达式
type TerminalExpression struct {
}

func (t *TerminalExpression) Interpret(context *Context) {
    if t == nil {
        return
    }
    context.text = context.text[:len(context.text)-1]
    fmt.Println(context)
}

// 非终结符表达式
type NonterminalExpression struct {
}

func (t *NonterminalExpression) Interpret(context *Context) {
    if t == nil {
        return
    }
    context.text = context.text[:len(context.text)-1]
    fmt.Println(context)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解释器模式定义一套语言文法，并设计该语言解释器，使用户能使用特定文法控制解释器行为。&lt;/p&gt;

&lt;p&gt;解释器模式的意义在于，它分离多种复杂功能的实现，每个功能只需关注自身的解释。&lt;/p&gt;

&lt;p&gt;对于调用者不用关心内部的解释器的工作，只需要用简单的方式组合命令就可以。&lt;/p&gt;

&lt;h2 id=&#34;访问者模式&#34;&gt;访问者模式&lt;/h2&gt;

&lt;p&gt;Visitor 访问者模式：表示一个作用于某对象结构中的各元素的操作，它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package visitor

import (
    &amp;quot;fmt&amp;quot;
)

// 访问接口
type IVisitor interface {
    VisitConcreteElementA(ConcreteElementA)
    VisitConcreteElementB(ConcreteElementB)
}

// 具体访问者A
type ConcreteVisitorA struct {
    name string
}

func (c *ConcreteVisitorA) VisitConcreteElementA(ce ConcreteElementA) {
    if c == nil {
        return
    }
    fmt.Println(ce.name, c.name)
    ce.OperatorA()
}

func (c *ConcreteVisitorA) VisitConcreteElementB(ce ConcreteElementB) {
    if c == nil {
        return
    }
    fmt.Println(ce.name, c.name)
    ce.OperatorB()
}

// 具体访问者B
type ConcreteVisitorB struct {
    name string
}

func (c *ConcreteVisitorB) VisitConcreteElementA(ce ConcreteElementA) {
    if c == nil {
        return
    }
    fmt.Println(ce.name, c.name)
    ce.OperatorA()
}

func (c *ConcreteVisitorB) VisitConcreteElementB(ce ConcreteElementB) {
    if c == nil {
        return
    }
    fmt.Println(ce.name, c.name)
    ce.OperatorB()
}

// 元素接口
type IElement interface {
    Accept(IVisitor)
}

// 具体元素A
type ConcreteElementA struct {
    name string
}

func (c *ConcreteElementA) Accept(visitor IVisitor) {
    if c == nil {
        return
    }
    visitor.VisitConcreteElementA(*c)
}
func (c *ConcreteElementA) OperatorA() {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;OperatorA&amp;quot;)
}

// 具体元素B
type ConcreteElementB struct {
    name string
}

func (c *ConcreteElementB) Accept(visitor IVisitor) {
    if c == nil {
        return
    }
    visitor.VisitConcreteElementB(*c)
}
func (c *ConcreteElementB) OperatorB() {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;OperatorB&amp;quot;)
}

// 维护元素集合
type ObjectStructure struct {
    list []IElement
}

func (o *ObjectStructure) Attach(e IElement) {
    if o == nil || e == nil {
        return
    }
    o.list = append(o.list, e)
}

func (o *ObjectStructure) Detach(e IElement) {
    if o == nil || e == nil {
        return
    }
    for i, val := range o.list {
        if val == e {
            o.list = append(o.list[:i], o.list[i+1:]...)
            break
        }
    }
}

func (o *ObjectStructure) Accept(v IVisitor) {
    if o == nil {
        return
    }
    for _, val := range o.list {
        val.Accept(v)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问者模式可以给一系列对象透明的添加功能，并且把相关代码封装到一个类中。&lt;/p&gt;

&lt;p&gt;对象只要预留访问者接口Accept则后期为对象添加功能的时候就不需要改动对象。&lt;/p&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;其实在golang语言编程设计中分为这么多模式是没有必要的，在以上的实践中，大体可以归纳我们设计时候需要使用的思想模式，其他都是异曲同工的。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;抽象，设定接口，根据不同的参数获取不同的实例，还可以在上面再封装一层，就如我们实现的简单工厂，策略，抽象工厂等模式,这种设计多数也是为了解耦，抽象解耦是设计中的最重要的思想。&lt;/li&gt;
&lt;li&gt;继承，开放封闭原则，可扩展不可修改，使用组合来完成具体实现在子类，父类就是一个接口，比如模版等模式&lt;/li&gt;
&lt;li&gt;封装，符合迪米特法则，就是在实现的基础上新建结构体暴露接口，比如外观，建造者等模式&lt;/li&gt;
&lt;li&gt;特殊场景，观察者模式，这类就是用于注册观察通知的使用方式.
        单例,全局一份。
        适配器，将两个不一样的结构进行适配。。。还有好几个，可以具体去看，这边就不一一列举了。&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>数据库mysql系列---- mysql前置缓存redis</title>
          <link>https://kingjcy.github.io/post/database/mysql/redis-mysql/</link>
          <pubDate>Sun, 22 Jan 2017 14:41:38 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/database/mysql/redis-mysql/</guid>
          <description>&lt;p&gt;mysql前置缓存redis是我们经常使用的提供性能的方案。&lt;/p&gt;

&lt;h1 id=&#34;常规使用&#34;&gt;常规使用&lt;/h1&gt;

&lt;p&gt;1、基于binlog使用mysql_udf_redis，将数据库中的数据同步到Redis。&lt;/p&gt;

&lt;p&gt;      无论MySQL还是Redis，自身都带有数据同步的机制，像比较常用的MySQL的Master/Slave模式，就是由Slave端分析Master的binlog来实现的，这样的数据其实还是一个异步过程，只不过当服务器都在同一内网时，异步的延迟几乎可以忽略。&lt;/p&gt;

&lt;p&gt;      那么理论上我们也可以用同样方式，分析MySQL的binlog文件并将数据插入Redis。但是这需要对binlog文件以及MySQL有非常深入的理解，同时由于binlog存在Statement/Row/Mixedlevel多种形式，分析binlog实现同步的工作量是非常大的。&lt;/p&gt;

&lt;p&gt;2、通过MySQL自动同步刷新Redis&lt;/p&gt;

&lt;p&gt;     当我们在业务层有数据查询需求时，先到Redis缓存中查询，如果查不到，再到MySQL数据库中查询，同时将查到的数据更新到Redis里；当我们在业务层有修改插入数据需求时，直接向MySQL发起请求，同时更新Redis缓存。 就是MySQL的CRUD发生后自动地更新到Redis里，这需要通过MySQL UDF来实现。具体来说，我们把更新Redis的逻辑放到MySQL中去做，即定义一个触发器Trigger，监听CRUD这些操作，当操作发生后，调用对应的UDF函数，远程写回Redis，所以业务逻辑只需要负责更新MySQL就行了，剩下的交给MySQL UDF去完成。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在我们的实际开发当中往往采用如下方式实现实现Mysql和Redis数据同步：当我们在MySQL数据库中进行增删改的时候，我们在增删改的service层将缓存中的数据清除，这个时候用户在此请求的时候我们缓存中没有数据了，直接去数据库中查询，查询回来之后将缓存中的数据放缓存当中，这个时候缓存中的数据就是最新的数据。&lt;/p&gt;

&lt;h1 id=&#34;实战&#34;&gt;实战&lt;/h1&gt;

&lt;p&gt;后台服务器信息查询&lt;/p&gt;

&lt;p&gt;1、后台重kafka订阅信息，如果有服务器信息，后台系统进行消费，放入到mysql数据库中&lt;/p&gt;

&lt;p&gt;2、很多场景都需要查询服务器的信息，比如我们接受到的zbabix的数据，需要根据ip的去查询服务器的信息&lt;/p&gt;

&lt;p&gt;3、这边在查询之间就加了一层redis缓存，先去redis缓存查数据，如果查到数据，就返回，如果没有查到数据就到mysql数据库进行查询，将查到的数据返回的同时更新到redis中，key是IP，json信息是value，同时设置过期事件，用来保存经常查询的数据。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Strings</title>
          <link>https://kingjcy.github.io/post/golang/go-strings/</link>
          <pubDate>Wed, 12 Oct 2016 19:37:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-strings/</guid>
          <description>&lt;p&gt;平时在开发过程中， 和字符串打交道还是比较多的，比如分割， 去除， 替换等等常用的方法， 这些都是由strings包来提供的。&lt;/p&gt;

&lt;h1 id=&#34;基本应用&#34;&gt;基本应用&lt;/h1&gt;

&lt;h2 id=&#34;字符串比较&#34;&gt;字符串比较&lt;/h2&gt;

&lt;p&gt;Compare 函数，用于比较两个字符串的大小，如果两个字符串相等，返回为 0。如果 a 小于 b ，返回 -1 ，反之返回 1 。不推荐使用这个函数，直接使用 == != &amp;gt; &amp;lt; &amp;gt;= &amp;lt;= 等一系列运算符更加直观。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Compare(a, b string) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EqualFold 函数，计算 s 与 t 忽略字母大小写后是否相等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func EqualFold(s, t string) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := &amp;quot;gopher&amp;quot;
b := &amp;quot;hello world&amp;quot;
fmt.Println(strings.Compare(a, b))
fmt.Println(strings.Compare(a, a))
fmt.Println(strings.Compare(b, a))

fmt.Println(strings.EqualFold(&amp;quot;GO&amp;quot;, &amp;quot;go&amp;quot;))
fmt.Println(strings.EqualFold(&amp;quot;壹&amp;quot;, &amp;quot;一&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-1
0
1
true
false
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;是否存在某个字符或子串&#34;&gt;是否存在某个字符或子串&lt;/h2&gt;

&lt;p&gt;有三个函数做这件事：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 子串 substr 在 s 中，返回 true
func Contains(s, substr string) bool
// chars 中任何一个 Unicode 代码点在 s 中，返回 true
func ContainsAny(s, chars string) bool
// Unicode 代码点 r 在 s 中，返回 true
func ContainsRune(s string, r rune) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里对 ContainsAny 函数进行一下说明，看如下例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.ContainsAny(&amp;quot;team&amp;quot;, &amp;quot;i&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;failure&amp;quot;, &amp;quot;u &amp;amp; i&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;in failure&amp;quot;, &amp;quot;s g&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;foo&amp;quot;, &amp;quot;&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;&amp;quot;, &amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;false
true
true
false
false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说，第二个参数 chars 中任意一个字符（Unicode Code Point）如果在第一个参数 s 中存在，则返回 true。&lt;/p&gt;

&lt;p&gt;查看这三个函数的源码，发现它们只是调用了相应的 Index 函数（子串出现的位置），然后和 0 作比较返回 true 或 fale。如，Contains：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Contains(s, substr string) bool {
  return Index(s, substr) &amp;gt;= 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;index则使用了我们常用的字符串匹配算法的rk算法。&lt;/p&gt;

&lt;h2 id=&#34;子串出现次数-字符串匹配&#34;&gt;子串出现次数 ( 字符串匹配 )&lt;/h2&gt;

&lt;p&gt;在数据结构与算法中，可能会讲解以下字符串匹配算法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;朴素匹配算法&lt;/li&gt;
&lt;li&gt;KMP 算法&lt;/li&gt;
&lt;li&gt;Rabin-Karp 算法&lt;/li&gt;
&lt;li&gt;Boyer-Moore 算法&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有其他的算法，这里不一一列举，感兴趣的可以网上搜一下。&lt;/p&gt;

&lt;p&gt;在 Go 中，查找子串出现次数即字符串模式匹配，根据长度，分别实现的是BF和 Rabin-Karp 算法。Count 函数的签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Count(s, sep string) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 Count 的实现中，处理了几种特殊情况，属于字符匹配预处理的一部分。这里要特别说明一下的是当 sep 为空时，Count 的返回值是：utf8.RuneCountInString(s) + 1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Count(&amp;quot;cheese&amp;quot;, &amp;quot;e&amp;quot;))
fmt.Println(len(&amp;quot;谷歌中国&amp;quot;))
fmt.Println(strings.Count(&amp;quot;谷歌中国&amp;quot;, &amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3
12
5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 Rabin-Karp 算法的实现，有兴趣的可以看看 Count 的源码。&lt;/p&gt;

&lt;p&gt;另外，Count 是计算子串在字符串中出现的无重叠的次数，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Count(&amp;quot;fivevev&amp;quot;, &amp;quot;vev&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串分割为-string&#34;&gt;字符串分割为[]string&lt;/h2&gt;

&lt;p&gt;这个需求很常见，倒不一定是为了得到[]string。&lt;/p&gt;

&lt;p&gt;该包提供了六个三组分割函数：Fields和FieldsFunc、Split和SplitAfter、SplitN和SplitAfterN。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Fields和FieldsFunc&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Fields(s string) []string
func FieldsFunc(s string, f func(rune) bool) []string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fields 用一个或多个连续的空格分隔字符串 s，返回子字符串的数组（slice）。如果字符串 s 只包含空格，则返回空列表 ([]string 的长度为 0）。其中，空格的定义是 unicode.IsSpace，之前已经介绍过。&lt;/p&gt;

&lt;p&gt;常见间隔符包括：&amp;rsquo;\t&amp;rsquo;, &amp;lsquo;\n&amp;rsquo;, &amp;lsquo;\v&amp;rsquo;, &amp;lsquo;\f&amp;rsquo;, &amp;lsquo;\r&amp;rsquo;, &amp;lsquo; &amp;lsquo;, U+0085 (NEL), U+00A0 (NBSP)&lt;/p&gt;

&lt;p&gt;由于是用空格分隔，因此结果中不会含有空格或空子字符串，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;Fields are: %q&amp;quot;, strings.Fields(&amp;quot;  foo bar  baz   &amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Fields are: [&amp;quot;foo&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;baz&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FieldsFunc通过实现一个回调函数来指定分隔字符串 s 的字符。比如上面的例子，我们通过 FieldsFunc 来实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.FieldsFunc(&amp;quot;  foo bar  baz   &amp;quot;, unicode.IsSpace))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上，Fields 函数就是调用 FieldsFunc 实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Fields(s string) []string {
  return FieldsFunc(s, unicode.IsSpace)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Split和SplitAfter、SplitN和SplitAfterN&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之所以将这四个函数放在一起讲，是因为它们都是通过一个同一个内部函数来实现的。它们的函数签名及其实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Split(s, sep string) []string { return genSplit(s, sep, 0, -1) }
func SplitAfter(s, sep string) []string { return genSplit(s, sep, len(sep), -1) }
func SplitN(s, sep string, n int) []string { return genSplit(s, sep, 0, n) }
func SplitAfterN(s, sep string, n int) []string { return genSplit(s, sep, len(sep), n) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它们都调用了 genSplit 函数。&lt;/p&gt;

&lt;p&gt;这四个函数都是通过 sep 进行分割，返回[]string。如果 sep 为空，相当于分成一个个的 UTF-8 字符，如 Split(&amp;ldquo;abc&amp;rdquo;,&amp;ldquo;&amp;rdquo;)，得到的是[a b c]。&lt;/p&gt;

&lt;p&gt;Split(s, sep) 和 SplitN(s, sep, -1) 等价；SplitAfter(s, sep) 和 SplitAfterN(s, sep, -1) 等价。&lt;/p&gt;

&lt;p&gt;那么，Split 和 SplitAfter 有啥区别呢？通过这两句代码的结果就知道它们的区别了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.SplitAfter(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;quot;foo&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;baz&amp;quot;]
[&amp;quot;foo,&amp;quot; &amp;quot;bar,&amp;quot; &amp;quot;baz&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说，Split 会将 s 中的 sep 去掉，而 SplitAfter 会保留 sep。&lt;/p&gt;

&lt;p&gt;带 N 的方法可以通过最后一个参数 n 控制返回的结果中的 slice 中的元素个数，当 n &amp;lt; 0 时，返回所有的子字符串；当 n == 0 时，返回的结果是 nil；当 n &amp;gt; 0 时，表示返回的 slice 中最多只有 n 个元素，其中，最后一个元素不会分割，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.SplitN(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;, 2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;quot;foo&amp;quot; &amp;quot;bar,baz&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外看一下官方文档提供的例子，注意一下输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;a,b,c&amp;quot;, &amp;quot;,&amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;a man a plan a canal panama&amp;quot;, &amp;quot;a &amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot; xyz &amp;quot;, &amp;quot;&amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;&amp;quot;, &amp;quot;Bernardo O&#39;Higgins&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;]
[&amp;quot;&amp;quot; &amp;quot;man &amp;quot; &amp;quot;plan &amp;quot; &amp;quot;canal panama&amp;quot;]
[&amp;quot; &amp;quot; &amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot; &amp;quot; &amp;quot;]
[&amp;quot;&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串是否有某个前缀或后缀&#34;&gt;字符串是否有某个前缀或后缀&lt;/h2&gt;

&lt;p&gt;这两个函数比较简单，源码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// s 中是否以 prefix 开始
func HasPrefix(s, prefix string) bool {
  return len(s) &amp;gt;= len(prefix) &amp;amp;&amp;amp; s[0:len(prefix)] == prefix
}
// s 中是否以 suffix 结尾
func HasSuffix(s, suffix string) bool {
  return len(s) &amp;gt;= len(suffix) &amp;amp;&amp;amp; s[len(s)-len(suffix):] == suffix
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 prefix 或 suffix 为 &amp;ldquo;&amp;rdquo; , 返回值总是 true。&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.HasPrefix(&amp;quot;Gopher&amp;quot;, &amp;quot;Go&amp;quot;))
fmt.Println(strings.HasPrefix(&amp;quot;Gopher&amp;quot;, &amp;quot;C&amp;quot;))
fmt.Println(strings.HasPrefix(&amp;quot;Gopher&amp;quot;, &amp;quot;&amp;quot;))
fmt.Println(strings.HasSuffix(&amp;quot;Amigo&amp;quot;, &amp;quot;go&amp;quot;))
fmt.Println(strings.HasSuffix(&amp;quot;Amigo&amp;quot;, &amp;quot;Ami&amp;quot;))
fmt.Println(strings.HasSuffix(&amp;quot;Amigo&amp;quot;, &amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;true
false
true
true
false
true
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符或子串在字符串中出现的位置&#34;&gt;字符或子串在字符串中出现的位置&lt;/h2&gt;

&lt;p&gt;有一序列函数与该功能有关：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 在 s 中查找 sep 的第一次出现，返回第一次出现的索引
func Index(s, sep string) int
// 在 s 中查找字节 c 的第一次出现，返回第一次出现的索引
func IndexByte(s string, c byte) int
// chars 中任何一个 Unicode 代码点在 s 中首次出现的位置
func IndexAny(s, chars string) int
// 查找字符 c 在 s 中第一次出现的位置，其中 c 满足 f(c) 返回 true
func IndexFunc(s string, f func(rune) bool) int
// Unicode 代码点 r 在 s 中第一次出现的位置
func IndexRune(s string, r rune) int

// 有三个对应的查找最后一次出现的位置
func LastIndex(s, sep string) int
func LastIndexByte(s string, c byte) int
func LastIndexAny(s, chars string) int
func LastIndexFunc(s string, f func(rune) bool) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一序列函数，只举 IndexFunc 的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;han := func(c rune) bool {
    return unicode.Is(unicode.Han, c) // 汉字
}
fmt.Println(strings.IndexFunc(&amp;quot;Hello, world&amp;quot;, han))
fmt.Println(strings.IndexFunc(&amp;quot;Hello, 世界&amp;quot;, han))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-1
7
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串-join-操作&#34;&gt;字符串 JOIN 操作&lt;/h2&gt;

&lt;p&gt;将字符串数组（或 slice）连接起来可以通过 Join 实现，函数签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Join(a []string, sep string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;假如没有这个库函数，我们自己实现一个，我们会这么实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Join(str []string, sep string) string {
  // 特殊情况应该做处理
  if len(str) == 0 {
      return &amp;quot;&amp;quot;
  }
  if len(str) == 1 {
      return str[0]
  }
  buffer := bytes.NewBufferString(str[0])
  for _, s := range str[1:] {
      buffer.WriteString(sep)
      buffer.WriteString(s)
  }
  return buffer.String()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里，我们使用了 bytes 包的 Buffer 类型，避免大量的字符串连接操作（因为 Go 中字符串是不可变的）。我们再看一下标准库的实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Join(a []string, sep string) string {
  if len(a) == 0 {
      return &amp;quot;&amp;quot;
  }
  if len(a) == 1 {
      return a[0]
  }
  n := len(sep) * (len(a) - 1)
  for i := 0; i &amp;lt; len(a); i++ {
      n += len(a[i])
  }

  b := make([]byte, n)
  bp := copy(b, a[0])
  for _, s := range a[1:] {
      bp += copy(b[bp:], sep)
      bp += copy(b[bp:], s)
  }
  return string(b)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;标准库的实现没有用 bytes 包，当然也不会简单的通过 + 号连接字符串。Go 中是不允许循环依赖的，标准库中很多时候会出现代码拷贝，而不是引入某个包。这里 Join 的实现方式挺好，我个人观点认为，不直接使用 bytes 包，也是不想依赖 bytes 包（其实 bytes 中的实现也是 copy 方式）。&lt;/p&gt;

&lt;p&gt;简单使用示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(Join([]string{&amp;quot;name=xxx&amp;quot;, &amp;quot;age=xx&amp;quot;}, &amp;quot;&amp;amp;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name=xxx&amp;amp;age=xx
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串重复几次&#34;&gt;字符串重复几次&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func Repeat(s string, count int) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 s 重复 count 次，如果 count 为负数或返回值长度 len(s)*count 超出 string 上限会导致 panic，这个函数使用很简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(&amp;quot;ba&amp;quot; + strings.Repeat(&amp;quot;na&amp;quot;, 2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;banana
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符替换&#34;&gt;字符替换&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;map&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Map(mapping func(rune) rune, s string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Map 函数，将 s 的每一个字符按照 mapping 的规则做映射替换，如果 mapping 返回值 &amp;lt;0 ，则舍弃该字符。该方法只能对每一个字符做处理，但处理方式很灵活，可以方便的过滤，筛选汉字等。&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mapping := func(r rune) rune {
    switch {
    case r &amp;gt;= &#39;A&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;Z&#39;: // 大写字母转小写
        return r + 32
    case r &amp;gt;= &#39;a&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;z&#39;: // 小写字母不处理
        return r
    case unicode.Is(unicode.Han, r): // 汉字换行
        return &#39;\n&#39;
    }
    return -1 // 过滤所有非字母、汉字的字符
}
fmt.Println(strings.Map(mapping, &amp;quot;Hello你#￥%……\n（&#39;World\n,好Hello^(&amp;amp;(*界gopher...&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hello
world
hello
gopher
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;replace&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;进行字符串替换时，考虑到性能问题，能不用正则尽量别用，应该用这里的函数。&lt;/p&gt;

&lt;p&gt;字符串替换的函数签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 用 new 替换 s 中的 old，一共替换 n 个。
// 如果 n &amp;lt; 0，则不限制替换次数，即全部替换
func Replace(s, old, new string, n int) string
// 该函数内部直接调用了函数 Replace(s, old, new , -1)
func ReplaceAll(s, old, new string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Replace(&amp;quot;oink oink oink&amp;quot;, &amp;quot;k&amp;quot;, &amp;quot;ky&amp;quot;, 2))
fmt.Println(strings.Replace(&amp;quot;oink oink oink&amp;quot;, &amp;quot;oink&amp;quot;, &amp;quot;moo&amp;quot;, -1))
fmt.Println(strings.ReplaceAll(&amp;quot;oink oink oink&amp;quot;, &amp;quot;oink&amp;quot;, &amp;quot;moo&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;oinky oinky oink
moo moo moo
moo moo moo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我们希望一次替换多个，比如我们希望替换 This is &lt;b&gt;HTML&lt;/b&gt; 中的 &amp;lt; 和 &amp;gt; 为 &amp;lt; 和 &amp;gt;，可以调用上面的函数两次。但标准库提供了另外的方法进行这种替换。&lt;/p&gt;

&lt;h2 id=&#34;大小写转换&#34;&gt;大小写转换&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func ToLower(s string) string
func ToLowerSpecial(c unicode.SpecialCase, s string) string
func ToUpper(s string) string
func ToUpperSpecial(c unicode.SpecialCase, s string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;大小写转换包含了 4 个相关函数，ToLower,ToUpper 用于大小写转换。ToLowerSpecial,ToUpperSpecial 可以转换特殊字符的大小写。 举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.ToLower(&amp;quot;HELLO WORLD&amp;quot;))
fmt.Println(strings.ToLower(&amp;quot;Ā Á Ǎ À&amp;quot;))
fmt.Println(strings.ToLowerSpecial(unicode.TurkishCase, &amp;quot;壹&amp;quot;))
fmt.Println(strings.ToLowerSpecial(unicode.TurkishCase, &amp;quot;HELLO WORLD&amp;quot;))
fmt.Println(strings.ToLower(&amp;quot;Önnek İş&amp;quot;))
fmt.Println(strings.ToLowerSpecial(unicode.TurkishCase, &amp;quot;Önnek İş&amp;quot;))

fmt.Println(strings.ToUpper(&amp;quot;hello world&amp;quot;))
fmt.Println(strings.ToUpper(&amp;quot;ā á ǎ à&amp;quot;))
fmt.Println(strings.ToUpperSpecial(unicode.TurkishCase, &amp;quot;一&amp;quot;))
fmt.Println(strings.ToUpperSpecial(unicode.TurkishCase, &amp;quot;hello world&amp;quot;))
fmt.Println(strings.ToUpper(&amp;quot;örnek iş&amp;quot;))
fmt.Println(strings.ToUpperSpecial(unicode.TurkishCase, &amp;quot;örnek iş&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hello world
ā á ǎ à
壹
hello world
önnek iş
önnek iş
HELLO WORLD
Ā Á Ǎ À       // 汉字拼音有效
一           //  汉字无效
HELLO WORLD
ÖRNEK IŞ
ÖRNEK İŞ    // 有细微差别
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;标题处理&#34;&gt;标题处理&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func Title(s string) string
func ToTitle(s string) string
func ToTitleSpecial(c unicode.SpecialCase, s string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;标题处理包含 3 个相关函数，其中 Title 会将 s 每个单词的首字母大写，不处理该单词的后续字符。ToTitle 将 s 的每个字母大写。ToTitleSpecial 将 s 的每个字母大写，并且会将一些特殊字母转换为其对应的特殊大写字母。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Title(&amp;quot;hElLo wOrLd&amp;quot;))
fmt.Println(strings.ToTitle(&amp;quot;hElLo wOrLd&amp;quot;))
fmt.Println(strings.ToTitleSpecial(unicode.TurkishCase, &amp;quot;hElLo wOrLd&amp;quot;))
fmt.Println(strings.Title(&amp;quot;āáǎà ōóǒò êēéěè&amp;quot;))
fmt.Println(strings.ToTitle(&amp;quot;āáǎà ōóǒò êēéěè&amp;quot;))
fmt.Println(strings.ToTitleSpecial(unicode.TurkishCase, &amp;quot;āáǎà ōóǒò êēéěè&amp;quot;))
fmt.Println(strings.Title(&amp;quot;dünyanın ilk borsa yapısı Aizonai kabul edilir&amp;quot;))
fmt.Println(strings.ToTitle(&amp;quot;dünyanın ilk borsa yapısı Aizonai kabul edilir&amp;quot;))
fmt.Println(strings.ToTitleSpecial(unicode.TurkishCase, &amp;quot;dünyanın ilk borsa yapısı Aizonai kabul edilir&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HElLo WOrLd
HELLO WORLD
HELLO WORLD
Āáǎà Ōóǒò Êēéěè
ĀÁǍÀ ŌÓǑÒ ÊĒÉĚÈ
ĀÁǍÀ ŌÓǑÒ ÊĒÉĚÈ
Dünyanın Ilk Borsa Yapısı Aizonai Kabul Edilir
DÜNYANIN ILK BORSA YAPISI AIZONAI KABUL EDILIR
DÜNYANIN İLK BORSA YAPISI AİZONAİ KABUL EDİLİR
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;修剪&#34;&gt;修剪&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 将 s 左侧和右侧中匹配 cutset 中的任一字符的字符去掉
func Trim(s string, cutset string) string
// 将 s 左侧的匹配 cutset 中的任一字符的字符去掉
func TrimLeft(s string, cutset string) string
// 将 s 右侧的匹配 cutset 中的任一字符的字符去掉
func TrimRight(s string, cutset string) string
// 如果 s 的前缀为 prefix 则返回去掉前缀后的 string , 否则 s 没有变化。
func TrimPrefix(s, prefix string) string
// 如果 s 的后缀为 suffix 则返回去掉后缀后的 string , 否则 s 没有变化。
func TrimSuffix(s, suffix string) string
// 将 s 左侧和右侧的间隔符去掉。常见间隔符包括：&#39;\t&#39;, &#39;\n&#39;, &#39;\v&#39;, &#39;\f&#39;, &#39;\r&#39;, &#39; &#39;, U+0085 (NEL)
func TrimSpace(s string) string
// 将 s 左侧和右侧的匹配 f 的字符去掉
func TrimFunc(s string, f func(rune) bool) string
// 将 s 左侧的匹配 f 的字符去掉
func TrimLeftFunc(s string, f func(rune) bool) string
// 将 s 右侧的匹配 f 的字符去掉
func TrimRightFunc(s string, f func(rune) bool) string
包含了 9 个相关函数用于修剪字符串。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x := &amp;quot;!!!@@@你好,!@#$ Gophers###$$$&amp;quot;
fmt.Println(strings.Trim(x, &amp;quot;@#$!%^&amp;amp;*()_+=-&amp;quot;))
fmt.Println(strings.TrimLeft(x, &amp;quot;@#$!%^&amp;amp;*()_+=-&amp;quot;))
fmt.Println(strings.TrimRight(x, &amp;quot;@#$!%^&amp;amp;*()_+=-&amp;quot;))
fmt.Println(strings.TrimSpace(&amp;quot; \t\n Hello, Gophers \n\t\r\n&amp;quot;))
fmt.Println(strings.TrimPrefix(x, &amp;quot;!&amp;quot;))
fmt.Println(strings.TrimSuffix(x, &amp;quot;$&amp;quot;))

f := func(r rune) bool {
    return !unicode.Is(unicode.Han, r) // 非汉字返回 true
}
fmt.Println(strings.TrimFunc(x, f))
fmt.Println(strings.TrimLeftFunc(x, f))
fmt.Println(strings.TrimRightFunc(x, f))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;你好,!@#$ Gophers
你好,!@#$ Gophers###$$$
!!!@@@你好,!@#$ Gophers
Hello, Gophers
!!@@@你好,!@#$ Gophers###$$$
!!!@@@你好,!@#$ Gophers###$$
你好
你好,!@#$ Gophers###$$$
!!!@@@你好
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;

&lt;h2 id=&#34;replacer-类型&#34;&gt;Replacer 类型&lt;/h2&gt;

&lt;p&gt;这是一个结构，没有导出任何字段，实例化通过 func NewReplacer(oldnew &amp;hellip;string) *Replacer 函数进行，其中不定参数 oldnew 是 old-new 对，即进行多个替换。如果 oldnew 长度与奇数，会导致 panic.&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r := strings.NewReplacer(&amp;quot;&amp;lt;&amp;quot;, &amp;quot;&amp;amp;lt;&amp;quot;, &amp;quot;&amp;gt;&amp;quot;, &amp;quot;&amp;amp;gt;&amp;quot;)
fmt.Println(r.Replace(&amp;quot;This is &amp;lt;b&amp;gt;HTML&amp;lt;/b&amp;gt;!&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This is &amp;amp;lt;b&amp;amp;gt;HTML&amp;amp;lt;/b&amp;amp;gt;!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，Replacer 还提供了另外一个方法，它在替换之后将结果写入 io.Writer 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Replacer) WriteString(w io.Writer, s string) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reader-类型&#34;&gt;Reader 类型&lt;/h2&gt;

&lt;p&gt;看到名字就能猜到，这是实现了 io 包中的接口。其实这就是缓存io，对缓存中的string进行读写操作。&lt;/p&gt;

&lt;p&gt;它实现了 io.Reader（Read 方法），io.ReaderAt（ReadAt 方法），io.Seeker（Seek 方法），io.WriterTo（WriteTo 方法），io.ByteReader（ReadByte 方法），io.ByteScanner（ReadByte 和 UnreadByte 方法），io.RuneReader（ReadRune 方法） 和 io.RuneScanner（ReadRune 和 UnreadRune 方法）。&lt;/p&gt;

&lt;p&gt;Reader 结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader struct {
    s        string    // Reader 读取的数据来源
    i        int // current reading index（当前读的索引位置）
    prevRune int // index of previous rune; or &amp;lt; 0（前一个读取的 rune 索引位置）
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见 Reader 结构没有导出任何字段，而是提供一个实例化方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(s string) *Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方法接收一个字符串，返回的 Reader 实例就是从该参数字符串读数据。&lt;/p&gt;

&lt;p&gt;在后面学习了 &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/#reader-类型&#34;&gt;bytes&lt;/a&gt; 包之后，可以知道 bytes.NewBufferString 有类似的功能，不过，如果只是为了读取，NewReader 会更高效。&lt;/p&gt;

&lt;h2 id=&#34;builder-类型&#34;&gt;Builder 类型&lt;/h2&gt;

&lt;p&gt;这个类型也是缓存io的一种实现方式。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Builder struct {
    addr *Builder // of receiver, to detect copies by value
    buf  []byte
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该类型实现了 io 包下的 Writer, ByteWriter, StringWriter 等接口，可以向该对象内写入数据，Builder 没有实现 Reader 等接口，所以该类型不可读，但提供了 String 方法可以获取对象内的数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 该方法向 b 写入一个字节
func (b *Builder) WriteByte(c byte) error
// WriteRune 方法向 b 写入一个字符
func (b *Builder) WriteRune(r rune) (int, error)
// WriteRune 方法向 b 写入字节数组 p
func (b *Builder) Write(p []byte) (int, error)
// WriteRune 方法向 b 写入字符串 s
func (b *Builder) WriteString(s string) (int, error)
// Len 方法返回 b 的数据长度。
func (b *Builder) Len() int
// Cap 方法返回 b 的 cap。
func (b *Builder) Cap() int
// Grow 方法将 b 的 cap 至少增加 n (可能会更多)。如果 n 为负数，会导致 panic。
func (b *Builder) Grow(n int)
// Reset 方法将 b 清空 b 的所有内容。
func (b *Builder) Reset()
// String 方法将 b 的数据以 string 类型返回。
func (b *Builder) String() string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Builder 有 4 个与写入相关的方法，这 4 个方法的 error 都总是为 nil.&lt;/p&gt;

&lt;p&gt;Builder 的 cap 会自动增长，一般不需要手动调用 Grow 方法。&lt;/p&gt;

&lt;p&gt;String 方法可以方便的获取 Builder 的内容。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b := strings.Builder{}
_ = b.WriteByte(&#39;7&#39;)
n, _ := b.WriteRune(&#39;夕&#39;)
fmt.Println(n)
n, _ = b.Write([]byte(&amp;quot;Hello, World&amp;quot;))
fmt.Println(n)
n, _ = b.WriteString(&amp;quot;你好，世界&amp;quot;)
fmt.Println(n)
fmt.Println(b.Len())
fmt.Println(b.Cap())
b.Grow(100)
fmt.Println(b.Len())
fmt.Println(b.Cap())
fmt.Println(b.String())
b.Reset()
fmt.Println(b.String())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3
12
15
31
32
31
164
7夕Hello, World你好，世界
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这边主要是要注意，使用返回值作为新值，原来值是不变的。&lt;/strong&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Strconv</title>
          <link>https://kingjcy.github.io/post/golang/go-strconv/</link>
          <pubDate>Wed, 12 Oct 2016 19:33:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-strconv/</guid>
          <description>&lt;p&gt;strconv包实现了基本数据类型和其字符串表示的相互转换。&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;p&gt;strconv主要就是字符之间的转化，我们直接看我们经常的使用就好。&lt;/p&gt;

&lt;h2 id=&#34;parseint&#34;&gt;ParseInt&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func ParseInt(s string, base int, bitSize int) (i int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回字符串表示的整数值，接受正负号。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;base指定进制（2到36），如果base为0，则会从字符串前置判断，&amp;rdquo;0x&amp;rdquo;是16进制，&amp;rdquo;0&amp;rdquo;是8进制，否则是10进制；&lt;/li&gt;
&lt;li&gt;bitSize指定结果必须能无溢出赋值的整数类型，0、8、16、32、64 分别代表 int、int8、int16、int32、int64；返回的err是*NumErr类型的，如果语法有误，err.Error = ErrSyntax；如果结果超出类型范围err.Error = ErrRange。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;int和string的转化&#34;&gt;int和string的转化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;int转string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;s := strconv.Itoa(i)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等价于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s := strconv.FormatInt(int64(i), 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;int64转string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;i := int64(123)
s := strconv.FormatInt(i, 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二个参数为基数，可选2~36&lt;/p&gt;

&lt;p&gt;注：对于无符号整形，可以使用FormatUint(i uint64, base int)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;string转int&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;i, err := strconv.Atoi(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;string转int64&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;i, err := strconv.ParseInt(s, 10, 64)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二个参数为基数（2~36），第三个参数位大小表示期望转换的结果类型，其值可以为0, 8, 16, 32和64，分别对应 int, int8, int16, int32和int64&lt;/p&gt;

&lt;h2 id=&#34;float相关&#34;&gt;float相关&lt;/h2&gt;

&lt;p&gt;float转string：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;v := 3.1415926535
s1 := strconv.FormatFloat(v, &#39;E&#39;, -1, 32)//float32s2 := strconv.FormatFloat(v, &#39;E&#39;, -1, 64)//float64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数原型及参数含义具体可查看：&lt;a href=&#34;https://golang.org/pkg/strconv/#FormatFloat&#34;&gt;https://golang.org/pkg/strconv/#FormatFloat&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;string转float：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s := &amp;quot;3.1415926535&amp;quot;
v1, err := strconv.ParseFloat(v, 32)
v2, err := strconv.ParseFloat(v, 64)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;error相关&#34;&gt;error相关&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;error转string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;err.Error()
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Io</title>
          <link>https://kingjcy.github.io/post/golang/go-io/</link>
          <pubDate>Sat, 30 Jul 2016 20:39:03 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-io/</guid>
          <description>&lt;p&gt;io包提供了所有需要交互的输入输出模式的基础。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;stream&#34;&gt;stream&lt;/h2&gt;

&lt;p&gt;我们先介绍一下stream的概念。stream就是数据流，数据流的概念其实非常基础，最早是在通讯领域使用的概念，这个概念最初在 1998 年由 Henzinger 在文献中提出，他将数据流定义为 “只能以事先规定好的顺序被读取一次的数据的一个序列”&lt;/p&gt;

&lt;p&gt;数据流就是由数据形成的流，就像由水形成的水流，非常形象，现代语言中，基本上都会有流的支持，比如 C++ 的 iostream，Node.js 的 stream 模块，以及 golang 的 io 包。&lt;/p&gt;

&lt;p&gt;Stream in Golang与流密切相关的就是 bufio io io/ioutil 这几个包：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、io 为 IO 原语（I/O primitives）提供基本的接口
2、io/ioutil 封装一些实用的 I/O 函数
3、fmt 实现格式化 I/O，类似 C 语言中的 printf 和 scanf
4、bufio 实现带缓冲I/O
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;io&#34;&gt;io&lt;/h2&gt;

&lt;p&gt;io 包为 I/O 原语提供了基本的接口。在 io 包中最重要的是两个接口：Reader 和 Writer 接口。本章所提到的各种 IO 包，都跟这两个接口有关，也就是说，只要满足这两个接口，它就可以使用 IO 包的功能。&lt;/p&gt;

&lt;h1 id=&#34;接口&#34;&gt;接口&lt;/h1&gt;

&lt;h2 id=&#34;读取器&#34;&gt;读取器&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;type Reader interface {
    //Read() 方法有两个返回值，一个是读取到的字节数，一个是发生错误时的错误。如果资源内容已全部读取完毕，应该返回 io.EOF 错误。
    Read(p []byte) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;io.Reader 表示一个读取器，它将数据从某个资源读取到传输缓冲区p。在缓冲区中，数据可以被流式传输和使用。&lt;/p&gt;

&lt;p&gt;实现这个接口需要实现如下功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Read 将 len(p) 个字节读取到 p 中。它返回读取的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)） 以及任何遇到的错误。

&lt;ul&gt;
&lt;li&gt;即使 Read 返回的 n &amp;lt; len(p)，它也会在调用过程中占用 len(p) 个字节作为暂存空间。&lt;/li&gt;
&lt;li&gt;若可读取的数据不到 len(p) 个字节，Read 会返回可用数据，而不是等待更多数据。&lt;/li&gt;
&lt;li&gt;当读取的时候没有数据也没有EOF的时候，会阻塞在这边等待。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;当 Read 在成功读取 n &amp;gt; 0 个字节后遇到一个错误或 EOF (end-of-file)，它会返回读取的字节数。

&lt;ul&gt;
&lt;li&gt;它可能会同时在本次的调用中返回一个non-nil错误,或在下一次的调用中返回这个错误（且 n 为 0）。&lt;/li&gt;
&lt;li&gt;一般情况下, Reader会返回一个非0字节数n, 若 n = len(p) 个字节从输入源的结尾处由 Read 返回，Read可能返回 err == EOF 或者 err == nil。并且之后的 Read() 都应该返回 (n:0, err:EOF)。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;调用者在考虑错误之前应当首先处理返回的数据。这样做可以正确地处理在读取一些字节后产生的 I/O 错误，同时允许EOF的出现。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于要用作读取器的类型，它必须实现 io.Reader 接口的唯一一个方法 Read(p []byte)。换句话说，只要实现了 Read(p []byte) ，那它就是一个读取器，使用标准库中已经实现的读写器，来举例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    reader := strings.NewReader(&amp;quot;Clear is better than clever&amp;quot;)
    p := make([]byte, 4)

    for {
        n, err := reader.Read(p)
        if err != nil{
            if err == io.EOF {
                fmt.Println(&amp;quot;EOF:&amp;quot;, n)
                break
            }
            fmt.Println(err)
            os.Exit(1)
        }
        fmt.Println(n, string(p[:n]))
    }
}

输出打印的内容：

4 Clea
4 r is
4  bet
4 ter 
4 than
4  cle
3 ver
EOF: 0 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;自定义reader&#34;&gt;自定义Reader&lt;/h3&gt;

&lt;p&gt;现在，让我们看看如何自己实现一个。它的功能是从流中过滤掉非字母字符。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type alphaReader struct {
    // 资源
    src string
    // 当前读取到的位置 
    cur int
}

// 创建一个实例
func newAlphaReader(src string) *alphaReader {
    return &amp;amp;alphaReader{src: src}
}

// 过滤函数
func alpha(r byte) byte {
    if (r &amp;gt;= &#39;A&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;Z&#39;) || (r &amp;gt;= &#39;a&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;z&#39;) {
        return r
    }
    return 0
}

// Read 方法，read函数是阻塞的
func (a *alphaReader) Read(p []byte) (int, error) {
    // 当前位置 &amp;gt;= 字符串长度 说明已经读取到结尾 返回 EOF
    if a.cur &amp;gt;= len(a.src) {
        return 0, io.EOF
    }

    // x 是剩余未读取的长度
    x := len(a.src) - a.cur
    n, bound := 0, 0
    if x &amp;gt;= len(p) {
        // 剩余长度超过缓冲区大小，说明本次可完全填满缓冲区
        bound = len(p)
    } else if x &amp;lt; len(p) {
        // 剩余长度小于缓冲区大小，使用剩余长度输出，缓冲区不补满
        bound = x
    }

    buf := make([]byte, bound)
    for n &amp;lt; bound {
        // 每次读取一个字节，执行过滤函数
        if char := alpha(a.src[a.cur]); char != 0 {
            buf[n] = char
        }
        n++
        a.cur++
    }
    // 将处理后得到的 buf 内容复制到 p 中
    copy(p, buf)
    return n, nil
}

func main() {
    reader := newAlphaReader(&amp;quot;Hello! It&#39;s 9am, where is the sun?&amp;quot;)
    p := make([]byte, 4)
    for {
        n, err := reader.Read(p)
        if err == io.EOF {
            break
        }
        fmt.Print(string(p[:n]))
    }
    fmt.Println()
}
输出打印的内容：
HelloItsamwhereisthesun
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;tcp粘包拆包&#34;&gt;TCP粘包拆包&lt;/h3&gt;

&lt;p&gt;这边讲解一下TCP粘包拆包问题，先看下面这个实例&lt;/p&gt;

&lt;p&gt;服务端代码 server/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    l, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:4044&amp;quot;)
    if err != nil {
        panic(err)
    }
    fmt.Println(&amp;quot;listen to 4044&amp;quot;)
    for {
        // 监听到新的连接，创建新的 goroutine 交给 handleConn函数 处理
        conn, err := l.Accept()
        if err != nil {
            fmt.Println(&amp;quot;conn err:&amp;quot;, err)
        } else {
            go handleConn(conn)
        }
    }
}

func handleConn(conn net.Conn) {
    defer conn.Close()
    defer fmt.Println(&amp;quot;关闭&amp;quot;)
    fmt.Println(&amp;quot;新连接：&amp;quot;, conn.RemoteAddr())

    result := bytes.NewBuffer(nil)
    var buf [1024]byte
    for {
        n, err := conn.Read(buf[0:])
        result.Write(buf[0:n])
        if err != nil {
            if err == io.EOF {
                continue
            } else {
                fmt.Println(&amp;quot;read err:&amp;quot;, err)
                break
            }
        } else {
            fmt.Println(&amp;quot;recv:&amp;quot;, result.String())
        }
        result.Reset()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端代码 client/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    data := []byte(&amp;quot;[这里才是一个完整的数据包]&amp;quot;)
    conn, err := net.DialTimeout(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:4044&amp;quot;, time.Second*30)
    if err != nil {
        fmt.Printf(&amp;quot;connect failed, err : %v\n&amp;quot;, err.Error())
        return
    }
    for i := 0; i &amp;lt;1000; i++ {
        _, err = conn.Write(data)
        if err != nil {
            fmt.Printf(&amp;quot;write failed , err : %v\n&amp;quot;, err)
            break
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;listen to 4044
新连接： [::1]:53079
recv: [这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据�
recv: �][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
...省略其它的...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从服务端的控制台输出可以看出，存在三种类型的输出：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一种是正常的一个数据包输出。&lt;/li&gt;
&lt;li&gt;一种是多个数据包“粘”在了一起，我们定义这种读到的包为粘包。&lt;/li&gt;
&lt;li&gt;一种是一个数据包被“拆”开，形成一个破碎的包，我们定义这种包为半包。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为什么会出现半包和粘包？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;客户端一段时间内发送包的速度太多，服务端没有全部处理完。于是数据就会积压起来，产生粘包。&lt;/li&gt;
&lt;li&gt;定义的读的buffer不够大，而数据包太大或者由于粘包产生，服务端不能一次全部读完，产生半包。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;什么时候需要考虑处理半包和粘包？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TCP连接是长连接，即一次连接多次发送数据。&lt;/li&gt;
&lt;li&gt;每次发送的数据是结构的，比如 JSON格式的数据 或者 数据包的协议是由我们自己定义的（包头部包含实际数据长度、协议魔数等）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解决思路&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;定长分隔(每个数据包最大为该长度，不足时使用特殊字符填充) ，但是数据不足时会浪费传输资源&lt;/li&gt;
&lt;li&gt;使用特定字符来分割数据包，但是若数据中含有分割字符则会出现Bug&lt;/li&gt;
&lt;li&gt;在数据包中添加长度字段，弥补了以上两种思路的不足，推荐使用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上述分析，我们最好通过第三种思路来解决拆包粘包问题。&lt;/p&gt;

&lt;p&gt;Golang的bufio库中有为我们提供了Scanner，来解决这类分割数据的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Scanner
Scanner provides a convenient interface for reading data such as a file of newline-delimited lines of text. Successive calls to the Scan method will step through the &#39;tokens&#39; of a file, skipping the bytes between the tokens. The specification of a token is defined by a split function of type SplitFunc; the default split function breaks the input into lines with line termination stripped. Split functions are defined in this package for scanning a file into lines, bytes, UTF-8-encoded runes, and space-delimited words. The client may instead provide a custom split function.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单来讲即是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Scanner为 读取数据 提供了方便的 接口。连续调用Scan方法会逐个得到文件的“tokens”，跳过 tokens 之间的字节。token 的规范由 SplitFunc 类型的函数定义。我们可以改为提供自定义拆分功能。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来看看 SplitFunc 类型的函数是什么样子的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SplitFunc func(data []byte, atEOF bool) (advance int, token []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    // An artificial input source.
    const input = &amp;quot;1234 5678 1234567901234567890&amp;quot;
    scanner := bufio.NewScanner(strings.NewReader(input))
    // Create a custom split function by wrapping the existing ScanWords function.
    split := func(data []byte, atEOF bool) (advance int, token []byte, err error) {
        advance, token, err = bufio.ScanWords(data, atEOF)
        if err == nil &amp;amp;&amp;amp; token != nil {
            _, err = strconv.ParseInt(string(token), 10, 32)
        }
        return
    }
    // Set the split function for the scanning operation.
    scanner.Split(split)
    // Validate the input
    for scanner.Scan() {
        fmt.Printf(&amp;quot;%s\n&amp;quot;, scanner.Text())
    }

    if err := scanner.Err(); err != nil {
        fmt.Printf(&amp;quot;Invalid input: %s&amp;quot;, err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;于是，我们可以这样改写我们的程序：&lt;/p&gt;

&lt;p&gt;服务端代码 server/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    l, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:4044&amp;quot;)
    if err != nil {
        panic(err)
    }
    fmt.Println(&amp;quot;listen to 4044&amp;quot;)
    for {
        conn, err := l.Accept()
        if err != nil {
            fmt.Println(&amp;quot;conn err:&amp;quot;, err)
        } else {
            go handleConn2(conn)
        }
    }
}

func packetSlitFunc(data []byte, atEOF bool) (advance int, token []byte, err error) {
        // 检查 atEOF 参数 和 数据包头部的四个字节是否 为 0x123456(我们定义的协议的魔数)
    if !atEOF &amp;amp;&amp;amp; len(data) &amp;gt; 6 &amp;amp;&amp;amp; binary.BigEndian.Uint32(data[:4]) == 0x123456 {
        var l int16
                // 读出 数据包中 实际数据 的长度(大小为 0 ~ 2^16)
        binary.Read(bytes.NewReader(data[4:6]), binary.BigEndian, &amp;amp;l)
        pl := int(l) + 6
        if pl &amp;lt;= len(data) {
            return pl, data[:pl], nil
        }
    }
    return
}

func handleConn2(conn net.Conn) {
    defer conn.Close()
    defer fmt.Println(&amp;quot;关闭&amp;quot;)
    fmt.Println(&amp;quot;新连接：&amp;quot;, conn.RemoteAddr())
    result := bytes.NewBuffer(nil)
        var buf [65542]byte // 由于 标识数据包长度 的只有两个字节 故数据包最大为 2^16+4(魔数)+2(长度标识)
    for {
        n, err := conn.Read(buf[0:])
        result.Write(buf[0:n])
        if err != nil {
            if err == io.EOF {
                continue
            } else {
                fmt.Println(&amp;quot;read err:&amp;quot;, err)
                break
            }
        } else {
            scanner := bufio.NewScanner(result)
            scanner.Split(packetSlitFunc)
            for scanner.Scan() {
                fmt.Println(&amp;quot;recv:&amp;quot;, string(scanner.Bytes()[6:]))
            }
        }
        result.Reset()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;复制代码客户端代码 client/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    data := []byte(&amp;quot;[这里才是一个完整的数据包]&amp;quot;)
    l := len(data)
    fmt.Println(l)
    magicNum := make([]byte, 4)
    binary.BigEndian.PutUint32(magicNum, 0x123456)
    lenNum := make([]byte, 2)
    binary.BigEndian.PutUint16(lenNum, uint16(l))
    packetBuf := bytes.NewBuffer(magicNum)
    packetBuf.Write(lenNum)
    packetBuf.Write(data)
    conn, err := net.DialTimeout(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:4044&amp;quot;, time.Second*30)
    if err != nil {
        fmt.Printf(&amp;quot;connect failed, err : %v\n&amp;quot;, err.Error())
                return
    }
    for i := 0; i &amp;lt;1000; i++ {
        _, err = conn.Write(packetBuf.Bytes())
        if err != nil {
            fmt.Printf(&amp;quot;write failed , err : %v\n&amp;quot;, err)
            break
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;复制代码运行结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;listen to 4044
新连接： [::1]:55738
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
...省略其它的...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;编写器&#34;&gt;编写器&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;type Writer
type Writer interface {
    //Write() 方法有两个返回值，一个是写入到目标资源的字节数，一个是发生错误时的错误。
    Write(p []byte) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;io.Writer 表示一个编写器，它从缓冲区读取数据，并将数据写入目标资源。&lt;/p&gt;

&lt;p&gt;实现这个接口就需要实现如下的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Write 将 len(p) 个字节从 p 中写入到基本数据流中。它返回从 p 中被写入的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)）以及任何遇到的引起写入提前停止的错误。若 Write 返回的 n &amp;lt; len(p)，它就必须返回一个 非nil 的错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于要用作编写器的类型，必须实现 io.Writer 接口的唯一一个方法 Write(p []byte),同样，只要实现了 Write(p []byte) ，那它就是一个编写器。举例，标准库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    proverbs := []string{
        &amp;quot;Channels orchestrate mutexes serialize&amp;quot;,
        &amp;quot;Cgo is not Go&amp;quot;,
        &amp;quot;Errors are values&amp;quot;,
        &amp;quot;Don&#39;t panic&amp;quot;,
    }
    var writer bytes.Buffer

    for _, p := range proverbs {
        n, err := writer.Write([]byte(p))
        if err != nil {
            fmt.Println(err)
            os.Exit(1)
        }
        if n != len(p) {
            fmt.Println(&amp;quot;failed to write data&amp;quot;)
            os.Exit(1)
        }
    }

    fmt.Println(writer.String())
}
输出打印的内容：
Channels orchestrate mutexes serializeCgo is not GoErrors are valuesDon&#39;t panic
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;自定义writer&#34;&gt;自定义Writer&lt;/h3&gt;

&lt;p&gt;下面我们来实现一个名为 chanWriter 的自定义 io.Writer ，它将其内容作为字节序列写入 channel 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type chanWriter struct {
    // ch 实际上就是目标资源
    ch chan byte
}

func newChanWriter() *chanWriter {
    return &amp;amp;chanWriter{make(chan byte, 1024)}
}

func (w *chanWriter) Chan() &amp;lt;-chan byte {
    return w.ch
}

func (w *chanWriter) Write(p []byte) (int, error) {
    n := 0
    // 遍历输入数据，按字节写入目标资源
    for _, b := range p {
        w.ch &amp;lt;- b
        n++
    }
    return n, nil
}

func (w *chanWriter) Close() error {
    close(w.ch)
    return nil
}

func main() {
    writer := newChanWriter()
    go func() {
        defer writer.Close()
        writer.Write([]byte(&amp;quot;Stream &amp;quot;))
        writer.Write([]byte(&amp;quot;me!&amp;quot;))
    }()
    for c := range writer.Chan() {
        fmt.Printf(&amp;quot;%c&amp;quot;, c)
    }
    fmt.Println()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要使用这个 Writer，只需在函数 main() 中调用 writer.Write()（在单独的goroutine中）。&lt;/p&gt;

&lt;p&gt;因为 chanWriter 还实现了接口 io.Closer ，所以调用方法 writer.Close() 来正确地关闭channel，以避免发生泄漏和死锁。&lt;/p&gt;

&lt;h2 id=&#34;closer&#34;&gt;closer&lt;/h2&gt;

&lt;p&gt;Closer 接口包装了基本的 Close 方法，用于关闭数据读写。Close 一般用于关闭文件，关闭通道，关闭连接，关闭数据库等，在不同的标准库实现中实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Closer interface {
    Close() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;seeker&#34;&gt;seeker&lt;/h2&gt;

&lt;p&gt;Seeker 接口包装了基本的 Seek 方法，用于移动数据的读写指针。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Seeker interface {
    Seek(offset int64, whence int) (ret int64, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seek 设置下一次读写操作的指针位置，每次的读写操作都是从指针位置开始的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;whence 的含义：

&lt;ul&gt;
&lt;li&gt;如果 whence 为 0：表示从数据的开头开始移动指针。&lt;/li&gt;
&lt;li&gt;如果 whence 为 1：表示从数据的当前指针位置开始移动指针。&lt;/li&gt;
&lt;li&gt;如果 whence 为 2：表示从数据的尾部开始移动指针。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;offset 是指针移动的偏移量。&lt;/li&gt;
&lt;li&gt;返回新指针位置和遇到的错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;whence 的值，在 io 包中定义了相应的常量，应该使用这些常量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
  SeekStart   = 0 // seek relative to the origin of the file
  SeekCurrent = 1 // seek relative to the current offset
  SeekEnd     = 2 // seek relative to the end
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而原先 os 包中的常量已经被标注为Deprecated&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Deprecated: Use io.SeekStart, io.SeekCurrent, and io.SeekEnd.
const (
  SEEK_SET int = 0 // seek relative to the origin of the file
  SEEK_CUR int = 1 // seek relative to the current offset
  SEEK_END int = 2 // seek relative to the end
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;组合接口&#34;&gt;组合接口&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;type ReadWriter interface {
    Reader
    Writer
}

type ReadSeeker interface {
    Reader
    Seeker
}

type WriteSeeker interface {
    Writer
    Seeker
}

type ReadWriteSeeker interface {
    Reader
    Writer
    Seeker
}

type ReadCloser interface {
    Reader
    Closer
}

type WriteCloser interface {
    Writer
    Closer
}

type ReadWriteCloser interface {
    Reader
    Writer
    Closer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些接口的作用是：有些时候同时需要某两个接口的所有功能，即必须同时实现了某两个接口的类型才能够被传入使用。可见，io 包中有大量的“小接口”，这样方便组合为“大接口”。&lt;/p&gt;

&lt;h2 id=&#34;其他接口&#34;&gt;其他接口&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;ReaderFrom&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReaderFrom 接口包装了基本的 ReadFrom 方法，用于从 r 中读取数据存入自身。直到遇到 EOF 或读取出错为止，返回读取的字节数和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ReaderFrom interface {
    ReadFrom(r Reader) (n int64, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ReadFrom 从 r 中读取数据，直到 EOF 或发生错误。其返回值 n 为读取的字节数。除 io.EOF 之外，在读取过程中遇到的任何错误也将被返回。&lt;/li&gt;
&lt;li&gt;如果 ReaderFrom 可用，Copy 函数就会使用它。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例：将文件中的数据全部读取（显示在标准输出）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file, err := os.Open(&amp;quot;writeAt.txt&amp;quot;)
if err != nil {
    panic(err)
}
defer file.Close()
writer := bufio.NewWriter(os.Stdout)
writer.ReadFrom(file)
writer.Flush()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，我们可以通过 ioutil 包的 ReadFile 函数获取文件全部内容。其实，跟踪一下 ioutil.ReadFile 的源码，会发现其实也是通过 ReadFrom 方法实现（用的是 bytes.Buffer，它实现了 ReaderFrom 接口）。&lt;/p&gt;

&lt;p&gt;如果不通过 ReadFrom 接口来做这件事，而是使用 io.Reader 接口，我们有两种思路：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;先获取文件的大小（File 的 Stat 方法），之后定义一个该大小的 []byte，通过 Read 一次性读取&lt;/li&gt;
&lt;li&gt;定义一个小的 []byte，不断的调用 Read 方法直到遇到 EOF，将所有读取到的 []byte 连接到一起&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;WriterTo&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;WriterTo 接口包装了基本的 WriteTo 方法，用于将自身的数据写入 w 中。直到数据全部写入完毕或遇到错误为止，返回写入的字节数和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type WriterTo interface {
    WriteTo(w Writer) (n int64, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;WriteTo 将数据写入 w 中，直到没有数据可写或发生错误。其返回值 n 为写入的字节数。 在写入过程中遇到的任何错误也将被返回。&lt;/li&gt;
&lt;li&gt;如果 WriterTo 可用，Copy 函数就会使用它。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;读者是否发现，其实 ReaderFrom 和 WriterTo 接口的方法接收的参数是 io.Reader 和 io.Writer 类型。根据 io.Reader 和 io.Writer 接口的讲解，对该接口的使用应该可以很好的掌握。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ReaderAt&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReaderAt 接口包装了基本的 ReadAt 方法，用于将自身的数据写入 p 中。ReadAt 忽略之前的读写位置，从起始位置的 off 偏移处开始读取。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ReaderAt interface {
    ReadAt(p []byte, off int64) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ReadAt 从基本输入源的偏移量 off 处开始，将 len(p) 个字节读取到 p 中。它返回读取的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)）以及任何遇到的错误。&lt;/li&gt;
&lt;li&gt;当 ReadAt 返回的 n &amp;lt; len(p) 时，它就会返回一个 非nil 的错误来解释 为什么没有返回更多的字节。在这一点上，ReadAt 比 Read 更严格。&lt;/li&gt;
&lt;li&gt;即使 ReadAt 返回的 n &amp;lt; len(p)，它也会在调用过程中使用 p 的全部作为暂存空间。若可读取的数据不到 len(p) 字节，ReadAt 就会阻塞,直到所有数据都可用或一个错误发生。 在这一点上 ReadAt 不同于 Read。&lt;/li&gt;
&lt;li&gt;若 n = len(p) 个字节从输入源的结尾处由 ReadAt 返回，Read可能返回 err == EOF 或者 err == nil&lt;/li&gt;
&lt;li&gt;若 ReadAt 携带一个偏移量从输入源读取，ReadAt 应当既不影响偏移量也不被它所影响。&lt;/li&gt;
&lt;li&gt;可对相同的输入源并行执行 ReadAt 调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;标准库上面说的很多都是实现了这个接口，简单示例代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := strings.NewReader(&amp;quot;Go语言中文网&amp;quot;)
p := make([]byte, 6)
n, err := reader.ReadAt(p, 2)
if err != nil {
    panic(err)
}
fmt.Printf(&amp;quot;%s, %d\n&amp;quot;, p, n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;语言, 6
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;WriterAt&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;WriterAt 接口包装了基本的 WriteAt 方法，用于将 p 中的数据写入自身。ReadAt 忽略之前的读写位置，从起始位置的 off 偏移处开始写入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type WriterAt interface {
    WriteAt(p []byte, off int64) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;WriteAt 从 p 中将 len(p) 个字节写入到偏移量 off 处的基本数据流中。它返回从 p 中被写入的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)）以及任何遇到的引起写入提前停止的错误。若 WriteAt 返回的 n &amp;lt; len(p)，它就必须返回一个 非nil 的错误。&lt;/li&gt;
&lt;li&gt;若 WriteAt 携带一个偏移量写入到目标中，WriteAt 应当既不影响偏移量也不被它所影响。&lt;/li&gt;
&lt;li&gt;若被写区域没有重叠，可对相同的目标并行执行 WriteAt 调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;os.File 实现了 WriterAt 接口，实例如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file, err := os.Create(&amp;quot;writeAt.txt&amp;quot;)
if err != nil {
    panic(err)
}
defer file.Close()
file.WriteString(&amp;quot;Golang中文社区——这里是多余&amp;quot;)
n, err := file.WriteAt([]byte(&amp;quot;Go语言中文网&amp;quot;), 24)
if err != nil {
    panic(err)
}
fmt.Println(n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Golang中文社区——Go语言中文网。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分析：file.WriteString(&amp;ldquo;Golang中文社区——这里是多余&amp;rdquo;) 往文件中写入 Golang中文社区——这里是多余，之后 file.WriteAt([]byte(&amp;ldquo;Go语言中文网&amp;rdquo;), 24) 在文件流的 offset=24 处写入 Go语言中文网（会覆盖该位置的内容）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ByteReader和ByteWriter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ByteReader 接口包装了基本的 ReadByte 方法，用于从自身读出一个字节。返回读出的字节和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ByteReader interface {
    ReadByte() (c byte, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ByteWriter 接口包装了基本的 WriteByte 方法，用于将一个字节写入自身返回遇到的错误&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ByteWriter interface {
    WriteByte(c byte) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这组接口在标准库中也有实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bufio.Reader/Writer 分别实现了io.ByteReader 和 io.ByteWriter
bytes.Buffer 同时实现了 io.ByteReader 和 io.ByteWriter
bytes.Reader 实现了 io.ByteReader
strings.Reader 实现了 io.ByteReader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ch byte
fmt.Scanf(&amp;quot;%c\n&amp;quot;, &amp;amp;ch)

buffer := new(bytes.Buffer)
err := buffer.WriteByte(ch)
if err == nil {
    fmt.Println(&amp;quot;写入一个字节成功！准备读取该字节……&amp;quot;)
    newCh, _ := buffer.ReadByte()
    fmt.Printf(&amp;quot;读取的字节：%c\n&amp;quot;, newCh)
} else {
    fmt.Println(&amp;quot;写入错误&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;ByteScanner&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ByteScanner 在 ByteReader 的基础上增加了一个 UnreadByte 方法，用于撤消最后一次的 ReadByte 操作，以便下次的 ReadByte 操作可以读出与前一次一样的数据。UnreadByte 之前必须是 ReadByte 才能撤消成功，否则可能会返回一个错误信息（根据不同的需求，UnreadByte 也可能返回 nil，允许随意调用 UnreadByte，但只有最后一次的 ReadByte 可以被撤销，其它 UnreadByte 不执行任何操作）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ByteScanner interface {
    ByteReader
    UnreadByte() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;RuneReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;RuneReader 接口包装了基本的 ReadRune 方法，用于从自身读取一个 UTF-8 编码的字符到 r 中。返回读取的字符、字符的编码长度和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type RuneReader interface {
    ReadRune() (r rune, size int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;RuneScanner&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;RuneScanner 在 RuneReader 的基础上增加了一个 UnreadRune 方法，用于撤消最后一次的 ReadRune 操作，以便下次的 ReadRune 操作可以读出与前一次一样的数据。UnreadRune 之前必须是 ReadRune 才能撤消成功，否则可能会返回一个错误信息（根据不同的需求，UnreadRune 也可能返回 nil，允许随意调用 UnreadRune，但只有最后一次的 ReadRune 可以被撤销，其它 UnreadRune 不执行任何操作）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type RuneScanner interface {
    RuneReader
    UnreadRune() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;bytes.NewBuffer 实现了很多基本的接口，可以通过 bytes 包学习接口的实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    buf := bytes.NewBuffer([]byte(&amp;quot;Hello World!&amp;quot;))
    b := make([]byte, buf.Len())

    n, err := buf.Read(b)
    fmt.Printf(&amp;quot;%s   %v\n&amp;quot;, b[:n], err)
    // Hello World!   &amp;lt;nil&amp;gt;

    buf.WriteString(&amp;quot;ABCDEFG\n&amp;quot;)
    buf.WriteTo(os.Stdout)
    // ABCDEFG

    n, err = buf.Write(b)
    fmt.Printf(&amp;quot;%d   %s   %v\n&amp;quot;, n, buf.String(), err)
    // 12   Hello World!   &amp;lt;nil&amp;gt;

    c, err := buf.ReadByte()
    fmt.Printf(&amp;quot;%c   %s   %v\n&amp;quot;, c, buf.String(), err)
    // H   ello World!   &amp;lt;nil&amp;gt;

    c, err = buf.ReadByte()
    fmt.Printf(&amp;quot;%c   %s   %v\n&amp;quot;, c, buf.String(), err)
    // e   llo World!   &amp;lt;nil&amp;gt;

    err = buf.UnreadByte()
    fmt.Printf(&amp;quot;%s   %v\n&amp;quot;, buf.String(), err)
    // ello World!   &amp;lt;nil&amp;gt;

    err = buf.UnreadByte()
    fmt.Printf(&amp;quot;%s   %v\n&amp;quot;, buf.String(), err)
    // ello World!   bytes.Buffer: UnreadByte: previous operation was not a read
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;

&lt;p&gt;io包中定义了很多原生的类型。都是实现了上面的接口，可以直接创建使用的类型。&lt;/p&gt;

&lt;h2 id=&#34;sectionreader-类型&#34;&gt;SectionReader 类型&lt;/h2&gt;

&lt;p&gt;SectionReader 是一个 struct（没有任何导出的字段），实现了 Read, Seek 和 ReadAt，同时，内嵌了 ReaderAt 接口。结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SectionReader struct {
    r     ReaderAt    // 该类型最终的 Read/ReadAt 最终都是通过 r 的 ReadAt 实现
    base  int64        // NewSectionReader 会将 base 设置为 off
    off   int64        // 从 r 中的 off 偏移处开始读取数据
    limit int64        // limit - off = SectionReader 流的长度
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从名称我们可以猜到，该类型读取数据流中部分数据。看一下常见的创建函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewSectionReader(r ReaderAt, off int64, n int64) *SectionReader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewSectionReader 返回一个 SectionReader，它从 r 中的偏移量 off 处读取 n 个字节后以 EOF 停止。也就是说，SectionReader 只是内部（内嵌）ReaderAt 表示的数据流的一部分：从 off 开始后的 n 个字节。这个类型的作用是：方便重复操作某一段 (section) 数据流；或者同时需要 ReadAt 和 Seek 的功能。&lt;/p&gt;

&lt;h2 id=&#34;limitedreader-类型&#34;&gt;LimitedReader 类型&lt;/h2&gt;

&lt;p&gt;LimitedReader 结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type LimitedReader struct {
    R Reader // underlying reader，最终的读取操作通过 R.Read 完成
    N int64  // max bytes remaining
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从 R 读取但将返回的数据量限制为 N 字节。每调用一次 Read 都将更新 N 来反应新的剩余数量。也就是说，最多只能返回 N 字节数据。LimitedReader 只实现了 Read 方法（Reader 接口）。&lt;/p&gt;

&lt;p&gt;使用示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;content := &amp;quot;This Is LimitReader Example&amp;quot;
reader := strings.NewReader(content)
limitReader := &amp;amp;io.LimitedReader{R: reader, N: 8}
for limitReader.N &amp;gt; 0 {
    tmp := make([]byte, 2)
    limitReader.Read(tmp)
    fmt.Printf(&amp;quot;%s&amp;quot;, tmp)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This Is
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，通过该类型可以达到 只允许读取一定长度数据 的目的。&lt;/p&gt;

&lt;p&gt;在 io 包中，LimitReader 函数的实现其实就是调用 LimitedReader：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LimitReader(r Reader, n int64) Reader { return &amp;amp;LimitedReader{r, n} }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pipereader-和-pipewriter-类型&#34;&gt;PipeReader 和 PipeWriter 类型&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;PipeReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PipeReader（一个没有任何导出字段的 struct）是管道的读取端。它实现了 io.Reader 和 io.Closer 接口。结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PipeReader struct {
    p *pipe
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 PipeReader.Read 方法的说明：从管道中读取数据。该方法会堵塞，直到管道写入端开始写入数据或写入端被关闭。如果写入端关闭时带有 error（即调用 CloseWithError 关闭），该Read返回的 err 就是写入端传递的error；否则 err 为 EOF。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;PipeWriter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PipeWriter（一个没有任何导出字段的 struct）是管道的写入端。它实现了 io.Writer 和 io.Closer 接口。结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PipeWriter struct {
    p *pipe
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 PipeWriter.Write 方法的说明：写数据到管道中。该方法会堵塞，直到管道读取端读完所有数据或读取端被关闭。如果读取端关闭时带有 error（即调用 CloseWithError 关闭），该Write返回的 err 就是读取端传递的error；否则 err 为 ErrClosedPipe。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Pipe&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;io.Pipe() 用于创建一个同步的内存管道 (synchronous in-memory pipe)，函数签名：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Pipe() (*PipeReader, *PipeWriter)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它将 io.Reader 连接到 io.Writer。一端的读取匹配另一端的写入，直接在这两端之间复制数据；它没有内部缓存。它对于并行调用 Read 和 Write 以及其它函数或 Close 来说都是安全的。一旦等待的 I/O 结束，Close 就会完成。并行调用 Read 或并行调用 Write 也同样安全：同种类的调用将按顺序进行控制。&lt;/p&gt;

&lt;p&gt;正因为是同步的，因此不能在一个 goroutine 中进行读和写。&lt;/p&gt;

&lt;p&gt;读关闭管道&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *PipeReader) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读关闭管道并传入错误信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *PipeReader) CloseWithError(err error) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从管道中读取数据，如果管道被关闭，则会返会一个错误信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、如果写入端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。&lt;/li&gt;
&lt;li&gt;2、如果写入端通过 Close 方法关闭了管道，则返回 io.EOF。&lt;/li&gt;
&lt;li&gt;3、如果是读取端关闭了管道，则返回 io.ErrClosedPipe。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例：管道（读取端关闭）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r, w := io.Pipe()
    // 启用一个例程进行读取
    go func() {
        buf := make([]byte, 5)
        for n, err := 0, error(nil); err == nil; {
            n, err = r.Read(buf)
            r.CloseWithError(errors.New(&amp;quot;管道被读取端关闭&amp;quot;))
            fmt.Printf(&amp;quot;读取：%d, %v, %s\n&amp;quot;, n, err, buf[:n])
        }
    }()
    // 主例程进行写入
    n, err := w.Write([]byte(&amp;quot;Hello World !&amp;quot;))
    fmt.Printf(&amp;quot;写入：%d, %v\n&amp;quot;, n, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写关闭管道&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (w *PipeWriter) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写关闭管道并传入错误信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (w *PipeWriter) CloseWithError(err error) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;向管道中写入数据，如果管道被关闭，则会返会一个错误信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、如果读取端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。&lt;/li&gt;
&lt;li&gt;2、如果读取端通过 Close 方法关闭了管道，则返回 io.ErrClosedPipe。&lt;/li&gt;
&lt;li&gt;3、如果是写入端关闭了管道，则返回 io.ErrClosedPipe。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例：管道（写入端关闭）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r, w := io.Pipe()
    // 启用一个例程进行读取
    go func() {
        buf := make([]byte, 5)
        for n, err := 0, error(nil); err == nil; {
            n, err = r.Read(buf)
            fmt.Printf(&amp;quot;读取：%d, %v, %s\n&amp;quot;, n, err, buf[:n])
        }
    }()
    // 主例程进行写入
    n, err := w.Write([]byte(&amp;quot;Hello World !&amp;quot;))
    fmt.Printf(&amp;quot;写入：%d, %v\n&amp;quot;, n, err)

    w.CloseWithError(errors.New(&amp;quot;管道被写入端关闭&amp;quot;))
    n, err = w.Write([]byte(&amp;quot;Hello World !&amp;quot;))
    fmt.Printf(&amp;quot;写入：%d, %v\n&amp;quot;, n, err)
    time.Sleep(time.Second * 1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;综合使用实例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    pipeReader, pipeWriter := io.Pipe()
    go PipeWrite(pipeWriter)
    go PipeRead(pipeReader)
    time.Sleep(30 * time.Second)
}

func PipeWrite(writer *io.PipeWriter){
    data := []byte(&amp;quot;Go语言中文网&amp;quot;)
    for i := 0; i &amp;lt; 3; i++{
        n, err := writer.Write(data)
        if err != nil{
            fmt.Println(err)
            return
        }
        fmt.Printf(&amp;quot;写入字节 %d\n&amp;quot;,n)
    }
    writer.CloseWithError(errors.New(&amp;quot;写入段已关闭&amp;quot;))
}

func PipeRead(reader *io.PipeReader){
    buf := make([]byte, 128)
    for{
        fmt.Println(&amp;quot;接口端开始阻塞5秒钟...&amp;quot;)
        time.Sleep(5 * time.Second)
        fmt.Println(&amp;quot;接收端开始接受&amp;quot;)
        n, err := reader.Read(buf)
        if err != nil{
            fmt.Println(err)
            return
        }
        fmt.Printf(&amp;quot;收到字节: %d\n buf内容: %s\n&amp;quot;,n,buf)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;函数&#34;&gt;函数&lt;/h1&gt;

&lt;p&gt;io包中也有一下原生实现可以使用的函数。其实都是直接操作结构体的函数。&lt;/p&gt;

&lt;h2 id=&#34;writestring&#34;&gt;WriteString&lt;/h2&gt;

&lt;p&gt;WriteString 将字符串 s 写入到 w 中，返回写入的字节数和遇到的错误。如果 w 实现了 WriteString 方法，则优先使用该方法将 s 写入 w 中。否则，将 s 转换为 []byte，然后调用 w.Write 方法将数据写入 w 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func WriteString(w Writer, s string) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;readatleast&#34;&gt;ReadAtLeast&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadAtLeast&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReadAtLeast 从 r 中读取数据到 buf 中，要求至少读取 min 个字节。返回读取的字节数和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ReadAtLeast(r Reader, buf []byte, min int) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 min 超出了 buf 的容量，则 err 返回 io.ErrShortBuffer，否则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、读出的数据长度 == 0  ，则 err 返回 EOF。&lt;/li&gt;
&lt;li&gt;2、读出的数据长度 &amp;lt;  min，则 err 返回 io.ErrUnexpectedEOF。&lt;/li&gt;
&lt;li&gt;3、读出的数据长度 &amp;gt;= min，则 err 返回 nil。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadFull&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReadFull 的功能和 ReadAtLeast 一样，只不过 min = len(buf)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ReadFull(r Reader, buf []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：WriteString、ReadAtLeast、ReadFull&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    io.WriteString(os.Stdout, &amp;quot;Hello World!\n&amp;quot;)
    // Hello World!

    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    b := make([]byte, 15)

    n, err := io.ReadAtLeast(r, b, 20)
    fmt.Printf(&amp;quot;%q   %d   %v\n&amp;quot;, b[:n], n, err)
    // &amp;quot;&amp;quot;   0   short buffer

    r.Seek(0, 0)
    b = make([]byte, 15)

    n, err = io.ReadFull(r, b)
    fmt.Printf(&amp;quot;%q   %d   %v\n&amp;quot;, b[:n], n, err)
    // &amp;quot;Hello World!&amp;quot;   12   unexpected EOF
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;LimitReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;LimitReader 对 r 进行封装，使其最多只能读取 n 个字节的数据。相当于对 r 做了一个切片 r[:n] 返回。底层实现是一个 *LimitedReader（只有一个 Read 方法）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LimitReader(r Reader, n int64) Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;MultiReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;MultiReader 将多个 Reader 封装成一个单独的 Reader，多个 Reader 会按顺序读取，当多个 Reader 都返回 EOF 之后，单独的 Reader 才返回 EOF，否则返回读取过程中遇到的任何错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MultiReader(readers ...Reader) Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;MultiWriter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;MultiWriter 将向自身写入的数据同步写入到所有 writers 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MultiWriter(writers ...Writer) Writer
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;TeeReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;TeeReader 对 r 进行封装，使 r 在读取数据的同时，自动向 w 中写入数据。它是一个无缓冲的 Reader，所以对 w 的写入操作必须在 r 的 Read 操作结束之前完成。所有写入时遇到的错误都会被作为 Read 方法的 err 返回。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func TeeReader(r Reader, w Writer) Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 LimitReader&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    lr := io.LimitReader(r, 5)

    n, err := io.Copy(os.Stdout, lr)  // Hello
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err) // 5   &amp;lt;nil&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 MultiReader&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r1 := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    r2 := strings.NewReader(&amp;quot;ABCDEFG&amp;quot;)
    r3 := strings.NewReader(&amp;quot;abcdefg&amp;quot;)
    b := make([]byte, 15)
    mr := io.MultiReader(r1, r2, r3)

    for n, err := 0, error(nil); err == nil; {
        n, err = mr.Read(b)
        fmt.Printf(&amp;quot;%q\n&amp;quot;, b[:n])
    }
    // &amp;quot;Hello World!&amp;quot;
    // &amp;quot;ABCDEFG&amp;quot;
    // &amp;quot;abcdefg&amp;quot;
    // &amp;quot;&amp;quot;

    r1.Seek(0, 0)
    r2.Seek(0, 0)
    r3.Seek(0, 0)
    mr = io.MultiReader(r1, r2, r3)
    io.Copy(os.Stdout, mr)
    // Hello World!ABCDEFGabcdefg
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 MultiWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!\n&amp;quot;)
    mw := io.MultiWriter(os.Stdout, os.Stdout, os.Stdout)

    r.WriteTo(mw)
    // Hello World!
    // Hello World!
    // Hello World!
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 TeeReader&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    b := make([]byte, 15)
    tr := io.TeeReader(r, os.Stdout)

    n, err := tr.Read(b)                  // Hello World!
    fmt.Printf(&amp;quot;\n%s   %v\n&amp;quot;, b[:n], err) // Hello World!   &amp;lt;nil&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;copy&#34;&gt;Copy&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;CopyN&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CopyN 从 src 中复制 n 个字节的数据到 dst 中，返回复制的字节数和遇到的错误。只有当 written = n 时，err 才返回 nil。如果 dst 实现了 ReadFrom 方法，则优先调用该方法执行复制操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func CopyN(dst Writer, src Reader, n int64) (written int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Copy&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Copy 从 src 中复制数据到 dst 中，直到所有数据都复制完毕，返回复制的字节数和遇到的错误。如果复制过程成功结束，则 err 返回 nil，而不是 EOF，因为 Copy 的定义为“直到所有数据都复制完毕”，所以不会将 EOF 视为错误返回。如果 src 实现了 WriteTo 方法，则调用 src.WriteTo(dst) 复制数据，否则如果 dst 实现了 ReadeFrom 方法，则调用 dst.ReadeFrom(src) 复制数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Copy(dst Writer, src Reader) (written int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;CopyBuffer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CopyBuffer 相当于 Copy，只不 Copy 在执行的过程中会创建一个临时的缓冲区来中转数据，而 CopyBuffer 则可以单独提供一个缓冲区，让多个复制操作共用同一个缓冲区，避免每次复制操作都创建新的缓冲区。如果 buf == nil，则 CopyBuffer 会自动创建缓冲区。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func CopyBuffer(dst Writer, src Reader, buf []byte) (written int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：CopyN、Copy、CopyBuffer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    buf := make([]byte, 32)

    n, err := io.CopyN(os.Stdout, r, 5) // Hello
    fmt.Printf(&amp;quot;\n%d   %v\n\n&amp;quot;, n, err) // 5   &amp;lt;nil&amp;gt;

    r.Seek(0, 0)
    n, err = io.Copy(os.Stdout, r)      // Hello World!
    fmt.Printf(&amp;quot;\n%d   %v\n\n&amp;quot;, n, err) // 12   &amp;lt;nil&amp;gt;

    r.Seek(0, 0)
    r2 := strings.NewReader(&amp;quot;ABCDEFG&amp;quot;)
    r3 := strings.NewReader(&amp;quot;abcdefg&amp;quot;)

    n, err = io.CopyBuffer(os.Stdout, r, buf) // Hello World!
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err)         // 12   &amp;lt;nil&amp;gt;

    n, err = io.CopyBuffer(os.Stdout, r2, buf) // ABCDEFG
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err)          // 7   &amp;lt;nil&amp;gt;

    n, err = io.CopyBuffer(os.Stdout, r3, buf) // abcdefg
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err)          // 7   &amp;lt;nil&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数还是我们在网络消息流量转发的时候还是经常使用的。&lt;/p&gt;

&lt;h1 id=&#34;场景举例&#34;&gt;场景举例&lt;/h1&gt;

&lt;h2 id=&#34;base64编码成字符串&#34;&gt;base64编码成字符串&lt;/h2&gt;

&lt;p&gt;encoding/base64包中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewEncoder(enc *Encoding, w io.Writer) io.WriteCloser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个用来做base64编码，但是仔细观察发现，它需要一个io.Writer作为输出目标，并用返回的WriteCloser的Write方法将结果写入目标，下面是Go官方文档的例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input := []byte(&amp;quot;foo\x00bar&amp;quot;)
encoder := base64.NewEncoder(base64.StdEncoding, os.Stdout)
encoder.Write(input)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个例子是将结果写入到Stdout，如果我们希望得到一个字符串呢？可以用bytes.Buffer作为目标io.Writer：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input := []byte(&amp;quot;foo\x00bar&amp;quot;)
buffer := new(bytes.Buffer)
encoder := base64.NewEncoder(base64.StdEncoding, buffer)
encoder.Write(input)
fmt.Println(string(buffer.Bytes())
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;byte和struct之间正反序列化&#34;&gt;[]byte和struct之间正反序列化&lt;/h2&gt;

&lt;p&gt;这种场景经常用在基于字节的协议上，比如有一个具有固定长度的结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Protocol struct {
    Version     uint8
    BodyLen     uint16
    Reserved    [2]byte
    Unit        uint8
    Value       uint32
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过一个[]byte来反序列化得到这个Protocol，一种思路是遍历这个[]byte，然后逐一赋值。其实在encoding/binary包中有个方便的方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Read(r io.Reader, order ByteOrder, data interface{}) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个方法从一个io.Reader中读取字节，并已order指定的端模式，来给填充data（data需要是fixed-sized的结构或者类型）。要用到这个方法首先要有一个io.Reader，从上面的图中不难发现，我们可以这么写：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var p Protocol
var bin []byte
//...
binary.Read(bytes.NewReader(bin), binary.LittleEndian, &amp;amp;p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;换句话说，我们将一个[]byte转成了一个io.Reader。&lt;/p&gt;

&lt;p&gt;反过来，我们需要将Protocol序列化得到[]byte，使用encoding/binary包中有个对应的Write方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Write(w io.Writer, order ByteOrder, data interface{}) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过将[]byte转成一个io.Writer即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var p Protocol
buffer := new(bytes.Buffer)
//...
binary.Writer(buffer, binary.LittleEndian, p)
bin := buffer.Bytes()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;从流中按行读取&#34;&gt;从流中按行读取&lt;/h2&gt;

&lt;p&gt;比如对于常见的基于文本行的HTTP协议的读取，我们需要将一个流按照行来读取。本质上，我们需要一个基于缓冲的读写机制（读一些到缓冲，然后遍历缓冲中我们关心的字节或字符）。在Go中有一个bufio的包可以实现带缓冲的读写：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(rd io.Reader) *Reader
func (b *Reader) ReadString(delim byte) (string, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个ReadString方法从io.Reader中读取字符串，直到delim，就返回delim和之前的字符串。如果将delim设置为\n，相当于按行来读取了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var conn net.Conn
//...
reader := NewReader(conn)
for {
    line, err := reader.ReadString([]byte(&#39;\n&#39;))
    //...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;string-to-byte&#34;&gt;string to byte&lt;/h2&gt;

&lt;p&gt;花式技（zuo）巧（si）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;string转[]byte
a := &amp;quot;Hello, playground&amp;quot;
fmt.Println([]byte(a))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等价于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := &amp;quot;Hello, playground&amp;quot;
buf := new(bytes.Buffer)
buf.ReadFrom(strings.NewReader(a))
fmt.Println(buf.Bytes())
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;标准库中实现的读取器和编写器的实例&#34;&gt;标准库中实现的读取器和编写器的实例&lt;/h1&gt;

&lt;p&gt;目前，Go 文档中还没有直接列出实现了某个接口的所有类型。不过，我们可以通过查看标准库文档，列出实现了 io.Reader 或 io.Writer 接口的类型（导出的类型）：（注：godoc 命令支持额外参数 -analysis ，能列出都有哪些类型实现了某个接口，相关参考 godoc -h 或 Static analysis features of godoc。另外，还有一个地址：&lt;a href=&#34;http://docs.studygolang.com。&#34;&gt;http://docs.studygolang.com。&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/&#34;&gt;os.File&lt;/a&gt; 同时实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/#reader-类型&#34;&gt;strings.Reader&lt;/a&gt; 实现了 io.Reader&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bufio/&#34;&gt;bufio.Reader/Writer&lt;/a&gt; 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/&#34;&gt;bytes.Buffer&lt;/a&gt; 同时实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/&#34;&gt;bytes.Reader&lt;/a&gt; 实现了 io.Reader&lt;/li&gt;
&lt;li&gt;compress/gzip.Reader/Writer 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;crypto/cipher.StreamReader/StreamWriter 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;crypto/tls.Conn 同时实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;encoding/csv.Reader/Writer 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;mime/multipart.Part 实现了 io.Reader&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net/&#34;&gt;net.Conn&lt;/a&gt; 分别实现了 io.Reader 和 io.Writer(Conn接口定义了Read/Write)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此之外，io 包本身也有这两个接口的实现类型。如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实现了 Reader 的类型：&lt;a href=&#34;#limitedreader-类型&#34;&gt;LimitedReader&lt;/a&gt;、&lt;a href=&#34;#pipereader-和-pipewriter-类型&#34;&gt;PipeReader&lt;/a&gt;、&lt;a href=&#34;#sectionreader-类型&#34;&gt;SectionReader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实现了 Writer 的类型：&lt;a href=&#34;#pipereader-和-pipewriter-类型&#34;&gt;PipeWriter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上类型中，常用的类型有，文件IO，缓冲IO，网络IO，在标准库中都有实现&lt;/p&gt;

&lt;p&gt;网络io/文件io/标准io&amp;ndash;其实就是操作网络数据和文件中的数据
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net/&#34;&gt;net.Conn&lt;/a&gt;, &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/&#34;&gt;os.Stdin&lt;/a&gt;, &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/&#34;&gt;os.File&lt;/a&gt;: 网络、标准输入输出、文件的流读取，对应&amp;mdash;frp就是基于这个基础上实现的&lt;/p&gt;

&lt;p&gt;缓存io&amp;ndash;其实就是操作缓存中的string，[]byte
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/#reader-类型&#34;&gt;strings.Reader&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/#builder-类型&#34;&gt;strings.Builder&lt;/a&gt;: 把字符串抽象成Reader
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/#reader-类型&#34;&gt;bytes.Reader&lt;/a&gt;: 把[]byte抽象成Reader
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/#buffer-类型&#34;&gt;bytes.Buffer&lt;/a&gt;: 把[]byte抽象成Reader和Writer&lt;/p&gt;

&lt;p&gt;缓存io&amp;ndash;还是使用缓存，但是主要是对io.reader实例进行读写
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bufio/&#34;&gt;bufio.Reader/Writer&lt;/a&gt;: 抽象成带缓冲的流读取（比如按行读写）&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Go Net 协议层</title>
          <link>https://kingjcy.github.io/post/golang/go-net/</link>
          <pubDate>Mon, 11 Jul 2016 17:34:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-net/</guid>
          <description>&lt;p&gt;网络编程是go语言使用的一个核心模块。golang的网络封装使用对于底层socket或者上层的http，甚至是web服务都很友好。&lt;/p&gt;

&lt;h1 id=&#34;net&#34;&gt;net&lt;/h1&gt;

&lt;p&gt;net包提供了可移植的网络I/O接口，包括TCP/IP、UDP、域名解析和Unix域socket等方式的通信。其中每一种通信方式都使用 xxConn 结构体来表示，诸如IPConn、TCPConn等，这些结构体都实现了Conn接口，Conn接口实现了基本的读、写、关闭、获取远程和本地地址、设置timeout等功能。&lt;/p&gt;

&lt;p&gt;conn的接口定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Conn interface {
    // Read从连接中读取数据
    // Read方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    Read(b []byte) (n int, err error)
    // Write从连接中写入数据
    // Write方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    Write(b []byte) (n int, err error)
    // Close方法关闭该连接
    // 并会导致任何阻塞中的Read或Write方法不再阻塞并返回错误
    Close() error
    // 返回本地网络地址
    LocalAddr() Addr
    // 返回远端网络地址
    RemoteAddr() Addr
    // 设定该连接的读写deadline，等价于同时调用SetReadDeadline和SetWriteDeadline
    // deadline是一个绝对时间，超过该时间后I/O操作就会直接因超时失败返回而不会阻塞
    // deadline对之后的所有I/O操作都起效，而不仅仅是下一次的读或写操作
    // 参数t为零值表示不设置期限
    SetDeadline(t time.Time) error
    // 设定该连接的读操作deadline，参数t为零值表示不设置期限
    SetReadDeadline(t time.Time) error
    // 设定该连接的写操作deadline，参数t为零值表示不设置期限
    // 即使写入超时，返回值n也可能&amp;gt;0，说明成功写入了部分数据
    SetWriteDeadline(t time.Time) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后每种类型都是对应的结构体实现这些接口。&lt;/p&gt;

&lt;p&gt;还有一个常用的接口定义PacketConn&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PacketConn interface {
    // ReadFrom方法从连接读取一个数据包，并将有效信息写入b
    // ReadFrom方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    // 返回写入的字节数和该数据包的来源地址
    ReadFrom(b []byte) (n int, addr Addr, err error)
    // WriteTo方法将有效数据b写入一个数据包发送给addr
    // WriteTo方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    // 在面向数据包的连接中，写入超时非常罕见
    WriteTo(b []byte, addr Addr) (n int, err error)
    // Close方法关闭该连接
    // 会导致任何阻塞中的ReadFrom或WriteTo方法不再阻塞并返回错误
    Close() error
    // 返回本地网络地址
    LocalAddr() Addr
    // 设定该连接的读写deadline
    SetDeadline(t time.Time) error
    // 设定该连接的读操作deadline，参数t为零值表示不设置期限
    // 如果时间到达deadline，读操作就会直接因超时失败返回而不会阻塞
    SetReadDeadline(t time.Time) error
    // 设定该连接的写操作deadline，参数t为零值表示不设置期限
    // 如果时间到达deadline，写操作就会直接因超时失败返回而不会阻塞
    // 即使写入超时，返回值n也可能&amp;gt;0，说明成功写入了部分数据
    SetWriteDeadline(t time.Time) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ip&#34;&gt;ip&lt;/h2&gt;

&lt;p&gt;使用IPConn结构体来表示，它实现了Conn、PacketConn两种接口。使用如下两个函数进行Dial（连接）和Listen（监听）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialIP(netProto string, laddr, raddr *IPAddr) (*IPConn, error)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DialIP在网络协议netProto上连接本地地址laddr和远端地址raddr，netProto必须是&amp;rdquo;ip&amp;rdquo;、&amp;rdquo;ip4&amp;rdquo;或&amp;rdquo;ip6&amp;rdquo;后跟冒号和协议名或协议号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenIP(netProto string, laddr *IPAddr) (*IPConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenIP创建一个接收目的地是本地地址laddr的IP数据包的网络连接，返回的*IPConn的ReadFrom和WriteTo方法可以用来发送和接收IP数据包。（每个包都可获取来源址或者设置目标地址）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、IPAddr类型&lt;/p&gt;

&lt;p&gt;位于iprawsock.go中在net包的许多函数和方法会返回一个指向IPAddr的指针。这不过只是一个包含IP类型的结构体。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type IPAddr struct {
    IP   IP
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个类型的另一个主要用途是通过IP主机名执行DNS查找。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ResolveIPAddr
ResolveIPAddr有两个参数第一个参数.必须为&amp;quot;ip&amp;quot;,&amp;quot;ip4&amp;quot;,&amp;quot;ip6&amp;quot;,第二个参数多为要解析的域名.返回一个IPAddr的指针类型

addr, _ := net.ResolveIPAddr(&amp;quot;ip&amp;quot;, &amp;quot;www.baidu.com&amp;quot;)
fmt.Println(addr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ip.go 中还定义了三个类型.分别是IP,IPMask,IPNet&lt;/p&gt;

&lt;p&gt;2、IP类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type IP []byte
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IP类型被定义为一个字节数组。 ParseIP(String) 可以将字符窜转换为一个IP类型.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name := &amp;quot;127.0.0.1&amp;quot;
addr := net.ParseIP(name)
fmt.Println(addr.IsLoopback())// IsLoopback reports whether ip is a loopback address.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、IPMask类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// An IP mask is an IP address.
type IPMask []byte
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个掩码的字符串形式是一个十六进制数，如掩码255.255.0.0为ffff0000。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func IPv4Mask(a, b, c, d byte) IPMask :用一个4字节的IPv4地址来创建一个掩码.
func CIDRMask(ones, bits int) IPMask : 用ones和bits来创建一个掩码
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、IPNet类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// An IPNet represents an IP network.
type IPNet struct {
    IP   IP     // network number
    Mask IPMask // network mask
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由IP类型和IPMask组成一个网段,其字符串形式是CIDR地址,如:“192.168.100.1/24”或“2001:DB8::/ 48”&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    mask := net.IPv4Mask(byte(255), byte(255), byte(255), byte(0))
    ip := net.ParseIP(&amp;quot;192.168.1.125&amp;quot;).Mask(mask)
    in := &amp;amp;net.IPNet{ip, mask}
    fmt.Println(in)         //  192.168.1.0/24
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边插播一个经常使用的实例：获取本地IP&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net&amp;quot;
    &amp;quot;os&amp;quot;
)
func main() {
    addrs, err := net.InterfaceAddrs()
    if err != nil {
        fmt.Println(err)
        os.Exit(1)
    }
    for _, address := range addrs {
        // 检查ip地址判断是否回环地址
        if ipnet, ok := address.(*net.IPNet); ok &amp;amp;&amp;amp; !ipnet.IP.IsLoopback() {
            if ipnet.IP.To4() != nil {
                fmt.Println(ipnet.IP.String())
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tcp&#34;&gt;tcp&lt;/h2&gt;

&lt;p&gt;使用TCPConn结构体来表示，它实现了Conn接口。&lt;/p&gt;

&lt;p&gt;使用DialTCP进行Dial操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialTCP(net string, laddr, raddr *TCPAddr) (*TCPConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DialTCP在网络协议net上连接本地地址laddr和远端地址raddr。net必须是&amp;rdquo;tcp&amp;rdquo;、&amp;rdquo;tcp4&amp;rdquo;、&amp;rdquo;tcp6&amp;rdquo;；如果laddr不是nil，将使用它作为本地地址，否则自动选择一个本地地址。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenTCP(net string, laddr *TCPAddr) (*TCPListener, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 ListenTCP函数进行Listen，产生一个TCPListener结构体，使用TCPListener的AcceptTCP方法建立通信链路，得到TCPConn。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;TCPAddr类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;位于tcpsock.go中TCPAddr类型包含一个IP和一个port的结构:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type TCPAddr struct {
    IP   IP
    Port int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResolveTCPAddr&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ResolveTCPAddr(net, addr string) (*TCPAddr, os.Error) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数用来创建一个TCPAddr,第一个参数为,tcp,tcp4或者tcp6,addr是一个字符串，由主机名或IP地址，以及&amp;rdquo;:&amp;ldquo;后跟随着端口号组成，例如： &amp;ldquo;www.google.com:80&amp;rdquo; 或 &amp;lsquo;127.0.0.1:22&amp;rdquo;。如果地址是一个IPv6地址，由于已经有冒号，主机部分，必须放在方括号内, 例如：&amp;rdquo;[::1]:23&amp;rdquo;. 另一种特殊情况是经常用于服务器, 主机地址为0, 因此，TCP地址实际上就是端口名称, 例如：&amp;rdquo;:80&amp;rdquo; 用来表示HTTP服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;addr, _ := net.ResolveTCPAddr(&amp;quot;tcp&amp;quot;, &amp;quot;www.baidu.com:80&amp;quot;)
fmt.Println(addr)   //220.181.111.147:80
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;udp&#34;&gt;udp&lt;/h2&gt;

&lt;p&gt;使用UDPConn接口体来表示，它实现了Conn、PacketConn两种接口。使用如下两个函数进行Dial和Listen。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialUDP(net string, laddr, raddr *UDPAddr) (*UDPConn, error)    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DialTCP在网络协议net上连接本地地址laddr和远端地址raddr。net必须是&amp;rdquo;udp&amp;rdquo;、&amp;rdquo;udp4&amp;rdquo;、&amp;rdquo;udp6&amp;rdquo;；如果laddr不是nil，将使用它作为本地地址，否则自动选择一个本地地址。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenUDP(net string, laddr *UDPAddr) (*UDPConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenUDP创建一个接收目的地是本地地址laddr的UDP数据包的网络连接。net必须是&amp;rdquo;udp&amp;rdquo;、&amp;rdquo;udp4&amp;rdquo;、&amp;rdquo;udp6&amp;rdquo;；如果laddr端口为0，函数将选择一个当前可用的端口，可以用Listener的Addr方法获得该端口。返回的*UDPConn的ReadFrom和WriteTo方法可以用来发送和接收UDP数据包（每个包都可获得来源地址或设置目标地址）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、UDPAddr类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type UDPAddr struct {
    IP   IP
    Port int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResolveUDPAddr同样的功能&lt;/p&gt;

&lt;p&gt;2、UnixAddr类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type UnixAddr struct {
    Name string
    Net  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResolveUnixAddr同样的功能&lt;/p&gt;

&lt;h2 id=&#34;unix&#34;&gt;unix&lt;/h2&gt;

&lt;p&gt;UnixConn实现了Conn、PacketConn两种接口，其中unix又分为SOCK_DGRAM、SOCK_STREAM。&lt;/p&gt;

&lt;p&gt;1.对于unix（SOCK_DGRAM），使用如下两个函数进行Dial和Listen。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialUnix(net string, laddr, raddr *UnixAddr) (*UnixConn, error)    

func ListenUnixgram(net string, laddr *UnixAddr) (*UnixConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.对于unix（SOCK_STREAM）&lt;/p&gt;

&lt;p&gt;客户端使用DialUnix进行Dial操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialUnix(net string, laddr, raddr *UnixAddr) (*UnixConn, error)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务端使用ListenUnix函数进行Listen操作，然后使用UnixListener进行AcceptUnix&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenUnix(net string, laddr *UnixAddr) (*UnixListener, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;函数整合&#34;&gt;函数整合&lt;/h1&gt;

&lt;p&gt;为了使用方便，golang将上面一些重复的操作集中到一个函数中。在参数中制定上面不同协议类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenPacket(net, laddr string) (PacketConn, error)　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数用于侦听ip、udp、unix（DGRAM）等协议，返回一个PacketConn接口，同样根据侦听的协议不同，这个接口可以包含IPCon、UDPConn、UnixConn等，它们都实现了PacketConn。可以发现与ip、unix（stream）协议不同，直接返回的是xxConn，不是间接的通过Listener进行Accept操作后，才得到一个Conn。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Listen(net, laddr string) (Listener, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数用于侦听tcp、unix（stream）等协议，返回一个Listener接口、根据侦听的协议不同，这个接口可以包含TCPListener、UnixListener等，它们都实现了Listener接口，然后通过调用其Accept方法可以得到Conn接口，进行通信。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Dial(network, address string) (Conn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数对于所有的协议都是相同的操作，返回一个Conn接口，根据协议的不同实际上包含IPConn、UDPConn、UnixConn、IPConn，它们都实现了Conn接口&lt;/p&gt;

&lt;h1 id=&#34;基本c-s功能&#34;&gt;基本c/s功能&lt;/h1&gt;

&lt;p&gt;在 Unix/Linux 中的 Socket 编程主要通过调用 listen, accept, write read 等函数来实现的. 具体如下图所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/unix_socket.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;服务端&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;服务端listen, accept&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func connHandler(c net.Conn) {
    for {
        cnt, err := c.Read(buf)
        c.Write(buf)
    }
}
func main() {
    server, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:1208&amp;quot;)
    for {
        conn, err := server.Accept()
        go connHandler(conn)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接使用net的listen返回的就是对应协议已经定义好的结构体，比如tcp&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type TCPListener struct {
    fd *netFD
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个结构体实现了listener接口的所有接口，所以可以作为返回值返回。其他协议类型也是一样。&lt;/p&gt;

&lt;p&gt;accept后返回的conn是一个存储着连接信息的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Network file descriptor.
type netFD struct {
    pfd poll.FD

    // immutable until Close
    family      int
    sotype      int
    isConnected bool // handshake completed or use of association with peer
    net         string
    laddr       Addr
    raddr       Addr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;客户端&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;客户端dial&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func connHandler(c net.Conn) {
    for {
        c.Write(...)
        c.Read(...)
    }
}
func main() {
    conn, err := net.Dial(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:1208&amp;quot;)
    connHandler(conn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Dial(net, addr string) (Conn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中net参数是网络协议的名字， addr参数是IP地址或域名，而端口号以“:”的形式跟随在地址
或域名的后面，端口号可选。如果连接成功，返回连接对象，否则返回error。&lt;/p&gt;

&lt;p&gt;Dial() 函数支持如下几种网络协议：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;tcp&amp;quot; 、 &amp;quot;tcp4&amp;quot; （仅限IPv4）、 &amp;quot;tcp6&amp;quot; （仅限IPv6）、 &amp;quot;udp&amp;quot; 、 &amp;quot;udp4&amp;quot;（仅限IPv4）、 &amp;quot;udp6&amp;quot;（仅限IPv6）、 &amp;quot;ip&amp;quot; 、 &amp;quot;ip4&amp;quot;（仅限IPv4）和&amp;quot;ip6&amp;quot;（仅限IPv6）。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以直接用相关协议的函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialTCP(net string, laddr, raddr *TCPAddr) (c *TCPConn, err error)
func DialUDP(net string, laddr, raddr *UDPAddr) (c *UDPConn, err error)
func DialIP(netProto string, laddr, raddr *IPAddr) (*IPConn, error)
func DialUnix(net string, laddr, raddr *UnixAddr) (c *UnixConn, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;特性功能&#34;&gt;特性功能&lt;/h2&gt;

&lt;p&gt;1、控制TCP连接&lt;/p&gt;

&lt;p&gt;TCP连接有很多控制函数，我们平常用到比较多的有如下几个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *TCPConn) SetTimeout(nsec int64) os.Error
func (c *TCPConn) SetKeepAlive(keepalive bool) os.Error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个函数用来设置超时时间，客户端和服务器端都适用，当超过设置的时间时那么该链接就失效。&lt;/p&gt;

&lt;p&gt;第二个函数用来设置客户端是否和服务器端一直保持着连接，即使没有任何的数据发送&lt;/p&gt;

&lt;h1 id=&#34;实例&#34;&gt;实例&lt;/h1&gt;

&lt;p&gt;从零开始写Socket Server： Socket-Client框架&lt;/p&gt;

&lt;p&gt;在golang中，网络协议已经被封装的非常完好了，想要写一个Socket的Server，我们并不用像其他语言那样需要为socket、bind、listen、receive等一系列操作头疼，只要使用Golang中自带的net包即可很方便的完成连接等操作~&lt;/p&gt;

&lt;p&gt;在这里，给出一个最最基础的基于Socket的Server的写法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
)


func main() {

//建立socket，监听端口
    netListen, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:1024&amp;quot;)
    CheckError(err)
    defer netListen.Close()

    Log(&amp;quot;Waiting for clients&amp;quot;)
    for {
        conn, err := netListen.Accept()
        if err != nil {
            continue
        }

        Log(conn.RemoteAddr().String(), &amp;quot; tcp connect success&amp;quot;)
        handleConnection(conn)
    }
}
//处理连接
func handleConnection(conn net.Conn) {

    buffer := make([]byte, 2048)

    for {

        n, err := conn.Read(buffer)

        if err != nil {
            Log(conn.RemoteAddr().String(), &amp;quot; connection error: &amp;quot;, err)
            return
        }


        Log(conn.RemoteAddr().String(), &amp;quot;receive data string:\n&amp;quot;, string(buffer[:n]))

    }

}
func Log(v ...interface{}) {
    log.Println(v...)
}

func CheckError(err error) {
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Fatal error: %s&amp;quot;, err.Error())
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;唔，抛除Go语言里面10行代码有5行error的蛋疼之处,你可以看到，Server想要建立并接受一个Socket，其核心流程就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netListen, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:1024&amp;quot;)
conn, err := netListen.Accept()
n, err := conn.Read(buffer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这三步，通过Listen、Accept 和Read，我们就成功的绑定了一个端口，并能够读取从该端口传来的内容~&lt;/p&gt;

&lt;p&gt;这边插播一个内容，关于read是阻塞的，如果读取不到内容，代码会阻塞在这边，直到有内容可以读取，包括connection断掉返回的io.EOF,一般对这个都有特殊处理。一般重conn读取数据也是在for循环中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;net&amp;quot;
)

func main(){
    ln, err := net.Listen(&amp;quot;tcp&amp;quot;,&amp;quot;127.0.0.1:10051&amp;quot;)

    if err != nil {
        panic(err)
    }

    for {
        conn, _ := ln.Accept() //The loop will be held here
        fmt.Println(&amp;quot;get connect&amp;quot;)
        go handleread(conn)


    }
}

func handleread(conn net.Conn){
    defer conn.Close()

    var tatalBuffer  []byte
    var all int
    for {
        buffer := make([]byte, 2)
        n,err := conn.Read(buffer)
        if err == io.EOF{
            fmt.Println(err,n)
            break
        }

        tatalBuffer = append(tatalBuffer,buffer...)
        all += n

        fmt.Println(string(buffer),n,string(tatalBuffer[:all]),all)
    }



}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这个例子中，会重conn中两个字符循环读取内容，这边slice不会动态扩容，所以需要使用append来获取全部内容。&lt;/p&gt;

&lt;p&gt;还有一点，buffer := make([]byte, 2)这个代码，放在for循环中，浪费内存，可以放在gor循环外部，然后使用n来截取buf[:n]可以解决buf最后一部分重复的问题。&lt;/p&gt;

&lt;p&gt;插播结束，回到server。&lt;/p&gt;

&lt;p&gt;Server写好之后，接下来就是Client方面啦，我手写一个HelloWorld给大家：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net&amp;quot;
    &amp;quot;os&amp;quot;
)

func sender(conn net.Conn) {
        words := &amp;quot;hello world!&amp;quot;
        conn.Write([]byte(words))
    fmt.Println(&amp;quot;send over&amp;quot;)

}



func main() {
    server := &amp;quot;127.0.0.1:1024&amp;quot;
    tcpAddr, err := net.ResolveTCPAddr(&amp;quot;tcp4&amp;quot;, server)
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Fatal error: %s&amp;quot;, err.Error())
        os.Exit(1)
    }

    conn, err := net.DialTCP(&amp;quot;tcp&amp;quot;, nil, tcpAddr)
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Fatal error: %s&amp;quot;, err.Error())
        os.Exit(1)
    }


    fmt.Println(&amp;quot;connect success&amp;quot;)
    sender(conn)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，Client这里的关键在于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tcpAddr, err := net.ResolveTCPAddr(&amp;quot;tcp4&amp;quot;, server)
conn, err := net.DialTCP(&amp;quot;tcp&amp;quot;, nil, tcpAddr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这两步，主要是负责解析端口和连接。&lt;/p&gt;

&lt;p&gt;这边插播一个tcp协议的三次握手图，加强理解。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/tcp_open_close.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;其实我们最常用的还是&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net-http/&#34;&gt;http协议&lt;/a&gt;，也即是应用层的协议，其实http协议是在tcp协议的基础上进行封装，最终还是使用的这边基本的网络IO，所以在网络传输中，网络IO的基本协议的实现是基础。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列----  Builtin</title>
          <link>https://kingjcy.github.io/post/golang/go-builtin/</link>
          <pubDate>Tue, 28 Jun 2016 20:36:55 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-builtin/</guid>
          <description>&lt;p&gt;builtin包是go的预声明定义，包括go语言中常用的各种类型和方法声明，包括变量和常量两部分．&lt;/p&gt;

&lt;p&gt;builtin 包为Go的预声明标识符提供了文档。我们常用的一下常量和函数就是在这个包中定义的，以便于我们直接使用，下面对一些用过的进行整理和记录。&lt;/p&gt;

&lt;h1 id=&#34;常量&#34;&gt;常量&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;true和false&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;const (
        true  = 0 == 0 // Untyped bool.
        false = 0 != 0 // Untyped bool.
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;true和false是两个无类型的bool值&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;error&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type error interface {
    Error() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内建error接口类型是约定用于表示错误信息，nil值表示无错误。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;iota&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;const iota = 0 // Untyped int.无类型int
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;变量&#34;&gt;变量&lt;/h1&gt;

&lt;p&gt;变量就有很多了，比如我们常量的append函数就是在这边定义的，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func append(slice []Type, elems ...Type) []Type
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;还有很多使用的时候看一下，这边就不多说了，主要知道内置的这边常量和变量定义的位置。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Os</title>
          <link>https://kingjcy.github.io/post/golang/go-os/</link>
          <pubDate>Thu, 02 Jun 2016 09:52:35 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-os/</guid>
          <description>&lt;p&gt;os包中实现了不依赖平台的操作系统函数接口(平台无关的接口)，设计向Unix风格，但是错误处理是go风格，当os包使用时，如果失败之后返回错误类型而不是错误数量,返回错误值而非错误码,可以包含更多信息。&lt;/p&gt;

&lt;h1 id=&#34;os&#34;&gt;os&lt;/h1&gt;

&lt;p&gt;os 依赖于 syscall。在实际编程中，我们应该总是优先使用 os 中提供的功能，而不是 syscall。&lt;/p&gt;

&lt;p&gt;os包提供了操作系统函数的不依赖平台的接口。一般都是linux下的一些基本命令的操作，比如文件，目录操作之类。&lt;/p&gt;

&lt;p&gt;我们运行程序常用的命令行参数就是在这个包中可以获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var Args []string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Args保管了命令行参数，第一个是程序名。&lt;/p&gt;

&lt;h2 id=&#34;文件io&#34;&gt;文件io&lt;/h2&gt;

&lt;p&gt;文件IO就是对文件的读写操作，我们先了解一些os中的基本概念。&lt;/p&gt;

&lt;h3 id=&#34;基本概念&#34;&gt;基本概念&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;文件描述符&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有 I/O 操作以文件描述符 ( 一个非负整数 , 通常是小整数 ) 来指代打开的文件。文件描述符用以表示所有类型的已打开文件，包括管道（pipe）、FIFO、socket、终端、设备和普通文件。&lt;/p&gt;

&lt;p&gt;在 Go 中，文件描述符封装在 os.File 结构中，通过 File.Fd() 可以获得底层的文件描述符：fd。&lt;/p&gt;

&lt;p&gt;File结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type File struct {
    *file
}
// file is the real representation of *File.
// The extra level of indirection ensures that no clients of os
// can overwrite this data, which could cause the finalizer
// to close the wrong file descriptor.
type file struct {
    fd      int
    name    string
    dirinfo *dirInfo // nil unless directory being read
}

// Auxiliary information if the File describes a directory
type dirInfo struct {
    buf  []byte // buffer for directory I/O
    nbuf int    // length of buf; return value from Getdirentries
    bufp int    // location of next record in buf.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;标准定义&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;按照惯例，大多数程序都期望能够使用 3 种标准的文件描述符：0- 标准输入；1- 标准输出；2- 标准错误。os 包提供了 3 个 File 对象，分别代表这 3 种标准描述符：Stdin、Stdout 和 Stderr，它们对应的文件名分别是：/dev/stdin、/dev/stdout 和 /dev/stderr。&lt;/p&gt;

&lt;h3 id=&#34;基本操作&#34;&gt;基本操作&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewFile(fd uintptr, name string) *File
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewFile使用给出的Unix文件描述符和名称创建一个文件。&lt;/p&gt;

&lt;p&gt;正常使用create来创建一个文件，比如文件不存在，就创建一个&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file,er:=os.Open(&amp;quot;xxx&amp;quot;)
defer func(){file.Close()}()
if er!=nil &amp;amp;&amp;amp; os.IfNotExist(er
r){
  file = os.Create(&amp;quot;xx&amp;quot;)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;打开&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Open(name string) (file *File, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open打开一个文件用于读取。如果操作成功，返回的文件对象的方法可用于读取数据；对应的文件描述符具有O_RDONLY模式。如果出错，错误底层类型是*PathError。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func OpenFile(name string, flag int, perm FileMode) (file *File, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OpenFile是一个更一般性的文件打开函数，大多数调用者都应用Open或Create代替本函数。它会使用指定的选项（如O_RDONLY等）、指定的模式（如0666等）打开指定名称的文件。如果操作成功，返回的文件对象可用于I/O。如果出错，错误底层类型是*PathError。&lt;/p&gt;

&lt;p&gt;位掩码参数 flag 用于指定文件的访问模式，可用的值在 os 中定义为常量（以下值并非所有操作系统都可用）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    O_RDONLY int = syscall.O_RDONLY // 只读模式打开文件
    O_WRONLY int = syscall.O_WRONLY // 只写模式打开文件
    O_RDWR   int = syscall.O_RDWR   // 读写模式打开文件
    O_APPEND int = syscall.O_APPEND // 写操作时将数据附加到文件尾部
    O_CREATE int = syscall.O_CREAT  // 如果不存在将创建一个新文件
    O_EXCL   int = syscall.O_EXCL   // 和 O_CREATE 配合使用，文件必须不存在
    O_SYNC   int = syscall.O_SYNC   // 打开文件用于同步 I/O
    O_TRUNC  int = syscall.O_TRUNC  // 如果可能，打开时清空文件
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O_TRUNC这个参数可以用来清空文件，如果可以的话，还可以用这个函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;os.Truncate(name, size)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Truncate(size int64) error
size 填0 就把文件清空了。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面有详细的说明&lt;/p&gt;

&lt;p&gt;位掩码参数 perm 指定了文件的模式和权限位，类型是 os.FileMode，文件模式位常量定义在 os 中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    // 单字符是被 String 方法用于格式化的属性缩写。
    ModeDir        FileMode = 1 &amp;lt;&amp;lt; (32 - 1 - iota) // d: 目录
    ModeAppend                                     // a: 只能写入，且只能写入到末尾
    ModeExclusive                                  // l: 用于执行
    ModeTemporary                                  // T: 临时文件（非备份文件）
    ModeSymlink                                    // L: 符号链接（不是快捷方式文件）
    ModeDevice                                     // D: 设备
    ModeNamedPipe                                  // p: 命名管道（FIFO）
    ModeSocket                                     // S: Unix 域 socket
    ModeSetuid                                     // u: 表示文件具有其创建者用户 id 权限
    ModeSetgid                                     // g: 表示文件具有其创建者组 id 的权限
    ModeCharDevice                                 // c: 字符设备，需已设置 ModeDevice
    ModeSticky                                     // t: 只有 root/ 创建者能删除 / 移动文件

    // 覆盖所有类型位（用于通过 &amp;amp; 获取类型位），对普通文件，所有这些位都不应被设置
    ModeType = ModeDir | ModeSymlink | ModeNamedPipe | ModeSocket | ModeDevice
    ModePerm FileMode = 0777 // 覆盖所有 Unix 权限位（用于通过 &amp;amp; 获取类型位）
)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Read(b []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Read 方法从 f 中读取最多 len(b) 字节数据并写入 b。它返回读取的字节数和可能遇到的任何错误。文件终止标志是读取 0 个字节且返回值 err 为 io.EOF。&lt;/p&gt;

&lt;p&gt;从方法声明可以知道，File 实现了 io.Reader 接口。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) ReadAt(b []byte, off int64) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ReadAt 从指定的位置（相对于文件开始位置）读取长度为 len(b) 个字节数据并写入 b。它返回读取的字节数和可能遇到的任何错误。当 n&amp;lt;len(b) 时，本方法总是会返回错误；如果是因为到达文件结尾，返回值 err 会是 io.EOF。它对应的系统调用是 pread。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var chunks []byte
buf := make([]byte, 1024)
var count = 0
for {
    n, err := f.Read(buf)
    if err != nil &amp;amp;&amp;amp; err != io.EOF {
        panic(err)
    }
    if 0 == n {
        break
    }
    count = count + n
    chunks = append(chunks, buf[:n]...)
}
r.logger.Debugf(&amp;quot;read file content : %s&amp;quot;,string(chunks[:count]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边这个实例主要是要说明一下几个重点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、buf必须make，不然会panic
2、read必须for循环，直到io.EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;write&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Write(b []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write 向文件中写入 len(b) 字节数据。它返回写入的字节数和可能遇到的任何错误。如果返回值 n!=len(b)，本方法会返回一个非 nil 的错误。&lt;/p&gt;

&lt;p&gt;从方法声明可以知道，File 实现了 io.Writer 接口。&lt;/p&gt;

&lt;p&gt;Write 与 WriteAt 的区别同 Read 与 ReadAt 的区别一样。为了方便，还提供了 WriteString 方法，它实际是对 Write 的封装。&lt;/p&gt;

&lt;p&gt;注意：Write 调用成功并不能保证数据已经写入磁盘，因为内核会缓存磁盘的 I/O 操作。如果希望立刻将数据写入磁盘（一般场景不建议这么做，因为会影响性能），有两种办法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 打开文件时指定 `os.O_SYNC`；
2. 调用 `File.Sync()` 方法。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：File.Sync() 底层调用的是 fsync 系统调用，这会将数据和元数据都刷到磁盘；如果只想刷数据到磁盘（比如，文件大小没变，只是变了文件数据），需要自己封装，调用 fdatasync（syscall.Fdatasync） 系统调用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;close&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;close() 系统调用关闭一个打开的文件描述符，并将其释放回调用进程，供该进程继续使用。当进程终止时，将自动关闭其已打开的所有文件描述符。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;os.File.Close() 是对 close() 的封装。我们应该养成关闭不需要的文件的良好编程习惯。文件描述符是资源，Go 的 gc 是针对内存的，并不会自动回收资源，如果不关闭文件描述符，长期运行的服务可能会把文件描述符耗尽。&lt;/p&gt;

&lt;p&gt;以下两种情况会导致 Close 返回错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 关闭一个未打开的文件；
2. 两次关闭同一个文件；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常，我们不会去检查 Close 的错误&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;seek&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Seek(offset int64, whence int) (ret int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seek 设置下一次读 / 写的位置。offset 为相对偏移量，而 whence 决定相对位置：0 为相对文件开头，1 为相对当前位置，2 为相对文件结尾。它返回新的偏移量（相对开头）和可能的错误。使用中，whence 应该使用 os 包中的常量：SEEK_SET、SEEK_CUR 和 SEEK_END&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file.Seek(0, os.SEEK_SET)    // 文件开始处
file.Seek(0, SEEK_END)        // 文件结尾处的下一个字节
file.Seek(-1, SEEK_END)        // 文件最后一个字节
file.Seek(-10, SEEK_CUR)     // 当前位置前 10 个字节
file.Seek(1000, SEEK_END)    // 文件结尾处的下 1001 个字节
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;trucate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;trucate 和 ftruncate 系统调用将文件大小设置为 size 参数指定的值；Go 语言中相应的包装函数是 os.Truncate 和 os.File.Truncate。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Truncate(name string, size int64) error
func (f *File) Truncate(size int64) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果文件当前长度大于参数 size，调用将丢弃超出部分，若小于参数 size，调用将在文件尾部添加一系列空字节或是一个文件空洞。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;remove&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Remove(name string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove删除name指定的文件或目录。如果出错，会返回*PathError底层类型的错误。&lt;/p&gt;

&lt;h2 id=&#34;文件属性&#34;&gt;文件属性&lt;/h2&gt;

&lt;h3 id=&#34;文件信息&#34;&gt;文件信息&lt;/h3&gt;

&lt;p&gt;可以通过包里的函数 Stat、Lstat 和 File.Stat 可以得到os.FileInfo 接口的信息。这三个函数对应三个系统调用：stat、lstat 和 fstat。&lt;/p&gt;

&lt;p&gt;这三个函数的区别：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;stat 会返回所命名文件的相关信息。&lt;/li&gt;
&lt;li&gt;lstat 与 stat 类似，区别在于如果文件是符号链接，那么所返回的信息针对的是符号链接自身（而非符号链接所指向的文件）。&lt;/li&gt;
&lt;li&gt;fstat 则会返回由某个打开文件描述符（Go 中则是当前打开文件 File）所指代文件的相关信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stat 和 Lstat 无需对其所操作的文件本身拥有任何权限，但针对指定 name 的父目录要有执行（搜索）权限。而只要 File 对象 ok，File.Stat 总是成功。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Stat() (fi FileInfo, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stat返回描述文件f的FileInfo类型值。如果出错，错误底层类型是*PathError。这个方法也可以用于检查文件是否有问题，上面说到文件的信息是存储在FileInfo 接口中的，我们来看一下这个接口&lt;/p&gt;

&lt;p&gt;FileInfo是一个接口，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// A FileInfo describes a file and is returned by Stat and Lstat.
type FileInfo interface {
    Name() string       // base name of the file 文件的名字（不含扩展名）
    Size() int64        // length in bytes for regular files; system-dependent for others  普通文件返回值表示其大小；其他文件的返回值含义各系统不同
    Mode() FileMode     // file mode bits   文件的模式位
    ModTime() time.Time // modification time    文件的修改时间
    IsDir() bool        // abbreviation for Mode().IsDir()  等价于 Mode().IsDir()
    Sys() interface{}   // underlying data source (can return nil)  底层数据来源（可以返回 nil）
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该接口提供了一个sys函数，Sys() 底层数据的 C 语言 结构 statbuf 格式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct stat {
  dev_t    st_dev;    // 设备 ID
  ino_t    st_ino;    // 文件 i 节点号
  mode_t    st_mode;    // 位掩码，文件类型和文件权限
  nlink_t    st_nlink;    // 硬链接数
  uid_t    st_uid;    // 文件属主，用户 ID
  gid_t    st_gid;    // 文件属组，组 ID
  dev_t    st_rdev;    // 如果针对设备 i 节点，则此字段包含主、辅 ID
  off_t    st_size;    // 常规文件，则是文件字节数；符号链接，则是链接所指路径名的长度，字节为单位；对于共享内存对象，则是对象大小
  blksize_t    st_blsize;    // 分配给文件的总块数，块大小为 512 字节
  blkcnt_t    st_blocks;    // 实际分配给文件的磁盘块数量
  time_t    st_atime;        // 对文件上次访问时间
  time_t    st_mtime;        // 对文件上次修改时间
  time_t    st_ctime;        // 文件状态发生改变的上次时间
}
Go 中 syscal.Stat_t 与该结构对应。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我们要获取 FileInfo 接口没法直接返回的信息，比如想获取文件的上次访问时间，示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fileInfo, err := os.Stat(&amp;quot;test.log&amp;quot;)
if err != nil {
  log.Fatal(err)
}
sys := fileInfo.Sys()
stat := sys.(*syscall.Stat_t)
fmt.Println(time.Unix(stat.Atimespec.Unix()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常返回的是实现这个接口的结构体，也就是fileStat，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// A fileStat is the implementation of FileInfo returned by Stat and Lstat.
type fileStat struct {
    name    string
    size    int64
    mode    FileMode
    modTime time.Time
    sys     syscall.Stat_t
}

func (fs *fileStat) Size() int64        { return fs.size }
func (fs *fileStat) Mode() FileMode     { return fs.mode }
func (fs *fileStat) ModTime() time.Time { return fs.modTime }
func (fs *fileStat) Sys() interface{}   { return &amp;amp;fs.sys }

func sameFile(fs1, fs2 *fileStat) bool {
    return fs1.sys.Dev == fs2.sys.Dev &amp;amp;&amp;amp; fs1.sys.Ino == fs2.sys.Ino
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中有一个syscall.Stat_t，源于syscall的结构体，这个结构体是需要区分系统的，不同的系统调用不一样，不然编译不通过，报错如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;registry/delete.go:49:27: stat.Ctimespec undefined (type *syscall.Stat_t has no field or method Ctimespec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;是因为在linux下结构体成名名是Ctim，在drawin下是Ctimespec，导致跨平台编译报错。&lt;/p&gt;

&lt;h3 id=&#34;文件时间&#34;&gt;文件时间&lt;/h3&gt;

&lt;p&gt;通过包里的Chtimes函数可以显式改变文件的访问时间和修改时间。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Chtimes(name string, atime time.Time, mtime time.Time) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chtimes 修改 name 指定的文件对象的访问时间和修改时间，类似 Unix 的 utime() 或 utimes() 函数。底层的文件系统可能会截断 / 舍入时间单位到更低的精确度。如果出错，会返回 *PathError 类型的错误。在 Unix 中，底层实现会调用 utimenstat()，它提供纳秒级别的精度&lt;/p&gt;

&lt;h3 id=&#34;文件权限&#34;&gt;文件权限&lt;/h3&gt;

&lt;p&gt;系统调用 chown、lchown 和 fchown 可用来改变文件的属主和属组，Go 中os包中对应的函数或方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Chown(name string, uid, gid int) error
func Lchown(name string, uid, gid int) error
func (f *File) Chown(uid, gid int) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它们的区别和上文提到的 Stat 相关函数类似。&lt;/p&gt;

&lt;p&gt;在文件相关操作报错时，可以通过 os.IsPermission 检查是否是权限的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func IsPermission(err error) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个布尔值说明该错误是否表示因权限不足要求被拒绝。ErrPermission 和一些系统调用错误会使它返回真。&lt;/p&gt;

&lt;p&gt;另外，syscall.Access 可以获取文件的权限。这对应系统调用 access。&lt;/p&gt;

&lt;p&gt;os.Chmod 和 os.File.Chmod 可以修改文件权限（包括 sticky 位），分别对应系统调用 chmod 和 fchmod。&lt;/p&gt;

&lt;h2 id=&#34;目录&#34;&gt;目录&lt;/h2&gt;

&lt;p&gt;在 Unix 文件系统中，目录的存储方式类似于普通文件。目录和普通文件的区别有二：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在其 i-node 条目中，会将目录标记为一种不同的文件类型。&lt;/li&gt;
&lt;li&gt;目录是经特殊组织而成的文件。本质上说就是一个表格，包含文件名和 i-node 标号&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;目录操作&#34;&gt;目录操作&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Mkdir(name string, perm FileMode) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mkdir 使用指定的权限和名称创建一个目录。如果出错，会返回 *PathError 类型的错误。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;name 参数指定了新目录的路径名，可以是相对路径，也可以是绝对路径。如果已经存在，则调用失败并返回 os.ErrExist 错误。&lt;/li&gt;
&lt;li&gt;perm 参数指定了新目录的权限。对该位掩码值的指定方式和 os.OpenFile 相同，也可以直接赋予八进制数值。注意，perm 值还将于进程掩码相与（&amp;amp;）。如果 perm 中设置了 sticky 位，那么将对新目录设置该权限。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为 Mkdir 所创建的只是路径名中的最后一部分，如果父目录不存在，创建会失败。os.MkdirAll 用于递归创建所有不存在的目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MkdirAll(path string, perm FileMode) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MkdirAll使用指定的权限和名称创建一个目录，包括任何必要的上级目录，并返回nil，否则返回错误。权限位perm会应用在每一个被本函数创建的目录上。如果path指定了一个已经存在的目录，MkdirAll不做任何操作并返回nil。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;删除&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Remove(name string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove 删除 name 指定的文件或目录。如果出错，会返回 *PathError 类型的错误。如果目录不为空，Remove 会返回失败。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func RemoveAll(path string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RemoveAll 删除 path 指定的文件，或目录及它包含的任何下级对象。它会尝试删除所有东西，除非遇到错误并返回。如果 path 指定的对象不存在，RemoveAll 会返回 nil 而不返回错误。&lt;/p&gt;

&lt;p&gt;RemoveAll 的内部实现逻辑如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;调用 Remove 尝试进行删除，如果成功或返回 path 不存在，则直接返回 nil；&lt;/li&gt;
&lt;li&gt;调用 Lstat 获取 path 信息，以便判断是否是目录。注意，这里使用 Lstat，表示不对符号链接解引用；&lt;/li&gt;
&lt;li&gt;调用 Open 打开目录，递归读取目录中内容，执行删除操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;读&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Readdirnames(n int) (names []string, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readdirnames 读取目录 f 的内容，返回一个最多有 n 个成员的[]string，切片成员为目录中文件对象的名字，采用目录顺序。对本函数的下一次调用会返回上一次调用未读取的内容的信息。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果 n&amp;gt;0，Readdirnames 函数会返回一个最多 n 个成员的切片。这时，如果 Readdirnames 返回一个空切片，它会返回一个非 nil 的错误说明原因。如果到达了目录 f 的结尾，返回值 err 会是 io.EOF。&lt;/li&gt;
&lt;li&gt;如果 n&amp;lt;=0，Readdirnames 函数返回目录中剩余所有文件对象的名字构成的切片。此时，如果 Readdirnames 调用成功（读取所有内容直到结尾），它会返回该切片和 nil 的错误值。如果在到达结尾前遇到错误，会返回之前成功读取的名字构成的切片和该错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Readdir 内部会调用 Readdirnames，将得到的 names 构造路径，通过 Lstat 构造出 []FileInfo。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Readdir(n int) (fi []FileInfo, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;连接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Link(oldname, newname string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Link 创建一个名为 newname 指向 oldname 的硬链接。如果出错，会返回 *LinkError 类型的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Symlink(oldname, newname string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Symlink 创建一个名为 newname 指向 oldname 的符号链接。如果出错，会返回 *LinkError 类型的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Readlink(name string) (string, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readlink 获取 name 指定的符号链接指向的文件的路径。如果出错，会返回 *PathError 类型的错误。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;更改文件名&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Rename(oldpath, newpath string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Rename 修改一个文件的名字或移动一个文件。如果 newpath 已经存在，则替换它。注意，可能会有一些个操作系统特定的限制。&lt;/p&gt;

&lt;h1 id=&#34;os-singal&#34;&gt;os/singal&lt;/h1&gt;

&lt;h2 id=&#34;类型&#34;&gt;类型&lt;/h2&gt;

&lt;p&gt;Signal是一个接口，所有的信号都实现了这个接口，可以直接传递，我们传递信号的时候，需要定义这个类型的channel来传递信号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Signal interface {
    String() string
    Signal() // to distinguish from other Stringers
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;syscall 包中定义了所有的信号常量，比如syscall.SIGINT，其实就是一个int的数字信号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SIGINT    = Signal(0x2)
type Signal int

func (s Signal) Signal() {}

func (s Signal) String() string {
    if 0 &amp;lt;= s &amp;amp;&amp;amp; int(s) &amp;lt; len(signals) {
        str := signals[s]
        if str != &amp;quot;&amp;quot; {
            return str
        }
    }
    return &amp;quot;signal &amp;quot; + itoa(int(s))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;函数&#34;&gt;函数&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Notify&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;singnal主要是用于信号的传递，一般程序中需要使用信号的时候使用。主要使用下面两个方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Notify(c chan&amp;lt;- os.Signal, sig ...os.Signal)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notify函数让signal包将输入信号转发到c。如果没有列出要传递的信号，会将所有输入信号传递到c；否则只传递列出的输入信号。&lt;/p&gt;

&lt;p&gt;signal包不会为了向c发送信息而阻塞（就是说如果发送时c阻塞了，signal包会直接放弃）：调用者应该保证c有足够的缓存空间可以跟上期望的信号频率。对使用单一信号用于通知的通道，缓存为1就足够了。&lt;/p&gt;

&lt;p&gt;可以使用同一通道多次调用Notify：每一次都会扩展该通道接收的信号集。可以使用同一信号和不同通道多次调用Notify：每一个通道都会独立接收到该信号的一个拷贝。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import &amp;quot;fmt&amp;quot;
import &amp;quot;os&amp;quot;
import &amp;quot;os/signal&amp;quot;
import &amp;quot;syscall&amp;quot;
func main() {
    sigs := make(chan os.Signal, 1)
    done := make(chan bool, 1)
    signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)

    go func() {
        sig := &amp;lt;-sigs
        fmt.Println()
        fmt.Println(sig)
        done &amp;lt;- true
    }()

    fmt.Println(&amp;quot;awaiting signal&amp;quot;)
    &amp;lt;-done
    fmt.Println(&amp;quot;exiting&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;stop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;唯一从信号集去除信号的方法是调用Stop。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Stop(c chan&amp;lt;- os.Signal)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stop函数让signal包停止向c转发信号。它会取消之前使用c调用的所有Notify的效果。当Stop返回后，会保证c不再接收到任何信号。&lt;/p&gt;

&lt;h1 id=&#34;os-exec&#34;&gt;os/exec&lt;/h1&gt;

&lt;h2 id=&#34;进程io&#34;&gt;进程io&lt;/h2&gt;

&lt;p&gt;exec包用于执行外部命令。它包装了os.StartProcess函数以便更容易的修正输入和输出，使用管道连接I/O。主要用于创建一个子进程来执行相关的命令。创建子进程一定要wait，不能出现僵死进程。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;调用脚本命令&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang标准库中提供了两种方式可以用来启动进程调用脚本&lt;/p&gt;

&lt;p&gt;第一种是在os库中的Process类型，Process类型包含一系列方法用来启动进程并对进程进行操作（参考： &lt;a href=&#34;https://golang.org/pkg/os/#Process）&#34;&gt;https://golang.org/pkg/os/#Process）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;示例 使用Process执行脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os&amp;quot;
)

func main() {
    shellPath := &amp;quot;/home/xx/test.sh&amp;quot;
    argv := make([]string, 1) 
    attr := new(os.ProcAttr)
    newProcess, err := os.StartProcess(shellPath, argv, attr)  //运行脚本
    if err != nil {
        fmt.Println(err)
    }
    fmt.Println(&amp;quot;Process PID&amp;quot;, newProcess.Pid)
    processState, err := newProcess.Wait() //等待命令执行完
    if err != nil {
        fmt.Println(err)
    }
    fmt.Println(&amp;quot;processState PID:&amp;quot;, processState.Pid())//获取PID
    fmt.Println(&amp;quot;ProcessExit:&amp;quot;, processState.Exited())//获取进程是否退出
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二种是在os/exec库种通过Cmd类型的各个函数实现对脚本的调用，实际上Cmd是对Process中各种方法的高层次封装（参考： &lt;a href=&#34;https://golang.org/pkg/os/exec/）&#34;&gt;https://golang.org/pkg/os/exec/）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1、LookPath&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LookPath(file string) (string, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在环境变量PATH指定的目录中搜索可执行文件，如file中有斜杠，则只在当前目录搜索。返回完整路径或者相对于当前目录的一个相对路径。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    output, err := exec.LookPath(&amp;quot;ls&amp;quot;)
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf(output)
}

output:

[ `go run test.go` | done: 616.254982ms ]
  /bin/ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、Cmd&lt;/p&gt;

&lt;p&gt;Cmd代表一个正在准备或者在执行中的外部命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Cmd struct {
    // Path是将要执行的命令的路径。
    //
    // 该字段不能为空，如为相对路径会相对于Dir字段。
    Path string
    // Args保管命令的参数，包括命令名作为第一个参数；如果为空切片或者nil，相当于无参数命令。
    //
    // 典型用法下，Path和Args都应被Command函数设定。
    Args []string
    // Env指定进程的环境，如为nil，则是在当前进程的环境下执行。
    Env []string
    // Dir指定命令的工作目录。如为空字符串，会在调用者的进程当前目录下执行。
    Dir string
    // Stdin指定进程的标准输入，如为nil，进程会从空设备读取（os.DevNull）
    Stdin io.Reader
    // Stdout和Stderr指定进程的标准输出和标准错误输出。
    //
    // 如果任一个为nil，Run方法会将对应的文件描述符关联到空设备（os.DevNull）
    //
    // 如果两个字段相同，同一时间最多有一个线程可以写入。
    Stdout io.Writer
    Stderr io.Writer
    // ExtraFiles指定额外被新进程继承的已打开文件流，不包括标准输入、标准输出、标准错误输出。
    // 如果本字段非nil，entry i会变成文件描述符3+i。
    //
    // BUG: 在OS X 10.6系统中，子进程可能会继承不期望的文件描述符。
    // http://golang.org/issue/2603
    ExtraFiles []*os.File
    // SysProcAttr保管可选的、各操作系统特定的sys执行属性。
    // Run方法会将它作为os.ProcAttr的Sys字段传递给os.StartProcess函数。
    SysProcAttr *syscall.SysProcAttr
    // Process是底层的，只执行一次的进程。
    Process *os.Process
    // ProcessState包含一个已经存在的进程的信息，只有在调用Wait或Run后才可用。
    ProcessState *os.ProcessState
    // 内含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用Command来创建cmd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Command(name string, arg ...string) *Cmd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数返回一个*Cmd，用于使用给出的参数执行name指定的程序。返回值只设定了Path和Args两个参数。&lt;/p&gt;

&lt;p&gt;如果name不含路径分隔符，将使用LookPath获取完整路径；否则直接使用name。参数arg不应包含命令名&lt;/p&gt;

&lt;p&gt;使用Run运行cmd命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Run() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run执行c包含的命令，并阻塞直到完成。如果命令成功执行，stdin、stdout、stderr的转交没有问题，并且返回状态码为0，方法的返回值为nil；如果命令没有执行或者执行失败，会返回错误；&lt;/p&gt;

&lt;p&gt;使用Start和wait来运行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Start() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start开始执行c包含的命令，但并不会等待该命令完成即返回。可以配合使用Wait方法来达到和Run一样的效果。wait方法会返回命令的返回状态码并在命令返回后释放相关的资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Wait() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait会阻塞直到该命令执行完成，该命令必须是被Start方法开始执行的。&lt;/p&gt;

&lt;p&gt;通过Run的源码可以看出其实Run方法内部也是调用了Start和Wait方法。Run方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Run() error {
    if err := c.Start(); err != nil {
        return err
    }
    return c.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    cmd := exec.Command(&amp;quot;tr&amp;quot;, &amp;quot;a-z&amp;quot;, &amp;quot;A-Z&amp;quot;)
    cmd.Stdin = strings.NewReader(&amp;quot;abc def&amp;quot;)
    var out bytes.Buffer
    cmd.Stdout = &amp;amp;out
    err := cmd.Run()
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;GOGOGO: %q\n&amp;quot;, out.String())
}

output:

[ `go run test.go` | done: 286.798242ms ]
  GOGOGO: &amp;quot;ABC DEF&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用Output输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Output() ([]byte, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行命令并返回标准输出的切片。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) CombinedOutput() ([]byte, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行命令并返回标准输出和错误输出合并的切片.&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    out, err := exec.Command(&amp;quot;date&amp;quot;).Output()
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;The date is %s\n&amp;quot;, out)
}

output:

[ `go run test.go` | done: 585.495467ms ]
  The date is Tue Aug  1 19:24:11 CST 2017
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用pipe&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Cmd) StdinPipe
func (c *Cmd) StdinPipe() (io.WriteCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StdinPipe方法返回一个在命令Start后与命令标准输入关联的管道。Wait方法获知命令结束后会关闭这个管道。必要时调用者可以调用Close方法来强行关闭管道，例如命令在输入关闭后才会执行返回时需要显式关闭管道。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Cmd) StdoutPipe
func (c *Cmd) StdoutPipe() (io.ReadCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StdoutPipe方法返回一个在命令Start后与命令标准输出关联的管道。Wait方法获知命令结束后会关闭这个管道，一般不需要显式的关闭该管道。但是在从管道读取完全部数据之前调用Wait是错误的；同样使用StdoutPipe方法时调用Run函数也是错误的。例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Cmd) StderrPipe
func (c *Cmd) StderrPipe() (io.ReadCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StderrPipe方法返回一个在命令Start后与命令标准错误输出关联的管道。Wait方法获知命令结束后会关闭这个管道，一般不需要显式的关闭该管道。但是在从管道读取完全部数据之前调用Wait是错误的；同样使用StderrPipe方法时调用Run函数也是错误的。请参照StdoutPipe的例子。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    cmd := exec.Command(&amp;quot;echo&amp;quot;, &amp;quot;-n&amp;quot;, `{&amp;quot;Name&amp;quot;: &amp;quot;Bob&amp;quot;, &amp;quot;Age&amp;quot;: 32}`)
    stdout, err := cmd.StdoutPipe()
    if err != nil {
        log.Fatal(err)
    }
    if err := cmd.Start(); err != nil {
        log.Fatal(err)
    }
    var person struct {
        Name string
        Age  int
    }
    json.NewDecoder(r)
    if err := json.NewDecoder(stdout).Decode(&amp;amp;person); err != nil {
        log.Fatal(err)
    }
    if err := cmd.Wait(); err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;%s is %d years old\n&amp;quot;, person.Name, person.Age)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取命令返回值&lt;/p&gt;

&lt;p&gt;实际上脚本或命令执行完后，会将结果返回到ProcessState中的status去， 但是status不是export的，所以我们需要通过一些手段将脚本返回值从syscall.WaitStatus找出来&lt;/p&gt;

&lt;p&gt;ProcessState定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ProcessState struct {
    pid    int                // The process&#39;s id.
    status syscall.WaitStatus // System-dependent status info.
    rusage *syscall.Rusage
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于上面使用Cmd的例子，可以在进程退出后可以通过以下语句获取到返回值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(&amp;quot;Exit Code&amp;quot;, command.ProcessState.Sys().(syscall.WaitStatus).ExitStatus())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用Process方式的也可以通过对ProcessState通过相同的方式获取到返回结果。&lt;/p&gt;

&lt;h1 id=&#34;os-user&#34;&gt;os/user&lt;/h1&gt;

&lt;p&gt;os/user 模块的主要作用是通过用户名或者 id 从而获取系统用户的相关属性。&lt;/p&gt;

&lt;h2 id=&#34;类型-1&#34;&gt;类型&lt;/h2&gt;

&lt;p&gt;User 结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type User struct {
    Uid      string
    Gid      string
    Username string
    Name     string
    HomeDir  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;User 代表一个用户账户：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Uid ：用户的 ID&lt;/li&gt;
&lt;li&gt;Gid ：用户所属组的 ID，如果属于多个组，那么此 ID 为主组的 ID&lt;/li&gt;
&lt;li&gt;Username ：用户名&lt;/li&gt;
&lt;li&gt;Name ：属组名称，如果属于多个组，那么此名称为主组的名称&lt;/li&gt;
&lt;li&gt;HomeDir ：用户的宿主目录&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;方法&#34;&gt;方法&lt;/h2&gt;

&lt;p&gt;返回当前用户。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Current() (*User, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过用户名查找用户，如果没有找到这个用户那么将返回 UnknownUserError 错误类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Lookup(username string) (*User, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过用户 ID 查找用户，如果没有找到这个用户那么将返回 UnknownUserIdError 错误类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LookupId(uid string) (*User, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os/user&amp;quot;
    &amp;quot;reflect&amp;quot;
)

func main() {
    fmt.Println(&amp;quot;== 测试 Current 正常情况 ==&amp;quot;)
    if u, err := user.Current(); err == nil {
        fmt.Println(&amp;quot;用户ID: &amp;quot; + u.Uid)
        fmt.Println(&amp;quot;主组ID: &amp;quot; + u.Gid)
        fmt.Println(&amp;quot;用户名: &amp;quot; + u.Username)
        fmt.Println(&amp;quot;主组名: &amp;quot; + u.Name)
        fmt.Println(&amp;quot;家目录: &amp;quot; + u.HomeDir)
    }

    fmt.Println(&amp;quot;== 测试 Lookup 正常情况 ==&amp;quot;)
    if u, err := user.Lookup(&amp;quot;root&amp;quot;); err == nil {
        fmt.Println(&amp;quot;用户ID: &amp;quot; + u.Uid)
        fmt.Println(&amp;quot;主组ID: &amp;quot; + u.Gid)
        fmt.Println(&amp;quot;用户名: &amp;quot; + u.Username)
        fmt.Println(&amp;quot;主组名: &amp;quot; + u.Name)
        fmt.Println(&amp;quot;家目录: &amp;quot; + u.HomeDir)
    }
    fmt.Println(&amp;quot;== 测试 Lookup 异常情况 ==&amp;quot;)
    if _, err := user.Lookup(&amp;quot;roo&amp;quot;); err == nil {
    } else {
        fmt.Println(&amp;quot;错误信息: &amp;quot; + err.Error())
        fmt.Print(&amp;quot;错误类型: &amp;quot;)
        fmt.Println(reflect.TypeOf(err))
    }

    fmt.Println(&amp;quot;== 测试 LookupId 正常情况 ==&amp;quot;)
    if u, err := user.LookupId(&amp;quot;0&amp;quot;); err == nil {
        fmt.Println(&amp;quot;用户ID: &amp;quot; + u.Uid)
        fmt.Println(&amp;quot;主组ID: &amp;quot; + u.Gid)
        fmt.Println(&amp;quot;用户名: &amp;quot; + u.Username)
        fmt.Println(&amp;quot;主组名: &amp;quot; + u.Name)
        fmt.Println(&amp;quot;家目录: &amp;quot; + u.HomeDir)
    }
    fmt.Println(&amp;quot;== 测试 LookupId 异常情况 ==&amp;quot;)
    if _, err := user.LookupId(&amp;quot;10000&amp;quot;); err == nil {
    } else {
        fmt.Println(&amp;quot;错误信息: &amp;quot; + err.Error())
        fmt.Print(&amp;quot;错误类型: &amp;quot;)
        fmt.Println(reflect.TypeOf(err))
    }

}
输出结果如下：

== 测试 Current 正常情况 ==
用户ID: 0
主组ID: 0
用户名: root
主组名: root
家目录: /root
== 测试 Lookup 正常情况 ==
用户ID: 0
主组ID: 0
用户名: root
主组名: root
家目录: /root
== 测试 Lookup 异常情况 ==
错误信息: user: unknown user roo
错误类型: user.UnknownUserError
== 测试 LookupId 正常情况 ==
用户ID: 0
主组ID: 0
用户名: root
主组名: root
家目录: /root
== 测试 LookupId 异常情况 ==
错误信息: user: unknown userid 10000
错误类型: user.UnknownUserIdError
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Fmt</title>
          <link>https://kingjcy.github.io/post/golang/go-fmt/</link>
          <pubDate>Mon, 30 May 2016 11:57:05 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-fmt/</guid>
          <description>&lt;p&gt;fmt是实现了格式化的I/O函数，这点类似Ｃ语言中的printf和scanf，但是更加简单。&lt;/p&gt;

&lt;h1 id=&#34;print&#34;&gt;Print&lt;/h1&gt;

&lt;h2 id=&#34;基本函数&#34;&gt;基本函数&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Print&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;// Print 将参数列表 a 中的各个参数转换为字符串并写入到标准输出中。
// 非字符串参数之间会添加空格，返回写入的字节数。
func Print(a ...interface{}) (n int, err error)

// Println 功能类似 Print，只不过最后会添加一个换行符。
// 所有参数之间会添加空格，返回写入的字节数。
func Println(a ...interface{}) (n int, err error)

// Printf 将参数列表 a 填写到格式字符串 format 的占位符中。
// 填写后的结果写入到标准输出中，返回写入的字节数。
func Printf(format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Fprint&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过将转换结果写入到 w 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Fprint(w io.Writer, a ...interface{}) (n int, err error)
func Fprintln(w io.Writer, a ...interface{}) (n int, err error)
func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Sprint&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过将转换结果以字符串形式返回。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Sprint(a ...interface{}) string
func Sprintln(a ...interface{}) string
func Sprintf(format string, a ...interface{}) string
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Errorf&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同 Sprintf，只不过结果字符串被包装成了 error 类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Errorf(format string, a ...interface{}) error
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    fmt.Print(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, 1, 2, 3, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;\n&amp;quot;)
    fmt.Println(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, 1, 2, 3, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
    fmt.Printf(&amp;quot;ab %d %d %d cd\n&amp;quot;, 1, 2, 3)
    // ab1 2 3cd
    // a b 1 2 3 c d
    // ab 1 2 3 cd

    if err := percent(30, 70, 90, 160); err != nil {
        fmt.Println(err)
    }
    // 30%
    // 70%
    // 90%
    // 数值 160 超出范围（100）
}

func percent(i ...int) error {
    for _, n := range i {
        if n &amp;gt; 100 {
            return fmt.Errorf(&amp;quot;数值 %d 超出范围（100）&amp;quot;, n)
        }
        fmt.Print(n, &amp;quot;%\n&amp;quot;)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自定义format类型&#34;&gt;自定义format类型&lt;/h2&gt;

&lt;p&gt;Formatter 由自定义类型实现，用于实现该类型的自定义格式化过程。&lt;/p&gt;

&lt;p&gt;当格式化器需要格式化该类型的变量时，会调用其 Format 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Formatter interface {
    // f 用于获取占位符的旗标、宽度、精度等信息，也用于输出格式化的结果
    // c 是占位符中的动词
    Format(f State, c rune)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由格式化器（Print 之类的函数）实现，用于给自定义格式化过程提供信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type State interface {
    // Formatter 通过 Write 方法将格式化结果写入格式化器中，以便输出。
    Write(b []byte) (ret int, err error)
    // Formatter 通过 Width 方法获取占位符中的宽度信息及其是否被设置。
    Width() (wid int, ok bool)
    // Formatter 通过 Precision 方法获取占位符中的精度信息及其是否被设置。
    Precision() (prec int, ok bool)
    // Formatter 通过 Flag 方法获取占位符中的旗标[+- 0#]是否被设置。
    Flag(c int) bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stringer 由自定义类型实现，用于实现该类型的自定义格式化过程。&lt;/p&gt;

&lt;p&gt;当格式化器需要输出该类型的字符串格式时就会调用其 String 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Stringer interface {
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stringer 由自定义类型实现，用于实现该类型的自定义格式化过程。&lt;/p&gt;

&lt;p&gt;当格式化器需要输出该类型的 Go 语法字符串（%#v）时就会调用其 String 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type GoStringer interface {
    GoString() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Ustr string

func (us Ustr) String() string {
    return strings.ToUpper(string(us))
}

func (us Ustr) GoString() string {
    return `&amp;quot;` + strings.ToUpper(string(us)) + `&amp;quot;`
}

func (u Ustr) Format(f fmt.State, c rune) {
    write := func(s string) {
        f.Write([]byte(s))
    }
    switch c {
    case &#39;m&#39;, &#39;M&#39;:
        write(&amp;quot;旗标：[&amp;quot;)
        for s := &amp;quot;+- 0#&amp;quot;; len(s) &amp;gt; 0; s = s[1:] {
            if f.Flag(int(s[0])) {
                write(s[:1])
            }
        }
        write(&amp;quot;]&amp;quot;)
        if v, ok := f.Width(); ok {
            write(&amp;quot; | 宽度：&amp;quot; + strconv.FormatInt(int64(v), 10))
        }
        if v, ok := f.Precision(); ok {
            write(&amp;quot; | 精度：&amp;quot; + strconv.FormatInt(int64(v), 10))
        }
    case &#39;s&#39;, &#39;v&#39;: // 如果使用 Format 函数，则必须自己处理所有格式，包括 %#v
        if c == &#39;v&#39; &amp;amp;&amp;amp; f.Flag(&#39;#&#39;) {
            write(u.GoString())
        } else {
            write(u.String())
        }
    default: // 如果使用 Format 函数，则必须自己处理默认输出
        write(&amp;quot;无效格式：&amp;quot; + string(c))
    }
}

func main() {
    u := Ustr(&amp;quot;Hello World!&amp;quot;)
    // &amp;quot;-&amp;quot; 标记和 &amp;quot;0&amp;quot; 标记不能同时存在
    fmt.Printf(&amp;quot;%-+ 0#8.5m\n&amp;quot;, u) // 旗标：[+- #] | 宽度：8 | 精度：5
    fmt.Printf(&amp;quot;%+ 0#8.5M\n&amp;quot;, u)  // 旗标：[+ 0#] | 宽度：8 | 精度：5
    fmt.Println(u)                // HELLO WORLD!
    fmt.Printf(&amp;quot;%s\n&amp;quot;, u)         // HELLO WORLD!
    fmt.Printf(&amp;quot;%#v\n&amp;quot;, u)        // &amp;quot;HELLO WORLD!&amp;quot;
    fmt.Printf(&amp;quot;%d\n&amp;quot;, u)         // 无效格式：d
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;scan&#34;&gt;Scan&lt;/h1&gt;

&lt;h2 id=&#34;基本函数-1&#34;&gt;基本函数&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Scan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Scan 从标准输入中读取数据，并将数据用空白分割并解析后存入 a 提供的变量中（换行符会被当作空白处理），变量必须以指针传入。当读到 EOF 或所有变量都填写完毕则停止扫描。返回成功解析的参数数量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Scan(a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scanln 和 Scan 类似，只不过遇到换行符就停止扫描。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Scanln(a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scanf 从标准输入中读取数据，并根据格式字符串 format 对数据进行解析，将解析结果存入参数 a 所提供的变量中，变量必须以指针传入。输入端的换行符必须和 format 中的换行符相对应（如果格式字符串中有换行符，则输入端必须输入相应的换行符）。占位符 %c 总是匹配下一个字符，包括空白，比如空格符、制表符、换行符。返回成功解析的参数数量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Scanf(format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Fscan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过从 r 中读取数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Fscan(r io.Reader, a ...interface{}) (n int, err error)
func Fscanln(r io.Reader, a ...interface{}) (n int, err error)
func Fscanf(r io.Reader, format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Sscan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过从 str 中读取数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Sscan(str string, a ...interface{}) (n int, err error)
func Sscanln(str string, a ...interface{}) (n int, err error)
func Sscanf(str string, format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;// 对于 Scan 而言，回车视为空白
func main() {
    a, b, c := &amp;quot;&amp;quot;, 0, false
    fmt.Scan(&amp;amp;a, &amp;amp;b, &amp;amp;c)
    fmt.Println(a, b, c)
    // 在终端执行后，输入 abc 1 回车 true 回车
    // 结果 abc 1 true
}

// 对于 Scanln 而言，回车结束扫描
func main() {
    a, b, c := &amp;quot;&amp;quot;, 0, false
    fmt.Scanln(&amp;amp;a, &amp;amp;b, &amp;amp;c)
    fmt.Println(a, b, c)
    // 在终端执行后，输入 abc 1 true 回车
    // 结果 abc 1 true
}

// 格式字符串可以指定宽度
func main() {
    a, b, c := &amp;quot;&amp;quot;, 0, false
    fmt.Scanf(&amp;quot;%4s%d%t&amp;quot;, &amp;amp;a, &amp;amp;b, &amp;amp;c)
    fmt.Println(a, b, c)
    // 在终端执行后，输入 1234567true 回车
    // 结果 1234 567 true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从键盘和标准输入 os.Stdin 读取输入，最简单的办法是使用 fmt 包提供的 Scan 和 Sscan 开头的函数。&lt;/p&gt;

&lt;p&gt;从控制台读取输入:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import &amp;quot;fmt&amp;quot;

var (
   firstName, lastName, s string
   i int
   f float32
   input = &amp;quot;56.12 / 5212 / Go&amp;quot;
   format = &amp;quot;%f / %d / %s&amp;quot;
)

func main() {
   fmt.Println(&amp;quot;Please enter your full name: &amp;quot;)
   fmt.Scanln(&amp;amp;firstName, &amp;amp;lastName)
   // fmt.Scanf(&amp;quot;%s %s&amp;quot;, &amp;amp;firstName, &amp;amp;lastName)
   fmt.Printf(&amp;quot;Hi %s %s!\n&amp;quot;, firstName, lastName) // Hi Chris Naegels
   fmt.Sscanf(input, format, &amp;amp;f, &amp;amp;i, &amp;amp;s)
   fmt.Println(&amp;quot;From the string we read: &amp;quot;, f, i, s)
    // 输出结果: From the string we read: 56.12 5212 Go
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scanln 扫描来自标准输入的文本，将空格分隔的值依次存放到后续的参数内，直到碰到换行。&lt;/p&gt;

&lt;p&gt;Scanf 与其类似，除了 Scanf 的第一个参数用作格式字符串，用来决定如何读取。&lt;/p&gt;

&lt;p&gt;Sscan 和以 Sscan 开头的函数则是从字符串读取，除此之外，与 Scanf相同。如果这些函数读取到的结果与您预想的不同，您可以检查成功读入数据的个数和返回的错误。&lt;/p&gt;

&lt;p&gt;也可以使用 bufio 包提供的缓冲读取（buffered reader）来读取数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;bufio&amp;quot;
    &amp;quot;os&amp;quot;
)

var inputReader *bufio.Reader
var input string
var err error

func main() {
    inputReader = bufio.NewReader(os.Stdin)
    fmt.Println(&amp;quot;Please enter some input: &amp;quot;)
    input, err = inputReader.ReadString(&#39;\n&#39;)
    if err == nil {
        fmt.Printf(&amp;quot;The input was: %s\n&amp;quot;, input)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自定义format类型-1&#34;&gt;自定义format类型&lt;/h2&gt;

&lt;p&gt;Scanner 由自定义类型实现，用于实现该类型的自定义扫描过程。&lt;/p&gt;

&lt;p&gt;当扫描器需要解析该类型的数据时，会调用其 Scan 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Scanner interface {
    // state 用于获取占位符中的宽度信息，也用于从扫描器中读取数据进行解析。
    // verb 是占位符中的动词
    Scan(state ScanState, verb rune) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由扫描器（Scan 之类的函数）实现，用于给自定义扫描过程提供数据和信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ScanState interface {
    // ReadRune 从扫描器中读取一个字符，如果用在 Scanln 类的扫描器中，
    // 则该方法会在读到第一个换行符之后或读到指定宽度之后返回 EOF。
    // 返回“读取的字符”和“字符编码所占用的字节数”
    ReadRune() (r rune, size int, err error)
    // UnreadRune 撤消最后一次的 ReadRune 操作，
    // 使下次的 ReadRune 操作得到与前一次 ReadRune 相同的结果。
    UnreadRune() error
    // SkipSpace 为 Scan 方法提供跳过开头空白的能力。
    // 根据扫描器的不同（Scan 或 Scanln）决定是否跳过换行符。
    SkipSpace()
    // Token 用于从扫描器中读取符合要求的字符串，
    // Token 从扫描器中读取连续的符合 f(c) 的字符 c，准备解析。
    // 如果 f 为 nil，则使用 !unicode.IsSpace(c) 代替 f(c)。
    // skipSpace：是否跳过开头的连续空白。返回读取到的数据。
    // 注意：token 指向共享的数据，下次的 Token 操作可能会覆盖本次的结果。
    Token(skipSpace bool, f func(rune) bool) (token []byte, err error)
    // Width 返回占位符中的宽度值以及宽度值是否被设置
    Width() (wid int, ok bool)
    // 因为上面实现了 ReadRune 方法，所以 Read 方法永远不应该被调用。
    // 一个好的 ScanState 应该让 Read 直接返回相应的错误信息。
    Read(buf []byte) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Ustr string

func (u *Ustr) Scan(state fmt.ScanState, verb rune) (err error) {
    var s []byte
    switch verb {
    case &#39;S&#39;:
        s, err = state.Token(true, func(c rune) bool { return &#39;A&#39; &amp;lt;= c &amp;amp;&amp;amp; c &amp;lt;= &#39;Z&#39; })
        if err != nil {
            return
        }
    case &#39;s&#39;, &#39;v&#39;:
        s, err = state.Token(true, func(c rune) bool { return &#39;a&#39; &amp;lt;= c &amp;amp;&amp;amp; c &amp;lt;= &#39;z&#39; })
        if err != nil {
            return
        }
    default:
        return fmt.Errorf(&amp;quot;无效格式：%c&amp;quot;, verb)
    }
    *u = Ustr(s)
    return nil
}

func main() {
    var a, b, c, d, e Ustr
    n, err := fmt.Scanf(&amp;quot;%3S%S%3s%2v%x&amp;quot;, &amp;amp;a, &amp;amp;b, &amp;amp;c, &amp;amp;d, &amp;amp;e)
    fmt.Println(a, b, c, d, e)
    fmt.Println(n, err)
    // 在终端执行后，输入 ABCDEFGabcdefg 回车
    // 结果：
    // ABC DEFG abc de
    // 4 无效格式：x
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- flag</title>
          <link>https://kingjcy.github.io/post/golang/go-flag/</link>
          <pubDate>Sun, 29 May 2016 10:09:03 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-flag/</guid>
          <description>&lt;p&gt;golang自身带的命令行包flag，各种case，有代码洁癖的人看着就令人头大,我们一般使用其他的命令行解析包比如pflag，cobra等，cobra是个非常不错的命令行包(golang命令行解析库)，docker，hugo都在使用.&lt;/p&gt;

&lt;h1 id=&#34;cobra&#34;&gt;cobra&lt;/h1&gt;

&lt;h2 id=&#34;基命令&#34;&gt;基命令&lt;/h2&gt;

&lt;p&gt;首先创建一个基命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package cmd

import (
    &amp;quot;github.com/spf13/cobra&amp;quot;
)

var RootCmd = &amp;amp;cobra.Command{
    Use: &amp;quot;gonne&amp;quot;,
    Run: func(cmd *cobra.Command, args []string) {
        println(&amp;quot;gonne is my ai friend&amp;quot;)
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用命令&lt;/p&gt;

&lt;p&gt;在main方法中调用命令，恩，就这么简单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os&amp;quot;

    &amp;quot;lastsweetop.com/cmd&amp;quot;
)

func main() {
    if err := cmd.RootCmd.Execute(); err != nil {
        fmt.Println(err)
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在命令行输入 gonne，就会执行基命令中Run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appletekiMacBook-Pro:src apple$ gonne
gonne is my ai friend
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;子命令&#34;&gt;子命令&lt;/h2&gt;

&lt;p&gt;在基命令上增加子命令也相当简单，根本无需在基命令和main方法中写任何代码，只需新建一个go文件，多个子命令间也是相互独立的，多么优雅的代码，告别各种case&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package cmd

import &amp;quot;github.com/spf13/cobra&amp;quot;

func init() {
    RootCmd.AddCommand(versionCmd)
}

var versionCmd = &amp;amp;cobra.Command{
    Use:   &amp;quot;version&amp;quot;,
    Short: &amp;quot;Print the version number of Gonne&amp;quot;,
    Run: func(cmd *cobra.Command, args []string) {
        println(&amp;quot;gonne version is 0.0.1&amp;quot;)
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appletekiMacBook-Pro:src apple$ gonne version
gonne version is 0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;启动命令&#34;&gt;启动命令&lt;/h2&gt;

&lt;p&gt;我们先来个非后台运行的启动命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    startCmd := &amp;amp;cobra.Command{
        Use:   &amp;quot;start&amp;quot;,
        Short: &amp;quot;Start Gonne&amp;quot;,
        Run: func(cmd *cobra.Command, args []string) {
            startHttp()
        },
    }
    startCmd.Flags().BoolVarP(&amp;amp;daemon, &amp;quot;deamon&amp;quot;, &amp;quot;d&amp;quot;, false, &amp;quot;is daemon?&amp;quot;)
    RootCmd.AddCommand(startCmd)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;startHttp方法启动一个http的web服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func startHttp() {
    http.HandleFunc(&amp;quot;/&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
        fmt.Fprintf(w, &amp;quot;Hello cmd!&amp;quot;)
    })
    if err := http.ListenAndServe(&amp;quot;:9090&amp;quot;, nil); err != nil {
        log.Fatal(&amp;quot;ListenAndServe: &amp;quot;, err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在通过gonne start便可以启动一个web服务了，但是程序停留在命令行，如果ctrl+C程序也会终止了&lt;/p&gt;

&lt;h2 id=&#34;命令行参数&#34;&gt;命令行参数&lt;/h2&gt;

&lt;p&gt;如果想要后台启动，那么得让start命令知道是要后台运行的，参照docker命令行的方式就是加上-d，给一个命令添加参数的判断只需很少的代码&lt;/p&gt;

&lt;p&gt;改造一下代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    var daemon bool
    startCmd := &amp;amp;cobra.Command{
        Use:   &amp;quot;start&amp;quot;,
        Short: &amp;quot;Start Gonne&amp;quot;,
        Run: func(cmd *cobra.Command, args []string) {
            if daemon {
        fmt.Println(&amp;quot;gonne start&amp;quot;,daemon)        
            }
            startHttp()
        },
    }
    startCmd.Flags().BoolVarP(&amp;amp;daemon, &amp;quot;deamon&amp;quot;, &amp;quot;d&amp;quot;, false, &amp;quot;is daemon?&amp;quot;)
    RootCmd.AddCommand(startCmd)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;命令行输入&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gonne start -d
1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以接收到-d参数了，这里要说明一下，第一个参数取值，第二个参数代码&amp;ndash;deamon，第三个参数代表-d ,第四个参数代码不加-d时候的默认值，第五参数是描述&lt;/p&gt;

&lt;h2 id=&#34;后台运行&#34;&gt;后台运行&lt;/h2&gt;

&lt;p&gt;后台运行其实这里使用的是一个巧妙的方法，就是使用系统的command命令行启动自己的命令行输入，是不是有点绕，再看看看改造后的代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Run: func(cmd *cobra.Command, args []string) {
  if daemon {
    command := exec.Command(&amp;quot;gonne&amp;quot;, &amp;quot;start&amp;quot;)
    command.Start()
    fmt.Printf(&amp;quot;gonne start, [PID] %d running...\n&amp;quot;, command.Process.Pid)
    ioutil.WriteFile(&amp;quot;gonne.lock&amp;quot;, []byte(fmt.Sprintf(&amp;quot;%d&amp;quot;, command.Process.Pid)), 0666)
    daemon = false
    os.Exit(0)
  } else {
    fmt.Println(&amp;quot;gonne start&amp;quot;)
  }
  startHttp()
},
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用exec的Command启动刚输入的gonne start -d，就会拦截到这条请求然后通过gonne start,但是程序就不会停留在命令行了，然后发现http服务还在，还可以访问。&lt;/p&gt;

&lt;p&gt;还有一点就是把pid输出到gonne.lock文件，给停止的程序调用&lt;/p&gt;

&lt;h2 id=&#34;终止后台程序&#34;&gt;终止后台程序&lt;/h2&gt;

&lt;p&gt;有了之前的操作后，停止就简单多了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    RootCmd.AddCommand(stopCmd)
}

var stopCmd = &amp;amp;cobra.Command{
    Use:   &amp;quot;stop&amp;quot;,
    Short: &amp;quot;Stop Gonne&amp;quot;,
    Run: func(cmd *cobra.Command, args []string) {
        strb, _ := ioutil.ReadFile(&amp;quot;gonne.lock&amp;quot;)
        command := exec.Command(&amp;quot;kill&amp;quot;, string(strb))
        command.Start()
        println(&amp;quot;gonne stop&amp;quot;)
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行 gonne stop 即可终止之前启动的http服务&lt;/p&gt;

&lt;h2 id=&#34;help命令&#34;&gt;help命令&lt;/h2&gt;

&lt;p&gt;好了，关于命令的操作讲完了，再看看cobra给的福利，自动生成的help命令&lt;/p&gt;

&lt;p&gt;这个不需要你做什么操作，只需要输入gonne help,相关信息已经帮你生产好了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appletekiMacBook-Pro:andev apple$ gonne help
Usage:
  gonne [flags]
  gonne [command]

Available Commands:
  help        Help about any command
  start       Start Gonne
  stop        Stop Gonne
  version     Print the version number of Gonne

Flags:
  -h, --help   help for gonne

Use &amp;quot;gonne [command] --help&amp;quot; for more information about a command.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，子命令也有&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appletekiMacBook-Pro:andev apple$ gonne start -h
Start Gonne

Usage:
  gonne start [flags]

Flags:
  -d, --deamon   is daemon?
  -h, --help     help for start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;自此告别各种脚本&lt;/p&gt;

&lt;h1 id=&#34;flag&#34;&gt;flag&lt;/h1&gt;

&lt;p&gt;golang自带的一个解析命令行参数的方法或库，是经常用的。&lt;/p&gt;

&lt;h2 id=&#34;结构体和默认实例&#34;&gt;结构体和默认实例&lt;/h2&gt;

&lt;p&gt;首先&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type FlagSet struct {
    // Usage is the function called when an error occurs while parsing flags.
    // The field is a function (not a method) that may be changed to point to
    // a custom error handler. What happens after Usage is called depends
    // on the ErrorHandling setting; for the command line, this defaults
    // to ExitOnError, which exits the program after calling Usage.
    Usage func()

    name          string
    parsed        bool
    actual        map[string]*Flag
    formal        map[string]*Flag
    args          []string // arguments after flags
    errorHandling ErrorHandling
    output        io.Writer // nil means stderr; use out() accessor
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个flag的集合&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Flag struct {
    Name     string // flag在命令行中的名字
    Usage    string // 帮助信息
    Value    Value  // 要设置的值
    DefValue string // 默认值（文本格式），用于使用信息
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;flag库中使用了CommandLine对flagset进行了初始化，默认传入参数是启动文件名&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var CommandLine = NewFlagSet(os.Args[0], ExitOnError)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个flagset的指针。&lt;/p&gt;

&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;

&lt;p&gt;定义 flags 有三种方式&lt;/p&gt;

&lt;p&gt;1）flag.Xxx()，其中 Xxx 可以是 Int、String 等；返回一个相应类型的指针，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ip = flag.Int(&amp;quot;flagname&amp;quot;, 1234, &amp;quot;help message for flagname&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种模式其实是对第二种模式的一种封装，是一种最顶层的使用，其实是使用默认的CommandLine实例调用下面这种方式声明的XxxVar（）函数。对应的第一个变量直接new一个，其实这个是用于存储默认值的。&lt;/p&gt;

&lt;p&gt;2）flag.XxxVar()，将 flag 绑定到一个变量上，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var flagvar int
flag.IntVar(&amp;amp;flagvar, &amp;quot;flagname&amp;quot;, 1234, &amp;quot;help message for flagname&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种模式其实是对自定义的一种封装，其实就是调用了var（）函数。&lt;/p&gt;

&lt;p&gt;3）还可以创建自定义 flag，只要实现 flag.Value 接口即可（要求 receiver 是指针），这时候可以通过如下方式定义该 flag：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flag.Var(&amp;amp;flagVal, &amp;quot;name&amp;quot;, &amp;quot;help message for flagname&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边才是真正的实现，其实就是将启动参数按着flag的结构体存储到flagset种的formal这个map中去，最后给parse去解析。第一个参数是一个value的接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Value interface {
    String() string
    Set(string) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用于接收定义类型的结构体对象指针，最终找到对应方法的实现。&lt;/p&gt;

&lt;p&gt;自定义这个可以举个例子加强理解&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type sliceValue []string

func newSliceValue(vals []string, p *[]string) *sliceValue {
    *p = vals
    return (*sliceValue)(p)
}

func (s *sliceValue) Set(val string) error {
    *s = sliceValue(strings.Split(val, &amp;quot;,&amp;quot;))
    return nil
}

func (s *sliceValue) Get() interface{} { return []string(*s) }

func (s *sliceValue) String() string { return strings.Join([]string(*s), &amp;quot;,&amp;quot;) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后可以这么使用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var languages []string
flag.Var(newSliceValue([]string{}, &amp;amp;languages), &amp;quot;slice&amp;quot;, &amp;quot;I like programming `languages`&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样通过 -slice &amp;ldquo;go,php&amp;rdquo; 这样的形式传递参数，languages 得到的就是 [go, php]。&lt;/p&gt;

&lt;p&gt;flag 中对 Duration 这种非基本类型的支持，使用的就是类似这样的方式。&lt;/p&gt;

&lt;h2 id=&#34;解析&#34;&gt;解析&lt;/h2&gt;

&lt;p&gt;在所有的 flag 定义完成之后，可以通过调用 flag.Parse() 进行解析。&lt;/p&gt;

&lt;p&gt;命令行 flag 的语法有如下三种形式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-flag // 只支持bool类型
-flag=x
-flag x // 只支持非bool类型
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;flag&amp;quot;
    &amp;quot;fmt&amp;quot;
)

var inputName = flag.String(&amp;quot;name&amp;quot;, &amp;quot;CHENJIAN&amp;quot;, &amp;quot;Input Your Name.&amp;quot;)
var inputAge = flag.Int(&amp;quot;age&amp;quot;, 27, &amp;quot;Input Your Age&amp;quot;)
var inputGender = flag.String(&amp;quot;gender&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;Input Your Gender&amp;quot;)
var inputFlagvar int

func Init() {
    flag.IntVar(&amp;amp;inputFlagvar, &amp;quot;flagname&amp;quot;, 1234, &amp;quot;Help&amp;quot;)
}
func main() {
    Init()
    flag.Parse()
    // func Args() []string
    // Args returns the non-flag command-line arguments.
    // func NArg() int
    // NArg is the number of arguments remaining after flags have been processed.
    fmt.Printf(&amp;quot;args=%s, num=%d\n&amp;quot;, flag.Args(), flag.NArg())
    for i := 0; i != flag.NArg(); i++ {
        fmt.Printf(&amp;quot;arg[%d]=%s\n&amp;quot;, i, flag.Arg(i))
    }
    fmt.Println(&amp;quot;name=&amp;quot;, *inputName)
    fmt.Println(&amp;quot;age=&amp;quot;, *inputAge)
    fmt.Println(&amp;quot;gender=&amp;quot;, *inputGender)
    fmt.Println(&amp;quot;flagname=&amp;quot;, inputFlagvar)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;操作:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go build example_flag.go

./example_flag -h

&amp;lt;&amp;lt;&#39;COMMENT&#39;
Usage of ./exampleFlag:
  -age int
        Input Your Age (default 27)
  -flagname int
        Help (default 1234)
  -gender string
        Input Your Gender (default &amp;quot;female&amp;quot;)
  -name string
        Input Your Name. (default &amp;quot;CHENJIAN&amp;quot;)
COMMENT

 ./example_flag chenjian

 &amp;lt;&amp;lt;&#39;COMMENT&#39;
args=[chenjian], num=1
arg[0]=chenjian
name= CHENJIAN
age= 27
gender= female
flagname= 1234
COMMENT

./example_flag -name balbalba -age 1111 -flagname=12333 dfdf xccccc eette

 &amp;lt;&amp;lt;&#39;COMMENT&#39;
args=[dfdf xccccc eette], num=3
arg[0]=dfdf
arg[1]=xccccc
arg[2]=eette
name= balbalba
age= 1111
gender= female
flagname= 12333
COMMENT
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;kingpin&#34;&gt;kingpin&lt;/h1&gt;

&lt;p&gt;功能比flag库强大，用法差不多，相比flag库，最重要的一点就是支持不加&amp;rdquo;-&amp;ldquo;的调用。&lt;/p&gt;

&lt;p&gt;下面实例就说明了大部分的用法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;os&amp;quot;
    &amp;quot;strings&amp;quot;
    &amp;quot;gopkg.in/alecthomas/kingpin.v2&amp;quot;
)
var (
    app          = kingpin.New(&amp;quot;chat&amp;quot;, &amp;quot;A command-line chat application.&amp;quot;)
    //bool类型参数，可以通过 --debug使该值为true
    debug        = app.Flag(&amp;quot;debug&amp;quot;, &amp;quot;Enable debug mode.&amp;quot;).Bool()
    //识别 ./cli register
    register     = app.Command(&amp;quot;register&amp;quot;, &amp;quot;Register a new user.&amp;quot;)
    // ./cli register之后的参数，可通过./cli register gggle 123456 传入name为gggle pwd为123456 参数类型为字符串
    registerName = register.Arg(&amp;quot;name&amp;quot;, &amp;quot;Name for user.&amp;quot;).Required().String()
    registerPwd  = register.Arg(&amp;quot;pwd&amp;quot;, &amp;quot;pwd of user.&amp;quot;).Required().String()
    //识别 ./cli post
    post         = app.Command(&amp;quot;post&amp;quot;, &amp;quot;Post a message to a channel.&amp;quot;)
    //可以通过 ./cli post --image file  或者 ./cli post -i file 传入文件
    postImage    = post.Flag(&amp;quot;image&amp;quot;, &amp;quot;Image to post.&amp;quot;).Short(&#39;i&#39;).String()
    //可以通过./cli post txt 传入字符串，有默认值&amp;quot;hello world&amp;quot;
    postText     = post.Arg(&amp;quot;text&amp;quot;, &amp;quot;Text to post.&amp;quot;).Default(&amp;quot;hello world&amp;quot;).Strings()
)
func main() {
    //从os接收参数传给kingpin处理
    switch kingpin.MustParse(app.Parse(os.Args[1:])) {
    case register.FullCommand():
        println(&amp;quot;name:&amp;quot; + *registerName)
        println(&amp;quot;pwd:&amp;quot; + *registerPwd)
    case post.FullCommand():
        println((*postImage))
        text := strings.Join(*postText, &amp;quot; &amp;quot;)
        println(&amp;quot;Post:&amp;quot;, text)
    }
    if *debug == true {
        println(&amp;quot;debug&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;pflag&#34;&gt;Pflag&lt;/h1&gt;

&lt;p&gt;Docker源码中使用了Pflag，安装spf13/pflag&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/spf13/pflag
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;p&gt;基本的使用和“flag包”基本相同&lt;/p&gt;

&lt;p&gt;新增:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;添加shorthand参数
// func IntP(name, shorthand string, value int, usage string) *int
// IntP is like Int, but accepts a shorthand letter that can be used after a single dash.
var ip= flag.IntP(&amp;quot;flagname&amp;quot;, &amp;quot;f&amp;quot;, 1234, &amp;quot;help message&amp;quot;)
设置非必须选项的默认值
var ip = flag.IntP(&amp;quot;flagname&amp;quot;, &amp;quot;f&amp;quot;, 1234, &amp;quot;help message&amp;quot;)
flag.Lookup(&amp;quot;flagname&amp;quot;).NoOptDefVal = &amp;quot;4321&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果如下图:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Parsed Arguments    Resulting Value
–flagname=1357  ip=1357
–flagname   ip=4321
[nothing]   ip=1234
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;命令行语法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--flag    // 布尔flags, 或者非必须选项默认值
--flag x  // 只对于没有默认值的flags
--flag=x
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;flag定制化&#34;&gt;flag定制化&lt;/h2&gt;

&lt;p&gt;例如希望使用“-”，“_”或者“.”，像&amp;ndash;my-flag == &amp;ndash;my_flag == &amp;ndash;my.flag:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func wordSepNormalizeFunc(f *pflag.FlagSet, name string) pflag.NormalizedName {
    from := []string{&amp;quot;-&amp;quot;, &amp;quot;_&amp;quot;}
    to := &amp;quot;.&amp;quot;
    for _, sep := range from {
        name = strings.Replace(name, sep, to, -1)
    }
    return pflag.NormalizedName(name)
}

myFlagSet.SetNormalizeFunc(wordSepNormalizeFunc)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如希望联合两个参数,像&amp;ndash;old-flag-name == &amp;ndash;new-flag-name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func aliasNormalizeFunc(f *pflag.FlagSet, name string) pflag.NormalizedName {
    switch name {
    case &amp;quot;old-flag-name&amp;quot;:
        name = &amp;quot;new-flag-name&amp;quot;
        break
    }
    return pflag.NormalizedName(name)
}

myFlagSet.SetNormalizeFunc(aliasNormalizeFunc)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;弃用flag或者它的shothand&lt;/p&gt;

&lt;p&gt;例如希望弃用名叫badflag参数，并告知开发者使用代替参数:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// deprecate a flag by specifying its name and a usage message
flags.MarkDeprecated(&amp;quot;badflag&amp;quot;, &amp;quot;please use --good-flag instead&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从而当使用badflag时，会提示Flag &amp;ndash;badflag has been deprecated, please use &amp;ndash;good-flag instead&lt;/p&gt;

&lt;p&gt;例如希望保持使用noshorthandflag，但想弃用简称n:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// deprecate a flag shorthand by specifying its flag name and a usage message
flags.MarkShorthandDeprecated(&amp;quot;noshorthandflag&amp;quot;, &amp;quot;please use --noshorthandflag only&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从而当使用n时，会提示Flag shorthand -n has been deprecated, please use &amp;ndash;noshorthandflag only&lt;/p&gt;

&lt;p&gt;隐藏flag&lt;/p&gt;

&lt;p&gt;例如希望保持使用secretFlag参数，但在help文档中隐藏这个参数的说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// hide a flag by specifying its name
flags.MarkHidden(&amp;quot;secretFlag&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关闭flags的排序&lt;/p&gt;

&lt;p&gt;例如希望关闭对help文档或使用说明的flag排序：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flags.BoolP(&amp;quot;verbose&amp;quot;, &amp;quot;v&amp;quot;, false, &amp;quot;verbose output&amp;quot;)
flags.String(&amp;quot;coolflag&amp;quot;, &amp;quot;yeaah&amp;quot;, &amp;quot;it&#39;s really cool flag&amp;quot;)
flags.Int(&amp;quot;usefulflag&amp;quot;, 777, &amp;quot;sometimes it&#39;s very useful&amp;quot;)
flags.SortFlags = false
flags.PrintDefaults()
输出：

-v, --verbose           verbose output
    --coolflag string   it&#39;s really cool flag (default &amp;quot;yeaah&amp;quot;)
    --usefulflag int    sometimes it&#39;s very useful (default 777)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;同时使用flag包和pflag包&#34;&gt;同时使用flag包和pflag包&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import (
    goflag &amp;quot;flag&amp;quot;
    flag &amp;quot;github.com/spf13/pflag&amp;quot;
)

var ip *int = flag.Int(&amp;quot;flagname&amp;quot;, 1234, &amp;quot;help message for flagname&amp;quot;)

func main() {
    flag.CommandLine.AddGoFlagSet(goflag.CommandLine)
    flag.Parse()
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Clinet</title>
          <link>https://kingjcy.github.io/post/golang/go-clinet/</link>
          <pubDate>Sun, 24 Apr 2016 14:50:29 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-clinet/</guid>
          <description>&lt;p&gt;go命令行工具是我们在写代码中常用的，我们这边做一个简单的整理。&lt;/p&gt;

&lt;h1 id=&#34;go-build&#34;&gt;go build&lt;/h1&gt;

&lt;p&gt;go build 命令主要是用于测试编译。在包的编译过程中，若有必要，会同时编译与之相关联的包。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;如果是普通包，当你执行go build命令后，不会产生任何文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果是main包，当只执行go build命令后，会在当前目录下生成一个可执行文件。如果需要在$GOPATH/bin木下生成相应的可执行文件，需要执行go install 或者使用 go build -o 路径/可执行文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果某个文件夹下有多个文件，而你只想编译其中某一个文件，可以在 go build 之后加上文件名，例如 go build a.go；go build 命令默认会编译当前目录下的所有go文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;你也可以指定编译输出的文件名。比如，我们可以指定go build -o myapp，默认情况是你的package名(非main包)，或者是第一个源文件的文件名(main包)。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;go build 会忽略目录下以”_”或者”.”开头的go文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;go build的时候会选择性地编译以系统名结尾的文件（Linux、Darwin、Windows、Freebsd）。例如linux系统下面编译只会选择array_linux.go文件，其它系统命名后缀文件全部忽略。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;go build -x -v  打印编译过程&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;golang中包的理解&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;定义：关键字 package XXXX&lt;/p&gt;

&lt;p&gt;我们知道一个非main包在编译后会生成一个.a文件（在临时目录下生成，除非使用go install安装到$GOROOT或$GOPATH下，否则你看不到.a），用于后续可执行程序链接使用。&lt;/p&gt;

&lt;p&gt;Go标准库中的包对应的源码部分路径在：$GOROOT/src，而标准库中包编译后的.a文件路径在$GOROOT/pkg/darwin_amd64下。&lt;/p&gt;

&lt;p&gt;执行go install libproj1/foo，Go编译器编译foo包，并将foo.a安装到$GOPATH/pkg/darwin_amd64/libproj1下&amp;mdash;不用先go build然后在go install&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;因此我们要依赖第三方包，就必须搞到第三方包的源码，这也是Golang包管理的一个特点。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编译main包时，编译器到底用的是.a还是源码？&amp;mdash;-在使用第三方包的时候，当源码和.a均已安装的情况下，&lt;strong&gt;编译器链接的是源码。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;最根本的是链接的是链接了以该最新源码编译的临时目录下的.a文件，而不是pkg下面的.a文件。&amp;mdash;如果想依赖pkg下面的.a文件，那只能分布编译了，把6l链接时的-L $WORK 去掉，才会找到pkg下面（具体参考-X -V参数的编译原理）&lt;/p&gt;

&lt;p&gt;标志库也是依赖源码编译产生的临时目录下的.a文件，但是当标准库的源码发生变化时，编译器不会尝试重新编译&amp;ndash;但是第三方库发生变化时，会重新编译生成临时文件，然后连接&lt;/p&gt;

&lt;p&gt;临时文件不是一直存在的，只是在编译的时候产生&lt;/p&gt;

&lt;p&gt;import后面的是路劲名还是包名？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;import后面的最后一个元素应该是路径，就是目录，并非包名。而调用的函数的那个是包名，所以源码路劲一定要存在，不然就can not find&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;引用包
import 文件所在的目录路劲（除去$GOPATH/src）
同一个目录下不能定义不同的package&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import m &amp;quot;lib/math&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;import语句用m替代lib/math  m指代的是lib/math路径下唯一的那个包&amp;ndash;一定是唯一，不然报错&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;交叉编译&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;mac和linux环境编译不兼容，不能互相运行。可以跨平台编译。&lt;/p&gt;

&lt;p&gt;Golang 支持交叉编译，在一个平台上生成另一个平台的可执行程序，最近使用了一下，非常好用，这里备忘一下。&lt;/p&gt;

&lt;p&gt;Mac 下编译 Linux 和 Windows 64位可执行程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build main.go
CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build main.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Linux 下编译 Mac 和 Windows 64位可执行程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CGO_ENABLED=0 GOOS=darwin GOARCH=amd64 go build main.go
CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build main.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Windows 下编译 Mac 和 Linux 64位可执行程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SET CGO_ENABLED=0
SET GOOS=darwin
SET GOARCH=amd64
go build main.go

SET CGO_ENABLED=0
SET GOOS=linux
SET GOARCH=amd64
go build main.go
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;GOOS：目标平台的操作系统（darwin、freebsd、linux、windows）&lt;/li&gt;
&lt;li&gt;GOARCH：目标平台的体系架构（386、amd64、arm）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;交叉编译不支持 CGO 所以要禁用它&lt;/p&gt;

&lt;p&gt;上面的命令编译 64 位可执行程序，你当然应该也会使用 386 编译 32 位可执行程序
很多博客都提到要先增加对其它平台的支持，但是我跳过那一步，上面所列的命令也都能成功，且得到我想要的结果，可见那一步应该是非必须的，或是我所使用的 Go 版本已默认支持所有平台。&lt;/p&gt;

&lt;h1 id=&#34;go-install&#34;&gt;go install&lt;/h1&gt;

&lt;p&gt;go install 命令在内部实际上分成了两步操作：&lt;/p&gt;

&lt;p&gt;第一步是生成结果文件(可执行文件或者.a包)&lt;/p&gt;

&lt;p&gt;第二步会把编译好的结果移到 $GOPATH/pkg 或者 $GOPATH/bin。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可执行文件： 一般是 go install 带main函数的go文件产生的，有函数入口，所有可以直接运行。&lt;/li&gt;
&lt;li&gt;.a应用包： 一般是 go install 不包含main函数的go文件产生的，没有函数入口，只能被调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;静态库的使用&#34;&gt;静态库的使用&lt;/h2&gt;

&lt;p&gt;编译静态库demo.a&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go install demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在命令行运行go install demo命令，会在%GOPATH%目录下生相应的静态库文件demo.a（windows平台一般在%GOPATH%\src\pkg\windows_amd64目录）。&lt;/p&gt;

&lt;p&gt;编译main.go&lt;/p&gt;

&lt;p&gt;进入main.go所在目录，编译main.go：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go tool compile -I E:\share\git\go_practice\pkg\windows_amd64 main.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;-I选项指定了demo包的安装路径，供main.go导入使用，即E:\share\git\go_practice\pkg\windows_amd64目录，编译成功后会生成相应的目标文件main.o。&lt;/p&gt;

&lt;p&gt;链接main.o&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go tool link -o main.exe -L E:\share\git\go_practice\pkg\windows_amd64 main.o
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;-L选项指定了静态库demo.a的路径，即E:\share\git\go_practice\pkg\windows_amd64目录，链接成功后会生成相应的可执行文件main.exe。&lt;/p&gt;

&lt;p&gt;运行main.exe&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;main.exe
call demo ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在，就算把demo目录删除，再次编译链接main.go，也能正确生成main.exe:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go tool compile -I E:\share\git\go_practice\pkg\windows_amd64 main.go
go tool link -o main.exe -L E:\share\git\go_practice\pkg\windows_amd64 main.o
main.exe
call demo ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是，如果删除了静态库demo.a，就不能编译main.go，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go tool compile -I E:\share\git\go_practice\pkg\windows_amd64 main.go
main.go:3: can&#39;t find import: &amp;quot;demo&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上就是go语言静态库的编译和使用方法.&lt;/p&gt;

&lt;h2 id=&#34;动态库&#34;&gt;动态库&lt;/h2&gt;

&lt;p&gt;将go语言标准库编译成动态库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go install -buildmode=shared -linkshared  std
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在命令行运行go install -buildmode=shared -linkshared std命令，-buildmode指定编译模式为共享模式，-linkshared表示链接动态库，成功编译后会在$GOROOT目录下生标准库的动态库文件libstd.so，一般位于$GOROOT/pkg/linux_amd64_dynlink目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd $GOROOT/pkg/linux_amd64_dynlink
$ ls libstd.so
libstd.so
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将demo.go编译成动态库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go install  -buildmode=shared -linkshared demo
$ cd $GOPATH/pkg
$ ls linux_amd64_dynlink/
demo.a  demo.shlibname  libdemo.so
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;成功编译后会在$GOPATH/pkg目录生成相应的动态库libdemo.so。&lt;/p&gt;

&lt;p&gt;以动态库方式编译main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build -linkshared main.go
$ ll -h
total 25K
drwxrwx---. 1 root vboxsf 4.0K Apr 28 17:30 ./
drwxrwx---. 1 root vboxsf 4.0K Apr 28 17:22 ../
drwxrwx---. 1 root vboxsf    0 Apr 28 08:37 demo/
-rwxrwx---. 1 root vboxsf  16K Apr 28 17:30 main*
-rwxrwx---. 1 root vboxsf   58 Apr 28 08:37 main.go*
$ ./main
call demo ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从示例中可以看到，以动态库方式编译生成的可执行文件main大小才16K。如果以静态库方式编译，可执行文件main大小为1.5M，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build main.go
$ ll -h
total 1.5M
drwxrwx---. 1 root vboxsf 4.0K Apr 28 17:32 ./
drwxrwx---. 1 root vboxsf 4.0K Apr 28 17:22 ../
drwxrwx---. 1 root vboxsf    0 Apr 28 08:37 demo/
-rwxrwx---. 1 root vboxsf 1.5M Apr 28 17:32 main*
-rwxrwx---. 1 root vboxsf   58 Apr 28 08:37 main.go*
$ ./main
call demo ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以动态库方式编译时，如果删除动态库libdemo.so或者动态库libstd.so，运行main都会由于找不到动态库导致出错，例如删除动态库libdemo.so：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rm ../pkg/linux_amd64_dynlink/libdemo.so
$ ./main
./main: error while loading shared libraries: libdemo.so: cannot open shared object file: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上就是go语言动态库的编译和使用方法，需要注意的是，其他go程序在使用go动态库时，必须提供动态库的源码，否则会编译失败。例如，这里将demo.go代码删除，再以动态库方式编译main.go时，会编译失败：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go install  -buildmode=shared -linkshared demo
$ rm demo/demo.go
$ go build -linkshared main.go
main.go:3:8: no buildable Go source files in /media/sf_share/git/go_practice/src/demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;动态库编译方式和静态库不一样，静态库可以不提供源码，直接使用静态库编译，而动态库不行。&lt;/p&gt;

&lt;h1 id=&#34;go-get&#34;&gt;go get&lt;/h1&gt;

&lt;p&gt;go get 命令主要是用来动态获取远程代码包的，目前支持的有BitBucket、GitHub、Google Code和Launchpad。这个命令在内部实际上分成了两步操作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一步是下载源码包&lt;/li&gt;
&lt;li&gt;第二步是执行go install。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下载源码包的go工具会自动根据不同的域名调用不同的源码工具.&lt;/p&gt;

&lt;p&gt;所以为了go get 能正常工作，你必须确保安装了合适的源码管理工具，并同时把这些命令加入你的PATH中。其实go get支持自定义域名的功能，具体参见go help remote。&lt;/p&gt;

&lt;p&gt;go get 命令本质：首先通过源码工具clone代码到src目录，然后执行go install。&lt;/p&gt;

&lt;p&gt;参数：&lt;/p&gt;

&lt;p&gt;go get命令可以接受所有可用于go build命令和go install命令的标记。这是因为go get命令的内部步骤中完全包含了编译和安装这两个动作。另外，go get命令还有一些特有的标记，如下表所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;标记名称    标记描述
-d  让命令程序只执行下载动作，而不执行安装动作。
-f  仅在使用-u标记时才有效。该标记会让命令程序忽略掉对已下载代码包的导入路径的检查。如果下载并安装的代码包所属的项目是你从别人那里Fork过来的，那么这样做就尤为重要了。
-fix    让命令程序在下载代码包后先执行修正动作，而后再进行编译和安装。
-insecure   允许命令程序使用非安全的scheme（如HTTP）去下载指定的代码包。如果你用的代码仓库（如公司内部的Gitlab）没有HTTPS支持，可以添加此标记。请在确定安全的情况下使用它。
-t  让命令程序同时下载并安装指定的代码包中的测试源码文件中依赖的代码包。
-u  让命令利用网络来更新已有代码包及其依赖包。默认情况下，该命令只会从网络上下载本地不存在的代码包，而不会更新已有的代码包。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;go-test&#34;&gt;go test&lt;/h1&gt;

&lt;p&gt;go test 命令，会自动读取源码目录下面名为*_test.go的文件，生成并运行测试用的可执行文件。输出的信息类似&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ok   archive/tar   0.011s
FAIL archive/zip   0.022s
ok   compress/gzip 0.033s
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认的情况下，不需要任何的参数，它会自动把你源码包下面所有test文件测试完毕，当然你也可以带上参数，详情请参考go help testflag&lt;/p&gt;

&lt;h2 id=&#34;单元测试&#34;&gt;单元测试&lt;/h2&gt;

&lt;p&gt;go语言的单元测试采用内置的测试框架,通过引入testing包以及go test来提供测试功能。&lt;/p&gt;

&lt;p&gt;在源代码包目录内，所有以_test.go为后缀名的源文件被go test认定为测试文件，这些文件不包含在go build的代码构建中,而是单独通过 go test来编译，执行。
前置条件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文件名须以&amp;rdquo;_test.go&amp;rdquo;结尾&lt;/li&gt;
&lt;li&gt;每个测试函数必须导入testing包,方法名须以&amp;rdquo;Test&amp;rdquo;打头，并且形参为 (t *testing.T)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当运行go test命令时，go test会遍历所有的*_test.go中符合上述命名规则的函数，然后生成一个临时的main包用于调用相应的测试函数，然后构建并运行、报告测试结果，最后清理测试中生成的临时文件。&lt;/p&gt;

&lt;p&gt;情景&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;测试单个文件，一定要带上被测试的原文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;go test -v  wechat_test.go wechat.go 

-v是显示出详细的测试结果, -cover 显示出执行的测试用例的测试覆盖率。
go test -v -cover=true ./src/utils/slice_test.go ./src/utils/slice.go
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;测试单个方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;go test -v wechat_test.go -test.run TestRefreshAccessToken
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;当测试整个utils包时，使用命令:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;go test -v -cover=true ./src/utils/...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当测试单个测试用例时，使用命令:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#./src/utils为包utils的路径
go test -v -cover=true ./src/utils -run TestSuccessStringInSlice
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;工具&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;自动生成测试用例的工具gotests，以后有需要使用可以了解一下&lt;/p&gt;

&lt;h2 id=&#34;基准测试&#34;&gt;基准测试&lt;/h2&gt;

&lt;p&gt;基准测试，是一种测试代码性能的方法，比如你有多种不同的方案，都可以解决问题，那么到底是那种方案性能更好呢？这时候基准测试就派上用场了。&lt;/p&gt;

&lt;p&gt;基准测试主要是通过测试CPU和内存的效率问题，来评估被测试代码的性能，进而找到更好的解决方案。比如链接池的数量不是越多越好，那么哪个值才是最优值呢，这就需要配合基准测试不断调优了。&lt;/p&gt;

&lt;p&gt;如何编写基准测试&lt;/p&gt;

&lt;p&gt;基准测试代码的编写和单元测试非常相似，它也有一定的规则，我们先看一个示例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;itoa_test.go

func BenchmarkSprintf(b *testing.B){
    num:=10
    b.ResetTimer()
    for i:=0;i&amp;lt;b.N;i++{
        fmt.Sprintf(&amp;quot;%d&amp;quot;,num)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个基准测试的例子，从中我们可以看出以下规则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基准测试的代码文件必须以_test.go结尾&lt;/li&gt;
&lt;li&gt;基准测试的函数必须以Benchmark开头，必须是可导出的&lt;/li&gt;
&lt;li&gt;基准测试函数必须接受一个指向Benchmark类型的指针作为唯一参数&lt;/li&gt;
&lt;li&gt;基准测试函数不能有返回值&lt;/li&gt;
&lt;li&gt;b.ResetTimer是重置计时器，这样可以避免for循环之前的初始化代码的干扰&lt;/li&gt;
&lt;li&gt;最后的for循环很重要，被测试的代码要放到循环里&lt;/li&gt;
&lt;li&gt;b.N是基准测试框架提供的，表示循环的次数，因为需要反复调用测试的代码，才可以评估性能&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我们运行下基准测试，看看效果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go test -bench=. -run=none
BenchmarkSprintf-8      20000000               117 ns/op
PASS
ok      flysnow.org/hello       2.474s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行基准测试也要使用go test命令，不过我们要加上-bench=标记，它接受一个表达式作为参数，匹配基准测试的函数，.表示运行所有基准测试。&lt;/p&gt;

&lt;p&gt;因为默认情况下 go test 会运行单元测试，为了防止单元测试的输出影响我们查看基准测试的结果，可以使用-run=匹配一个从来没有的单元测试方法，过滤掉单元测试的输出，我们这里使用none，因为我们基本上不会创建这个名字的单元测试方法。&lt;/p&gt;

&lt;p&gt;下面着重解释下说出的结果，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;函数后面的-8了吗？这个表示运行时对应的GOMAXPROCS的值。接着的20000000表示运行for循环的次数，也就是调用被测试代码的次数，最后的117 ns/op表示每次需要话费117纳秒。&lt;/li&gt;
&lt;li&gt;Benchmarkxxx-4 格式为基准测试函数名-GOMAXPROCS，后面的-4代表测试函数运行时对应的CPU核数&lt;/li&gt;
&lt;li&gt;1 表示执行的次数&lt;/li&gt;
&lt;li&gt;xx ns/op 表示每次的执行时间&lt;/li&gt;
&lt;li&gt;xx B/op 表示每次执行分配的总字节数（内存消耗）&lt;/li&gt;
&lt;li&gt;xx allocs/op 表示每次执行发生了多少次内存分配&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上是测试时间默认是1秒，也就是1秒的时间，调用两千万次，每次调用花费117纳秒。如果想让测试运行的时间更长，可以通过-benchtime指定，比如3秒。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go test -bench=. -benchtime=3s -run=none
BenchmarkSprintf-8      50000000               109 ns/op
PASS
ok      flysnow.org/hello       5.628s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以发现，我们加长了测试时间，测试的次数变多了，但是最终的性能结果：每次执行的时间，并没有太大变化。一般来说这个值最好不要超过3秒，意义不大。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;性能对比&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面那个基准测试的例子，其实是一个int类型转为string类型的例子，标准库里还有几种方法，我们看下哪种性能更加。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func BenchmarkSprintf(b *testing.B){
    num:=10
    b.ResetTimer()
    for i:=0;i&amp;lt;b.N;i++{
        fmt.Sprintf(&amp;quot;%d&amp;quot;,num)
    }
}

func BenchmarkFormat(b *testing.B){
    num:=int64(10)
    b.ResetTimer()
    for i:=0;i&amp;lt;b.N;i++{
        strconv.FormatInt(num,10)
    }
}

func BenchmarkItoa(b *testing.B){
    num:=10
    b.ResetTimer()
    for i:=0;i&amp;lt;b.N;i++{
        strconv.Itoa(num)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行基准测试，看看结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go test -bench=. -run=none
BenchmarkSprintf-8      20000000               117 ns/op
BenchmarkFormat-8       50000000                33.3 ns/op
BenchmarkItoa-8         50000000                34.9 ns/op
PASS
ok      flysnow.org/hello       5.951s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从结果上看strconv.FormatInt函数是最快的，其次是strconv.Itoa，然后是fmt.Sprintf最慢，前两个函数性能达到了最后一个的3倍多。那么最后一个为什么这么慢的，我们再通过-benchmem找到根本原因。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go test -bench=. -benchmem -run=none
BenchmarkSprintf-8      20000000               110 ns/op              16 B/op          2 allocs/op
BenchmarkFormat-8       50000000                31.0 ns/op             2 B/op          1 allocs/op
BenchmarkItoa-8         50000000                33.1 ns/op             2 B/op          1 allocs/op
PASS
ok      flysnow.org/hello       5.610s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;-benchmem可以提供每次操作分配内存的次数，以及每次操作分配的字节数。从结果我们可以看到，性能高的两个函数，每次操作都是进行1次内存分配，而最慢的那个要分配2次；性能高的每次操作分配2个字节内存，而慢的那个函数每次需要分配16字节的内存。从这个数据我们就知道它为什么这么慢了，内存分配都占用都太高。&lt;/p&gt;

&lt;p&gt;在代码开发中，对于我们要求性能的地方，编写基准测试非常重要，这有助于我们开发出性能更好的代码。不过性能、可用性、复用性等也要有一个相对的取舍，不能为了追求性能而过度优化。&lt;/p&gt;

&lt;h1 id=&#34;go-clean&#34;&gt;go clean&lt;/h1&gt;

&lt;p&gt;go clean 命令是用来移除当前源码包里面编译生成的文件，这些文件包括&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;_obj/ 旧的object目录，由Makefiles遗留
_test/ 旧的test目录，由Makefiles遗留
_testmain.go 旧的gotest文件，由Makefiles遗留
test.out 旧的test记录，由Makefiles遗留
build.out 旧的test记录，由Makefiles遗留
*.[568ao] object文件，由Makefiles遗留
DIR(.exe) 由 go build 产生
DIR.test(.exe) 由 go test -c 产生
MAINFILE(.exe) 由 go build MAINFILE.go产生
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;go-fmt&#34;&gt;go fmt&lt;/h1&gt;

&lt;p&gt;go fmt 命令主要是用来帮你格式化所写好的代码文件。&lt;/p&gt;

&lt;p&gt;比如我们写了一个格式很糟糕的test.go文件，我们只需要使用fmt命令，就可以让go帮我们格式化我们的代码文件。但是我们一般很少使用这个命令，因为我们的开发工具一般都带有保存时自动格式化功能，这个功能底层其实就是调用了 go fmt 命令而已。&lt;/p&gt;

&lt;p&gt;使用go fmt命令，更多时候是用gofmt，而且需要参数-w，否则格式化结果不会写入文件。gofmt -w src，可以格式化整个项目。&lt;/p&gt;

&lt;h1 id=&#34;go-doc&#34;&gt;go doc&lt;/h1&gt;

&lt;p&gt;go doc 命令其实就是一个很强大的文档工具。&lt;/p&gt;

&lt;p&gt;如何查看相应package的文档呢？ 例如builtin包，那么执行go doc builtin；如果是http包，那么执行go doc net/http；查看某一个包里面的函数，那么执行godoc fmt Printf；也可以查看相应的代码，执行godoc -src fmt Printf；&lt;/p&gt;

&lt;p&gt;通过命令在命令行执行 godoc -http=:端口号 比如godoc -http=:8080。然后在浏览器中打开127.0.0.1:8080，你将会看到一个golang.org的本地copy版本，通过它你可以查询pkg文档等其它内容。如果你设置了GOPATH，在pkg分类下，不但会列出标准包的文档，还会列出你本地GOPATH中所有项目的相关文档，这对于经常被限制访问的用户来说是一个不错的选择。&lt;/p&gt;

&lt;h1 id=&#34;其他命令&#34;&gt;其他命令&lt;/h1&gt;

&lt;p&gt;Go语言还提供了其它有用的工具，例如下面的这些工具&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;go fix 用来修复以前老版本的代码到新版本，例如go1之前老版本的代码转化到go1&lt;/li&gt;
&lt;li&gt;go version 查看go当前的版本&lt;/li&gt;
&lt;li&gt;go env 查看当前go的环境变量&lt;/li&gt;
&lt;li&gt;go list 列出当前全部安装的package&lt;/li&gt;
&lt;li&gt;go run 编译并运行Go程序&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>golang使用系列---- Time</title>
          <link>https://kingjcy.github.io/post/golang/go-time/</link>
          <pubDate>Tue, 12 Apr 2016 20:11:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-time/</guid>
          <description>&lt;p&gt;time包中包括两类时间：时间点（某一时刻）和时长（某一段时间）的基本操作。&lt;/p&gt;

&lt;h1 id=&#34;time&#34;&gt;time&lt;/h1&gt;

&lt;h2 id=&#34;基本结构&#34;&gt;基本结构&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Time&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Time struct {
    wall uint64
    ext  int64
    loc *Location
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;wall  秒&lt;/li&gt;
&lt;li&gt;ext   纳秒&lt;/li&gt;
&lt;li&gt;loc *Location

&lt;ul&gt;
&lt;li&gt;time.UTC utc时间&lt;/li&gt;
&lt;li&gt;time.Local 本地时间&lt;/li&gt;
&lt;li&gt;FixedZone(name string, offset int) *Location   设置时区名,以及与UTC0的时间偏差.返回Location&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Duration&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Duration int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Duration类型代表两个时间点之间经过的时间，以纳秒为单位。可表示的最长时间段大约290年。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;时间常量&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Duration的单位为 nanosecond，为了便于使用，time中定义了时间常量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    Nanosecond Duration = 1
    Microsecond = 1000 * Nanosecond
    Millisecond = 1000 * Microsecond
    Second = 1000 * Millisecond
    Minute = 60 * Second
    Hour = 60 * Minute
)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Ticker&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Ticker
type Ticker struct {
    C &amp;lt;-chan Time // 周期性传递时间信息的通道
    // 内含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ticker保管一个通道，并每隔一段时间向其传递&amp;rdquo;tick&amp;rdquo;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewTicker
func NewTicker(d Duration) *Ticker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewTicker返回一个新的Ticker，该Ticker包含一个通道字段，并会每隔时间段d就向该通道发送当时的时间。它会调整时间间隔或者丢弃tick信息以适应反应慢的接收者。如果d&amp;lt;=0会panic。关闭该Ticker可以释放相关资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Ticker) Stop
func (t *Ticker) Stop()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stop关闭一个Ticker。在关闭后，将不会发送更多的tick信息。Stop不会关闭通道t.C，以避免从该通道的读取不正确的成功。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;time.Duration（时长，耗时）&lt;/li&gt;
&lt;li&gt;time.Time（时间点）&lt;/li&gt;
&lt;li&gt;time.C（放时间点的管道）[ Time.C:=make(chan time.Time) ]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;基本函数&#34;&gt;基本函数&lt;/h2&gt;

&lt;p&gt;time包提供了时间的显示和测量用的函数。日历的计算采用的是公历。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Now
func Now() Time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now返回当前本地时间。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (Time) Before
func (t Time) Before(u Time) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果t代表的时间点在u之前，返回真；否则返回假。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (Time) Add
func (t Time) Add(d Duration) Time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add返回时间点t+d。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (Time) Second
func (t Time) Second() int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回t对应的那一分钟的第几秒，范围[0, 59]。&lt;/p&gt;

&lt;h1 id=&#34;常规使用&#34;&gt;常规使用&lt;/h1&gt;

&lt;h2 id=&#34;sleep&#34;&gt;sleep&lt;/h2&gt;

&lt;p&gt;golang的休眠可以使用time包中的sleep。&lt;/p&gt;

&lt;p&gt;函数原型为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Sleep(d Duration)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面实现休眠2秒功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {

    fmt.Println(&amp;quot;begin&amp;quot;)
    time.Sleep(time.Duration(2)*time.Second)
    fmt.Println(&amp;quot;end&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;time使用变量的时候需要强制转换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time.Duration(cfg.CTimeOut) * time.Second
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;定时器&#34;&gt;定时器&lt;/h2&gt;

&lt;p&gt;定时器只会传达一次到期事件，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Timer struct {
    C &amp;lt;-chan Time
    r runtimeTimer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每天定时0点执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;time&amp;quot;
    &amp;quot;fmt&amp;quot;
)

//定时结算Boottime表数据
func BoottimeTimingSettlement() {
    for {
        now := time.Now()
        // 计算下一个零点
        next := now.Add(time.Hour * 24)
        next = time.Date(next.Year(), next.Month(), next.Day(), 0, 0, 0, 0, next.Location())
        t := time.NewTimer(next.Sub(now))
        &amp;lt;-t.C
        Printf(&amp;quot;定时结算Boottime表数据，结算完成: %v\n&amp;quot;,time.Now())
        //以下为定时执行的操作
        BoottimeSettlement()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;断续器&#34;&gt;断续器&lt;/h2&gt;

&lt;p&gt;周期性的传达到期事件的装置，定时器只会传达一次到期事件，断续器会持续工作直到停止。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Ticker struct {
    C &amp;lt;-chan Time // The channel on which the ticks are delivered.
    r runtimeTimer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewTicker(d Duration) *Ticker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewTicker返回一个新的Ticker，该Ticker包含一个通道字段，并会每隔时间段d就向该通道发送当时的时间。它会调整时间间隔或者丢弃tick信息以适应反应慢的接收者。如果d&amp;lt;=0会panic。关闭该Ticker可以释放相关资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ticker := time.NewTicker(time.Millisecond * 500)
go func() {
    for t := range ticker.C {
        fmt.Println(&amp;quot;Tick at&amp;quot;, t)
    }
}()

time.Sleep(time.Millisecond * 1500)   //阻塞，则执行次数为sleep的休眠时间/ticker的时间
ticker.Stop()    
fmt.Println(&amp;quot;Ticker stopped&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取时间&#34;&gt;获取时间&lt;/h2&gt;

&lt;p&gt;各种现有时间的获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    fmt.Printf(&amp;quot;时间戳（秒）：%v;\n&amp;quot;, time.Now().Unix())
    fmt.Printf(&amp;quot;时间戳（纳秒）：%v;\n&amp;quot;,time.Now().UnixNano())
    fmt.Printf(&amp;quot;时间戳（毫秒）：%v;\n&amp;quot;,time.Now().UnixNano() / 1e6)
    fmt.Printf(&amp;quot;时间戳（纳秒转换为秒）：%v;\n&amp;quot;,time.Now().UnixNano() / 1e9)
}


时间戳（秒）：1530027865;
时间戳（纳秒）：1530027865231834600;
时间戳（毫秒）：1530027865231;
时间戳（纳秒转换为秒）：1530027865;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;时间转化&#34;&gt;时间转化&lt;/h2&gt;

&lt;p&gt;处理时间单位自动转化问题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ParseDuration(s string) (Duration, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;传入字符串，返回响应的时间，其中传入的字符串中的有效时间单位如下：h,m,s,ms,us,ns，其他单位均无效，如果传入无效时间单位，则会返回０&lt;/p&gt;

&lt;p&gt;获取前n天的时间&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//获取两天前的时间
currentTime := time.Now()
oldTime := currentTime.AddDate(0, 0, -2)        //若要获取3天前的时间，则应将-2改为-3
//oldTime 的结果为go的时间time类型，2018-09-25 13:24:58.287714118 +0000 UTC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比较时间，使用before&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time1 := &amp;quot;2015-03-20 08:50:29&amp;quot;
time2 := &amp;quot;2015-03-21 09:04:25&amp;quot;
//先把时间字符串格式化成相同的时间类型
t1, err := time.Parse(&amp;quot;2006-01-02 15:04:05&amp;quot;, time1)
t2, err := time.Parse(&amp;quot;2006-01-02 15:04:05&amp;quot;, time2)
if err == nil &amp;amp;&amp;amp; t1.Before(t2) {
    //处理逻辑
    fmt.Println(&amp;quot;true&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取文件的各种时间&#34;&gt;获取文件的各种时间&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    finfo, _ := os.Stat(filename)
    // Sys()返回的是interface{}，所以需要类型断言，不同平台需要的类型不一样，linux上为*syscall.Stat_t
    stat_t := finfo.Sys().(*syscall.Stat_t)
    fmt.Println(stat_t)
    // atime，ctime，mtime分别是访问时间，创建时间和修改时间，具体参见man 2 stat
    fmt.Println(timespecToTime(stat_t.Atim))
    fmt.Println(timespecToTime(stat_t.Ctim))
    fmt.Println(timespecToTime(stat_t.Mtim))
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Zabbix监控方案</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix-scheme/</link>
          <pubDate>Fri, 04 Mar 2016 17:54:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix-scheme/</guid>
          <description>&lt;p&gt;zabbix是目前各大互联网公司使用最广泛的开源监控之一,其历史最早可追溯到1998年,在业内拥有各种成熟的解决方案，但是对容器的监控还是比较薄弱，我们也不多说，主要用于基础设施VM的监控。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/zabbix/zabbix.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;详细说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;agent：负载采集数据，所有的采集都在这一个进程中，不像prometheus的exporter有很多。&lt;/li&gt;
&lt;li&gt;proxy：是一个汇聚层，将数据聚合后发送到server。&lt;/li&gt;
&lt;li&gt;server：服务端，用于存储数据，对外进行查询展示。&lt;/li&gt;
&lt;li&gt;DB：数据库，存储数据&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;zabbix的核心组件&#34;&gt;zabbix的核心组件&lt;/h1&gt;

&lt;p&gt;1、zabbix server 负责采集和收取agent采集的信息。&lt;/p&gt;

&lt;p&gt;2、zabbix database   用于存储zabbix的配置信息，监控数据&lt;/p&gt;

&lt;p&gt;3、zabbix web zabbix的管理界面，监控界面，可以独立部署，只要能连接到database就可以&lt;/p&gt;

&lt;p&gt;4、zabbix agent 不数据监控主机主机上，负责采集数据，把数据推送到server或者server来去数据（主动和被动模式，可以同时设置）&lt;/p&gt;

&lt;p&gt;5、zabbix proxy 用于分布式监控，作用就是用于聚合部分数据，最后统一发完server&lt;/p&gt;

&lt;p&gt;zabbix对分布式的数据采集非常好,支持两种分布式架构,一种是Proxy,一种是Node.Proxy作为zabbix server的代理去监控服务器,并发数据汇聚到Zabbix server.而Node本身就是一个完整的Zabbix server, 使用Node可以将多个Zabbix server组成一个具有基层关系的分布式架构.&lt;/p&gt;

&lt;p&gt;两者的区别如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                proxy   Node
轻量级         √       ×
GUI前端           ×       √
是否可以独立运行    √       ×
容易运维            √       ×
本地Admin管理   ×       √
中心化配置       √       ×
产生通知            ×       √
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、zabbix get 安装服务器上，来测试获取agent的数据的工具&lt;/p&gt;

&lt;p&gt;7、zabbix sender 安装在客户端机器上，用于测试推送数据到server的的工具&lt;/p&gt;

&lt;h1 id=&#34;zabbix监控方式&#34;&gt;Zabbix监控方式&lt;/h1&gt;

&lt;p&gt;1、被动模式&lt;/p&gt;

&lt;p&gt;被动检测：相对于agent而言；agent, server向agent请求获取配置的各监控项相关的数据，agent接收请求、获取数据并响应给server；&lt;/p&gt;

&lt;p&gt;2、主动模式&lt;/p&gt;

&lt;p&gt;主动检测：相对于agent而言；agent(active),agent向server请求与自己相关监控项配置，主动地将server配置的监控项相关的数据发送给server；&lt;/p&gt;

&lt;p&gt;主动监控能极大节约监控server 的资源。&lt;/p&gt;

&lt;h1 id=&#34;zabbix的使用&#34;&gt;zabbix的使用&lt;/h1&gt;

&lt;p&gt;基本安装使用可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix/&#34;&gt;这里&lt;/a&gt;,相关源码解析可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbixcode/&#34;&gt;这里&lt;/a&gt;,这些就不多说了。&lt;/p&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;随着系统监控规模的越来越大，zabbix出现越来越多的瓶颈，随着时序数据库的广泛使用，监控已经渐渐切换到了时序数据库，对于原始的zabbix监控项，监控数据如何处理？我们可以将zabbix数据存到时序数据库中，统一使用，相关&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix2tsdb/&#34;&gt;实现方案&lt;/a&gt;就需要自己实现了。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 文件存储系统nfs</title>
          <link>https://kingjcy.github.io/post/distributed/store/fs/nfs/</link>
          <pubDate>Sat, 16 Jan 2016 20:32:58 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/fs/nfs/</guid>
          <description>&lt;p&gt;NFS是Network File System的缩写，就是网络文件系统，主要功能是让应用程序在客户端通过网络访问位于服务器磁盘中的数据，是在类Unix系统间实现磁盘文件共享的一种方法。&lt;/p&gt;

&lt;p&gt;NFS系统和Windows网络共享、网络驱动器类似, 只不过windows用于局域网, NFS用于企业集群架构中, 如果是大型网站, 会用到更复杂的分布式文件系统&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/fastfs/&#34;&gt;fastdfs&lt;/a&gt;,&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/glusterfs/&#34;&gt;glusterfs&lt;/a&gt;,&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/hfds/&#34;&gt;HDFS&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;安装使用&#34;&gt;安装使用&lt;/h1&gt;

&lt;h2 id=&#34;服务器&#34;&gt;服务器&lt;/h2&gt;

&lt;p&gt;系统的默认软件包安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nfs-utils-* :包括基本的NFS命令与监控程序
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note：这里要注意的是需要在集群每个节点都安装nfs-utils安装包，不然挂载会失败！&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master mnt]# yum install nfs-utils
已加载插件：fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.cn99.com
 * extras: mirrors.cn99.com
 * updates: mirrors.cn99.com
软件包 1:nfs-utils-1.3.0-0.61.el7.x86_64 已安装并且是最新版本
无须任何处理
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NFS服务的配置相对简单，只需要在对应文件中进行配置，然后启动NFS服务即可。&lt;/p&gt;

&lt;p&gt;NFS常用文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/etc/exports NFS服务的主要配置文件&lt;/li&gt;
&lt;li&gt;/usr/sbin/exportfs NFS服务的管理命令&lt;/li&gt;
&lt;li&gt;/usr/sbin/showmount 客户端的查看命令&lt;/li&gt;
&lt;li&gt;/var/lib/nfs/etab 记录NFS分享出来的目录的完整权限设定值&lt;/li&gt;
&lt;li&gt;/var/lib/nfs/xtab 记录曾经登录过的Clinent 信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;编辑/etc/exports文件添加以下内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost share]# vim /etc/exports
    /share  192.168.254.0/24(insecure,rw,no_root_squash)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置文件说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[输出目录 客户端 选项（访问权限,用户映射,其他）] [客户端2 选项（访问权限,用户映射,其他）]

1、输出目录：
输出目录是指NFS系统中需要共享给客户端使用的目录
2、客户端：
客户端是指网络中可以访问这个NFS Server的主机，客户端常用的指定方式如下：
    指定IP地址：172.16.7.57
    指定子网中的主机：172.16.7.0/24
    指定域名的主机：ssq-54-57.zerounix.com
    指定域中的所有主机：*.zerounix.com
    所有主机：*
3、选项
主要有3类选项：
访问权限选项：
    设置输出目录只读：ro
    设置输出目录读写：rw
用户映射选项：
    all_squash： 将远程访问的所有普通用户及属组都映射为匿名用户或用户组(nfsnobody)；
    no_all_squash： 与all_squash相反（default）；
    root_squash： 将root用户及属组都映射问匿名用户或用户组（default）；
    no_root_squash：
    anonuid=xxx： 将远程访问的所有用户都映射为匿名用户，并指定用户问本地用户（UID=xxx）；
    anongid=xxx： 将远程访问的所有用户都映射为匿名用户组，并指定用户问本地用户组（GID=xxx）；
其他选项：
    secure： 限制客户端只能从小于1024的tcp端口连接NFS Server（default）；
    insecure： 允许客户端从大于1024的tcp端口连接NFS Server；
    sync： 将数据同步下乳内存缓冲区与磁盘中，效率低，但是可以保证数据的一致性；
    async： 将数据先保存在内存缓冲区中，必要时才写入磁盘；
    wdelay： 检查是否有相关的写操作，如果有则见这些写操作一起执行，可以提高效率（default）；
    no_wdelay： 若有写操作立即执行，应与sync配合使用；
    subtree： 若输出目录是一个子目录，则NFS Server将检查其父目录权限（default）；
    no_subtree： 若输出目录是一个子目录，则NFS Server将不检查其父目录权限；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在启动之前需要关闭防火墙。&lt;/p&gt;

&lt;p&gt;启动NFS Server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service nfs start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看进程状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# systemctl status nfs-server
● nfs-server.service - NFS server and services
   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)
   Active: active (exited) since Thu 2019-10-31 09:53:29 CST; 20s ago
 Main PID: 20402 (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/nfs-server.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证nfs的服务是否正常&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo rpcinfo -p|grep nfs
100003    3   tcp   2049  nfs
100003    4   tcp   2049  nfs
100003    3   udp   2049  nfs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看具体目录的挂载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /var/lib/nfs/etab
/share  *(rw,sync,wdelay,hide,nocrossmnt,secure,no_root_squash,no_all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid=65534,anongid=65534,sec=sys,rw,secure,no_root_squash,no_all_squash)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在/share目录中写一个index.html文件并且写入内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost share]# echo &amp;quot;nfs server&amp;quot; &amp;gt; /share/index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;客户端&#34;&gt;客户端&lt;/h2&gt;

&lt;p&gt;同样是安装nfs服务，然后可以通过命令来查看server端的共享信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@web01 ~]# showmount -e 172.16.1.31

Export list for 172.16.1.31:

/share 172.16.1.0/24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们使用挂载命令将远程服务的共享目录挂载到本地&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@web01 ~]# mkdir /share

[root@web01 ~]# mount -t nfs 172.16.1.31:/share /share/

[root@web01 ~]# df -h

文件系统                   容量  已用   可用    已用% 挂载

172.16.1.31:/share         50G  2.6G   48G    6% /share
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以到/share目录下获取文件了。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/fs/nfs&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;NFS在文件传送或信息传送过程中依赖于RPC协议。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户进程访问NFS客户端，使用不同的函数对数据进行处理&lt;/li&gt;
&lt;li&gt;NFS客户端通过TCP/IP的方式传递给NFS服务端。&lt;/li&gt;
&lt;li&gt;NFS服务端接收到请求后，会先调用portmap进程进行端口映射。&lt;/li&gt;
&lt;li&gt;nfsd进程用于判断NFS客户端是否拥有权限连接NFS服务端。&lt;/li&gt;
&lt;li&gt;Rpc.mount进程判断客户端是否有对应的权限进行验证。&lt;/li&gt;
&lt;li&gt;idmap进程实现用户映射和压缩&lt;/li&gt;
&lt;li&gt;最后NFS服务端会将对应请求的函数转换为本地能识别的命令，传递至内核，由内核驱动硬件。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见，nfs其实就是一个rpc调用传输文件的系统，在这个过程中加了很多权限的控制。&lt;/p&gt;

&lt;h1 id=&#34;应用&#34;&gt;应用&lt;/h1&gt;

&lt;p&gt;NFS存储优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NFS文件系统简单易用、方便部署，数据可靠，服务稳定，满足中小企业需求&lt;/li&gt;
&lt;li&gt;、NFS文件系统内存放的数据都在文件系统之上，所有数据都是可见的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NFS存储局限&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;存在单点故障，构建高可用维护麻烦web&amp;mdash;nfs&amp;mdash;backup&lt;/li&gt;
&lt;li&gt;NFS数据明文，没有校验&lt;/li&gt;
&lt;li&gt;客户端挂载没有密码，安全性一般（也就内网使用）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NFS应用建议&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;生产场景应用将静态数据尽可能向前推送，减少后端存储的压力&lt;/li&gt;
&lt;li&gt;必须将存储的静态资源通过CDN缓存jpg/png/mp4/avi/css/js&lt;/li&gt;
&lt;li&gt;如果没有缓存，存储再多对网站的速度也没有太大帮助&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;大小问题&lt;/p&gt;

&lt;p&gt;当我们在WEB上挂载了NFS共享的/data目录之后，查看一下，我们发现/data的大小是50G，为什么是50G，我们难道共享了一个目录就一下子共享了50G吗？其实就是这样的，你想呀，我们在/data的根下创建了目录，那这个目录是多大？我们在创建目录的时候也不能指定这个目录最大能到多少G，这就是和我们在windows上创建一个文件夹一样，这个文件夹最终能变大取决于该文件夹所在的分区有多大，在NFS上/data是在根所有的分区上创建的，理论上NFS的根最大能变多大，/data目录就能变多大。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@Web01 html]# df -h
Filesystem               Size  Used Avail Use% Mounted on
192.168.80.221:/data      50G  4.0G   47G   8% /var/www/html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在NFS上查看一下根的大小，发现根就是50G，这下你明白了吧！&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@Nfs ~]# df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-root   50G  4.0G   47G   8% /
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>c/c&#43;&#43;系列---- Thread</title>
          <link>https://kingjcy.github.io/post/linux/c&#43;&#43;/thread/</link>
          <pubDate>Sat, 01 Aug 2015 09:32:17 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/linux/c&#43;&#43;/thread/</guid>
          <description>&lt;p&gt;多线程（英文：multithreading）多线程程序包含了可以并发运行的两个或更多个程序部分。&lt;/p&gt;

&lt;p&gt;互联网时代，由于在线用户数量的爆炸，单台服务器处理的连接也水涨船高，迫使编程模式由从前的串行模式升级到并发模型，而几十年来，并发模型也是一代代地升级，有&lt;strong&gt;IO多路复用、多进程以及多线程&lt;/strong&gt;，这几种模型都各有长短，现代复杂的高并发架构大多是几种模型协同使用，不同场景应用不同模型，扬长避短，发挥服务器的最大性能，而多线程，因为其轻量和易用，成为并发编程中使用频率最高的并发模型，而后衍生的协程等其他子产品，也都基于它，而我们今天要使用的 &lt;strong&gt;goroutine&lt;/strong&gt; 也是基于线程。&lt;/p&gt;

&lt;h1 id=&#34;c语言中使用多线程的函数&#34;&gt;C语言中使用多线程的函数&lt;/h1&gt;

&lt;p&gt;pthread_t在头文件/usr/include/bits/pthreadtypes.h中定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typedef unsigned long int pthread_t;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它是一个线程的标识符&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;创建线程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;pthread_create(thread, attr, start_routine, arg);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pthread_create创建一个进程，并且让它可执行&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;thread： 指向线程标识符指针。&lt;/li&gt;
&lt;li&gt;attr：一个不透明的属性对象，可以被用来设置线程属性，也可以使用默认值 NULL。&lt;/li&gt;
&lt;li&gt;start_routine：线程运行函数起始地址，一旦线程被创建就会执行。&lt;/li&gt;
&lt;li&gt;arg：运行函数的参数。它必须通过把引用作为指针强制转换为 void 类型进行传递。如果没有传递参数，则使用 NULL。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;创建线程成功时，函数返回 0，若返回值不为 0 则说明创建线程失败。&lt;/p&gt;

&lt;p&gt;创建线程成功后，新创建的线程则运行参数三和参数四确定的函数，原来的线程则继续运行下一行代码。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;结束线程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;pthread_exit (status);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pthread_exit 用于显式地退出一个线程，唯一的参数是函数的返回代码。&lt;/p&gt;

&lt;p&gt;通常情况下，pthread_exit() 函数是在线程完成工作后无需继续存在时被调用。&lt;/p&gt;

&lt;p&gt;如果 main() 是在它所创建的线程之前结束，并通过 pthread_exit() 退出，那么其他线程将继续执行。否则，它们将在 main() 结束时自动被终止。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;线程等待&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;pthread_join (threadid, status)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个参数为被等待的线程标识符，第二个参数为一个用户定义的指针，它可以用来存储被等待线程的返回值。&lt;/p&gt;

&lt;p&gt;这个函数是一个线程阻塞的函数，调用它的函数将一直等待到被等待的线程结束为止，当函数返回时，被等待线程的资源被收回。&lt;/p&gt;

&lt;p&gt;最后要说明的是，一个线程不能被多个线程等待，否则第一个接收到信号的线程成功返回，其余调用pthread_join的线程则返回错误代码ESRCH。&lt;/p&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;pthread.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

#define NUM_THREADS 5      //线程个数

void *say_hello(void *args)
{
    printf(&amp;quot;Hello Runoob！\n&amp;quot;);
}

int main()
{
    //定义线程的 id 变量，多个变量使用数组
    pthread_t tids[NUM_THREADS];
    for (int i = 0; i &amp;lt; NUM_THREADS; ++i) {
        //参数依次是：创建的线程id，线程参数，调用的函数，传入的函数参数
        int ret = pthread_create(&amp;amp;tids[i], NULL, say_hello, NULL);
        if (ret != 0) {
            printf(&amp;quot;pthread_create error: error_code = %d\n&amp;quot;, ret);
        }
    }

    //等各个线程退出后，进程才结束，否则进程强制结束了，线程可能还没反应过来；
    pthread_exit(NULL);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;//g++ test.cpp -lpthread -o test&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;pthread.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

#define NUM_THREADS 5      //线程个数

void *print_hello(void *threadid)
{
    // 对传入的参数进行强制类型转换，由无类型指针变为整形数指针，然后再读取
    int tid = *((int*)threadid);
    printf(&amp;quot;Hello Runoob! 线程 ID, %d\n&amp;quot;, tid);
    pthread_exit(NULL);
}

int main()
{
    pthread_t threads[NUM_THREADS];
    int index[NUM_THREADS];
    for (int i = 0; i &amp;lt; NUM_THREADS; ++i) {
        printf(&amp;quot;main() : 创建线程, %d\n&amp;quot;, i);
        index[i] = i;
        int ret = pthread_create(&amp;amp;threads[i], NULL, print_hello, (void*)&amp;amp;(index[i]));
        if (ret != 0) {
            printf(&amp;quot;pthread_create error: error_code = %d\n&amp;quot;, ret);
            exit(-1);
        }
    }

    pthread_exit(NULL);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过结构体传递多个参数。可以在线程回调中传递任意的数据类型，因为它指向 void&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;pthread.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

#define NUM_THREADS 5      //线程个数

struct thread_data{
    int thread_id;
    double message;
};

void *print_hello(void *threadarg)
{
    struct thread_data *my_data =  (struct thread_data *) threadarg;

    printf(&amp;quot;Thread ID : %d\n&amp;quot;, my_data-&amp;gt;thread_id);
    printf(&amp;quot;Message : %f\n&amp;quot;, my_data-&amp;gt;message);

    pthread_exit(NULL);
}

int main()
{
    //定义线程的 id 变量，多个变量使用数组
    pthread_t threads[NUM_THREADS];
    struct thread_data td[NUM_THREADS];
    for (int i = 0; i &amp;lt; NUM_THREADS; ++i) {
        printf(&amp;quot;main() : creating thread, %d\n&amp;quot;, i);
        td[i].thread_id = i;
        td[i].message = i;
        int ret = pthread_create(&amp;amp;threads[i], NULL, print_hello, (void*)&amp;amp;(td[i]));
        if (ret != 0) {
            printf(&amp;quot;pthread_create error: error_code = %d\n&amp;quot;, ret);
            exit(-1);
        }
    }

    //等各个线程退出后，进程才结束，否则进程强制结束了，线程可能还没反应过来；
    pthread_exit(NULL);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;main() : creating thread, 0
main() : creating thread, 1
Thread ID : 0
Message : 0.000000
main() : creating thread, 2
Thread ID : 1
main() : creating thread, 3
Message : 1.000000
Thread ID : 2
main() : creating thread, 4
Message : 2.000000
Thread ID : 3
Thread ID : 4
Message : 3.000000
Message : 4.000000
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>计算机基础系列---- Datastruct</title>
          <link>https://kingjcy.github.io/post/computerbase/datastruct/datastruct/</link>
          <pubDate>Thu, 16 Apr 2015 20:04:05 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/computerbase/datastruct/datastruct/</guid>
          <description>&lt;p&gt;关于数据结构的一些基础总结。&lt;/p&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/azl397985856/leetcode&#34;&gt;https://github.com/azl397985856/leetcode&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;基本数据结构&#34;&gt;基本数据结构&lt;/h1&gt;

&lt;p&gt;逻辑结构：是指数据元素之间的逻辑关系，从逻辑关系上描述信息。&lt;/p&gt;

&lt;p&gt;存储结构（又称物理结构）：数据结构在计算机中的表示（又称映像）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;按物理结构分为顺序结构和链式结构，在计算机上只有这两种存储方式。其实还有一种特殊的，就是散点集合，数据之间没有关系的。我们一般描述的都是数据之间有关系的。

按逻辑结构进行分为我们常说的散点集合，线性数据结构和非线性数据结构（树状数据结构和网状数据结构）。这个是我们常规使用的方式，其实数据结构就是那几种。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;散点集合&#34;&gt;散点集合&lt;/h2&gt;

&lt;p&gt;就是正常的set集合，数据之间没有什么关系。&lt;/p&gt;

&lt;p&gt;集合结构中的数据元素除了 同属于一个集合外，它们之间没有其他关系。 各个数据元素是&amp;rdquo;平等&amp;rsquo;的，它们的共同属性是&amp;rdquo;同属于一个集合&amp;rdquo;。数据结构中的集合关系就类似于数学中的集合。&lt;/p&gt;

&lt;p&gt;在物理结构上也是单独存在的点，没有什么关系。&lt;/p&gt;

&lt;h2 id=&#34;线性数据结构&#34;&gt;线性数据结构&lt;/h2&gt;

&lt;h3 id=&#34;线性表&#34;&gt;线性表&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;数组&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数组其实是一段连续的内存，通过唯一索引下标（由于地址也是连续的，所以下标根据地址循序来就行，不需要存）来获取对应内存的值。&lt;/p&gt;

&lt;p&gt;数组可以说是最基本的数据结构，可以实现很多的复杂的数据结构，比如队列，栈，树，图等，只不过有些数据结构使用数组会造成很多的资源空间的浪费。选择使用了链式来实现，在计算机中就是这两种基本结构。&lt;/p&gt;

&lt;p&gt;0、数组寻址的原理&lt;/p&gt;

&lt;p&gt;内存分为了堆内存和栈内存，当初始化数组时，堆内存分配相应大小的连续的内存块，并将第一个内存块的地址放入栈内存中存储。这样读取数据的时候取第0个就是首地址的内存中的数据，第1个就是首地址+1的内存块中数据。其余删除与写入操作与读取类似。&lt;/p&gt;

&lt;p&gt;当定义一个数组a时，编译器根据指定的元素个数和元素的类型分配确定大小（元素类型大小×元素个数）的一块内存，并把这块内存的名字命名为 a，名字 a 一旦与这块内存匹配就不能再改变。其中，a[0]、a[1]、a[2]、a[3] 与 a[4] 都为 a 的元素，但并非元素的名字（数组的每一个元素都是没有名字的）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/array.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在 32 位系统中，由于 int 类型的数据占 4 字节单元，因此该数组 a 在内存中共占据连续的 4×5=20 字节单元，依次保存 a[0]、a[1]、a[2]、a[3] 与 a[4] 共 5 个元素。如果这里假设元素 a[0] 的地址是 10000，则元素 a[1] 的地址是 10000+1×4=10004; 元素 a[2] 的地址是 10000+2×4=10008; 元素 a[3] 的地址是 10000+3×4=10012; 元素 a[4] 的地址是 10000+4×4=10016。&lt;/p&gt;

&lt;p&gt;由此可见，数组的存储具有如下特点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;索引从 0 开始。
数组在内存中占据连续的字节单元。
数组占据的字节单元数等于数组元素个数乘以该数组所属数据类型的数据占据的字节单元数（元素个数乘以元素类型大小）。
数组元素按顺序连续存放。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、代码表示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[]int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;两数之和（暴力遍历，找到相加相等返回index，还可以使用map）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func twoSum(nums []int, target int) []int {
    var index []int
    for i:=0;i&amp;lt;len(nums);i++{
        temp := target - nums[i]
        for j:=0;j&amp;lt;len(nums);j++{
            if j!=i{
                if temp == nums[j]{
                    index = append(index,i)
                    index = append(index,j)
                    return index
                }
            }
        }
    }
    return index
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、golang中数组array和slice的实现&lt;/p&gt;

&lt;p&gt;golang数组也是一样的，一段连续的内存，连续的地址来实现来数组的存储，但是Go语言的数组又不同于C语言或者其他语言的数组，C语言的数组变量是指向数组第一个元素的指针；而Go语言的数组是一个值，Go语言中的数组是值类型，一个数组变量就表示着整个数组，意味着Go语言的数组在传递的时候，传递的是原数组的拷贝。&lt;/p&gt;

&lt;p&gt;slice就是一个指向数组的指针。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;稀疏数组&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;稀疏数组就是我们在使用数组来存储数据结构的时候，数组中有很多的空间是浪费的，比如一个二维数组中只存储来几个数字，我们可以将有数据的的位置的坐标和数值用我们正常的数组来存储，这样可以节省很多的空间，这个数组就是稀疏数组，其实这也是压缩的一种方式。&lt;/p&gt;

&lt;p&gt;1、实现即应用，用于存储压缩数据（定义一个结构体存储数据的位置和值，然后存放到数组中去）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
)

type ValNode struct {
    row int
    col int
    val int
}

func main() {
    //一、先创建一个原始数组
    var chessMap [11][11]int
    chessMap[1][2] = 1 //黑子
    chessMap[2][3] = 2 //蓝子

    //输出原始数组
    for _, v := range chessMap {
        for _, v2 := range v {
            fmt.Printf(&amp;quot;%d\t&amp;quot;, v2)
        }
        fmt.Println()
    }

    //二、转成稀疏数组
    //1、遍历chessMap，如果发现有一个元素的值不为0，创建一个node结构体
    //2、将其放入到对应的切片中即可
    var sparseArr []ValNode
    //标准的一个稀疏数组应该还有一个记录元素的二维数组的规模（行和列，默认值）
    //创建一个ValNode值节点
    valNode := ValNode{
        row: 11,
        col: 11,
        val: 0,
    }
    sparseArr = append(sparseArr, valNode)
    //遍历
    for i, v := range chessMap {
        for j, v2 := range v {
            if v2 != 0 {
                //创建一个ValNode值节点
                valNode := ValNode{
                    row: i,
                    col: j,
                    val: v2,
                }
                sparseArr = append(sparseArr, valNode)
            }
        }
    }
    //输出稀疏数组
    fmt.Println(&amp;quot;当前的稀疏数组是：&amp;quot;)
    for i, valNode := range sparseArr {
        fmt.Printf(&amp;quot;%d: %d %d %d\n&amp;quot;, i, valNode.row, valNode.col, valNode.val)
    }
    //三、恢复原始的数组
    //1、这里使用稀疏数组恢复
    //2、先创建一个原始数组
    var chessMap2 [11][11]int
    //遍历稀疏数组(文件的每一行)
    for i, valNode := range sparseArr {
        if i != 0 { //跳过第一行的数据
            chessMap2[valNode.row][valNode.col] = valNode.val
        }
    }
    //输出chessMap2
    fmt.Println(&amp;quot;恢复后的原始数据：&amp;quot;)
    for _, v := range chessMap2 {
        for _, v2 := range v {
            fmt.Printf(&amp;quot;%d\t&amp;quot;, v2)
        }
        fmt.Println()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、压缩过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;bufio&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;strconv&amp;quot;
    &amp;quot;strings&amp;quot;
)

type nodeval struct {
    row int
    col int
    val interface{}
}

//文件读取转成原始数据
func ReadData(filename string) {
    file, err := os.OpenFile(filename, os.O_RDONLY, 0666)
    if err != nil {
        log.Fatalf(&amp;quot;%s&amp;quot;, err)
    }
    defer file.Close()
    bfrd := bufio.NewReader(file)
    var index = 0
    var arr [][]int
    for {
        line, err := bfrd.ReadBytes(&#39;\n&#39;)
        if err != nil {
            break
        }
        index++
        temp := strings.Split(string(line), &amp;quot; &amp;quot;)
        row, _ := strconv.Atoi(temp[0])
        col, _ := strconv.Atoi(temp[1])
        value, _ := strconv.Atoi(temp[2])
        if index == 1 {
            for i := 0; i &amp;lt; row; i++ {
                var arr_temp []int
                for j := 0; j &amp;lt; col; j++ {
                    arr_temp = append(arr_temp, value)
                }
                arr = append(arr, arr_temp)
            }
        }
        if index != 1 {
            arr[row][col] = value
        }
    }
    // 打印数据
    fmt.Println(&amp;quot;从磁盘读取后的数据&amp;quot;)
    for _, v := range arr {
        for _, v1 := range v {
            fmt.Printf(&amp;quot;%d\t&amp;quot;, v1)
        }
        fmt.Println()
    }
}

func main() {

    var chessmap [11][11]int
    chessmap[1][2] = 1
    chessmap[2][3] = 2

    // 看看原始数据
    for _, v := range chessmap {
        for _, v1 := range v {
            fmt.Printf(&amp;quot;%d\t&amp;quot;, v1)
        }
        fmt.Println()
    }

    // 转成稀疏数据
    var sparseArr []nodeval
    // 数据规模
    sparseArr = append(sparseArr, nodeval{
        row: 11,
        col: 11,
        val: 0,
    })
    //稀疏数组
    for row, val := range chessmap {
        for col, val1 := range val {
            if val1 != 0 {
                sparseArr = append(sparseArr, nodeval{
                    row: row,
                    col: col,
                    val: val1,
                })
            }
        }
    }
    // 稀疏数组存盘
    filepath := &amp;quot;c:/test.txt&amp;quot;
    file, err := os.OpenFile(filepath, os.O_WRONLY|os.O_CREATE, 0666)
    if err != nil {
        fmt.Printf(&amp;quot;open file err=%v\n&amp;quot;, err)
    }
    defer file.Close()
    writer := bufio.NewWriter(file)
    for _, node := range sparseArr {
        str := fmt.Sprintf(&amp;quot;%d %d %d \n&amp;quot;, node.row, node.col, node.val)
        writer.WriteString(str)
    }
    writer.Flush()

    // 稀疏数据从磁盘读取转换成原始数据
    ReadData(filepath)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;将这个稀疏数组，存盘  &lt;/li&gt;
&lt;li&gt;打开这个文件，恢复原始数组&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;稀疏矩阵&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实就是上面转化的矩阵，在矩阵中，若数值为0的元素数目远远多于非0元素的数目，并且非0元素分布没有规律时，则称该矩阵为稀疏矩阵；与之相反，若非0元素数目占大多数时，则称该矩阵为稠密矩阵。定义非零元素的总数比上矩阵所有元素的总数为矩阵的稠密度。&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;p&gt;其实就是一个二维数组&lt;/p&gt;

&lt;p&gt;2、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;稀疏矩阵的一个典型应用就是稀疏数组，用于数据的压缩存储。在上面已经实现。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;对称矩阵&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对称矩阵（Symmetric Matrices）是指以主对角线为对称轴，各元素对应相等的矩阵。 在线性代数中，对称矩阵是一个方形矩阵，其转置矩阵和自身相等&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、对称矩阵是一个方阵，即就是行和列长度相等
2、对称矩阵中的所有元素都是相互对称的，即就是矩阵的下三角和上三角是对称的
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;常规矩阵&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;常规矩阵就是一个矩阵，矩阵（Matrix）是一个按照长方阵列排列的复数或实数集合。&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;p&gt;其实就是一个二维数组，在二维数组上的算法的应用还是比较多的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[][]int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;矩阵转置（矩阵反转）（矩阵的转置是指将矩阵的主对角线翻转，交换矩阵的行索引与列索引。所以只要获取一列重下往上的数据，放到对应的行中就可以，依次类推）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func transpose(A [][]int) [][]int { // 867. 转置矩阵
    if len(A) == 0 {
        return A
    }

    rlength := len(A)                // 转置矩阵的长
    rwidth := len(A[0])              // 转置矩阵的宽
    results := make([][]int, rwidth) // 返回转置矩阵
    for i := 0; i &amp;lt; rwidth; i++ {    // 分配二维矩阵空间
        results[i] = make([]int, rlength)
    }

    for r, row := range A { // 遍历当前矩阵
        for c, _ := range row {
            results[c][r] = A[r][c]
        }
    }
    return results
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;矩阵旋转（90度，就是先转置也就是沿着主对角线反转，在上下反转，就是进行对应位置值的互换）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func rotate(matrix [][]int) {
    n := len(matrix)
    // 水平翻转
    for i := 0; i &amp;lt; n/2; i++ {
        matrix[i], matrix[n-1-i] = matrix[n-1-i], matrix[i]
    }
    // 主对角线翻转
    for i := 0; i &amp;lt; n; i++ {
        for j := 0; j &amp;lt; i; j++ {
            matrix[i][j], matrix[j][i] = matrix[j][i], matrix[i][j]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实这个原理还是源于一个规律：对于矩阵中第 ii 行的第 jj 个元素，在旋转后，它出现在倒数第 ii 列的第 jj 个位置。根据这个原理我们本身就能解决&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func rotate(matrix [][]int) {
    n := len(matrix)
    tmp := make([][]int, n)
    for i := range tmp {
        tmp[i] = make([]int, n)
    }
    for i, row := range matrix {
        for j, v := range row {
            tmp[j][n-1-i] = v
        }
    }
    copy(matrix, tmp) // 拷贝 tmp 矩阵每行的引用
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;矩阵螺旋输出（模拟路径，当需要边缘或在之前走过的点的时候，方向就顺时针旋转一下。定义方向指针和四个方向的走过的数据）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func spiralOrder(matrix [][]int) []int {
    if len(matrix) == 0 || len(matrix[0]) == 0 {
        return []int{}
    }
    rows, columns := len(matrix), len(matrix[0])
    visited := make([][]bool, rows)
    for i := 0; i &amp;lt; rows; i++ {
        visited[i] = make([]bool, columns)
    }

    var (
        total = rows * columns
        order = make([]int, total)
        row, column = 0, 0
        directions = [][]int{[]int{0, 1}, []int{1, 0}, []int{0, -1}, []int{-1, 0}}
        directionIndex = 0
    )

    for i := 0; i &amp;lt; total; i++ {
        order[i] = matrix[row][column]
        visited[row][column] = true
        nextRow, nextColumn := row + directions[directionIndex][0], column + directions[directionIndex][1]
        if nextRow &amp;lt; 0 || nextRow &amp;gt;= rows || nextColumn &amp;lt; 0 || nextColumn &amp;gt;= columns || visited[nextRow][nextColumn] {
            directionIndex = (directionIndex + 1) % 4
        }
        row += directions[directionIndex][0]
        column += directions[directionIndex][1]
    }
    return order
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最小路径和（&lt;a href=&#34;https://kingjcy.github.io/post/computerbase/algorithm/algotithm/#动态规划法&#34;&gt;动态规划&lt;/a&gt;，求出所有的矩阵节点的最小路径和）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func minPathSum(grid [][]int) int {
    colume := len(grid[0])
    row := len(grid)

    dp := make([][]int,row)
    for i:=0;i&amp;lt;row;i++{
        dp[i]=make([]int, colume)
    }

    dp[0][0] = grid[0][0]

    for i:=1;i&amp;lt;row;i++{
        dp[i][0] = dp[i-1][0] + grid[i][0]
    }

    for i:=1;i&amp;lt;colume;i++{
        dp[0][i] = dp[0][i-1] + grid[0][i]
    }


    for i:=1;i&amp;lt;row;i++{
        for j:=1;j&amp;lt;colume;j++{
            dp[i][j] = min(dp[i-1][j],dp[i][j-1]) + grid[i][j]
        }
    }

    return dp[row-1][colume-1]
}


func min(a,b int)int{
    if a &amp;lt; b {
        return a
    }
    return b
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;扫雷&lt;/p&gt;

&lt;p&gt;给定一个代表游戏板的二维字符矩阵。 &amp;rsquo;M&amp;rsquo; 代表一个未挖出的地雷，&amp;rsquo;E&amp;rsquo; 代表一个未挖出的空方块，&amp;rsquo;B&amp;rsquo; 代表没有相邻（上，下，左，右，和所有4个对角线）地雷的已挖出的空白方块，数字（&amp;rsquo;1&amp;rsquo; 到 &amp;lsquo;8&amp;rsquo;）表示有多少地雷与这块已挖出的方块相邻，&amp;rsquo;X&amp;rsquo; 则表示一个已挖出的地雷。&lt;/p&gt;

&lt;p&gt;现在给出在所有未挖出的方块中（&amp;rsquo;M&amp;rsquo;或者&amp;rsquo;E&amp;rsquo;）的下一个点击位置（行和列索引），根据以下规则，返回相应位置被点击后对应的面板：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果一个地雷（&amp;rsquo;M&amp;rsquo;）被挖出，游戏就结束了- 把它改为 &amp;rsquo;X&amp;rsquo;。&lt;/li&gt;
&lt;li&gt;如果一个没有相邻地雷的空方块（&amp;rsquo;E&amp;rsquo;）被挖出，修改它为（&amp;rsquo;B&amp;rsquo;），并且所有和其相邻的未挖出方块都应该被递归地揭露。&lt;/li&gt;
&lt;li&gt;如果一个至少与一个地雷相邻的空方块（&amp;rsquo;E&amp;rsquo;）被挖出，修改它为数字（&amp;rsquo;1&amp;rsquo;到&amp;rsquo;8&amp;rsquo;），表示相邻地雷的数量。&lt;/li&gt;
&lt;li&gt;如果在此次点击中，若无更多方块可被揭露，则返回面板。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;模拟&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当前点击的是「未挖出的地雷」，我们将其值改为 X 即可。&lt;/li&gt;
&lt;li&gt;当前点击的是「未挖出的空方块」，我们需要统计它周围相邻的方块里地雷的数量 cnt（即 M 的数量）。如果 cnt 为零，即执行规则 22，此时需要将其改为 B，且递归地处理周围的八个未挖出的方块，递归终止条件即为规则 44，没有更多方块可被揭露的时候。否则执行规则 33，将其修改为数字即可。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var dirX = []int{0, 1, 0, -1, 1, 1, -1, -1}
var dirY = []int{1, 0, -1, 0, 1, -1, 1, -1}

func updateBoard(board [][]byte, click []int) [][]byte {
    x, y := click[0], click[1]
    if board[x][y] == &#39;M&#39; {
        board[x][y] = &#39;X&#39;
    } else {
        dfs(board, x, y)
    }
    return board
}

func dfs(board [][]byte, x, y int) {
    cnt := 0
    for i := 0; i &amp;lt; 8; i++ {
        tx, ty := x + dirX[i], y + dirY[i]
        if tx &amp;lt; 0 || tx &amp;gt;= len(board) || ty &amp;lt; 0 || ty &amp;gt;= len(board[0]) {
            continue
        }
        // 不用判断 M，因为如果有 M 的话游戏已经结束了
        if board[tx][ty] == &#39;M&#39; {
            cnt++
        }
    }
    if cnt &amp;gt; 0 {
        board[x][y] = byte(cnt + &#39;0&#39;)
    } else {
        board[x][y] = &#39;B&#39;
        for i := 0; i &amp;lt; 8; i++ {
            tx, ty := x + dirX[i], y + dirY[i]
            // 这里不需要在存在 B 的时候继续扩展，因为 B 之前被点击的时候已经被扩展过了
            if tx &amp;lt; 0 || tx &amp;gt;= len(board) || ty &amp;lt; 0 || ty &amp;gt;= len(board[0]) || board[tx][ty] != &#39;E&#39; {
                continue
            }
            dfs(board, tx, ty)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;hash表&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/computerbase/algorithm/algotithm/#哈希表&#34;&gt;hash表&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;链表&#34;&gt;链表&lt;/h3&gt;

&lt;p&gt;其实链表也是一种基本的数据结构，因为在计算机中只有两种存储方式，所以这两种数据可以说是数据结构的基本单位，可以实现很多的复杂的数据结构，比如队列，栈，树，图等，其实大多数都是使用链表来实现的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;单链表&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、头节点和头指针&lt;/p&gt;

&lt;p&gt;头结点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;头结点是加在单链表之前附设的一个头结点。
头结点的数据域一般不存储任何信息，也可以存放一些关于线性表的长度的附加信息。
头结点的指针域存放指向第一个结点的指针(即第一个结点的存储位置)。
头结点不一定是链表的必要元素。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;头指针：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;头指针是指链表指向第一个结点的指针，若链表有头结点，则是指向头结点的指针。
头指针具有标识作用，所以常用头指针冠以链表的名字(指针变量的名字)。
无论链表是否为空，头指针均不为空。
头指针是链表的必要元素。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;单链表中最后一个结点的指针域为空(NULL)。&lt;/p&gt;

&lt;p&gt;2、代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//数据
type Object interface{}

//节点
type Node struct {
    data Object
    next *Node
}

//初始化一个链表就是新建一个头节点，链表的主要操作还是增删改查。
var head Node = Node{Data: nil, next: nil}

//我们还可以建立一个链表的结构体来实现
type List struct{
    mutex *sync.RWMutex
    Size uint
    Head *Node
    Tail *Node
}

//初始化就是
func (list *List) Init() {
    (*list).size = 0    // 此时链表是空的
    (*list).head = nil  // 没有车头
    (*list).tail = nil  // 没有车尾
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;两数相加（两个数用链表表示，按位相加，主要是超过十进制各种情况的考虑）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for singly-linked list.
 * type ListNode struct {
 *     Val int
 *     Next *ListNode
 * }
 */
func addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode {
    l := &amp;amp;ListNode{}
    head := l
    var up = 0
    for l1 != nil &amp;amp;&amp;amp; l2 != nil {
        tmp := new(ListNode)
        tmp.Val = (l1.Val +l2.Val + up) % 10
        up = (l1.Val +l2.Val + up) / 10

        l.Next = tmp

        l = l.Next
        l1 = l1.Next
        l2 = l2.Next

    }


    for l1 != nil {
        tmp := new(ListNode)
        tmp.Val = (l1.Val + up) % 10
        up = (l1.Val + up) / 10

        l.Next = tmp

        l = l.Next
        l1 = l1.Next
    }

    for l2 != nil {
        tmp := new(ListNode)
        tmp.Val = (l2.Val + up) % 10
        up = (l2.Val + up) / 10

        l.Next = tmp

        l = l.Next
        l2 = l2.Next
    }

    if up != 0 {
        tmp := new(ListNode)
        tmp.Val = up
        l.Next = tmp
    }

    return head.Next
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;链表反转(新定义两个指针进行遍历改变真正的指向)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for singly-linked list.
 * type ListNode struct {
 *     Val int
 *     Next *ListNode
 * }
 */
func reverseList(head *ListNode) *ListNode {
    if head == nil {
        return nil
    }

    pre,cur := head,head.Next
    for cur != nil{
        temp := cur.Next
        cur.Next = pre
        pre = cur
        cur = temp
    }

    return pre
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;链表部分反转（找到反转的位置，保留一个不反转的节点和反转的头节点，然后就是定义两个指针来反转，最后连接）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for singly-linked list.
 * type ListNode struct {
 *     Val int
 *     Next *ListNode
 * }
 */
func reverseBetween(head *ListNode, m int, n int) *ListNode {
    if head == nil {
        return head
    }

    dummy := &amp;amp;ListNode{Val: 0, Next: head}
    // 找到需要反转节点在链表中的索引
    firstIndex, endIndex := m-1, n-1

    // pre 指向需要反转节点的前一个节点
    var pre *ListNode = dummy
    for i := 1; i &amp;lt;= m - 1; i++ {
        pre = pre.Next
    }

    // newHead 节点指向反转部分完成后的头节点
    var newHead *ListNode
    // cur 指向反转部分完成前的下一个节点
    cur := pre.Next
    for cur != nil &amp;amp;&amp;amp; firstIndex &amp;lt;= endIndex {
        temp := cur.Next
        cur.Next = newHead
        newHead = cur
        cur = temp
        firstIndex++
    }

    // 关键点在于 pre 节点的下一个节点是反转后的最后那个节点，需要与 cur 节点进行连接
    // 然后 pre 节点与反转后的 newHead 节点进行连接
    pre.Next.Next, pre.Next = cur, newHead

    return dummy.Next
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;寻找链表相交点（开两个指针分别遍历这两个链表，在第一次遍历到尾部的时候，指向另一个链表头部继续遍历，这样会抵消长度差。如果链表有相交，那么会在中途相等，返回相交节点；如果链表不相交，那么最后会 nil == nil，返回 nil；）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func getIntersectionNode(headA, headB *ListNode) *ListNode {
    curA,curB := headA,headB
    for curA != curB {
        if curA == nil {    // 如果第一次遍历到链表尾部，就指向另一个链表的头部，继续遍历，这样会抵消长度差。如果没有相交，因为遍历长度相等，最后会是 nil ==  nil
            curA = headB
        } else {
            curA = curA.Next
        }
        if curB == nil {
            curB = headA
        } else {
            curB = curB.Next
        }
    }
    return curA
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;链表合并（遍历两个链表，将较小的数字插入到新链表中）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func mergeTwoLists(l1 *ListNode, l2 *ListNode) *ListNode {
    prehead := &amp;amp;ListNode{}
    result := prehead
    for l1 != nil &amp;amp;&amp;amp; l2 != nil {
        if l1.Val &amp;lt; l2.Val {
            prehead.Next = l1
            l1 = l1.Next
        }else{
            prehead.Next = l2
            l2 = l2.Next
        }
        prehead = prehead.Next
    }
    if l1 != nil {
        prehead.Next = l1
    }
    if l2 != nil {
        prehead.Next = l2
    }
    return result.Next
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;链表分区（双指针法，把大于x的值放入到一个链表中，小于x的值放到一个链表中，都是按着原来的顺序，然后把两张表相连接）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func partition(head *ListNode, x int) *ListNode {
    l1 := &amp;amp;ListNode{Val:0}
    l2 := &amp;amp;ListNode{Val:0}
    l1_pre := l1
    l2_pre := l2
    for head != nil {
        if head.Val &amp;lt; x {
            l1.Next = &amp;amp;ListNode{Val: head.Val}
            l1 = l1.Next
        } else {
            l2.Next = &amp;amp;ListNode{Val: head.Val}
            l2 = l2.Next
        }
        head = head.Next
    }
    l1_pre = l1_pre.Next
    l2_pre = l2_pre.Next
    if l1_pre == nil {
        return l2_pre
    }
    result := l1_pre
    for l1_pre.Next != nil {
        l1_pre = l1_pre.Next
    }
    l1_pre.Next = l2_pre
    return result
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;复杂链表的复制（深度拷贝）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;浅拷贝只复制指向某个对象的指针，而不复制对象本身，新旧对象还是共享同一块内存。但深拷贝会另外创造一个一模一样的对象，新对象跟原对象不共享内存，修改新对象不会改到原对象。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;先复制节点，在重新变量找到random的节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a Node.
 * type Node struct {
 *     Val int
 *     Next *Node
 *     Random *Node
 * }
 */

func copyRandomList(head *Node) *Node {
    if head == nil {
        return nil
    }
    newHead := Node{
        Val:    head.Val,
        Next:   nil,
        Random: nil,
    }
    p := head.Next
    pre := &amp;amp;newHead
    for p != nil {
        newNode := &amp;amp;Node{
            Val:    p.Val,
            Next:   nil,
            Random: nil,
        }
        pre.Next = newNode
        p = p.Next
        pre = pre.Next
    }
    p = head
    newP := &amp;amp;newHead
    for p != nil {
        if p.Random != nil {
            step := findStep(head, p.Random)
            newP.Random = target(&amp;amp;newHead, step)
        }
        p = p.Next
        newP = newP.Next
    }
    return &amp;amp;newHead
}

//确定从头结点到目标节点所经过的节点数
func findStep(head, target *Node) int {
    p := head
    step := 0
    for p != target {
        p = p.Next
        step++
    }
    return step
}
//返回从头结点开始，走step步所到达的节点
func target(head *Node, step int) *Node {
    p := head
    for step &amp;gt; 0 {
        p = p.Next
        step--
    }
    return p
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在第一种方法的基础上，通过将老节点和新节点相互对应的关系保存在map中，来找到random的节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a Node.
 * type Node struct {
 *     Val int
 *     Next *Node
 *     Random *Node
 * }
 */

func copyRandomList(head *Node) *Node {
    if head == nil {
        return nil
    }
    newHead := Node{
        Val:    head.Val,
        Next:   nil,
        Random: nil,
    }
    p := head.Next
    pre := &amp;amp;newHead
    connection := make(map[*Node]*Node)
    connection[head] = pre
    for p != nil {
        newNode := &amp;amp;Node{
            Val:    p.Val,
            Next:   nil,
            Random: nil,
        }
        pre.Next = newNode
        connection[p] = newNode
        p = p.Next
        pre = pre.Next
    }
    p = head
    newP := &amp;amp;newHead
    for p != nil {
        if p.Random != nil {
            newP.Random = connection[p.Random]
        }
        p = p.Next
        newP = newP.Next
    }
    return &amp;amp;newHead
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;双向链表&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在单链表的每个结点中，再设置一个指向其前驱结点的指针域。所以在双向链表中的结点都有两个指针域，一个指向直接后继，另一个指向直接前驱。&lt;/p&gt;

&lt;p&gt;1、代码表示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 节点数据
type DoubleObject interface{}

// 双链表节点
type DoubleNode struct {
    Data DoubleObject
    Prev *DoubleNode
    Next *DoubleNode
}

//初始化依然是建立一个head节点
var head Node = Node{Data: nil, Prev: nil，Next: nil,}

// 双链表
type DoubleList struct{
    mutex *sync.RWMutex
    Size uint
    Head *DoubleNode
    Tail *DoubleNode
}

//初始化和单链表也是一致的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;二叉搜索树转换双向循环链表，主要是下面三块&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;构建&lt;a href=&#34;https://kingjcy.github.io/post/computerbase/datastruct/datastruct/#二叉查找树&#34;&gt;二叉搜索树&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/computerbase/datastruct/datastruct/#树&#34;&gt;中序遍历&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;中序遍历二叉搜索树是从小到大的循序，在每个节点的改变指针的指向，左指针表示双链表向前指，右指针表示双链表向后指&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;

//将BST转化为双向循环链表，不允许新建节点
//为防止歧义，左指针表示双链表向前指，右指针表示双链表向后指
type TreeNode struct {
    Val   int
    Left  *TreeNode
    Right *TreeNode
}

var pre *TreeNode //必须在全局变量上才可以实现

//其实就是中序遍历
func treeToDoublyList(root *TreeNode) *TreeNode {
    if root == nil {
        return root
    }
    helper(root)
    //处理首位指针
    head, tail := root, root
    for head.Left != nil {
        head = head.Left
    }
    for tail.Right != nil {
        tail = tail.Right
    }
    head.Left = tail
    tail.Right = head
    return head
}

//中序遍历
func helper(root *TreeNode) {
    if root == nil {
        return
    }
    helper(root.Left)
    //改变指针
    if pre != nil {
        root.Left = pre
        pre.Right = root
    }
    //获取当前节点
    pre = root
    helper(root.Right)
}

func main() {
    root := &amp;amp;TreeNode{4, nil, nil}
    node1 := &amp;amp;TreeNode{2, nil, nil}
    node2 := &amp;amp;TreeNode{5, nil, nil}
    node3 := &amp;amp;TreeNode{1, nil, nil}
    node4 := &amp;amp;TreeNode{3, nil, nil}
    root.Left = node1
    root.Right = node2
    node1.Left = node3
    node1.Right = node4
    //上面是比较直观的，我们也可以使用二叉搜索树的特性来构建一颗二叉搜索树，具体可以看二叉搜索树。
    head := treeToDoublyList(root)
    tail := head.Left
    //从头开始遍历
    for i := 0; i &amp;lt;= 9; i++ {
        fmt.Printf(&amp;quot;%d\t&amp;quot;, head.Val)
        head = head.Right
    }
    //从尾开始遍历
    for i := 0; i &amp;lt;= 9; i++ {
        fmt.Printf(&amp;quot;%d\t&amp;quot;, tail.Val)
        tail = tail.Left
    }

}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、标准库list的实现&lt;/p&gt;

&lt;p&gt;就是定义好了如上的结构体，实现了双向链表，写好了增删改查的api，就是&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-container-list/&#34;&gt;标准库的实现&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;静态链表&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;静态链表就是不使用指针，而使用下标来实现单链表。&lt;/p&gt;

&lt;p&gt;用数组描述的链表叫做静态链表。C语言中，让数组的元素都是由两个数据域组成，data和cur。数组的每个下标都对应着一个data和一个cur。数据域data，用来存放数据元素，也就是要处理的数据；而cur相当于单链表中的next指针，存放该元素的后继在数据中的下标，把cur叫游标。另外，数组的第一个和最后一个元素作为特殊元素处理，不存数据。数组的第一个元素，即下标为0的元素的cur存放备用链表（未被使用的数组元素）第一个结点的下标，而数组的最后一个元素的cur则存放第一个有数值的元素的下标，相当于单链表中的头结点的作用。&lt;/p&gt;

&lt;p&gt;在高级语言的今天，基本上不实用了。&lt;/p&gt;

&lt;p&gt;代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//静态链表节点
type Node struct{
    data string
    cursor int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基本不实用了，了解一下就好&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;循环链表&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将单链表中终端结点的指针端由空指针改为指向头结点，就是整个单链表形成一个环，这种头尾相接的单链表称为单循环链表，简称循环链表。&lt;/p&gt;

&lt;p&gt;循环链表和单链表的主要差异就在于循环的判断条件上，原来是判断p-&amp;gt;next是否为空，现在则是判断p-&amp;gt;nex不等于头结点，则循环未结束。不在需要head节点&lt;/p&gt;

&lt;p&gt;为了用O(1)的时间由链表指针访问到最后一个结点，可以采用这样的方法：不使用头指针（head），而是用指向终端结点的尾指针来表示循环链表，此时查找开始结点和终端结点都很方便了。即终端结点用尾指针rear指示，而头结点就是rear-&amp;gt;next，开始结点就是rear-&amp;gt;next-&amp;gt;next。&lt;/p&gt;

&lt;p&gt;双向链表也可以有循环链表，即对于某一结点p，它的后继的前驱是它自己，它的前驱的后继还是它自己：p-&amp;gt;next-&amp;gt;prior = p = p-&amp;gt;prior-&amp;gt;next。&lt;/p&gt;

&lt;p&gt;1、代码实现&lt;/p&gt;

&lt;p&gt;其实循环链表的的单还是双的结构体都是正常的一样的，只不过最后操作的时候需要形成一个循环，就是指向头指针，至于初始化，可以指向自己，可以不指向自己。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;单链
    //数据
    type Object interface{}

    //节点
    type CNode struct {
        data Object
        next *CNode
    }

    //正常初始化
    var head CNode = Node{Data: nil, next: nil}

    //定一个结构体，这种属于正常使用，会带链表的大小
    type CList struct {
        size uint64    // 车厢数量
        head *CNode    // 车头
    }

    //初始化
    func (cList *CList) Init() {
        lst := *cList
        lst.size = 0    // 没车厢
        lst.head = nil  // 没车头
    }


双链

    //结点
    type Node struct {
        Data ElemType
        Pre  *Node
        Next *Node
    }

    //链表
    type List struct {
        First *Node
        Last  *Node
        Size  int
    }

    //工厂函数
    func CreateList() *List {
        s := new(Node)
        s.Next, s.Pre = s, s
        return &amp;amp;List{s, s, 0}
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;判断链表中是否有环&lt;/p&gt;

&lt;p&gt;map（key重复说明有环）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func hasCycle(head *ListNode) bool {    // hash表
    hash := make(map[*ListNode]int)     // 开一个哈希表记录该节点是否已经遍历过，值记录节点索引
    for head != nil {
        if _,ok := hash[head]; ok {     // 该节点遍历过，形成了环
            return true
        }
        hash[head] = head.Val           // 记录该节点已经遍历过
        head = head.Next
    }
    return false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;快慢指针一（快指针一次走两步，慢指针一次走一步，如果链表有环，那么两个指针始终会相遇。）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func hasCycle(head *ListNode) bool {    // 快慢指针。假如爱有天意，那么快慢指针终会相遇
    if head == nil {
        return false
    }
    fastHead := head.Next       // 快指针，每次走两步
    for fastHead != nil &amp;amp;&amp;amp; head != nil &amp;amp;&amp;amp; fastHead.Next != nil {
        if fastHead == head {   // 快慢指针相遇，表示有环
            return true
        }
        fastHead = fastHead.Next.Next
        head = head.Next        // 慢指针，每次走一步
    }
    return false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;快慢指针二（两个指针，一个一步步遍历，一个每次重头开始遍历，出现步数不一样的时候就存在环。）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for singly-linked list.
 * type ListNode struct {
 *     Val int
 *     Next *ListNode
 * }
 */
func hasCycle(head *ListNode) bool {
    quick,slow := head,head
    n,m := 0,0
    for {
        if quick == nil {
            return false
        }

        if quick.Next != nil {
            quick = quick.Next
            n++
        }else{
            return false
        }

        for{
            if slow.Val == quick.Val{
                if m==n {
                    slow = head
                    m = 0
                    break
                }else{
                    return true
                }
            }else{
                slow = slow.Next
                m++
            }
        }

    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、ring库的实现&lt;/p&gt;

&lt;p&gt;就是一个双向循环链表的实现，封装了很多的API，就是&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-container-ring/&#34;&gt;标准库的实现&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;栈&#34;&gt;栈&lt;/h3&gt;

&lt;p&gt;栈是数据按照后进先出 LIFO(Last-In-First-Out) 原则组成的集合。添加和移除元素都是在栈顶进行，类比书堆，不能在栈底增删元素。&lt;/p&gt;

&lt;p&gt;其实栈就是一种特殊的线性表，只不过遵守着特殊的规则，可以使用数组和链表来实现。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;链表实现方式&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang中有标准库list，所以可以省去了链表的定义和创建模块。&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//栈
type Stack struct {
    list *list.List
    lock *sync.RWMutex
}

//初始化
func NewStack() *Stack {
    list := list.New()
    l := &amp;amp;sync.RWMutex{}
    return &amp;amp;Stack{list, l}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;container/list&amp;quot;
    &amp;quot;sync&amp;quot;
)

type Stack struct {
    list *list.List
    lock *sync.RWMutex
}

func NewStack() *Stack {
    list := list.New()
    l := &amp;amp;sync.RWMutex{}
    return &amp;amp;Stack{list, l}
}

func (stack *Stack) Push(value interface{}) {
    stack.lock.Lock()
    defer stack.lock.Unlock()
    stack.list.PushBack(value)
}

func (stack *Stack) Pop() interface{} {
    stack.lock.Lock()
    defer stack.lock.Unlock()
    e := stack.list.Back()
    if e != nil {
        stack.list.Remove(e)
        return e.Value
    }
    return nil
}

func (stack *Stack) Peak() interface{} {
    e := stack.list.Back()
    if e != nil {
        return e.Value
    }

    return nil
}

func (stack *Stack) Len() int {
    return stack.list.Len()
}

func (stack *Stack) Empty() bool {
    return stack.list.Len() == 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例简化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import &amp;quot;container/list&amp;quot;

// 初始化
queue := list.New()
stack := list.New()

// 入队 入栈
queue.PushBack(123)
stack.PushBack(123)

// 出队 出栈 返回的数据是结构类型 Value 需要断言成相应的类型
num1 = queue.Front()
queue.Remove(num1)

num2 = queue.Back()
stack.Remove(num2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、正常编码使用中，我都会直接用list来生产对应的栈和队列，其实list的实现也是上面我们使用的链表，所以我们可以使用链表原生实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;sync&amp;quot;
)

type (
    Stack struct {
        top    *node
        length int
        lock   *sync.RWMutex
    }
    node struct {
        value interface{}
        prev  *node
    }
)

// Create a new stack
func NewStack() *Stack {
    return &amp;amp;Stack{nil, 0, &amp;amp;sync.RWMutex{}}
}

// Return the number of items in the stack
func (this *Stack) Len() int {
    return this.length
}

// View the top item on the stack
func (this *Stack) Peek() interface{} {
    if this.length == 0 {
        return nil
    }
    return this.top.value
}

// Pop the top item of the stack and return it
func (this *Stack) Pop() interface{} {
    this.lock.Lock()
    defer this.lock.Unlock()
    if this.length == 0 {
        return nil
    }
    n := this.top
    this.top = n.prev
    this.length--
    return n.value
}

// Push a value onto the top of the stack
func (this *Stack) Push(value interface{}) {
    this.lock.Lock()
    defer this.lock.Unlock()
    n := &amp;amp;node{value, this.top}
    this.top = n
    this.length++
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;数组实现方式&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在实际项目中不要这么使用，这么做会带来内存泄漏的风险。那么这个场景用来干啥呢，刷刷 leetcode 题还是蛮方便的。一般来说会用链表来实现队列和栈，当然 golang 内置的 container/list 库提供了双向链表的数据结构。我们用这个也是很方便的。&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ItemStack struct {
    items []Item
    lock  sync.RWMutex
}

// New creates a new ItemStack
func NewStack() *ItemStack {
    s := &amp;amp;ItemStack{}
    s.items = []Item{}
    return s
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package stack

import (
    &amp;quot;github.com/cheekybits/genny/generic&amp;quot;
    &amp;quot;sync&amp;quot;
)

type Item generic.Type

type ItemStack struct {
    items []Item
    lock  sync.RWMutex
}

// 创建栈
func (s *ItemStack) New() *ItemStack {
    s.items = []Item{}
    return s
}

// 入栈
func (s *ItemStack) Push(t Item) {
    s.lock.Lock()
    s.items = append(s.items, t)
    s.lock.Unlock()
}

// 出栈
func (s *ItemStack) Pop() *Item {
    s.lock.Lock()
    item := s.items[len(s.items)-1]
    s.items = s.items[:len(s.items)-1 ]
    s.lock.Unlock()
    return &amp;amp;item
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用队列实现栈&lt;/p&gt;

&lt;p&gt;栈和队列都是用list实现的，本身提供了这些功能，就简单了，这个主要是一个思想，满足队列的特性，需要将最上面的值放到队头，栈实现队列也是一个原理，将队头放在栈顶。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import &amp;quot;container/list&amp;quot;

type stack struct{
    *list.List
}

func Init()stack{
    return stack{list.New()}
}

func(s *stack)peek() int{
    if s.Len() == 0 {
        return -1
    }
    return s.Back().Value.(int)
}

func(s *stack)push(x int){
    s.PushBack(x)
}

func(s *stack)pop()int {
    if s.Len() == 0 {
        return -1
    }
    v := s.Back()
    s.Remove(v)
    return v.Value.(int)


}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;栈是否是合法的出栈顺序&lt;/p&gt;

&lt;p&gt;模拟入栈出栈，按序列匹配出栈，看是否能得到这个序列，可以用list库，但是没有ide的时候可能使用slice更加好写一点。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import &amp;quot;container/list&amp;quot;

type stack struct{
    *list.List
}

func Init()stack{
    return stack{list.New()}
}

func(s *stack)peek() int{
    if s.Len() == 0 {
        return -1
    }
    return s.Back().Value.(int)
}

func(s *stack)push(x int){
    s.PushBack(x)
}

func(s *stack)pop()int {
    if s.Len() == 0 {
        return -1
    }
    v := s.Back()
    s.Remove(v)
    return v.Value.(int)


}

func validateStackSequences(pushed []int, popped []int) bool {
    lpush := len(pushed)
    lpop  := len(popped)

    if lpush == 0 {
        if lpop == 0 {
            return true
        }
        return false
    }

    stack := Init()
    var index = 0

    for i:=0;i&amp;lt;lpush;i++{
        stack.push(pushed[i])
        for stack.peek() == popped[index] {
            stack.pop()
            index++
            if index == lpop {
                return true
            }
        }
    }

    return false
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;获取最小值&lt;/p&gt;

&lt;p&gt;单独用一个栈来存储这个最小值，必须要是栈，因为在弹出的时候，可能弹出最小值，需要知道下一个最小值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type MinStack struct {
    stack    []int
    minstack []int
}

/** initialize your data structure here. */
func Constructor() MinStack {
    return MinStack{stack: make([]int, 0),
        minstack: make([]int, 0)}
}

func (this *MinStack) Push(x int) {
    this.stack = append(this.stack, x)
    if len(this.stack) == 1 {
        this.minstack = append(this.minstack, x)
    } else {
        if this.minstack[len(this.minstack)-1] &amp;gt;= x {
            this.minstack = append(this.minstack, x)
        }
    }
}

func (this *MinStack) Pop() {
    if len(this.stack) == 0 {
        return
    }
    if this.stack[len(this.stack)-1] == this.minstack[len(this.minstack)-1] {
        this.minstack = this.minstack[:len(this.minstack)-1]
    }
    this.stack = this.stack[:len(this.stack)-1]

}

func (this *MinStack) Top() int {
    return this.stack[len(this.stack)-1]
}

func (this *MinStack) GetMin() int {
    return this.minstack[len(this.minstack)-1]
}


/**
 * Your MinStack object will be instantiated and called as such:
 * obj := Constructor();
 * obj.Push(x);
 * obj.Pop();
 * param_3 := obj.Top();
 * param_4 := obj.GetMin();
 */
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;计算器&lt;/p&gt;

&lt;p&gt;中缀表达式转后缀表达式计算，转换有规则，计算有规则，详解看附录-&lt;a href=&#34;#用栈实现计算器&#34;&gt;计算器&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;队列&#34;&gt;队列&lt;/h3&gt;

&lt;p&gt;队列是数据按照先进先出 FIFO(First-In-First-Out) 原则组成的集合，类比排队，在队列任一端添加元素，从对应的另一端删除元素。&lt;/p&gt;

&lt;p&gt;其实队列也是一种特殊的线性表，数据结构和栈是一样的，只不过遵守着不同的特殊的规则，可以使用数组和链表来实现。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;链表实现方式&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个也可以直接使用标准库的list来实现，减去了链表的定义和创建&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Queue 队列信息
type Queue struct{
    list *list.List
    lock *sync.RWMutex
}

// Init 队列初始化
func (q *Queue)Init()  {
    list := list.New()
    l := &amp;amp;sync.RWMutex{}
    return &amp;amp;Queue{list, l}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;数组实现方式&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ItemQueue struct {
    items []Item
    lock  sync.RWMutex
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package queue

import (
    &amp;quot;github.com/cheekybits/genny/generic&amp;quot;
    &amp;quot;sync&amp;quot;
)

type Item generic.Type

type ItemQueue struct {
    items []Item
    lock  sync.RWMutex
}

// 创建队列
func (q *ItemQueue) New() *ItemQueue {
    q.items = []Item{}
    return q
}

// 如队列
func (q *ItemQueue) Enqueue(t Item) {
    q.lock.Lock()
    q.items = append(q.items, t)
    q.lock.Unlock()
}

// 出队列
func (q *ItemQueue) Dequeue() *Item {
    q.lock.Lock()
    item := q.items[0]
    q.items = q.items[1:len(q.items)]
    q.lock.Unlock()
    return &amp;amp;item
}

// 获取队列的第一个元素，不移除
func (q *ItemQueue) Front() *Item {
    q.lock.Lock()
    item := q.items[0]
    q.lock.Unlock()
    return &amp;amp;item
}

// 判空
func (q *ItemQueue) IsEmpty() bool {
    return len(q.items) == 0
}

// 获取队列的长度
func (q *ItemQueue) Size() int {
    return len(q.items)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、channel的实现&lt;/p&gt;

&lt;p&gt;我们一般都不需要自己去创建队列了，大部分都是使用封装好的通信机制channel&lt;/p&gt;

&lt;p&gt;channel是使用循环链表做缓存，中间有两个队列，分别是接受和发送，满了就会阻塞，我们可以看&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-channel/&#34;&gt;原生实现&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;4、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用栈实现队列，和用队列实现栈是一个思想&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type MyQueue struct {
    *list.List
}


/** Initialize your data structure here. */
func Constructor() MyQueue {
    return MyQueue{list.New()}
}


/** Push element x to the back of queue. */
func (this *MyQueue) Push(x int)  {
    this.PushBack(x)
}


/** Removes the element from in front of queue and returns that element. */
func (this *MyQueue) Pop() int {
    if this.Len() == 0 {
        return -1
    }
    v := this.Front()
    this.Remove(v)
    return v.Value.(int)
}


/** Get the front element. */
func (this *MyQueue) Peek() int {
    if this.Len() == 0 {
        return -1
    }
    return this.Front().Value.(int)
}


/** Returns whether the queue is empty. */
func (this *MyQueue) Empty() bool {
    return this.Len() == 0
}


/**
 * Your MyQueue object will be instantiated and called as such:
 * obj := Constructor();
 * obj.Push(x);
 * param_2 := obj.Pop();
 * param_3 := obj.Peek();
 * param_4 := obj.Empty();
 */
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;循环队列&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;循环队列其实也就是循环表，只不过我们有这么一个概念，了解一下。&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;数组&lt;/p&gt;

&lt;p&gt;结构定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type MyCircularQueue struct {
    arr    []int
    front  int
    rear   int
    maxCap int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;队空的判断条件是this.front == this.rear，队满的判断条件是(this.rear+1)%this.maxCap == this.front，通过%cap来完成rear循环的&lt;/li&gt;
&lt;li&gt;队满时，tail指向的位置上没有存储数据，会浪费一个空间，所以arr的大小为k+1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type MyCircularQueue struct {
    arr    []int
    front  int
    rear   int
    maxCap int
}

/** Initialize your data structure here. Set the size of the queue to be k. */
func Constructor(k int) MyCircularQueue {
    return MyCircularQueue{
        arr:    make([]int, k+1),
        front:  0,
        rear:   -1,
        maxCap: k+1,
    }
}

/** Insert an element into the circular queue. Return true if the operation is successful. */
func (this *MyCircularQueue) EnQueue(value int) bool {
    if this.IsFull() {
        return false
    }

    this.rear = (this.rear + 1) % this.maxCap
    this.arr[this.rear] = value
    return true
}

/** Delete an element from the circular queue. Return true if the operation is successful. */
func (this *MyCircularQueue) DeQueue() bool {
    if this.IsEmpty() {
        return false
    }

    this.front = (this.front + 1) % this.maxCap
    return true
}

/** Get the front item from the queue. */
func (this *MyCircularQueue) Front() int {
    if this.IsEmpty() {
        return -1
    }
    return this.arr[this.front]
}

/** Get the last item from the queue. */
func (this *MyCircularQueue) Rear() int {
    if this.IsEmpty() {
        return -1
    }
    return this.arr[this.rear]
}

/** Checks whether the circular queue is empty or not. */
func (this *MyCircularQueue) IsEmpty() bool {
    return this.front-1 == this.rear
}

/** Checks whether the circular queue is full or not. */
func (this *MyCircularQueue) IsFull() bool {
    return (this.rear+2)%this.maxCap == this.front
}

/**
 * Your MyCircularQueue object will be instantiated and called as such:
 * obj := Constructor(k);
 * param_1 := obj.EnQueue(value);
 * param_2 := obj.DeQueue();
 * param_3 := obj.Front();
 * param_4 := obj.Rear();
 * param_5 := obj.IsEmpty();
 * param_6 := obj.IsFull();
 */
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;链表&lt;/p&gt;

&lt;p&gt;结构其实就是循环链表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type queueNode struct {
    val  int
    next *queueNode
}

type MyCircularQueue struct {
    header   *queueNode
    tail     *queueNode
    len      int
    capacity int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完整实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type queueNode struct {
    val  int
    next *queueNode
}

type MyCircularQueue struct {
    header   *queueNode
    tail     *queueNode
    len      int
    capacity int
}

/** Initialize your data structure here. Set the size of the queue to be k. */
func Constructor(k int) MyCircularQueue {
    return MyCircularQueue{
        header:   nil,
        tail:     nil,
        len:      0,
        capacity: k,
    }
}

/** Insert an element into the circular queue. Return true if the operation is successful. */
func (this *MyCircularQueue) EnQueue(value int) bool {
    if this.len &amp;gt;= this.capacity {
        return false
    }


    if this.len == 0 {
        this.header = &amp;amp;queueNode{
            val:  value,
            next: nil,
        }
        this.tail = this.header
    } else {
        this.tail.next = &amp;amp;queueNode{
            val:  value,
            next: nil,
        }
        this.tail=this.tail.next
    }
    this.len++

    return true
}

/** Delete an element from the circular queue. Return true if the operation is successful. */
func (this *MyCircularQueue) DeQueue() bool {
    if this.len == 0 {
        return false
    }
    this.header = this.header.next
    this.len--
    return true
}

/** Get the front item from the queue. */
func (this *MyCircularQueue) Front() int {
    if this.len == 0 {
        return -1
    }
    return this.header.val
}

/** Get the last item from the queue. */
func (this *MyCircularQueue) Rear() int {
    if this.len == 0 {
        return -1
    }
    return this.tail.val
}

/** Checks whether the circular queue is empty or not. */
func (this *MyCircularQueue) IsEmpty() bool {
    if this.len == 0 {
        return true
    }
    return false
}

/** Checks whether the circular queue is full or not. */
func (this *MyCircularQueue) IsFull() bool {
    if this.len == this.capacity {
        return true
    }
    return false
}


/**
 * Your MyCircularQueue object will be instantiated and called as such:
 * obj := Constructor(k);
 * param_1 := obj.EnQueue(value);
 * param_2 := obj.DeQueue();
 * param_3 := obj.Front();
 * param_4 := obj.Rear();
 * param_5 := obj.IsEmpty();
 * param_6 := obj.IsFull();
 */
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;约瑟夫环&lt;/li&gt;
&lt;li&gt;魔术师发牌
参考算法经典问题&lt;a href=&#34;https://kingjcy.github.io/post/computerbase/algorithm/algotithm/#约瑟夫环&#34;&gt;约瑟夫环&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/computerbase/algorithm/algotithm/#魔术师发牌问题&#34;&gt;魔术师发牌&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、channel的实现&lt;/p&gt;

&lt;p&gt;channel带缓存的其实也是一个循环队列。&lt;/p&gt;

&lt;h3 id=&#34;字符串&#34;&gt;字符串&lt;/h3&gt;

&lt;p&gt;字符串其实就是[]string,字符串在这边单独列出来，主要是使用上比较广泛，比较经典的就是字符串匹配问题。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;BF算法&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;就是我们的暴力匹配方法，循环遍历，也是我们的常规思维，第一个匹配上再匹配第二个，匹配不上在向后移动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func IndexFind(s, sep string) int {
    for i := 0; i &amp;lt; len(s); i++ {
        if s[i] == sep[0] {
            is := true
            j := 0
            for ; j &amp;lt; len(sep); j++ {
                if sep[j] != s[i+j] {
                    is = false
                    break
                }
            }
            if is {
                return i
            }
        }
    }
    return -1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;KMP算法&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;就是减少回溯，在我们匹配不上的时候知道下一步去哪边匹配，而不是一步一步的去匹配，主要是如何移位的问题，要么全移动，要么移动匹配位&lt;/p&gt;

&lt;p&gt;KMP算法的主要原理如下:&lt;/p&gt;

&lt;p&gt;s为目标文本, 长度为m&lt;/p&gt;

&lt;p&gt;p为搜索词,长度为n&lt;/p&gt;

&lt;p&gt;假设p[i]与s[x]匹配失败,那么p[i-1]与s[x-1]是匹配成功的, 则试图找到一个索引 j, 使得p[0:j]=p[i-j-1:i-1]其中p[0:j] 包含p[j]，其实就是构建前缀数组（其实就是已经匹配过的字符串p[:i])每一个位置的字符串的前缀和后缀公共部分的最大长度，不包括字符串本身，否则最大长度始终是字符串本身）&lt;/p&gt;

&lt;p&gt;如果有则s[x]继续与p[j+1]进行比较, 相当于搜索词移动i-j-1位（移动位置=已匹配的字符数 - 对应的部分匹配值）&lt;/p&gt;

&lt;p&gt;无则s[x]与p[0]比较. (具体代码实现时无可以表示为-1, 这样+1 后正好为0) 相当于搜索词移动i位&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func findIdx(dst string) int{

    str := dst[0:len(dst)-1] #输入ABCD，我们看的是D前面的元素，所以去掉最后一个字符，然后分隔字符串判断是否相等
    if len(str) == 0 {
        return -1
    }
    if len(str) == 1 {
        return 0
    }

    cn := 0
    for i:=len(str)/2;i&amp;gt;0;i--{
        if str[0:i] == str[len(str)-i:] {
            cn = i #因为是从中间开始向两边移动，所以第一个相等的字符串长度最长
            break
        }
    }
    return cn
}

func search(src,dst string,next []int) int{
    var s_idx,d_idx,idx int

    for ;s_idx&amp;lt;=len(src)&amp;amp;&amp;amp;d_idx &amp;lt; len(dst);{
        if d_idx == -1|| src[s_idx] == dst[d_idx] {
            s_idx++
            d_idx++
        }else{
            d_idx = next[d_idx]
        }
    }
    if d_idx == len(dst) {
        idx = s_idx - d_idx
    }else{
        idx = -1
    }
    return idx
}

func main(){
    src := &amp;quot;BBC ABCDAB ABCDABCDABDE&amp;quot;
    dst := &amp;quot;ABCDABD&amp;quot;
    var next []int

    for i:=1;i&amp;lt;=len(dst);i++ {
        next = append(next,findIdx(dst[0:i]))
    }
    fmt.Println(next)
    fmt.Println(search(src,dst,next))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Rabin-Karp算法&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;假设匹配文本的长度为M,目标文本的长度为N&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;计算匹配文本的hash值&lt;/li&gt;
&lt;li&gt;计算目标字符串中每个长度为M的子串的hash值（需要计算N-M+1次）&lt;/li&gt;
&lt;li&gt;比较hash值, 如果hash值不同，字符串必然不匹配，如果hash值相同，还需要使用朴素算法（BF）再次判断&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;还有一些其他的算法，可以了解一下。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BM：BM算法，基于好后缀和坏字符规则(重后向前匹配，匹配到一个字符不匹配，查看匹配的字符串中根本没有这个字符，就可以直接移到这个字符的下一位进行匹配，如果有就移到对应的匹配位)&lt;/li&gt;
&lt;li&gt;Horspool：基于坏字符规则的算法，BM的简化版&lt;/li&gt;
&lt;li&gt;Sunday：基于坏字符规则的算法，Horspool的进化版&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;经典应用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;最长不重复字符串&lt;/p&gt;

&lt;p&gt;给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。滑动窗口在左边发现这个最新字符时，就将左边移动到这个index继续遍历，并且比较这个重复的字符串的长度，获取最大的不重复字符串。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func lengthOfLongestSubstring(s string) int {
    if len(s) == 1 {
        return 1
    }

    var j = 0
    var maxLength = 0

    for i:=1;i&amp;lt;len(s);i++{
        index := strings.Index(s[j:i],string(s[i]))
        if index == -1 {
            if i-j +1 &amp;gt; maxLength{
                maxLength = i-j +1
            }
        }else{
            j = j+index+1
            if i-j +1 &amp;gt; maxLength{
                maxLength = i-j +1
            }
        }
    }

    return maxLength
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;string库的实现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;string的contain函数使用了rk算法。string查找使用了BF和RK算法，根据长度shortStringLen来决定使用什么算法，shortStringLen 大小根据机器来决定&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var shortStringLen int
func init() {
    if cpu.X86.HasAVX2 {
        shortStringLen = 63
    } else {
        shortStringLen = 31
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;rk算法在hash的时候使用了E进制，还有这个数16777619&lt;/p&gt;

&lt;h2 id=&#34;非线型数据结构&#34;&gt;非线型数据结构&lt;/h2&gt;

&lt;h3 id=&#34;树状数据结构&#34;&gt;树状数据结构&lt;/h3&gt;

&lt;h4 id=&#34;树&#34;&gt;树&lt;/h4&gt;

&lt;p&gt;它是由n（n&amp;gt;0）个有限节点组成一个具有层次关系的集合。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个节点有零个或多个子节点；&lt;/li&gt;
&lt;li&gt;没有父节点的节点称为根节点；&lt;/li&gt;
&lt;li&gt;每一个非根节点有且只有一个父节点；&lt;/li&gt;
&lt;li&gt;除了根节点外，每个子节点可以分为多个不相交的子树；&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;二叉树&#34;&gt;二叉树&lt;/h4&gt;

&lt;p&gt;每个节点最多含有两个子树的树称为二叉树。&lt;/p&gt;

&lt;p&gt;1、代码表示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//节点
type Node struct {
    Val int
    Left *Node
    Right *Node
}

//初始化
var root tree.Node
root = tree.Node{Value: 3}
root.Left = &amp;amp;tree.Node{}
root.Right = &amp;amp;tree.Node{5, nil, nil}
root.Right.Left = new(tree.Node)
root.Left.Right = tree.CreateNode(2)
root.Right.Left.SetValue(4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、遍历&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;深度优先遍历（DFS）：（Depth-First-Search）它沿着树的深度遍历树的节点，尽可能深的搜索树的分支。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;前序&amp;ndash;根左右&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */

var result []int

func preorderTraversal(root *TreeNode) []int {
    result = []int{}
    dfs(root)
    return result
}

func dfs(root *TreeNode){
    if root != nil {
        result = append(result,root.Val)
        dfs(root.Left)
        dfs(root.Right)
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;中序&amp;ndash;左根右&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */

var result []int

func inorderTraversal(root *TreeNode) []int {
    result = []int{}
    dfs(root)
    return result
}

func dfs(root *TreeNode){
    if root != nil {
        dfs(root.Left)
        result = append(result,root.Val)
        dfs(root.Right)
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;后序&amp;ndash;左右根&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */

var result []int

func postorderTraversal(root *TreeNode) []int {
    result = []int{}
    dfs(root)
    return result
}

func dfs(root *TreeNode){
    if root != nil {
        dfs(root.Left)
        dfs(root.Right)
        result = append(result,root.Val)
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实在矩阵中也是可以使用DFS，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    dx = []int{1, 0, 0, -1}
    dy = []int{0, 1, -1, 0}
)

func floodFill(image [][]int, sr int, sc int, newColor int) [][]int {
    currColor := image[sr][sc]
    if currColor != newColor {
        dfs(image, sr, sc, currColor, newColor)
    }
    return image
}

func dfs(image [][]int, x, y, color, newColor int) {
    if image[x][y] == color {
        image[x][y] = newColor
        for i := 0; i &amp;lt; 4; i++ {
            mx, my := x + dx[i], y + dy[i]
            if mx &amp;gt;= 0 &amp;amp;&amp;amp; mx &amp;lt; len(image) &amp;amp;&amp;amp; my &amp;gt;= 0 &amp;amp;&amp;amp; my &amp;lt; len(image[0]) {
                dfs(image, mx, my, color, newColor)
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将指定格子颜色以及周围可连接到的最后的节点先换成新的颜色，然后回溯遍历过的一个个换成新的颜色，这既是DFS算法。所以深度优先搜索就是重发生点开始遍历，进行不断递归回溯。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;广度优先遍历（BFS）：BFS（Breadth-First-Search）是从根节点开始，沿着树(图)的宽度遍历树(图)的节点。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;层序&amp;ndash;按层，有两种方式&lt;/p&gt;

&lt;p&gt;1、BFS，遍历当前层的所有子节点，放入队列中，先进先出来获取数组&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func levelOrder(root *TreeNode) [][]int {
    var result [][]int
    if root == nil {
        return result
    }
    // 定义一个双向队列
    queue := list.New()
    // 头部插入根节点
    queue.PushFront(root)
    // 进行广度搜索
    for queue.Len() &amp;gt; 0 {
        var current []int
        listLength := queue.Len()
        for i := 0; i &amp;lt; listLength; i++ {
            // 消耗尾部
            // queue.Remove(queue.Back()).(*TreeNode)：移除最后一个元素并将其转化为TreeNode类型
            node := queue.Remove(queue.Back()).(*TreeNode)
            current = append(current, node.Val)
            if node.Left != nil {
                //插入头部
                queue.PushFront(node.Left)
            }
            if node.Right != nil {
                queue.PushFront(node.Right)
            }
        }
        result = append(result, current)
    }
    return result
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、DFS，记录层数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func levelOrder(root *TreeNode) [][]int {
    return dfs(root, 0, [][]int{})
}

func dfs(root *TreeNode, level int, res [][]int) [][]int {
    if root == nil {
        return res
    }
    if len(res) == level {
        res = append(res, []int{root.Val})
    } else {
        res[level] = append(res[level], root.Val)
    }
    res = dfs(root.Left, level+1, res)
    res = dfs(root.Right, level+1, res)
    return res
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实在矩阵中也是可以使用BFS，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    dx = []int{1, 0, 0, -1}
    dy = []int{0, 1, -1, 0}
)

func floodFill(image [][]int, sr int, sc int, newColor int) [][]int {
    currColor := image[sr][sc]
    if currColor == newColor {
        return image
    }
    n, m := len(image), len(image[0])
    queue := [][]int{}
    queue = append(queue, []int{sr, sc})
    image[sr][sc] = newColor
    for i := 0; i &amp;lt; len(queue); i++ {
        cell := queue[i]
        for j := 0; j &amp;lt; 4; j++ {
            mx, my := cell[0] + dx[j], cell[1] + dy[j]
            if mx &amp;gt;= 0 &amp;amp;&amp;amp; mx &amp;lt; n &amp;amp;&amp;amp; my &amp;gt;= 0 &amp;amp;&amp;amp; my &amp;lt; m &amp;amp;&amp;amp; image[mx][my] == currColor {
                queue = append(queue, []int{mx, my})
                image[mx][my] = newColor
            }
        }
    }
    return image
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将指定格子颜色以及周围可连接到的节点先换成新的颜色，然后在遍历这些连接节点，依次重复这两个步骤，就是BFS算法。所以广度优先搜索就是重头开始，一层层进行搜索。&lt;/p&gt;

&lt;p&gt;比如求二进制矩阵中的最短路径：在一个 N × N 的方形网格中，每个单元格有两种状态：空（0）或者阻塞（1），求一条从左上角到右下角、长度最短的畅通路径，就可以使用BFS算法。&lt;/p&gt;

&lt;p&gt;首先定义方向矩阵，访问矩阵，最短路径矩阵，最短是最先被访问的，然后按方向去遍历，返回最短路径矩阵节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   （1）BFS的问题一般都会选用队列方式实现；
   （2）代码模板如下：

       void BFS()
       {
           定义队列;
           定义备忘录，用于记录已经访问的位置；

           判断边界条件，是否能直接返回结果的。

           将起始位置加入到队列中，同时更新备忘录。

           while (队列不为空) {
               获取当前队列中的元素个数。
               for (元素个数) {
                   取出一个位置节点。
                   判断是否到达终点位置。
                   获取它对应的下一个所有的节点。
                   条件判断，过滤掉不符合条件的位置。
                   新位置重新加入队列。
               }
           }

       }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func shortestPathBinaryMatrix(grid [][]int) int {
    dirs := [][]int{{-1, 0}, {-1, 1}, {0, 1}, {1, 1},
        {1, 0}, {1, -1}, {0, -1}, {-1, -1}}
    R := len(grid)
    C := len(grid[0])
    visited := make([][]bool, 0) // 是否遍历过
    for range make([]int, R) {   // 初始化空的 visited 二维数组
        visited = append(visited, make([]bool, C))
    }
    dis := make([][]int, 0) // 到每个顶点的最短路径长度
    for range make([]int, R) {
        dis = append(dis, make([]int, C))
    }

    if grid[0][0] == 1 { // 起始点阻塞
        return -1
    }
    if R == 1 &amp;amp;&amp;amp; C == 1 {
        return 1
    }

    //BFS
    var queue []int          // 申请一个队列
    queue = append(queue, 0) // 队列添加起始点0
    visited[0][0] = true     // 起始点已经遍历过了
    dis[0][0] = 1            // 起始点记录
    for len(queue) &amp;gt; 0 {     // 只要队列不为空就继续执行循环
        cur := queue[0]            // 取出队首元素顶点
        queue = queue[1:]          // 移除队首元素顶点
        curx, cury := cur/C, cur%C // 一维坐标转二维坐标
        for d := 0; d &amp;lt; 8; d++ {   // 查看顶点周围8个方向的相邻顶点
            nextx := curx + dirs[d][0] // 周围8个方向坐标差值
            nexty := cury + dirs[d][1] // 相邻顶点坐标(nextx, nexty)
            //  合法  &amp;amp;&amp;amp; 没有被访问过 &amp;amp;&amp;amp; 没有被阻塞
            if inArea(nextx, nexty, R, C) &amp;amp;&amp;amp; !visited[nextx][nexty] &amp;amp;&amp;amp; grid[nextx][nexty] == 0 {
                queue = append(queue, nextx*C+nexty)    // 二维坐标转一维坐标入队
                visited[nextx][nexty] = true            // 记录这个顶点已经被访问过
                dis[nextx][nexty] = dis[curx][cury] + 1 // 到该顶点的路径是从cur顶点+1

                if nextx == R-1 &amp;amp;&amp;amp; nexty == C-1 { // 如果(nextx,nexty)是终点
                    return dis[nextx][nexty] // 返回到该顶点的距离
                }
            }
        }
    }
    return -1
}

func inArea(x, y, R, C int) bool {
    return x &amp;gt;= 0 &amp;amp;&amp;amp; x &amp;lt; R &amp;amp;&amp;amp; y &amp;gt;= 0 &amp;amp;&amp;amp; y &amp;lt; C
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实是一种动态规划的思想。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、实例&lt;/p&gt;

&lt;p&gt;上面的方式一般都是一起实现的，可以直接重实例中看出来&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package tree

import (
    &amp;quot;fmt&amp;quot;
)

type TreeNode struct {
    ID    int
    Val   int
    Left  *TreeNode
    Right *TreeNode
}

func PreOrder(root *TreeNode) {
    if root != nil {
        fmt.Printf(&amp;quot;%d &amp;quot;, root.Val)
        PreOrder(root.Left)
        PreOrder(root.Right)
    }
}

func InOrder(root *TreeNode) {
    if root != nil {
        InOrder(root.Left)
        fmt.Printf(&amp;quot;%d &amp;quot;, root.Val)
        InOrder(root.Right)
    }
}

func PostOrder(root *TreeNode) {
    if root != nil {
        PostOrder(root.Left)
        PostOrder(root.Right)
        fmt.Printf(&amp;quot;%d &amp;quot;, root.Val)
    }
}


package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;go_code/data_structure/tree&amp;quot;
)

func main() {

    node7 := &amp;amp;tree.TreeNode{
        ID:    7,
        Val:   7,
        Left:  nil,
        Right: nil,
    }
    node6 := &amp;amp;tree.TreeNode{
        ID:    6,
        Val:   6,
        Left:  nil,
        Right: nil,
    }
    node5 := &amp;amp;tree.TreeNode{
        ID:    5,
        Val:   5,
        Left:  nil,
        Right: nil,
    }
    node4 := &amp;amp;tree.TreeNode{
        ID:    4,
        Val:   4,
        Left:  nil,
        Right: nil,
    }
    node3 := &amp;amp;tree.TreeNode{
        ID:    3,
        Val:   3,
        Left:  node6,
        Right: node7,
    }
    node2 := &amp;amp;tree.TreeNode{
        ID:    2,
        Val:   2,
        Left:  node4,
        Right: node5,
    }

    node1 := &amp;amp;tree.TreeNode{
        ID:    1,
        Val:   1,
        Left:  node2,
        Right: node3,
    }

    fmt.Println(&amp;quot;先序遍历&amp;quot;)
    tree.PreOrder(node1)
    fmt.Println()
    fmt.Println(&amp;quot;中序遍历&amp;quot;)
    tree.InOrder(node1)
    fmt.Println()
    fmt.Println(&amp;quot;后序遍历&amp;quot;)
    tree.PostOrder(node1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;路径之和&lt;/p&gt;

&lt;p&gt;DFS，一种是遍历到一条路径上的一个数字，就用sum减去这个数字，最后看剩余值和最后一个值是否相等，还有一种思路就是把每个路径的和加起来，最后进行对比，都需要用到DFS，都要用到递归&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//减法
/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */
func hasPathSum(root *TreeNode, sum int) bool {
    if root == nil {
        return false
    }

    if root.Left == nil &amp;amp;&amp;amp; root.Right == nil {
        return sum == root.Val
    }

    return hasPathSum(root.Left,sum - root.Val)||hasPathSum(root.Right,sum - root.Val)
}

//加法
func hasPathSum(root *TreeNode, sum int) bool {
    if root == nil {
        return false
    }
    queNode := []*TreeNode{}
    queVal := []int{}
    queNode = append(queNode, root)
    queVal = append(queVal, root.Val)
    for len(queNode) != 0 {
        now := queNode[0]
        queNode = queNode[1:]
        temp := queVal[0]
        queVal = queVal[1:]
        if now.Left == nil &amp;amp;&amp;amp; now.Right == nil {
            if temp == sum {
                return true
            }
            continue
        }
        if now.Left != nil {
            queNode = append(queNode, now.Left)
            queVal = append(queVal, now.Left.Val + temp)
        }
        if now.Right != nil {
            queNode = append(queNode, now.Right)
            queVal = append(queVal, now.Right.Val + temp)
        }
    }
    return false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有相对变化，比如求出具体路径等，就需要进行节点存储。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最近公共祖先&lt;/p&gt;

&lt;p&gt;最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”&lt;/p&gt;

&lt;p&gt;如果pq同时位于左边，或者右边，则第一个找到的就是公共祖先，如果分别位于左右子数，那么他们的根就是公共祖先，对于子数依然成立&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for TreeNode.
 * type TreeNode struct {
 *     Val int
 *     Left *ListNode
 *     Right *ListNode
 * }
 */
func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode {
    if root == p || root == q || root == nil {
        return root
    }
    left := lowestCommonAncestor(root.Left,p,q)
    right := lowestCommonAncestor(root.Right,p,q)
    if left == nil {
        return right
    }else if right == nil {
        return left
    }else{
        return root
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;侧面观察二叉树&lt;/p&gt;

&lt;p&gt;层序遍历求第一个或者最后一个节点&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;二叉树最下深度&lt;/p&gt;

&lt;p&gt;求二叉树的深度使用深度优先搜索就可以，只要进行前序遍历，求出所有的深度，然后求出最小值就可以&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */
func minDepth(root *TreeNode) int {
    if root == nil {
        return 0
    }

    if root.Left == nil &amp;amp;&amp;amp; root.Right == nil {
        return 1
    }

    minD := math.MaxInt32
    if root.Left != nil{
        minD = min(minD,minDepth(root.Left))
    }

    if root.Right != nil{
        minD = min(minD,minDepth(root.Right))
    }

    return minD + 1

}

func min(x,y int)int {
    if x&amp;gt;y {
        return y
    }
    return x
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;二叉树反转&lt;/p&gt;

&lt;p&gt;递归回溯，先交互节点，然后交换子树。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */
func invertTree(root *TreeNode) *TreeNode {
    if root == nil {
        return nil
    }
    left := invertTree(root.Left)
    right := invertTree(root.Right)
    root.Left = right
    root.Right = left
    return root
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5、完全二叉树和满二叉树&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;满二叉树&amp;mdash;深度为K则结点个数2^k-1&lt;/li&gt;
&lt;li&gt;完全二叉树&amp;mdash;重根开始编号，层序可以按顺序的获取所有的值，完全二叉树可以说是为了堆而存在的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实计算机中只有两种存储方式，一个顺序一个是链式，所以树是可以使用数组来存储的，当然这样会浪费很多的空间，所以一般都是使用链式来存储的，但是如果是完全二叉树，使用数组存储就比较好来，不浪费。&lt;/p&gt;

&lt;h4 id=&#34;堆&#34;&gt;堆&lt;/h4&gt;

&lt;p&gt;最大堆：根结点的键值是所有堆结点键值中最大者，且每个结点的值都比其孩子的值大。&lt;/p&gt;

&lt;p&gt;最小堆：根结点的键值是所有堆结点键值中最小者，且每个结点的值都比其孩子的值小。&lt;/p&gt;

&lt;p&gt;构建最大堆，最小堆，也就是堆的初始化，首先重倒数第二层进行遍历，它的子节点2k+1，2k+2如果比根节点大，就进行交换，当然交换后如果子节点由树还是需要进行对比交换，直到满足堆的特性，由底想上，最后最大的就在顶上，&lt;/p&gt;

&lt;p&gt;1、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;寻找最大k个数&lt;/p&gt;

&lt;p&gt;构建最大堆，获取根节点，然后再构建最大堆，正常是这种排序方式，当然针对这个题目，可以使用构建k大小的堆（当然这边用的是堆排序，也可以使用其他的排序方法来解决这个问题，但是在数据量比较大的时候堆排序比较快），这边是直接使用了golang的heap，如果想要自己实现堆，可以参考算法排序的堆排序中堆的构建。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func getLeastNumbers(arr []int, k int) []int {
    if k==0 || k&amp;gt;len(arr){
        return []int{}
    }
    h := &amp;amp;intHeap{}
    heap.Init(h)
    for _,v:=range(arr){
        if h.Len()&amp;lt;k{
            heap.Push(h, v)
        }else{
            if (*h)[0]&amp;gt;v{
                heap.Pop(h)
                heap.Push(h, v)
            }
        }
    }

    res:=[]int{}
    for h.Len()&amp;gt;0{
        res = append(res, heap.Pop(h).(int))
    }
    return res
}

type intHeap []int

func (h intHeap) Len() int {
    return len(h)
}

func (h intHeap) Less(i, j int) bool {
    return h[i] &amp;gt; h[j]
}

func (h intHeap) Swap(i, j int) {
    h[i], h[j] = h[j], h[i]
}

func (h *intHeap) Push(x interface{}) {
    // Push 使用 *h，是因为
    // Push 增加了 h 的长度
    *h = append(*h, x.(int))
}

func (h *intHeap) Pop() interface{} {
    // Pop 使用 *h ，是因为
    // Pop 减短了 h 的长度
    res := (*h)[len(*h)-1]
    *h = (*h)[:len(*h)-1]
    return res
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;寻找中位数&lt;/p&gt;

&lt;p&gt;构建一个最大堆一个最小堆&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;线索二叉树&#34;&gt;线索二叉树&lt;/h4&gt;

&lt;p&gt;就是在二叉树中有很多的节点是空的，我们是不是能把他利用起来，作为指向前驱后继，这样能完善树的不知道前驱的缺陷，很大程度的提高了查询的效率。但是这种只有在中序的情况才能进行线索化，主要是因为左根右的结构可以允许在相隔的节点上有空间进行操作。&lt;/p&gt;

&lt;p&gt;结点结构中增加两个标志域LTag和RTag。LTag=0时，lchild域指示结点的左孩子，LTag=1时，lchild域指示结点的前驱；RTag=0时，rchild域指示结点的右孩子，RTag=1时，rchild域指示结点的后继。&lt;/p&gt;

&lt;p&gt;这样定义的好处是既可以从第一个结点起顺后继进行遍历，也可以从最后一个结点起顺前驱进行遍历。便于查找。&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; //二叉树的二叉线索存储表示
 typedef BiThrNode struct{
    TElemType       data,
    lchild,rchild   BiThrNode, /* 左右孩子指针 */
    LTag,RTag       int; /* 左右标志 */定义
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建线索二叉树是对树进行中序遍历线索化的过程。&lt;/p&gt;

&lt;p&gt;2、经典应用&lt;/p&gt;

&lt;h4 id=&#34;二叉查找树&#34;&gt;二叉查找树&lt;/h4&gt;

&lt;p&gt;Binary Search Tree 「BST」&lt;/p&gt;

&lt;p&gt;二叉查找树又称排序二叉树，二叉排序树，二叉搜索树，就是所有的左节点都小于根节点，所有的右节点都大于根节点，且左子树和右子树也同样为二叉搜索树。这样中序遍历就是一个有序数组，在用于查找的时候类似于二分查找，能够减少查询次数，快速的查到数据。&lt;/p&gt;

&lt;p&gt;这个算法的查找效率很高，但是如果使用这种查找方法要首先创建树，确保树的左分支的值小于右分支的值，所有的右节点都大于根节点。&lt;/p&gt;

&lt;p&gt;0、使用场景&lt;/p&gt;

&lt;p&gt;会出现斜树这种极端场景，这个时候效率极差，所以使用不多&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;p&gt;构建二叉查找树&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewTree(arr []int) *Tree{
    // 先在内存中构造 二叉树
    tree := new(Tree)
    for i, v := range arr {
        Insert(tree, v, i)
    }
    return tree
}

// 节点结构
type Node struct {
    Value, Index int  // 元素的值和在数组中的位置
    Left, Right *Node
}

// 树结构
type Tree struct {
    Root *Node
}

// 把数组的的元素插入树中
func Insert(tree *Tree, value, index int){
    if nil == tree.Root {
        tree.Root = newNode(value, index)
    }else {
        InsertNode(tree.Root, newNode(value, index))
    }
}

// 把新增的节点插入树的对应位置
func InsertNode(root, childNode *Node) {
    // 否则，先和根的值对比
    if childNode.Value &amp;lt;= root.Value {
        // 如果小于等于跟的值，则插入到左子树
        if  nil == root.Left {
            root.Left = childNode
        }else {
            InsertNode(root.Left, childNode)
        }
    }else{
        // 否则，插入到右子树
        if nil == root.Right {
            root.Right = childNode
        }else {
            InsertNode(root.Right, childNode)
        }
    }
}

func newNode(value, index int) *Node {
    return &amp;amp;Node{
        Value: value,
        Index: index,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后再用所查数据和每个节点的父节点比较大小，查找最适合的范围。依次比较，查找出对应的值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func BSTsearch(tree *Tree, key int) int{
    // 开始二叉树查找目标key
    return searchKey(tree.Root, key)
}

// 在构建好的二叉树中，从root开始往下查找对应的key 返回其在数组中的位置
func searchKey(root *Node, key int) int {
    if nil == root {
        return -1
    }
    if  key == root.Value {
        return root.Index
    }else if key &amp;lt; root.Value {
        // 往左子树查找
        return searchKey(root.Left, key)
    }else {
        // 往右子树查找
        return searchKey(root.Right, key)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;上面用于查找，类似于二分查找的就是最经典的应用&lt;/li&gt;

&lt;li&gt;&lt;p&gt;合法二叉树&lt;/p&gt;

&lt;p&gt;中序遍历看是否按有大到小的顺序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */
func isValidBST(root *TreeNode) bool {
    arr := []int{}
    midorder(root,&amp;amp;arr)
    for i:=1;i&amp;lt;len(arr);i++{
        if arr[i] &amp;lt;= arr[i-1] {
            return false
        }
    }
    return true
}

func midorder(root *TreeNode,arr *[]int){
    if root != nil {
        midorder(root.Left,arr)
        *arr = append(*arr,root.Val)
        midorder(root.Right,arr)
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修剪二叉搜索树&lt;/p&gt;

&lt;p&gt;dfs中序遍历，如果小于最小值，则砍掉所有的左子树，如果大于最大值，则砍掉所有的右子树，如果在这个区间就赋值给原来的节点。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */
func trimBST(root *TreeNode, L int, R int) *TreeNode {
    if root != nil {
        if root.Val &amp;lt; L {//太小 修剪 ，包括左子树全部砍掉
            return trimBST(root.Right,L,R)
        }
        if root.Val &amp;gt; R {//太大 修剪，包括右子树全部砍掉
            return trimBST(root.Left,L,R)
        }
        root.Left = trimBST(root.Left,L,R)//没问题的修剪左子树
        root.Right = trimBST(root.Right,L,R)//没问题的修剪右子树
    }

    return root
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;哈夫曼树-最优二叉树&#34;&gt;哈夫曼树（最优二叉树）&lt;/h4&gt;

&lt;p&gt;最优二叉树就是在权为wl，w2，…，wn的n个叶子所构成的所有二叉树中，带权路径长度最小(即代价最小)的二叉树称为最优二叉树或哈夫曼树。&lt;/p&gt;

&lt;p&gt;树的带权路径长度：所有叶子结点的带权路径长度之和，记为WPL。这个小，反应的是查询的效率高。&lt;/p&gt;

&lt;p&gt;最优二叉树主要在每个连接之间加一个权重，可以减少遍历的次数。从而提高查询的效率。&lt;/p&gt;

&lt;p&gt;比如给定4个叶子结点a，b，c和d，分别带权7，5，2和4。构造如下图所示的三棵二叉树(还有许多棵)，它们的带权路径长度分别为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/wpl&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(a)WPL=7*2+5*2+2*2+4*2=36
(b)WPL=7*3+5*3+2*1+4*2=46
(c)WPL=7*1+5*2+2*3+4*3=35
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中&amp;copy;树的WPL最小，可以验证，它就是哈夫曼树。&lt;/p&gt;

&lt;p&gt;构建哈夫曼树&lt;/p&gt;

&lt;p&gt;先获取最小的两个点作为子节点，对应的和就是父节点，然后再取出最小的一个点和这个和进行比较，小就作为左节点和这个和最和一个新的树，反之则作为右节点，这样构造的出来的WPL最小，就是一个哈夫曼树，也是最优二叉树&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//节点
type BNode struct {
    key string
    value float64
    ltree, rtree *BNode
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种树不存在初始化，和二叉查找树一样，是对已经有的节点进行构造哈夫曼树，构造规则，先获取最小的两个点作为子节点，对应的和就是父节点，然后再取出最小的一个点和这个和进行比较，小就作为左节点和这个和最和一个新的树，反之则作为右节点，这样构造的就是一个哈夫曼树，也是最优二叉树。&lt;/p&gt;

&lt;p&gt;2、构造哈夫曼树&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;errors&amp;quot;
    &amp;quot;os&amp;quot;
)

type BNode struct {
    key string
    value float64
    ltree, rtree *BNode
}

func getMinNodePos(treeList []*BNode) (pos int, err error) {
    if len(treeList) == 0 {
        return -1, errors.New(&amp;quot;treeList length is 0&amp;quot;)
    }
    pos = -1
    for i, _ := range treeList {
        if pos &amp;lt; 0 {
            pos = i
            continue
        }
        if treeList[pos].value &amp;gt; treeList[i].value {
            pos = i
        }
    }

    return pos, nil
}

func get2MinNodes(treeList []*BNode) (node1, node2 *BNode, newlist []*BNode) {
    if len(treeList) &amp;lt; 2 {
    }
    pos, err := getMinNodePos(treeList)
    if nil != err {
        return nil, nil, treeList
    }
    node1 = treeList[pos]
    newlist = append(treeList[:pos], treeList[pos + 1 :]...)

    pos, err = getMinNodePos(newlist)
    if nil != err {
        return nil, nil, treeList
    }
    node2 = newlist[pos]
    newlist = append(newlist[:pos], newlist[pos + 1 :]...)

    return node1, node2, newlist
}

func makeHuffmanTree(treeList []*BNode) (tree *BNode, err error) {
    if len(treeList) &amp;lt; 1 {
        return nil, errors.New(&amp;quot;Error : treeList length is 0&amp;quot;)
    }
    if len(treeList) == 1 {
        return treeList[0], nil
    }
    lnode, rnode, newlist := get2MinNodes(treeList)

    newNode := new(BNode)
    newNode.ltree = lnode
    newNode.rtree = rnode

    newNode.value = newNode.ltree.value + newNode.rtree.value
    newNode.key = newNode.ltree.key + newNode.rtree.key;

    newlist = append(newlist, newNode)

    return makeHuffmanTree(newlist)
}

func main() {
    keyList   := []byte    {&#39;A&#39;,  &#39;B&#39;, &#39;C&#39;,  &#39;D&#39;,  &#39;E&#39;, &#39;F&#39;,  &#39;G&#39;,  &#39;H&#39;}
    valueList := []float64 {0.12, 0.4, 0.29, 0.90, 0.1, 1.1, 1.23, 0.01}

    treeList := []*BNode   {}
    for i, x := range keyList {
        n := BNode{key:string(x), value:valueList[i]}
        treeList = append(treeList, &amp;amp;n)
    }

    tree, err := makeHuffmanTree(treeList)
    if nil != err {
        fmt.Println(err.Error())
    }

    //TODO you can make it yourself
    //showTree(tree)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建来哈夫曼树，一般都是根据查询的次数来确定权重，权重大的肯定查询多，所以放在最上面，每次都能很快的查询到，虽然没有排序，但是根据数据预判，提高了查询的效率。&lt;/p&gt;

&lt;p&gt;3、哈夫曼编码&lt;/p&gt;

&lt;p&gt;哈夫曼编码是可变字长编码(VLC)的一种，Huffman于1952年提出的编码方法， 该方法完全依据字符出现概率来构造异字头的平均长度最短的码字， 有时称之为最佳编码，一般就叫做Huffman编码&lt;/p&gt;

&lt;p&gt;哈夫曼树也是为了哈夫曼编码的构建的，主要目的是根据使用频率来最大化节省字符（编码）的存储空间。&lt;/p&gt;

&lt;p&gt;比如有A,B,C,D,E五个字符，出现的频率（即权值）分别为5,4,3,2,1,那么我们需要构建一个哈夫曼树，左子树用0来表示，右子树用1来表示，然后对其进行编码，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/hfm&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/hfm1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;所以各字符对应的编码为：A-&amp;gt;11,B-&amp;gt;10,C-&amp;gt;00,D-&amp;gt;011,E-&amp;gt;010&lt;/p&gt;

&lt;p&gt;霍夫曼编码是一种无前缀编码。解码时不会混淆。其主要应用在数据压缩，加密解密等场合。&lt;/p&gt;

&lt;p&gt;如果考虑到进一步节省存储空间，就应该将出现概率大（占比多）的字符用尽量少的0-1进行编码，也就是更靠近根（节点少），这也就是最优二叉树-哈夫曼树。&lt;/p&gt;

&lt;p&gt;4、经典应用&lt;/p&gt;

&lt;h4 id=&#34;avl树-二叉平衡树&#34;&gt;AVL树（二叉平衡树）&lt;/h4&gt;

&lt;p&gt;AVL树（二叉平衡树）是带有平衡条件的二叉查找树。&lt;/p&gt;

&lt;p&gt;平衡条件：其每个节点的左子树和右子树的高度最多相差 1 的二叉查找树。（空树的高度为 -1）。&lt;/p&gt;

&lt;p&gt;AVL树保证了树两边的深度差不多，这样在查找的时候，可以减少查找次数，不会出现一边树很深，需要查询很多次数的特殊情况，提高了查询的效率，当插入时有可能会破坏平衡条件，我们通过旋转（rotation）来进行修正。&lt;/p&gt;

&lt;p&gt;0、应用场景&lt;/p&gt;

&lt;p&gt;由于维护这种高度平衡所付出的代价比从中获得的效率收益还大,故而实际的应用不多，更多的地方是用追求局部而不是非常严格整体平衡的红黑树.当然,如果应用场景中对插入删除不频繁,只是对查找要求较高,那么AVL还是较优于红黑树.&lt;/p&gt;

&lt;p&gt;Windows NT内核中广泛存在.&lt;/p&gt;

&lt;p&gt;1、代码结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type AVL struct {
    value int      //值
    height int     //深度
    left *AVL      //左子树
    right *AVL     //右子树
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、旋转&lt;/p&gt;

&lt;p&gt;保持平衡才是AVL树的特点，如何旋转才能保持平衡，我们先来看看不平衡的情况&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/avl1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;不平衡只有这四种情况，我们有四种旋转方式如上图所示&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RR旋转：左单旋转：就是第一种情况，需要将最上面一个节点左旋下来。&lt;/li&gt;
&lt;li&gt;LL旋转：右单旋转：就是第二种情况，需要将最上面一个节点右旋下来。&lt;/li&gt;
&lt;li&gt;RL旋转：先右旋转后左旋转，就是第三种情况，需要将最下面一个节点先右旋成RR类型，然后再将RR进行左旋。&lt;/li&gt;
&lt;li&gt;LR旋转：先左旋转后右旋转，就是第三种情况，需要将最下面一个节点先左旋成LL类型，然后再将LL进行右旋。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体实现，这个我们通过实例来说明&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;

type AVL struct {
    value int      //值
    height int     //深度
    left *AVL      //左子树
    right *AVL     //右子树
}

//查找元素
func (t *AVL) Search(value int) bool {
    if t == nil {
        return false
    }
    compare := value - t.value
    if compare &amp;lt; 0 {
        return t.left.Search(value)
    }else if compare &amp;gt; 0 {
        return t.right.Search(value)
    }else {
        return true
    }
}

func (t *AVL) leftRotate() *AVL {  //左旋转
    headNode := t.right
    t.right = headNode.left
    headNode.left = t
    //更新结点高度
    t.height = max(t.left.getHeight(),t.right.getHeight()) + 1
    headNode.height = max(headNode.left.getHeight(),headNode.right.getHeight()) + 1
    return headNode
}

func (t *AVL) rightRotate() *AVL {  //右旋转
    headNode := t.left
    t.left = headNode.right
    headNode.right = t
    //更新结点高度
    t.height = max(t.left.getHeight(),t.right.getHeight()) +1
    headNode.height = max(headNode.left.getHeight(),headNode.right.getHeight()) + 1
    return headNode
}

func (t *AVL) rightThenLeftRotate() *AVL {  //右旋转,之后左旋转
    //以失衡点右结点先右旋转
    sonHeadNode := t.right.rightRotate()
    t.right = sonHeadNode
    //再以失衡点左旋转
    return t.leftRotate()
}

func (t *AVL) LeftThenRightRotate() *AVL {  //左旋转,之后右旋转
    //以失衡点左结点先左旋转
    sonHeadNode := t.left.leftRotate()
    t.left = sonHeadNode
    //再以失衡点左旋转
    return t.rightRotate()
}

func (t *AVL) adjust() *AVL {
    if t.right.getHeight() - t.left.getHeight() == 2 {
        if t.right.right.getHeight() &amp;gt; t.right.left.getHeight() {
            t = t.leftRotate()
        }else {
            t = t.rightThenLeftRotate()
        }
    }else if t.left.getHeight() - t.right.getHeight() == 2 {
        if t.left.left.getHeight() &amp;gt; t.left.right.getHeight() {
            t = t.rightRotate()
        } else {
            t = t.LeftThenRightRotate()
        }
    }
    return t
}

//添加元素
func (t *AVL) Insert(value int) *AVL {
    if t == nil {
        newNode := AVL{value,1,nil,nil}
        return &amp;amp;newNode
    }
    if value &amp;lt; t.value {
        t.left = t.left.Insert(value)
        t = t.adjust()
    }else if value &amp;gt; t.value{
        t.right = t.right.Insert(value)
        t = t.adjust()
    }else {
        fmt.Println(&amp;quot;the node exit&amp;quot;)
    }
    t.height = max(t.left.getHeight(),t.right.getHeight()) + 1
    return t
}

/*删除元素
*1、如果被删除结点只有一个子结点，就直接将A的子结点连至A的父结点上，并将A删除
*2、如果被删除结点有两个子结点，将该结点右子数内的最小结点取代A。
*3、查看是否平衡,该调整调整
*/
func (t *AVL) Delete(value int) *AVL {
    if t ==nil {
        return t
    }
    compare := value - t.value
    if compare &amp;lt; 0 {
        t.left = t.left.Delete(value)
    }else if compare &amp;gt; 0{
        t.right = t.right.Delete(value)
    }else { //找到结点,删除结点（）
        if t.left != nil &amp;amp;&amp;amp; t.right != nil {
            t.value = t.right.getMin()
            t.right = t.right.Delete(t.value)
        } else if t.left !=nil {
            t = t.left
        }else {//只有一个右孩子或没孩子
            t = t.right
        }
    }
    if t != nil {
        t.height = max(t.left.getHeight(),t.right.getHeight()) + 1
        t = t.adjust()
    }
    return t
}

//按顺序获得树中元素
func (t *AVL) getAll() []int {
    values := []int{}
    return addValues(values,t)
}

//将一个节点加入切片中
func addValues(values []int,t *AVL) []int {
    if t != nil {
        values = addValues(values,t.left)
        values = append(values,t.value)
        fmt.Println(t.value,t.height)
        values = addValues(values,t.right)
    }
    return values
}

//查找子树最小值
func (t *AVL) getMin() int {
    if t == nil {
        return -1
    }
    if t.left == nil {
        return t.value
    } else {
        return t.left.getMin()
    }
}

//查找子树最大值
func (t *AVL) getMax() int {
    if t == nil {
        return -1
    }
    if t.right == nil {
        return t.value
    } else {
        return t.right.getMax()
    }
}

//查找最小结点
func (t *AVL) getMinNode() *AVL {
    if t == nil {
        return nil
    }else {
        for t.left != nil {
            t = t.left
        }
    }
    return t
}

//查找最大结点
func (t *AVL) getMaxNode() *AVL {
    if t == nil {
        return nil
    }else {
        for t.right != nil {
            t = t.right
        }
    }
    return t
}

//得到树高
func (t *AVL) getHeight() int {
    if t == nil {
        return 0
    }
    return t.height
}

func max(a int,b int) int{
    if a &amp;gt; b {
        return a
    }else {
        return b
    }
}


func main() {
    bsTree := AVL{100,1,nil,nil}
    newTree := bsTree.Insert(60)
    newTree = bsTree.Insert(120)
    newTree = bsTree.Insert(110)
    newTree = bsTree.Insert(130)
    newTree = bsTree.Insert(105)
    fmt.Println(newTree.getAll())

    newTree.Delete(110)
    fmt.Println(newTree.getAll())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;一个二叉查找树是否是平衡二叉树&lt;/p&gt;

&lt;p&gt;直接对比左子树和右子树的高度差是否大于1，并且每一个子树都满足这种情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */
func isBalanced(root *TreeNode) bool {
    if root == nil {
        return true
    }

    if abs(hight(root.Left) - hight(root.Right)) &amp;lt;= 1 &amp;amp;&amp;amp; isBalanced(root.Left) &amp;amp;&amp;amp; isBalanced(root.Right){
        return true
    }

    return false
}

func hight(root *TreeNode) int {
    if root == nil {
        return 0
    }

    return max(hight(root.Left),hight(root.Right)) + 1
}

func max(x,y int)int {
    if x &amp;gt; y {
        return x
    }
    return y
}


func abs(x int)int {
    if x &amp;lt; 0 {
        return x * -1
    }
    return x
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一个升序链表转化为一颗二叉平衡树&lt;/p&gt;

&lt;p&gt;因为升序，所以找到中间节点为根节点，这样左右数量均衡，依次类推，便可取巧构建一颗二叉平衡树，当然这个是针对升序链表，如果是正常数组还是需要上面的方法进行构建，需要对应的旋转。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Definition for singly-linked list.
 * type ListNode struct {
 *     Val int
 *     Next *ListNode
 * }
 */
/**
 * Definition for a binary tree node.
 * type TreeNode struct {
 *     Val int
 *     Left *TreeNode
 *     Right *TreeNode
 * }
 */
func sortedListToBST(head *ListNode) *TreeNode {
    return buildTree(head,nil)
}

func buildTree(head,tail *ListNode) *TreeNode{
    if head == tail {
        return nil
    }

    mid := getMid(head,tail)

    root := &amp;amp;TreeNode{Val:mid.Val}
    root.Left = buildTree(head,mid)
    root.Right = buildTree(mid.Next,tail)

    return root
}

func getMid(left,right *ListNode) *ListNode{
    if left == right {
        return nil
    }

    fast := left
    slow := left
    for fast != right &amp;amp;&amp;amp; fast.Next != right {
        fast = fast.Next.Next
        slow = slow.Next
    }
    return slow

}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;红黑树&#34;&gt;红黑树&lt;/h4&gt;

&lt;p&gt;Red-Black Tree 「RBT」&lt;/p&gt;

&lt;p&gt;红黑树是一种基于二叉查找树的数据结构，是一种自平衡的二叉搜索树，它包含了二叉搜索树的特性，同时具备以下性质：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;所有节点的颜色不是红色就是黑色。&lt;/li&gt;
&lt;li&gt;根节点是黑色。&lt;/li&gt;
&lt;li&gt;每个叶子节点都是黑色的空节点(nil)。&lt;/li&gt;
&lt;li&gt;每个红色节点的两个子节点都是黑色。(从每个叶子到根节点的所有路径上不能有两个连续的红色节点)&lt;/li&gt;
&lt;li&gt;从任一节点到其叶子节点的所有路径上都包含相同数目的黑节点。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/RBT&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见红黑树是通过颜色来保持自平衡的，红黑树有两大操作，一个是我们在二叉平衡树中的旋转，还有一个就是标色，符合上面的规则。&lt;/p&gt;

&lt;p&gt;0、使用场景&lt;/p&gt;

&lt;p&gt;它是一种弱平衡二叉树(由于是若平衡,可以推出,相同的节点情况下,AVL树的高度低于红黑树),相对于要求严格的AVL树来说,它的旋转次数变少,所以对于搜索,插入,删除操作多的情况下,我们就用红黑树.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;广泛用于C++的STL中,map和set都是用红黑树实现的.&lt;/li&gt;
&lt;li&gt;著名的linux进程调度Completely Fair Scheduler,用红黑树管理进程控制块,进程的虚拟内存区域都存储在一颗红黑树上,每个虚拟地址区域都对应红黑树的一个节点,左指针指向相邻的地址虚拟存储区域,右指针指向相邻的高地址虚拟地址空间.&lt;/li&gt;
&lt;li&gt;IO多路复用epoll的实现采用红黑树组织管理sockfd，以支持快速的增删改查.&lt;/li&gt;
&lt;li&gt;ngnix中,用红黑树管理timer,因为红黑树是有序的,可以很快的得到距离当前最小的定时器.&lt;/li&gt;
&lt;li&gt;java中TreeMap的实现.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;能用平衡树的地方，就可以用红黑树。用红黑树之后，读取略逊于AVL，维护强于AVL。设计红黑树的目的，就是解决平衡树的维护起来比较麻烦的问题，红黑树，读取略逊于AVL，维护强于AVL，每次插入和删除的平均旋转次数应该是远小于平衡树。&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    // RED 红树设为true
    RED bool = true
    // BLACK 黑树设为false
    BLACK bool = false
)

// RBNode 红黑树
type RBNode struct {
    value               int64
    color               bool
    left, right, parent *RBNode
}

type RBTree struct {
    root *RBNode
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、红黑树&lt;/p&gt;

&lt;p&gt;我们直接通过实例来看具体的构建和实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import(
    &amp;quot;log&amp;quot;
)

const (
    RED = true
    BLACK = false
)

//-----------------------------------
//Item interface
//
type Item interface {
    Less(than Item) bool
}

type Int int

func (x Int) Less(than Item) bool {
    log.Println(x, &amp;quot; &amp;quot;, than.(Int))
    return x &amp;lt; than.(Int)
}

type Uint32 uint32

func (x Uint32) Less(than Item) bool {
    log.Println(x, &amp;quot; &amp;quot;, than.(Uint32))
    return x &amp;lt; than.(Uint32)
}

type String string

func (x String) Less(than Item) bool {
    log.Println(x, &amp;quot; &amp;quot;, than.(String))
    return x &amp;lt; than.(String)
}

//-----------------------------------

type Node struct {
    Parent *Node
    Left   *Node
    Right  *Node
    color  bool
    Item
}

type Rbtree struct {
    NIL  *Node
    root *Node
    count uint64
}

func New() *Rbtree{
    node := &amp;amp;Node{nil, nil, nil, BLACK, nil}
    return &amp;amp;Rbtree{
        NIL   : node,
        root  : node,
        count : 0,
    }
}

func less(x, y Item) bool {
    return x.Less(y)
}

// Left Rotate
func (rbt *Rbtree) LeftRotate(no *Node) {
    // Since we are doing the left rotation, the right child should *NOT* nil.
    if no.Right == rbt.NIL {
        return
    }

    //          |                                  |
    //          X                                  Y
    //         / \         left rotate            / \
    //        α  Y       -------------&amp;gt;         X   γ
    //           / \                            / \
    //          β  γ                            α  β

    rchild := no.Right
    no.Right = rchild.Left

    if rchild.Left != rbt.NIL {
        rchild.Left.Parent = no
    }

    rchild.Parent = no.Parent

    if no.Parent == rbt.NIL {
        rbt.root = rchild
    } else if no == no.Parent.Left {
        no.Parent.Left = rchild
    } else {
        no.Parent.Right = rchild
    }

    rchild.Left = no

    no.Parent = rchild

}

// Right Rotate
func (rbt *Rbtree) RightRotate(no *Node) {
    if no.Left == rbt.NIL {
        return
    }

    //          |                                  |
    //          X                                  Y
    //         / \         right rotate           / \
    //        Y   γ      -------------&amp;gt;         α  X
    //       / \                                    / \
    //      α  β                                    β  γ

    lchild := no.Left
    no.Left = lchild.Right

    if lchild.Right != rbt.NIL {
        lchild.Right.Parent = no
    }

    lchild.Parent = no.Parent

    if no.Parent == rbt.NIL {
        rbt.root = lchild
    } else if no == no.Parent.Left {
        no.Parent.Left = lchild
    } else {
        no.Parent.Right = lchild
    }

    lchild.Right = no

    no.Parent = lchild

}

func (rbt *Rbtree) Insert(no *Node) {
    x := rbt.root
    var y *Node = rbt.NIL

    for x != rbt.NIL {
        y = x
        if less(no.Item, x.Item) {
            x = x.Left
        } else if less(x.Item, no.Item) {
            x = x.Right
        } else {
            log.Println(&amp;quot;that node already exist&amp;quot;)
        }
    }

    no.Parent = y
    if y == rbt.NIL {
        rbt.root = no
    } else if less(no.Item, y.Item) {
        y.Left = no
    } else {
        y.Right = no
    }

    rbt.count++
    rbt.insertFixup(no)

}

func (rbt *Rbtree) insertFixup(no *Node) {
    for no.Parent.color == RED {
        if no.Parent == no.Parent.Parent.Left {
            y := no.Parent.Parent.Right
            if y.color == RED {
                //
                // 情形 4

                log.Println(&amp;quot;TRACE Do Case 4 :&amp;quot;, no.Item)

                no.Parent.color = BLACK
                y.color = BLACK
                no.Parent.Parent.color = RED
                no = no.Parent.Parent  //循环向上自平衡.
            } else {
                if no == no.Parent.Right {
                    //
                    // 情形 5 : 反向情形
                    // 直接左旋转 , 然后进行情形3(变色-&amp;gt;右旋)
                    log.Println(&amp;quot;TRACE Do Case 5 :&amp;quot;, no.Item)

                    if no == no.Parent.Right {
                        no = no.Parent
                        rbt.LeftRotate(no)
                    }
                }
                log.Println(&amp;quot;TRACE Do Case 6 :&amp;quot;, no.Item)

                no.Parent.color = BLACK
                no.Parent.Parent.color = RED
                rbt.RightRotate(no.Parent.Parent)
            }
        } else { //为父父节点右孩子情形，和左孩子一样，改下转向而已.
            y := no.Parent.Parent.Left
            if y.color == RED {
                no.Parent.color = BLACK
                y.color = BLACK
                no.Parent.Parent.color = RED
                no = no.Parent.Parent
            } else {
                if no == no.Parent.Left {
                    no = no.Parent
                    rbt.RightRotate(no)
                }

                no.Parent.color = BLACK
                no.Parent.Parent.color = RED
                rbt.LeftRotate(no.Parent.Parent)
            }
        }
    }
    rbt.root.color = BLACK
}

func LeftRotateTest(){
    var i10 Int = 10
    var i12 Int = 12

    rbtree := New()

    x := &amp;amp;Node{rbtree.NIL, rbtree.NIL, rbtree.NIL, BLACK, i10}
    rbtree.root = x
    y := &amp;amp;Node{rbtree.root.Right, rbtree.NIL, rbtree.NIL, RED, i12}
    rbtree.root.Right = y

    log.Println(&amp;quot;root : &amp;quot;, rbtree.root)
    log.Println(&amp;quot;left : &amp;quot;, rbtree.root.Left)
    log.Println(&amp;quot;right : &amp;quot;, rbtree.root.Right)

    rbtree.LeftRotate(rbtree.root)

    log.Println(&amp;quot;root : &amp;quot;, rbtree.root)
    log.Println(&amp;quot;left : &amp;quot;, rbtree.root.Left)
    log.Println(&amp;quot;right : &amp;quot;, rbtree.root.Right)

}

func RightRotateTest(){
    var i10 Int = 10
    var i12 Int = 12

    rbtree := New()

    x := &amp;amp;Node{rbtree.NIL, rbtree.NIL, rbtree.NIL, BLACK, i10}
    rbtree.root = x
    y := &amp;amp;Node{rbtree.root.Right, rbtree.NIL, rbtree.NIL, RED, i12}
    rbtree.root.Left = y

    log.Println(&amp;quot;root : &amp;quot;, rbtree.root)
    log.Println(&amp;quot;left : &amp;quot;, rbtree.root.Left)
    log.Println(&amp;quot;right : &amp;quot;, rbtree.root.Right)

    rbtree.RightRotate(rbtree.root)

    log.Println(&amp;quot;root : &amp;quot;, rbtree.root)
    log.Println(&amp;quot;left : &amp;quot;, rbtree.root.Left)
    log.Println(&amp;quot;right : &amp;quot;, rbtree.root.Right)

}

func ItemTest(){
    var itype1 Int = 10
    var itype2 Int = 12

    log.Println(itype1.Less(itype2))


    var strtype1 String = &amp;quot;sola&amp;quot;
    var strtype2 String = &amp;quot;ailumiyana&amp;quot;

    log.Println(strtype1.Less(strtype2))
}

func InsertTest(){
    rbtree := New()

    rbtree.Insert(&amp;amp;Node{rbtree.NIL, rbtree.NIL, rbtree.NIL, RED, Int(10)})
    rbtree.Insert(&amp;amp;Node{rbtree.NIL, rbtree.NIL, rbtree.NIL, RED, Int(9)})
    rbtree.Insert(&amp;amp;Node{rbtree.NIL, rbtree.NIL, rbtree.NIL, RED, Int(8)})
    rbtree.Insert(&amp;amp;Node{rbtree.NIL, rbtree.NIL, rbtree.NIL, RED, Int(6)})
    rbtree.Insert(&amp;amp;Node{rbtree.NIL, rbtree.NIL, rbtree.NIL, RED, Int(7)})

    log.Println(&amp;quot;rbtree counts : &amp;quot;, rbtree.count)

    log.Println(&amp;quot;------ &amp;quot;, rbtree.root.Item)
    log.Println(&amp;quot;----&amp;quot;, rbtree.root.Left.Item, &amp;quot;---&amp;quot;, rbtree.root.Right.Item)
    log.Println(&amp;quot;--&amp;quot;, rbtree.root.Left.Left.Item, &amp;quot;-&amp;quot;, rbtree.root.Left.Right.Item)

}


func main()  {
    log.Println(&amp;quot; ---- main ------ &amp;quot;)
    LeftRotateTest()
    RightRotateTest()
    ItemTest()
    InsertTest()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、经典应用&lt;/p&gt;

&lt;h4 id=&#34;b树&#34;&gt;B树&lt;/h4&gt;

&lt;p&gt;B树也称B-树,它是一颗多路平衡查找树。我们描述一颗B树时需要指定它的阶数，阶数表示了一个结点最多有多少个孩子结点，一般用字母m表示阶数。当m取2时，就是我们常见的二叉搜索树。
一颗m阶的B树定义如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个结点最多有m-1个关键字。&lt;/li&gt;
&lt;li&gt;根结点最少可以只有1个关键字。&lt;/li&gt;
&lt;li&gt;非根结点至少有Math.ceil(m/2)-1个关键字。&lt;/li&gt;
&lt;li&gt;每个结点中的关键字都按照从小到大的顺序排列，每个关键字的左子树中的所有关键字都小于它，而右子树中的所有关键字都大于它。&lt;/li&gt;
&lt;li&gt;所有叶子结点都位于同一层，或者说根结点到每个叶子结点的长度都相同。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;条件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;根节点至少有2个子节点&lt;/li&gt;
&lt;li&gt;每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &amp;lt;= k &amp;lt;= m&lt;/li&gt;
&lt;li&gt;每一个叶子节点都包含k-1个元素，其中 m/2 &amp;lt;= k &amp;lt;= m&lt;/li&gt;
&lt;li&gt;所有的叶子结点都位于同一层&lt;/li&gt;
&lt;li&gt;每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/BT&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、实例&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/BT3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查找E&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取根节点的关键字进行比较，当前根节点关键字为M，E&amp;lt;M（26个字母顺序），所以往找到指向左边的子节点（二分法规则，左小右大，左边放小于当前节点值的子节点、右边放大于当前节点值的子节点）；&lt;/li&gt;
&lt;li&gt;拿到关键字D和G，D&amp;lt;E&amp;lt;G 所以直接找到D和G中间的节点；&lt;/li&gt;
&lt;li&gt;拿到E和F，因为E=E 所以直接返回关键字和指针信息（如果树结构里面没有包含所要查找的节点则返回null）；&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;b-树&#34;&gt;B+树&lt;/h4&gt;

&lt;p&gt;B+树是应文件系统所需而产生的一种B树的变形树，B+树是B树的一个升级版，相对于B树来说B+树更充分的利用了节点的空间，让查询速度更加稳定，其速度完全接近于二分法查找。&lt;/p&gt;

&lt;p&gt;条件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;有k个子结点的结点必然有k个关键字&lt;/li&gt;
&lt;li&gt;非叶结点仅具有索引作用，跟记录有关的信息均存放在叶结点中&lt;/li&gt;
&lt;li&gt;树的所有叶结点构成一个有序链表，可以按照关键码排序的次序遍历全部记录&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;典型应用场景：Mysql数据库的索引类型 - B+索引&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/BT2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;0、应用场景&lt;/p&gt;

&lt;p&gt;B和B+树主要用在文件系统以及数据库做索引.比如Mysql;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;红黑树多用在内部排序，即全放在内存中的，STL的map和set的内部实现就是红黑树。&lt;/li&gt;
&lt;li&gt;B+树多用于外存上时，B+也被成为一个磁盘友好的数据结构。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;b-树-1&#34;&gt;B*树&lt;/h4&gt;

&lt;p&gt;B*树是B+树的变种，相对于B+树他们的不同之处如下：&lt;/p&gt;

&lt;p&gt;首先是关键字个数限制问题，B+树初始化的关键字初始化个数是cei(m/2)，b*树的初始化个数为（cei(&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;*m)）&lt;/p&gt;

&lt;p&gt;B+树节点满时就会分裂，而B*树节点满时会检查兄弟节点是否满（因为每个节点都有指向兄弟的指针），如果兄弟节点未满则向兄弟节点转移关键字，如果兄弟节点已满，则从当前节点和兄弟节点各拿出1/3的数据创建一个新的节点出来；&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/BT.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在B+树的基础上因其初始化的容量变大，使得节点空间使用率更高，而又存有兄弟节点的指针，可以向兄弟节点转移关键字的特性使得B*树额分解次数变得更少；&lt;/p&gt;

&lt;h4 id=&#34;常规树和森林&#34;&gt;常规树和森林&lt;/h4&gt;

&lt;p&gt;主要还是将常规树和森林转化为二叉树来进行操作。&lt;/p&gt;

&lt;p&gt;1、树转化二叉树&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/t&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第一步：在树中所有兄弟结点间加一条连线&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/t1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第二步：对每个结点，除了保留与最左边孩子的连线外，去掉该结点其他孩子的连线&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/t2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第三步：调整位置&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/t3.webp&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、森林转化为二叉树&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/s&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第一步：先将森林中的每棵树变为二叉树&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/s1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第二步：将各二叉树的根节点从左到右连在一起，形成二叉树&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/s2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第三步，调整位置&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/datastruct/s3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然是都可以逆向转化的。&lt;/p&gt;

&lt;h3 id=&#34;网状数据结构&#34;&gt;网状数据结构&lt;/h3&gt;

&lt;h4 id=&#34;图&#34;&gt;图&lt;/h4&gt;

&lt;p&gt;图就是网状数据结构，有顶点和边组成。可以分为有向图和无向图，连通图和非连通图，有权图和无权图，这些分类也可以组合, 比如有向无环图(DAG)。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无向图：若顶点Vi到顶点Vj之间的边没有方向，则称这条边为无向边(Edge)，用无序偶对（Vi，Vj）来表示。如果图中任意两个顶点之间的边都是无向边，则称该图为无向图&lt;/li&gt;
&lt;li&gt;有向图：若从顶点Vi到Vj的边有方向，则称这条边为有向边，也称为弧（Arc）。用有序偶来表示，Vi称为弧尾，Vj称为弧头。如果图中任意两个顶点之间的边都是有向边，则称为有向图&lt;/li&gt;
&lt;li&gt;连通图和非连通图：在无向图中，如果从顶点A到顶点B有路径，则称A和B是连通的。如果对于图中任意两个顶点Vi和Vj都是能够连通的，则称该图是连通图。反之则是非连通图。&lt;/li&gt;
&lt;li&gt;连通形成环的叫有环图&lt;/li&gt;
&lt;li&gt;节点与节点之间的边上存在一定的“权值”，叫有权图&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;图的存储结构&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;邻接矩阵-也就是一个二维数组记录这顶点与顶点之间的连接数据，还有一个数组记录着顶点数据&lt;/li&gt;
&lt;li&gt;邻接表，我认为就是一张哈希表，有数组和链表组成，数组记录着顶点，链表记录这连接的顶点&lt;/li&gt;
&lt;li&gt;还有十字链表和邻接多重表，简单了解一下，前者对有向图进行了优化，后者对无向图进行了优化。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;拓扑排序&lt;/p&gt;

&lt;p&gt;给定一个包含 n个节点的有向图 G，我们给出它的节点编号的一种排列，如果满足：对于图 G 中的任意一条有向边 (u, v)，u在排列中都出现在 v 的前面。
那么称该排列是图 G 的「拓扑排序」。&lt;/p&gt;

&lt;p&gt;图是一个很复杂的数据结构，但是很多高端的算法都是用的图结构，有时间可以深入研究一下。&lt;/p&gt;

&lt;p&gt;1、代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Node struct {
    value int
}

type Graph struct {
    nodes []*Node          // 节点集
    edges map[Node][]*Node // 邻接表表示的无向图
    lock  sync.RWMutex     // 保证线程安全
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 增加节点
func (g *Graph) AddNode(n *Node) {
    g.lock.Lock()
    defer g.lock.Unlock()
    g.nodes = append(g.nodes, n)
}

// 增加边
func (g *Graph) AddEdge(u, v *Node) {
    g.lock.Lock()
    defer g.lock.Unlock()
    // 首次建立图
    if g.edges == nil {
        g.edges = make(map[Node][]*Node)
    }
    g.edges[*u] = append(g.edges[*u], v) // 建立 u-&amp;gt;v 的边
    g.edges[*v] = append(g.edges[*v], u) // 由于是无向图，同时存在 v-&amp;gt;u 的边
}

// 输出图
func (g *Graph) String() {
    g.lock.RLock()
    defer g.lock.RUnlock()
    str := &amp;quot;&amp;quot;
    for _, iNode := range g.nodes {
        str += iNode.String() + &amp;quot; -&amp;gt; &amp;quot;
        nexts := g.edges[*iNode]
        for _, next := range nexts {
            str += next.String() + &amp;quot; &amp;quot;
        }
        str += &amp;quot;\n&amp;quot;
    }
    fmt.Println(str)
}

// 输出节点
func (n *Node) String() string {
    return fmt.Sprintf(&amp;quot;%v&amp;quot;, n.value)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、遍历&lt;/p&gt;

&lt;p&gt;图的遍历主要就是这两种遍历思想，深度优先搜索使用递归方式，需要栈结构辅助实现。广度优先搜索需要使用队列结构辅助实现。&lt;/p&gt;

&lt;p&gt;4、经典应用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;课程表&lt;/p&gt;

&lt;p&gt;使用拓扑排序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func canFinish(numCourses int, prerequisites [][]int) bool {
    var (
        edges = make([][]int, numCourses)
        visited = make([]int, numCourses)
        result []int
        valid = true
        dfs func(u int)
    )

    dfs = func(u int) {
        visited[u] = 1
        for _, v := range edges[u] {
            if visited[v] == 0 {
                dfs(v)
                if !valid {
                    return
                }
            } else if visited[v] == 1 {
                valid = false
                return
            }
        }
        visited[u] = 2
        result = append(result, u)
    }

    for _, info := range prerequisites {
        edges[info[1]] = append(edges[info[1]], info[0])
    }

    for i := 0; i &amp;lt; numCourses &amp;amp;&amp;amp; valid; i++ {
        if visited[i] == 0 {
            dfs(i)
        }
    }
    return valid
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重新安排行程&lt;/p&gt;

&lt;p&gt;使用dfs，先构建图的邻接表，然后将[][]string转化为邻接表进行dfs，由于回溯，所以要变向。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func findItinerary(tickets [][]string) []string {
    var (
        m  = map[string][]string{}
        res []string
    )

    for _, ticket := range tickets {
        src, dst := ticket[0], ticket[1]
        m[src] = append(m[src], dst)
    }
    for key := range m {
        sort.Strings(m[key])
    }

    var dfs func(curr string)
    dfs = func(curr string) {
        for {
            if v, ok := m[curr]; !ok || len(v) == 0 {
                break
            }
            tmp := m[curr][0]
            m[curr] = m[curr][1:]
            dfs(tmp)
        }
        res = append(res, curr)
    }

    dfs(&amp;quot;JFK&amp;quot;)
    for i := 0; i &amp;lt; len(res)/2; i++ {
        res[i], res[len(res) - 1 - i] = res[len(res) - 1 - i], res[i]
    }
    return res
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;其他数据结构&#34;&gt;其他数据结构&lt;/h1&gt;

&lt;h2 id=&#34;并查集&#34;&gt;并查集&lt;/h2&gt;

&lt;p&gt;并查集是一种树型的数据结构，用于处理一些不相交集（Disjoint Sets）的合并及查询问题。&lt;/p&gt;

&lt;p&gt;有一个联合-查找算法（Union-find Algorithm）定义了两个用于此数据结构的操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Find：确定元素属于哪一个子集。它可以被用来确定两个元素是否属于同一子集。
Union：将两个子集合并成同一个集合。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;合并的时候会有集中优化的方式来合并保证树不是太高，减少查询的次数，提高查询的性能。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;经典应用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;交换字符串中的元素&lt;/p&gt;

&lt;p&gt;并查集+连通图+排序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Djset struct{
    Parent []int
    Rank []int
}
func newDjset(n int) Djset {
    parent := make([]int, n)
    rank := make([]int, n)
    for i := 0; i &amp;lt; n; i++ {
        parent[i] = i
        rank[i] = 0
    }
    return Djset{parent, rank}
}
func (ds Djset) Find(x int) int {
    if ds.Parent[x] != x {
        ds.Parent[x] = ds.Find(ds.Parent[x])
    }
    return ds.Parent[x]
}
func (ds Djset) Merge(x, y int) {
    rx := ds.Find(x)
    ry := ds.Find(y)
    if rx != ry {
        if ds.Rank[rx] &amp;lt; ds.Rank[ry] {
            rx, ry = ry,rx
        }
        ds.Parent[ry] = rx;
        if ds.Rank[rx] == ds.Rank[ry] {
            ds.Rank[rx] += 1
        }
    }
}
func smallestStringWithSwaps(s string, pairs [][]int) string {
    n := len(s)
    ds := newDjset(n)
    rs := make([]byte, n)
    for _, v := range(pairs) {
        ds.Merge(v[0], v[1])
    }

    um := make(map[int][]int)
    for i := 0; i &amp;lt; n; i++ {
        um[ds.Find(i)] = append(um[ds.Find(i)], i)
    }

    for _, v := range(um) {
        c := make([]int, len(v))
        copy(c, v)
        sort.Slice(v, func(i, j int) bool {
            return s[v[i]] &amp;lt; s[v[j]]
        })
        for i := 0; i &amp;lt; len(c); i++ {
            rs[c[i]] = s[v[i]]
        }
    }
    return string(rs)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;附录&#34;&gt;附录&lt;/h1&gt;

&lt;h2 id=&#34;用栈实现计算器&#34;&gt;用栈实现计算器&lt;/h2&gt;

&lt;p&gt;计算后缀表达式的代码实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func calculate(postfix string) int {
    stack := stack.ItemStack{}
    fixLen := len(postfix)
    for i := 0; i &amp;lt; fixLen; i++ {
        nextChar := string(postfix[i])
        // 数字：直接压栈
        if unicode.IsDigit(rune(postfix[i])) {
            stack.Push(nextChar)
        } else {
            // 操作符：取出两个数字计算值，再将结果压栈
            num1, _ := strconv.Atoi(stack.Pop())
            num2, _ := strconv.Atoi(stack.Pop())
            switch nextChar {
            case &amp;quot;+&amp;quot;:
                stack.Push(strconv.Itoa(num1 + num2))
            case &amp;quot;-&amp;quot;:
                stack.Push(strconv.Itoa(num1 - num2))
            case &amp;quot;*&amp;quot;:
                stack.Push(strconv.Itoa(num1 * num2))
            case &amp;quot;/&amp;quot;:
                stack.Push(strconv.Itoa(num1 / num2))
            }
        }
    }
    result, _ := strconv.Atoi(stack.Top())
    return result
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在只需知道如何将中缀转为后缀，再利用栈计算即可。&lt;/p&gt;

&lt;p&gt;中缀表达式转后缀表达式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;从左到右逐个字符遍历中缀表达式，输出的字符序列即是后缀表达式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;遇到数字直接输出&lt;/li&gt;
&lt;li&gt;遇到运算符则判断：

&lt;ul&gt;
&lt;li&gt;栈顶运算符优先级更低则入栈，更高或相等则直接输出&lt;/li&gt;
&lt;li&gt;栈为空、栈顶是 ( 直接入栈&lt;/li&gt;
&lt;li&gt;运算符是 ) 则将栈顶运算符全部弹出，直到遇见 )&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;中缀表达式遍历完毕，运算符栈不为空则全部弹出，依次追加到输出&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;转换的代码实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 中缀表达式转后缀表达式
func infix2ToPostfix(exp string) string {
    stack := stack.ItemStack{}
    postfix := &amp;quot;&amp;quot;
    expLen := len(exp)

    // 遍历整个表达式
    for i := 0; i &amp;lt; expLen; i++ {
        char := string(exp[i])
        switch char {
        case &amp;quot; &amp;quot;:
            continue
        case &amp;quot;(&amp;quot;:
            // 左括号直接入栈
            stack.Push(&amp;quot;(&amp;quot;)
        case &amp;quot;)&amp;quot;:
            // 右括号则弹出元素直到遇到左括号
            for !stack.IsEmpty() {
                preChar := stack.Top()
                if preChar == &amp;quot;(&amp;quot; {
                    stack.Pop() // 弹出 &amp;quot;(&amp;quot;
                    break
                }
                postfix += preChar
                stack.Pop()
            }

            // 数字则直接输出
        case &amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;:
            j := i
            digit := &amp;quot;&amp;quot;
            for ; j &amp;lt; expLen &amp;amp;&amp;amp; unicode.IsDigit(rune(exp[j])); j++ {
                digit += string(exp[j])
            }
            postfix += digit
            i = j - 1 // i 向前跨越一个整数，由于执行了一步多余的 j++，需要减 1

        default:
            // 操作符：遇到高优先级的运算符，不断弹出，直到遇见更低优先级运算符
            for !stack.IsEmpty() {
                top := stack.Top()
                if top == &amp;quot;(&amp;quot; || isLower(top, char) {
                    break
                }
                postfix += top
                stack.Pop()
            }
            // 低优先级的运算符入栈
            stack.Push(char)
        }
    }

    // 栈不空则全部输出
    for !stack.IsEmpty() {
        postfix += stack.Pop()
    }

    return postfix
}

// 比较运算符栈栈顶 top 和新运算符 newTop 的优先级高低
func isLower(top string, newTop string) bool {
    // 注意 a + b + c 的后缀表达式是 ab + c +，不是 abc + +
    switch top {
    case &amp;quot;+&amp;quot;, &amp;quot;-&amp;quot;:
        if newTop == &amp;quot;*&amp;quot; || newTop == &amp;quot;/&amp;quot; {
            return true
        }
    case &amp;quot;(&amp;quot;:
        return true
    }
    return false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;倒排索引&#34;&gt;倒排索引&lt;/h2&gt;

&lt;p&gt;理解正向索引和反向索引，也就是我们说的倒排索引&lt;/p&gt;

&lt;p&gt;正向索引，将文件包含哪些词作为存储，到文件中去寻找一些词，需要先找到文件，然后再看有没有这些词&lt;/p&gt;

&lt;p&gt;反向索引，将包含这些词的文件作为一个存储，找到这个词就知道哪些文件包含这些词了。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>用hugo&#43;github构建自己的blog</title>
          <link>https://kingjcy.github.io/post/tool/hugo-blog-build/</link>
          <pubDate>Fri, 29 Aug 2014 09:29:40 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/tool/hugo-blog-build/</guid>
          <description>&lt;p&gt;这个是我用hugo+github搭建起个人blog写的第一篇文章，有点小兴奋。。。首先把搭建测过程写起来和大家分享一下吧。&lt;/p&gt;

&lt;p&gt;首先，作为一个程序员，不拥有自己搭建的blog，而去用别人搭建好的去注册一下，我是无法接受的！！搭建个人blog需要两个东西：&lt;/p&gt;

&lt;p&gt;1、静态网页生成器，有jekyll，hexo，hugo等，由于最近在玩go语言，所以就选择了hugo，其他的也没有深入了解，后面搭建起来，发现hugo还是比较简单。&lt;/p&gt;

&lt;p&gt;2、github pages 这个是github提供的一个托管工作，相当好用。&lt;/p&gt;

&lt;h1 id=&#34;静态页面生成器hugo&#34;&gt;静态页面生成器hugo&lt;/h1&gt;

&lt;p&gt;这个比较方便的静态页面生成器，首先需要安装，我的系统是centos 64位的.&lt;/p&gt;

&lt;p&gt;现在换成了macos系统了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;install&#34;&gt;install&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、直接下载二进制文件，这也是我说的方便的地方。&lt;/p&gt;

&lt;p&gt;Hugo二进制下载地址：&lt;a href=&#34;https://github.com/spf13/hugo/releases&#34;&gt;https://github.com/spf13/hugo/releases&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2。使用macos系统后直接使用homebrew进行安装更新，这个就是一个类似于linux的yum的工具。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install hugo
brew upgrade hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;use&#34;&gt;use&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;下载下来后，首先要生成自己的站点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`hugo new site mysite`--这边hugo的二进制文件不一定是这个名字，可以起个别名alias来用
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时会在mysite目录下生成一些目录和文件，这边简单的介绍一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、config.toml是网站的配置文件，这是它的作者GitHub联合创始人Tom Preston-Werner 觉得YAML不够优雅，捣鼓出来的一个新格式。如果你不喜欢这种格式，你可以将config.toml替换为YAML格式的config.yaml，或者json格式的config.json。hugo都支持。
2、content目录里放的是你写的markdown文章。
3、layouts目录里放的是网站的模板文件。
4、static目录里放的是一些图片、css、js等资源。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后进入站点目录mysite，新建文档&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`cd mysite`

`hugo new about.md`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边新建一个md文件会出现在content目录下，一般这个about.md文件是一个关于本站的介绍或者blog个人介绍，在这边将一下md文件的编辑，其实就是MarkDown格式文件的编写，具体的格式可以参考本文的编辑，或者去网上去搜索一下就ok,这边我说几点，我经常记错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、就是&amp;quot;+++&amp;quot;内的赋值用&amp;quot;=&amp;quot;，&amp;quot;---&amp;quot;内的用&amp;quot;:&amp;quot;。

2、`###`后面必须有空格。

3、有空行才能换行。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般我们写博文，会放在content/post下，正如我这边编写的第一篇文&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`hugo new post/first.md`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后用vim编辑器进行编辑，编辑好后，就可以将你编辑的文字生成静态网页了，当然你肯定需要一个模板，这样可以使你的网页根据美观，这边在讲一下模板的使用&lt;/p&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;模版&#34;&gt;模版&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、模板放在站点的themes下，一般木有这个文件夹，我们需要新增一个&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`mkdir themes`

`cd themes`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、模板可以到hugo官网上去找,那边可以showcase预览一下自己喜欢的，具体的安装方式也有介绍，就是用&lt;code&gt;git clone&lt;/code&gt;把源码下到themes目录下就好&lt;/p&gt;

&lt;p&gt;官网：&lt;a href=&#34;https://gohugo.io/overview/introduction/&#34;&gt;https://gohugo.io/overview/introduction/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、编辑模板的配置文件，这个视具体模板，可以参考我的配置&lt;a href=&#34;https://github.com/kingjcy/&#34;&gt;https://github.com/kingjcy/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下面就是生成我们需要的静态网页了，也就是前端的html文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`hugo --theme=hyde --baseUrl=&amp;quot;http://kingjcy.github.io/&amp;quot;`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不出意外的话，应该在站点目录下生成一个public文件夹，这个就是我们需要的所有文件了，至此第一步已经完成了。可以看见直接编译是hugo，启动一个web服务是hugo server&lt;/p&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、huo new XXXX生成文件是可以直接生成自己想要的内容的，取决于模版，默认是archetypes/default.md，可以对其进行修改，变成自己的样子。&lt;/p&gt;

&lt;p&gt;2、使用图片，默认把图片放在media目录下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;![](/media/worklife/baby/XXX.JPG)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;github-pages托管&#34;&gt;github pages托管&lt;/h1&gt;

&lt;p&gt;这个就简单了，因为本身就是github提供现成的东西，首先新增一个repo，命名为：&lt;code&gt;kingjcy.github.io&lt;/code&gt; （kingjcy替换为你的github用户名）。&lt;/p&gt;

&lt;p&gt;然后将第一步的public加入git版本，上传到这个项目，就可以访问你的个人blog：&lt;code&gt;http://kingjcy.github.io/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;至于git版本控制和github直接的传输，这边就不多讲了，如果需要可以参考我的另外一篇博文《git和github的使用》。&lt;/p&gt;

&lt;p&gt;这边简单列举一些过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd public
$ git init
$ git remote add origin https://github.com/kingjcy/kingjcy.github.io.git
$ git add -A
$ git commit -m &amp;quot;first commit&amp;quot;
$ git push -u origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;终于搭建完了，欢迎指正,tks。&lt;/p&gt;</description>
        </item>
      
    
      
    

  </channel>
</rss>

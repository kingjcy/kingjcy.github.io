<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kingjcy blog </title>
    <link>https://kingjcy.github.io/</link>
    <language>en-us</language>
    <author></author>
    <rights>(C) 2020</rights>
    <updated>2020-12-15 20:21:18 &#43;0800 CST</updated>

    
      
        <item>
          <title>分布式系列---- 分布式存储</title>
          <link>https://kingjcy.github.io/post/distributed/store/store/</link>
          <pubDate>Tue, 15 Dec 2020 20:21:18 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/store/</guid>
          <description>&lt;p&gt;分布式存储是相对于集中式存储来说的，分布式存储是一个大的概念，其包含的种类繁多，除了传统意义上的分布式文件系统、分布式块存储和分布式对象存储外，还包括分布式数据库和分布式缓存等。&lt;/p&gt;

&lt;h1 id=&#34;块存储&#34;&gt;块存储&lt;/h1&gt;

&lt;p&gt;我们需要先理解一下块的概念：块级是指以扇区为基础，一个或我连续的扇区组成一个块，也叫物理块。它是在文件系统与块设备（例如：磁盘驱动器）之间。&lt;/p&gt;

&lt;p&gt;块存储主要是将裸磁盘空间整个映射给主机使用的。比如磁盘阵列里面有5块硬盘，每个硬盘1G，然后可以通过划逻辑盘、做Raid、或者LVM（逻辑卷）等种种方式逻辑划分出N个逻辑的硬盘。假设划分完的逻辑盘也是5个，每个也是1G，但是这5个1G的逻辑盘已经于原来的5个物理硬盘意义完全不同了。比如第一个逻辑硬盘A里面，可能第一个200M是来自物理硬盘1，第二个200M是来自物理硬盘2，所以逻辑硬盘A是由多个物理硬盘逻辑虚构出来的硬盘。&lt;/p&gt;

&lt;p&gt;块存储会采用映射的方式将这几个逻辑盘映射给主机，主机上面的操作系统会识别到有5块硬盘，但是操作系统是区分不出到底是逻辑还是物理的，它一概就认为只是5块裸的物理硬盘而已，跟直接拿一块物理硬盘挂载到操作系统没有区别的，至少操作系统感知上没有区别。&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;磁盘组合变大容量，几块磁盘可以并发读写，提供读写效率&lt;/li&gt;
&lt;li&gt;直接雨磁盘进行交互，能满足一些需要直接裸盘映射的需求，比如数据库&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要网络连接，成本高&lt;/li&gt;
&lt;li&gt;数据无法在不同主机，不同操作系统之间共享&lt;/li&gt;
&lt;li&gt;相对来说，效率低下&lt;/li&gt;
&lt;li&gt;不能形象展示&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单来说，块存储就是直接与磁盘打交道，比如数据库需要将数据直接映射到磁盘上，就需要用到块存储，一般现在就是数据库在使用块存储，其他的应用都使用更加高级的文件存储，甚至对象存储。通常使用块存储的都是系统而非用户，在市场上并没有很多的块存储的产品，大多数都是数据库直接使用，目前只知道&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/ceph/ceph/&#34;&gt;ceph&lt;/a&gt;在对象存储的基础上提供了块存储的能力RDB。&lt;/p&gt;

&lt;h1 id=&#34;文件存储&#34;&gt;文件存储&lt;/h1&gt;

&lt;p&gt;我们需要先理解一下文件的概念：文件级是指文件系统，单个文件可能由于一个或多个逻辑块组成，且逻辑块之间是不连续分布。逻辑块大于或等于物理块整数倍，与上面的块之间的关系是扇区→物理块→逻辑块→文件系统。&lt;/p&gt;

&lt;p&gt;计算机中所有的数据都是0和1，存储在硬件介质上的一连串的01组合对我们来说完全无法去分辨以及管理。因此我们用“文件”这个概念对这些数据进行组织，所有用于同一用途的数据，按照不同应用程序要求的结构方式组成不同类型的文件（通常用不同的后缀来指代不同的类型），然后我们给每一个文件起一个方便理解记忆的名字。而当文件很多的时候，我们按照某种划分方式给这些文件分组，每一组文件放在同一个目录（或者叫文件夹）里面，当然我们也需要给这些目录起一个容易理解和记忆的名字。而且目录下面除了文件还可以有下一级目录（称之为子目录或者子文件夹），所有的文件、目录形成一个树状结构&lt;/p&gt;

&lt;p&gt;把存储介质上的数据组织成目录-子目录-文件这种形式的数据结构，用于从这个结构中寻找、添加、修改、删除文件的程序，以及用于维护这个结构的程序，组成的系统有一个专用的名字：文件系统（File System）。&lt;/p&gt;

&lt;p&gt;文件系统有很多，常见的有Windows的FAT/FAT32/NTFS，Linux的EXT2/EXT3/EXT4/XFS/BtrFS等。而在网络存储中，底层数据并非存储在本地的存储介质，而是另外一台服务器上，不同的客户端都可以用类似文件系统的方式访问这台服务器上的文件，这样的系统叫网络文件系统（Network File System），常见的网络文件系统有Windows网络的CIFS（也叫SMB）、类Unix系统网络的NFS等。而文件存储除了网络文件系统外，FTP、HTTP其实也算是文件存储的某种特殊实现，都是可以通过某个url来访问一个文件。&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;形象，使用方便&lt;/li&gt;
&lt;li&gt;网络文件系统可以实现共享&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要文件系统的协调&lt;/li&gt;
&lt;li&gt;由于封装速度变慢&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实文件系统是我们最常使用的，分布式文件系统也是一步步演化过来的:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;单机时代&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;初创时期由于时间紧迫，在各种资源有限的情况下，通常就直接在项目目录下建立静态文件夹，用于用户存放项目中的文件资源。如果按不同类型再细分，可以在项目目录下再建立不同的子目录来区分。例如：resources\static\file、resources\static\img等。&lt;/p&gt;

&lt;p&gt;优点：这样做比较便利，项目直接引用就行，实现起来也简单，无需任何复杂技术，保存数据库记录和访问起来也很方便。&lt;/p&gt;

&lt;p&gt;缺点：如果只是后台系统的使用一般也不会有什么问题，但是作为一个前端网站使用的话就会存在弊端。一方面，文件和代码耦合在一起，文件越多存放越混乱；另一方面，如果流量比较大，静态文件访问会占据一定的资源，影响正常业务进行，不利于网站快速发展。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;独立文件服务器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;随着公司业务不断发展，将代码和文件放在同一服务器的弊端就会越来越明显。为了解决上面的问题引入独立图片服务器，工作流程如下：项目上传文件时，首先通过ftp或者ssh将文件上传到图片服务器的某个目录下，再通过ngnix或者apache来访问此目录下的文件，返回一个独立域名的图片URL地址，前端使用文件时就通过这个URL地址读取。&lt;/p&gt;

&lt;p&gt;优点：图片访问是很消耗服务器资源的（因为会涉及到操作系统的上下文切换和磁盘I/O操作），分离出来后，Web/App服务器可以更专注发挥动态处理的能力；独立存储，更方便做扩容、容灾和数据迁移；方便做图片访问请求的负载均衡，方便应用各种缓存策略（HTTP Header、Proxy Cache等），也更加方便迁移到CDN。&lt;/p&gt;

&lt;p&gt;缺点：单机存在性能瓶颈，容灾、垂直扩展性稍差&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式文件系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过独立文件服务器可以解决一些问题，如果某天存储文件的那台服务突然down了怎么办？可能你会说，定时将文件系统备份，这台down机的时候，迅速切换到另一台就OK了，但是这样处理需要人工来干预。另外，当存储的文件超过100T的时候怎么办？单台服务器的性能问题？这个时候我们就应该考虑分布式文件系统了。&lt;/p&gt;

&lt;p&gt;业务继续发展，单台服务器存储和响应也很快到达了瓶颈，新的业务需要文件访问具有高响应性、高可用性来支持系统。分布式文件系统，一般分为三块内容来配合，服务的存储、访问的仲裁系统，文件存储系统，文件的容灾系统来构成，仲裁系统相当于文件服务器的大脑，根据一定的算法来决定文件存储的位置，文件存储系统负责保存文件，容灾系统负责文件系统和自己的相互备份。&lt;/p&gt;

&lt;p&gt;优点：扩展能力: 毫无疑问，扩展能力是一个分布式文件系统最重要的特点；高可用性: 在分布式文件系统中，高可用性包含两层，一是整个文件系统的可用性，二是数据的完整和一致性；弹性存储: 可以根据业务需要灵活地增加或缩减数据存储以及增删存储池中的资源，而不需要中断系统运行&lt;/p&gt;

&lt;p&gt;缺点：系统复杂度稍高，需要更多服务器&lt;/p&gt;

&lt;p&gt;文件存储其实就是在块存储之上包装出来的，有了文件的概念，加上文件系统的管理，更加形象的记录展示使用，是我们最长使用的方式。其实在分布式文件系统之前，我们也是经常使用文件来存储的，比如日志，比如持久化数据等。只不过随着数据量的越来越多，数据共享需求越来越多，我们需要使用分布式存储系统，我们常用的分布式文件存储系统有很多，比如&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/nfs/&#34;&gt;NFS&lt;/a&gt;(nfs主要是在共享，还不能算分布式，不过确实是存在网络节点上的。)，&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/glusterfs/&#34;&gt;glusterfs&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/fastfs/&#34;&gt;fastdfs&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/hfds/&#34;&gt;hdfs&lt;/a&gt;等，包括上面块存储&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/ceph/ceph/&#34;&gt;ceph&lt;/a&gt;也提供了文件存储。&lt;/p&gt;

&lt;h1 id=&#34;对象存储&#34;&gt;对象存储&lt;/h1&gt;

&lt;p&gt;对象存储就是面向对象的存储，英文是Object-based Storage。现在很多云厂商，也直接称之为“云存储”。以下是对象存储出现的主要原因：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据量爆炸式增长。Web应用的崛起、社交需求的刺激，极大地推动了多媒体内容的创作和分享。人们开始上传大量的照片、音乐、视频，加剧了数据量的爆发。&lt;/li&gt;
&lt;li&gt;非结构化数据的占比显著增加。图像、音频、视频、word文章、演示胶片这样的数据，就是非结构化数据。2020年（也就是今年），全球数据总量的80%，将是非结构化数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以对象存储&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;能够处理大量非结构化数据的数据存储架构，我们日常生活中见到的文档、文本、图片、XML, HTML、各类报表、音视频信息等等都是非结构化数据。据统计，自社交网络发展以来，非结构化数据占总数据量的75%。。&lt;/li&gt;
&lt;li&gt;扁平结构：对象存储中没有文件夹的概念，所有数据均存储在同一个层级中，你不需要知道他存在哪里，只需要通过“凭证”就可以快速获取数据。&lt;/li&gt;
&lt;li&gt;弹性扩容：可以通过分布式多节点快速扩容。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前有&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/oss/&#34;&gt;很多公有云提供对象存储&lt;/a&gt;，比如阿里云的OSS,华为云的OBS，腾讯云的COS，七牛云Kodo，百度云BOS，网易云NOS。通过提供的SDK就可以访问。如果不想用公有云的话，也有一些开源方案可以自己搭建，比如ceph，minio。&lt;/p&gt;

&lt;p&gt;在对象存储系统里，你不能直接打开/修改文件，只能先下载、修改，再上传文件。（如果大家用过百度网盘或ftp服务，一定可以秒懂。），所以只是一个存储系统，而不是我们的文件系统。所以我们也需要块存储和文件存储。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;为什么对象存储兼具块存储与文件存储的好处，还要使用块存储或文件存储呢？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、有一类应用是需要存储直接裸盘映射的，例如数据库。因为数据库需要存储裸盘映射给自己后，再根据自己的数据库文件系统来对裸盘进行格式化的，所以是不能够采用其他已经被格式化为某种文件系统的存储的。此类应用更适合使用块存储。&lt;/p&gt;

&lt;p&gt;2、对象存储的成本比起普通的文件存储还是较高，需要购买专门的对象存储软件以及大容量硬盘。如果对数据量要求不是海量，只是为了做文件共享的时候，直接用文件存储的形式好了，性价比高。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基本原理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对象存储最常用的方案，就是多台服务器内置大容量硬盘，再装上对象存储软件，然后再额外搞几台服务作为管理节点，安装上对象存储管理软件。管理节点可以管理其他服务器对外提供读写访问功能。其实就是一个文件存储或者块存储的分布式管理系统方案。&lt;/p&gt;

&lt;p&gt;比如AT32这种文件系统，是直接将一份文件的数据与metadata一起存储的，存储过程先将文件按照文件系统的最小块大小来打散（如4M的文件，假设文件系统要求一个块4K，那么就将文件打散成为1000个小块），再写进硬盘里面，过程中没有区分数据/metadata的。而每个块最后会告知你下一个要读取的块的地址，然后一直这样顺序地按图索骥，最后完成整份文件的所有块的读取。这种情况下读写速率很慢，因为就算你有100个机械手臂在读写，但是由于你只有读取到第一个块，才能知道下一个块在哪里，其实相当于只能有1个机械手臂在实际工作。&lt;/p&gt;

&lt;p&gt;而对象存储则将元数据独立了出来，控制节点叫元数据服务器（服务器+对象存储管理软件），里面主要负责存储对象的属性（主要是对象的数据被打散存放到了那几台分布式服务器中的信息），而其他负责存储数据的分布式服务器叫做OSD，主要负责存储文件的数据部分。当用户访问对象，会先访问元数据服务器，元数据服务器只负责反馈对象存储在哪些OSD，假设反馈文件A存储在B、C、D三台OSD，那么用户就会再次直接访问3台OSD服务器去读取数据。这时候由于是3台OSD同时对外传输数据，所以传输的速度就加快了。当OSD服务器数量越多，这种读写速度的提升就越大，通过此种方式，实现了读写快的目的。&lt;/p&gt;

&lt;p&gt;另一方面，对象存储软件是有专门的文件系统的，所以OSD对外又相当于文件服务器，那么就不存在文件共享方面的困难了，也解决了文件共享方面的问题。&lt;/p&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;1、所有的存储底层都是硬盘。&lt;/li&gt;
&lt;li&gt;2、块存储，操作对象是磁盘。存储协议是SCSI、iSCSI、FC，以 SCSI 为例，主要接口命令有 Read/Write/Read Capacity/Inquiry 等等。比如DAS和SAN，基于物理块的存储方式。&lt;/li&gt;
&lt;li&gt;3、文件存储，操作对象是文件和文件夹。存储协议是NFS、SAMBA（SMB）、POSIX等。以NFS（大家应该都用过“网上邻居”共享文件吧？就是那个）为例，文件相关的接口命令包括：READ/WRITE/CREATE/REMOVE/RENAME/LOOKUP/ACCESS 等等，文件夹相关的接口命令包括：MKDIR/RMDIR/READDIR 等等。&lt;/li&gt;
&lt;li&gt;4、对象存储，主要操作对象是对象（Object）。存储协议是S3、Swift等。以 S3 为例，主要接口命令有 PUT/GET/DELETE 等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;分布式存储的基本架构无非就是中心控制和无中心结构，无中心架构也是中心架构的中心瓶颈发展而成的，其实不管是分布式存储还是其他分布式实现，使用中心架构都会出现性能瓶颈，都在往无中心化架构的实现方法发展，使用最多的就是哈希映射思想。&lt;/p&gt;

&lt;h2 id=&#34;中间控制节点架构-hdfs&#34;&gt;中间控制节点架构（HDFS）&lt;/h2&gt;

&lt;p&gt;分布式存储最早是由谷歌提出的，其目的是通过廉价的服务器来提供使用与大规模，高并发场景下的Web访问问题。如图3是谷歌分布式存储（HDFS）的简化的模型。在该系统的整个架构中将服务器分为两种类型，一种名为namenode，这种类型的节点负责管理管理数据（元数据），另外一种名为datanode，这种类型的服务器负责实际数据的管理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/hdfs&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图分布式存储中，如果客户端需要从某个文件读取数据，首先从namenode获取该文件的位置（具体在哪个datanode），然后从该位置获取具体的数据。在该架构中namenode通常是主备部署，而datanode则是由大量节点构成一个集群。由于元数据的访问频度和访问量相对数据都要小很多，因此namenode通常不会成为性能瓶颈，而datanode集群可以分散客户端的请求。因此，通过这种分布式存储架构可以通过横向扩展datanode的数量来增加承载能力，也即实现了动态横向扩展的能力。&lt;/p&gt;

&lt;h2 id=&#34;完全无中心架构&#34;&gt;完全无中心架构&lt;/h2&gt;

&lt;h3 id=&#34;计算模式-ceph&#34;&gt;计算模式（Ceph）&lt;/h3&gt;

&lt;p&gt;如图是Ceph存储系统的架构，在该架构中与HDFS不同的地方在于该架构中没有中心节点。客户端是通过一个设备映射关系计算出来其写入数据的位置，这样客户端可以直接与存储节点通信，从而避免中心节点的性能瓶颈。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/ceph&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在Ceph存储系统架构中核心组件有Mon服务、OSD服务和MDS服务等。对于块存储类型只需要Mon服务、OSD服务和客户端的软件即可。其中Mon服务用于维护存储系统的硬件逻辑关系，主要是服务器和硬盘等在线信息。Mon服务通过集群的方式保证其服务的可用性。OSD服务用于实现对磁盘的管理，实现真正的数据读写，通常一个磁盘对应一个OSD服务。
客户端访问存储的大致流程是，客户端在启动后会首先从Mon服务拉取存储资源布局信息，然后根据该布局信息和写入数据的名称等信息计算出期望数据的位置（包含具体的物理服务器信息和磁盘信息），然后该位置信息直接通信，读取或者写入数据。&lt;/p&gt;

&lt;p&gt;底层是RADOS，这是个标准的对象存储。以RADOS为基础，Ceph 能够提供文件，块和对象三种存储服务。其中通过RBD提供出来的块存储是比较有价值的地方，毕竟因为市面上开源的分布式块存储少见嘛（以前倒是有个sheepdog，但是现在不当红了）。当然它也通过CephFS模块和相应的私有Client提供了文件服务，这也是很多人认为Ceph是个文件系统的原因。另外它自己原生的对象存储可以通过RadosGW存储网关模块向外提供对象存储服务，并且和对象存储的事实标准Amazon S3以及Swift兼容。所以能看出来这其实是个大一统解决方案，啥都齐全。&lt;/p&gt;

&lt;h3 id=&#34;一致性哈希-swift&#34;&gt;一致性哈希（Swift）&lt;/h3&gt;

&lt;p&gt;与Ceph的通过计算方式获得数据位置的方式不同，另外一种方式是通过一致性哈希的方式获得数据位置。一致性哈希的方式就是将设备做成一个哈希环，然后根据数据名称计算出的哈希值映射到哈希环的某个位置，从而实现数据的定位。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/swift&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如图是一致性哈希的基本原理，为了绘制简单，本文以一个服务器上的一个磁盘为例进行介绍。为了保证数据分配的均匀性及出现设备故障时数据迁移的均匀性，一致性哈希将磁盘划分为比较多的虚拟分区，每个虚拟分区是哈希环上的一个节点。整个环是一个从0到32位最大值的一个区间，并且首尾相接。当计算出数据（或者数据名称）的哈希值后，必然落到哈希环的某个区间，然后以顺时针，必然能够找到一个节点。那么，这个节点就是存储数据的位置。
Swift存储的整个数据定位算法就是基于上述一致性哈希实现的。在Swift对象存储中，通过账户名/容器名/对象名三个名称组成一个位置的标识，通过该唯一标识可以计算出一个整型数来。而在存储设备方面，Swift构建一个虚拟分区表，表的大小在创建集群是确定（通常为几十万），这个表其实就是一个数组。这样，根据上面计算的整数值，以及这个数组，通过一致性哈希算法就可以确定该整数在数组的位置。而数组中的每项内容是数据3个副本（也可以是其它副本数量）的设备信息（包含服务器和磁盘等信息）。也就是经过上述计算，可以确定一个数据存储的具体位置。这样，Swift就可以将请求重新定向到该设备进行处理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/swift2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上述计算过程是在一个名为Proxy的服务中进行的，该服务可以集群化部署。因此可以分摊请求的负载，不会成为性能瓶颈。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 文件存储系统glusterfs</title>
          <link>https://kingjcy.github.io/post/distributed/store/fs/glusterfs/</link>
          <pubDate>Tue, 15 Dec 2020 20:21:18 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/fs/glusterfs/</guid>
          <description>&lt;p&gt;GlusterFS (Gluster File System) 是一个开源的分布式文件系统，是 Scale-Out 存储解决方案 Gluster 的核心，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS 借助 TCP/IP 或 InfiniBand RDMA 网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据,基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;1、Brick：GlusterFS中的存储单元，通过是一个受信存储池中的服务器的一个导出目录。可以通过主机名和目录名来标识，如&amp;rsquo;SERVER:EXPORT&amp;rsquo;。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Brick是一个节点和一个导出目录的集合，e.g. node1:/brick1&lt;/li&gt;
&lt;li&gt;Brick是底层的RAID或磁盘经XFS或ext4文件系统格式化而来，所以继承了文件系统的限制&lt;/li&gt;
&lt;li&gt;每个节点上的brick数是不限的&lt;/li&gt;
&lt;li&gt;理想的状况是，一个集群的所有Brick大小都一样。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、Node：一个拥有若干brick的设备。&lt;/p&gt;

&lt;p&gt;3、Trusted Storage Pool：一堆存储节点的集合&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过一个节点“邀请”其他节点创建，这里叫probe&lt;/li&gt;
&lt;li&gt;成员可以动态加入，动态删除&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、GFID：GlusterFS卷中的每个文件或目录都有一个唯一的128位的数据相关联，其用于模拟inode&lt;/p&gt;

&lt;p&gt;5、Namespace：每个Gluster卷都导出单个ns作为POSIX的挂载点。&lt;/p&gt;

&lt;p&gt;6、Volume：一组bricks的逻辑集合，Volume是一个可挂载的目录。支持如下种类：&lt;/p&gt;

&lt;p&gt;a) 分布式卷：默认模式，DHT&lt;/p&gt;

&lt;p&gt;又称哈希卷，近似于RAID0，文件没有分片，文件根据hash算法写入各个节点的硬盘上，优点是容量大，缺点是没冗余。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;b) 条带卷&lt;/p&gt;

&lt;p&gt;相当于raid0，文件是分片均匀写在各个节点的硬盘上的，优点是分布式读写，性能整体较好。缺点是没冗余，分片随机读写可能会导致硬盘IOPS饱和。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume stripe 2 transport tcp server1:/exp1 server2:/exp2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;c) 复制卷（复制模式，AFR）&lt;/p&gt;

&lt;p&gt;相当于raid1，复制的份数，决定集群的大小，通常与分布式卷或者条带卷组合使用，解决前两种存储卷的冗余缺陷。缺点是磁盘利用率低。复本卷在创建时可指定复本的数量，通常为2或者3，复本在存储时会在卷的不同brick上，因此有几个复本就必须提供至少多个brick，当其中一台服务器失效后，可以从另一台服务器读取数据，因此复制GlusterFS卷提高了数据可靠性的同事，还提供了数据冗余的功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;d) 分布式复制卷&lt;/p&gt;

&lt;p&gt;最少需要4台服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;e) 分布式条带复制卷&lt;/p&gt;

&lt;p&gt;当单个文件的体型十分巨大，客户端数量更多时，条带卷已经无法满足需求，此时将分布式与条带化结合起来是一个比较好的选择。其性能与服务器数量有关。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# gluster volume create test-volume stripe 4 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 server7:/exp7 server8:/exp8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;传统分布式文件系统大都会设置元数据服务器或者功能相近的管理服务器，主要作用就是用来管理文件与数据区块之间的存储位置关系。相较其他分布式文件系统而言，GlusterFS并没有集中或者分布式的元数据的概念，取而代之的是弹性哈希算法。集群中的任何服务器和客户端都可以利用哈希算法、路径及文件名进行计算，就可以对数据进行定位，并执行读写访问操作。&lt;/p&gt;

&lt;p&gt;每个节点服务器都掌握了集群的配置信息，这样做的好处是每个节点度拥有节点的配置信息，高度自治，所有信息都可以在本地查询。每个节点的信息更新都会向其他节点通告，保证节点间信息的一致性。但如果集群规模较大，节点众多时，信息同步的效率就会下降，节点信息的非一致性概率就会大大提高。因此GlusterFS未来的版本有向集中式管理变化的趋势。&lt;/p&gt;

&lt;p&gt;通过GlusterFS的数据访问流程，我们可以看出其基本的实现&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/glusterfs2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先是在客户端， 用户通过glusterfs的mount point 来读写数据， 对于用户来说，集群系统的存在对用户是完全透明的，用户感觉不到是操作本地系统还是远端的集群系统。&lt;/li&gt;
&lt;li&gt;用户的这个操作被递交给 本地linux系统的VFS来处理。&lt;/li&gt;
&lt;li&gt;VFS 将数据递交给FUSE 内核文件系统:在启动 glusterfs 客户端以前，需要想系统注册一个实际的文件系统FUSE,如上图所示，该文件系统与ext3在同一个层次上面， ext3 是对实际的磁盘进行处理， 而fuse 文件系统则是将数据通过/dev/fuse 这个设备文件递交给了glusterfs client端。所以， 我们可以将 fuse文件系统理解为一个代理。&lt;/li&gt;
&lt;li&gt;数据被fuse 递交给Glusterfs client 后， client 对数据进行一些指定的处理（所谓的指定，是按照client 配置文件据来进行的一系列处理， 我们在启动glusterfs client 时需要指定这个文件。&lt;/li&gt;
&lt;li&gt;在glusterfs client的处理末端，通过网络将数据递交给 Glusterfs Server，并且将数据写入到服务器所控制的存储设备上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样， 整个数据流的处理就完成了。&lt;/p&gt;

&lt;h1 id=&#34;实践&#34;&gt;实践&lt;/h1&gt;

&lt;p&gt;理论和实践上分析，GlusterFS目前主要适用大文件存储场景，对于小文件尤其是海量小文件，存储效率和访问性能都表现不佳。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;元数据性能&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GlusterFS使用弹性哈希算法代替传统分布式文件系统中的集中或分布式元数据服务，这个是GlusterFS最核心的思想，从而获得了接近线性的高扩展性，同时也提高了系统性能和可靠性。GlusterFS使用算法进行数据定位，集群中的任何服务器和客户端只需根据路径和文件名就可以对数据进行定位和读写访问，文件定位可独立并行化进行。&lt;/p&gt;

&lt;p&gt;这种算法的特点是，给定确定的文件名，查找和定位会非常快。但是，如果事先不知道文件名，要列出文件目录（ls或ls -l），性能就会大幅下降。对于Distributed哈希卷，文件通过HASH算法分散到集群节点上，每个节点上的命名空间均不重叠，所有集群共同构成完整的命名空间，访问时使用HASH算法进行查找定位。列文件目录时，需要查询所有节点，并对文件目录信息及属性进行聚合。这时，哈希算法根本发挥不上作用，相对于有中心的元数据服务，查询效率要差很多。&lt;/p&gt;

&lt;p&gt;从接触的一些用户和实践来看，当集群规模变大以及文件数量达到百万级别时，ls文件目录和rm删除文件目录这两个典型元数据操作就会变得非常慢，创建和删除100万个空文件可能会花上15分钟。如何解决这个问题呢？我们建议合理组织文件目录，目录层次不要太深，单个目录下文件数量不要过多；增大服务器内存配置，并且增大GlusterFS目录缓存参数；网络配置方面，建议采用万兆或者InfiniBand。从研发角度看，可以考虑优化方法提升元数据性能。比如，可以构建全局统一的分布式元数据缓存系统；也可以将元数据与数据重新分离，每个节点上的元数据采用全内存或数据库设计，并采用SSD进行元数据持久化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;小文件问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;理论和实践上分析，GlusterFS目前主要适用大文件存储场景，对于小文件尤其是海量小文件，存储效率和访问性能都表现不佳。海量小文件LOSF问题是工业界和学术界公认的难题，GlusterFS作为通用的分布式文件系统，并没有对小文件作额外的优化措施，性能不好也是可以理解的。&lt;/p&gt;

&lt;p&gt;对于LOSF而言，IOPS/OPS是关键性能衡量指标，造成性能和存储效率低下的主要原因包括元数据管理、数据布局和I/O管理、Cache管理、网络开销等方面。从理论分析以及LOSF优化实践来看，优化应该从元数据管理、缓存机制、合并小文件等方面展开，而且优化是一个系统工程，结合硬件、软件，从多个层面同时着手，优化效果会更显著。GlusterFS小文件优化可以考虑这些方法，这里不再赘述，关于小文件问题请参考“海量小文件问题综述”一文。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;集群管理模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GlusterFS集群采用全对等式架构，每个节点在集群中的地位是完全对等的，集群配置信息和卷配置信息在所有节点之间实时同步。这种架构的优点是，每个节点都拥有整个集群的配置信息，具有高度的独立自治性，信息可以本地查询。但同时带来的问题的，一旦配置信息发生变化，信息需要实时同步到其他所有节点，保证配置信息一致性，否则GlusterFS就无法正常工作。在集群规模较大时，不同节点并发修改配置时，这个问题表现尤为突出。因为这个配置信息同步模型是网状的，大规模集群不仅信息同步效率差，而且出现数据不一致的概率会增加。&lt;/p&gt;

&lt;p&gt;实际上，大规模集群管理应该是采用集中式管理更好，不仅管理简单，效率也高。可能有人会认为集中式集群管理与GlusterFS的无中心架构不协调，其实不然。GlusterFS 2.0以前，主要通过静态配置文件来对集群进行配置管理，没有Glusterd集群管理服务，这说明glusterd并不是GlusterFS不可或缺的组成部分，它们之间是松耦合关系，可以用其他的方式来替换。从其他分布式系统管理实践来看，也都是采用集群式管理居多，这也算一个佐证，GlusterFS 4.0开发计划也表现有向集中式管理转变的趋势。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;容量负载均衡&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GlusterFS的哈希分布是以目录为基本单位的，文件的父目录利用扩展属性记录了子卷映射信息，子文件在父目录所属存储服务器中进行分布。由于文件目录事先保存了分布信息，因此新增节点不会影响现有文件存储分布，它将从此后的新创建目录开始参与存储分布调度。这种设计，新增节点不需要移动任何文件，但是负载均衡没有平滑处理，老节点负载较重。GlusterFS实现了容量负载均衡功能，可以对已经存在的目录文件进行Rebalance，使得早先创建的老目录可以在新增存储节点上分布，并可对现有文件数据进行迁移实现容量负载均衡。&lt;/p&gt;

&lt;p&gt;GlusterFS目前的容量负载均衡存在一些问题。由于采用Hash算法进行数据分布，容量负载均衡需要对所有数据重新进行计算并分配存储节点，对于那些不需要迁移的数据来说，这个计算是多余的。Hash分布具有随机性和均匀性的特点，数据重新分布之后，老节点会有大量数据迁入和迁出，这个多出了很多数据迁移量。相对于有中心的架构，可谓节点一变而动全身，增加和删除节点增加了大量数据迁移工作。GlusterFS应该优化数据分布，最小化容量负载均衡数据迁移。此外，GlusterFS容量负载均衡也没有很好考虑执行的自动化、智能化和并行化。目前，GlusterFS在增加和删除节点上，需要手工执行负载均衡，也没有考虑当前系统的负载情况，可能影响正常的业务访问。GlusterFS的容量负载均衡是通过在当前执行节点上挂载卷，然后进行文件复制、删除和改名操作实现的，没有在所有集群节点上并发进行，负载均衡性能差。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据分布问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Glusterfs主要有三种基本的集群模式，即分布式集群(Distributed cluster)、条带集群(Stripe cluster)、复制集群(Replica cluster)。这三种基本集群还可以采用类似堆积木的方式，构成更加复杂的复合集群。三种基本集群各由一个translator来实现，分别由自己独立的命名空间。对于分布式集群，文件通过HASH算法分散到集群节点上，访问时使用HASH算法进行查找定位。复制集群类似RAID1，所有节点数据完全相同，访问时可以选择任意个节点。条带集群与RAID0相似，文件被分成数据块以Round Robin方式分布到所有节点上，访问时根据位置信息确定节点。&lt;/p&gt;

&lt;p&gt;哈希分布可以保证数据分布式的均衡性，但前提是文件数量要足够多，当文件数量较少时，难以保证分布的均衡性，导致节点之间负载不均衡。这个对有中心的分布式系统是很容易做到的，但GlusteFS缺乏集中式的调度，实现起来比较复杂。复制卷包含多个副本，对于读请求可以实现负载均衡，但实际上负载大多集中在第一个副本上，其他副本负载很轻，这个是实现上问题，与理论不太相符。条带卷原本是实现更高性能和超大文件，但在性能方面的表现太差强人意，远远不如哈希卷和复制卷，没有被好好实现，连官方都不推荐应用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据可用性问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;副本（Replication）就是对原始数据的完全拷贝。通过为系统中的文件增加各种不同形式的副本，保存冗余的文件数据，可以十分有效地提高文件的可用性，避免在地理上广泛分布的系统节点由网络断开或机器故障等动态不可测因素而引起的数据丢失或不可获取。GlusterFS主要使用复制来提供数据的高可用性，通过的集群模式有复制卷和哈希复制卷两种模式。复制卷是文件级RAID1，具有容错能力，数据同步写到多个brick上，每个副本都可以响应读请求。当有副本节点发生故障，其他副本节点仍然正常提供读写服务，故障节点恢复后通过自修复服务或同步访问时自动进行数据同步。&lt;/p&gt;

&lt;p&gt;一般而言，副本数量越多，文件的可靠性就越高，但是如果为所有文件都保存较多的副本数量，存储利用率低（为副本数量分之一），并增加文件管理的复杂度。目前GlusterFS社区正在研发纠删码功能，通过冗余编码提高存储可用性，并且具备较低的空间复杂度和数据冗余度，存储利用率高。&lt;/p&gt;

&lt;p&gt;GlusterFS的复制卷以brick为单位进行镜像，这个模式不太灵活，文件的复制关系不能动态调整，在已经有副本发生故障的情况下会一定程度上降低系统的可用性。对于有元数据服务的分布式系统，复制关系可以是以文件为单位的，文件的不同副本动态分布在多个存储节点上；当有副本发生故障，可以重新选择一个存储节点生成一个新副本，从而保证副本数量，保证可用性。另外，还可以实现不同文件目录配置不同的副本数量，热点文件的动态迁移。对于无中心的GlusterFS系统来说，这些看起来理所当然的功能，实现起来都是要大费周折的。不过值得一提的是，4.0开发计划已经在考虑这方面的副本特性。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据安全问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GlusterFS以原始数据格式（如EXT4、XFS、ZFS）存储数据，并实现多种数据自动修复机制。因此，系统极具弹性，即使离线情形下文件也可以通过其他标准工具进行访问。如果用户需要从GlusterFS中迁移数据，不需要作任何修改仍然可以完全使用这些数据。&lt;/p&gt;

&lt;p&gt;然而，数据安全成了问题，因为数据是以平凡的方式保存的，接触数据的人可以直接复制和查看。这对很多应用显然是不能接受的，比如云存储系统，用户特别关心数据安全。私有存储格式可以保证数据的安全性，即使泄露也是不可知的。GlusterFS要实现自己的私有格式，在设计实现和数据管理上相对复杂一些，也会对性能产生一定影响。&lt;/p&gt;

&lt;p&gt;GlusterFS在访问文件目录时根据扩展属性判断副本是否一致，这个进行数据自动修复的前提条件。节点发生正常的故障，以及从挂载点进行正常的操作，这些情况下发生的数据不一致，都是可以判断和自动修复的。但是，如果直接从节点系统底层对原始数据进行修改或者破坏，GlusterFS大多情况下是无法判断的，因为数据本身也没有校验，数据一致性无法保证。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Cache一致性&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了简化Cache一致性，GlusterFS没有引入客户端写Cache，而采用了客户端只读Cache。GlusterFS采用简单的弱一致性，数据缓存的更新规则是根据设置的失效时间进行重置的。对于缓存的数据，客户端周期性询问服务器，查询文件最后被修改的时间，如果本地缓存的数据早于该时间，则让缓存数据失效，下次读取数据时就去服务器获取最新的数据。&lt;/p&gt;

&lt;p&gt;GlusterFS客户端读Cache刷新的时间缺省是1秒，可以通过重新设置卷参数Performance.cache-refresh-timeout进行调整。这意味着，如果同时有多个用户在读写一个文件，一个用户更新了数据，另一个用户在Cache刷新周期到来前可能读到非最新的数据，即无法保证数据的强一致性。因此实际应用时需要在性能和数据一致性之间进行折中，如果需要更高的数据一致性，就得调小缓存刷新周期，甚至禁用读缓存；反之，是可以把缓存周期调大一点，以提升读性能。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 并发</title>
          <link>https://kingjcy.github.io/post/architecture/concurrence/</link>
          <pubDate>Mon, 09 Nov 2020 19:25:10 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/concurrence/</guid>
          <description>&lt;p&gt;高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;首先并发中的一些重要概念&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;并发度：指在同一个时间点发起的请求数量，比如 12306 统一在下午两点钟放票，100 个人在下午两点钟同时向 12306 发起请求，此时可以认为并发度为 100。&lt;/li&gt;
&lt;li&gt;响应时间：系统对请求做出响应的时间。例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。&lt;/li&gt;
&lt;li&gt;吞吐量：单位时间内处理的请求数量。&lt;/li&gt;
&lt;li&gt;TPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。TPS包括一条消息入和一条消息出，加上一次用户数据库访问。（业务TPS = CAPS × 每个呼叫平均TPS）TPS是软件测试结果的测量单位。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。一般的，评价系统性能均以每秒钟完成的技术交易的数量来衡量。系统整体处理能力取决于处理能力最低模块的TPS值。&lt;/li&gt;
&lt;li&gt;QPS：每秒查询率QPS是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。对应fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;高并发&#34;&gt;高并发&lt;/h2&gt;

&lt;p&gt;那么什么才叫高并发呢？&lt;/p&gt;

&lt;p&gt;高并发要根据场景来定义，多高的并发算高并发？对于阿里来说可能几千上万才算高并发，对于普通小公司来说，可能几十上百就算高并发了。&lt;/p&gt;

&lt;p&gt;高并发其实是和响应时间有很大关系的，你能把接口的响应时间做到1ms，你一个线程就能有1000QPS！一个服务怎么也能跑200个线程吧，QPS轻松地达到了20万呐！你看看有几个整天把高并发挂嘴上的能做到20万的QPS？可见响应时间是不那么容易快速响应的。&lt;/p&gt;

&lt;p&gt;其实高并发的场景多用于web端的请求处理。我们就web常见架构，对于百万级的的QPS怎么进行处理，常见web架构如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/concurrence/cc.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Qps如果不是太高，只要简单的使用上面的进行交互就能满足基本需要。但是如果在QPS达到百万级甚至千万级别的，就会在各种交互组件上出现瓶颈。这个时候就是经验的使用和积累，来使用不同的架构来完成这种需求。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;水平扩展&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、反向代理层的水平扩展&lt;/p&gt;

&lt;p&gt;反向代理层的水平扩展，是通过“DNS轮询”实现的：dns-server对于一个域名配置了多个解析ip，每次DNS解析请求来访问dns-server，会轮询返回这些ip。&lt;/p&gt;

&lt;p&gt;当nginx成为瓶颈的时候，只要增加服务器数量，新增nginx服务的部署，增加一个外网ip，就能扩展反向代理层的性能，做到理论上的无限高并发。&lt;/p&gt;

&lt;p&gt;2、站点层的水平扩展&lt;/p&gt;

&lt;p&gt;站点层的水平扩展，是通过“nginx”实现的。通过修改nginx.conf，可以设置多个web后端。&lt;/p&gt;

&lt;p&gt;当web后端成为瓶颈的时候，只要增加服务器数量，新增web服务的部署，在nginx配置中配置上新的web后端，就能扩展站点层的性能，做到理论上的无限高并发。&lt;/p&gt;

&lt;p&gt;3、服务层的水平扩展&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/concurrence/cc1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;服务层的水平扩展，是通过“服务连接池”实现的。&lt;/p&gt;

&lt;p&gt;站点层通过RPC-client调用下游的服务层RPC-server时，RPC-client中的连接池会建立与下游服务多个连接，当服务成为瓶颈的时候，只要增加服务器数量，新增服务部署，在RPC-client处建立新的下游服务连接，就能扩展服务层性能，做到理论上的无限高并发。如果需要优雅的进行服务层自动扩容，这里可能需要配置中心里服务自动发现功能的支持。&lt;/p&gt;

&lt;p&gt;4、数据层的水平扩展&lt;/p&gt;

&lt;p&gt;在数据量很大的情况下，数据层（缓存，数据库）涉及数据的水平扩展，将原本存储在一台服务器上的数据（缓存，数据库）水平拆分到不同服务器上去，以达到扩充系统性能的目的。&lt;/p&gt;

&lt;p&gt;互联网数据层常见的水平拆分方式有这么几种，以数据库为例：按照范围水平拆分，按照哈希水平拆分。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;垂直扩展&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、提供每台机器的配置&lt;/p&gt;

&lt;p&gt;2、使用多线程thread（正常是线程池）并发，也有使用fork出来多进行的进行处理，这边可以在逻辑里进行业务划分。&lt;/p&gt;

&lt;p&gt;3、优化业务，解决哪些很耗时间的操作，for循环什么的，数据库连接次数，&lt;/p&gt;

&lt;p&gt;4、数据库的优化，使用缓存数据库，优化数据库正常先对我们写的sql进行优化，可以看执行计划，然后数据库索引进行优化，然后就是分区，分表，分库的各种切分。&lt;/p&gt;

&lt;p&gt;其实数据库是很多需求的响应的瓶颈所在，在数据库上花功夫才是重点，一般数据库正常使用情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql 的合理上限不应该超过500万
oracle。20亿数据。  清单
HBase在50000条数据批量写的性能大概是在2s左右，单个查，5-10ms左右
redis qps 500-2000。     几百G
prometheus。  100W。30s 60。200G
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、完善架构，多使用缓存，减少io的交互等。&lt;/p&gt;

&lt;p&gt;在互联网业务发展非常迅猛的早期，如果预算不是问题，强烈建议使用“增强单机硬件性能”的方式提升系统并发能力，因为这个阶段，公司的战略往往是发展业务抢时间，而“增强单机硬件性能”往往是最快的方法。京东的架构说了句大并发，大数据下的业务其实还是靠堆机器保证的，我们现在研究的是如何在堆机器的情况下保证业务的连贯性，容错性，可用性。&lt;/p&gt;

&lt;h1 id=&#34;实践&#34;&gt;实践&lt;/h1&gt;

&lt;h2 id=&#34;java&#34;&gt;java&lt;/h2&gt;

&lt;p&gt;java在web并发中使用很多&lt;/p&gt;

&lt;p&gt;JVMJEE容器中运行的JVM参数配置参数的正确使用直接关系到整个系统的性能和处理能力，JVM的调优主要是对内存管理方面的调优，优化的方向分为以下4点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.HeapSize             堆的大小，也可以说Java虚拟机使用内存的策略，这点是非常关键的。
2.GarbageCollector  通过配置相关的参数进行Java中的垃圾收集器的4个算法(策略)进行使用。
3.StackSize             栈是JVM的内存指令区,每个线程都有他自己的Stack，Stack的大小限制着线程的数量。
4.DeBug/Log           在JVM中还可以设置对JVM运行时的日志和JVM挂掉后的日志输出，这点非常的关键，根据各类JVM的日志输出才能配置合适的参数。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;JDBC针对MySQL的JDBC的参数在之前的文章中也有介绍过，在单台机器或者集群的环境下合理的使用JDBC中的配置参数对操作数据库也有很大的影响。一些所谓高性能的 Java ORM开源框架也就是打开了很多JDBC中的默认参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.例如：autoReconnect、prepStmtCacheSize、cachePrepStmts、useNewIO、blobSendChunkSize 等，&lt;/li&gt;
&lt;li&gt;2.例如集群环境下：roundRobinLoadBalance、failOverReadOnly、autoReconnectForPools、secondsBeforeRetryMaster。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据库连接池(DataSource)应用程序与数据库连接频繁的交互会给系统带来瓶颈和大量的开销会影响到系统的性能，JDBC连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而再不是重新建立一个连接，因此应用程序不需要频繁的与数据库开关连接，并且可以释放空闲时间超过最大空闲时间的数据库连接来避免因为没有释放数据库连接而引起的数据库连接遗漏。这项技术能明显提高对数据库操作的性能。&lt;/p&gt;

&lt;p&gt;数据存取数据库服务器的优化和数据的存取，什么类型的数据放在什么地方更好是值得去思考的问题，将来的存储很可能是混用的，Cache，NOSQL，DFS，DataBase 在一个系统中都会有。&lt;/p&gt;

&lt;p&gt;缓存在宏观上看缓存一般分为2种：本地缓存和分布式缓存&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.本地缓存，对于Java的本地缓存而言就是讲数据放入静态(static)的数据结合中，然后需要用的时候就从静态数据结合中拿出来,对于高并发的环境建议使用 ConcurrentHashMap或者CopyOnWriteArrayList作为本地缓存。缓存的使用更具体点说就是对系统内存的使用，使用多少内存的资源需要有一个适当比例，如果超过适当的使用存储访问，将会适得其反，导致整个系统的运行效率低下。&lt;/li&gt;
&lt;li&gt;2.分布式缓存，一般用于分布式的环境，将每台机器上的缓存进行集中化的存储，并且不仅仅用于缓存的使用范畴，还可以作为分布式系统数据同步/传输的一种手段，一般被使用最多的就是Memcached和Redis。数据存储在不同的介质上读/写得到的效率是不同的，在系统中如何善用缓存，让你的数据更靠近cpu，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;并发/多线程在高并发环境下建议开发者使用JDK中自带的并发包(java.util.concurrent)，在JDK1.5以后使用java.util.concurrent下的工具类可以简化多线程开发，在java.util.concurrent的工具中主要分为以下几个主要部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.线程池，线程池的接口(Executor、ExecutorService)与实现类(ThreadPoolExecutor、 ScheduledThreadPoolExecutor），利用jdk自带的线程池框架可以管理任务的排队和安排，并允许受控制的关闭。因为运行一个线程需要消耗系统CPU资源，而创建、结束一个线程也对系统CPU资源有开销，使用线程池不仅仅可以有效的管理多线程的使用，还是可以提高线程的运行效率。&lt;/li&gt;
&lt;li&gt;2.本地队列，提供了高效的、可伸缩的、线程安全的非阻塞 FIFO 队列。java.util.concurrent 中的五个实现都支持扩展的 BlockingQueue 接口，该接口定义了 put 和 take 的阻塞版本：LinkedBlockingQueue、ArrayBlockingQueue、SynchronousQueue、PriorityBlockingQueue 和 DelayQueue。这些不同的类覆盖了生产者-使用者、消息传递、并行任务执行和相关并发设计的大多数常见使用的上下文。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;拒绝策略介绍&lt;/p&gt;

&lt;p&gt;线程池的拒绝策略，是指当任务添加到线程池中被拒绝，而采取的处理措施。
当任务添加到线程池中之所以被拒绝，可能是由于：第一，线程池异常关闭。第二，任务数量超过线程池的最大限制。&lt;/p&gt;

&lt;p&gt;线程池共包括4种拒绝策略，它们分别是：AbortPolicy, CallerRunsPolicy, DiscardOldestPolicy和DiscardPolicy。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AbortPolicy         -- 当任务添加到线程池中被拒绝时，它将抛出 RejectedExecutionException 异常。
CallerRunsPolicy    -- 当任务添加到线程池中被拒绝时，会在线程池当前正在运行的Thread线程池中处理被拒绝的任务。
DiscardOldestPolicy -- 当任务添加到线程池中被拒绝时，线程池会放弃等待队列中最旧的未处理任务，然后将被拒绝的任务添加到等待队列中。
DiscardPolicy       -- 当任务添加到线程池中被拒绝时，线程池将丢弃被拒绝的任务。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;线程池默认的处理策略是AbortPolicy！&lt;/p&gt;

&lt;h2 id=&#34;go&#34;&gt;go&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-concurrence/&#34;&gt;go并发&lt;/a&gt;是golang语言的核心能力。&lt;/p&gt;

&lt;h2 id=&#34;并发安全&#34;&gt;并发安全&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;是并发中核心需要解决的问题。&lt;/p&gt;

&lt;h2 id=&#34;高并发需要注意的事情&#34;&gt;高并发需要注意的事情&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;高并发下一定要减少锁的使用，这边也是channel在go中的重要作用之一。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;好的习惯是，稍大的类型存到map都存储指针而不是值。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;定义数据结构的时候，减少后面使用的转换&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;defer是性能杀手，我的原则是能不用尽量避开。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;能不在循环内部做的，就不要在循环内存处理&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;减少内存的频繁分配，减少使用全局锁的可能&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;设置并发数&#34;&gt;设置并发数&lt;/h2&gt;

&lt;p&gt;常规的并发模型就是我们使用的工作池模型，我们需要了解具体的工作模式，可以量化的分析并发，比如下图是一个典型的工作线程的处理过程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/concurrence/concurrence.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从开始处理start到结束处理end，该任务的处理共有7个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）从工作队列里拿出任务，进行一些本地初始化计算，例如http协议分析、参数解析、参数校验等&lt;/li&gt;
&lt;li&gt;2）访问cache拿一些数据&lt;/li&gt;
&lt;li&gt;3）拿到cache里的数据后，再进行一些本地计算，这些计算和业务逻辑相关&lt;/li&gt;
&lt;li&gt;4）通过RPC调用下游service再拿一些数据，或者让下游service去处理一些相关的任务&lt;/li&gt;
&lt;li&gt;5）RPC调用结束后，再进行一些本地计算，怎么计算和业务逻辑相关&lt;/li&gt;
&lt;li&gt;6）访问DB进行一些数据操作&lt;/li&gt;
&lt;li&gt;7）操作完数据库之后做一些收尾工作，同样这些收尾工作也是本地计算，和业务逻辑相关&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;分析整个处理的时间轴，会发现：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）其中1，3，5，7步骤中【上图中粉色时间轴】，线程进行本地业务逻辑计算时需要占用CPU&lt;/li&gt;
&lt;li&gt;2）而2，4，6步骤中【上图中橙色时间轴】，访问cache、service、DB过程中线程处于一个等待结果的状态，不需要占用CPU，进一步的分解，这个“等待结果”的时间共分为三部分：

&lt;ul&gt;
&lt;li&gt;请求在网络上传输到下游的cache、service、DB&lt;/li&gt;
&lt;li&gt;下游cache、service、DB进行任务处理&lt;/li&gt;
&lt;li&gt;cache、service、DB将报文在网络上传回工作线程&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上面的分析，Worker线程在执行的过程中，有一部计算时间需要占用CPU，另一部分等待时间不需要占用CPU，通过量化分析，例如打日志进行统计，可以统计出整个Worker线程执行过程中这两部分时间的比例，例如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）时间轴1，3，5，7【上图中粉色时间轴】的计算执行时间是100ms&lt;/li&gt;
&lt;li&gt;2）时间轴2，4，6【上图中橙色时间轴】的等待时间也是100ms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;得到的结果是，这个线程计算和等待的时间是1：1，即有50%的时间在计算（占用CPU），50%的时间在等待（不占用CPU）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）假设此时是单核，则设置为2个工作线程就可以把CPU充分利用起来，让CPU跑到100%&lt;/li&gt;
&lt;li&gt;2）假设此时是N核，则设置为2N个工作现场就可以把CPU充分利用起来，让CPU跑到N*100%&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;N核服务器，通过执行业务的单线程分析出本地计算时间为x，等待时间为y，则工作线程数（线程池线程数）设置为 N*(x+y)/x，能让CPU的利用率最大化。&lt;/p&gt;

&lt;p&gt;一般来说，非CPU密集型的业务（加解密、压缩解压缩、搜索排序等业务是CPU密集型的业务），瓶颈都在后端数据库，本地CPU计算的时间很少，所以设置几十或者几百个工作线程也都是可能的。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- Architecture 总结</title>
          <link>https://kingjcy.github.io/post/architecture/architecture/</link>
          <pubDate>Thu, 05 Nov 2020 10:04:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/architecture/</guid>
          <description>&lt;p&gt;架构就是解决问题给出的整体技术方案，既要掌握整体，也要知道局部瓶颈能够解决具体业务的方案。&lt;/p&gt;

&lt;p&gt;架构师，是一个既需要掌控整体又需要洞悉局部瓶颈并依据具体的业务场景给出解决方案的团队领导型人物。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;在软件工程和企业信息系统领域，又有很多细分，如所谓的系统架构师（System Architect）、应用架构师（Application Architect）、企业架构师（Enterprise Architect）以及基础设施架构师（Infrastructure Architect）等等。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;应用架构师负责构建一个以解决特定问题为目标的软件应用的内部结合结构，一般以满足各种功能性需求以及维护性需求为设计考虑目标；&lt;/li&gt;
&lt;li&gt;系统架构师则提供运营支撑软件应用的信息系统的结构设计，一般以满足各种非功能性需求或运营性需求为设计目标（如安全性、可伸缩性、可互操作性等等）；&lt;/li&gt;
&lt;li&gt;企业架构师，就不光只顾IT系统的架构了，他应以企业的持续经营目标为考虑要素来构建企业所需要的内在结构设计；&lt;/li&gt;
&lt;li&gt;基础设施架构师主要是负责基础平台的设计。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;架构设计&#34;&gt;架构设计&lt;/h1&gt;

&lt;p&gt;其实就是设计模式中的七大原则&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.开闭原则
2.依赖倒置原则
3.单一职责原则
4.接口隔离原则
5.迪米特法则（最小知道原则）
6.里氏替换原则
7.合成/聚合复用原则
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对我实践过程中总结来说：&lt;/p&gt;

&lt;p&gt;1、抽象，封装，可扩展，使用接口实现&lt;/p&gt;

&lt;p&gt;定义对外调用的接口，实现这些方法的结构体就是实现了接口，不需要关心具体的实现，只需要调用就好，其实也就是设计模式中的开闭原则和依赖倒置原则，接口隔离的原则，迪米特法则，实现依赖于接口，接口尽量细化，不互相依赖。&lt;/p&gt;

&lt;p&gt;2、尽量设计原子功能&lt;/p&gt;

&lt;p&gt;每个服务职责单一，可以服务化，不相互依赖，解耦，同时组合功能就是尽量模块清晰，只实现原子的功能，想实现大的功能就是将原子的功能组合起来，也就是合成复用原则&lt;/p&gt;

&lt;p&gt;3、关注性能、可用性、伸缩性、扩展性、安全性这5个架构要素的实现&lt;/p&gt;

&lt;h2 id=&#34;架构要素&#34;&gt;架构要素&lt;/h2&gt;

&lt;p&gt;在架构设计中一些常规的思想是我们设计的核心要素，我们必须拥有解决的能力，掌握其常规方案，结合实际场景有具体设计的能力。&lt;/p&gt;

&lt;h3 id=&#34;公共服务服务化&#34;&gt;公共服务服务化&lt;/h3&gt;

&lt;p&gt;其实就是一种共享服务的&lt;strong&gt;抽象&lt;/strong&gt;，可以实现架构和业务上的解耦，解决各种依赖问题，可以提高共同部分的优化效率和管控等等好处，没有具体的实现方案，视具体情况进行抽象设计。&lt;a href=&#34;https://kingjcy.github.io/post/architecture/coupling/&#34;&gt;架构解耦还有很多方案&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;高可用-容错机制&#34;&gt;高可用，容错机制&lt;/h3&gt;

&lt;p&gt;高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。&lt;/p&gt;

&lt;p&gt;高可用保证是通过&lt;strong&gt;冗余+自动故障转移&lt;/strong&gt;来保证系统的高可用特性。&lt;/p&gt;

&lt;p&gt;我们先来看一下正常的服务架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/webserver&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;常见互联网分布式架构如上，分为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）客户端层：典型调用方是浏览器browser或者手机应用APP&lt;/li&gt;
&lt;li&gt;（2）反向代理层：系统入口，反向代理&lt;/li&gt;
&lt;li&gt;（3）站点应用层：实现核心应用逻辑，返回html或者json&lt;/li&gt;
&lt;li&gt;（4）服务层：如果实现了服务化，就有这一层&lt;/li&gt;
&lt;li&gt;（5）数据-缓存层：缓存加速访问存储&lt;/li&gt;
&lt;li&gt;（6）数据-数据库层：数据库固化数据存储&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整个系统的高可用，又是通过每一层的冗余+自动故障转移来综合实现的。&lt;/p&gt;

&lt;p&gt;1、【客户端层-&amp;gt;反向代理层】的高可用&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【客户端层】到【反向代理层】的高可用，是通过反向&lt;strong&gt;代理层的冗余&lt;/strong&gt;来实现的。以nginx为例：有两台nginx，一台对线上提供服务，另一台冗余以保证高可用，常见的实践是keepalived存活探测，相同virtual IP提供服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当nginx挂了的时候，keepalived能够探测到，会自动的进行故障转移，将流量自动迁移到shadow-nginx，由于使用的是相同的virtual IP，这个切换过程对调用方是透明的。&lt;/p&gt;

&lt;p&gt;2、【反向代理层-&amp;gt;站点层】的高可用&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【反向代理层】到【站点层】的高可用，是通过&lt;strong&gt;站点层的冗余&lt;/strong&gt;来实现的。假设反向代理层是nginx，nginx.conf里能够配置多个web后端，并且nginx能够探测到多个后端的存活性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当web-server挂了的时候，nginx能够探测到，会自动的进行故障转移，将流量自动迁移到其他的web-server，整个过程由nginx自动完成，对调用方是透明的。&lt;/p&gt;

&lt;p&gt;3、【站点层-&amp;gt;服务层】的高可用&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【站点层】到【服务层】的高可用，是通过&lt;strong&gt;服务层的冗余&lt;/strong&gt;来实现的。“服务连接池”会建立与下游服务多个连接，每次请求会“随机”选取连接来访问下游服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当service挂了的时候，service-connection-pool能够探测到，会自动的进行故障转移，将流量自动迁移到其他的service，整个过程由连接池自动完成，对调用方是透明的（所以说RPC-client中的服务连接池是很重要的基础组件）。&lt;/p&gt;

&lt;p&gt;4、【服务层&amp;gt;缓存层】的高可用&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【服务层】到【缓存层】的高可用，是通过&lt;strong&gt;缓存数据的冗余&lt;/strong&gt;来实现的。&lt;/p&gt;

&lt;p&gt;缓存层的数据冗余又有几种方式：第一种是利用客户端的封装，service对cache进行双读或者双写。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;缓存层也可以通过支持主从同步的缓存集群来解决缓存层的高可用问题。以redis为例，redis天然支持主从同步，redis官方也有sentinel哨兵机制，来做redis的存活性检测。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当redis主挂了的时候，sentinel能够探测到，会通知调用方访问新的redis，整个过程由sentinel和redis集群配合完成，对调用方是透明的。&lt;/p&gt;

&lt;p&gt;5、【服务层&amp;gt;数据库层】的高可用&lt;/p&gt;

&lt;p&gt;大部分互联网技术，数据库层都用了“主从同步，读写分离”架构，所以数据库层的高可用，又分为“读库高可用”与“写库高可用”两类。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【服务层】到【数据库读】的高可用，是通过&lt;strong&gt;读库的冗余&lt;/strong&gt;来实现的。既然冗余了读库，一般来说就至少有2个从库，“数据库连接池”会建立与读库多个连接，每次请求会路由到这些读库。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive13.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动故障转移&lt;/strong&gt;：当读库挂了的时候，db-connection-pool能够探测到，会自动的进行故障转移，将流量自动迁移到其他的读库，整个过程由连接池自动完成，对调用方是透明的（所以说DAO中的数据库连接池是很重要的基础组件）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive14.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【服务层】到【数据库写】的高可用，是通过&lt;strong&gt;写库的冗余&lt;/strong&gt;来实现的。以mysql为例，可以设置两个mysql双主同步，一台对线上提供服务，另一台冗余以保证高可用，常见的实践是keepalived存活探测，相同virtual IP提供服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/alive15.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;自动故障转移：当写库挂了的时候，keepalived能够探测到，会自动的进行故障转移，将流量自动迁移到shadow-db-master，由于使用的是相同的virtual IP，这个切换过程对调用方是透明的。&lt;/p&gt;

&lt;p&gt;通过整体架构的分析，我们可以看到高可用的思想就是通过&lt;strong&gt;冗余（多实例，集群化）和在调度（最常见的就是负载均衡）的基础上实现故障自动转移&lt;/strong&gt;来完成的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;负载均衡&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;提到高可用，不得不提负载均衡，其实高可用的实现还是依赖于负载均衡的，是在负载均衡的基础上实现的故障自动转移，没有冗余节点也即是多实例的负载均衡，也谈不上故障自动转移。但是负载均衡的重点还是在调度均衡，不光在
最前端的nginx，还有后端数据库连接池都存在这个均衡的问题。&lt;/p&gt;

&lt;p&gt;常见互联网分布式架构如上，分为客户端层、反向代理nginx层、站点层、服务层、数据层。可以看到，每一个下游都有多个上游调用，只需要做到，每一个上游都均匀访问每一个下游，就能实现“将请求/数据【均匀】分摊到多个操作单元上执行”。&lt;/p&gt;

&lt;p&gt;1、【客户端层-&amp;gt;反向代理层】的负载均衡&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/lb.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【客户端层】到【反向代理层】的负载均衡，是通过“DNS轮询”实现的：DNS-server对于一个域名配置了多个解析ip，每次DNS解析请求来访问DNS-server，会轮询返回这些ip，保证每个ip的解析概率是相同的。这些ip就是nginx的外网ip，以做到每台nginx的请求分配也是均衡的。&lt;/p&gt;

&lt;p&gt;2、【反向代理层-&amp;gt;站点层】的负载均衡&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/lb1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【反向代理层】到【站点层】的负载均衡，是通过“nginx”实现的。通过修改nginx.conf，可以实现多种负载均衡策略：轮询，最少连接路由，IPhash等&lt;/p&gt;

&lt;p&gt;3、【站点层-&amp;gt;服务层】的负载均衡&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/lb2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;【站点层】到【服务层】的负载均衡，是通过“服务连接池”实现的。连接池需要涉及负载均衡、故障转移、超时处理的相关机制。&lt;/p&gt;

&lt;p&gt;4、【数据层】的负载均衡&lt;/p&gt;

&lt;p&gt;在数据量很大的情况下，由于数据层（db，cache）涉及数据的水平切分，所以数据层的负载均衡更为复杂一些，它分为“数据的均衡”，与“请求的均衡”。业内常见的水平切分方式有这么几种：按照range水平切分，按照id哈希水平切分&lt;/p&gt;

&lt;p&gt;提到负载均衡我们需要了解一些专业名称&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）nginx：一个高性能的web-server和实施反向代理的软件&lt;/li&gt;
&lt;li&gt;2）lvs：Linux Virtual Server，使用集群技术，实现在linux操作系统层面的一个高性能、高可用、负载均衡服务器&lt;/li&gt;
&lt;li&gt;3）keepalived：一款用来检测服务状态存活性的软件，常用来做高可用&lt;/li&gt;
&lt;li&gt;4）f5：一个高性能、高可用、负载均衡的硬件设备（听上去和lvs功能差不多？）&lt;/li&gt;
&lt;li&gt;5）DNS轮询：通过在DNS-server上对一个域名设置多个ip解析，来扩充web-server性能及实施负载均衡的技术&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来看一下负载均衡随着量的扩大不断演进的过程&lt;/p&gt;

&lt;p&gt;1、单机架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;浏览器通过DNS-server，域名解析到ip，直接访问webserver，这种情况下的缺点显而易见&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）非高可用，web-server挂了整个系统就挂了&lt;/li&gt;
&lt;li&gt;2）扩展性差，当吞吐量达到web-server上限时，无法扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、DNS轮询&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过dns轮询将流量分到不同的webserver。这种情况下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）非高可用：DNS-server只负责域名解析ip，这个ip对应的服务是否可用，DNS-server是不保证的，假设有一个web-server挂了，部分服务会受到影响&lt;/li&gt;
&lt;li&gt;2）扩容非实时：DNS解析有一个生效周期&lt;/li&gt;
&lt;li&gt;3）暴露了太多的外网ip&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、nginx&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个方案在站点层与浏览器层之间加入了一个反向代理层，利用高性能的nginx来做反向代理，利用nginx来实现流量分流，但是nginx也是单点。&lt;/p&gt;

&lt;p&gt;4、nginx+keepalived&lt;/p&gt;

&lt;p&gt;为了解决nginx高可用的问题，keepalived出场了，当nginx挂了，另一个nginx就顶上来了&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这种情况下，仍会存在问题&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）资源利用率只有50%&lt;/li&gt;
&lt;li&gt;2）nginx仍然是接入单点，如果接入吞吐量超过的nginx的性能上限怎么办&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、lvs/f5&lt;/p&gt;

&lt;p&gt;nginx毕竟是软件，性能比tomcat好，但总有个上限，超出了上限，还是扛不住。lvs就不一样了，它实施在操作系统层面；f5的性能又更好了，它实施在硬件层面；它们性能比nginx好很多，例如每秒可以抗10w，这样可以利用他们来扩容，常见的架构图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;基本上公司到这一步基本就能解决接入层高可用、扩展性、负载均衡的问题。当然如果业务量继续扩大，我们还可以进行扩展&lt;/p&gt;

&lt;p&gt;5、DNS轮询&lt;/p&gt;

&lt;p&gt;facebook，google，baidu的PV是不是超过80亿呢，它们的域名只对应一个ip么，终点又是起点，还是得通过DNS轮询来进行扩容：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/fzjh5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;架构图可见，在lvs上再新增了水平扩展，能力更加强大了，其实在上面也可以使用这种方式，来水平扩展nginx，所以总体的思路就是&lt;strong&gt;水平扩展，垂直提升&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;前面将的都是相同服务实例的均衡，其实也存在异构服务器，也就是实例能力不相同的情况，也就是负载均衡的另一种体现，后端的service有可能部署在硬件条件不同的服务器上：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）如果对标最低配的服务器“均匀”分摊负载，高配的服务器的利用率不足；&lt;/li&gt;
&lt;li&gt;2）如果对标最高配的服务器“均匀”分摊负载，低配的服务器可能会扛不住；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;需要做到根据能力来实现负载均衡，其实在nginx中就有这个概念，我们通过权重来给服务器来实现分配&lt;/p&gt;

&lt;p&gt;1、静态权重&lt;/p&gt;

&lt;p&gt;为每个下游service设置一个“权重”，代表service的处理能力，来调整访问到每个service的概率，使用nginx做反向代理与负载均衡，就有类似的机制。&lt;/p&gt;

&lt;p&gt;这个方案的优点是：简单，能够快速的实现异构服务器的负载均衡。&lt;/p&gt;

&lt;p&gt;缺点也很明显：这个权重是固定的，无法自适应动态调整，而很多时候，服务器的处理能力是很难用一个固定的数值量化。&lt;/p&gt;

&lt;p&gt;2、动态权重&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）用一个动态权重来标识每个service的处理能力，默认初始处理能力相同，即分配给每个service的概率相等；&lt;/li&gt;
&lt;li&gt;2）每当service成功处理一个请求，认为service处理能力足够，权重动态+1；&lt;/li&gt;
&lt;li&gt;3）每当service超时处理一个请求，认为service处理能力可能要跟不上了，权重动态-10（权重下降会更快）；&lt;/li&gt;
&lt;li&gt;4）为了方便权重的处理，可以把权重的范围限定为[0, 100]，把权重的初始值设为60分。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这种情况下，我们需要使用过载保护，就是在服务器承受压力到瓶颈的时候，就能够维持在这个能力，而不是继续增加将其压垮，最简易的方式，服务端设定一个负载阈值，超过这个阈值的请求压过来，全部抛弃。这个方式不是特别优雅。&lt;/p&gt;

&lt;p&gt;我们在动态权重中，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）如果某一个service的连接上，连续3个请求都超时，即连续-10分三次，客户端就可以认为，服务器慢慢的要处理不过来了，得给这个service缓一小口气，于是设定策略：接下来的若干时间内，例如1秒（或者接下来的若干个请求），请求不再分配给这个service；防止其处理能力变为0&lt;/li&gt;
&lt;li&gt;2）如果某一个service的动态权重，降为了0（像连续10个请求超时，中间休息了3次还超时），客户端就可以认为，服务器完全处理不过来了，得给这个service喘一大口气，于是设定策略：接下来的若干时间内，例如1分钟（为什么是1分钟，根据经验，此时service一般在发生fullGC，差不多1分钟能回过神来），请求不再分配给这个service；但是要进行回复就要继续给他分配。&lt;/li&gt;
&lt;li&gt;3）可以有更复杂的保护策略…&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;单点系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;单点系统并不是一种架构思想，但是提到高可用，肯定就要知道单点系统，并不是所有的系统都能实现高可用，单点系统一般来说存在两个很大的问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;非高可用：既然是单点，master一旦发生故障，服务就会受到影响&lt;/li&gt;
&lt;li&gt;性能瓶颈：既然是单点，不具备良好的扩展性，服务性能总有一个上限，这个单点的性能上限往往就是整个系统的性能上限&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如我们常用的架构中，也有避免不了的单点，在这个互联网架构中，站点层、服务层、数据库的从库都可以通过冗余的方式来保证高可用，但至少&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nginx层是一个潜在的单点&lt;/li&gt;
&lt;li&gt;数据库写库master也是一个潜在的单点&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;再比如GFS&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/gfs.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;GFS的系统架构里主要有这么几种角色：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）client，就是发起文件读写的调用端&lt;/li&gt;
&lt;li&gt;（2）master，这是一个单点服务，它有全局事业，掌握文件元信息&lt;/li&gt;
&lt;li&gt;（3）chunk-server，实际存储文件额服务器&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个系统里，master也是一个单点的服务，Map-reduce系统里也有类似的全局协调的master单点角色。系统架构设计中，像nginx，db-master，gfs-master这样的单点服务是存在的，也是我们需要解决的&lt;/p&gt;

&lt;p&gt;1、shadow-master解决单点高可用问题&lt;/p&gt;

&lt;p&gt;“影子master”，顾名思义，服务正常时，它只是单点master的一个影子，在master出现故障时，shadow-master会自动变成master，继续提供服务。&lt;/p&gt;

&lt;p&gt;shadow-master它能够解决高可用的问题，并且故障的转移是自动的，不需要人工介入，但不足是它使服务资源的利用率降为了50%，业内经常使用keepalived+vip的方式实现这类单点的高可用。&lt;/p&gt;

&lt;p&gt;以GFS的master为例，master正常时：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）client会连接正常的master，shadow-master不对外提供服务&lt;/li&gt;
&lt;li&gt;（2）master与shadow-master之间有一种存活探测机制&lt;/li&gt;
&lt;li&gt;（3）master与shadow-master有相同的虚IP（virtual-IP）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/gfs1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/gfs2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当发现master异常时：shadow-master会自动顶上成为master，虚IP机制可以保证这个过程对调用方是透明的，比如我们k8s的master的三主机制，比如数据库的双master机制都是使用了这种方案，只不过数据库还需要实现双主的数据同步，实现一致性的要求。&lt;/p&gt;

&lt;p&gt;2、减少与单点的交互，是存在单点的系统优化的核心方向&lt;/p&gt;

&lt;p&gt;既然知道单点存在性能上限，单点的性能（例如GFS中的master）有可能成为系统的瓶颈，那么，减少与单点的交互，便成了存在单点的系统优化的核心方向。怎么来减少与单点的交互，这里提两种常见的方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;批量写&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;批量写是一种常见的提升单点性能的方式。比如利用数据库写单点生成做“ID生成器”，常规方法就是利用数据库写单点的auto increament id来生成和返回ID，这样生成ID的并发上限，取决于单点数据库的写性能上限。如果我们使用批量的方式&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/pl.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）中间加一个服务，每次从数据库拿出100个id&lt;/li&gt;
&lt;li&gt;（2）业务方需要ID&lt;/li&gt;
&lt;li&gt;（3）服务直接返回100个id中的1个，100个分配完，再访问数据库&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样一来，每分配100个才会写数据库一次，分配id的性能可以认为提升了100倍。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;客户端缓存&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;客户端缓存也是一种降低与单点交互次数，提升系统整体性能的方法。比如上面的GFS的master使用了客户端缓存减少了与master的交互&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1）GFS的调用客户端client要访问shenjian.txt，先查询本地缓存，miss了&lt;/li&gt;
&lt;li&gt;（2）client访问master问说文件在哪里，master告诉client在chunk3上&lt;/li&gt;
&lt;li&gt;（3）client把shenjian.txt存放在chunk3上记录到本地的缓存，然后进行文件的读写操作&lt;/li&gt;
&lt;li&gt;（4）未来client要访问文件，从本地缓存中查找到对应的记录，就不用再请求master了，可以直接访问chunk-server。如果文件发生了转移，chunk3返回client说“文件不在我这儿了”，client再访问master，询问文件所在的服务器。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据经验，这类缓存的命中非常非常高，可能在99.9%以上（因为文件的自动迁移是小概率事件），这样与master的交互次数就降低了1000倍。&lt;/p&gt;

&lt;p&gt;3、“DNS轮询”技术支持DNS-server返回不同的nginx外网IP，实现nginx负载均衡层的水平扩展。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/architecture/nginx.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;DNS-server部分，一个域名可以配置多个IP，每次DNS解析请求，轮询返回不同的IP，就能实现nginx的水平扩展，扩充负载均衡层的整体性能。&lt;/p&gt;

&lt;p&gt;数据库单点写库也是同样的道理，在数据量很大的情况下，可以通过水平拆分，来提升写入性能。&lt;/p&gt;

&lt;p&gt;遗憾的是，并不是所有的业务场景都可以水平拆分，例如秒杀业务，商品的条数可能不多，数据库的数据量不大，就不能通过水平拆分来提升秒杀系统的整体写性能&lt;/p&gt;

&lt;h3 id=&#34;高并发-高性能-扩展性&#34;&gt;高并发，高性能，扩展性&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrence/&#34;&gt;高并发（High Concurrency）&lt;/a&gt;是互联网分布式系统架构设计中必须考虑的因素之一，互联网分布式架构设计，提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;垂直扩展

&lt;ul&gt;
&lt;li&gt;机器配置升级，这种最原始的方式，也是提高性能最快的方式。所以是初期互联网业务发展非常迅猛的最推荐的方式，如果预算不是问题。&lt;/li&gt;
&lt;li&gt;架构完善，组件优化，其实也是属于垂直扩展，通过优化架构，能够提高单机的支持的能力，这个就需要大量的实践经验，调优经验，如：使用Cache来减少IO次数，使用异步来增加单服务吞吐量，使用无锁数据结构来减少响应时间；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;水平扩展，不管是提升单机硬件性能，还是提升单机架构性能，都有一个致命的不足：单机性能总是有极限的。所以互联网分布式架构设计高并发终极解决方案还是水平扩展：只要增加服务器数量，就能线性扩充系统性能。水平扩展在现在最直观的就是分布式集群的应用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;高并发是我们架构设计中最常见的，很多细节实现可以看&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrence/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;技术思想&#34;&gt;技术思想&lt;/h1&gt;

&lt;p&gt;1、技术和业务：技术和业务是相辅相成的，任何技术的初衷都是为了服务业务，发展技术的目的是为了更好的服务业务，一旦脱离业务，技术就失去了意义。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 企业架构</title>
          <link>https://kingjcy.github.io/post/architecture/enterprise-architecture/</link>
          <pubDate>Fri, 04 Sep 2020 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/enterprise-architecture/</guid>
          <description>&lt;p&gt;重下往上：基础平台，应用&lt;/p&gt;

&lt;h1 id=&#34;基础平台&#34;&gt;基础平台&lt;/h1&gt;

&lt;p&gt;目前主要是&lt;a href=&#34;https://kingjcy.github.io/post/cloud/compute/&#34;&gt;云计算&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;iaas基础设施建设&#34;&gt;IAAS基础设施建设&lt;/h2&gt;

&lt;p&gt;其实就是服务器，已经很成熟，大部分厂商，包括阿里云都是直接一台什么配置的服务器多少钱，其实就是一台vm，当天有钱也可以卖物理机，还可以买物理机回来自己搭建数据中心。&lt;/p&gt;

&lt;h2 id=&#34;paas平台建设&#34;&gt;PAAS平台建设&lt;/h2&gt;

&lt;p&gt;PAAS平台其实是基于vm基础上一套申请机器，扩缩容的流程，也可以基于vm搭建基础应用。AWS就是典型的&lt;/p&gt;

&lt;p&gt;随着docker虚拟化技术的发展，出现了一种新型的paas平台，就是基于docker的，这个其实比较倾向于应用，利用镜像能够快速的部署应用，扩缩容，十分轻量级，不像vm部署可能需要很长的时间，所以这个就比较倾向于应用了，而不是docker的分配。&lt;/p&gt;

&lt;p&gt;其实上面讲的都是应用型的PAAS，也就是APaaS，APAAS是一种面向IT企业和机构的云计算应用开发与部署平台。APaaS主要为应用提供运行环境和数据存储，能够将本地部署的传统应用直接部署到APaaS上。容器厂商和IaaS厂商的PaaS大致为APaaS。&lt;/p&gt;

&lt;p&gt;还有一种平台访问型的PASS，也就是IPAAS，大数据厂商的PaaS实际上是属于IPaaS。&lt;/p&gt;

&lt;p&gt;IPaaS是用于集成和协同的PaaS平台，不仅可以支持与现有云服务间的连接性，而且可以以安全的方式提供企业应用的访问能力。IPaaS主要用于集成和构建复合应用。&lt;/p&gt;

&lt;p&gt;基础平台并不是所有的企业都会建设的，只有大型的企业会建设这些东西，比如阿里云，他们有自己的数据中心，并且机器并不是全年都在使用的，而小企业只要在大企业搭建的云平台的基础上进行业务中台的建设就可以
一般paas平台都是以卖自己的产品并且在自己的加上运行的为主的企业会在发展的过程中会需要发展&lt;/p&gt;

&lt;p&gt;1、对外提供saas服务，但是机器会在一段时间内有使用峰值，正常情况下不需要那么多的机器，这些企业一般都是在云上租用的服务器，所以能够快速扩缩容能够应对需求，不用的时候不租用服务器可以降低成本&lt;/p&gt;

&lt;p&gt;2、使用云平台提供的paas服务，但是随着规模的扩大，本来昂贵的paas方案，越来越高，需要自己构建一套方案&lt;/p&gt;

&lt;p&gt;所以现在以docker为核心的paas平台是核心与主流，&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/paas&#34;&gt;核心流程&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;saas服务&#34;&gt;Saas服务&lt;/h2&gt;

&lt;p&gt;用户通过标准的 Web 浏览器来使用网络上的软件。从用户角度来说，这意味着前期无需在服务器或软件许可证授权上进行投资；从供应商角度来看，与常规的软件服务模式相比，维护一个应用软件的成本要相对低廉。SaaS供应商通常是按照客户所租用的软件模块来进行收费的，因此用户可以根据需求按需订购软件应用服务，而且SaaS的供应商会负责系统的部署、升级和维护。比如我们常用的邮箱服务等。&lt;/p&gt;

&lt;p&gt;SaaS提供商对应的用户是应用软件使用的终端用户。其实和我们下面说的应用是息息相关的。&lt;/p&gt;

&lt;p&gt;现在市场上也有saas应用已经很成熟了。其实就是我们开箱即用的服务，也可以理解为运行在paas平台上的应用。但是paas确实千变万化的。&lt;/p&gt;

&lt;h1 id=&#34;应用&#34;&gt;应用&lt;/h1&gt;

&lt;p&gt;其实就是运行在基础平台上的业务系统，可以是单体系统，可以是分布式系统。比如说使用最多的购物系统和打车系统。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/shopping/&#34;&gt;购物系统&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/car/&#34;&gt;打车系统&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其实系统架构都是演进的，并不是所有的企业都要走到最后的架构，必须以业务驱动为核心，比如你的企业并不需要大并发，单体系统就可以，有的微服务的SOA架构既可以，但是像淘宝等就需要服务化，微服务。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/architecture-evolution/&#34;&gt;架构的演进&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在不同的架构思想和体系中需要考虑的问题也不一样&lt;/p&gt;

&lt;p&gt;1、单体架构&lt;/p&gt;

&lt;p&gt;也就是我们常用的前后台体系&lt;/p&gt;

&lt;p&gt;应用的开发无非就是解决如下问题,其实就是我们&lt;a href=&#34;https://kingjcy.github.io/post/architecture/architecture/#架构要素&#34;&gt;常规的架构思想&lt;/a&gt;所要解决的问题：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;安全性&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net-http/&#34;&gt;web开发&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2、&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-web-safe/&#34;&gt;安全访问&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;高并发高性能&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-context/&#34;&gt;并发控制&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2、&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、异步处理：定一个channel，以goroutine来运行，用于接受异步返回的结果&lt;/p&gt;

&lt;p&gt;4、&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrence/&#34;&gt;并发&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;5、&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-goroutinechannel/&#34;&gt;goroutine并发&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;高可用&lt;/p&gt;

&lt;p&gt;可扩展&lt;/p&gt;

&lt;p&gt;容错性&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2、第一代微服务架构&amp;ndash;SOA&lt;/p&gt;

&lt;p&gt;考虑单体架构的所有问题，以及分布式带来的&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed/&#34;&gt;分布式问题&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、第一代微服务架构&amp;ndash;大中台&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;统一抽象规划在中台，中台建设&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;作为一种组织架构模式，“中台”突出的是规划控制和协调的能力，主要是将企业总线的瓶颈转化为中台服务的相互调用，而“前台”强调的是创新和灵活多变。这是一种快速设计和迭代的方法。&lt;/p&gt;

&lt;p&gt;4、下一代微服务架构&lt;/p&gt;

&lt;p&gt;去中心化&lt;/p&gt;

&lt;h2 id=&#34;小前台&#34;&gt;小前台&lt;/h2&gt;

&lt;p&gt;灵活多变，适应很多需求&lt;/p&gt;

&lt;h2 id=&#34;大中台&#34;&gt;大中台&lt;/h2&gt;

&lt;p&gt;其实就是将一些能够统一的业务进行统一规划，所以系统的接入和流出都是标准化的操作。&lt;/p&gt;

&lt;p&gt;可见随着中台的发展，中台已经开始分为业务中台，数据中台，技术中台等，其实就是中台越来越庞大，需要每个领域进行专注，每一种中台都往平台化的方向发展，便于使用。具体中台可以看&lt;a href=&#34;https://kingjcy.github.io/post/architecture/electronic-commerce/&#34;&gt;中台建设&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;轻后台&#34;&gt;轻后台&lt;/h2&gt;

&lt;p&gt;主要的实现业务的不能抽象统一的逻辑系统还是在后台，比如ERP系统等。
在建设好的中台上进行业务处理，和服务端开发，或者平台建设。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 对象存储云存储</title>
          <link>https://kingjcy.github.io/post/distributed/store/oss/</link>
          <pubDate>Thu, 04 Jun 2020 15:52:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/oss/</guid>
          <description>&lt;p&gt;不同的云厂商对它有不同的英文缩写命名。例如阿里云把自家的对象存储服务叫做OSS，华为云叫OBS，腾讯云叫COS，七牛叫Kodo，百度叫BOS，网易叫NOS……五花八门，反正都是一个技术。&lt;/p&gt;

&lt;h1 id=&#34;阿里云oss&#34;&gt;阿里云OSS&lt;/h1&gt;

&lt;p&gt;阿里云存储服务（Open Storage Service，简称OSS），是阿里云对外提供的海量，安全，低成本，高可靠的云存储服务。用户可以通过调用API，在任何应用、任何时间、任何地点上传和下载数据，也可以通过用户Web控制台对数据进行简单的管理。OSS适合存放任意文件类型，适合各种网站、开发企业及开发者使用。&lt;/p&gt;

&lt;h1 id=&#34;华为云obs&#34;&gt;华为云OBS&lt;/h1&gt;

&lt;h1 id=&#34;腾讯云cos&#34;&gt;腾讯云COS&lt;/h1&gt;

&lt;h1 id=&#34;七牛云kodo&#34;&gt;七牛云Kodo&lt;/h1&gt;

&lt;h1 id=&#34;百度云bos&#34;&gt;百度云BOS&lt;/h1&gt;

&lt;h1 id=&#34;技术方案&#34;&gt;技术方案&lt;/h1&gt;

&lt;h2 id=&#34;ceph&#34;&gt;ceph&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/ceph/ceph/&#34;&gt;ceph&lt;/a&gt;我们都知道同时支持对象存储，块存储和文件系统服务。&lt;/p&gt;

&lt;h2 id=&#34;minio&#34;&gt;minio&lt;/h2&gt;

&lt;p&gt;minio是一个基于Apache License V2.0开源协议的对象存储服务，它兼容亚马逊S3云存储服务，非常适合于存储大容量非结构化的数据，如图片，视频，日志文件等。而一个对象文件可以任意大小，从几KB到最大的5T不等。它是一个非常轻量级的服务，可以很简单的和其它的应用结合，类似于NodeJS, Redis或者MySQL。&lt;/p&gt;

&lt;p&gt;minio默认不计算MD5，除非传输给客户端的时候，所以很快，支持windows，有web页进行管理。&lt;/p&gt;

&lt;p&gt;从目前来看，如果想自建对象存储服务的话，有能力，规模比较大的话，采用ceph感觉更好一点。如果只是想要一个对象存储，要求没有那么多的话，可以采用minio。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- Distributed</title>
          <link>https://kingjcy.github.io/post/distributed/distributed/</link>
          <pubDate>Tue, 26 May 2020 20:10:41 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/distributed/</guid>
          <description>&lt;p&gt;分布式和集群相关的东西，已经是未来系统发展的趋势。&lt;/p&gt;

&lt;h1 id=&#34;分布式&#34;&gt;分布式&lt;/h1&gt;

&lt;p&gt;为什么会有分布式？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以很好的减少依赖（每个服务都只要开发测试自己的逻辑数据）&lt;/li&gt;
&lt;li&gt;可以简单的进行扩展，（分布式存储这种）&lt;/li&gt;
&lt;li&gt;能够很简单的支持高可用（负载均衡到多实例）&lt;/li&gt;
&lt;li&gt;容错性（将数据以备份的信息存储多分）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;单系统&#34;&gt;单系统&lt;/h2&gt;

&lt;p&gt;单块系统就是所有的代码都在一个工程里，最多可能就是通过maven等构件工具拆分了一下代码工程模块，不同的模块可以放在不同的工程代码里。&lt;/p&gt;

&lt;p&gt;很多流量很小的企业内部系统，比如OA、CRM、财务等系统，甚至可能就直接在一台机器的tomcat下部署一下。&lt;/p&gt;

&lt;p&gt;然后直接配置一下域名解析，就可以让这个系统的可能几十个，或者几百个用户通过访问域名来使用这个软件了。&lt;/p&gt;

&lt;p&gt;你哪怕就部署一台机器，这个系统也可以运行，只不过为了所谓的“高可用”，可能一般会部署两台机器，前面加一层负载均衡设备，这样其中一个机器挂了，另外一个机器上还有一个系统可以用。&lt;/p&gt;

&lt;h2 id=&#34;分布式系统&#34;&gt;分布式系统&lt;/h2&gt;

&lt;p&gt;分布式的主要特点就是分而治之，实现解耦，高性能的解决问题。比如把一个大的系统拆分为很多小的系统，甚至很多小的服务，然后几个人组成一个小组就专门维护其中一个小系统，或者每个人维护一个小服务。这样就可以分而治之，这样每个人可以专注维护自己的代码。不同的子系统之间，就是通过接口互相来回调用，每个子系统都有自己的数据库。&lt;/p&gt;

&lt;p&gt;下面是一些应用，都是采用了分布式的思想。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;比如在load balance后面的web server，就是一种基本的分布式处理请求的模式，他可以并行处理请求，提高响应效率，也就提高了性能&lt;/li&gt;
&lt;li&gt;比如hadoop中的hdfs它就是在不同节点上存储不同的数据，这就是一种扩大容量的分布式存储。一份数据三副本也实现分布式的数据存储，实现来分布式数据的容错能力&lt;/li&gt;
&lt;li&gt;比如redis在不同的节点上分布不同的slot来完成分布式的存储。但是每个实例的m/s的最终一致性也实现分布式的数据容错能力。&lt;/li&gt;
&lt;li&gt;比如etcd的分布式存储，实现强一致性，在各个节点的数据时刻一致，实现高可用。&lt;/li&gt;
&lt;li&gt;比如ceph一样实现各个节点上数据一致的分布式共享。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些都是分布式的应用，分布式只是一种可以使不同资源共同去完成一个目标的方法。分布式让不同的节点完成着不同的或者相同的任务，来提高能力，可以是计算，可以是存储，可以是容错。其实分布式更多的是一种思想。当然这种思想可以实现不同的事情，比如上面提到的，提高性能，提高存储能力，提高可用能力，提高容错能力，实现数据共享等等。&lt;/p&gt;

&lt;p&gt;通过分布式思想实现的系统，也就是我们常说的分布式系统，分布式系统相对于单系统来说，更加的复杂，很多传统的问题都需要分布式的解决方案。&lt;/p&gt;

&lt;h2 id=&#34;分布式系统所带来的技术问题&#34;&gt;分布式系统所带来的技术问题&lt;/h2&gt;

&lt;p&gt;那么大家这个时候可以思考一下，如果你的公司是采用这种分布式系统的方式来构建公司的一个大规模系统的，那么这个时候会涉及到哪些技术问题？&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式服务框架&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;你如果要让不同的子系统或者服务之间互相通信，首先必须有一套分布式服务框架。主要是做服务发现和服务间通信，也就是各个服务可以互相感知到对方在哪里，可以发送请求过去，可以通过HTTP或者RPC的方式。最常见的技术就是dubbo以及spring cloud，当然大厂一般都是自己有服务框架。基本上在这个领域目前还是阿里的java写框架一家独大，
我所使用的c/c++，golang都是使用标准库自己来实现这些服务发现通信的问题，所以也没有什么比较好的框架。目前在容器领域，随着k8s的模式的趋势发展，下一代的分布式框架开始衍生，出现了k8s的相关解决方案-&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/microservices/&#34;&gt;微服务&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式事务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一旦你的系统拆分为了多个子系统之后，那么一个贯穿全局的分布式事务应该怎么来实现？&lt;/p&gt;

&lt;p&gt;这个你需要了解TCC、最终一致性、2PC等&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-event/&#34;&gt;分布式事务的实现方案和开源技术&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;不同的系统之间如果需要在全局加锁获取某个资源的锁定，此时应该怎么来做？毕竟大家不是在一个JVM里了，不可能用synchronized来在多个子系统之间实现锁吧，是不是？&lt;/p&gt;

&lt;p&gt;所以这个时候就需要使用&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-lock/&#34;&gt;分布式锁&lt;/a&gt;来解决这些问题。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式存储&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果你原来就是个单块系统，那么你其实是可以在单个JVM里进行本地缓存就可以了，比如搞一个HashMap来缓存一些数据。但是现在你有很多个子系统，他们如果要共享一个缓存，你应该怎么办？是不是需要引入&lt;a href=&#34;https://kingjcy.github.io/post/database/redis/redis/&#34;&gt;Redis等缓存系统&lt;/a&gt;，实现数据缓存共享。&lt;/p&gt;

&lt;p&gt;还有很多的数据共享存储的实例，我们都需要使用到&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/store/&#34;&gt;分布式存储&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式消息系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在单块系统内，就一个JVM进程内部，你可以用类似LinkedList之类的数据结构作为一个本地内存里的队列。&lt;/p&gt;

&lt;p&gt;但是多个子系统之间要进行消息队列的传递呢？那是不是要引入类似RabbitMQ之类的分布式&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/mq-compare/&#34;&gt;消息队列中间件&lt;/a&gt;？&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式搜索系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果在单块系统内，你可以比如在本地就基于Lucene（支持全文索引的数据库系统）来开发一个全文检索模块，但是如果是分布式系统下的很多子系统，你还能直接基于Lucene吗？&lt;/p&gt;

&lt;p&gt;明显不行，你需要在系统里引入一个外部的分布式搜索系统，比如&lt;a href=&#34;https://kingjcy.github.io/post/database/elasticsearch/&#34;&gt;Elasticsearch&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他很多的技术&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;比如说&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-config/&#34;&gt;分布式配置中心&lt;/a&gt;、分布式日志中心、分布式监控告警中心、分布式会话，等等，都是分布式系统场景下你需要使用和了解的一些技术。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;一致性问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一致性问题其实也是分布式事务中的，但是这个问题比较突出，单独说一下，一致性又可以分为强一致性与弱一致性。&lt;/p&gt;

&lt;p&gt;强一致性可以理解为在任意时刻，所有节点中的数据是一样的。同一时间点，你在节点A中获取到key1的值与在节点B中获取到key1的值应该都是一样的。&lt;/p&gt;

&lt;p&gt;弱一致性包含很多种不同的实现，目前分布式系统中广泛实现的是最终一致性。所谓最终一致性，就是不保证在任意时刻任意节点上的同一份数据都是相同的，但是随着时间的迁移，不同节点上的同一份数据总是在向趋同的方向变化。也可以简单的理解为在一段时间后，节点间的数据会最终达到一致状态。&lt;/p&gt;

&lt;p&gt;所以对于一致性，一致的程度不同大体可以分为强、弱、最终一致性三类。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;强一致性
对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。比如小明更新V0到V1，那么小华读取的时候也应该是V1。&lt;/li&gt;
&lt;li&gt;弱一致性
如果能容忍后续的部分或者全部访问不到，则是弱一致性。比如小明更新VO到V1，可以容忍那么小华读取的时候是V0。&lt;/li&gt;
&lt;li&gt;最终一致性
如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。比如小明更新VO到V1，可以使得小华在一段时间之后读取的时候是V0。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;分布式的难点是一致性，基本解决方案&lt;/p&gt;

&lt;p&gt;CAP理论&lt;/p&gt;

&lt;p&gt;CAP理论指的是一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。其中分区容错性（Partition tolerance）是分布式的根本，是必须要实现的，所以一般都是实现AP和CP。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Consistency (一致性)：
“all nodes see the same data at the same time”,即更新操作成功并返回客户端后，所有节点在同一时间的数据完全一致，这就是分布式的一致性。一致性的问题在并发系统中不可避免，对于客户端来说，一致性指的是并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。&lt;/li&gt;
&lt;li&gt;Availability (可用性):
可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。&lt;/li&gt;
&lt;li&gt;Partition Tolerance (分区容错性):
即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。这个是必须要保障的，不然都没有分布式的意义了。
分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，对于用户而言并没有什么体验上的影响。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;假设N1和N2之间通信的时候网络突然出现故障，有用户向N1发送数据更新请求，那N1中的数据DB0将被更新为DB1，由于网络是断开的，N2中的数据库仍旧是DB0；

如果这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据DB1，怎么办呢？有二种选择，第一，牺牲数据一致性，响应旧的数据DB0给用户；第二，牺牲可用性，阻塞等待，直到网络连接恢复，数据更新操作完成之后，再给用户响应最新的数据DB1。

上面的过程比较简单，但也说明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。也就是说分布式系统不可能同时满足三个特性。这就需要我们在搭建系统时进行取舍了
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们分析一下既然可以满足两个，那么舍弃哪一个比较好呢？&lt;/p&gt;

&lt;p&gt;（1）满足CA舍弃P，也就是满足一致性和可用性，舍弃容错性。但是这也就意味着你的系统不是分布式的了，因为涉及分布式的想法就是把功能分开，部署到不同的机器上。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。&lt;/p&gt;

&lt;p&gt;（2）满足CP舍弃A，也就是满足一致性和容错性，舍弃可用性。如果你的系统允许有段时间的访问失效等问题，这个是可以满足的。就好比多个人并发买票，后台网络出现故障，你买的时候系统就崩溃了。设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。&lt;/p&gt;

&lt;p&gt;（3）满足AP舍弃C，也就是满足可用性和容错性，舍弃一致性。这也就是意味着你的系统在并发访问的时候可能会出现数据不一致的情况。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。&lt;/p&gt;

&lt;p&gt;实时证明，大多数都是牺牲了一致性。像12306还有淘宝网，就好比是你买火车票，本来你看到的是还有一张票，其实在这个时刻已经被买走了，你填好了信息准备买的时候发现系统提示你没票了。这就是牺牲了一致性。&lt;/p&gt;

&lt;p&gt;所以在牺牲强一致性，使用最终一致性的情况下，eBay 架构师 Dan Pritchett 提出了 BASE 理论，用于解决大规模分布式系统下的数据一致性问题。&lt;/p&gt;

&lt;p&gt;BASE 理论告诉我们：可以通过放弃系统在每个时刻的强一致性来换取系统的可扩展性。&lt;/p&gt;

&lt;p&gt;BASE 是 Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent (最终一致性)三个短语的缩写。是对CAP中AP的一个扩展&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基本可用:分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。&lt;/li&gt;
&lt;li&gt;软状态:允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是CAP中的不一致。&lt;/li&gt;
&lt;li&gt;最终一致:最终一致是指经过一段时间后，所有节点数据都将会达到一致。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是不是说牺牲一致性一定是最好的。就好比mysql中的事务机制，张三给李四转了100块钱，这时候必须保证张三的账户上少了100，李四的账户多了100。因此需要数据的一致性，而且什么时候转钱都可以，也需要可用性。但是可以转钱失败是可以允许的。&lt;/p&gt;

&lt;h2 id=&#34;分布式的发展&#34;&gt;分布式的发展&lt;/h2&gt;

&lt;p&gt;分布式是单机系统发展到一定时候的产物，其实和微服务的发展是一样的，微服务是分布式发展到一定阶段的概念，两者其实是差不多的，所以&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/microservices/&#34;&gt;微服务的演进&lt;/a&gt;也是分布式的发展。&lt;/p&gt;

&lt;h1 id=&#34;集群&#34;&gt;集群&lt;/h1&gt;

&lt;p&gt;集群就是集中一堆机器来一起做事。&lt;/p&gt;

&lt;p&gt;比如上面的webserver可以是一个集群，他们互不干扰，共同的处理着同一种任务。前面的load balance也可以分布式部署，也可以是一个集群，后段的数据库也可以分布式部署，也可以是一个集群，这个几个集群组合在一起可以是一个大的集群。&lt;/p&gt;

&lt;p&gt;比如hadoop中的hdfs也是一个集群，但是它们互相通信，能够知道各个节点的情况，同时由中心统一管理调度&lt;/p&gt;

&lt;p&gt;比如上面的redis也是一个集群，他们也互相通信，却是存在着无中心化的管理&lt;/p&gt;

&lt;p&gt;比如上面的etcd，ceph几个节点也是一个集群，要求节点上数据完全保持一致&lt;/p&gt;

&lt;p&gt;这些都是集群，所以集群通俗点说就是一堆节点，当然都是相关的，不相关的不会组合在一起&lt;/p&gt;

&lt;p&gt;所以说集群和分布式是不同的概念，好比“分头做事”和“一堆相关的人”。他们也是紧密相连的，由分布式一般都是集群，有集群一般都是分布式的。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 计数系统</title>
          <link>https://kingjcy.github.io/post/architecture/count/</link>
          <pubDate>Mon, 04 May 2020 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/count/</guid>
          <description>&lt;p&gt;很多业务都有“计数”需求，在业务复杂，计数扩展频繁，数据量大，并发量大的情况下，计数系统的架构演进与实践。&lt;/p&gt;

&lt;h1 id=&#34;初始架构&#34;&gt;初始架构&lt;/h1&gt;

&lt;p&gt;我们可以很容易想到，关注服务+粉丝服务+消息服务均提供相应接口，就能拿到相关计数数据。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样将所有的数据记录入表，然后对某个属性进行count就能得到计数的数据。这个方案叫做“count”计数法，在数据量并发量不大的情况下，最容易想到且最经常使用的就是这种方法，但随着数据量的上升，并发量的上升，这个方法的弊端将逐步展现：计算量特别大，访问数据特别多。&lt;/p&gt;

&lt;h1 id=&#34;计数外置的架构设计&#34;&gt;计数外置的架构设计&lt;/h1&gt;

&lt;p&gt;计数是一个通用的需求，有没有可能，这个计数的需求实现在一个通用的系统里，而不是由关注服务、粉丝服务、微博服务来分别来提供相应的功能呢（否则扩展性极差）？&lt;/p&gt;

&lt;p&gt;通过分析，上述微博的业务可以抽象成两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户(uid)维度的计数：用户的关注计数，粉丝计数，发布的微博计数&lt;/li&gt;
&lt;li&gt;微博消息(msg_id)维度的计数：消息转发计数，评论计数，点赞计数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;于是可以抽象出两个表，针对这两个维度来进行计数的存储：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;t_user_count (uid, gz_count, fs_count, wb_count);
t_msg_count (msg_id, forword_count, comment_count, praise_count);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;甚至可以更为抽象，一个表搞定所有计数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;t_count(id, type, c1, c2, c3, …)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过type来判断，id究竟是uid还是msg_id，但并不建议这么做。&lt;/p&gt;

&lt;p&gt;存储抽象完，再抽象出一个计数服务对这些数据进行管理，提供友善的RPC接口：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样，在查询一条微博消息的若干个计数的时候，不用进行多次数据库count操作，而会转变为一条数据的多个属性的查询。但是当有微博被转发、评论、点赞的时候，计数服务如何同步的进行计数的变更呢？如果让业务服务来调用计数服务，势必会导致业务系统与计数系统耦合。&lt;/p&gt;

&lt;p&gt;对于不关心下游结果的业务，可以使用MQ来解耦，在业务发生变化的时候，向MQ发送一条异步消息，通知计数系统计数发生了变化即可&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;计数外置，本质是数据的冗余，架构设计上，数据冗余必将引发数据的一致性问题，需要有机制来保证计数系统里的数据与业务系统里的数据一致，常见的方法有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于一致性要求比较高的业务，要有定期check并fix的机制，例如关注计数，粉丝计数，微博消息计数等&lt;/li&gt;
&lt;li&gt;对于一致性要求比较低的业务，即使有数据不一致，业务可以接受，例如微博浏览数，微博转发数等&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;计数外置缓存优化&#34;&gt;计数外置缓存优化&lt;/h2&gt;

&lt;p&gt;计数外置很大程度上解决了计数存取的性能问题，但是否还有优化空间呢？像关注计数，粉丝计数，微博消息计数，变化的频率很低，查询的频率很高，这类读多些少的业务场景，非常适合使用缓存来进行查询优化，减少数据库的查询次数，降低数据库的压力。&lt;/p&gt;

&lt;p&gt;但是，缓存是kv结构的，无法像数据库一样，设置成t_uid_count(uid, c1, c2, c3)这样的schema，如何来对kv进行设计呢？缓存kv结构的value是计数，看来只能在key上做设计，很容易想到，可以使用uid:type来做key，存储对应type的计数。&lt;/p&gt;

&lt;p&gt;对于uid=123的用户，其关注计数，粉丝计数，微博消息计数的缓存就可以设计为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此时对应的counting-service架构变为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个“计数外置缓存优化”方案，可以总结为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用缓存来保存读多写少的计数（其实写多读少，一致性要求不高的计数，也可以先用缓存保存，然后定期刷到数据库中，以降低数据库的读写压力）&lt;/li&gt;
&lt;li&gt;使用id:type的方式作为缓存的key，使用count来作为缓存的value&lt;/li&gt;
&lt;li&gt;多次读取缓存来查询多个uid的计数&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;缓存批量读取优化&#34;&gt;缓存批量读取优化&lt;/h2&gt;

&lt;p&gt;缓存的使用能够极大降低数据库的压力，但多次缓存交互依旧存在优化空间，有没有办法进一步优化呢？&lt;/p&gt;

&lt;p&gt;不要陷入思维定式，谁说value一定只能是一个计数，难道不能多个计数存储在一个value中么？缓存kv结构的key是uid，value可以是多个计数同时存储。&lt;/p&gt;

&lt;p&gt;对于uid=123的用户，其关注计数，粉丝计数，微博消息计数的缓存就可以设计为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样多个用户，多个计数的查询就可以一次搞定。&lt;/p&gt;

&lt;p&gt;这个“计数外置缓存批量优化”方案，可以总结为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用id作为key，使用同一个id的多个计数的拼接作为value&lt;/li&gt;
&lt;li&gt;多个id的多个计数查询，一次搞定&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;计数扩展性优化&#34;&gt;计数扩展性优化&lt;/h2&gt;

&lt;p&gt;考虑完效率，架构设计上还需要考虑扩展性，如果uid除了关注计数，粉丝计数，微博计数，还要增加一个计数，就需要变更表结构，频繁的变更数据库schema的结构显然是不可取的。&lt;/p&gt;

&lt;p&gt;我们可以这样设计表结构来通过行来扩展属性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;t_user_count(uid, count_key, count_value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如果需要新增一个计数XX_count，只需要增加一行即可，而不需要变更表结构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/count/count7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- Event And Distributed Event</title>
          <link>https://kingjcy.github.io/post/distributed/distributed-event/</link>
          <pubDate>Tue, 07 Apr 2020 14:53:56 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/distributed-event/</guid>
          <description>&lt;p&gt;事务提供一种机制将一个活动涉及的所有操作纳入到一个不可分割的执行单元，组成事务的所有操作只有在所有操作均能正常执行的情况下方能提交，只要其中任一操作执行失败，都将导致整个事务的回滚。简单地说，事务提供一种“要么什么都不做，要么做全套（All or Nothing）”机制。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/event&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;数据库事务&#34;&gt;数据库事务&lt;/h1&gt;

&lt;p&gt;数据库事务中的四大特性，ACID:&lt;/p&gt;

&lt;p&gt;A:原子性(Atomicity)&lt;/p&gt;

&lt;p&gt;一个事务(transaction)中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。
就像你买东西要么交钱收货一起都执行，要么要是发不出货，就退钱。&lt;/p&gt;

&lt;p&gt;C:一致性(Consistency)&lt;/p&gt;

&lt;p&gt;事务的一致性指的是在一个事务执行之前和执行之后数据库都必须处于一致性状态。如果事务成功地完成，那么系统中所有变化将正确地应用，系统处于有效状态。如果在事务中出现错误，那么系统中的所有变化将自动地回滚，系统返回到原始状态。&lt;/p&gt;

&lt;p&gt;I:隔离性(Isolation)&lt;/p&gt;

&lt;p&gt;指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看到中间状态的数据。
打个比方，你买东西这个事情，是不影响其他人的。&lt;/p&gt;

&lt;p&gt;D:持久性(Durability)&lt;/p&gt;

&lt;p&gt;指的是只要事务成功结束，它对数据库所做的更新就必须永久保存下来。即使发生系统崩溃，重新启动数据库系统后，数据库还能恢复到事务成功结束时的状态。
打个比方，你买东西的时候需要记录在账本上，即使老板忘记了那也有据可查。&lt;/p&gt;

&lt;p&gt;正常数据库操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 开始会话
2. 开始事务
3. 操作1，2，3，4。。。。
4. 提交／回滚事务
5. 完成会话
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很清晰的看出什么是事务。&lt;/p&gt;

&lt;h1 id=&#34;分布式事务&#34;&gt;分布式事务&lt;/h1&gt;

&lt;p&gt;分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。&lt;/p&gt;

&lt;h2 id=&#34;分布式事务产生的原因&#34;&gt;分布式事务产生的原因&lt;/h2&gt;

&lt;p&gt;从上面本地事务来看，我们可以看为两块，一个是service产生多个节点（微服务，将服务拆分成多个，然后按一定步骤调用，但是属于一个事务），另一个是resource（多个数据中心，分库分表）产生多个节点。&lt;/p&gt;

&lt;p&gt;举个互联网常用的交易业务为例&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/event1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图中包含了库存和订单两个独立的微服务，每个微服务维护了自己的数据库。&lt;/p&gt;

&lt;p&gt;在交易系统的业务逻辑中，一个商品在下单之前需要先调用库存服务，进行扣除库存，再调用订单服务，创建订单记录。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/event2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，如果多个数据库之间的数据更新没有保证事务，将会导致出现子系统数据不一致，业务出现问题。&lt;/p&gt;

&lt;h2 id=&#34;分布式事务的难点&#34;&gt;分布式事务的难点&lt;/h2&gt;

&lt;p&gt;事务的原子性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;事务操作跨不同节点，当多个节点某一节点操作失败时，需要保证多节点操作的要么什么都不做，要么做全套（All or Nothing）的原子性。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;事务的一致性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;当发生网络传输故障或者节点故障，节点间数据复制通道中断，在进行事务操作时需要保证数据一致性，保证事务的任何操作都不会使得数据违反数据库定义的约束、触发器等规则。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;事务的隔离性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;事务隔离性的本质就是如何正确处理多个并发事务的读写冲突和写写冲突，因为在分布式事务控制中，可能会出现提交不同步的现象，这个时候就有可能出现“部分已经提交”的事务。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时并发应用访问数据如果没有加以控制，有可能出现“脏读”问题。&lt;/p&gt;

&lt;p&gt;这些的最终影响是导致数据出现不一致，可见现在基本的ACID四大特性，已经无法满足我们分布式事务，所以引入了CAP定理，又被叫作布鲁尔定理。&lt;/p&gt;

&lt;h2 id=&#34;cap-base&#34;&gt;CAP&amp;amp;BASE&lt;/h2&gt;

&lt;p&gt;在分布式系统中，一致性(Consistency)、可用性(Availability)和分区容忍性(Partition Tolerance)3 个要素最多只能同时满足两个，不可兼得。其中，分区容忍性又是不可或缺的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C (一致性):分布式环境下，多个节点的数据是否强一致。&lt;/li&gt;
&lt;li&gt;A (可用性)：非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的，这里的正确指的是比如应该返回50，而不是返回40。&lt;/li&gt;
&lt;li&gt;P (分区容错性):当出现网络分区后，系统能够继续工作。打个比方，这里个集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正常工作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据时间证明cap不能共存。比舍弃其中一个，看使用场景取舍。比如Cassandra、Dynamo 等，默认优先选择 AP，弱化 C;HBase、MongoDB 等，默认优先选择 CP，弱化 A。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/cap&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;两阶段提交协议(简称2PC)是实现分布式事务较为经典的方案，但 2PC 的可扩展性很差，在分布式架构下应用代价较大,mysql的分布式集群就是使用这个分布式事务的解决方法。&lt;/p&gt;

&lt;p&gt;eBay 架构师 Dan Pritchett 提出了 BASE 理论，用于解决大规模分布式系统下的数据一致性问题。&lt;/p&gt;

&lt;p&gt;BASE 理论告诉我们：可以通过放弃系统在每个时刻的强一致性来换取系统的可扩展性。&lt;/p&gt;

&lt;p&gt;BASE 是 Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent (最终一致性)三个短语的缩写。是对CAP中AP的一个扩展&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基本可用:分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。&lt;/li&gt;
&lt;li&gt;软状态:允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是CAP中的不一致。&lt;/li&gt;
&lt;li&gt;最终一致:最终一致是指经过一段时间后，所有节点数据都将会达到一致。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;它的核心思想是即使无法做到强一致性（CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性。&lt;/p&gt;

&lt;p&gt;这里的关键词是“一定时间” 和 “最终”，“一定时间”和数据的特性是强关联的，不同业务不同数据能够容忍的不一致时间是不同的。&lt;/p&gt;

&lt;p&gt;例如支付类业务是要求秒级别内达到一致，因为用户时时关注；用户发的最新微博，可以容忍 30 分钟内达到一致的状态，因为用户短时间看不到明星发的微博是无感知的。&lt;/p&gt;

&lt;p&gt;而“最终”的含义就是不管多长时间，最终还是要达到一致性的状态。&lt;/p&gt;

&lt;p&gt;BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充：CAP 理论是忽略延时的，而实际应用中延时是无法避免的。&lt;/p&gt;

&lt;p&gt;这一点就意味着完美的 CP 场景是不存在的，即使是几毫秒的数据复制延迟，在这几毫秒时间间隔内，系统是不符合 CP 要求的。&lt;/p&gt;

&lt;p&gt;因此 CAP 中的 CP 方案，实际上也是实现了最终一致性，只是“一定时间”是指几毫秒而已。&lt;/p&gt;

&lt;p&gt;AP 方案中牺牲一致性只是指发生分区故障期间，而不是永远放弃一致性。&lt;/p&gt;

&lt;p&gt;这一点其实就是 BASE 理论延伸的地方，分区期间牺牲一致性，但分区故障恢复后，系统应该达到最终一致性。&lt;/p&gt;

&lt;h2 id=&#34;数据一致性模型&#34;&gt;数据一致性模型&lt;/h2&gt;

&lt;p&gt;前面介绍的 BASE 模型提过“强一致性”和“最终一致性”，下面对这些一致性模型展开介绍。&lt;/p&gt;

&lt;p&gt;分布式系统通过复制数据来提高系统的可靠性和容错性，并且将数据的不同的副本存放在不同的机器上，由于维护数据副本的一致性代价很高，因此许多系统采用弱一致性来提高性能。&lt;/p&gt;

&lt;p&gt;下面介绍常见的一致性模型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;强一致性：要求无论更新操作是在哪个数据副本上执行，之后所有的读操作都要能获得最新的数据。 对于单副本数据来说，读写操作是在同一数据上执行的，容易保证强一致性。对多副本数据来说，则需要使用分布式事务协议。&lt;/li&gt;
&lt;li&gt;弱一致性：在这种一致性下，用户读到某一操作对系统特定数据的更新需要一段时间，我们将这段时间称为&amp;rdquo;不一致性窗口&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;最终一致性：是弱一致性的一种特例，在这种一致性下系统保证用户最终能够读取到某操作对系统特定数据的更新（读取操作之前没有该数据的其他更新操作）。 &amp;ldquo;不一致性窗口&amp;rdquo;的大小依赖于交互延迟、系统的负载，以及数据的副本数等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;系统选择哪种一致性模型取决于应用对一致性的需求，所选取的一致性模型还会影响到系统如何处理用户的请求以及对副本维护技术的选择等。&lt;/p&gt;

&lt;p&gt;后面将基于上面介绍的一致性模型分别介绍分布式事务的解决方案。&lt;/p&gt;

&lt;h1 id=&#34;分布式事务解决方案&#34;&gt;分布式事务解决方案&lt;/h1&gt;

&lt;p&gt;有了上面的理论基础后，这里介绍开始介绍几种常见的分布式事务的解决方案。&lt;/p&gt;

&lt;h2 id=&#34;两阶段提交-2pc-two-phase-commit-方案-强一致性-还有3pc&#34;&gt;两阶段提交（2PC, Two Phase Commit）方案（强一致性），还有3PC&lt;/h2&gt;

&lt;p&gt;mysql使用的这种模式。核心就是通过事务协调器将多个事务合并为一个大的事务。&lt;/p&gt;

&lt;p&gt;基于XA协议的两阶段提交:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一阶段是表决阶段，所有参与者都将本事务能否成功的信息反馈发给协调者；&lt;/li&gt;
&lt;li&gt;第二阶段是执行阶段，协调者根据所有参与者的反馈，通知所有参与者，步调一致地在所有分支上提交或者回滚;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/2pc&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod11&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod12&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;单点问题：事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。&lt;/li&gt;
&lt;li&gt;同步阻塞：在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。&lt;/li&gt;
&lt;li&gt;数据不一致：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能。比如：在第二阶段中，假设协调者发出了事务 Commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 Commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总的来说，XA 协议比较简单，成本较低，但是其单点问题，以及不能支持高并发(由于同步阻塞)依然是其最大的弱点。&lt;/p&gt;

&lt;h2 id=&#34;基于可靠消息的最终一致性方案&#34;&gt;基于可靠消息的最终一致性方案&lt;/h2&gt;

&lt;p&gt;本地消息表的方案最初是由 eBay 提出，核心思路是将分布式事务拆分成本地事务进行处理。通过可靠消息系统进行同步，可以是各种消息中间件，也可以是MQ等。&lt;/p&gt;

&lt;p&gt;业务处理服务在业务事务提交之前，向实时消息服务请求发送消息，实时消息服务只记录消息数据，而不是真正的发送。业务处理服务在业务事务提交之后，向实时消息服务确认发送。只有在得到确认发送指令后，实时消息服务才会真正发送。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;方案通过在事务主动发起方额外新建事务消息表，事务发起方处理业务和记录事务消息在本地事务中完成，轮询事务消息表的数据发送事务消息，事务被动方基于消息中间件消费事务消息表中的事务。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面把分布式事务最先开始处理的事务方称为事务主动方，在事务主动方之后处理的业务内的其他事务称为事务被动方。&lt;/p&gt;

&lt;p&gt;为了方便理解，下面继续以电商下单为例进行方案解析，这里把整个过程简单分为扣减库存，订单创建 2 个步骤。&lt;/p&gt;

&lt;p&gt;库存服务和订单服务分别在不同的服务器节点上，其中库存服务是事务主动方，订单服务是事务被动方。&lt;/p&gt;

&lt;p&gt;事务的主动方需要额外新建事务消息表，用于记录分布式事务的消息的发生、处理状态。&lt;/p&gt;

&lt;p&gt;整个业务处理流程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod10&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;步骤1：事务主动方处理本地事务。&lt;/p&gt;

&lt;p&gt;事务主动方在本地事务中处理业务更新操作和写消息表操作。上面例子中库存服务阶段在本地事务中完成扣减库存和写消息表(图中 1、2)。&lt;/p&gt;

&lt;p&gt;步骤 2：事务主动方通过消息中间件，通知事务被动方处理事务通知事务待消息。
消息中间件可以基于 Kafka、RocketMQ 消息队列，事务主动方主动写消息到消息队列，事务消费方消费并处理消息队列中的消息。&lt;/p&gt;

&lt;p&gt;上面例子中，库存服务把事务待处理消息写到消息中间件，订单服务消费消息中间件的消息，完成新增订单（图中 3 - 5）。&lt;/p&gt;

&lt;p&gt;步骤 3：事务被动方通过消息中间件，通知事务主动方事务已处理的消息。
上面例子中，订单服务把事务已处理消息写到消息中间件，库存服务消费中间件的消息，并将事务消息的状态更新为已完成(图中 6 - 8)。&lt;/p&gt;

&lt;p&gt;为了数据的一致性，当处理错误需要重试，事务发送方和事务接收方相关业务处理需要支持幂等。&lt;/p&gt;

&lt;p&gt;具体保存一致性的容错处理如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当步骤 1 处理出错，事务回滚，相当于什么都没发生。&lt;/li&gt;
&lt;li&gt;当步骤 2、步骤 3 处理出错，由于未处理的事务消息还是保存在事务发送方，事务发送方可以定时轮询为超时消息数据，再次发送到消息中间件进行处理。事务被动方消费事务消息重试处理。&lt;/li&gt;
&lt;li&gt;如果是业务上的失败，事务被动方可以发消息给事务主动方进行回滚。&lt;/li&gt;
&lt;li&gt;如果多个事务被动方已经消费消息，事务主动方需要回滚事务时需要通知事务被动方回滚。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tcc事务补偿型方案-原子性-最终一致性&#34;&gt;TCC事务补偿型方案（原子性，最终一致性）&lt;/h2&gt;

&lt;p&gt;某业务模型如图，由服务 A、服务B、服务C、服务D 共同组成的一个微服务架构系统。服务A 需要依次调用服务B、服务C 和服务D 共同完成一个操作。当服务A 调用服务D 失败时，若要保证整个系统数据的一致性，就要对服务B 和服务C 的invoke 操作进行回滚，执行反向的revert 操作。回滚成功后，整个微服务系统是数据一致的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod22&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实现：一个完整的业务活动由一个主业务服务于若干的从业务服务组成。主业务服务负责发起并完成整个业务活动。从业务服务提供TCC型业务操作。业务活动管理器控制业务活动的一致性，它登记业务活动的操作，并在业务活动提交时确认所有的TCC型操作的Confirm操作，在业务活动取消时调用所有TCC型操作的Cancel操作。&lt;/li&gt;
&lt;li&gt;成本：实现TCC操作的成本较高，业务活动结束的时候Confirm和Cancel操作的执行成本。业务活动的日志成本。TCC 的 Try、Confirm 和 Cancel 操作功能要按具体业务来实现，业务耦合度较高，提高了开发成本。&lt;/li&gt;
&lt;li&gt;使用范围：强隔离性，严格一致性要求的业务活动。适用于执行时间较短的业务，比如处理账户或者收费等等。&lt;/li&gt;
&lt;li&gt;特点：不与具体的服务框架耦合，位于业务服务层，而不是资源层，可以灵活的选择业务资源的锁定粒度。TCC里对每个服务资源操作的是本地事务，数据被锁住的时间短，可扩展性好，可以说是为独立部署的SOA服务而设计的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;TCC方案很少用人使用，因为事务回滚操作实际上是严重依赖于手动编写代码来进行回滚和补偿操作，这样的话就会造成补偿代码过多，使得项目非常难以维护。比较适合的场景就是除非真的一致性要求非常高，是系统中的核心业务场景，例如常见的就是资金类的场景，那可以用TCC方案。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;为了方便理解，下面以电商下单为例进行方案解析，这里把整个过程简单分为扣减库存，订单创建 2 个步骤，库存服务和订单服务分别在不同的服务器节点上。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Try 阶段&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从执行阶段来看，与传统事务机制中业务逻辑相同。但从业务角度来看，却不一样。&lt;/p&gt;

&lt;p&gt;TCC 机制中的 Try 仅是一个初步操作，它和后续的确认一起才能真正构成一个完整的业务逻辑，这个阶段主要完成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;完成所有业务检查( 一致性 ) 。&lt;/li&gt;
&lt;li&gt;预留必须业务资源( 准隔离性 ) 。&lt;/li&gt;
&lt;li&gt;Try 尝试执行业务。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TCC 事务机制以初步操作（Try）为中心的，确认操作（Confirm）和取消操作（Cancel）都是围绕初步操作（Try）而展开。&lt;/p&gt;

&lt;p&gt;因此，Try 阶段中的操作，其保障性是最好的，即使失败，仍然有取消操作（Cancel）可以将其执行结果撤销。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod23&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;假设商品库存为 100，购买数量为 2，这里检查和更新库存的同时，冻结用户购买数量的库存，同时创建订单，订单状态为待确认。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Confirm / Cancel 阶段&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;根据 Try 阶段服务是否全部正常执行，继续执行确认操作（Confirm）或取消操作（Cancel）。&lt;/p&gt;

&lt;p&gt;Confirm 和 Cancel 操作满足幂等性，如果 Confirm 或 Cancel 操作执行失败，将会不断重试直到执行完成。&lt;/p&gt;

&lt;p&gt;Confirm：当 Try 阶段服务全部正常执行， 执行确认业务逻辑操作&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod24&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里使用的资源一定是 Try 阶段预留的业务资源。在 TCC 事务机制中认为，如果在 Try 阶段能正常的预留资源，那 Confirm 一定能完整正确的提交。&lt;/p&gt;

&lt;p&gt;Confirm 阶段也可以看成是对 Try 阶段的一个补充，Try+Confirm 一起组成了一个完整的业务逻辑。&lt;/p&gt;

&lt;p&gt;Cancel：当 Try 阶段存在服务执行失败， 进入 Cancel 阶段&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod25&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cancel 取消执行，释放 Try 阶段预留的业务资源，上面的例子中，Cancel 操作会把冻结的库存释放，并更新订单状态为取消。&lt;/p&gt;

&lt;h2 id=&#34;最大努力通知型-弱一致性&#34;&gt;最大努力通知型（弱一致性）&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;实现：业务活动的主动方在完成处理之后向业务活动的被动方发送消息，允许消息丢失。业务活动的被动方根据定时策略，向业务活动的主动方查询，恢复丢失的业务消息。
约束：被动方的处理结果不影响主动方的处理结果。
成本：业务查询与校对系统的建设成本。
使用范围：对业务最终一致性的时间敏感度低。跨企业的业务活动。
特点：业务活动的主动方在完成业务处理之后，向业务活动的被动方发送通知消息。主动方可以设置时间阶梯通知规则，在通知失败后按规则重复通知，知道通知N次后不再通知。主动方提供校对查询接口给被动方按需校对查询，用户恢复丢失的业务消息。
适用范围：银行通知，商户通知。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/event/mothod3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最大努力通知方案的主要思路如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;系统A本地事务执行完毕之后，发送个消息到消息服务MQ&lt;/li&gt;
&lt;li&gt;这里会有个专门消费消息服务MQ的最大努力通知服务，该服务会消费消息服务MQ，然后写入数据库中记录下来或者是放入个内存队列，接下来调用系统B的接口&lt;/li&gt;
&lt;li&gt;要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是失败就放弃。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前国内互联网公司大都是这么玩儿的，要不你使用RocketMQ支持的，要不你就基于其他MQ中间件自己封装一套类似的逻辑，总之思路就是这样的。其实就是MQ是上面可靠消息的一种，这边只是强调不一定可靠，还有通知。&lt;/p&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;从应用角度看，分布式事务的现实场景常常无法规避，在有能力给出其他解决方案前，2PC也是一个不错的选择。&lt;/p&gt;

&lt;p&gt;对购物转账等电商和金融业务，中间件层的2PC最大问题在于业务不可见，一旦出现不可抗力或意想不到的一致性破坏，如数据节点永久性宕机，业务难以根据2PC的日志进行补偿。金融场景下，数据一致性是命根，业务需要对数据有百分之百的掌控力，建议使用TCC这类分布式事务模型，或基于消息队列的柔性事务框架，这两种方案都在业务层实现，业务开发者具有足够掌控力，可以结合SOA框架来架构，包括Dubbo、Spring Cloud等&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 架构的演进</title>
          <link>https://kingjcy.github.io/post/architecture/architecture-evolution/</link>
          <pubDate>Thu, 05 Mar 2020 19:11:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/architecture-evolution/</guid>
          <description>&lt;p&gt;随着计算机软件的发展，不同的应用在落地，应用架构随着规模的越来越大，也在一步步的进行演进，从最初的单体架构，到后来的集群，然后分布式架构一步步的发展着。&lt;/p&gt;

&lt;h1 id=&#34;架构演变&#34;&gt;架构演变&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/architecture-change&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;整个过程是重单体架构向微服务发展的过程，下面我们来看看每一代架构的发展。&lt;/p&gt;

&lt;h2 id=&#34;单体架构&#34;&gt;单体架构&lt;/h2&gt;

&lt;p&gt;以前使用 Laravel 做 web 项目时，是根据 MVC 去划分目录结构的，即 Controller 层处理业务逻辑，Model 层处理数据库的 CURD，View 层处理数据渲染与页面交互。以及 MVP、MVVM 都是将整个项目的代码是集中在一个代码库中，进行业务处理。这种单一聚合代码的方式在前期实现业务的速度很快，但在后期会暴露很多问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、开发与维护困难：随着业务复杂度的增加，代码的耦合度往往会变高，多个模块相互耦合后不易横向扩展。
2、效率和可靠性低：过大的代码量将降低响应速度，应用潜在的安全问题也会累积。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;单体架构最终走向了单体地狱，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、代码全量编译成一个实例，因为一个模块的问题可能会导致全量的模块不能使用
2、问题难以地位，比如有一个死循环，会导致cpu占用100%，还会影响机器上其他的应用的使用，同时，很难知道是哪个模块出问题
3、代码量大，可能编译启动就需要很长时间，到最后就不可维护了，而且线上出问题就没办法处理了，因为重启都需要很长的时间
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个时候我们使用的组织架构就是最简单的&lt;strong&gt;前台-后台&lt;/strong&gt;在一个实例进程中。&lt;/p&gt;

&lt;h3 id=&#34;垂直拆分的垂直架构&#34;&gt;垂直拆分的垂直架构&lt;/h3&gt;

&lt;p&gt;单体架构的瓶颈太严重了，变的越来越臃肿，每次开发业务，都要将全部功能进行回归，于是就将前后端进行拆分成不同的进程，通过报文进行相互调用，在一定程度上实现了解耦，但是其实并没有很强的水平扩展的能力，所以还算是单体架构。&lt;/p&gt;

&lt;p&gt;在这个时候我们使用的组织架构还是最简单的架构&lt;strong&gt;前台-后台&lt;/strong&gt;,但是前台后台已经运行在不同的实例中，实现了一定的解耦，由最初的mvc的api的调用，变成了http的调用模式。&lt;/p&gt;

&lt;p&gt;但是随着规模越来越大，每个模块很快各种瓶颈有出现了，需要进行各种水平扩展，这个时候就需要进行服务化，使用分布式系统了。&lt;/p&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;架构经过了垂直拆分，已具有一定的解耦性，但是随着规模的扩展，每个流程都出现了单机处理的瓶颈，需要进行水平扩展，这时候就需要集群来并行共同处理业务来提高处理的能力。&lt;/p&gt;

&lt;p&gt;这个时候我们使用的组织架构还是最简单的架构&lt;strong&gt;前台-后台&lt;/strong&gt;，但是后台已经有了进一步的垂直拆分和水平扩展，随着拆分扩展的越来越多，也就有了&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/microservices/&#34;&gt;微服务的概念，下一代架构的诞生&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;第一代微服务-服务化&#34;&gt;第一代微服务&amp;mdash;服务化&lt;/h2&gt;

&lt;p&gt;现在传统的mvc架构带来的单体问题已经十分严重了，基本上已经不再使用了，提倡微服务的思想。&lt;/p&gt;

&lt;p&gt;第一代微服务架构是建立在服务化的基础之上的，什么是服务化？核心就是不同服务之间的通信，一种以服务为核心的解决方案。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务注册&lt;/li&gt;
&lt;li&gt;服务发布&lt;/li&gt;
&lt;li&gt;服务调用&lt;/li&gt;
&lt;li&gt;服务监控&lt;/li&gt;
&lt;li&gt;负载均衡&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实简单的说就是将一些服务封装起来，对外提供tcp/http/rpc接口，不需要关心内部的细节，内部有专门的人去维护，就是对这个服务的服务化，这个封装可以是新增一个服务层，也可以是在这个服务的基础提供接口等。&lt;/p&gt;

&lt;p&gt;其中最著名的就是SOA的面向服务架构思想。&lt;/p&gt;

&lt;h3 id=&#34;soa&#34;&gt;SOA&lt;/h3&gt;

&lt;p&gt;SOA是什么？SOA全英文是Service-Oriented Architecture，中文意思是面向服务架构，是一种思想，一种方法论，一种分布式的服务架构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/20180305.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实就是每个服务独立治理，相互解耦，服务共享不在重复，通过注册中心来服务发现调用的散装架构。我们最常见的架构就是&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Spring Cloud为开发者提供了快速构建分布式系统的通用模型的工具（包括配置管理、服务发现、熔断器、智能路由、微代理、控制总线、一次性令牌、全局锁、领导选举、分布式会话、集群状态等）&lt;/li&gt;
&lt;li&gt;Dubbo是一个阿里巴巴开源出来的一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案，其实主要解决的服务间通信的问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在这个时候组织架构就发生了很大的变化，基本都是要加上一层网关或者企业总线，也就是&lt;strong&gt;前台-网关（ESB）-后台&lt;/strong&gt;的结构。&lt;/p&gt;

&lt;h2 id=&#34;下一代微服务-去中心化&#34;&gt;下一代微服务&amp;ndash;去中心化&lt;/h2&gt;

&lt;h3 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h3&gt;

&lt;p&gt;随着注册中心的出现，任何调用都走网关，网关的瓶颈有到来，于是出现了下一代的微服务架构service mesh，主要落地的项目就是istio，当然还有其他的一些项目，主要是去中心化的设计。&lt;/p&gt;

&lt;p&gt;目前流行的 Service Mesh 开源软件有 Linkerd、Envoy 和 Istio，而最近 Buoyant（开源 Linkerd 的公司）又发布了基于 Kubernetes 的 Service Mesh 开源项目 Conduit，来看一下Service Mesh 开源项目简介：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linkerd（&lt;a href=&#34;https://github.com/linkerd/linkerd）：第一代&#34;&gt;https://github.com/linkerd/linkerd）：第一代&lt;/a&gt; Service Mesh，2016 年 1 月 15 日首发布，业界第一个 Service Mesh 项目，由 Buoyant 创业小公司开发（前 Twitter 工程师），2017 年 7 月 11 日，宣布和 Istio 集成，成为 Istio 的数据面板。&lt;/li&gt;
&lt;li&gt;Envoy（&lt;a href=&#34;https://github.com/envoyproxy/envoy）：第一代&#34;&gt;https://github.com/envoyproxy/envoy）：第一代&lt;/a&gt; Service Mesh，2016 年 9 月 13 日首发布，由 Matt Klein 个人开发（Lyft 工程师），之后默默发展，版本较稳定。&lt;/li&gt;
&lt;li&gt;Istio（&lt;a href=&#34;https://github.com/istio/istio）：第二代&#34;&gt;https://github.com/istio/istio）：第二代&lt;/a&gt; Service Mesh，2017 年 5 月 24 日首发布，由 Google、IBM 和 Lyft 联合开发，只支持 Kubernetes 平台，2017 年 11 月 30 日发布 0.3 版本，开始支持非 Kubernetes 平台，之后稳定的开发和发布。这个在我们k8s基础平台中&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-istio/&#34;&gt;最常使用&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;Conduit（&lt;a href=&#34;https://github.com/runconduit/conduit）：第二代&#34;&gt;https://github.com/runconduit/conduit）：第二代&lt;/a&gt; Service Mesh，2017 年 12 月 5 日首发布，由 Buoyant 公司开发（借鉴 Istio 整体架构，部分进行了优化），对抗 Istio 压力山大，也期待 Buoyant 公司的毅力。&lt;/li&gt;
&lt;li&gt;nginMesh（&lt;a href=&#34;https://github.com/nginmesh/nginmesh）：2017&#34;&gt;https://github.com/nginmesh/nginmesh）：2017&lt;/a&gt; 年 9 月首发布，由 Nginx 开发，定位是作为 Istio 的服务代理，也就是替代 Envoy，思路跟 Linkerd 之前和 Istio 集成很相似，极度低调，GitHub 上的 star 也只有不到 100。&lt;/li&gt;
&lt;li&gt;Kong（&lt;a href=&#34;https://github.com/Kong/kong）：比&#34;&gt;https://github.com/Kong/kong）：比&lt;/a&gt; nginMesh 更加低调，默默发展中。&lt;/li&gt;
&lt;li&gt;go-micro是基于Go语言实现的插件化RPC微服务框架，与go-kit，kite等微服务框架相比，它具有易上手、部署简单、工具插件化等优点。go-micro框架提供了服务发现、负载均衡、同步传输、异步通信以及事件驱动等机制，它尝试去简化分布式系统间的通信，让我们可以专注于自身业务逻辑的开发。所以对于新手而言，go-micro是个不错的微服务实践的开始。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在云原生模型里，一个应用可以由数百个服务组成，每个服务可能有数千个实例，而每个实例可能会持续地发生变化。这种情况下，服务间通信不仅异常复杂，而且也是运行时行为的基础。管理好服务间通信对于保证端到端的性能和可靠性来说是无疑是非常重要的。种种复杂局面便催生了服务间通信层的出现，这个层既不会与应用程序的代码耦合，又能捕捉到底层环境高度动态的特点，让业务开发者只关注自己的业务代码，并将应用云化后带来的诸多问题以不侵入业务代码的方式提供给开发者。&lt;/p&gt;

&lt;p&gt;这个服务间通信层就是 Service Mesh，它可以提供安全、快速、可靠的服务间通讯（service-to-service）。&lt;/p&gt;

&lt;p&gt;Service Mesh，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给 Service Mesh 就可以了。&lt;/p&gt;

&lt;h1 id=&#34;电商系统的演进&#34;&gt;电商系统的演进&lt;/h1&gt;

&lt;p&gt;我们结合上面的架构演进，通过我们最常见的电商系统来看单体到分布式的变化：&lt;/p&gt;

&lt;h2 id=&#34;单体&#34;&gt;单体&lt;/h2&gt;

&lt;p&gt;当我们的项目比较小时，我们只有一个系统，并且把他们写到一起，放在一个服务器上，但是随着平台越来越大，数据量越来越大，我们不得不通过分库，把多个模块的数据库分别放在对应得服务器上，每个模块调用自己的子系统即可。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/ac1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;集群-1&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;当后台单台没有办法承受的时候，我们需要堆机器来水平扩展集群来处理问题，但是一连串的流程都是在一个实例中进行。&lt;/p&gt;

&lt;h2 id=&#34;esb&#34;&gt;ESB&lt;/h2&gt;

&lt;p&gt;随着我们系统的进一步复杂度的提示，在集群的基础上，我们不得不进一步对系统的性能进行提升，我们将多个模块分成多个子系统，多个子系统直接互相调用（因为SOA一般用于大型项目，比较复杂，所以一般总系统不会再集成，会拆分多个，分别做成服务，相互调用）。当我们的电商UI进行一个下订单的任务时，多个服务直接互相调用，系统通过数据总线(java的ESB，其实也就是我们所说的网关)，分别调用对于的子系统即可。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/ac2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;企业数据总线：企业数据总线不是对多个子模块的集成，他在这里充当数据通道的作用，数据总线不关心业务，数据总线根据给的地址和协议去调服务，上端不关心服务在哪里是什么，只找数据总线。&lt;/p&gt;

&lt;p&gt;数据总线是起到调度服务的作用，数据总线不是集成服务，数据总线更新一个调度框架，每个服务需要根据约定向数据总线注册服务，那么如何注册那？其实数据总线就像一个字典结构，&lt;/p&gt;

&lt;p&gt;数据总线里面一个key对于一个value，key指的是服务名，value则是服务的调度方式，还有一点需要说明的是，数据总线只是指路人，服务是不经过数据总线的，如上图的黄色线的路径。&lt;/p&gt;

&lt;p&gt;数据总线通过域名解析实现:一个域名绑定多台服务器，ajax也可以，dns也可以，解析域名嘛。&lt;/p&gt;

&lt;p&gt;其实数据总线还有一些高级应用，比如心跳检测，实现负载均衡等等，就不细说了，目前应用数据总线的有阿里的dubbo,还有zookeeper。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;这个时候组织架构加上了企业总线&lt;strong&gt;前台&amp;mdash;ESB&amp;mdash;后台&lt;/strong&gt;的结构。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;随着企业发展到一定程度，需要规范化管理的时候，我们上了ERP系统，要通过供应链建设B2C业务的时候，我们上了SRM系统，仓储管理到一定规模我们上WMS，物流管理我们上TMS……整个公司各个系统功能有重叠，有交叉，内部协同成了重大问题。&lt;/p&gt;

&lt;p&gt;这就好像我们一直在打是局部信息化战争，头痛医头脚痛医脚，需要解决什么样的问题，就上什么样的系统，最终就形成了所谓烟囱式的系统架构。&lt;/p&gt;

&lt;p&gt;导致整个架构重复造轮子，跨系统管理也给运营人员造成了不必要的时间精力浪费。整个系统管理冗杂又造成资源浪费，这时候就需要将原有的系统规范化、一体化，通过数据总线进去进行深度的整合，打通各个信息孤岛，形成前后贯通的信息化建设。&lt;/p&gt;

&lt;p&gt;我们把这个时代称之为ESB数据孤岛时代。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/electroniccommerce/esb.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;大部分情况下基本上都是够用，虽然ESB很重，核心很难改变，不够灵活，但是随着发展，出现了更加灵活的方式。&lt;/p&gt;

&lt;h2 id=&#34;大中台&#34;&gt;大中台&lt;/h2&gt;

&lt;p&gt;其实中台严格意义上来说，不是一种架构，也不是一种系统，而是一种战略。即可以使用第一代微服务架构来构建，也可以使用第二代微服务架构来构建，目前各大企业也正在容器化推进的过程中&lt;/p&gt;

&lt;p&gt;中台的核心价值是在于，在对企业业务有了柔性支撑和贯通的前提下，再形成协同与智慧的运营体系。&lt;/p&gt;

&lt;p&gt;一般企业架构分成了三个层次：前台、中台、后台。中台又分成三大块，业务中台、数据中台和技术中台。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/electroniccommerce/middle&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;技术中台支撑企业业务发展，通过打通企业内异构系统，支持业务中台；

业务中台围绕公司业务运营进行服务，将获取的多维度数据传递给数据中台，由数据中台分析反馈给业务中台，以优化业务运营。同时数据中台通过BI智能分析，帮助企业管理者更好的做决策分析。三者是相辅相成，相互协作的。
业务中台其实就是把原有的前端的会员中心、营销中心、商品中心，后端的供应链中心、采配中心等重点模块放在业务中台模块，以后前端不管对接多少个第三方，线上线下增加多少家门店，都能进行统一会员、统一商品编码、统一供应链整合，整个系统一体化。真正做到用技术支持业务，通过业务收集大量数据进行决策，统一高效的进行管理。

数据中台：一般指以数据采集、数据集成、数据治理，指标体系和数据仓库统一建设等数据管理活动为典型特征的中台。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当前最需要建设的中台有两种：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;狭义的业务中台：一般指在线业务为典型特征的中台。在OLDI（Online Data-Intensive）时代，越来越多的企业的核心业务都是在线业务，因此把在线业务中台简称为业务中台。
数据中台：一般指以数据采集、数据集成、数据治理，指标体系和数据仓库统一建设等数据管理活动为典型特征的中台。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对业务中台来说，比较符合的场景主要有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;业务系统研发团队至少大几十人（含外包的），需求多变化快，系统又涉及多个领域（比如做ERP、电商的），业务逻辑比较复杂。
这时业务中台可以把系统和业务领域划分清楚，提高研发效率。做相似行业的外包项目为主，业务规模也做的比较大的（一年有两位数的项目）。
这时业务中台可以提升软件复用，降低定制化成本，提高研发效率。如果每个项目都完全不一样，那中台也救不了你。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;支持业务中台的技术体系，包括微服务、DevOps、云原生和分布式事务等。&lt;/p&gt;

&lt;p&gt;将需求设计成微服务架构，然后每个服务使用各种技术栈来开发业务，比如golang的技术栈的高并发的特性来开发web服务等，然后将一些统一的模块进行统一的接入和输出，使用devops的开发模式，在业务中还是需要解决分布式事务等问题。&lt;/p&gt;

&lt;p&gt;比如在网易，是网易轻舟微服务平台，提供微服务应用全生命周期的完整支持，包括下一代微服务Service Mesh支持、经典微服务框架NSF、包括CI/CD的DevOps、分布式事务框架GXTS、APM、API网关、GoAPI全自动化测试以及容器应用管理服务等。&lt;/p&gt;

&lt;p&gt;对数据中台来说，比较符合的场景：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数据产品比较多，每天要看数据如果没数据就不知道怎么工作的运营人员比较多的业务。
比如电商就是典型。尤其是数据产品和运营人员还在多个团队。
用数据的姿势比较复杂，问题比较多，比如经常出现指标不一致、数据出错、想要的数据不知道哪里有等问题。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;支持数据中台的技术体系，包括指标管理、数据服务、元数据管理、数仓开发与管理、数据安全管理、数据资产管理、大数据计算引擎、数据集成/同步/交换引擎等，&lt;/p&gt;

&lt;p&gt;其实数据中台就是将数据进行处理，不同数据资源，统一的输出标准，中间用到大部分就是数据引擎，比如kafka队列，sprak，flink等流式引擎，hadoop，hbase和hive等大数据引擎。&lt;/p&gt;

&lt;p&gt;比如在网易，是以网易猛犸为核心的网易全链路数据中台解决方案。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 电商购物系统</title>
          <link>https://kingjcy.github.io/post/architecture/shopping/</link>
          <pubDate>Tue, 04 Feb 2020 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/shopping/</guid>
          <description>&lt;p&gt;电商购物系统算是目前软件技术落地的很大的一个发展方向，主要以阿里为主导的电商购物系统占据整个行业的半壁江山，是直接和价值挂钩的重要业务方向。&lt;/p&gt;

&lt;h1 id=&#34;架构演进&#34;&gt;架构演进&lt;/h1&gt;

&lt;p&gt;电商系统也是和其他系统一样一步步演进的，架构演进都是通用的，可以看&lt;a href=&#34;https://kingjcy.github.io/post/architecture/architecture-evolution/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通用架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/ms2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;目前电商的整体架构流程如下，并且在每一层如何实现在高并发，大流量的情况下，最大程度能够保证网站的健康。主要是流量控制，让用户的流量像漏斗模型一样逐层减少，让流量始终处于可控的范围之内。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;客户端可以是各种终端，可以是web，进行各种操作。在web这边，主要是&lt;strong&gt;限流&lt;/strong&gt;，可以使用js技术，比如购买按钮置灰，可以减少大量的重复请求。&lt;/li&gt;
&lt;li&gt;CDN层，CDN存储静态页面缓存技术。在这一层主要是&lt;strong&gt;缓存&lt;/strong&gt;，很多的静态资源存储在cdn中，能够快速对请求进行返回。

&lt;ul&gt;
&lt;li&gt;限制用户维度访问频率：针对同一个用户（ Userid 维度），做页面级别缓存，单元时间内的请求，统一走缓存，返回同一个页面。&lt;/li&gt;
&lt;li&gt;限制商品维度访问频率：大量请求同时间段查询同一个商品时，可以做页面级别缓存，不管下回是谁来访问，只要是这个页面就直接返回。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;接入层，nginx等负载均衡技术。这一层主要是&lt;strong&gt;限流截流&lt;/strong&gt;，比如相同请求，只转发一个，分发到不同的服务实例，限制同一个ip的多次请求。&lt;/li&gt;
&lt;li&gt;然后将请求放到MQ中，排队处理，这已经算是到应用层。这一层主要是&lt;strong&gt;削峰&lt;/strong&gt;，请任务放到队列中，峰值的时候排队处理，不会阻塞&lt;/li&gt;
&lt;li&gt;应用层，处理各种应用。这一层就是&lt;strong&gt;高并发模型和异步处理&lt;/strong&gt;，实例实现并发的多进程，多线程的处理，也就上面购物系统处理的一个流程。

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-context/&#34;&gt;并发控制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;异步处理：定一个channel，以goroutine来运行，用于接受异步返回的结果，是实现高并发的一种方式。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-event/&#34;&gt;分布式事务&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-lock/&#34;&gt;分布式安全&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;最后就是DB层，这一层是瓶颈所在，分层的目的是为了将压力留在上层，一般瓶颈都是在数据库，所以需要把有效得请求到数据库中进行处理，针对数据库的瓶颈，主要就是&lt;strong&gt;索引读写分离，分区分库分表&lt;/strong&gt;等。&lt;/li&gt;
&lt;li&gt;在每一层能有缓存的地方都要用&lt;strong&gt;缓存&lt;/strong&gt;，缓存的效率比数据库要高很多，哪怕是缓存数据库。&lt;/li&gt;
&lt;li&gt;在每一层最好能使用&lt;strong&gt;集群和分布式&lt;/strong&gt;来加强系统的承受能力。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;整体来说，核心思想就是限流、缓存、削峰、高并发、异步处理，集群和分布式，做好这些模块才能保证系统的稳定性和最大的承受能力，在高并发和大流量的情况下。&lt;/p&gt;

&lt;p&gt;当然这些就是属于业务架构了，都是可以使用微服务的架构来实现，每一块都可以做成一个微服务。&lt;/p&gt;

&lt;h2 id=&#34;限流截流算法&#34;&gt;限流截流算法&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;令牌桶算法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;令牌桶算法主要用于限制流量的平均流入速率，允许出现一定的突发流量。比如我们常用的nginx就是一个典型的令牌桶算法的实现：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每秒会有r个令牌被放入桶内，也就是说以1/r的速度向桶中放入令牌&lt;/li&gt;
&lt;li&gt;桶的容量是固定不变的，所以多出来的令牌就会被丢弃&lt;/li&gt;
&lt;li&gt;当一个n字节的请求包到达时，消耗n个令牌，然后才能发送该数据包&lt;/li&gt;
&lt;li&gt;如果桶内令牌小于n，数据包就会被限流，比如缓存或者丢弃&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;nginx限流模块的使用，tengine是在nginx的基础进行二次开发，针对高并发，大流量的场景进行优化，也可以了解一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#统一在http域中进行配置

 #限制请求：表示每个ip每秒的请求数不能超过50个
 limit_req_zone $uri zone=api_read:20m rate=50r/s;

 #按ip配置一个连接 zone
 limit_conn_zone $binary_remote_addr zone=perip_conn:10m;

 #按server配置一个连接 zone
 limit_conn_zone $server_name zone=perserver_conn:100m;

location / {
 if (!-e $request_filename){
  rewrite ^/(.*) /index.php last;
 }

 #请求限流排队通过 burst代表着桶的大小，默认是0，这里代表缓存100个
 limit_req zone=api_read burst=100;

 #连接数限制,每个IP并发请求为50
 limit_conn perip_conn 50;

 #服务所限制的连接数(即限制了该server并发连接数量)
 limit_conn perserver_conn 200;

 #连接限速
 #limit_rate 100k;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;漏斗算法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;可以以任何速度向桶内发送请求&lt;/li&gt;
&lt;li&gt;当然桶的容量还是固定的，如果请求大于桶了，就会被丢弃&lt;/li&gt;
&lt;li&gt;然后按固定过得速度来处理桶内的请求&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;计数器算法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;就是对某个对象在单位时间内允许被操作的次数，一旦超过了所设定的阈值，就会拒绝请求，到时间后重置计数器，重新限制。&lt;/p&gt;

&lt;p&gt;比如某个商品在10s内只能抢购5000次，对抢购进行计数，当达到5000次之后，就会拒绝请求，10s过后，对已经抢购的次数进行重置，重新进行抢购。&lt;/p&gt;

&lt;h2 id=&#34;缓存&#34;&gt;缓存&lt;/h2&gt;

&lt;p&gt;使用缓存将频繁访问的热点数据存储在最近的地方（比如CDN），最快响应的地方（比如本地内存），能够快速响应。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;本地缓存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;优点只有一个，操作本地内存简单，响应快。但是他的缺点很明显，因此适用场景也是很明确的&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据不能共享&lt;/li&gt;
&lt;li&gt;数据会丢失&lt;/li&gt;
&lt;li&gt;内存一般不大，用尽后会OOM&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以分布式缓存是比如发展的趋势，正常我们都是会将本地缓存和分布式缓存结合使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分布式缓存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;分布式缓存也就是我们常用的缓存数据库，比如redis，memcache等，具体就不多说了，可以自己去&lt;a href=&#34;https://kingjcy.github.io/post/database/redis/redis/&#34;&gt;详细了解&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;同一key的读要求&#34;&gt;同一key的读要求&lt;/h3&gt;

&lt;p&gt;1、使用redis进行多读多写&lt;/p&gt;

&lt;p&gt;就是采用冗余存储的方案，将一个key进行计算加工将其存储到redis集群中的各个节点，这个时候进行查询的时候就可以通过处理key来轮询均衡处理，解决热点商品哪怕使用集群也会出现单点的问题。需要注意数据一致性问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/redis.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、本地缓存+redis&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/cache.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;定时重redis缓存中同步数据到本地缓存，时间随着商品的热点来定，在定时期间可能出现数据不一致的情况，需要在最后的数据库扣除的时候做处理，在redis中使用分布式锁，在mysql中就是使用正常的锁。&lt;/p&gt;

&lt;p&gt;还可以通过推送消息的方式来减少不一致的情况，如果redis有变更就推送到队列中，本地消费更新。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/cache2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3、实时热点系统&lt;/p&gt;

&lt;p&gt;我们不可能准确的把所有的热点数据都推送到缓存中，这就需要我们在交易的过程中发现热点数据同步的缓存中，一般通过收集日志进行分析，最后自动推送到redis中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/sd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;同一key的写要求&#34;&gt;同一key的写要求&lt;/h3&gt;

&lt;p&gt;1、在redis中进行扣减库存&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/redis2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;还有进行优化，就是合并操作批量一次处理&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/redis3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、还有用一些优化的数据库，比如ALISQL&lt;/p&gt;

&lt;h2 id=&#34;削峰&#34;&gt;削峰&lt;/h2&gt;

&lt;p&gt;削峰其实就是对峰值流量进行分散处理，避免在同一时间内产生较大的用户流量冲击系统，从而降低系统的负载压力。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分时削峰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将抢购不要设置为一个时间点，比如0点，而是通过多设置几个时间点，来分散用户流量，降低系统压力&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;验证削峰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过验证来拦截缓存用户的请求，比如现在做的验证码新高度的12306的验证，虽然各种各样的验证码，但是确实成功的阻挡了秒杀器和降低了峰值流量&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;队列缓存削峰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将请求放到Q中，按顺序固定速率的处理，实现流量可控，保证系统的稳定，不会奔溃。&lt;/p&gt;

&lt;h2 id=&#34;异步&#34;&gt;异步&lt;/h2&gt;

&lt;p&gt;异步处理并不算一种并发的使用方式，但是却是并发中经常使用的，在工作池的基础上使用goroutine处理，但是不用等返回，留一个channenl返回，使用select读取channel中的数据，完成处理，这样可以加大处理的速度，也就提高了并发能力。&lt;/p&gt;

&lt;h2 id=&#34;数据库优化&#34;&gt;数据库优化&lt;/h2&gt;

&lt;p&gt;对于非结构化或者可以设计为非结构化的数据，我们可以放在redis这种缓存数据库中，但是重要的业务数据最终还是需要关系型数据库的保证，所以对于关系型数据库的优化使用，重来都没有停止过。&lt;/p&gt;

&lt;p&gt;关系型数据库最常见的瓶颈&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大量读写难以承受&lt;/li&gt;
&lt;li&gt;大量查询索引，效率低下&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解决这些问题，最常见的就是读写分离，分区分库分表。&lt;/p&gt;

&lt;h3 id=&#34;读写分离&#34;&gt;读写分离&lt;/h3&gt;

&lt;p&gt;读写分离很简单，一般数据库都是一主一从，主库用于写，从库用于读，可以很大程度上缓存数据库的压力。&lt;/p&gt;

&lt;p&gt;当然这样也有问题，就是主从之间存在着数据延迟，一般都是重设计上解决这个问题，在数据落到主库的时候落一份到缓存，避免从库查不到数据。&lt;/p&gt;

&lt;h3 id=&#34;分区分库分表&#34;&gt;分区分库分表&lt;/h3&gt;

&lt;p&gt;设计表的时候就应该有分区，这样可以加快查询，就像设计好的索引一样。&lt;/p&gt;

&lt;p&gt;首先就是垂直拆分，也就是分库，将不同类型的数据放到不同的库中，比如不停地方的订单放到不同的库中，每个库的压力就不大了。&lt;/p&gt;

&lt;p&gt;然后就是水平拆分，也就是分表，同一个业务表，数据量达到500W，查询效率就很低下了，优化索引什么都没有用，就需要对业务表进行拆分，将将业务按一定的序号拆分为子表，这个时候就需要一些组件完成路由，我们常见的有mysql sharding，mycat等。&lt;/p&gt;

&lt;p&gt;其实在业务量再大的情况下，可能需要同一个业务进行分库分表的操作。&lt;/p&gt;

&lt;h1 id=&#34;基本系统&#34;&gt;基本系统&lt;/h1&gt;

&lt;p&gt;不管是正常的购物还是抢购还是秒杀都是这么一个基本流程,包含这个一下基本的系统&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;搜索系统---订单系统---物流系统
订单系统：购物车系统---确认订单---支付系统，核心就是出发数据库修改库存
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首页，用户，广告，购物，订单，商品，结算&lt;/p&gt;

&lt;h1 id=&#34;基本业务场景&#34;&gt;基本业务场景&lt;/h1&gt;

&lt;h2 id=&#34;秒杀&#34;&gt;秒杀&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/timg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;秒杀系统会做，基本的购物，抢购，抢红包，大促都会做，秒杀的特性：限时限量，导致的场景就是瞬时并发量大。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;热场&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在活动开始之前，最好设计一个“热场”。&lt;/p&gt;

&lt;p&gt;“热场”的形式多种多样，例如：分享活动领优惠券，领秒杀名额等等。“热场”的形式不重要，重要的是通过它获取一些准备信息。&lt;/p&gt;

&lt;p&gt;例如：有可能参与的用户数，他们的地域分布，他们感兴趣的商品。为后面的技术架构提供数据支持，可以先对相关数据进行缓存，减少缓存的压力。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;千万级秒杀系统架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们再具体看一下包含秒杀的逻辑具体架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/timg2&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/architecture/shop/ms3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;秒杀系统设计的核心思想：尽量将请求拦截在上层（限流，削峰），高并发（异步处理），充分使用内存缓存。&lt;/p&gt;

&lt;p&gt;整体流程中优化的点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（限流）            1、web层使用js技术，比如购买按钮置灰，可以减少大量的重复请求
（限流截流）        2、负载均衡nginx（也就是网关），进行截流，比如相同请求，相同的IP，只转发一个，分发到不同的服务实例
（缓存）            3、cdn缓存技术，将很多的静态资源存储在cdn中，能够快速返回
                        1、限制用户维度访问频率
                            针对同一个用户（ Userid 维度），做页面级别缓存，单元时间内的请求，统一走缓存，返回同一个页面。
                        2、限制商品维度访问频率
                            大量请求同时间段查询同一个商品时，可以做页面级别缓存，不管下回是谁来访问，只要是这个页面就直接返回。
（削峰）            4、然后将请求放到MQ中，排队处理，
（高并发，异步处理）  5、到后台实例实现并发的多进程，多线程的处理，也就上面购物系统处理的一个流程
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有一下业务上的优化，其实就是使用了均衡和削峰填谷的思想，比如12306所做的，分时分段售票，原来统一10点卖票，现在8点，8点半，9点，&amp;hellip;每隔半个小时放出一批：将流量摊匀。&lt;/p&gt;

&lt;p&gt;1、&lt;a href=&#34;https://kingjcy.github.io/posts/golang/go-context/&#34;&gt;并发控制&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2、&lt;a href=&#34;https://kingjcy.github.io/posts/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、异步处理：定一个channel，以goroutine来运行，用于接受异步返回的结果&lt;/p&gt;

&lt;p&gt;4、&lt;a href=&#34;https://kingjcy.github.io/posts/distributed/distributed-event/&#34;&gt;分布式事务&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;5、&lt;a href=&#34;https://kingjcy.github.io/posts/distributed/distributed-lock/&#34;&gt;分布式安全&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（高并发）           6、最后到数据库包括缓存数据库的读写分离，分区分库分表。
（缓存）             7、缓存是很快的用在每一层能够使用的地方
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分层的目的是为了将压力留在上层，一般瓶颈都是在数据库，所以需要把有效得请求到数据库中进行处理&lt;/p&gt;

&lt;p&gt;秒杀系统设计的核心思想：限流，削峰，高并发，异步处理，内存缓存&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- loki</title>
          <link>https://kingjcy.github.io/post/monitor/log/loki/loki/</link>
          <pubDate>Sat, 18 Jan 2020 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/loki/loki/</guid>
          <description>&lt;p&gt;Loki是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签，为 Prometheus和 Kubernetes用户做了相关优化。项目受 Prometheus 启发，类似于 Prometheus 的日志系统。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;使用场景&#34;&gt;使用场景&lt;/h2&gt;

&lt;p&gt;当我们的容器云运行的应用或者某个节点出现问题了，解决思路应该如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一般在容器云中使用prometheus生态来做监控告警，在metrics触发告警的时候，我们就需要查看日志来处理问题，这个时候就需要日志系统来收集日志进行搜索查看。&lt;/p&gt;

&lt;p&gt;现有的很多日志采集的方案都是采用全文检索对日志进行索引（如ELK方案），优点是功能丰富，允许复杂的操作。但是，这些方案往往规模复杂，资源占用高，操作苦难。很多功能往往用不上，大多数查询只关注一定时间范围和一些简单的参数（如host、service等），这个时候就需要一个轻量级的日志系统，这个时候loki就比较合适了。&lt;/p&gt;

&lt;h2 id=&#34;基本组件&#34;&gt;基本组件&lt;/h2&gt;

&lt;p&gt;Loki 整个系统需要三个组件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、Loki: 相当于 EFK 中的 ElasticSearch，用于存储和查询日志
2、Promtail: 相当于 EFK 中的 Filebeat/Fluentd，用于采集和发送日志
3、Grafana: 相当于 EFK 中的 Kibana，用于 UI 展示
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些组件以以下的部署在我们的系统中&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们可以看出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、loki: 以 Statefulset 方式部署，可横向扩容
2、promtail: 以 Daemonset 方式部署，采集每个节点上容器日志并发送给 loki
3、grafana: 默认不开启，如果集群中已经有 grafana 就可以不用在部署 grafana，如果没有，部署时可以选择也同时部署 grafana
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不使用容器部署，也大体可以看出对应的部署方式，就是Promtail作为采集组件需要部署在每个一个机器上然后将数据推送到loki中，grafana在loki中拉去数据进行展示。&lt;/p&gt;

&lt;h1 id=&#34;部署使用&#34;&gt;部署使用&lt;/h1&gt;

&lt;h2 id=&#34;k8s部署&#34;&gt;k8s部署&lt;/h2&gt;

&lt;p&gt;新增helm源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm repo add loki https://grafana.github.io/loki/charts
$ helm repo update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用helm3部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install loki loki/loki-stack
# 安装到指定命名空间
# helm install loki loki/loki-stack -n monitoring
# 持久化 loki 的数据，避免 loki 重启后数据丢失
# helm install loki loki/loki-stack --set=&amp;quot;loki.persistence.enabled=ture,loki.persistence.size=100G&amp;quot;
# 部署 grafana
# helm install loki loki/loki-stack --set=&amp;quot;grafana.enabled=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们就可以看到对应启动了如下应用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get all -n monitoring | grep loki
pod/loki-0                                 1/1     Running   1          20h
pod/loki-promtail-8phlp                    1/1     Running   1          20h
service/loki                    NodePort    10.111.208.19    &amp;lt;none&amp;gt;        3100:31278/TCP               20h
service/loki-headless           ClusterIP   None             &amp;lt;none&amp;gt;        3100/TCP                     20h
daemonset.apps/loki-promtail   1         1         1       1            1           &amp;lt;none&amp;gt;                   20h
statefulset.apps/loki                1/1     20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如上使用了以 Daemonset 方式部署了promtail，使用Statefulset 方式部署loki，然后用service暴露给grafana。&lt;/p&gt;

&lt;h2 id=&#34;配置文件&#34;&gt;配置文件&lt;/h2&gt;

&lt;p&gt;上面启动了对应的应用，我们来看一下默认的启动情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti loki-0 -n monitoring -- sh
/ $ ps -ef
PID   USER     TIME  COMMAND
    1 loki      0:01 /usr/bin/loki -config.file=/etc/loki/loki.yaml
   23 loki      0:00 sh
   28 loki      0:00 ps -ef
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到就是使用二进制文件和配置文件进行启动，所以我们关键看一下配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/ $ cat /etc/loki/loki.yaml
auth_enabled: false
chunk_store_config:
  max_look_back_period: 0s
ingester:
  chunk_block_size: 262144
  chunk_idle_period: 3m
  chunk_retain_period: 1m
  lifecycler:
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  max_transfer_retries: 0
limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
schema_config:
  configs:
  - from: &amp;quot;2018-04-15&amp;quot;
    index:
      period: 168h
      prefix: index_
    object_store: filesystem
    schema: v9
    store: boltdb
server:
  http_listen_port: 3100
storage_config:
  boltdb:
    directory: /data/loki/index
  filesystem:
    directory: /data/loki/chunks
table_manager:
  retention_deletes_enabled: false
  retention_period: 0s/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们这边详细说明一下配置文件，配置文件主要有以下几块组成&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、target：[target: &amp;lt;string&amp;gt; | default = &amp;quot;all&amp;quot;]
2、auth_enabled：[auth_enabled: &amp;lt;boolean&amp;gt; | default = true] 启动验证，默认是启动的，如果需要关闭，需要设置为false
3、server：主要是配置loki的http模块，最常见的就是配置http的地址和端口
    # HTTP server listen host
    [http_listen_address: &amp;lt;string&amp;gt;]

    # HTTP server listen port
    [http_listen_port: &amp;lt;int&amp;gt; | default = 80]

    # gRPC server listen host
    [grpc_listen_address: &amp;lt;string&amp;gt;]

    # gRPC server listen port
    [grpc_listen_port: &amp;lt;int&amp;gt; | default = 9095]

    # Register instrumentation handlers (/metrics, etc.)
    [register_instrumentation: &amp;lt;boolean&amp;gt; | default = true]

    # Timeout for graceful shutdowns
    [graceful_shutdown_timeout: &amp;lt;duration&amp;gt; | default = 30s]

    # Read timeout for HTTP server
    [http_server_read_timeout: &amp;lt;duration&amp;gt; | default = 30s]

    # Write timeout for HTTP server
    [http_server_write_timeout: &amp;lt;duration&amp;gt; | default = 30s]

    # Idle timeout for HTTP server
    [http_server_idle_timeout: &amp;lt;duration&amp;gt; | default = 120s]

    # Max gRPC message size that can be received
    [grpc_server_max_recv_msg_size: &amp;lt;int&amp;gt; | default = 4194304]

    # Max gRPC message size that can be sent
    [grpc_server_max_send_msg_size: &amp;lt;int&amp;gt; | default = 4194304]

    # Limit on the number of concurrent streams for gRPC calls (0 = unlimited)
    [grpc_server_max_concurrent_streams: &amp;lt;int&amp;gt; | default = 100]

    # Log only messages with the given severity or above. Supported values [debug,
    # info, warn, error]
    [log_level: &amp;lt;string&amp;gt; | default = &amp;quot;info&amp;quot;]

    # Base path to server all API routes from (e.g., /v1/).
    [http_path_prefix: &amp;lt;string&amp;gt;]
4、distributor主要是配置loki的分发，目前只有ring轮询
    [ring: &amp;lt;ring_config&amp;gt;]
5、ring_config主要是用来发现和连接Ingesters
    kvstore:
      # The backend storage to use for the ring. Supported values are
      # consul, etcd, inmemory
      store: &amp;lt;string&amp;gt;

      # The prefix for the keys in the store. Should end with a /.
      [prefix: &amp;lt;string&amp;gt; | default = &amp;quot;collectors/&amp;quot;]

      # Configuration for a Consul client. Only applies if store
      # is &amp;quot;consul&amp;quot;
      consul:
        # The hostname and port of Consul.
        [host: &amp;lt;string&amp;gt; | duration = &amp;quot;localhost:8500&amp;quot;]

        # The ACL Token used to interact with Consul.
        [acl_token: &amp;lt;string&amp;gt;]

        # The HTTP timeout when communicating with Consul
        [http_client_timeout: &amp;lt;duration&amp;gt; | default = 20s]

        # Whether or not consistent reads to Consul are enabled.
        [consistent_reads: &amp;lt;boolean&amp;gt; | default = true]

      # Configuration for an ETCD v3 client. Only applies if
      # store is &amp;quot;etcd&amp;quot;
      etcd:
        # The ETCD endpoints to connect to.
        endpoints:
          - &amp;lt;string&amp;gt;

        # The Dial timeout for the ETCD connection.
        [dial_timeout: &amp;lt;duration&amp;gt; | default = 10s]

        # The maximum number of retries to do for failed ops to ETCD.
        [max_retries: &amp;lt;int&amp;gt; | default = 10]

    # The heartbeat timeout after which ingesters are skipped for
    # reading and writing.
    [heartbeat_timeout: &amp;lt;duration&amp;gt; | default = 1m]

    # The number of ingesters to write to and read from. Must be at least
    # 1.
    [replication_factor: &amp;lt;int&amp;gt; | default = 3]
6、querier主要是查询配置
    # Timeout when querying ingesters or storage during the execution of a
    # query request.
    [query_timeout: &amp;lt;duration&amp;gt; | default = 1m]

    # Limit of the duration for which live tailing requests should be
    # served.
    [tail_max_duration: &amp;lt;duration&amp;gt; | default = 1h]

    # Time to wait before sending more than the minimum successful query
    # requests.
    [extra_query_delay: &amp;lt;duration&amp;gt; | default = 0s]

    # Maximum lookback beyond which queries are not sent to ingester.
    # 0 means all queries are sent to ingester.
    [query_ingesters_within: &amp;lt;duration&amp;gt; | default = 0s]

    # Configuration options for the LogQL engine.
    engine:
      # Timeout for query execution
      [timeout: &amp;lt;duration&amp;gt; | default = 3m]

      # The maximum amount of time to look back for log lines. Only
      # applicable for instant log queries.
      [max_look_back_period: &amp;lt;duration&amp;gt; | default = 30s]
7、ingester_client配置ingester的客户端，其实就是distributor连接ingester的配置
    # Configures how connections are pooled
    pool_config:
      # Whether or not to do health checks.
      [health_check_ingesters: &amp;lt;boolean&amp;gt; | default = false]

      # How frequently to clean up clients for servers that have gone away after
      # a health check.
      [client_cleanup_period: &amp;lt;duration&amp;gt; | default = 15s]

      # How quickly a dead client will be removed after it has been detected
      # to disappear. Set this to a value to allow time for a secondary
      # health check to recover the missing client.
      [remotetimeout: &amp;lt;duration&amp;gt;]

    # The remote request timeout on the client side.
    [remote_timeout: &amp;lt;duration&amp;gt; | default = 5s]

    # Configures how the gRPC connection to ingesters work as a
    # client.
    [grpc_client_config: &amp;lt;grpc_client_config&amp;gt;]
8、grpc_client_config上面的client可以使用grpc，这个时候就要对grpc进行配置
    # The maximum size in bytes the client can receive
    [max_recv_msg_size: &amp;lt;int&amp;gt; | default = 104857600]

    # The maximum size in bytes the client can send
    [max_send_msg_size: &amp;lt;int&amp;gt; | default = 16777216]

    # Whether or not messages should be compressed
    [use_gzip_compression: &amp;lt;bool&amp;gt; | default = false]

    # Rate limit for gRPC client. 0 is disabled
    [rate_limit: &amp;lt;float&amp;gt; | default = 0]

    # Rate limit burst for gRPC client.
    [rate_limit_burst: &amp;lt;int&amp;gt; | default = 0]

    # Enable backoff and retry when a rate limit is hit.
    [backoff_on_ratelimits: &amp;lt;bool&amp;gt; | default = false]

    # Configures backoff when enabled.
    backoff_config:
      # Minimum delay when backing off.
      [min_period: &amp;lt;duration&amp;gt; | default = 100ms]

      # The maximum delay when backing off.
      [max_period: &amp;lt;duration&amp;gt; | default = 10s]

      # Number of times to backoff and retry before failing.
      [max_retries: &amp;lt;int&amp;gt; | default = 10]
9、ingester_config配置Ingesters，主要是配置Ingesters的范围
    # Configures how the lifecycle of the ingester will operate
    # and where it will register for discovery.
    [lifecycler: &amp;lt;lifecycler_config&amp;gt;]

    # Number of times to try and transfer chunks when leaving before
    # falling back to flushing to the store. Zero = no transfers are done.
    [max_transfer_retries: &amp;lt;int&amp;gt; | default = 10]

    # How many flushes can happen concurrently from each stream.
    [concurrent_flushes: &amp;lt;int&amp;gt; | default = 16]

    # How often should the ingester see if there are any blocks
    # to flush
    [flush_check_period: &amp;lt;duration&amp;gt; | default = 30s]

    # The timeout before a flush is cancelled
    [flush_op_timeout: &amp;lt;duration&amp;gt; | default = 10s]

    # How long chunks should be retained in-memory after they&#39;ve
    # been flushed.
    [chunk_retain_period: &amp;lt;duration&amp;gt; | default = 15m]

    # How long chunks should sit in-memory with no updates before
    # being flushed if they don&#39;t hit the max block size. This means
    # that half-empty chunks will still be flushed after a certain
    # period as long as they receive no further activity.
    [chunk_idle_period: &amp;lt;duration&amp;gt; | default = 30m]

    # The targeted _uncompressed_ size in bytes of a chunk block
    # When this threshold is exceeded the head block will be cut and compressed inside the chunk
    [chunk_block_size: &amp;lt;int&amp;gt; | default = 262144]

    # A target _compressed_ size in bytes for chunks.
    # This is a desired size not an exact size, chunks may be slightly bigger
    # or significantly smaller if they get flushed for other reasons (e.g. chunk_idle_period)
    # The default value of 0 for this will create chunks with a fixed 10 blocks,
    # A non zero value will create chunks with a variable number of blocks to meet the target size.
    [chunk_target_size: &amp;lt;int&amp;gt; | default = 0]

    # The compression algorithm to use for chunks. (supported: gzip, lz4, snappy)
    # You should choose your algorithm depending on your need:
    # - `gzip` highest compression ratio but also slowest decompression speed. (144 kB per chunk)
    # - `lz4` fastest compression speed (188 kB per chunk)
    # - `snappy` fast and popular compression algorithm (272 kB per chunk)
    [chunk_encoding: &amp;lt;string&amp;gt; | default = gzip]

    # Parameters used to synchronize ingesters to cut chunks at the same moment.
    # Sync period is used to roll over incoming entry to a new chunk. If chunk&#39;s utilization
    # isn&#39;t high enough (eg. less than 50% when sync_min_utilization is set to 0.5), then
    # this chunk rollover doesn&#39;t happen.
    [sync_period: &amp;lt;duration&amp;gt; | default = 0]
    [sync_min_utilization: &amp;lt;float&amp;gt; | Default = 0]

    # The maximum number of errors a stream will report to the user
    # when a push fails. 0 to make unlimited.
    [max_returned_stream_errors: &amp;lt;int&amp;gt; | default = 10]

    # The maximum duration of a timeseries chunk in memory. If a timeseries runs for longer than this the current chunk will be flushed to the store and a new chunk created.
    [max_chunk_age: &amp;lt;duration&amp;gt; | default = 1h]

    # How far in the past an ingester is allowed to query the store for data.
    # This is only useful for running multiple loki binaries with a shared ring with a `filesystem` store which is NOT shared between the binaries
    # When using any &amp;quot;shared&amp;quot; object store like S3 or GCS this value must always be left as 0
    # It is an error to configure this to a non-zero value when using any object store other than `filesystem`
    # Use a value of -1 to allow the ingester to query the store infinitely far back in time.
    [query_store_max_look_back_period: &amp;lt;duration&amp;gt; | default = 0]
10、lifecycler_config主要就是控制
    # Configures the ring the lifecycler connects to
    [ring: &amp;lt;ring_config&amp;gt;]

    # The number of tokens the lifecycler will generate and put into the ring if
    # it joined without transferring tokens from another lifecycler.
    [num_tokens: &amp;lt;int&amp;gt; | default = 128]

    # Period at which to heartbeat to the underlying ring.
    [heartbeat_period: &amp;lt;duration&amp;gt; | default = 5s]

    # How long to wait to claim tokens and chunks from another member when
    # that member is leaving. Will join automatically after the duration expires.
    [join_after: &amp;lt;duration&amp;gt; | default = 0s]

    # Minimum duration to wait before becoming ready. This is to work around race
    # conditions with ingesters exiting and updating the ring.
    [min_ready_duration: &amp;lt;duration&amp;gt; | default = 1m]

    # Name of network interfaces to read addresses from.
    interface_names:
      - [&amp;lt;string&amp;gt; ... | default = [&amp;quot;eth0&amp;quot;, &amp;quot;en0&amp;quot;]]

    # Duration to sleep before exiting to ensure metrics are scraped.
    [final_sleep: &amp;lt;duration&amp;gt; | default = 30s]
11、storage_config主要是存储的配置，可以是本地file，可以是s3等远程存储。这边有很多配置就不一一看了。
12、cache_config就是将数据放到缓存中，比如memche，redis等
13、chunk_store_config是对chunk存储的设置包括多长时间进行存储等
    # The cache configuration for storing chunks
    [chunk_cache_config: &amp;lt;cache_config&amp;gt;]

    # The cache configuration for deduplicating writes
    [write_dedupe_cache_config: &amp;lt;cache_config&amp;gt;]

    # The minimum time between a chunk update and being saved
    # to the store.
    [min_chunk_age: &amp;lt;duration&amp;gt;]

    # Cache index entries older than this period. Default is
    # disabled.
    [cache_lookups_older_than: &amp;lt;duration&amp;gt;]

    # Limit how long back data can be queried. Default is disabled.
    # This should always be set to a value less than or equal to
    # what is set in `table_manager.retention_period`.
    [max_look_back_period: &amp;lt;duration&amp;gt;]
14、schema_config主要是对时间进行设置，格式是period_config
    # The configuration for chunk index schemas.
    configs:
      - [&amp;lt;period_config&amp;gt;]
    # The date of the first day that index buckets should be created. Use
    # a date in the past if this is your only period_config, otherwise
    # use a date when you want the schema to switch over.
    [from: &amp;lt;daytime&amp;gt;]

    # store and object_store below affect which &amp;lt;storage_config&amp;gt; key is
    # used.

    # Which store to use for the index. Either aws, gcp, bigtable, bigtable-hashed,
    # cassandra, or boltdb.
    store: &amp;lt;string&amp;gt;

    # Which store to use for the chunks. Either aws, aws-dynamo, azure, gcp,
    # bigtable, gcs, cassandra, swift or filesystem. If omitted, defaults to the same
    # value as store.
    [object_store: &amp;lt;string&amp;gt;]

    # The schema version to use, current recommended schema is v11.
    schema: &amp;lt;string&amp;gt;

    # Configures how the index is updated and stored.
    index:
      # Table prefix for all period tables.
      prefix: &amp;lt;string&amp;gt;
      # Table period.
      [period: &amp;lt;duration&amp;gt; | default = 168h]
      # A map to be added to all managed tables.
      tags:
        [&amp;lt;string&amp;gt;: &amp;lt;string&amp;gt; ...]

    # Configured how the chunks are updated and stored.
    chunks:
      # Table prefix for all period tables.
      prefix: &amp;lt;string&amp;gt;
      # Table period.
      [period: &amp;lt;duration&amp;gt; | default = 168h]
      # A map to be added to all managed tables.
      tags:
        [&amp;lt;string&amp;gt;: &amp;lt;string&amp;gt; ...]

    # How many shards will be created. Only used if schema is v10 or greater.
    [row_shards: &amp;lt;int&amp;gt; | default = 16]
15、limits_config
    # Whether the ingestion rate limit should be applied individually to each
    # distributor instance (local), or evenly shared across the cluster (global).
    # The ingestion rate strategy cannot be overridden on a per-tenant basis.
    #
    # - local: enforces the limit on a per distributor basis. The actual effective
    #   rate limit will be N times higher, where N is the number of distributor
    #   replicas.
    # - global: enforces the limit globally, configuring a per-distributor local
    #   rate limiter as &amp;quot;ingestion_rate / N&amp;quot;, where N is the number of distributor
    #   replicas (it&#39;s automatically adjusted if the number of replicas change).
    #   The global strategy requires the distributors to form their own ring, which
    #   is used to keep track of the current number of healthy distributor replicas.
    [ingestion_rate_strategy: &amp;lt;string&amp;gt; | default = &amp;quot;local&amp;quot;]

    # Per-user ingestion rate limit in sample size per second. Units in MB.
    [ingestion_rate_mb: &amp;lt;float&amp;gt; | default = 4]

    # Per-user allowed ingestion burst size (in sample size). Units in MB.
    # The burst size refers to the per-distributor local rate limiter even in the
    # case of the &amp;quot;global&amp;quot; strategy, and should be set at least to the maximum logs
    # size expected in a single push request.
    [ingestion_burst_size_mb: &amp;lt;int&amp;gt; | default = 6]

    # Maximum length of a label name.
    [max_label_name_length: &amp;lt;int&amp;gt; | default = 1024]

    # Maximum length of a label value.
    [max_label_value_length: &amp;lt;int&amp;gt; | default = 2048]

    # Maximum number of label names per series.
    [max_label_names_per_series: &amp;lt;int&amp;gt; | default = 30]

    # Whether or not old samples will be rejected.
    [reject_old_samples: &amp;lt;bool&amp;gt; | default = false]

    # Maximum accepted sample age before rejecting.
    [reject_old_samples_max_age: &amp;lt;duration&amp;gt; | default = 336h]

    # Duration for a table to be created/deleted before/after it&#39;s
    # needed. Samples won&#39;t be accepted before this time.
    [creation_grace_period: &amp;lt;duration&amp;gt; | default = 10m]

    # Enforce every sample has a metric name.
    [enforce_metric_name: &amp;lt;boolean&amp;gt; | default = true]

    # Maximum number of active streams per user, per ingester. 0 to disable.
    [max_streams_per_user: &amp;lt;int&amp;gt; | default = 10000]

    # Maximum line size on ingestion path. Example: 256kb.
    # There is no limit when unset.
    [max_line_size: &amp;lt;string&amp;gt; | default = none ]

    # Maximum number of log entries that will be returned for a query. 0 to disable.
    [max_entries_limit: &amp;lt;int&amp;gt; | default = 5000 ]

    # Maximum number of active streams per user, across the cluster. 0 to disable.
    # When the global limit is enabled, each ingester is configured with a dynamic
    # local limit based on the replication factor and the current number of healthy
    # ingesters, and is kept updated whenever the number of ingesters change.
    [max_global_streams_per_user: &amp;lt;int&amp;gt; | default = 0]

    # Maximum number of chunks that can be fetched by a single query.
    [max_chunks_per_query: &amp;lt;int&amp;gt; | default = 2000000]

    # The limit to length of chunk store queries. 0 to disable.
    [max_query_length: &amp;lt;duration&amp;gt; | default = 0]

    # Maximum number of queries that will be scheduled in parallel by the
    # frontend.
    [max_query_parallelism: &amp;lt;int&amp;gt; | default = 14]

    # Cardinality limit for index queries
    [cardinality_limit: &amp;lt;int&amp;gt; | default = 100000]

    # Maximum number of stream matchers per query.
    [max_streams_matchers_per_query: &amp;lt;int&amp;gt; | default = 1000]

    # Feature renamed to &#39;runtime configuration&#39;, flag deprecated in favor of -runtime-config.file (runtime_config.file in YAML)
    [per_tenant_override_config: &amp;lt;string&amp;gt;]

    # Feature renamed to &#39;runtime configuration&#39;, flag deprecated in favor of -runtime-config.reload-period (runtime_config.period in YAML)
    [per_tenant_override_period: &amp;lt;duration&amp;gt; | default = 10s]
16、frontend_worker_config
    # Address of query frontend service, in host:port format.
    # CLI flag: -querier.frontend-address
    [frontend_address: &amp;lt;string&amp;gt; | default = &amp;quot;&amp;quot;]

    # Number of simultaneous queries to process.
    # CLI flag: -querier.worker-parallelism
    [parallelism: &amp;lt;int&amp;gt; | default = 10]

    # How often to query DNS.
    # CLI flag: -querier.dns-lookup-period
    [dns_lookup_duration: &amp;lt;duration&amp;gt; | default = 10s]

    grpc_client_config:
      # gRPC client max receive message size (bytes).
      # CLI flag: -querier.frontend-client.grpc-max-recv-msg-size
      [max_recv_msg_size: &amp;lt;int&amp;gt; | default = 104857600]

      # gRPC client max send message size (bytes).
      # CLI flag: -querier.frontend-client.grpc-max-send-msg-size
      [max_send_msg_size: &amp;lt;int&amp;gt; | default = 16777216]

      # Use compression when sending messages.
      # CLI flag: -querier.frontend-client.grpc-use-gzip-compression
      [use_gzip_compression: &amp;lt;boolean&amp;gt; | default = false]

      # Rate limit for gRPC client; 0 means disabled.
      # CLI flag: -querier.frontend-client.grpc-client-rate-limit
      [rate_limit: &amp;lt;float&amp;gt; | default = 0]

      # Rate limit burst for gRPC client.
      # CLI flag: -querier.frontend-client.grpc-client-rate-limit-burst
      [rate_limit_burst: &amp;lt;int&amp;gt; | default = 0]

      # Enable backoff and retry when we hit ratelimits.
      # CLI flag: -querier.frontend-client.backoff-on-ratelimits
      [backoff_on_ratelimits: &amp;lt;boolean&amp;gt; | default = false]

      backoff_config:
        # Minimum delay when backing off.
        # CLI flag: -querier.frontend-client.backoff-min-period
        [min_period: &amp;lt;duration&amp;gt; | default = 100ms]

        # Maximum delay when backing off.
        # CLI flag: -querier.frontend-client.backoff-max-period
        [max_period: &amp;lt;duration&amp;gt; | default = 10s]

        # Number of times to backoff and retry before failing.
        # CLI flag: -querier.frontend-client.backoff-retries
        [max_retries: &amp;lt;int&amp;gt; | default = 10]
17、table_manager_config，provision_config都是用于DynamoDB。
18、auto_scaling_config用于DynamoDB的自动伸缩
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体可以参考&lt;a href=&#34;https://github.com/grafana/loki/tree/v1.5.0/docs/configuration&#34;&gt;官网&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;loki的配置还是比较复杂的，下面我们再来看一下promtail的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat promtail.yaml
client:
  backoff_config:
    max_period: 5s
    max_retries: 20
    min_period: 100ms
  batchsize: 102400
  batchwait: 1s
  external_labels: {}
  timeout: 10s
positions:
  filename: /run/promtail/positions.yaml
server:
  http_listen_port: 3101
target_config:
  sync_period: 10s
scrape_configs:
- job_name: kubernetes-pods-name
  pipeline_stages:
    - docker: {}
  kubernetes_sd_configs:
  - role: pod
  relabel_configs:
  - source_labels:
    - __meta_kubernetes_pod_label_name
    target_label: __service__
  - source_labels:
    - __meta_kubernetes_pod_node_name
    target_label: __host__
  - action: drop
    regex: &#39;&#39;
    source_labels:
    - __service__
  - action: labelmap
    regex: __meta_kubernetes_pod_label_(.+)
  - action: replace
    replacement: $1
    separator: /
    source_labels:
    - __meta_kubernetes_namespace
    - __service__
    target_label: job
  - action: replace
    source_labels:
    - __meta_kubernetes_namespace
    target_label: namespace
  - action: replace
    source_labels:
    - __meta_kubernetes_pod_name
    target_label: pod
  - action: replace
    source_labels:
    - __meta_kubernetes_pod_container_name
    target_label: container
  - replacement: /var/log/pods/*$1/*.log
    separator: /
    source_labels:
    - __meta_kubernetes_pod_uid
    - __meta_kubernetes_pod_container_name
    target_label: __path__
- job_name: kubernetes-pods-app
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;promtail的配置和prometheus很像，我们也简单说明一下，promtail的复杂配置分为四个部分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server_config 配置promtail作为一个服务器。开启一个http端口
client_config 配置promtail怎么连接loki，它作为loki的客户端
position_config 指明promtail的配置文件在什么地方生成，重启的时候会读取一些信息
scrape_config 配置一些常用的抓取策略
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们主要配置的地方，就是scrape_config 。它又分为几种常见的抓取方式，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journal_config
syslog_config
relabel_config
static_config
file_sd_config
kubernetes_sd_config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于我们来说，最常使用的就是static_config，比如指定业务的某个日志文件。这部分的描述很长，具体可以参考github文档。&lt;/p&gt;

&lt;p&gt;一个配置文件中，是可以针对不同类型的日志文件同时进行监控的。比如下面的长长的配置文件，就加入了三个抓取策略。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://localhost:3100/loki/api/v1/push

scrape_configs:
  - job_name: journal
    journal:
      max_age: 12h
      labels:
        job: systemd-journal
    relabel_configs:
      - source_labels: [&#39;__journal__systemd_unit&#39;]
        target_label: &#39;unit&#39;
  - job_name: system
    pipeline_stages:
    static_configs:
    - labels:
       job: varlogs
       host: yourhost
       __path__: /var/log/*.log
  - job_name: biz001
    pipeline_stages:
    - match:
       selector: &#39;{app=&amp;quot;test&amp;quot;}&#39;
       stages:
       - regex:
          expression: &#39;.*level=(?P&amp;lt;level&amp;gt;[a-zA-Z]+).*ts=(?P&amp;lt;timestamp&amp;gt;[T\d-:.Z]*).*component=(?P&amp;lt;component&amp;gt;[a-zA-Z]+)&#39;
       - labels:
          level:
          component:
          ts:
          timestrap:
    static_configs:
    - labels:
       job: biz001
       app: test
       node: 001
       host: localhost
       __path__: /alertmgr/dingtalk/nohup.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们配置了三个job（概念见普罗米修斯），journal，system和biz001。尤其注意biz001的配置，这代表了我们对一些日志的通用配置方式。&lt;/p&gt;

&lt;p&gt;首先，看一下biz001的日志格式。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level=info ts=2020-04-30T01:20:38.631Z caller=entry.go:22 component=web http_scheme=http http_proto=HTTP/1.1 http_method=POST remote_addr=[::1]:57710 user_agent=Alertmanager/0.20.0 uri=http://localhost:8060/dingtalk/webhook1/send resp_status=200 resp_bytes_length=2 resp_elapsed_ms=5207.398549 msg=&amp;quot;request complete&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在将日志传送到Loki之前，promtail可以对其进行一系列的操作。比如过滤一些日志，提取一些label，替换一些日志的内容等。&lt;/p&gt;

&lt;p&gt;对于这部分的操作，现有的日志收集工具都搞了一套自己的，而且都很难用。&lt;/p&gt;

&lt;p&gt;比如我们用来解析我们固定格式的nginx日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ps -ef | grep promtail
root     14449 14356  0 21:06 pts/0    00:00:00 grep promtail
root     28509     1  0 Jul21 ?        00:23:12 /opt/promes/loki/promtail-linux-amd64 --config.file=/opt/promes/loki/nginx.yaml
[root@promessitweb19 ~]# cat /opt/promes/loki/nginx.yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /opt/promes/loki/positions.yaml

clients:
  - url: http://10.243.51.50:3100/loki/api/v1/push

scrape_configs:
- job_name: nginx
  static_configs:
  - targets:
      - localhost
    labels:
      job: nginxAccess
      __path__: /opt/rsync_log/access_http.log
      ip: &amp;quot;10.243.58.14&amp;quot;
      appId: PROMES
      softType: blackbox
  pipeline_stages:
  - match:
      selector: &#39;{app=&amp;quot;nginx&amp;quot;}&#39;
      stages:
      - regex:
          expression: &#39;^(?P&amp;lt;remote_addr&amp;gt;\\S+)   (?P&amp;lt;http_x_forwarded_for&amp;gt;\\S+)  (?P&amp;lt;http_x_forwarded_for2&amp;gt;\\S+) (?P&amp;lt;http_x_forwarded_for3&amp;gt;\\S+) (?P&amp;lt;time_iso8601&amp;gt;\\S+)  (?P&amp;lt;request_method&amp;gt;\\S+)    &amp;quot;(?P&amp;lt;document_uri&amp;gt;\\S+)&amp;quot;    &amp;quot;(?P&amp;lt;query_string&amp;gt;\\S+)&amp;quot;    (?P&amp;lt;request_http_protocol&amp;gt;\\S+) (?P&amp;lt;status&amp;gt;\\d{3}|-)    (?P&amp;lt;body_bytes_sent&amp;gt;\\d{3}|-)   (?P&amp;lt;request_time&amp;gt;\\S+)  &amp;quot;(?P&amp;lt;http_referer&amp;gt;\\S+)&amp;quot;    &amp;quot;(?P&amp;lt;user_agent&amp;gt;\\S+)&amp;quot;  traceId:(?P&amp;lt;traceId&amp;gt;\\S+),spanId:(?P&amp;lt;spanId&amp;gt;\\S+)   (?P&amp;lt;server_addr&amp;gt;\\S+)   (?P&amp;lt;hostname&amp;gt;\\S+)  (?P&amp;lt;host&amp;gt;\\S+)  (?P&amp;lt;remote_port&amp;gt;\\S+)   (?P&amp;lt;server_port&amp;gt;\\S+)   &amp;quot;(?P&amp;lt;upstream_addr&amp;gt;\\S+)&amp;quot;   &amp;quot;(?P&amp;lt;upstream_status&amp;gt;\\S+)&amp;quot; &amp;quot;(?P&amp;lt;upstream_response_time&amp;gt;\\S+)&amp;quot;  (?P&amp;lt;version&amp;gt;\\S+)?$&#39;
      - labels:
          remote_addr:
          http_x_forwarded_for:
          http_x_forwarded_for2:
          http_x_forwarded_for3:
          timestamp:
          request_method:
          document_uri:
          query_string:
          request_http_protocol:
          status:
          body_bytes_sent:
          request_time:
          http_referer:
          user_agent:
          traceId:
          spanId:
          server_addr:
          hostname:
          host:
          remote_port:
          server_port:
          upstream_addr:
          upstream_status:
          upstream_response_time:
          version:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;物理部署&#34;&gt;物理部署&lt;/h2&gt;

&lt;p&gt;物理部署很简单，可以直接下载二进制文件，官方还提供来repo，我们还可以编译&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/grafana/loki $GOPATH/src/github.com/grafana/loki
$ cd $GOPATH/src/github.com/grafana/loki
$ make loki
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后直接用二进制文件加配置文件进行启动就可以了，配置文件在/etc/loki/promtail.yaml and /etc/loki/loki.yaml。&lt;/p&gt;

&lt;h2 id=&#34;基本使用&#34;&gt;基本使用&lt;/h2&gt;

&lt;p&gt;下面我们就可以到grafana界面进行操作了，进入 grafana 界面，添加 loki 作为数据源，grafana原生就是支持loki的，所以直接添加loki 在集群中的地址，比如: &lt;a href=&#34;http://loki.monitoring.svc.cluster.local:3100&#34;&gt;http://loki.monitoring.svc.cluster.local:3100&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据源添加好了，我们就可以开始查询分析日志了，点击 Explore，下拉选择 loki 作为数据源，切到 Logs 模式(不用 Metrics 模式)，在 Log labels 按钮那里就能通过 label 筛选日志了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;选择器&#34;&gt;选择器&lt;/h2&gt;

&lt;p&gt;对于查询表达式的标签部分，将其包装在花括号中{}，然后使用键值对的语法来选择标签，多个标签表达式用逗号分隔，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{app=&amp;quot;mysql&amp;quot;,name=&amp;quot;mysql-backup&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前支持以下标签匹配运算符：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;=等于
!=不相等
=~正则表达式匹配
!~不匹配正则表达式
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{name=~&amp;quot;mysql.+&amp;quot;}
{name!~&amp;quot;mysql.+&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;适用于Prometheus标签选择器规则同样也适用于Loki日志流选择器,可以查看官网的&lt;a href=&#34;https://github.com/grafana/loki/blob/v1.5.0/docs/logql.md&#34;&gt;logQL&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;h2 id=&#34;promtail&#34;&gt;Promtail&lt;/h2&gt;

&lt;p&gt;promtail 可以理解为采集日志的 “Prometheus”. 它最巧妙的设计是完全复用了 Prometheus 的服务发现机制与 label 机制.&lt;/p&gt;

&lt;p&gt;以 Kubernetes 服务发现为例, Prometheus 可以通过 Pod 的 Annotations 与 Labels 等信息来确定 Pod 是否需要抓取指标, 假如要的话 Pod 的指标暴露在哪个端口上, 以及这个 Pod 本身有哪些 label, 即 target label.&lt;/p&gt;

&lt;p&gt;确定了这些信息之后, Prometheus 就可以去拉应用的指标了. 同时, 这些指标都会被打上 target label, 用于标注指标的来源. 等到在查询的时候, 我们就可以通过 target label, 比方说 pod_name=foo-123512 或 service=user-service 来获取特定的一个或一组 Pod 上的指标信息.&lt;/p&gt;

&lt;p&gt;promtail 是一样的道理. 它也是通过 Pod 的一些元信息来确定该 Pod 的日志文件位置, 同时为日志打上特定的 target label. 但要注意, 这个 label 不是标注在每一行日志事件上的, 而是被标注在”整个日志”上的. 这里”整个日志”在 loki 中抽象为 stream(日志流). 这就是 loki 文档中所说的”不索引日志, 只索引日志流”. 最终在查询端, 我们通过这些 label 就可以快速查询一个或一组特定的 stream.&lt;/p&gt;

&lt;p&gt;服务发现部分的代码非常直白, 可以去 pkg/promtail/targetmanager.go 中自己看一下, 提两个实现细节:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;promtail 要求所有 target 都跟自己属于同一个 node, 处于其它 node 上的 target 会被忽略;
promtail 使用 target 的 __path__ label 来确定日志路径;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过服务发现确定要收集的应用以及应用的日志路径后, promtail 就开始了真正的日志收集过程. 这里分三步:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、用 fsnotify 监听对应目录下的文件创建与删除(处理 log rolling)
2、对每个活跃的日志文件起一个 goroutine 进行类似 tail -f 的读取, 读取到的内容发送给 channel
3、一个单独的 goroutine 会解析 channel 中的日志行, 分批发送给 loki 的 backend
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;监听&#34;&gt;监听&lt;/h3&gt;

&lt;p&gt;fsnotify负责监听&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    select {
    case event := &amp;lt;-t.watcher.Events:
        switch event.Op {
        case fsnotify.Create:
            // protect against double Creates.
            if _, ok := t.tails[event.Name]; ok {
                level.Info(t.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;got &#39;create&#39; for existing file&amp;quot;, &amp;quot;filename&amp;quot;, event.Name)
                continue
            }

            // newTailer 中会启动一个 goroutine 来读目标文件
            tailer := newTailer(t.logger, t.handler, t.positions, t.path, event.Name)
            t.tails[event.Name] = tailer

        case fsnotify.Remove:
            tailer, ok := t.tails[event.Name]
            if ok {
                // 关闭 tailer
                helpers.LogError(&amp;quot;stopping tailer&amp;quot;, tailer.stop)
                delete(t.tails, event.Name)
            }
        }
    case err := &amp;lt;-t.watcher.Errors:
        level.Error(t.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error from fswatch&amp;quot;, &amp;quot;error&amp;quot;, err)
    case &amp;lt;-t.quit:
        return
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个for循环，一直来处理对应目录下的文件创建与删除的事件。&lt;/p&gt;

&lt;h3 id=&#34;tail日志&#34;&gt;tail日志&lt;/h3&gt;

&lt;p&gt;newTailer() 这个方法中启动的日志文件读取逻辑&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unc newTailer() {
    tail := tail.TailFile(path, tail.Config{
        Follow: true,
        Location: &amp;amp;tail.SeekInfo{
            Offset: positions.Get(path),
            Whence: 0,
        },
    })

    tailer := ...
    go tailer.run()
}

func (t *tailer) run() {
    for {
        select {
        case &amp;lt;-positionWait.C:
            // 定时同步当前读取位置
            pos := t.tail.Tell()
            t.positions.Put(t.path, pos)

        case line, ok := &amp;lt;-t.tail.Lines:
            // handler.Handle() 中是一些日志行的预处理逻辑, 最后将日志行转化为 `Entry` 对象扔进 channel
            if err := t.handler.Handle(model.LabelSet{}, line.Time, line.Text); err != nil {
                level.Error(t.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error handling line&amp;quot;, &amp;quot;error&amp;quot;, err)
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里直接调用了 hpcloud/tail 这个包来完成文件的 tail 操作. hpcloud/tail 的内部实现中, 在读到 EOF 之后, 同样调用了 fsnotify 来获取新内容写入的通知. fsnotify 这个包内部则是依赖了 inotify_init 和 inotify_add_watch 这两个系统调用。&lt;/p&gt;

&lt;h3 id=&#34;日志channel&#34;&gt;日志channel&lt;/h3&gt;

&lt;p&gt;这里有一个单独的 goroutine 会读取所有 tailer 通过 channel 传过来的日志(Entry对象), 然后按批发送给 loki&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    // 每次发送之后要重置计时器
    maxWait.Reset(c.cfg.BatchWait)
    select {
    case &amp;lt;-c.quit:
        return
    case e := &amp;lt;-c.entries:
        // Batch 足够大之后, 执行发送逻辑
        if batchSize+len(e.Line) &amp;gt; c.cfg.BatchSize {
            c.send(batch)
            // 重置 Batch
            batchSize = 0
            batch = map[model.Fingerprint]*logproto.Stream{}
        }

        // 收到 Entry, 先写进 Batch 当中
        batchSize += len(e.Line)

        // 每个 entry 要根据 label 放进对应的日志流(Stream)中
        fp := e.labels.FastFingerprint()
        stream, ok := batch[fp]
        if !ok {
            stream = &amp;amp;logproto.Stream{
                Labels: e.labels.String(),
            }
            batch[fp] = stream
        }
        stream.Entries = append(stream.Entries, e.Entry)

    case &amp;lt;-maxWait.C:
        // 到达每个批次的最大等待时间, 同样执行发送
        if len(batch) &amp;gt; 0 {
            c.send(batch);
            batchSize = 0
            batch = map[model.Fingerprint]*logproto.Stream{}
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用 channel + select 写 batch 逻辑真的挺优雅, 简单易读.&lt;/p&gt;

&lt;h2 id=&#34;loki&#34;&gt;loki&lt;/h2&gt;

&lt;p&gt;loki的基本架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;distributor&#34;&gt;Distributor&lt;/h3&gt;

&lt;p&gt;我们都知道promtail封装后label后的log数据发生到loki，Distributor就是第一个接收日志的组件。由于日志的写入量可能很大，所以不能在它们传入时将它们写入数据库。这会毁掉数据库。我们需要批处理和压缩数据。&lt;/p&gt;

&lt;p&gt;Loki通过构建压缩数据块来实现这一点，方法是在日志进入时对其进行gzip操作，组件ingester是一个有状态的组件，负责构建和刷新chunck，当chunk达到一定的数量或者时间后，刷新到存储中去。每个流的日志对应一个ingester,当日志到达Distributor后，根据元数据和hash算法计算出应该到哪个ingester上面。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们具体看一下promtail 的日志写入请求, 请求体由 protobuf 编码, 格式如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 一次写入请求, 包含多段日志流
type PushRequest struct {
    Streams []*Stream `protobuf:&amp;quot;bytes,1,rep,name=streams&amp;quot; json:&amp;quot;streams,omitempty&amp;quot;`
}
// 一段日志流, 包含它的 label, 以及这段日志流当中的每个日志事件: Entry
type Stream struct {
    Labels  string  `protobuf:&amp;quot;bytes,1,opt,name=labels,proto3&amp;quot; json:&amp;quot;labels,omitempty&amp;quot;`
    Entries []Entry `protobuf:&amp;quot;bytes,2,rep,name=entries&amp;quot; json:&amp;quot;entries&amp;quot;`
}
// 一个日志事件, 包含时间戳与内容
type Entry struct {
    Timestamp time.Time `protobuf:&amp;quot;bytes,1,opt,name=timestamp,stdtime&amp;quot; json:&amp;quot;timestamp&amp;quot;`
    Line      string    `protobuf:&amp;quot;bytes,2,opt,name=line,proto3&amp;quot; json:&amp;quot;line,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;distributor 收到请求后, 会将一个 PushRequest 中的 Stream 根据 labels 拆分成多个 PushRequest, 这个过程使用一致性哈希:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;streams := make([]streamTracker, len(req.Streams))
keys := make([]uint32, 0, len(req.Streams))
for i, stream := range req.Streams {
    // 获取每个 stream 的 label hash
    keys = append(keys, tokenFor(userID, stream.Labels))
    streams[i].stream = stream
}

// 根据 label hash 到 hash ring 上获取对应的 ingester 节点
// 这里的节点指 hash ring 上的节点, 一个节点可能有多个对等的 ingester 副本来做 HA
replicationSets := d.ring.BatchGet(keys, ring.Write)

// 将 Stream 按对应的 ingester 节点进行分组
samplesByIngester := map[string][]*streamTracker{}
ingesterDescs := map[string]ring.IngesterDesc{}
for i, replicationSet := range replicationSets {
    for _, ingester := range replicationSet.Ingesters {
        samplesByIngester[ingester.Addr] = append(samplesByIngester[ingester.Addr], &amp;amp;streams[i])
        ingesterDescs[ingester.Addr] = ingester
    }
}

for ingester, samples := range samplesByIngester {
    // 每组 Stream[] 又作为一个 PushRequest, 下发给对应的 ingester 节点
    d.sendSamples(localCtx, ingester, samples, &amp;amp;tracker)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 All in One 的运行模式中, hash ring 直接存储在内存中. 在生产环境, 由于要起多个 distributor 节点做高可用, 这个 hash ring 会存储到外部的 Consul 集群中.&lt;/p&gt;

&lt;h3 id=&#34;ingester&#34;&gt;Ingester&lt;/h3&gt;

&lt;p&gt;ingester接收到日志并开始构建chunk:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;基本上就是将日志进行压缩并附加到chunk上面。一旦chunk“填满”（数据达到一定数量或者过了一定期限），ingester将其刷新到数据库。我们对块和索引使用单独的数据库，因为它们存储的数据类型不同。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;刷新一个chunk之后，ingester然后创建一个新的空chunk并将新条目添加到该chunk中。&lt;/p&gt;

&lt;p&gt;我们再重代码层来分析一下，ingester 接收 distributor 下发的 PushRequest, 也就是多段日志流([]Entry). 在 ingester 内部会先将收到的 []Entry Append 到内存中的 Chunk 流([]Chunk). 同时会有一组 goroutine 异步将 Chunk 流存储到对象存储当中:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki9&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第一个 Append 过程很关键&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (i *instance) Push(ctx context.Context, req *logproto.PushRequest) error {
    for _, s := range req.Streams {
        // 将收到的日志流 Append 到内存中的日志流上, 同样地, 日志流按 label hash 索引
        fp := client.FastFingerprint(req.labels)
        stream, ok := i.streams[fp]
        if !ok {
            stream = newStream(fp, req.labels)
            // 这个过程中, 还会维护日志流的倒排索引(label -&amp;gt; stream)
            i.index.Add(labels, fp)
            i.streams[fp] = stream
        }
        stream.Push(ctx, s.Entries)
    }
    return nil
}

func (s *stream) Push(_ context.Context, entries []logproto.Entry) error {
    for i := range entries {
        // 假如当前 Chunk 已经关闭或者已经到达设定的最大 Chunk 大小, 则再创建一个新的 Chunk
        if s.chunks[0].closed || !s.chunks[0].chunk.SpaceFor(&amp;amp;entries[i]) {
            s.chunks = append(s.chunks, chunkDesc{
                chunk: chunkenc.NewMemChunk(chunkenc.EncGZIP),
            })
        }
        s.chunks[len(s.chunks)-1].chunk.Append(&amp;amp;entries[i])
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chunk 其实就是多条日志构成的压缩包. 将日志压成 Chunk 的意义是可以直接存入对象存储, 而对象存储是最便宜的(便宜是 loki 的核心目标之一). 在 一个 Chunk 到达指定大小之前它就是 open 的, 会不断 Append 新的日志(Entry) 到里面. 而在达到大小之后, Chunk 就会关闭等待持久化(强制持久化也会关闭 Chunk, 比如关闭 ingester 实例时就会关闭所有的 Chunk并持久化).&lt;/p&gt;

&lt;p&gt;对 Chunk 的大小控制是一个调优要点:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;假如 Chunk 容量过小: 首先是导致压缩效率不高. 同时也会增加整体的 Chunk 数量, 导致倒排索引过大. 最后, 对象存储的操作次数也会变多, 带来额外的性能开销;
假如 Chunk 过大: 一个 Chunk 的 open 时间会更长, 占用额外的内存空间, 同时, 也增加了丢数据的风险. 最后, Chunk 过大也会导致查询读放大, 比方说查一小时的数据却要下载整天的 Chunk;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;丢数据问题: 所有 Chunk 要在 close 之后才会进行存储. 因此假如 ingester 异常宕机, 处于 open 状态的 Chunk, 以及 close 了但还没有来得及持久化的 Chunk 数据都会丢失. 从这个角度来说, ingester 其实也是 stateful 的, 在生产中可以通过给 ingester 跑多个副本来解决这个问题. 另外, ingester 里似乎还没有写 WAL, 这感觉是一个 PR 机会, 可以练习一下写存储的基本功.&lt;/p&gt;

&lt;p&gt;异步存储过程就很简单了, 是一个一对多的生产者消费者模型:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 一个 goroutine 将所有的待存储的 chunks enqueue
func (i *Ingester) sweepStream(instance *instance, stream *stream, immediate bool) {

    // 有一组待存储的队列(默认16个), 取模找一个队列把要存储的 chunk 的引用塞进去
    flushQueueIndex := int(uint64(stream.fp) % uint64(i.cfg.ConcurrentFlushes))
    firstTime, _ := stream.chunks[0].chunk.Bounds()
    i.flushQueues[flushQueueIndex].Enqueue(&amp;amp;flushOp{
        model.TimeFromUnixNano(firstTime.UnixNano()), instance.instanceID,
        stream.fp, immediate,
    })
}

// 每个队列都有一个 goroutine 作为消费者在 dequeue
func (i *Ingester) flushLoop(j int) {
    for {
        op := i.flushQueues[j].Dequeue()
        // 实际的存储操作在这个方法中, 存储完成后, Chunk 会被清理掉
        i.flushUserSeries(op.userID, op.fp, op.immediate)

        // 存储失败的 chunk 会重新塞回队列中
        if op.immediate &amp;amp;&amp;amp; err != nil {
            op.from = op.from.Add(flushBackoff)
            i.flushQueues[j].Enqueue(op)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后是清理过程, 同样是一个单独的 goroutine 定时在跑. ingester 里的所有 Chunk 会在持久化之后隔一小段时间才被清理掉. 这个”一小段时间”由 chunk-retain-time 参数进行控制(默认 15 分钟). 这么做是为了加速热点数据的读取(真正被人看的日志中, 有99%都是生成后的一小段时间内被查看的).&lt;/p&gt;

&lt;h3 id=&#34;querier&#34;&gt;Querier&lt;/h3&gt;

&lt;p&gt;读取就非常简单了，由Querier负责给定一个时间范围和标签选择器，Querier查看索引以确定哪些块匹配，并通过greps将结果显示出来。它还从Ingester获取尚未刷新的最新数据，合并后返回。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;合并返回日志的时候，loki 里用了堆, 时间正序就用最小堆, 时间逆序就用最大堆:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这部分代码实现了一个简单的二叉堆, MinHeap 和 MaxHeap 实现了相反的 `Less()` 方法
type iteratorHeap []EntryIterator
func (h iteratorHeap) Len() int            { return len(h) }
func (h iteratorHeap) Swap(i, j int)       { h[i], h[j] = h[j], h[i] }
func (h iteratorHeap) Peek() EntryIterator { return h[0] }
func (h *iteratorHeap) Push(x interface{}) {
    *h = append(*h, x.(EntryIterator))
}
func (h *iteratorHeap) Pop() interface{} {
    old := *h
    n := len(old)
    x := old[n-1]
    *h = old[0 : n-1]
    return x
}
type iteratorMinHeap struct {
    iteratorHeap
}
func (h iteratorMinHeap) Less(i, j int) bool {
    return h.iteratorHeap[i].Entry().Timestamp.Before(h.iteratorHeap[j].Entry().Timestamp)
}
type iteratorMaxHeap struct {
    iteratorHeap
}
func (h iteratorMaxHeap) Less(i, j int) bool {
    return h.iteratorHeap[i].Entry().Timestamp.After(h.iteratorHeap[j].Entry().Timestamp)
}

// 将一组 Stream 的 iterator 合并成一个 HeapIterator
func NewHeapIterator(is []EntryIterator, direction logproto.Direction) EntryIterator {
    result := &amp;amp;heapIterator{}
    switch direction {
    case logproto.BACKWARD:
        result.heap = &amp;amp;iteratorMaxHeap{}
    case logproto.FORWARD:
        result.heap = &amp;amp;iteratorMinHeap{}
    default:
        panic(&amp;quot;bad direction&amp;quot;)
    }
    // pre-next each iterator, drop empty.
    for _, i := range is {
        result.requeue(i)
    }
    return result
}

func (i *heapIterator) requeue(ei EntryIterator) {
    if ei.Next() {
        heap.Push(i.heap, ei)
        return
    }
    if err := ei.Error(); err != nil {
        i.errs = append(i.errs, err)
    }
    helpers.LogError(&amp;quot;closing iterator&amp;quot;, ei.Close)
}

func (i *heapIterator) Next() bool {
    if i.curr != nil {
        i.requeue(i.curr)
    }
    if i.heap.Len() == 0 {
        return false
    }
    i.curr = heap.Pop(i.heap).(EntryIterator)
    currEntry := i.curr.Entry()
    // keep popping entries off if they match, to dedupe
    for i.heap.Len() &amp;gt; 0 {
        next := i.heap.Peek()
        nextEntry := next.Entry()
        if !currEntry.Equal(nextEntry) {
            break
        }

        next = heap.Pop(i.heap).(EntryIterator)
        i.requeue(next)
    }
    return true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;1、Loki的索引存储可以是cassandra/bigtable/dynamodb来进行扩展，chuncks可以是各种对象存储，放入对象存储中进行扩展。&lt;/p&gt;

&lt;p&gt;2、Querier和Distributor都是无状态的组件，可以水平扩展，可以使用负载均衡。&lt;/p&gt;

&lt;p&gt;3、对于ingester他虽然是有状态的但是，当新的节点加入或者减少，整节点间的chunk会重新分配，已适应新的散列环。这些信息需要存储到etcd或者consul等第三方工具中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.5.20&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;loki今天发布了1.5.0版本！引入了名为boltdb-shipper的新索引选项，这个新索引允许您仅使用对象存储（S3，GCS，文件系统等）来运行Loki。您不再需要单独的专用索引存储（DynamoDB，Bigtable，Cassandra等）！&lt;/p&gt;

&lt;p&gt;该boltdb-shipper索引使用内存中的boltdb索引，但会定期将快照发送到对象存储。这允许通过对象存储共享索引信息。&lt;/p&gt;

&lt;p&gt;将来可扩展可以通过boltdb-shipper索引和memberlist的gossip来完成集群功能。&lt;/p&gt;

&lt;p&gt;在云存储上，ring的信息可以通过gossip协议来进行同步。可以看一下下面的这个配置，基于s3和memberlist的可扩展模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;auth_enabled: false

server:
  http_listen_port: 3100

distributor:
  ring:
    store: memberlist

ingester:
  lifecycler:
    ring:
      kvstore:
        store: memberlist
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 5m
  chunk_retain_period: 30s

memberlist:
  abort_if_cluster_join_fails: false

  # Expose this port on all distributor, ingester
  # and querier replicas.
  bind_port: 7946

  # You can use a headless k8s service for all distributor,
  # ingester and querier components.
  join_members:
  - loki-gossip-ring.loki.svc.cluster.local:7946

  max_join_backoff: 1m
  max_join_retries: 10
  min_join_backoff: 1s

schema_config:
  configs:
  - from: 2020-05-15
    store: boltdb-shipper
    object_store: s3
    schema: v11
    index:
      prefix: index_
      period: 168h

storage_config:
 boltdb_shipper:
   active_index_directory: /loki/index
   cache_location: /loki/index_cache
   resync_interval: 5s
   shared_store: s3

 aws:
   s3: s3://access_key:secret_access_key@custom_endpoint/bucket_name
   s3forcepathstyle: true

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Grok_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/grok_exporter/</link>
          <pubDate>Fri, 10 Jan 2020 17:53:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/grok_exporter/</guid>
          <description>&lt;p&gt;grok_exporter是基于logstash的grok的插件开发的日志分析工具，可以分析非结构化日志根据正则表达式进行匹配，然后生成适合prometheus的规则规范的metrics。&lt;/p&gt;

&lt;h1 id=&#34;编译安装&#34;&gt;编译安装&lt;/h1&gt;

&lt;p&gt;下载代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://github.com/fstab/grok_exporter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译环境&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. go
2. gcc
3. Oniguruma
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前面两个就不多说了，最后一个安装说明一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. Installing the Oniguruma library on OS X

    brew install oniguruma

2.Installing the Oniguruma library on Ubuntu Linux

    sudo apt-get install libonig-dev

3.Installing the Oniguruma library from source

    curl -sLO https://github.com/kkos/oniguruma/releases/download/v6.9.4/onig-6.9.4.tar.gz
    tar xfz onig-6.9.4.tar.gz
    cd /tmp/onig-6.9.4
    ./configure
    make
    make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/fstab/grok_exporter
cd grok_exporter
git submodule update --init --recursive
go install .
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;p&gt;基本启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./grok_exporter -config ./example/config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以在 &lt;a href=&#34;http://localhost:9144/metrics&#34;&gt;http://localhost:9144/metrics&lt;/a&gt; 来访问指标&lt;/p&gt;

&lt;p&gt;其他启动参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage of ./grok_exporter:
  -config string
        Path to the config file. Try &#39;-config ./example/config.yml&#39; to get started.
  -showconfig
        Print the current configuration to the console. Example: &#39;grok_exporter -showconfig -config ./example/config.yml&#39;
  -version
        Print the grok_exporter version.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The grok_exporter configuration file consists of five main sections:

global:
    # Config version
input:
    # How to read log lines (file or stdin).
grok:
    # Available Grok patterns.
metrics:
    # How to map Grok fields to Prometheus metrics.
server:
    # How to expose the metrics via HTTP(S).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;global 主要是配置config的版本，目前最新版都是V2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
    config_version: 2
    retention_check_interval: 53s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1、config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grok_exporter   config_version
≤ 0.1.4 1 (see CONFIG_v1.md)
0.2.X, 1.0.X    2 (current version)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、retention_check_interval&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The retention_check_interval is the interval at which grok_exporter checks for expired metrics.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;input 主要是重哪边采集日志，可以是文件，标准输入等,我们使用的是文件输入的方式，在这边只要将文件路径配置好就行。&lt;/p&gt;

&lt;p&gt;1、file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input:
    type: file
    paths:
    - /var/logdir1/*.log
    - /var/logdir2/*.log
    readall: false
    fail_on_missing_logfile: true
    poll_interval_seconds: 5 # should not be needed in most cases, see below
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;type就是类型&lt;/li&gt;
&lt;li&gt;path就是获取文件配置&lt;/li&gt;
&lt;li&gt;readall表示是否重文件开头开始读取，true表示重文件开头读取，false表示重结尾读取&lt;/li&gt;
&lt;li&gt;fail_on_missing_logfile表示不存在采集的文件是否启动成功，如果是true代表文件不存在就启动失败，反之亦然。&lt;/li&gt;
&lt;li&gt;poll_interval_seconds不重要&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、stdin&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input:
    type: stdin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比如monitor the output of journalctl&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journalctl -f | grok_exporter -config config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Webhook&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input:

    type: webhook

    # HTTP Path to POST the webhook
    # Default is `/webhook`
    webhook_path: /webhook

    # HTTP Body POST Format
    # text_single: Webhook POST body is a single plain text log entry
    # text_bulk: Webhook POST body contains multiple plain text log entries
    #   separated by webhook_text_bulk_separator (default: \n\n)
    # json_single: Webhook POST body is a single json log entry.  Log entry
    #   text is selected from the value of a json key determined by
    #   webhook_json_selector.
    # json_bulk: Webhook POST body contains multiple json log entries.  The
    #   POST body envelope must be a json array &amp;quot;[ &amp;lt;entry&amp;gt;, &amp;lt;entry&amp;gt; ]&amp;quot;.  Log
    #   entry text is selected from the value of a json key determined by
    #   webhook_json_selector.
    # Default is `text_single`
    webhook_format: json_bulk

    # JSON Path Selector
    # Within an json log entry, text is selected from the value of this json selector
    #   Example &amp;quot;.path.to.element&amp;quot;
    # Default is `.message`
    webhook_json_selector: .message

    # Bulk Text Separator
    # Separator for text_bulk log entries
    # Default is `\n\n`
    webhook_text_bulk_separator: &amp;quot;\n\n&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;grok 主要是匹配规则的相关正则表达式的定义，我们可以自定义的我们url相关的路径，然后根据路径进行匹配&lt;/p&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grok:
    patterns_dir: ./logstash-patterns-core/patterns
    additional_patterns:
    - &#39;EXIM_MESSAGE [a-zA-Z ]*&#39;
    - &#39;EXIM_SENDER_ADDRESS F=&amp;lt;%{EMAILADDRESS}&amp;gt;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;patterns_dir是指定我们写好的正则表达式文件的目录，我们可以自己去这个目录下编写，，正常logstash-patterns-core是以前用于logstash的，包含了大部分的正则，可以进去查看使用，当然如果自定义的话，也可以自己去写这个文件在这个目录下。&lt;/li&gt;
&lt;li&gt;additional_patterns也是我们给正则表达式起个名字，不用写在文件里，写在这里直接用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如我做的nginx的url的匹配获取参数为标签的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grok:
    additional_patterns:
      - &#39;URL /springRed/getRewards.do!?&#39;            //获取url
      - &#39;ID (?&amp;lt;=promotionId=).*?(?=[&amp;quot;|&amp;amp;|  ])&#39;       //获取参数id
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;metrics 主要定义我们采集的指标&lt;/p&gt;

&lt;p&gt;支持四种类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Counter
Gauge
Histogram
Summary
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.counter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
    - type: counter
      name: alice_occurrences_total
      help: number of log lines containing alice
      match: &#39;alice&#39;
      labels:
          logfile: &#39;{{base .logfile}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;match就是我们匹配的字段，可以正则表达式，可以是中间的任何一段。&lt;/li&gt;
&lt;li&gt;labels就是我们指标中使用的标签，可以使用match中定义的变量&lt;/li&gt;
&lt;li&gt;counter不需要指定value，是自己累加的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
    - type: counter
      name: count_total
      help: Total Number of RedPackage Request.
      match: &#39;%{URL:url}.*%{ID:id}&#39;
      labels:
        url: &#39;{{.url}}&#39;
        id: &#39;{{.id}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.gauge&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
    - type: gauge
      name: grok_example_values
      help: Example gauge metric with labels.
      match: &#39;%{DATE} %{TIME} %{USER:user} %{NUMBER:val}&#39;
      value: &#39;{{.val}}&#39;
      cumulative: false
      labels:
          user: &#39;{{.user}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.Histogram&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
    - type: histogram
      name: grok_example_values
      help: Example histogram metric with labels.
      match: &#39;%{DATE} %{TIME} %{USER:user} %{NUMBER:val}&#39;
      value: &#39;{{.val}}&#39;
      buckets: [1, 2, 3]
      labels:
          user: &#39;{{.user}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.Summary&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics:
   - type: summary
      name: grok_example_values
      help: Summary metric with labels.
      match: &#39;%{DATE} %{TIME} %{USER:user} %{NUMBER:val}&#39;
      value: &#39;{{.val}}&#39;
      quantiles: {0.5: 0.05, 0.9: 0.01, 0.99: 0.001}
      labels:
          user: &#39;{{.user}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;server服务器监听配置&lt;/p&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server:
    protocol: https
    host: localhost
    port: 9144
    path: /metrics
    cert: /path/to/cert
    key: /path/to/key
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;实例&#34;&gt;实例&lt;/h1&gt;

&lt;p&gt;采集nginx日志中 &lt;a href=&#34;http://test.com:8088/spring/getRewards?promotionId=21&#34;&gt;http://test.com:8088/spring/getRewards?promotionId=21&lt;/a&gt;    类似于这个url的 的数量的统计&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test grok_exporter]# cat config.yml
global:
    config_version: 2
input:
    type: file
    path: /usr/local/nginx/logs/access_http.log
    readall: true # Read from the beginning of the file? False means we start at the end of the file and read only new lines.
grok:
    additional_patterns:
      - &#39;URL /spring/getRewards!?&#39;
      - &#39;ID (?&amp;lt;=promotionId=).*?(?=[&amp;quot;|&amp;amp;|  ])&#39;
metrics:
    - type: counter
      name: count_total
      help: Total Number of RedPackage Request.
      match: &#39;%{URL:url}.*%{ID:id}&#39;
      labels:
        url: &#39;{{.url}}&#39;
        id: &#39;{{.id}}&#39;

server:
    host: 0.0.0.0
    port: 9144
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./grok_exporter -config ./example/config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;采集数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# HELP count_total Total Number of RedPackage Request.
# TYPE count_total counter
count_total{id=&amp;quot;21&amp;quot;,url=&amp;quot;/springRed/getRewards.do&amp;quot;} 387184
count_total{id=&amp;quot;22&amp;quot;,url=&amp;quot;/springRed/getRewards.do&amp;quot;} 384322
count_total{id=&amp;quot;23&amp;quot;,url=&amp;quot;/springRed/getRewards.do&amp;quot;} 381606
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;进程会一直对文件进行读取计算，所以就算不采集数据，程序在这一块也是有消耗的。读取的方式是可以选择的，重文件开始还是文件结束。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus mtail</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/mtail/</link>
          <pubDate>Fri, 10 Jan 2020 17:53:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/mtail/</guid>
          <description>&lt;p&gt;mtail是一个可以从应用程序日志中提取指标，并将其导出到时间序列数据库或时间序列计算器中，以便配置警报和仪表盘的工具。&lt;/p&gt;

&lt;p&gt;提取由定义了模式和动作的mtail程序来控制。&lt;/p&gt;

&lt;h1 id=&#34;安装和使用&#34;&gt;安装和使用&lt;/h1&gt;

&lt;h2 id=&#34;源码编译&#34;&gt;源码编译&lt;/h2&gt;

&lt;p&gt;需要安装运行版本不低于1.9的Go环境。如果不想在本地环境中安装Go，也可以选择Dockerfile。&lt;/p&gt;

&lt;h2 id=&#34;下载编译好的二进制文件后运行即可&#34;&gt;下载编译好的二进制文件后运行即可&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# nohup ./mtail -progs /path/to/mtailprograms -logs /path/to/logfile &amp;amp;
# cat /path/to/mtailprograms
counter nginx_line_count
#正则统计访问日志的行数，可以在grafana中配置视图观察访问量趋势
/$/ {
  nginx_line_count++
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;./mtail -h
mtail version v3.0.0-rc12 git revision 936050df9856da258f13e5df71a5a0c7f8f6acdc go version go1.10.1

Usage:
  -address string
        Host or IP address on which to bind HTTP listener
  -alsologtostderr
        log to standard error as well as files
  -block_profile_rate int
        Nanoseconds of block time before goroutine blocking events reported. 0 turns off.  See https://golang.org/pkg/runtime/#SetBlockProfileRate
  -collectd_prefix string
        Prefix to use for collectd metrics.
  -collectd_socketpath string
        Path to collectd unixsock to write metrics to.
  -compile_only
        Compile programs only, do not load the virtual machine.
  -dump_ast
        Dump AST of programs after parse (to INFO log).
  -dump_ast_types
        Dump AST of programs with type annotation after typecheck (to INFO log).
  -dump_bytecode
        Dump bytecode of programs (to INFO log).
  -emit_prog_label
        Emit the &#39;prog&#39; label in variable exports. (default true)
  -graphite_host_port string
        Host:port to graphite carbon server to write metrics to.
  -graphite_prefix string
        Prefix to use for graphite metrics.
  -log_backtrace_at value
        when logging hits line file:N, emit a stack trace
  -log_dir string
        If non-empty, write log files in this directory
  -logfds value
        List of file descriptor numbers to monitor, separated by commas.  This flag may be specified multiple times.
  -logs value
        List of log files to monitor, separated by commas.  This flag may be specified multiple times.
  -logtostderr
        log to standard error instead of files
  -metric_push_interval_seconds int
        Interval between metric pushes, in seconds. (default 60)
  -metric_push_write_deadline duration
        Time to wait for a push to succeed before exiting with an error. (default 10s)
  -mtailDebug int
        Set parser debug level.
  -mutex_profile_fraction int
        Fraction of mutex contention events reported.  0 turns off.  See http://golang.org/pkg/runtime/#SetMutexProfileFraction
  -one_shot
        Compile the programs, then read the contents of the provided logs from start until EOF, print the values of the metrics store and exit. This is a debugging flag only, not for production use.
  -one_shot_metrics
        DEPRECATED: Dump metrics (to stdout) after one shot mode.
  -override_timezone string
        If set, use the provided timezone in timestamp conversion, instead of UTC.
  -port string
        HTTP port to listen on. (default &amp;quot;3903&amp;quot;)
  -progs string
        Name of the directory containing mtail programs
  -statsd_hostport string
        Host:port to statsd server to write metrics to.
  -statsd_prefix string
        Prefix to use for statsd metrics.
  -stderrthreshold value
        logs at or above this threshold go to stderr
  -syslog_use_current_year
        Patch yearless timestamps with the present year. (default true)
  -v value
        log level for V logs
  -version
        Print mtail version information.
  -vmodule value
        comma-separated list of pattern=N settings for file-filtered logging
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 架构解耦</title>
          <link>https://kingjcy.github.io/post/architecture/coupling/</link>
          <pubDate>Sun, 05 Jan 2020 10:04:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/coupling/</guid>
          <description>&lt;p&gt;自身服务的变动，需要其他依赖服务跟着升级变更，这就叫服务耦合，比如数据库换了一个ip，此时往往连接此数据库的上游需要修改配置重启，明明换ip的是你，凭什么配合重启的却是我？这就是一种典型的架构设计上“反向依赖”的问题。&lt;/p&gt;

&lt;h1 id=&#34;常规方案&#34;&gt;常规方案&lt;/h1&gt;

&lt;p&gt;我们在架构设计的时候就要将服务进行解耦，解决反向依赖带来的各种问题。&lt;/p&gt;

&lt;h2 id=&#34;公共库导致耦合&#34;&gt;公共库导致耦合&lt;/h2&gt;

&lt;p&gt;三个服务s1/s2/s3，通过一个公共的库biz.jar来实现一段业务逻辑，s1/s2/s3其实间接通过biz.jar耦合在了一起，一个业务s1修改一块公共的代码，导致影响其他业务s2/s3，架构上是不合理的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;业务垂直拆分&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果公共的库biz.jar中实现的逻辑“业务特性”很强，可以拆分为biz1.jar/biz2.jar/biz3.jar，来对s1/s2/s3进行解耦。这样的话，任何业务的改动，影响范围只是自己，不会影响其他人。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;服务化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果biz.jar中实现的逻辑“业务共性”很强，可以将biz.jar优化为biz.service服务，来对s1/s2/s3进行解耦。服务化之后，兼容性能更好的通过接口自动化回归测试来保证。&lt;/p&gt;

&lt;p&gt;基础服务的抽象，本身是一种共性聚焦，是系统解耦常见的方案。&lt;/p&gt;

&lt;h1 id=&#34;服务化不彻底导致耦合&#34;&gt;服务化不彻底导致耦合&lt;/h1&gt;

&lt;p&gt;服务化是解决“业务共性”组件库导致系统耦合的常见方案之一，但如果服务化不彻底，service本身也容易成为业务耦合点。典型的服务化不彻底导致的业务耦合的特征是，共性服务中，包含大量“根据不同业务，执行不同个性分支”的代码。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;switch (biz-type)
case biz-1 : exec1
case biz-2 : exec2
case biz-3 : exec3
…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/coupling/coupling.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在这种架构下，biz-1/biz-2/biz-3有个性的业务需求，可能导致修改代码的是共性的biz-service，使其成为研发瓶颈，架构上也是不合理的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;业务特性代码上浮，业务共性代码下沉，彻底解耦&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/coupling/coupling2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;把swithc case中业务特性代码放到业务层实现，这样biz-1/biz-2/biz-3有个性的业务需求，升级的是自己的业务系统。&lt;/p&gt;

&lt;h2 id=&#34;notify的不合理实现导致的耦合&#34;&gt;notify的不合理实现导致的耦合&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/coupling/coupling3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;消息发送方不关注消息接收方的执行结果，如果采用调用的方式来实现通知，会导消息发送方和消息接收方耦合。比如如何新增消息接收方biz-4，会发现修改代码的是消息发送方，新增一个对biz-4的调用，极不合理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过MQ实现解耦&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/coupling/coupling4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;消息发送方upper将消息发布给MQ，消息接收方从MQ去订阅，任何新增对消息的消费，upper都不需要修改代码。&lt;/p&gt;

&lt;h2 id=&#34;配置中的ip导致上下游耦合&#34;&gt;配置中的ip导致上下游耦合&lt;/h2&gt;

&lt;p&gt;下游服务换ip，可能导致多个服务调用方修改配置重启。上下游间接的通过ip这个配置耦合在了一起，架构不合理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过内网域名而不是ip来进行下游连接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果在配置中使用内网域名来进行下游连接，当下游服务或者数据库更换ip时，只需要运维层面将内网域名指向新的ip，然后统一切断原有旧的连接，连接就能够自动切换到新的ip上来。这个过程不需要所有上游配合，非常帅气，强烈推荐！这也是我们常用的vip的架构设计，也可以新增服务发现服务来解决这个问题，用于解决下游集群扩容等问题。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 搜索系统</title>
          <link>https://kingjcy.github.io/post/architecture/search/</link>
          <pubDate>Sat, 04 Jan 2020 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/search/</guid>
          <description>&lt;p&gt;搜索系统在我们日常生活中经常使用，比如baidu，google等，我们来看看其架构和原理。&lt;/p&gt;

&lt;h1 id=&#34;全网搜索引擎架构&#34;&gt;全网搜索引擎架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;核心系统主要分为三部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spider爬虫系统&lt;/li&gt;
&lt;li&gt;search&amp;amp;index建立索引与查询索引系统

&lt;ul&gt;
&lt;li&gt;一部分用于生成索引数据build_index&lt;/li&gt;
&lt;li&gt;一部分用于查询索引数据search_index&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;rank打分排序系统&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心数据主要分为两部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;web网页库&lt;/li&gt;
&lt;li&gt;index索引数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心流程：&lt;/p&gt;

&lt;p&gt;写入&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spider把互联网网页抓过来&lt;/li&gt;
&lt;li&gt;spider把互联网网页存储到网页库中（这个对存储的要求很高，要存储几乎整个“万维网”的镜像）&lt;/li&gt;
&lt;li&gt;build_index从网页库中读取数据，完成分词&lt;/li&gt;
&lt;li&gt;build_index生成倒排索引&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;检索&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;search_index获得用户的搜索词，完成分词&lt;/li&gt;
&lt;li&gt;search_index查询倒排索引，获得“字符匹配”网页，这是初筛的结果&lt;/li&gt;
&lt;li&gt;rank对初筛的结果进行打分排序&lt;/li&gt;
&lt;li&gt;rank对排序后的第一页结果返回&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如baidu，google就是全网搜索引擎架构，我们来看一下百度如何实现实时搜索十五分钟内的新闻的？&lt;/p&gt;

&lt;p&gt;首先任何对数据的更新，并不会实时修改索引，一旦产生碎片，会大大降低检索效率。既然索引数据不能实时修改，如何保证最新的网页能够被索引到呢？&lt;/p&gt;

&lt;p&gt;就是采用了将索引分为全量库、日增量库、小时增量库。比如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;300亿数据在全量索引库中&lt;/li&gt;
&lt;li&gt;1000万1天内修改过的数据在天库中&lt;/li&gt;
&lt;li&gt;50万1小时内修改过的数据在小时库中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当有查询请求发生时，会同时查询各个级别的索引，将结果合并，得到最新的数据：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;全量库是紧密存储的索引，无碎片，速度快&lt;/li&gt;
&lt;li&gt;天库是紧密存储，速度快&lt;/li&gt;
&lt;li&gt;小时库数据量小，速度也快&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当有修改请求发生时，只会操作最低级别的索引，例如小时库。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据库的同步&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dumper：将在线的数据导出&lt;/li&gt;
&lt;li&gt;merger：将离线的数据合并到高一级别的索引中去&lt;/li&gt;
&lt;li&gt;小时库，一小时一次，合并到天库中去；&lt;/li&gt;
&lt;li&gt;天库，一天一次，合并到全量库中去；&lt;/li&gt;
&lt;li&gt;这样就保证了小时库和天库的数据量都不会特别大；&lt;/li&gt;
&lt;li&gt;如果数据量和并发量更大，还能增加星期库，月库来缓冲。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据的写入和读取都是实时的，所以58同城能够检索到1秒钟之前发布的帖子，即使全量库有300亿的数据。百度也能搜索到十五分钟内的数据。&lt;/p&gt;

&lt;h1 id=&#34;站内搜索引擎架构&#34;&gt;站内搜索引擎架构&lt;/h1&gt;

&lt;p&gt;全网搜索需要spider要被动去抓取数据，站内搜索是内部系统生成的数据，例如“发布系统”会将生成的帖子主动推给build_data系统，看似“很小”的差异，架构实现上难度却差很多：全网搜索如何“实时”发现“全量”的网页是非常困难的，而站内搜索容易实时得到全部数据。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/search/search1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;spider和search&amp;amp;index是相对工程的系统，rank是和业务、策略紧密、算法相关的系统，搜索体验的差异主要在此，而业务、策略的优化是需要时间积累的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google的体验比Baidu好，根本在于前者rank牛逼&lt;/li&gt;
&lt;li&gt;国内互联网公司短时间要搞一个体验超越Baidu的搜索引擎，是很难的，真心需要时间的积累&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;搜索原理&#34;&gt;搜索原理&lt;/h1&gt;

&lt;h2 id=&#34;核心数据结构&#34;&gt;核心数据结构&lt;/h2&gt;

&lt;h3 id=&#34;正排索引-forward-index&#34;&gt;正排索引（forward index）&lt;/h3&gt;

&lt;p&gt;由key查询实体的过程，是正排索引。比如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户表：t_user(uid, name, passwd, age, sex)，由uid查询整行的过程，就是正排索引查询。&lt;/li&gt;
&lt;li&gt;网页库：t_web_page(url, page_content)，由url查询整个网页的过程，也是正排索引查询。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举个例子，假设有3个网页：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;url1 -&amp;gt; “我爱北京”
url2 -&amp;gt; “我爱到家”
url3 -&amp;gt; “到家美好”
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个正排索引Map&lt;url, page_content&gt;。&lt;/p&gt;

&lt;h3 id=&#34;分词&#34;&gt;分词&lt;/h3&gt;

&lt;p&gt;分词就是将基本的词进行拆分，比如将上面的内容进行分词：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;url1 -&amp;gt; {我，爱，北京}
url2 -&amp;gt; {我，爱，到家}
url3 -&amp;gt; {到家，美好}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个分词后的正排索引Map&lt;url, list&lt;item&gt;&amp;gt;。&lt;/p&gt;

&lt;h3 id=&#34;倒排索引-inverted-index&#34;&gt;倒排索引（inverted index）&lt;/h3&gt;

&lt;p&gt;由item查询key的过程，是倒排索引。比如上面的网页进行倒排&lt;/p&gt;

&lt;p&gt;分词后倒排索引：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;我 -&amp;gt; {url1, url2}
爱 -&amp;gt; {url1, url2}
北京 -&amp;gt; {url1}
到家 -&amp;gt; {url2, url3}
美好 -&amp;gt; {url3}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由检索词item快速找到包含这个查询词的网页Map&lt;item, list&lt;url&gt;&amp;gt;就是倒排索引。&lt;/p&gt;

&lt;p&gt;正排索引和倒排索引是spider和build_index系统提前建立好的数据结构，为什么要使用这两种数据结构，是因为它能够快速的实现“用户网页检索”需求（业务需求决定架构实现）&lt;/p&gt;

&lt;h2 id=&#34;基本搜索原理&#34;&gt;基本搜索原理&lt;/h2&gt;

&lt;p&gt;假设搜索词是“我爱”，原理如下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;分词，“我爱”会分词为{我，爱}，时间复杂度为O(1)&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个分词后的item，从倒排索引查询包含这个item的网页list&lt;url&gt;，时间复杂度也是O(1)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;我 -&amp;gt; {url1, url2}
爱 -&amp;gt; {url1, url2}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;求list&lt;url&gt;的交集，就是符合所有查询词的结果网页，对于这个例子，{url1, url2}就是最终的查询结果&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;核心就在于如何求两个子集的交集&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;循环遍历，土办法，时间复杂度O(n*n)&lt;/li&gt;
&lt;li&gt;有序list求交集，拉链法，时间复杂度O(n)&lt;/li&gt;
&lt;li&gt;分桶并行优化，多线程并行&lt;/li&gt;
&lt;li&gt;bitmap再次优化，大大提高运算并行度，时间复杂度O(n)&lt;/li&gt;
&lt;li&gt;跳表skiplist，时间复杂度为O(log(n))&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些都是搜索引擎中常用的求交集的方法，可见搜索引擎中算法使用还是比较多的，其实引擎基本都是算法的天下。&lt;/p&gt;

&lt;h1 id=&#34;搜索架构演进&#34;&gt;搜索架构演进&lt;/h1&gt;

&lt;h2 id=&#34;原始阶段-like&#34;&gt;原始阶段-LIKE&lt;/h2&gt;

&lt;p&gt;直接通过数据看的like关键词进行模糊匹配，能够快速满足业务需求，显然效率低，每次需要全表扫描，计算量大，并发高时cpu容易100%，而且不支持分词，很多查询不了。&lt;/p&gt;

&lt;h2 id=&#34;初级阶段-全文索引&#34;&gt;初级阶段-全文索引&lt;/h2&gt;

&lt;p&gt;正常数据库查询的时候，我们一般都会想到给数据库建索引，然后用于查询，但是这情况情况也存在一些问题。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;只适用于MyISAM&lt;/li&gt;
&lt;li&gt;由于全文索引利用的是数据库特性，搜索需求和普通CURD需求耦合在数据库中：检索需求并发大时，可能影响CURD的请求；CURD并发大时，检索会非常的慢；&lt;/li&gt;
&lt;li&gt;数据量达到百万级别，性能还是会显著降低，查询返回时间很长，业务难以接受&lt;/li&gt;
&lt;li&gt;比较难水平扩展&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;中级阶段-开源外置索引&#34;&gt;中级阶段-开源外置索引&lt;/h2&gt;

&lt;p&gt;外置索引的核心思路是：索引数据与原始数据分离，前者满足搜索需求，后者满足CURD需求，通过一定的机制（双写，通知，定期重建）来保证数据的一致性。&lt;/p&gt;

&lt;p&gt;Solr，Lucene，ES都是常见的开源方案。比如ES“封装一个接口友好的服务，屏蔽底层复杂性”&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ES是一个以Lucene为内核来实现搜索功能，提供REStful接口的服务&lt;/li&gt;
&lt;li&gt;ES能够支持很大数据量的信息存储，支持很高并发的搜索请求&lt;/li&gt;
&lt;li&gt;ES支持集群，向使用者屏蔽高可用/可扩展/负载均衡等复杂特性&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ES完全能满足10亿数据量，5k吞吐量的常见搜索业务需求，强烈推荐。&lt;/p&gt;

&lt;h2 id=&#34;高级阶段-自研搜索引擎&#34;&gt;高级阶段-自研搜索引擎&lt;/h2&gt;

&lt;p&gt;当数据量进一步增加，达到10亿、100亿数据量；并发量也进一步增加，达到每秒10万吞吐；业务个性也逐步增加的时候，就需要自研搜索引擎了，定制化实现搜索内核了。比如58同城的E-search，等等。&lt;/p&gt;

&lt;p&gt;到了定制化自研搜索引擎的阶段，超大数据量、超高并发量为设计重点，为了达到“无限容量、无限并发”的需求，架构设计需要重点考虑“扩展性”，力争做到：增加机器就能扩容（数据量+并发量）。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 文件存储系统hdfs</title>
          <link>https://kingjcy.github.io/post/distributed/store/fs/hfds/</link>
          <pubDate>Sun, 15 Dec 2019 20:21:18 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/fs/hfds/</guid>
          <description>&lt;p&gt;Hadoop：一个分布式系统基础架构，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。&lt;/p&gt;

&lt;p&gt;Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;特点&#34;&gt;特点&lt;/h2&gt;

&lt;p&gt;优势&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;专为存储超大文件而设计：hdfs应该能够支持GB级别大小的文件；它应该能够提供很大的数据带宽并且能够在集群中拓展到成百上千个节点；它的一个实例应该能够支持千万数量级别的文件。&lt;/li&gt;
&lt;li&gt;适用于流式的数据访问：hdfs适用于批处理的情况而不是交互式处理；它的重点是保证高吞吐量而不是低延迟的用户响应&lt;/li&gt;
&lt;li&gt;容错性：完善的冗余备份机制&lt;/li&gt;
&lt;li&gt;支持简单的一致性模型：HDFS需要支持一次写入多次读取的模型，而且写入过程文件不会经常变化&lt;/li&gt;
&lt;li&gt;移动计算优于移动数据：HDFS提供了使应用计算移动到离它最近数据位置的接口&lt;/li&gt;
&lt;li&gt;兼容各种硬件和软件平台&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大量小文件：文件的元数据都存储在NameNode内存中，大量小文件会占用大量内存。&lt;/li&gt;
&lt;li&gt;低延迟数据访问：hdfs是专门针对高数据吞吐量而设计的&lt;/li&gt;
&lt;li&gt;多用户写入，任意修改文件&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基本架构&#34;&gt;基本架构&lt;/h1&gt;

&lt;p&gt;HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode，HDFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/hdfs2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;NameNode&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当一个客户端请求一个文件或者存储一个文件时，它需要先知道具体到哪个DataNode上存取，获得这些信息后，客户端再直接和这个DataNode进行交互，而这些信息的维护者就是NameNode。&lt;/p&gt;

&lt;p&gt;NameNode管理着文件系统命名空间，它维护这文件系统树及树中的所有文件和目录。NameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。对于实际文件数据的保存与操作，都是由DataNode负责。当一个客户端请求数据时，它仅仅是从NameNode中获取文件的元信息，而具体的数据传输不需要经过NameNode，是由客户端直接与相应的DataNode进行交互。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;SecondaryNameNode&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;SecondaryNameNode并不是NameNode的备份。我们从前面的介绍已经知道，所有HDFS文件的元信息都保存在NameNode的内存中。在NameNode启动时，它首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在了内存中，同时为了防止数据丢失，这些操作又会不断被持久化到本地edits文件中。&lt;/p&gt;

&lt;p&gt;Edits文件存在的目的是为了提高系统的操作效率，NameNode在更新内存中的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题而诞生的。&lt;/p&gt;

&lt;p&gt;SecondaryNameNode的角色就是定期的合并edits和fsimage文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;DataNode&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;DataNode是hdfs中的worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除、和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。当对hdfs文件系统进行读写时，NameNode告知客户端每个数据驻留在哪个DataNode，客户端直接与DataNode进行通信，DataNode还会与其它DataNode通信，复制这些块以实现冗余。&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;数据备份&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;HDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其它所有数据块大小都是一样的。数据块的大小和备份因子都是可以配置的。NameNode负责各个数据块的备份，DataNode会通过心跳的方式定期的向NameNode发送自己节点上的Block 报告，这个报告中包含了DataNode节点上的所有数据块的列表。&lt;/p&gt;

&lt;p&gt;文件副本的分布位置直接影响着HDFS的可靠性和性能。一个大型的HDFS文件系统一般都是需要跨很多机架的，不同机架之间的数据传输需要经过网关，并且，同一个机架中机器之间的带宽要大于不同机架机器之间的带宽。如果把所有的副本都放在不同的机架中，这样既可以防止机架失败导致数据块不可用，又可以在读数据时利用到多个机架的带宽，并且也可以很容易的实现负载均衡。但是，如果是写数据，各个数据块需要同步到不同的机架，会影响到写数据的效率。&lt;/p&gt;

&lt;p&gt;而在Hadoop中，如果副本数量是3的情况下，Hadoop默认是这么存放的，把第一个副本放到机架的一个节点上，另一个副本放到同一个机架的另一个节点上，把最后一个节点放到不同的机架上。这种策略减少了跨机架副本的个数提高了写的性能，也能够允许一个机架失败的情况，算是一个很好的权衡。&lt;/p&gt;

&lt;p&gt;关于副本的选择，在读的过程中，HDFS会选择最近的一个副本给请求者。&lt;/p&gt;

&lt;p&gt;关于安全模式，当 Hadoop的NameNode节点启动时，会进入安全模式阶段。在此阶段，DataNode会向NameNode上传它们数据块的列表，让 NameNode得到块的位置信息，并对每个文件对应的数据块副本进行统计。当最小副本条件满足时，即一定比例的数据块都达到最小副本数，系统就会退出安全模式，而这需要一定的延迟时间。当最小副本条件未达到要求时，就会对副本数不足的数据块安排DataNode进行复制，直至达到最小副本数。而在安全模式下，系统会处于只读状态，NameNode不会处理任何块的复制和删除命令。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;HDFS中的沟通协议&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有的HDFS中的沟通协议都是基于tcp/ip协议，一个客户端通过指定的tcp端口与NameNode机器建立连接，并通过ClientProtocol协议与NameNode交互。而DataNode则通过DataNode Protocol协议与NameNode进行沟通。HDFS的RCP(远程过程调用)对ClientProtocol和DataNode Protocol做了封装。按照HDFS的设计，NameNode不会主动发起任何请求，只会被动接受来自客户端或DataNode的请求。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;可靠性保证&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以允许DataNode失败。DataNode会定期（默认3秒）的向NameNode发送心跳，若NameNode在指定时间间隔内没有收到心跳，它就认为此节点已经失败。此时，NameNode把失败节点的数据（从另外的副本节点获取）备份到另外一个健康的节点。这保证了集群始终维持指定的副本数。&lt;/p&gt;

&lt;p&gt;可以检测到数据块损坏。在读取数据块时，HDFS会对数据块和保存的校验和文件匹配，如果发现不匹配，NameNode同样会重新备份损坏的数据块。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;hdfs文件读取过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;hdfs有一个FileSystem实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。hdfs通过rpc调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回含有该块副本的DataNode的节点地址，另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就存储在客户端所在的节点上。&lt;/p&gt;

&lt;p&gt;hdfs会返回一个FSDataInputStream对象，FSDataInputStream类转而封装成DFSDataInputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;客户端发起读请求&lt;/li&gt;
&lt;li&gt;客户端与NameNode得到文件的块及位置信息列表&lt;/li&gt;
&lt;li&gt;客户端直接和DataNode交互读取数据&lt;/li&gt;
&lt;li&gt;读取完成关闭连接&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/hdfs4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当FSDataInputStream与DataNode通信时遇到错误，它会选取另一个较近的DataNode，并为出故障的DataNode做标记以免重复向其读取数据。FSDataInputStream还会对读取的数据块进行校验和确认，发现块损坏时也会重新读取并通知NameNode。&lt;/p&gt;

&lt;p&gt;这样设计的巧妙之处：&lt;/p&gt;

&lt;p&gt;1、让客户端直接联系DataNode检索数据，可以使hdfs扩展到大量的并发客户端，因为数据流就是分散在集群的每个节点上的，在运行MapReduce任务时，每个客户端就是一个DataNode节点。&lt;/p&gt;

&lt;p&gt;2、NameNode仅需相应块的位置信息请求（位置信息在内存中，速度极快），否则随着客户端的增加，NameNode会很快成为瓶颈。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;hdfs文件写入过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;hdfs有一个DistributedFileSystem实例，客户端通过调用这个实例的create()方法就可以创建文件。DistributedFileSystem会发送给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件，在创建文件前NameNode会做一些检查，如文件是否存在，客户端是否有创建权限等，若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog，若不通过会向客户端抛出IOException。创建成功之后DistributedFileSystem会返回一个FSDataOutputStream对象，客户端由此开始写入数据。&lt;/p&gt;

&lt;p&gt;同读文件过程一样，FSDataOutputStream类转而封装成DFSDataOutputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;客户端在向NameNode请求之前先写入文件数据到本地文件系统的一个临时文件&lt;/li&gt;
&lt;li&gt;待临时文件达到块大小时开始向NameNode请求DataNode信息&lt;/li&gt;
&lt;li&gt;NameNode在文件系统中创建文件并返回给客户端一个数据块及其对应DataNode的地址列表（列表中包含副本存放的地址）&lt;/li&gt;
&lt;li&gt;客户端通过上一步得到的信息把创建临时文件块flush到列表中的第一个DataNode&lt;/li&gt;
&lt;li&gt;当文件关闭，NameNode会提交这次文件创建，此时，文件在文件系统中可见&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面第四步描述的flush过程实际处理过程比较负杂，现在单独描述一下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;首先，第一个DataNode是以数据包(数据包一般4KB)的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。&lt;/li&gt;
&lt;li&gt;在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包&lt;/li&gt;
&lt;li&gt;第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点&lt;/li&gt;
&lt;li&gt;传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK,最终，第一个DataNode会向客户端发回一个ACK&lt;/li&gt;
&lt;li&gt;当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点。然后，客户端会向NameNode发送一个确认&lt;/li&gt;
&lt;li&gt;如果管道中的任何一个DataNode失败，管道会被关闭。数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点&lt;/li&gt;
&lt;li&gt;数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在hdfs中，供读取时进行完整性校验&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;hdfs文件删除过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;hdfs文件删除过程一般需要如下几步：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;一开始删除文件，NameNode只是重命名被删除的文件到/trash目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在/trash中文件会被保留一定间隔的时间（可配置，默认是6小时），在这期间，文件可以很容易的恢复，恢复只需要将文件从/trash移出即可。&lt;/li&gt;
&lt;li&gt;当指定的时间到达，NameNode将会把文件从命名空间中删除&lt;/li&gt;
&lt;li&gt;标记删除的文件块释放空间，HDFS文件系统显示空间增加&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 常用ID生成的方法</title>
          <link>https://kingjcy.github.io/post/architecture/id/</link>
          <pubDate>Wed, 16 Oct 2019 10:04:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/id/</guid>
          <description>&lt;p&gt;几乎所有的业务系统，都有生成一个记录标识的需求，全局唯一，趋势有序是记录标识生成的两大核心需求。&lt;/p&gt;

&lt;p&gt;如何高效生成趋势有序的全局唯一ID？&lt;/p&gt;

&lt;h1 id=&#34;auto-increment&#34;&gt;auto_increment&lt;/h1&gt;

&lt;p&gt;使用数据库的 auto_increment 来生成全局唯一递增ID&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;简单，使用数据库已有的功能&lt;/li&gt;
&lt;li&gt;能够保证唯一性&lt;/li&gt;
&lt;li&gt;能够保证递增性&lt;/li&gt;
&lt;li&gt;步长固定&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可用性难以保证：数据库常见架构是一主多从+读写分离，生成自增ID是写请求，主库挂了就玩不转了&lt;/li&gt;
&lt;li&gt;扩展性差，性能有上限：因为写入是单点，数据库主库的写性能决定ID的生成性能上限，并且难以扩展&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;改进方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;增加主库，避免写入单点&lt;/li&gt;
&lt;li&gt;数据水平切分，保证各主库生成的ID不重复&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/id/id.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所述，由1个写库变成3个写库，每个写库设置不同的auto_increment初始值，以及相同的增长步长，以保证每个数据库生成的ID是不同的（上图中库0生成0,3,6,9…，库1生成1,4,7,10，库2生成2,5,8,11…）&lt;/p&gt;

&lt;p&gt;改进后的架构保证了可用性，但缺点是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;丧失了ID生成的“绝对递增性”：先访问库0生成0,3，再访问库1生成1，可能导致在非常短的时间内，ID生成不是绝对递增的（这个问题不大，我们的目标是趋势递增，不是绝对递增）&lt;/li&gt;
&lt;li&gt;数据库的写压力依然很大，每次生成ID都要访问数据库&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决上述两个问题，引出了第二个常见的方案&lt;/p&gt;

&lt;h1 id=&#34;单点批量id生成服务&#34;&gt;单点批量ID生成服务&lt;/h1&gt;

&lt;p&gt;分布式系统之所以难，很重要的原因之一是“没有一个全局时钟，难以保证绝对的时序”，要想保证绝对的时序，还是只能使用单点服务，用本地时钟保证“绝对时序”。数据库写压力大，是因为每次生成ID都访问了数据库，可以使用批量的方式降低数据库写压力。&lt;/p&gt;

&lt;p&gt;数据库使用双master保证可用性，数据库中只存储当前ID的最大值，例如0。ID生成服务假设每次批量拉取6个ID，服务访问数据库，将当前ID的最大值修改为5，这样应用访问ID生成服务索要ID，ID生成服务不需要每次访问数据库，就能依次派发0,1,2,3,4,5这些ID了，当ID发完后，再将ID的最大值修改为11，就能再次派发6,7,8,9,10,11这些ID了，于是数据库的压力就降低到原来的1/6了。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;保证了ID生成的绝对递增有序&lt;/li&gt;
&lt;li&gt;大大的降低了数据库的压力，ID生成可以做到每秒生成几万几十万个&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务仍然是单点&lt;/li&gt;
&lt;li&gt;如果服务挂了，服务重启起来之后，继续生成ID可能会不连续，中间出现空洞（服务内存是保存着0,1,2,3,4,5，数据库中max-id是5，分配到3时，服务重启了，下次会从6开始分配，4和5就成了空洞，不过这个问题也不大）&lt;/li&gt;
&lt;li&gt;虽然每秒可以生成几万几十万个ID，但毕竟还是有性能上限，无法进行水平扩展&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;改进方法：&lt;/p&gt;

&lt;p&gt;单点服务的常用高可用优化方案是“备用服务”，也叫“影子服务”，有一个影子服务时刻处于备用状态，当主服务挂了的时候影子服务顶上。这个切换的过程对调用方是透明的，可以自动完成，常用的技术是vip+keepalived，具体就不在这里展开。&lt;/p&gt;

&lt;h1 id=&#34;uuid&#34;&gt;uuid&lt;/h1&gt;

&lt;p&gt;上述方案来生成ID，虽然性能大增，但由于是单点系统，总还是存在性能上限的。同时，上述两种方案，不管是数据库还是服务来生成ID，业务方Application都需要进行一次远程调用，比较耗时。有没有一种本地生成ID的方法，即高性能，又时延低呢？&lt;/p&gt;

&lt;p&gt;uuid是一种常见的方案：string ID =GenUUID();&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;本地生成ID，不需要进行远程调用，时延低&lt;/li&gt;
&lt;li&gt;扩展性好，基本可以认为没有性能上限&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无法保证趋势递增&lt;/li&gt;
&lt;li&gt;uuid过长，往往用字符串表示，作为主键建立索引查询效率低，常见优化方案为“转化为两个uint64整数存储”或者“折半存储”（折半后不能保证唯一性）&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;取当前毫秒数&#34;&gt;取当前毫秒数&lt;/h1&gt;

&lt;p&gt;uuid是一个本地算法，生成性能高，但无法保证趋势递增，且作为字符串ID检索效率低，有没有一种能保证递增的本地算法呢？&lt;/p&gt;

&lt;p&gt;取当前毫秒数是一种常见方案：uint64 ID = GenTimeMS();&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;本地生成ID，不需要进行远程调用，时延低&lt;/li&gt;
&lt;li&gt;生成的ID趋势递增&lt;/li&gt;
&lt;li&gt;生成的ID是整数，建立索引后查询效率高&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果并发量超过1000，会生成重复的ID&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我去，这个缺点要了命了，不能保证ID的唯一性。当然，使用微秒可以降低冲突概率，但每秒最多只能生成1000000个ID，再多的话就一定会冲突了，所以使用微秒并不从根本上解决问题。&lt;/p&gt;

&lt;h1 id=&#34;类snowflake算法&#34;&gt;类snowflake算法&lt;/h1&gt;

&lt;p&gt;snowflake是twitter开源的分布式ID生成算法，其核心思想是：一个long型的ID，使用其中41bit作为毫秒数，10bit作为机器编号，12bit作为毫秒内序列号。这个算法单机每秒内理论上最多可以生成1000*(2^12)，也就是400W的ID，完全能满足业务的需求。&lt;/p&gt;

&lt;p&gt;借鉴snowflake的思想，结合各公司的业务逻辑和并发量，可以实现自己的分布式ID生成算法。&lt;/p&gt;

&lt;p&gt;举例，假设某公司ID生成器服务的需求如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;（1）单机高峰并发量小于1W，预计未来5年单机高峰并发量小于10W
（2）有2个机房，预计未来5年机房数量小于4个
（3）每个机房机器数小于100台
（4）目前有5个业务线有ID生成需求，预计未来业务线数量小于10个
（5）…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分析过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高位取从2016年1月1日到现在的毫秒数（假设系统ID生成器服务在这个时间之后上线），假设系统至少运行10年，那至少需要10年*365天*24小时*3600秒*1000毫秒=320*10^9，差不多预留39bit给毫秒数&lt;/li&gt;
&lt;li&gt;每秒的单机高峰并发量小于10W，即平均每毫秒的单机高峰并发量小于100，差不多预留7bit给每毫秒内序列号&lt;/li&gt;
&lt;li&gt;5年内机房数小于4个，预留2bit给机房标识&lt;/li&gt;
&lt;li&gt;每个机房小于100台机器，预留7bit给每个机房内的服务器标识
0 业务线小于10个，预留4bit给业务线标识&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样设计的64bit标识，可以保证：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个业务线、每个机房、每个机器生成的ID都是不同的&lt;/li&gt;
&lt;li&gt;同一个机器，每个毫秒内生成的ID都是不同的&lt;/li&gt;
&lt;li&gt;同一个机器，同一个毫秒内，以序列号区区分保证生成的ID是不同的&lt;/li&gt;
&lt;li&gt;将毫秒数放在最高位，保证生成的ID是趋势递增的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;由于“没有一个全局时钟”，每台服务器分配的ID是绝对递增的，但从全局看，生成的ID只是趋势递增的（有些服务器的时间早，有些服务器的时间晚）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后一个容易忽略的问题：&lt;/p&gt;

&lt;p&gt;生成的ID，例如message-id/ order-id/ tiezi-id，在数据量大时往往需要分库分表，这些ID经常作为取模分库分表的依据，为了分库分表后数据均匀，ID生成往往有“取模随机性”的需求，所以我们通常把每秒内的序列号放在ID的最末位，保证生成的ID是随机的。&lt;/p&gt;

&lt;p&gt;又如果，我们在跨毫秒时，序列号总是归0，会使得序列号为0的ID比较多，导致生成的ID取模后不均匀。解决方法是，序列号不是每次都归0，而是归一个0到9的随机数，这个地方。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 调用优化</title>
          <link>https://kingjcy.github.io/post/architecture/call/</link>
          <pubDate>Sat, 05 Oct 2019 10:04:53 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/call/</guid>
          <description>&lt;p&gt;第三方接口挂掉，最好是不影响我们自身服务的运行，但是我们没有办法控制第三方接口稳定，所以我们需要优化我们的调用架构。&lt;/p&gt;

&lt;h1 id=&#34;基本情况&#34;&gt;基本情况&lt;/h1&gt;

&lt;p&gt;我们以跨公网调用其他平台接口为例，很多时候，业务需要跨公网调用一个第三方服务提供的接口，为了避免每个调用方都依赖于第三方服务，往往会抽象一个服务，所以基本调用流程都是&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/call/call.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见基本流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;业务调用方调用内部service&lt;/li&gt;
&lt;li&gt;内部service跨公网调用第三方接口&lt;/li&gt;
&lt;li&gt;第三方接口返回结果给内部service&lt;/li&gt;
&lt;li&gt;内部service返回结果给业务调用方&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;内部服务可能对上游业务提供了很多服务接口，当有一个接口跨公网第三方调用超时时，可能导致所有接口都不可用，即使大部分接口不依赖于跨公网第三方调用。&lt;/p&gt;

&lt;p&gt;而且我们经常会遇到这种情况，内部服务对业务方提供的N个接口，会共用服务容器内的工作线程（假设有100个工作线程）。假设这N个接口的某个接口跨公网依赖于第三方的接口，发生了网络抖动，或者接口超时（不妨设超时时间为5秒）。&lt;/p&gt;

&lt;h1 id=&#34;解决方案&#34;&gt;解决方案&lt;/h1&gt;

&lt;p&gt;最常想到的就是，加大调用次数，降低超时时间，或者拆分业务，但是都不能重根本上解决问题。&lt;/p&gt;

&lt;h2 id=&#34;异步代理法&#34;&gt;异步代理法&lt;/h2&gt;

&lt;p&gt;增加一个代理，向服务屏蔽究竟是“本地实时”还是“异步远程”去获取返回结果&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/call/call1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;本地实时流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;业务调用方调用内部service&lt;/li&gt;
&lt;li&gt;内部service调用异步代理service&lt;/li&gt;
&lt;li&gt;异步代理service通过OpenID在本地拿取数据&lt;/li&gt;
&lt;li&gt;异步代理service将数据返回内部service&lt;/li&gt;
&lt;li&gt;内部service返回结果给业务调用方&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;异步远程流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;异步代理service定期跨公网调用微信服务&lt;/li&gt;
&lt;li&gt;微信服务返回数据&lt;/li&gt;
&lt;li&gt;刷新本地数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样公网抖动，第三方接口超时，不影响内部接口调用，只会返回的不是最新的数据，很多业务场景都是适用允许的。&lt;/p&gt;

&lt;h2 id=&#34;第三方接口备份与切换法&#34;&gt;第三方接口备份与切换法&lt;/h2&gt;

&lt;p&gt;同时使用（或者备份）多个第三方服务。这样就可以调用第一个三方接口超时后，调用第二个备份服务，未来都直接调用备份服务，直到超时的服务恢复，这样公网抖动，第三方接口超时，不影响内部接口调用（初期少数几个请求会超时），但是不是所有公网调用都能够像短息网关，电子合同服务一样有备份接口的，像微信、支付宝等就只此一家&lt;/p&gt;

&lt;h2 id=&#34;异步调用法&#34;&gt;异步调用法&lt;/h2&gt;

&lt;p&gt;本地调用成功就返回成功，异步调用第三方接口同步数据（和异步代理有微小差别），其实就是将数据拉到本地。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控trace系列---- jaeger</title>
          <link>https://kingjcy.github.io/post/monitor/trace/jaeger/</link>
          <pubDate>Tue, 13 Aug 2019 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/trace/jaeger/</guid>
          <description>&lt;p&gt;Jaeger 是 Uber 开源的分布式追踪系统，兼容 OpenTracing 标准，于 2017 年 9 月加入 CNCF 基金会。&lt;/p&gt;

&lt;h1 id=&#34;jaeger&#34;&gt;jaeger&lt;/h1&gt;

&lt;p&gt;受Dapper和OpenZipkin启发的Jaeger是由Uber Technologies作为开源发布的分布式跟踪系统。它用于监视和诊断基于微服务的分布式系统，包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;分布式上下文传播&lt;/li&gt;
&lt;li&gt;分布式交易监控&lt;/li&gt;
&lt;li&gt;根本原因分析&lt;/li&gt;
&lt;li&gt;服务依赖性分析性能/延迟优化&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;特性&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;高扩展性&lt;/p&gt;

&lt;p&gt;Jaeger后端的设计没有单点故障，可以根据业务需求进行扩展。例如，Uber上任何给定的Jaeger安装通常每天要处理数十亿个跨度。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;原生支持OpenTracing&lt;/p&gt;

&lt;p&gt;Jaeger后端，Web UI和工具库已完全设计为支持OpenTracing标准。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过跨度引用将迹线表示为有向无环图（不仅是树）&lt;/li&gt;
&lt;li&gt;支持强类型的跨度标签和结构化日志通过行李&lt;/li&gt;
&lt;li&gt;支持通用的分布式上下文传播机制&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多存储后端&lt;/p&gt;

&lt;p&gt;Jaeger支持两个流行的开源NoSQL数据库作为跟踪存储后端：Cassandra 3.4+和Elasticsearch 5.x / 6.x / 7.x。正在进行使用其他数据库的社区实验，例如ScyllaDB，InfluxDB，Amazon DynamoDB。Jaeger还附带了一个简单的内存存储区，用于测试设置。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;现代化的UI&lt;/p&gt;

&lt;p&gt;Jaeger Web UI是使用流行的开源框架（如React）以Javascript实现的。v1.0中发布了几项性能改进，以允许UI有效处理大量数据，并显示具有成千上万个跨度的跟踪（例如，我们尝试了具有80,000个跨度的跟踪）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;云原生部署&lt;/p&gt;

&lt;p&gt;Jaeger后端作为Docker映像的集合进行分发。这些二进制文件支持各种配置方法，包括命令行选项，环境变量和多种格式（yaml，toml等）的配置文件。Kubernetes模板和Helm图表有助于将其部署到Kubernetes集群。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可观察性&lt;/p&gt;

&lt;p&gt;默认情况下，所有Jaeger后端组件都公开Prometheus指标（也支持其他指标后端）。使用结构化日志库zap将日志写到标准输出。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安全&lt;/p&gt;

&lt;p&gt;Jaeger的第三方安全审核可在&lt;a href=&#34;https://github.com/jaegertracing/security-audits&#34;&gt;https://github.com/jaegertracing/security-audits&lt;/a&gt; 中获得。有关Jaeger中可用安全机制的摘要，请参见问题＃1718。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;与Zipkin的向后兼容性&lt;/p&gt;

&lt;p&gt;尽管我们建议使用OpenTracing API来对应用程序进行检测并绑定到Jaeger客户端库，以从其他地方无法获得的高级功能中受益，但是如果您的组织已经使用Zipkin库对检测进行了投资，则不必重写所有代码。Jaeger通过在HTTP上接受Zipkin格式（Thrift或JSON v1 / v2）的跨度来提供与Zipkin的向后兼容性。从Zipkin后端切换只是将流量从Zipkin库路由到Jaeger后端的问题。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装部署&#34;&gt;安装部署&lt;/h2&gt;

&lt;p&gt;开始多合一的最简单方法是使用发布到DockerHub的预构建映像（单个命令行）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.14
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or run the jaeger-all-in-one(.exe) executable from the binary distribution archives:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jaeger-all-in-one --collector.zipkin.http-port=9411
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then navigate to &lt;a href=&#34;http://localhost:16686&#34;&gt;http://localhost:16686&lt;/a&gt; to access the Jaeger UI.&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/jaeger&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;整体可以分为四个部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;jaeger-client：Jaeger 的客户端，实现了 OpenTracing 的 API，支持主流编程语言。客户端直接集成在目标 Application 中，其作用是记录和发送 Span 到 Jaeger Agent。在 Application 中调用 Jaeger Client Library 记录 Span 的过程通常被称为埋点。&lt;/li&gt;
&lt;li&gt;jaeger-agent：暂存 Jaeger Client 发来的 Span，并批量向 Jaeger Collector 发送 Span，一般每台机器上都会部署一个 Jaeger Agent。官方的介绍中还强调了 Jaeger Agent 可以将服务发现的功能从 Client 中抽离出来，不过从架构角度讲，如果是部署在 Kubernetes 或者是 Nomad 中，Jaeger Agent 存在的意义并不大。&lt;/li&gt;
&lt;li&gt;jaeger-collector：接受 Jaeger Agent 发来的数据，并将其写入存储后端，目前支持采用 Cassandra 和 Elasticsearch 作为存储后端。个人还是比较推荐用 Elasticsearch，既可以和日志服务共用同一个 ES，又可以使用 Kibana 对 Trace 数据进行额外的分析。架构图中的存储后端是 Cassandra，旁边还有一个 Spark，讲的就是可以用 Spark 等其他工具对存储后端中的 Span 进行直接分析。&lt;/li&gt;
&lt;li&gt;jaeger-query &amp;amp; jaeger-ui：读取存储后端中的数据，以直观的形式呈现。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控trace系列---- zipkin</title>
          <link>https://kingjcy.github.io/post/monitor/trace/zipkin/</link>
          <pubDate>Tue, 13 Aug 2019 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/trace/zipkin/</guid>
          <description>&lt;p&gt;zipkin是分布式链路调用监控系统，聚合各业务系统调用延迟数据，达到链路调用监控跟踪。&lt;/p&gt;

&lt;h1 id=&#34;zipkin&#34;&gt;zipkin&lt;/h1&gt;

&lt;p&gt;一个独立的分布式追踪系统，客户端存在于应用中（即各服务中），应具备追踪信息生成、采集发送等功能，而服务端应该包含以下基本的三个功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;信息收集：用来收集各服务端采集的信息，并对这些信息进行梳理存储、建立索引。&lt;/li&gt;
&lt;li&gt;数据存储：存储追踪数据。&lt;/li&gt;
&lt;li&gt;查询服务：提供查询请求链路信息的接口。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装使用&#34;&gt;安装使用&lt;/h2&gt;

&lt;p&gt;下载jar包，直接运行。简单粗暴，但要注意必须jdk1.8及以上。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget -O zipkin.jar &#39;https://search.maven.org/remote_content?g=io.zipkin.java&amp;amp;a=zipkin-server&amp;amp;v=LATEST&amp;amp;c=exec&#39;
java -jar zipkin.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动成功后，打开浏览器访问zipkin的webUI，输入&lt;a href=&#34;http://ip:9411/&#34;&gt;http://ip:9411/&lt;/a&gt; ,显示web界面。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/zipkin&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;zipkin(服务端)包含四个组件，分别是collector、storage、search、web UI。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;collector 就是信息收集器,作为一个守护进程，它会时刻等待客户端传递过来的追踪数据，对这些数据进行验证、存储以及创建查询需要的索引。&lt;/li&gt;
&lt;li&gt;storage  是存储组件。zipkin 默认直接将数据存在内存中，此外支持使用Cassandra、ElasticSearch 和 Mysql。&lt;/li&gt;
&lt;li&gt;search 是一个查询进程，它提供了简单的JSON API来供外部调用查询。&lt;/li&gt;
&lt;li&gt;web UI 是zipkin的服务端展示平台，主要调用search提供的接口，用图表将链路信息清晰地展示给开发人员。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;数据发送&#34;&gt;数据发送&lt;/h2&gt;

&lt;p&gt;一个 span 表示一次服务调用，那么追踪器必定是被服务调用发起的动作触发，生成基本信息，同时为了追踪服务提供方对其他服务的调用情况，便需要传递本次追踪链路的traceId和本次调用的span-id。服务提供方完成服务将结果响应给调用方时，需要根据调用发起时记录的时间戳与当前时间戳计算本次服务的持续时间进行记录，至此这次调用的追踪span完成，就可以发送给zipkin服务端了。但是需要注意的是，发送span给zipkin collector不得影响此次业务结果，其发送成功与否跟业务无关，因此这里需要采用异步的方式发送，防止追踪系统发送延迟与发送失败导致用户系统的延迟与中断。下图就表示了一次http请求调用的追踪流程（基于zipkin官网提供的流程图）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/zipkin1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出服务A请求服务B时先被追踪器拦截，记录tag信息、时间戳，同时将追踪标识添加进http header中传递给服务B，在服务B响应后，记录持续时间，最终采取异步的方式发送给zipkin收集器。span从被追踪的服务传送到Zipkin收集器有三种主要的传送方式：http、Kafka以及Scribe（Facebook开源的日志收集系统）。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Kafka Client</title>
          <link>https://kingjcy.github.io/post/middleware/mq/kafka-client/</link>
          <pubDate>Thu, 08 Aug 2019 15:49:11 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/kafka-client/</guid>
          <description>&lt;p&gt;Go Kafka客户端简单示例&lt;/p&gt;

&lt;h1 id=&#34;客户端&#34;&gt;客户端&lt;/h1&gt;

&lt;h2 id=&#34;生产者&#34;&gt;生产者&lt;/h2&gt;

&lt;h3 id=&#34;库&#34;&gt;库&lt;/h3&gt;

&lt;p&gt;1.sarama目前使用最多的golang的client&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/Shopify/sarama
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该库要求kafka版本在0.8及以上，支持kafka定义的high-level API和low-level API，但不支持常用的consumer自动rebalance和offset追踪，所以一般得结合cluster版本使用。&lt;/p&gt;

&lt;p&gt;2.sarama-cluster依赖库，弥补了上面了不足&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/bsm/sarama-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要kafka 0.9及以上版本&lt;/p&gt;

&lt;h3 id=&#34;生产模式&#34;&gt;生产模式&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;同步消息模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;github.com/Shopify/sarama&amp;quot;
    &amp;quot;time&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;os/signal&amp;quot;
    &amp;quot;sync&amp;quot;
)

var Address = []string{&amp;quot;10.130.138.164:9092&amp;quot;,&amp;quot;10.130.138.164:9093&amp;quot;,&amp;quot;10.130.138.164:9094&amp;quot;}

func main()  {
    syncProducer(Address)
    //asyncProducer1(Address)
}

//同步消息模式
func syncProducer(address []string)  {
    config := sarama.NewConfig()
    config.Producer.Return.Successes = true
    config.Producer.Timeout = 5 * time.Second
    p, err := sarama.NewSyncProducer(address, config)
    if err != nil {
        log.Printf(&amp;quot;sarama.NewSyncProducer err, message=%s \n&amp;quot;, err)
        return
    }
    defer p.Close()
    topic := &amp;quot;test&amp;quot;
    srcValue := &amp;quot;sync: this is a message. index=%d&amp;quot;
    for i:=0; i&amp;lt;10; i++ {
        value := fmt.Sprintf(srcValue, i)
        msg := &amp;amp;sarama.ProducerMessage{
            Topic:topic,
            Value:sarama.ByteEncoder(value),
        }
        part, offset, err := p.SendMessage(msg)
        if err != nil {
            log.Printf(&amp;quot;send message(%s) err=%s \n&amp;quot;, value, err)
        }else {
            fmt.Fprintf(os.Stdout, value + &amp;quot;发送成功，partition=%d, offset=%d \n&amp;quot;, part, offset)
        }
        time.Sleep(2*time.Second)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同步生产者发送消息，使用的不是channel，并且SendMessage方法有三个返回的值，分别为这条消息的被发送到了哪个partition，处于哪个offset，是否有error。&lt;/p&gt;

&lt;p&gt;也就是说，只有在消息成功的发送并写入了broker，才会有返回值。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;异步消息之Goroutines&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;异步消费者(Goroutines)：用不同的goroutine异步读取Successes和Errors channel&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func asyncProducer1(address []string)  {
    config := sarama.NewConfig()
    config.Producer.Return.Successes = true
    //config.Producer.Partitioner = 默认为message的hash
    p, err := sarama.NewAsyncProducer(address, config)
    if err != nil {
        log.Printf(&amp;quot;sarama.NewSyncProducer err, message=%s \n&amp;quot;, err)
        return
    }

    //Trap SIGINT to trigger a graceful shutdown.
    signals := make(chan os.Signal, 1)
    signal.Notify(signals, os.Interrupt)

    var wg sync.WaitGroup
    var enqueued, successes, errors int
    wg.Add(2) //2 goroutine

    // 发送成功message计数
    go func() {
        defer wg.Done()
        for range p.Successes() {
            successes++
        }
    }()

    // 发送失败计数
    go func() {
        defer wg.Done()
        for err := range p.Errors() {
            log.Printf(&amp;quot;%s 发送失败，err：%s\n&amp;quot;, err.Msg, err.Err)
            errors++
        }
    }()

    // 循环发送信息
    asrcValue := &amp;quot;async-goroutine: this is a message. index=%d&amp;quot;
    var i int
    Loop:
    for {
        i++
        value := fmt.Sprintf(asrcValue, i)
        msg := &amp;amp;sarama.ProducerMessage{
            Topic:&amp;quot;test&amp;quot;,
            Value:sarama.ByteEncoder(value),
        }
        select {
        case p.Input() &amp;lt;- msg: // 发送消息
            enqueued++
            fmt.Fprintln(os.Stdout, value)
        case &amp;lt;-signals: // 中断信号
            p.AsyncClose()
            break Loop
        }
        time.Sleep(2 * time.Second)
    }
    wg.Wait()

    fmt.Fprintf(os.Stdout, &amp;quot;发送数=%d，发送成功数=%d，发送失败数=%d \n&amp;quot;, enqueued, successes, errors)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;异步生产者使用channel接收（生产成功或失败）的消息，并且也通过channel来发送消息，这样做通常是性能最高的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;异步消息之Select&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;异步消费者(Select)：同一线程内，通过select同时发送消息 和 处理errors计数。该方式效率较低，如果有大量消息发送， 很容易导致success和errors的case无法执行，从而阻塞一定时间。&lt;/p&gt;

&lt;p&gt;当然可以通过设置config.Producer.Return.Successes=false;config.Producer.Return.Errors=false来解决&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func asyncProducer2(address []string)  {
    config := sarama.NewConfig()
    config.Producer.Return.Errors = true
    p, err := sarama.NewAsyncProducer(address, config)
    if err != nil {
        log.Printf(&amp;quot;sarama.NewSyncProducer err, message=%s \n&amp;quot;, err)
        return
    }

    //Trap SIGINT to trigger a graceful shutdown.
    signals := make(chan os.Signal, 1)
    signal.Notify(signals, os.Interrupt)

    var enqueued, successes, errors int
    asrcValue := &amp;quot;async-select: this is a message. index=%d&amp;quot;
    var i int
    Loop:
    for {
        i++
        value := fmt.Sprintf(asrcValue, i)
        msg := &amp;amp;sarama.ProducerMessage{
            Topic:&amp;quot;test&amp;quot;,
            Value:sarama.ByteEncoder(value),
        }
        select {
        case p.Input() &amp;lt;- msg:
            fmt.Fprintln(os.Stdout, value)
            enqueued++
        case &amp;lt;-p.Successes():
            successes++
        case err := &amp;lt;-p.Errors():
            log.Printf(&amp;quot;%s 发送失败，err：%s\n&amp;quot;, err.Msg, err.Err)
            errors++
        case &amp;lt;-signals:
            p.AsyncClose()
            break Loop
        }
        time.Sleep(2 * time.Second)
    }

    fmt.Fprintf(os.Stdout, &amp;quot;发送数=%d，发送失败数=%d \n&amp;quot;, enqueued, errors)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;注意事项&#34;&gt;注意事项&lt;/h3&gt;

&lt;p&gt;我们在来看看Shopify/sarama的producer有两种运行模式的一些注意的的地方&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;同步模式:producer把消息发给kafka之后会等待结果返回。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;config := sarama.NewConfig()
config.Producer.Return.Successes = true
client, err := sarama.NewClient([]{&amp;quot;localhost:9092&amp;quot;}, config)
if err != nil {
    log.Fatalf(&amp;quot;unable to create kafka client: %q&amp;quot;, err)
}

producer, err := sarama.NewSyncProducerFromClient(client)
if err != nil {
    log.Fatalf(&amp;quot;unable to create kafka producer: %q&amp;quot;, err)
}
defer producer.Close()

text := fmt.Sprintf(&amp;quot;message %08d&amp;quot;, i)
partition, offset , err := producer.SendMessage(&amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)})
if err != nil {
    log.Fatalf(&amp;quot;unable to produce message: %q&amp;quot;, err)
}
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意同步模式下，下面配置必须置上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config.Producer.Return.Successes = true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;否则运行报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2018/12/25 08:08:30 unable to create kafka producer: &amp;quot;kafka:
invalid configuration (Producer.Return.Successes must be true to be used in a SyncProducer)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;异步模式:producer把消息发给kafka之后不会等待结果返回。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;异步模式，顾名思义就是produce一个message之后不等待发送完成返回；这样调用者可以继续做其他的工作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config := sarama.NewConfig()
// config.Producer.Return.Successes = true
client, err := sarama.NewClient([]{&amp;quot;localhost:9092&amp;quot;}, config)
if err != nil {
    log.Fatalf(&amp;quot;unable to create kafka client: %q&amp;quot;, err)
}

producer, err := sarama.NewAsyncProducerFromClient
if err != nil {
    log.Fatalf(&amp;quot;unable to create kafka producer: %q&amp;quot;, err)
}
defer producer.Close()

text := fmt.Sprintf(&amp;quot;message %08d&amp;quot;, i)
producer.Input() &amp;lt;- &amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
// wait response
select {
        //case msg := &amp;lt;-producer.Successes():
        //    log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
        case err := &amp;lt;-producer.Errors():
            log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
        default:
            log.Println(&amp;quot;Produced message default&amp;quot;,)
}
...
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;异步模式produce一个消息后，缺省并不会报告成功状态，需要打开返回配置。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;config.Producer.Return.Successes = false
...
producer.Input() &amp;lt;- &amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
log.Printf(&amp;quot;Produced message: [%s]\n&amp;quot;,text)

// wait response
select {
    case msg := &amp;lt;-producer.Successes():
        log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
    case err := &amp;lt;-producer.Errors():
        log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;则这段代码会挂住，因为设置没有要求返回成功config.Producer.Return.Successes = false，那么在select等待的时候producer.Successes()不会返回，producer.Errors()也不会返回(假设没有错误发生)，就挂在这儿。当然可以加一个default分支绕过去，就不会挂住了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select {
    case msg := &amp;lt;-producer.Successes():
        log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
    case err := &amp;lt;-producer.Errors():
        log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
    default:
        log.Println(&amp;quot;Produced message default&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果打开了Return.Successes配置，则上述代码段等同于同步方式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config.Producer.Return.Successes = true
...
producer.Input() &amp;lt;- &amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
log.Printf(&amp;quot;Produced message: [%s]\n&amp;quot;,text)

// wait response
select {
    case msg := &amp;lt;-producer.Successes():
        log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
    case err := &amp;lt;-producer.Errors():
        log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从log可以看到，每发送一条消息，收到一条Return.Successes，类似于：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2018/12/25 08:51:51 Produced message: [message 00002537]
2018/12/25 08:51:51 Produced message successes: [message 00002537]
2018/12/25 08:51:51 Produced message: [message 00002538]
2018/12/25 08:51:51 Produced message successes: [message 00002538]
2018/12/25 08:51:51 Produced message: [message 00002539]
2018/12/25 08:51:51 Produced message successes: [message 00002539]
2018/12/25 08:51:51 Produced message: [message 00002540]
2018/12/25 08:51:51 Produced message successes: [message 00002540]
2018/12/25 08:51:51 Produced message: [message 00002541]
2018/12/25 08:51:51 Produced message successes: [message 00002541]
2018/12/25 08:51:51 Produced message: [message 00002542]
2018/12/25 08:51:51 Produced message successes: [message 00002542]
2018/12/25 08:51:51 Produced message: [message 00002543]
2018/12/25 08:51:51 Produced message successes: [message 00002543]
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就像是同步produce一样的行为了。&lt;/p&gt;

&lt;p&gt;如果打开了Return.Successes配置，而又没有producer.Successes()提取，那么Successes()这个chan消息会被写满。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config.Producer.Return.Successes = true
...
log.Printf(&amp;quot;Reade to Produced message: [%s]\n&amp;quot;,text)
producer.Input() &amp;lt;- &amp;amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
log.Printf(&amp;quot;Produced message: [%s]\n&amp;quot;,text)

// wait response
select {
    //case msg := &amp;lt;-producer.Successes():
    //    log.Printf(&amp;quot;Produced message successes: [%s]\n&amp;quot;,msg.Value)
    case err := &amp;lt;-producer.Errors():
        log.Println(&amp;quot;Produced message failure: &amp;quot;, err)
    default:
        log.Println(&amp;quot;Produced message default&amp;quot;,)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写满的结果就是不能再写入了，导致后面的Return.Successes消息丢失, 而且producer也会挂住，因为共享的buffer被占满了，大量的Return.Successes没有被消耗掉。&lt;/p&gt;

&lt;p&gt;运行一段时间后：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2018/12/25 08:58:24 Reade to Produced message: [message 00000603]
2018/12/25 08:58:24 Produced message: [message 00000603]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000604]
2018/12/25 08:58:24 Produced message: [message 00000604]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000605]
2018/12/25 08:58:24 Produced message: [message 00000605]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000606]
2018/12/25 08:58:24 Produced message: [message 00000606]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000607]
2018/12/25 08:58:24 Produced message: [message 00000607]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000608]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在produce第00000608个message的时候被挂住了，因为消息缓冲满了；这个缓冲的大小是可配的(可能是这个MaxRequestSize?)，但是不管大小是多少，如果没有去提取Success消息最终都会被占满的。&lt;/p&gt;

&lt;p&gt;结论就是说配置config.Producer.Return.Successes = true和操作&amp;lt;-producer.Successes()必须配套使用；配置成true，那么就要去读取Successes，如果配置成false，则不能去读取Successes。&lt;/p&gt;

&lt;h3 id=&#34;重要配置参数&#34;&gt;重要配置参数&lt;/h3&gt;

&lt;p&gt;1、MaxMessageBytes int&lt;/p&gt;

&lt;p&gt;这个参数影响了一条消息的最大字节数，默认是1000000。但是注意，这个参数必须要小于broker中的 message.max.bytes。&lt;/p&gt;

&lt;p&gt;2、RequiredAcks RequiredAcks&lt;/p&gt;

&lt;p&gt;这个参数影响了消息需要被多少broker写入之后才返回。取值可以是0、1、-1&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1代表了不需要等待broker确认才返回、这样最容易丢失消息但同时性能却是最好的&lt;/li&gt;
&lt;li&gt;0代表需要分区的leader确认后才返回，这是一种折中的方案，它会等待副本 Leader 响应，但不会等到 follower 的响应。一旦 Leader 挂掉消息就会丢失。但性能和消息安全性都得到了一定的保证。&lt;/li&gt;
&lt;li&gt;-1代表需要分区的所有副本确认后返回这样可以保证消息不会丢失，但同时性能和吞吐量却是最低的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、Partitioner PartitionerConstructor&lt;/p&gt;

&lt;p&gt;这个是分区器。Sarama默认提供了几种分区器，如果不指定默认使用Hash分区器。&lt;/p&gt;

&lt;p&gt;4、Retry&lt;/p&gt;

&lt;p&gt;这个参数代表了重试的次数，以及重试的时间，主要发生在一些可重试的错误中。&lt;/p&gt;

&lt;p&gt;在重试过程中需要考虑幂等操作了，比如当同时发送了2个请求，如果第一个请求发送到broker中，broker写入失败了，但是第二个请求写入成功了，那么客户端将重新发送第一个消息的请求，这个时候会造成乱序。又比如当第一个请求返回acks的时候，因为网络原因，客户端没有收到，所以客户端进行了重发，这个时候就会造成消息的重复。&lt;/p&gt;

&lt;p&gt;所以，幂等生产者就是为了保证消息发送到broker中是有序且不重复的。一共有两个参数&lt;/p&gt;

&lt;p&gt;5、MaxOpenRequests int&lt;/p&gt;

&lt;p&gt;这个参数代表了允许没有收到acks而可以同时发送的最大batch数。&lt;/p&gt;

&lt;p&gt;6、Idempotent bool&lt;/p&gt;

&lt;p&gt;用于幂等生产者，当这一项设置为true的时候，生产者将保证生产的消息一定是有序且精确一次的。&lt;/p&gt;

&lt;p&gt;7、Flush&lt;/p&gt;

&lt;p&gt;用于设置将消息打包发送，简单来讲就是每次发送消息到broker的时候，不是生产一条消息就发送一条消息，而是等消息累积到一定的程度了，再打包发送。所以里面含有两个参数。一个是多少条消息触发打包发送，一个是累计的消息大小到了多少，然后发送。&lt;/p&gt;

&lt;p&gt;8、Compression&lt;/p&gt;

&lt;p&gt;压缩数据进行发送，选择不同支持的压缩方式，也可以不压缩，压缩比较消耗资源，不压缩可以提高速度。&lt;/p&gt;

&lt;h2 id=&#34;源码解析&#34;&gt;源码解析&lt;/h2&gt;

&lt;h3 id=&#34;创建过程&#34;&gt;创建过程&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;producer, err := sarama.NewAsyncProducer([]string{&amp;quot;localhost:9092&amp;quot;}, config)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一切都从这么一行开始讲起。在这里其实就只有两个部分，先是通过地址和配置，构建一个 client 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewAsyncProducer(addrs []string, conf *Config) (AsyncProducer, error) {

  // 构建client
    client, err := NewClient(addrs, conf)
    if err != nil {
        return nil, err
    }

  // 构建AsyncProducer
    return newAsyncProducer(client)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Client的创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;先构建一个 client 结构体。然后创建完之后，刷新元数据，并且启动一个协程，在后台进行刷新。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewClient(addrs []string, conf *Config) (Client, error) {
    ...
  // 构建一个client
  client := &amp;amp;client{
        conf:                    conf,
        closer:                  make(chan none),
        closed:                  make(chan none),
        brokers:                 make(map[int32]*Broker),
        metadata:                make(map[string]map[int32]*PartitionMetadata),
        metadataTopics:          make(map[string]none),
        cachedPartitionsResults: make(map[string][maxPartitionIndex][]int32),
        coordinators:            make(map[string]int32),
    }
  // 把用户输入的broker地址作为“种子broker”增加到seedBrokers中
  // 随后客户端会根据已有的broker地址，自动刷新元数据，以获取更多的broker地址
  // 所以称之为种子
  random := rand.New(rand.NewSource(time.Now().UnixNano()))
    for _, index := range random.Perm(len(addrs)) {
        client.seedBrokers = append(client.seedBrokers, NewBroker(addrs[index]))
    }
    ...
  // 启动协程在后台刷新元数据
  go withRecover(client.backgroundMetadataUpdater)
  return client, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;元数据的更新&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;后台更新元数据的设计其实很简单，利用一个 ticker ，按时对元数据进行更新，直到 client 关闭。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) backgroundMetadataUpdater() {

  // 按照配置的时间更新元数据
  ticker := time.NewTicker(client.conf.Metadata.RefreshFrequency)
    defer ticker.Stop()

  // 循环获取channel，判断是执行更新操作还是终止
  for {
        select {
        case &amp;lt;-ticker.C:
            if err := client.refreshMetadata(); err != nil {
                Logger.Println(&amp;quot;Client background metadata update:&amp;quot;, err)
            }
        case &amp;lt;-client.closer:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们继续来看看 client.refreshMetadata() 这个方法，在这里我们设置了需要刷新元数据的主题，重试的次数，超时的时间。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) RefreshMetadata(topics ...string) error {
  deadline := time.Time{}
    if client.conf.Metadata.Timeout &amp;gt; 0 {
        deadline = time.Now().Add(client.conf.Metadata.Timeout)
    }
  // 设置参数
    return client.tryRefreshMetadata(topics, client.conf.Metadata.Retry.Max, deadline)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看tryRefreshMetadata这个方法。在这个方法中，会选取已经存在的broker，构造获取元数据的请求。在收到回应后，如果不存在任何的错误，就将这些元数据用于更新客户端。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) tryRefreshMetadata(topics []string, attemptsRemaining int, deadline time.Time) error {
  ...

  broker := client.any()
    for ; broker != nil &amp;amp;&amp;amp; !pastDeadline(0); broker = client.any() {
    ...
            req := &amp;amp;MetadataRequest{
          Topics: topics,
          // 是否允许创建不存在的主题
          AllowAutoTopicCreation: allowAutoTopicCreation
        }
    response, err := broker.GetMetadata(req)
    switch err.(type) {
        case nil:
            allKnownMetaData := len(topics) == 0
      // 对元数据进行更新
            shouldRetry, err := client.updateMetadata(response, allKnownMetaData)
            if shouldRetry {
                Logger.Println(&amp;quot;client/metadata found some partitions to be leaderless&amp;quot;)
                return retry(err)
            }
            return err
        case ...
      ...
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当客户端拿到了 response 之后，首先，先对本地保存 broker 进行更新。然后，对 topic 进行更新，以及这个 topic 下面的那些 partition 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) updateMetadata(data *MetadataResponse, allKnownMetaData bool) (retry bool, err error) {
  ...
  // 假设返回了新的broker id，那么保存这些新的broker，这意味着增加了broker、或者下线的broker重新上线了
  // 如果返回的id我们已经保存了，但是地址变化了，那么更新地址
  // 如果本地保存的一些id没有返回，说明这些broker下线了，那么删除他们
  client.updateBroker(data.Brokers)

  // 然后对topic也进行元数据的更新
  // 主要是更新topic以及topic对应的partition
  for _, topic := range data.Topics {
    ...
    // 更新每个topic以及对应的partition
    client.metadata[topic.Name] = make(map[int32]*PartitionMetadata, len(topic.Partitions))
        for _, partition := range topic.Partitions {
            client.metadata[topic.Name][partition.ID] = partition
            ...
        }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;与Broker建立连接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要是通过broker := client.any()来实现的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (client *client) any() *Broker {
    ...
    if len(client.seedBrokers) &amp;gt; 0 {
        _ = client.seedBrokers[0].Open(client.conf)
        return client.seedBrokers[0]
    }

    // 不保证一定是按顺序的
    for _, broker := range client.brokers {
        _ = broker.Open(client.conf)
        return broker
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open方法异步的建立了一个tcp连接，然后创建了一个缓冲大小为MaxOpenRequests的channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) Open(conf *Config) error {
  if conf == nil {
        conf = NewConfig()
    }
  ...
  go withRecover(func() {
    ...
    dialer := conf.getDialer()
        b.conn, b.connErr = dialer.Dial(&amp;quot;tcp&amp;quot;, b.addr)

    ...
    b.responses = make(chan responsePromise, b.conf.Net.MaxOpenRequests-1)
    ...
    go withRecover(b.responseReceiver)
  })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个名为 responses 的 channel ，用于接收从 broker发送回来的消息。然后，又启动了一个协程，用于接收消息。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;从Broker接收响应&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当 broker 收到一个 response 的时候，先解析消息的头部，然后再解析消息的内容。并把这些内容写进 response 的 packets 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) responseReceiver() {
  for response := range b.responses {

    ...
    // 先根据Header的版本读取对应长度的Header
    var headerLength = getHeaderLength(response.headerVersion)
        header := make([]byte, headerLength)
        bytesReadHeader, err := b.readFull(header)
    decodedHeader := responseHeader{}
        err = versionedDecode(header, &amp;amp;decodedHeader, response.headerVersion)

    ...
    // 解析具体的内容
    buf := make([]byte, decodedHeader.length-int32(headerLength)+4)
        bytesReadBody, err := b.readFull(buf)

    // 省略了一些错误处理，总之，如果发生了错误，就把错误信息写进 response.errors 中
    response.packets &amp;lt;- buf
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;发送与接受消息&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们回到这一行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;response, err := broker.GetMetadata(req)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们直接进去，之前看的是元数据的处理，其实也可以用于发送接收消息。发现在这里构造了一个接受返回信息的结构体，然后调用了sendAndReceive方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) GetMetadata(request *MetadataRequest) (*MetadataResponse, error) {
    response := new(MetadataResponse)

    err := b.sendAndReceive(request, response)

    if err != nil {
        return nil, err
    }

    return response, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这里我们可以看到，先是调用了send方法，然后返回了一个promise。并且当有消息写入这个promise的时候，就得到了结果。&lt;/p&gt;

&lt;p&gt;而且回想一下我们在receiver中，是不是把获取到的 response 写进了 packets ，把错误结果写进了 errors 呢，跟这里是一致的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) sendAndReceive(req protocolBody, res protocolBody) error {
    responseHeaderVersion := int16(-1)
    if res != nil {
        responseHeaderVersion = res.headerVersion()
    }

    promise, err := b.send(req, res != nil, responseHeaderVersion)
    if err != nil {
        return err
    }

    if promise == nil {
        return nil
    }

  // 这里的promise，是上面send方法返回的
    select {
    case buf := &amp;lt;-promise.packets:
        return versionedDecode(buf, res, req.version())
    case err = &amp;lt;-promise.errors:
        return err
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在send方法中，把需要发送的消息通过与broker的tcp连接，同步发送到broker中。&lt;/p&gt;

&lt;p&gt;然后构建了一个responsePromise类型的channel，然后直接将这个结构体丢进这个channel中。然后回想一下，我们在responseReceiver这个方法中，不断消费接收到的response。&lt;/p&gt;

&lt;p&gt;此时在responseReceiver中，收到了send方法传递的responsePromise，他就会通过conn来读取数据，然后将数据写入这个responsePromise的packets中，或者将错误信息写入errors中。&lt;/p&gt;

&lt;p&gt;而此时，再看看send方法，他返回了这个responsePromise的指针。所以，sendAndReceive方法就在等待这个responsePromise内的packets或者errors的channel被写入数据。当responseReceiver接收到了响应并且写入数据的时候，packets或者errors就会被写入消息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) send(rb protocolBody, promiseResponse bool, responseHeaderVersion int16) (*responsePromise, error) {

  ...
  // 将请求的内容封装进 request ，然后发送到Broker中
  // 注意一下这里的 b.write(buf)
  // 里面做了 b.conn.Write(buf) 这件事情
  req := &amp;amp;request{correlationID: b.correlationID, clientID: b.conf.ClientID, body: rb}
    buf, err := encode(req, b.conf.MetricRegistry)
  bytes, err := b.write(buf)

  ...
  // 如果我们的response为nil，也就是说当不需要response的时候，是不会放进inflight发送队列的
  if !promiseResponse {
        // Record request latency without the response
        b.updateRequestLatencyAndInFlightMetrics(time.Since(requestTime))
        return nil, nil
    }

  // 构建一个接收响应的 channel ，返回这个channel的指针
  // 这个 channel 内部包含了两个 channel，一个用来接收响应，一个用来接收错误
  promise := responsePromise{requestTime, req.correlationID, responseHeaderVersion, make(chan []byte), make(chan error)}
    b.responses &amp;lt;- promise

  // 这里返回指针特别的关键，是把消息的发送跟消息的接收联系在一起了
    return &amp;amp;promise, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我们来用一张图说明一下上面这个发送跟接收的过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/kafka/sarama.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;asyncprocuder&#34;&gt;AsyncProcuder&lt;/h3&gt;

&lt;p&gt;AsyncProcuder是如何发送消息的。我们从newAsyncProducer(client)这一行开始讲起。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newAsyncProducer(client Client) (AsyncProducer, error) {
  ...
  p := &amp;amp;asyncProducer{
        client:     client,
        conf:       client.Config(),
        errors:     make(chan *ProducerError),
        input:      make(chan *ProducerMessage),
        successes:  make(chan *ProducerMessage),
        retries:    make(chan *ProducerMessage),
        brokers:    make(map[*Broker]*brokerProducer),
        brokerRefs: make(map[*brokerProducer]int),
        txnmgr:     txnmgr,
    }

  go withRecover(p.dispatcher)
    go withRecover(p.retryHandler)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先是构建了asyncProducer结构体，然后协程启动的go withRecover(p.dispatcher)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) dispatcher() {
  handlers := make(map[string]chan&amp;lt;- *ProducerMessage)
  ...
  for msg := range p.input {
    ...
    // 拦截器
    for _, interceptor := range p.conf.Producer.Interceptors {
            msg.safelyApplyInterceptor(interceptor)
        }

    ...
    // 找到这个Topic对应的Handler
    handler := handlers[msg.Topic]
        if handler == nil {
      // 如果此时还不存在这个Topic对应的Handler，那么创建一个
      // 虽然说他叫Handler，但他其实是一个无缓冲的
            handler = p.newTopicProducer(msg.Topic)
            handlers[msg.Topic] = handler
        }
        // 然后把这条消息写进这个Handler中
        handler &amp;lt;- msg
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个方法中，首先创建了一个以Topic为key的map，这个map的value是无缓冲的channel。&lt;/p&gt;

&lt;p&gt;到这里我们很容易可以推测得出，当通过input发送一条消息的时候，消息会到dispatcher这里，被分配到各个Topic中。&lt;/p&gt;

&lt;p&gt;然后让我们来handler = p.newTopicProducer(msg.Topic)这一行的代码。&lt;/p&gt;

&lt;p&gt;在这里创建了一个缓冲大小为ChannelBufferSize的channel，用于存放发送到这个主题的消息。&lt;/p&gt;

&lt;p&gt;然后创建了一个topicProducer，在这个时候你可以认为消息已经交付给各个topic的topicProducer了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) newTopicProducer(topic string) chan&amp;lt;- *ProducerMessage {
    input := make(chan *ProducerMessage, p.conf.ChannelBufferSize)
    tp := &amp;amp;topicProducer{
        parent:      p,
        topic:       topic,
        input:       input,
        breaker:     breaker.New(3, 1, 10*time.Second),
        handlers:    make(map[int32]chan&amp;lt;- *ProducerMessage),
        partitioner: p.conf.Producer.Partitioner(topic),
    }
    go withRecover(tp.dispatch)
    return input
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们来看看go withRecover(tp.dispatch)这一行代码。同样是启动了一个协程，来处理消息。&lt;/p&gt;

&lt;p&gt;也就是说，到了这一步，对于每一个Topic，都有一个协程来处理消息。&lt;/p&gt;

&lt;p&gt;在这个dispatch()方法中，也同样的接收到一条消息，就会去找这条消息所在的分区的channel，然后把消息写进去。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (tp *topicProducer) dispatch() {
  for msg := range tp.input {
    ...

    // 同样是找到这条消息所在的分区对应的channel，然后把消息丢进去
    handler := tp.handlers[msg.Partition]
        if handler == nil {
            handler = tp.parent.newPartitionProducer(msg.Topic, msg.Partition)
            tp.handlers[msg.Partition] = handler
        }

        handler &amp;lt;- msg
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们进tp.parent.newPartitionProducer(msg.Topic, msg.Partition)这里看看。&lt;/p&gt;

&lt;p&gt;你可以发现partitionProducer跟topicProducer是很像的。&lt;/p&gt;

&lt;p&gt;其实他们就是代表了一条消息的分发，从producer到topic到partition。&lt;/p&gt;

&lt;p&gt;注意，这里面的channel缓冲大小，也是ChannelBufferSize。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) newPartitionProducer(topic string, partition int32) chan&amp;lt;- *ProducerMessage {
    input := make(chan *ProducerMessage, p.conf.ChannelBufferSize)
    pp := &amp;amp;partitionProducer{
        parent:    p,
        topic:     topic,
        partition: partition,
        input:     input,

        breaker:    breaker.New(3, 1, 10*time.Second),
        retryState: make([]partitionRetryState, p.conf.Producer.Retry.Max+1),
    }
    go withRecover(pp.dispatch)
    return input
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到了这一步，我们再来看看消息到了每个partition所在的channel，是如何处理的。&lt;/p&gt;

&lt;p&gt;其实在这一步中，主要是做一些错误处理之类的，然后把消息丢进brokerProducer。&lt;/p&gt;

&lt;p&gt;可以理解为这一步是业务逻辑层到网络IO层的转变，在这之前我们只关心消息去到了哪个分区，而在这之后，我们需要找到这个分区所在的broker的地址，并使用之前已经建立好的TCP连接，发送这条消息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pp *partitionProducer) dispatch() {

  // 找到这个主题和分区的leader所在的broker
  pp.leader, _ = pp.parent.client.Leader(pp.topic, pp.partition)
  // 如果此时找到了这个leader
  if pp.leader != nil {
        pp.brokerProducer = pp.parent.getBrokerProducer(pp.leader)
        pp.parent.inFlight.Add(1)
    // 发送一条消息来表示同步
        pp.brokerProducer.input &amp;lt;- &amp;amp;ProducerMessage{Topic: pp.topic, Partition: pp.partition, flags: syn}
    }
  ...// 各种异常情况

  // 然后把消息丢进brokerProducer中
  pp.brokerProducer.input &amp;lt;- msg
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到了这里，大概算是整个发送流程最后的一个步骤了。&lt;/p&gt;

&lt;p&gt;我们来看看pp.parent.getBrokerProducer(pp.leader)这行代码里面的内容。&lt;/p&gt;

&lt;p&gt;其实就是找到asyncProducer中的brokerProducer，如果不存在，则创建一个。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) getBrokerProducer(broker *Broker) *brokerProducer {
    p.brokerLock.Lock()
    defer p.brokerLock.Unlock()

    bp := p.brokers[broker]

    if bp == nil {
        bp = p.newBrokerProducer(broker)
        p.brokers[broker] = bp
        p.brokerRefs[bp] = 0
    }

    p.brokerRefs[bp]++

    return bp
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那我们就来看看brokerProducer是怎么创建出来的。&lt;/p&gt;

&lt;p&gt;看这个方法中启动的第二个协程，我们可以推测bridge这个channel收到消息后，会把收到的消息打包成一个request，然后调用Produce方法。&lt;/p&gt;

&lt;p&gt;并且，将返回的结果的指针地址，写进response中。&lt;/p&gt;

&lt;p&gt;然后构造好brokerProducerResponse，并且写入responses中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *asyncProducer) newBrokerProducer(broker *Broker) *brokerProducer {
    var (
        input     = make(chan *ProducerMessage)
        bridge    = make(chan *produceSet)
        responses = make(chan *brokerProducerResponse)
    )

    bp := &amp;amp;brokerProducer{
        parent:         p,
        broker:         broker,
        input:          input,
        output:         bridge,
        responses:      responses,
        stopchan:       make(chan struct{}),
        buffer:         newProduceSet(p),
        currentRetries: make(map[string]map[int32]error),
    }
    go withRecover(bp.run)

    // minimal bridge to make the network response `select`able
    go withRecover(func() {
        for set := range bridge {
            request := set.buildRequest()

            response, err := broker.Produce(request)

            responses &amp;lt;- &amp;amp;brokerProducerResponse{
                set: set,
                err: err,
                res: response,
            }
        }
        close(responses)
    })

    if p.conf.Producer.Retry.Max &amp;lt;= 0 {
        bp.abandoned = make(chan struct{})
    }

    return bp
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我们再来看看broker.Produce(request)这一行代码。&lt;/p&gt;

&lt;p&gt;是不是很熟悉呢，我们在client部分讲到的sendAndReceive方法。&lt;/p&gt;

&lt;p&gt;而且我们可以发现，如果我们设置了需要Acks，就会返回一个response；如果没设置，那么消息发出去之后，就不管了。&lt;/p&gt;

&lt;p&gt;此时在获取了response，并且填入了response的内容后，返回这个response的内容。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) Produce(request *ProduceRequest) (*ProduceResponse, error) {
    var (
        response *ProduceResponse
        err      error
    )

    if request.RequiredAcks == NoResponse {
        err = b.sendAndReceive(request, nil)
    } else {
        response = new(ProduceResponse)
        err = b.sendAndReceive(request, response)
    }

    if err != nil {
        return nil, err
    }

    return response, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，Sarama生产者相关的内容就介绍完毕了。&lt;/p&gt;

&lt;h3 id=&#34;syncproducer-和asyncproducer的关系&#34;&gt;syncProducer 和asyncProducer的关系&lt;/h3&gt;

&lt;p&gt;syncProducer 是所有功能都是由asyncProducer实现的，而syncProducer 之所以可以同步发送消息，答案就在SendMessage 函数中，源码如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; func(sp *syncProducer)SendMessage(msg *ProducerMessage) (partitionint32,offsetint64,errerror) {

   expectation :=make(chan*ProducerError,1)

   msg.expectation = expectation

   sp.producer.Input() &amp;lt;- msg

   if err := &amp;lt;-expectation;err != nil {    // 阻塞等待返回结果

        return-1,-1,err.Err

    }

   return msg.Partition,msg.Offset,nil

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而使用asyncProducer 时，只需要 直接将信息producer.Input()&amp;lt;-&amp;amp;ProducerMessage{} 放入进producer.Input(), 然后异步读取返回结果 chan*ProducerError&lt;/p&gt;

&lt;p&gt;消息传递过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; // one per topic

// partitions messages, then dispatches them by partition

type topicProducer struct{

    parent *asyncProducer

    topic string

    input &amp;lt;-chan*ProducerMessage

    breaker *breaker.Breaker

    handlers map[int32] chan&amp;lt;- *ProducerMessage

    partitioner Partitioner

}



type brokerProducer struct{

    parent *asyncProducer

    broker *Broker

    input  &amp;lt;-chan*ProducerMessage

    output chan&amp;lt;- *produceSet

    responses  &amp;lt;-chan*brokerProducerResponse

    buffer *produceSet

    timer  &amp;lt;-chantime.Time

    timerFired bool

    closing error

    currentRetries map[string]map[int32]error

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由代码可以看出topicProducer，partitionProducer，brokerProducer的parent都是asyncProducer&lt;/p&gt;

&lt;p&gt;消息传递过程：&lt;/p&gt;

&lt;p&gt;asyncProducer.dispatcher -&amp;gt;topicProducer.dispath -&amp;gt; partitionProducer.dispatch -&amp;gt; brokerProducer -&amp;gt;produceSet&lt;/p&gt;

&lt;p&gt;其中produceSet 对消息进行聚集，若配置了压缩的参数，则会压缩一个set中的所有的msg, 即批量压缩， 然后构建一个ProduceRequest ,然后由 broker.Produce 将请求发送出去，其中 broker 结构体代表一个kafka broker 的连接&lt;/p&gt;

&lt;p&gt;partitionProducer 会选择leader broker地址 ,若选择失败，则会重新选择leader broker ，然后由这个连接发送消息根据kafka版本不同，消息会放入到不同的结构体中若版本大于V0.11，set.recordsToSend.RecordBatch.addRecord(rec) 将一个rec添加进去，否则将set.recordsToSend.MsgSet.addMessage(msgToSend)
 &lt;/p&gt;

&lt;p&gt;在生成一个newBrokerProducer时，broker会开启消费output, 而output就是一个存放produceSet的channel,阻塞等待刷新ProduceRequest  并将其发送出去&lt;/p&gt;

&lt;h2 id=&#34;消费者&#34;&gt;消费者&lt;/h2&gt;

&lt;p&gt;消费者集群模实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main()  {
    topic := []string{&amp;quot;test&amp;quot;}
    var wg = &amp;amp;sync.WaitGroup{}
    wg.Add(2)
    //广播式消费：消费者1
    go clusterConsumer(wg, Address, topic, &amp;quot;group-1&amp;quot;)
    //广播式消费：消费者2
    go clusterConsumer(wg, Address, topic, &amp;quot;group-2&amp;quot;)

    wg.Wait()
}

// 支持brokers cluster的消费者
func clusterConsumer(wg *sync.WaitGroup,brokers, topics []string, groupId string)  {
    defer wg.Done()
    config := cluster.NewConfig()
    config.Consumer.Return.Errors = true
    config.Group.Return.Notifications = true
    config.Consumer.Offsets.Initial = sarama.OffsetNewest

    // init consumer
    consumer, err := cluster.NewConsumer(brokers, groupId, topics, config)
    if err != nil {
        log.Printf(&amp;quot;%s: sarama.NewSyncProducer err, message=%s \n&amp;quot;, groupId, err)
        return
    }
    defer consumer.Close()

    // trap SIGINT to trigger a shutdown
    signals := make(chan os.Signal, 1)
    signal.Notify(signals, os.Interrupt)

    // consume errors
    go func() {
        for err := range consumer.Errors() {
            log.Printf(&amp;quot;%s:Error: %s\n&amp;quot;, groupId, err.Error())
        }
    }()

    // consume notifications
    go func() {
        for ntf := range consumer.Notifications() {
            log.Printf(&amp;quot;%s:Rebalanced: %+v \n&amp;quot;, groupId, ntf)
        }
    }()

    // consume messages, watch signals
    var successes int
    Loop:
    for {
        select {
        case msg, ok := &amp;lt;-consumer.Messages():
            if ok {
                fmt.Fprintf(os.Stdout, &amp;quot;%s:%s/%d/%d\t%s\t%s\n&amp;quot;, groupId, msg.Topic, msg.Partition, msg.Offset, msg.Key, msg.Value)
                consumer.MarkOffset(msg, &amp;quot;&amp;quot;)  // mark message as processed
                successes++
            }
        case &amp;lt;-signals:
            break Loop
        }
    }
    fmt.Fprintf(os.Stdout, &amp;quot;%s consume %d messages \n&amp;quot;, groupId, successes)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;如何优雅的使用-kafka-生产者&#34;&gt;如何优雅的使用 Kafka 生产者&lt;/h1&gt;

&lt;h2 id=&#34;发送流程&#34;&gt;发送流程&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;初始化以及真正发送消息的 kafka-producer-network-thread IO 线程。&lt;/li&gt;
&lt;li&gt;将消息序列化。&lt;/li&gt;
&lt;li&gt;得到需要发送的分区。&lt;/li&gt;
&lt;li&gt;写入内部的一个缓存区中。&lt;/li&gt;
&lt;li&gt;初始化的 IO 线程不断的消费这个缓存来发送消息。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;分区策略&#34;&gt;分区策略&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;指定分区&lt;/li&gt;
&lt;li&gt;自定义路由&lt;/li&gt;
&lt;li&gt;默认策略，通常都是使用这种策略，其实就是一种对分区的轮询简单的来说分为以下几步：

&lt;ul&gt;
&lt;li&gt;获取 Topic 分区数。&lt;/li&gt;
&lt;li&gt;将内部维护的一个线程安全计数器 +1。&lt;/li&gt;
&lt;li&gt;与分区数取模得到分区编号。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其实这就是很典型的轮询算法，所以只要分区数不频繁变动这种方式也会比较均匀。&lt;/p&gt;

&lt;h1 id=&#34;性能&#34;&gt;性能&lt;/h1&gt;

&lt;p&gt;生产速度&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;12c32G1000M--3台

单个生产者线程，单副本--821557个记录/秒（78.3 MB /秒）
单个生产者线程，三个副本，异步方式--786980 record / sec（75.1 MB / sec）
单个生产者线程，3个副本，同步复制----428823条记录/秒（40.2 MB /秒）
三个生产者，3个副本，异步复制----2024032个记录/秒（193.0 MB /秒）
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;问题&#34;&gt;问题&lt;/h1&gt;

&lt;h2 id=&#34;golang连接kafka-sarama-内存暴涨问题记录&#34;&gt;golang连接kafka(sarama)内存暴涨问题记录&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;问题背景&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用kafka客户端类库(sarama）步发布消息， qps为100+， 上线后内存，cpu爆炸。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;排查过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;首先排查代码层面写的有逻辑bug， 比如连接未close， 排查无问题&lt;/li&gt;
&lt;li&gt;排查发布的消息较大， 导致golang频繁gc, 和同事确认，无频繁gc&lt;/li&gt;
&lt;li&gt;通过查看源码，发现每次http请求，操作kafka都是短链接， 即频繁的会新建短链接， 排查到这里，还是不能特别确认是因为短链接导致， 因为之前接入rabbitmq类库，也是使用的短链接。&lt;/li&gt;
&lt;li&gt;使用pprof打印出火焰图, profile, 和block的， 也没发现特别大的bug点。&lt;/li&gt;
&lt;li&gt;使用sarama meory搜索官方issue, 和谷歌查询。 得到出具体结论&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;分析具体问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从官方issue 得知， sarama类库自动依赖第三方的统计类库go-mertic, 主要是为了方便给prometheus统计数据。 sarama类库默认打开。导致该统计站的内存，迟迟未释放&lt;/p&gt;

&lt;p&gt;因此使用sarama前， 将该统计关闭即可。&lt;/p&gt;

&lt;p&gt;对应代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metrics.UseNilMetrics = true
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Kafka</title>
          <link>https://kingjcy.github.io/post/middleware/mq/kafka/</link>
          <pubDate>Fri, 19 Jul 2019 20:21:50 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/kafka/</guid>
          <description>&lt;p&gt;Apache Kafka由著名职业社交公司LinkedIn开发，最初是被设计用来解决LinkedIn公司内部海量日志传输等问题。Kafka使用Scala语言编写，于2011年开源并进入Apache孵化器，2012年10月正式毕业，现在为Apache顶级项目。Kafka是一个分布式数据流平台，具有高吞吐、低延迟、高容错等特点。&lt;/p&gt;

&lt;h1 id=&#34;kafka简介&#34;&gt;Kafka简介&lt;/h1&gt;

&lt;h2 id=&#34;基本术语&#34;&gt;基本术语&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;消息message&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;先不管其他的，我们使用Kafka这个消息系统肯定是先关注消息这个概念，在Kafka中，每一个消息由键、值和一个时间戳组成（key、value和timestamp）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;主题topic&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kafka集群存储同一类别的消息流称为主题&lt;/p&gt;

&lt;p&gt;主题会有多个订阅者（0个1个或多个），当主题发布消息时，会向订阅者推送记录&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;日志 offset&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;针对每一个主题，Kafka集群维护了一个像下面这样的分区日志offset：&lt;/p&gt;

&lt;p&gt;这些分区位offset于不同的服务器上，每一个分区可以看做是一个结构化的提交日志offset，每写入一条记录都会记录到其中一个分区并且分配一个唯一地标识其位置的数字称为偏移量offset&lt;/p&gt;

&lt;p&gt;Kafka集群会将发布的消息保存一段时间，不管是否被消费。例如，如果设置保存天数为2天，那么从消息发布起的两天之内，该消息一直可以被消费，但是超过两天后就会被丢弃以节省空间。其次，Kafka的数据持久化性能很好，所以长时间存储数据不是问题&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;分区 Partition&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每个topic可以有一个或多个partition（分区）。分区是在物理层面上的，不同的分区对应着不同的数据文件。Kafka使用分区支持物理上的并发写入和读取，从而大大提高了吞吐量&lt;/p&gt;

&lt;p&gt;Kafka中采用分区的设计有几个目的。一是可以处理更多的消息，不受单台服务器的限制。Topic拥有多个分区意味着它可以不受限的处理更多的数据。第二，分区可以作为并行处理的单元，稍后会谈到这一点&lt;/p&gt;

&lt;p&gt;Log的分区被分布到集群中的多个服务器上。每个服务器处理它分到的分区。根据配置每个分区还可以复制到其它服务器作为备份容错。 每个分区有一个leader，零或多个follower。Leader处理此分区的所有的读写请求，而follower被动的复制数据。如果leader宕机，其它的一个follower会被推举为新的leader。 一台服务器可能同时是一个分区的leader，另一个分区的follower。 这样可以平衡负载，避免所有的请求都只让一台或者某几台服务器处理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;生产者 producer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;生产者往某个Topic上发布消息。生产者还可以选择将消息分配到Topic的哪个节点上。最简单的方式是轮询分配到各个分区以平衡负载，也可以根据某种算法依照权重选择分区&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;消费者 consumer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kafka有一个消费者组的概念，生产者把消息发到的是消费者组，在消费者组里面可以有很多个消费者实例。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;消费者组 Consumer Group&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一个消费者组可以包含一个或多个消费者。使用多分区+多消费者方式可以极大提高数据下游的处理速度。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;节点 Broker&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;消息队列中常用的概念，在Kafka中指部署了Kafka实例的服务器节点。&lt;/p&gt;

&lt;h1 id=&#34;安装部署&#34;&gt;安装部署&lt;/h1&gt;

&lt;h2 id=&#34;单机&#34;&gt;单机&lt;/h2&gt;

&lt;p&gt;1、安装jdk&lt;/p&gt;

&lt;p&gt;以oracle jdk为例，下载地址&lt;a href=&#34;http://java.sun.com/javase/downloads/index.jsp&#34;&gt;http://java.sun.com/javase/downloads/index.jsp&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum -y install jdk-8u141-linux-x64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、下载解压&lt;/p&gt;

&lt;p&gt;下载地址：&lt;a href=&#34;http://kafka.apache.org/downloads，如0.10.1.0版本的Kafka下载&#34;&gt;http://kafka.apache.org/downloads，如0.10.1.0版本的Kafka下载&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://apache.fayea.com/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz
tar -xvf kafka_2.11-0.10.1.0.tgz
cd kafka_2.11-0.10.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# grep -Ev &amp;quot;^#|^$&amp;quot; /data/kafka/config/server.properties
broker.id=0
delete.topic.enable=true
listeners=PLAINTEXT://192.168.15.131:9092
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/data/kafka/data
num.partitions=1
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.flush.interval.messages=10000
log.flush.interval.ms=1000
log.retention.hours=168
log.retention.bytes=1073741824
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
zookeeper.connect=192.168.15.131:2181,192.168.15.132:2181,192.168.15.133:2181
zookeeper.connection.timeout.ms=6000
group.initial.rebalance.delay.ms=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;提示：其他主机将该机器的kafka目录拷贝即可，然后需要修改broker.id、listeners地址。有关kafka配置文件参数，参考：&lt;a href=&#34;http://orchome.com/12；&#34;&gt;http://orchome.com/12；&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、安装Zookeeper&lt;/p&gt;

&lt;p&gt;Kafka需要Zookeeper的监控，所以先要安装Zookeeper。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://apache.forsale.plus/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz
tar zxf zookeeper-3.4.9.tar.gz
mv zookeeper-3.4.9 /data/zk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改配置文件内容如下所示:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# cat /data/zk/conf/zoo.cfg
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/data/zk/data/zookeeper
dataLogDir=/data/zk/data/logs
clientPort=2181
maxClientCnxns=60
autopurge.snapRetainCount=3
autopurge.purgeInterval=1

server.1=zk01:2888:3888
server.2=zk02:2888:3888
server.3=zk03:2888:3888
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;server.id=host:port:port:表示了不同的zookeeper服务器的自身标识，作为集群的一部分，每一台服务器应该知道其他服务器的信息。用户可以从“server.id=host:port:port” 中读取到相关信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在服务器的data(dataDir参数所指定的目录)下创建一个文件名为myid的文件，这个文件的内容只有一行，指定的是自身的id值。比如，服务器“1”应该在myid文件中写入“1”。这个id必须在集群环境中服务器标识中是唯一的，且大小在1～255之间。这一样配置中，zoo1代表第一台服务器的IP地址。第一个端口号（port）是从follower连接到leader机器的
端口，第二个端口是用来进行leader选举时所用的端口。所以，在集群配置过程中有三个非常重要的端口：clientPort：2181、port:2888、port:3888。&lt;/p&gt;

&lt;p&gt;如果想更换日志输出位置，除了在zoo.cfg加入&amp;rdquo;dataLogDir=/data/zk/data/logs&amp;rdquo;外，还需要修改zkServer.sh文件，大概修改方式地方在125行左右，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;125 ZOO_LOG_DIR=&amp;quot;$($GREP &amp;quot;^[[:space:]]*dataLogDir&amp;quot; &amp;quot;$ZOOCFG&amp;quot; | sed -e &#39;s/.*=//&#39;)&amp;quot;
126 if [ ! -w &amp;quot;$ZOO_LOG_DIR&amp;quot; ] ; then
127 mkdir -p &amp;quot;$ZOO_LOG_DIR&amp;quot;
128 fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在启动服务之前，还需要分别在zookeeper创建myid，方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo 1 &amp;gt;  /data/zk/data/zookeeper/myid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/data/zk/bin/zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证服务&lt;/p&gt;

&lt;p&gt;查看相关端口号&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost ~]# ss -lnpt|grep java
LISTEN     0      50          :::34442                   :::*                   users:((&amp;quot;java&amp;quot;,pid=2984,fd=18))
LISTEN     0      50       ::ffff:192.168.15.133:3888                    :::*                   users:((&amp;quot;java&amp;quot;,pid=2984,fd=26))
LISTEN     0      50          :::2181                    :::*                   users:((&amp;quot;java&amp;quot;,pid=2984,fd=25))


###查看zookeeper服务状态
[root@localhost ~]# /data/zk/bin/zkServer.sh status

ZooKeeper JMX enabled by default

Using config: /data/zk/bin/../conf/zoo.cfgMode: follower
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、Kafka目录介绍&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/bin 操作kafka的可执行脚本，还包含windows下脚本&lt;/li&gt;
&lt;li&gt;/config 配置文件所在目录&lt;/li&gt;
&lt;li&gt;/libs 依赖库目录&lt;/li&gt;
&lt;li&gt;/logs 日志数据目录，目录kafka把server端日志分为5种类型，分为:server,request,state，log-cleaner，controller&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5、启动Kafka&lt;/p&gt;

&lt;p&gt;在每台服务器上进入Kafka目录，分别执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-server-start.sh config/server.properties &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检测2181与9092端口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netstat -tunlp|egrep &amp;quot;(2181|9092)&amp;quot;
tcp        0      0 :::2181                     :::*                        LISTEN      19787/java          
tcp        0      0 :::9092                     :::*                        LISTEN      28094/java 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;p&gt;Kafka的进程ID为28094，占用端口为9092&lt;/p&gt;

&lt;p&gt;QuorumPeerMain为对应的zookeeper实例，进程ID为19787，在2181端口监听&lt;/p&gt;

&lt;p&gt;6、单机连通性测试&lt;/p&gt;

&lt;p&gt;启动2个XSHELL客户端，一个用于生产者发送消息，一个用于消费者接受消息。&lt;/p&gt;

&lt;p&gt;运行producer，随机敲入几个字符，相当于把这个敲入的字符消息发送给队列。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --broker-list 192.168.153.118:9092 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：早版本的Kafka，–broker-list 192.168.1.181:9092需改为–zookeeper 192.168.1.181:2181&lt;/p&gt;

&lt;p&gt;运行consumer，可以看到刚才发送的消息列表。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --zookeeper 192.168.153.118:2181 --topic test --from-beginning  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;p&gt;producer，指定的Socket(192.168.1.181+9092),说明生产者的消息要发往kafka，也即是broker&lt;/p&gt;

&lt;p&gt;consumer, 指定的Socket(192.168.1.181+2181),说明消费者的消息来自zookeeper（协调转发）&lt;/p&gt;

&lt;p&gt;上面的只是一个单个的broker，下面我们来实验一个多broker的集群。&lt;/p&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;搭建一个多个broker的伪集群，刚才只是启动了单个broker，现在启动有3个broker组成的集群，这些broker节点也都是在本机上。&lt;/p&gt;

&lt;p&gt;1、为每一个broker提供配置文件&lt;/p&gt;

&lt;p&gt;然后修改各个服务器的配置文件：进入Kafka的config目录，修改server.properties&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# brokerid就是指各台服务器对应的id，所以各台服务器值不同
broker.id=0
# 端口号，无需改变
port=9092
# 当前服务器的IP，各台服务器值不同
host.name=192.168.0.10
# Zookeeper集群的ip和端口号
zookeeper.connect=192.168.0.10:2181,192.168.0.11:2181,192.168.0.12:2181
# 日志目录
log.dirs=/home/www/kafka-logs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们先看看config/server0.properties配置信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;broker.id=0
listeners=PLAINTEXT://:9092
port=9092
host.name=192.168.1.181
num.network.threads=4
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/tmp/kafka-logs
num.partitions=5
num.recovery.threads.per.data.dir=1
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
log.cleaner.enable=false
zookeeper.connect=192.168.1.181:2181
zookeeper.connection.timeout.ms=6000
queued.max.requests =500
log.cleanup.policy = delete
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;p&gt;broker.id为集群中唯一的标注一个节点，因为在同一个机器上，所以必须指定不同的端口和日志文件，避免数据被覆盖。&lt;/p&gt;

&lt;p&gt;在上面单个broker的实验中，为什么kafka的端口为9092，这里可以看得很清楚。&lt;/p&gt;

&lt;p&gt;kafka cluster怎么同zookeeper交互的，配置信息中也有体现。&lt;/p&gt;

&lt;p&gt;那么下面，我们仿照上面的配置文件，提供2个broker的配置文件：&lt;/p&gt;

&lt;p&gt;server2.properties:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;broker.id=2
listeners=PLAINTEXT://:9094
port=9094
host.name=192.168.1.181
num.network.threads=4
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/tmp/kafka-logs2
num.partitions=5
num.recovery.threads.per.data.dir=1
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
log.cleaner.enable=false
zookeeper.connect=192.168.1.181:2181
zookeeper.connection.timeout.ms=6000
queued.max.requests =500
log.cleanup.policy = delete
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、启动所有的broker&lt;/p&gt;

&lt;p&gt;命令如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-server-start.sh config/server0.properties &amp;amp;   #启动broker0
bin/kafka-server-start.sh config/server1.properties &amp;amp; #启动broker1
bin/kafka-server-start.sh config/server2.properties &amp;amp; #启动broker2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看2181、9092、9093、9094端口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netstat -tunlp|egrep &amp;quot;(2181|9092|9093|9094)&amp;quot;
tcp        0      0 :::9093                     :::*                        LISTEN      29725/java          
tcp        0      0 :::2181                     :::*                        LISTEN      19787/java          
tcp        0      0 :::9094                     :::*                        LISTEN      29800/java          
tcp        0      0 :::9092                     :::*                        LISTEN      29572/java  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个zookeeper在2181端口上监听，3个kafka cluster(broker)分别在端口9092,9093,9094监听。&lt;/p&gt;

&lt;p&gt;3、创建topic&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --create --topic topic_1 --partitions 1 --replication-factor 3  \--zookeeper localhost:2181
bin/kafka-topics.sh --create --topic topic_2 --partitions 1 --replication-factor 3  \--zookeeper localhost:2181
bin/kafka-topics.sh --create --topic topic_3 --partitions 1 --replication-factor 3  \--zookeeper localhost:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看topic创建情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --list --zookeeper localhost:2181
test
topic_1
topic_2
topic_3
[root@atman081 kafka_2.10-0.9.0.0]# bin/kafka-topics.sh --describe --zookeeper localhost:2181
Topic:test  PartitionCount:1  ReplicationFactor:1 Configs:
  Topic: test Partition: 0  Leader: 0 Replicas: 0 Isr: 0
Topic:topic_1 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_1  Partition: 0  Leader: 2 Replicas: 2,1,0 Isr: 2,1,0
Topic:topic_2 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_2  Partition: 0  Leader: 1 Replicas: 1,2,0 Isr: 1,2,0
Topic:topic_3 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_3  Partition: 0  Leader: 0 Replicas: 0,2,1 Isr: 0,2,1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个行显示所有partitions的一个总结，以下每一行给出一个partition中的信息，如果我们只有一个partition，则只显示一行。&lt;/p&gt;

&lt;p&gt;leader 是在给出的所有partitons中负责读写的节点，每个节点都有可能成为leader&lt;/p&gt;

&lt;p&gt;replicas 显示给定partiton所有副本所存储节点的节点列表，不管该节点是否是leader或者是否存活。&lt;/p&gt;

&lt;p&gt;isr 副本都已同步的的节点集合，这个集合中的所有节点都是存活状态，并且跟leader同步&lt;/p&gt;

&lt;p&gt;4、模拟客户端发送，接受消息&lt;/p&gt;

&lt;p&gt;发送消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --topic topic_1 --broker-list 192.168.1.181:9092,192.168.1.181:9093,192.168.1.181:9094
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接收消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --topic topic_1 --zookeeper 192.168.1.181:2181 --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要注意，此时producer将topic发布到了3个broker中，现在就有点分布式的概念了。&lt;/p&gt;

&lt;p&gt;5、kill some broker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kill broker(id=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先，我们根据前面的配置，得到broker(id=0)应该在9092监听,这样就能确定它的PID了。&lt;/p&gt;

&lt;p&gt;broker0没kill之前topic在kafka cluster中的情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --describe --zookeeper localhost:2181
Topic:test  PartitionCount:1  ReplicationFactor:1 Configs:
  Topic: test Partition: 0  Leader: 0 Replicas: 0 Isr: 0
Topic:topic_1 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_1  Partition: 0  Leader: 2 Replicas: 2,1,0 Isr: 2,1,0
Topic:topic_2 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_2  Partition: 0  Leader: 1 Replicas: 1,2,0 Isr: 1,2,0
Topic:topic_3 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_3  Partition: 0  Leader: 2 Replicas: 0,2,1 Isr: 2,1,0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kill之后，再观察，做下对比。很明显，主要变化在于Isr，以后再分析&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --describe --zookeeper localhost:2181
Topic:test  PartitionCount:1  ReplicationFactor:1 Configs:
  Topic: test Partition: 0  Leader: -1  Replicas: 0 Isr: 
Topic:topic_1 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_1  Partition: 0  Leader: 2 Replicas: 2,1,0 Isr: 2,1
Topic:topic_2 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_2  Partition: 0  Leader: 1 Replicas: 1,2,0 Isr: 1,2
Topic:topic_3 PartitionCount:1  ReplicationFactor:3 Configs:
  Topic: topic_3  Partition: 0  Leader: 2 Replicas: 0,2,1 Isr: 2,1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试下，发送消息，接受消息，是否收到影响。&lt;/p&gt;

&lt;p&gt;发送消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --topic topic_1 --broker-list 192.168.1.181:9092,192.168.1.181:9093,192.168.1.181:9094
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接收消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --topic topic_1 --zookeeper 192.168.1.181:2181 --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，kafka的分布式机制，容错能力还是挺好的~&lt;/p&gt;

&lt;h1 id=&#34;kafka常用命令&#34;&gt;Kafka常用命令&lt;/h1&gt;

&lt;p&gt;1、新建一个主题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --create --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --replication-factor 2 --partitions 2 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test有两个复制因子和两个分区&lt;/p&gt;

&lt;p&gt;2、查看新建的主题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --describe --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.8.8版本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./kafka-create-topic.sh --partition 1 --replica 1 --zookeeper localhost:2181 --topic test

./kafka-list-topic.sh --zookeeper localhost:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下查询的结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@dx3 bin]# ./kafka-topics.sh --describe --zookeeper localhost:2183 --topic test
Topic:test    PartitionCount:3    ReplicationFactor:3    Configs:
    Topic: test    Partition: 0    Leader: 0    Replicas: 0,1,2    Isr: 0,2,1
    Topic: test    Partition: 1    Leader: 1    Replicas: 1,2,0    Isr: 1,2,0
    Topic: test    Partition: 2    Leader: 2    Replicas: 2,0,1    Isr: 2,0,1

其中第一行是所有分区的信息，下面的每一行对应一个分区
Leader：负责某个分区所有读写操作的节点
Replicas：复制因子节点
Isr：存活节点
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以这个Kafka集群一共三个节点，test这个Topic, 编号为0的Partition,Leader在broker.id=0这个节点上，副本在broker.id为0 1 2这个三个几点，并且所有副本都存活，并跟broker.id=0这个节点同步&lt;/p&gt;

&lt;p&gt;3、查看Kafka所有的主题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-topics.sh --list --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、在终端发送消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、在终端接收（消费）消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/kafka-console-consumer.sh --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --bootstrap-server localhost:9092 --topic test --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们正常情况下不会使用自带的命令行进行数据的发送和消费，我们都是使用第三方库进行&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/kafka-client/&#34;&gt;客户端包括生产者和消费者&lt;/a&gt;的编码，实现业务的正常使用。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;先举一个例子&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/kafka/20170719.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kafka集群有两台服务器，四个分区，此外有两个消费者组A和B，消费者组A具有2个消费者实例C1-2，消费者B具有4个消费者实例C3-6&lt;/p&gt;

&lt;p&gt;例如此时我们创建了一个主题test，有两个分区，分别是Server1的P0和Server2的P1，假设此时我们通过test发布了一条消息，那么这条消息只会发到P0或P1其中之一，也就是消息只会发给其中的一个分区&lt;/p&gt;

&lt;p&gt;分区接收到消息后会记录在分区日志中，记录的方式我们讲过了，就是通过offset，分区中的消息都是以k-v形式存在。k表示offset，称之为偏移量，一个64位整型的唯一标识，offset代表了Topic分区中所有消息流中该消息的起始字节位置。 v就是实际的消息内容,正因为有这个偏移量的存在，所以一个分区内的消息是有先后顺序的，即offset大的消息比offset小的消息后到。但是注意，由于消息随机发往主题的任意一个分区，因此虽然同一个分区的消息有先后顺序，但是不同分区之间的消息就没有先后顺序了，那么如果我们要求消费者顺序消费主题发的消息那该怎么办呢，此时只要在创建主题的时候只提供一个分区即可&lt;/p&gt;

&lt;p&gt;讲完了主题发消息，接下来就该消费者消费消息了，假设上面test的消息发给了分区P0，此时从图中可以看到，有两个消费者组，那么P0将会把消息发到哪个消费者组呢？从图中可以看到，P0把消息既发给了消费者组A也发给了B，但是A中消息仅被C1消费，B中消息仅被C3消费。这就是我们要讲的，主题发出的消息会发往所有的消费者组，而每一个消费者组下面可以有很多消费者实例，这条消息只会被他们中的一个消费掉。&lt;/p&gt;

&lt;p&gt;在多分区的情况下如何保证有序性呢？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、kafka分布式的单位是partition，同一个partition用一个write ahead log组织，记录的offset，所以可以保证FIFO的顺序。&lt;/li&gt;
&lt;li&gt;2、不同partition之间不能保证顺序。但是绝大多数用户都可以通过message key来定义，因为同一个key的message可以保证只发送到同一个partition，比如说key是user id，table row id等等，所以同一个user或者同一个record的消息永远只会发送到同一个partition上，保证了同一个user或record的顺序。&lt;/li&gt;
&lt;li&gt;3、消费出来自己根据一些数据进行排序，比如时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;核心API&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kafka具有4个核心API：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Producer API：用于向Kafka主题发送消息。&lt;/li&gt;
&lt;li&gt;Consumer API：用于从订阅主题接收消息并且处理这些消息。&lt;/li&gt;
&lt;li&gt;Streams API：作为一个流处理器，用于从一个或者多个主题中消费消息流然后为其他主题生产消息流，高效地将输入流转换为输出流。&lt;/li&gt;
&lt;li&gt;Connector API：用于构建和运行将Kafka主题和已有应用或者数据系统连接起来的可复用的生产者或消费者。例如一个主题到一个关系型数据库的连接能够捕获表的任意变化。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;KAFKA吞吐量大的原因&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、消息顺序写入磁盘，并且批量处理。&lt;/p&gt;

&lt;p&gt;KAFKA维护一个Topic中的分区log，以顺序追加的方式向各个分区中写入消息，每个分区都是不可变的消息队列。分区中的消息都是以k-v形式存在。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;k表示offset，称之为偏移量，一个64位整型的唯一标识，offset代表了Topic分区中所有消息流中该消息的起始字节位置。&lt;/li&gt;
&lt;li&gt;v就是实际的消息内容，每个分区中的每个offset都是唯一存在的，所有分区的消息都是一次写入，在消息未过期之前都可以调整offset来实现多次读取。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们知道现在的磁盘大多数都还是机械结构（SSD不在讨论的范围内），如果将消息以随机写的方式存入磁盘，就会按柱面、磁头、扇区的方式进行（寻址过程），缓慢的机械运动（相对内存）会消耗大量时间，导致磁盘的写入速度只能达到内存写入速度的几百万分之一，为了规避随机写带来的时间消耗，KAFKA采取顺序写的方式存储数据。&lt;/p&gt;

&lt;p&gt;新来的消息只能追加到已有消息的末尾，并且已经生产的消息不支持随机删除以及随机访问，但是消费者可以通过重置offset的方式来访问已经消费过的数据。&lt;/p&gt;

&lt;p&gt;即使顺序读写，过于频繁的大量小I/O操作一样会造成磁盘的瓶颈，所以KAFKA在此处的处理是把这些消息集合在一起批量发送，这样减少对磁盘IO的过度读写，而不是一次发送单个消息。&lt;/p&gt;

&lt;p&gt;2、标准化二进制消息格式&lt;/p&gt;

&lt;p&gt;尤其是在负载比较高的情况下无效率的字节复制影响是显着的。为了避免这种情况，KAFKA采用由Producer，broker和consumer共享的标准化二进制消息格式，这样数据块就可以在它们之间自由传输，无需转换，降低了字节复制的成本开销。&lt;/p&gt;

&lt;p&gt;同时，KAFKA采用了MMAP(Memory Mapped Files，内存映射文件)技术。很多现代操作系统都大量使用主存做磁盘缓存，一个现代操作系统可以将内存中的所有剩余空间用作磁盘缓存，而当内存回收的时候几乎没有性能损失。&lt;/p&gt;

&lt;p&gt;由于KAFKA是基于JVM的，并且任何与Java内存使用打过交道的人都知道两件事：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对象的内存开销非常高，通常是实际要存储数据大小的两倍；&lt;/li&gt;
&lt;li&gt;随着数据的增加，java的垃圾收集也会越来越频繁并且缓慢。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于此，使用文件系统，同时依赖页面缓存就比使用其他数据结构和维护内存缓存更有吸引力：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;不使用进程内缓存，就腾出了内存空间，可以用来存放页面缓存的空间几乎可以翻倍。&lt;/li&gt;
&lt;li&gt;如果KAFKA重启，进行内缓存就会丢失，但是使用操作系统的页面缓存依然可以继续使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可能有人会问KAFKA如此频繁利用页面缓存，如果内存大小不够了怎么办？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KAFKA会将数据写入到持久化日志中而不是刷新到磁盘。实际上它只是转移到了内核的页面缓存。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;利用文件系统并且依靠页缓存比维护一个内存缓存或者其他结构要好，它可以直接利用操作系统的页缓存来实现文件到物理内存的直接映射。完成映射之后对物理内存的操作在适当时候会被同步到硬盘上。&lt;/p&gt;

&lt;p&gt;3、页缓存技术&lt;/p&gt;

&lt;p&gt;为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。这样做的好处有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;避免Object消耗：如果是使用 Java 堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。&lt;/li&gt;
&lt;li&gt;避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相比于使用JVM或in-memory cache等数据结构，利用操作系统的Page Cache更加简单可靠。首先，操作系统层面的缓存利用率会更高，因为存储的都是紧凑的字节结构而不是独立的对象。其次，操作系统本身也对于Page Cache做了大量优化，提供了 write-behind、read-ahead以及flush等多种机制。再者，即使服务进程重启，系统缓存依然不会消失，避免了in-process cache重建缓存的过程。&lt;/p&gt;

&lt;p&gt;通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。&lt;/p&gt;

&lt;p&gt;4、零拷贝&lt;/p&gt;

&lt;p&gt;如下所示，数据从磁盘传输到socket要经过以下几个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;操作系统将数据从磁盘读入到内核空间的页缓存&lt;/li&gt;
&lt;li&gt;应用程序将数据从内核空间读入到用户空间缓存中&lt;/li&gt;
&lt;li&gt;应用程序将数据写回到内核空间到socket缓存中&lt;/li&gt;
&lt;li&gt;操作系统将数据从socket缓冲区复制到网卡缓冲区，以便将数据经网络发出&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里有四次拷贝，两次系统调用，这是非常低效的做法。如果使用sendfile，只需要一次拷贝就行&lt;/p&gt;

&lt;p&gt;linux操作系统 “零拷贝” 机制使用了sendfile方法， 允许操作系统将数据从Page Cache 直接发送到网络，只需要最后一步的copy操作将数据复制到 NIC 缓冲区， 这样避免重新复制数据 。&lt;/p&gt;

&lt;p&gt;通过这种 “零拷贝” 的机制，Page Cache 结合 sendfile 方法，Kafka消费端的性能也大幅提升。这也是为什么有时候消费端在不断消费数据时，我们并没有看到磁盘io比较高，此刻正是操作系统缓存在提供数据。&lt;/p&gt;

&lt;p&gt;5、批量压缩&lt;/p&gt;

&lt;p&gt;在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络带宽，对于需要在广域网上的数据中心之间发送消息的数据流水线尤其如此。所以数据压缩就很重要。可以每个消息都压缩，但是压缩率相对很低。所以KAFKA使用了批量压缩，即将多个消息一起压缩而不是单个消息压缩。&lt;/p&gt;

&lt;p&gt;KAFKA允许使用递归的消息集合，批量的消息可以通过压缩的形式传输并且在日志中也可以保持压缩格式，直到被消费者解压缩。&lt;/p&gt;

&lt;p&gt;KAFKA支持Gzip和Snappy压缩协议。&lt;/p&gt;

&lt;p&gt;6、批量读写&lt;/p&gt;

&lt;p&gt;Kafka数据读写也是批量的而不是单条的。&lt;/p&gt;

&lt;p&gt;除了利用底层的技术外，Kafka还在应用程序层面提供了一些手段来提升性能。最明显的就是使用批次。在向Kafka写入数据时，可以启用批次写入，这样可以避免在网络上频繁传输单个消息带来的延迟和带宽开销。假设网络带宽为10MB/S，一次性传输10MB的消息比传输1KB的消息10000万次显然要快得多。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;KAFKA数据可靠性深度解读&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KAFKA的消息保存在Topic中，Topic可分为多个分区，为保证数据的安全性，每个分区又有多个Replia。&lt;/p&gt;

&lt;p&gt;多分区的设计的特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;为了并发读写，加快读写速度；&lt;/li&gt;
&lt;li&gt;是利用多分区的存储，利于数据的均衡；&lt;/li&gt;
&lt;li&gt;是为了加快数据的恢复速率，一但某台机器挂了，整个集群只需要恢复一部分数据，可加快故障恢复的时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个Partition分为多个Segment，每个Segment有.log和.index 两个文件，每个log文件承载具体的数据，每条消息都有一个递增的offset，Index文件是对log文件的索引，Consumer查找offset时使用的是二分法根据文件名去定位到哪个Segment，然后解析msg，匹配到对应的offset的msg。&lt;/p&gt;

&lt;p&gt;Partition recovery过程&lt;/p&gt;

&lt;p&gt;每个Partition会在磁盘记录一个RecoveryPoint,，记录已经flush到磁盘的最大offset。当broker 失败重启时，会进行loadLogs。首先会读取该Partition的RecoveryPoint，找到包含RecoveryPoint的segment及以后的segment， 这些segment就是可能没有完全flush到磁盘segments。然后调用segment的recover，重新读取各个segment的msg，并重建索引。每次重启KAFKA的broker时，都可以在输出的日志看到重建各个索引的过程。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据同步&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Producer和Consumer都只与Leader交互，每个Follower从Leader拉取数据进行同步。&lt;/p&gt;

&lt;p&gt;如上图所示，ISR是所有不落后的replica集合，不落后有两层含义：距离上次FetchRequest的时间不大于某一个值或落后的消息数不大于某一个值，Leader失败后会从ISR中随机选取一个Follower做Leader，该过程对用户是透明的。&lt;/p&gt;

&lt;p&gt;当Producer向Broker发送数据时,可以通过request.required.acks参数设置数据可靠性的级别。&lt;/p&gt;

&lt;p&gt;此配置是表明当一次Producer请求被认为完成时的确认值。特别是，多少个其他brokers必须已经提交了数据到它们的log并且向它们的Leader确认了这些信息。&lt;/p&gt;

&lt;p&gt;典型的值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;0： 表示Producer从来不等待来自broker的确认信息。这个选择提供了最小的时延但同时风险最大（因为当server宕机时，数据将会丢失）。&lt;/li&gt;
&lt;li&gt;1：表示获得Leader replica已经接收了数据的确认信息。这个选择时延较小同时确保了server确认接收成功。&lt;/li&gt;
&lt;li&gt;-1：Producer会获得所有同步replicas都收到数据的确认。同时时延最大，然而，这种方式并没有完全消除丢失消息的风险，因为同步replicas的数量可能是1。如果你想确保某些replicas接收到数据，那么你应该在Topic-level设置中选项min.insync.replicas设置一下。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;仅设置 acks= -1 也不能保证数据不丢失,当ISR列表中只有Leader时,同样有可能造成数据丢失。要保证数据不丢除了设置acks=-1，还要保证ISR的大小大于等于2。&lt;/p&gt;

&lt;p&gt;具体参数设置：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;request.required.acks:设置为-1 等待所有ISR列表中的Replica接收到消息后采算写成功。&lt;/li&gt;
&lt;li&gt;min.insync.replicas: 设置为&amp;gt;=2,保证ISR中至少两个Replica。&lt;/li&gt;
&lt;li&gt;Producer：要在吞吐率和数据可靠性之间做一个权衡。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;KAFKA作为现代消息中间件中的佼佼者，以其速度和高可靠性赢得了广大市场和用户青睐，其中的很多设计理念都是非常值得我们学习的，本文所介绍的也只是冰山一角，希望能够对大家了解KAFKA有一定的作用。&lt;/p&gt;

&lt;h1 id=&#34;kafka的应用场景&#34;&gt;Kafka的应用场景&lt;/h1&gt;

&lt;h2 id=&#34;kafka用作消息系统&#34;&gt;&lt;strong&gt;Kafka用作消息系统&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Kafka流的概念与传统企业消息系统有什么异同？&lt;/p&gt;

&lt;p&gt;传统消息系统有两个模型：队列和发布-订阅系统。在队列模式中，每条服务器的消息会被消费者池中的一个所读取；而发布-订阅系统中消息会广播给所有的消费者。这两种模式各有优劣。队列模式的优势是可以将消息数据让多个消费者处理以实现程序的可扩展，然而这就导致其没有多个订阅者，只能用于一个进程。发布-订阅模式的好处在于数据可以被多个进程消费使用，但是却无法使单一程序扩展性能&lt;/p&gt;

&lt;p&gt;Kafka中消费者组的概念同时涵盖了这两方面。对应于队列的概念，Kafka中每个消费者组中有多个消费者实例可以接收消息；对应于发布-订阅模式，Kafka中可以指定多个消费者组来订阅消息&lt;/p&gt;

&lt;p&gt;相对传统消息系统，Kafka可以提供更强的顺序保证&lt;/p&gt;

&lt;h2 id=&#34;kafka用作存储系统&#34;&gt;&lt;strong&gt;Kafka用作存储系统&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;任何发布消息与消费消息解耦的消息队列其实都可以看做是用来存放发布的消息的存储系统，而Kafka是一个非常高效的存储系统&lt;/p&gt;

&lt;p&gt;写入Kafka的数据会被存入磁盘并且复制到集群中以容错。Kafka允许生产者等待数据完全复制并且确保持久化到磁盘的确认应答&lt;/p&gt;

&lt;p&gt;Kafka使用的磁盘结构扩容性能很好——不管服务器上有50KB还是50TB，Kafka的表现都是一样的&lt;/p&gt;

&lt;p&gt;由于能够精致的存储并且供客户端程序进行读操作，你可以把Kafka看做是一个用于高性能、低延迟的存储提交日志、复制及传播的分布式文件系统&lt;/p&gt;

&lt;h2 id=&#34;kafka的流处理&#34;&gt;&lt;strong&gt;Kafka的流处理&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;仅仅读、写、存储流数据是不够的，Kafka的目的是实现实时流处理。&lt;/p&gt;

&lt;p&gt;在Kafka中一个流处理器的处理流程是首先持续性的从输入主题中获取数据流，然后对其进行一些处理，再持续性地向输出主题中生产数据流。例如一个销售商应用，接收销售和发货量的输入流，输出新订单和调整后价格的输出流&lt;/p&gt;

&lt;p&gt;可以直接使用producer和consumer API进行简单的处理。对于复杂的转换，Kafka提供了更强大的Streams API。可构建聚合计算或连接流到一起的复杂应用程序&lt;/p&gt;

&lt;p&gt;流处理有助于解决这类应用面临的硬性问题：处理无序数据、代码更改的再处理、执行状态计算等&lt;/p&gt;

&lt;p&gt;Streams API所依托的都是Kafka的核心内容：使用producer和consumer API作为输入，使用Kafka作为状态存储，在流处理实例上使用相同的组机制来实现容错&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 推荐系统</title>
          <link>https://kingjcy.github.io/post/architecture/recommend/</link>
          <pubDate>Thu, 04 Jul 2019 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/recommend/</guid>
          <description>&lt;p&gt;推荐系统主要依赖于算法，是将大数据进行分析后得到一个想要结果，进行评分推荐，其实和搜索系统有异曲同工之妙。&lt;/p&gt;

&lt;h1 id=&#34;offline&#34;&gt;offline&lt;/h1&gt;

&lt;p&gt;线下推荐子系统又主要分为线下挖掘模块、数据管理工具两大部分。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;挖掘模块&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/recommend/reconmend.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;线下挖掘模块，是各类线下挖掘算法实施的核心，它读取各种数据源，运用各种算法实施线下数据挖掘，产出初步的挖掘结果，并将挖掘结果以一定格式保存下来。典型的，实施这些挖掘策略的是一些跑在hadoop平台上的job，并行实施策略，并将挖掘结果保存到hadoop上。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据管理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数据管理工具，即DataMgrTools，它是一个工具（或者服务），它能够接受一些管理命令，读取某些特定格式的线下数据，将这些数据实时或者周期性的打到线上的redis或者内存中，供线上服务读取。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/recommend/recommend1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;定义好线下数据格式，线上数据格式，通过上下游API做数据的迁移和转换。&lt;/p&gt;

&lt;p&gt;数据管理工具是一个与业务无关的通用工具，它需要支持多种特定格式数据的上传，因为线下挖掘模块产出的数据可能存储在文件里，HDFS上，数据库里，甚至是特定二进制数据。&lt;/p&gt;

&lt;h1 id=&#34;online&#34;&gt;online&lt;/h1&gt;

&lt;p&gt;线上推荐子系统主要分为展示服务、分流服务、推荐内核、策略module服务等几个部分。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;展示服务，或者说是接入服务，它是整个推荐系统线上部分的入口，即整个推荐系统的接入层，它向上游提供接口，供上游业务方调用。&lt;/li&gt;
&lt;li&gt;分流服务，它是推荐系统中一个非常有特色也非常重要的一个服务，它的作用是将上游过来的请求，按照不同的策略，以不同的比例，分流到不同的推荐算法实验平台（也就是下游的推荐内核）中去。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;推荐内核，是各类线上推荐算法实施的核心，它其实只是一个通用的实验平台容器，每个推荐服务内部可能跑的是不同类型的推荐算法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;虽然推荐服务中跑着不同的推荐算法，但每个算法的实施步骤都是相同的，都需要经过：

（1）预处理；
（2）预分析；
（3）去重过滤；
（4）排序；
（5）推荐解释；

等五个步骤，每个步骤都可能存在多种不同的算法，不同的模型，各个步骤中的一种算法组合起来，完成一个完整的流程，构成一个“推荐算法实验平台”。
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;策略服务，又叫策略module服务，它实现了一个个推荐内核下游的推荐module。在推荐内核执行各个推荐步骤时，每个步骤中都可能存在不同的算法/策略，这些算法/和策略可能需要调用一些和策略绑定比较紧密的module服务，它们并不是通用服务，而是相对专有的服务。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/recommend/recommend2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实就是收集数据，数据处理，过滤，打分，推荐。&lt;/p&gt;

&lt;h1 id=&#34;推荐系统&#34;&gt;推荐系统&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;首页推荐：提取用户画像，根据线下提取出的用户年龄、性别、品类偏好等在首页综合推荐宝贝&lt;/li&gt;
&lt;li&gt;宝贝详情页推荐：买了还买，看了还看类的关联宝贝推荐&lt;/li&gt;
&lt;li&gt;附近推荐：和首页推荐的差异在于，提高了地理位置的权重，地理位置不仅要包含当前地理位置，还需要包含常见活跃区域，例如家里、公司等&lt;/li&gt;
&lt;li&gt;搜索推荐：除了关键词全匹配，要考虑同义词、近义词、易错词、拼音等推荐，产品层面，提示“你是不是想找xxoo宝贝”&lt;/li&gt;
&lt;li&gt;召回推荐：在用户退出系统后，通过RFM模型做优惠券推送或者消息推送做客户挽留与召回&lt;/li&gt;
&lt;li&gt;列表页推荐：用户既然进入到了美甲，成交意愿是非常强烈的，首页的推荐至关重要&lt;/li&gt;
&lt;li&gt;宝贝详情页推荐：买了还买，看了还看类的关联宝贝推荐&lt;/li&gt;
&lt;li&gt;下单成功页推荐：既然下单了某个甲样，可能会喜欢相近的甲样哟&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus blackbox_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/blackbox_exporter/</link>
          <pubDate>Wed, 03 Jul 2019 10:10:09 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/blackbox_exporter/</guid>
          <description>&lt;p&gt;blackbox主要是用这个探针去探测其他机器的网络情况，比如可以使用icmp协议来完成ping其他机器监控的任务，可以使用http协议来完成url探测的功能。&lt;/p&gt;

&lt;h1 id=&#34;实现原理&#34;&gt;实现原理&lt;/h1&gt;

&lt;p&gt;1、实现probe接口，然后根据不同的协议，走不通的实现&lt;/p&gt;

&lt;p&gt;2、icmp实现过程，首先就是使用socket请求，等待返回，icmp不存在几次握手连接的，只是发送请求等待返回（改造，实现ping丢包率：使用for循环发送十次请求，根据返回情况和时间来计算丢包率和延时）&lt;/p&gt;

&lt;p&gt;3、http协议，就是发送http请求，解析返回码&lt;/p&gt;

&lt;h1 id=&#34;使用注意&#34;&gt;使用注意&lt;/h1&gt;

&lt;p&gt;1、有的url返回容易超时，默认是1S，可以设置在配置文件中超时时间&lt;/p&gt;

&lt;p&gt;2、有的URL需要验证，则使用指定秘要文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http_2xx:
prober: http
timeout: 20000000000
http:
  tls_config:
    ca_file: /opt/promes/exporter/blackbox_exporter/blackbox_exporter_v0.12.1_linux-amd64/ssl/cargo.crt
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- Cache</title>
          <link>https://kingjcy.github.io/post/architecture/cache/</link>
          <pubDate>Sat, 15 Jun 2019 20:09:52 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/cache/</guid>
          <description>&lt;p&gt;缓存是一种提高系统读性能的常见技术，对于读多写少的应用场景，我们经常使用缓存来进行优化。&lt;/p&gt;

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;p&gt;我们在实际情况下总是遇到读多写少的场景，比如用户的余额信息表account(uid, money)，对于查询余额的需求，占99%，对于更改余额的需求只有1%，这个时候我们就要使用缓存来降低数据的压力，提高查询效率。&lt;/p&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;h2 id=&#34;读操作&#34;&gt;读操作&lt;/h2&gt;

&lt;p&gt;有了数据库和缓存两个地方存放数据之后（uid-&amp;gt;money），每当需要读取相关数据时（money），操作流程一般是这样的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;读取缓存中是否有相关数据，uid-&amp;gt;money&lt;/li&gt;
&lt;li&gt;如果缓存中有相关数据money，则返回【这就是所谓的数据命中“hit”】&lt;/li&gt;
&lt;li&gt;如果缓存中没有相关数据money，则从数据库读取相关数据money【这就是所谓的数据未命中“miss”】，放入缓存中uid-&amp;gt;money，再返回&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缓存的命中率 = 命中缓存请求个数/总缓存访问请求个数 = hit/(hit+miss)&lt;/p&gt;

&lt;h2 id=&#34;写操作&#34;&gt;写操作&lt;/h2&gt;

&lt;p&gt;写操作就比较复杂了，涉及三个问题&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;更新缓存 VS 淘汰缓存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;更新缓存（setCache(uid, money)）：数据不但写入数据库，还会写入缓存。一般数据获取并不是太复杂，我们都会更新缓存。&lt;/p&gt;

&lt;p&gt;淘汰缓存（deleteCache(uid)）：数据只会写入数据库，不会写入缓存，只会把数据淘汰掉，也就是删除。一般数据需要很复杂的获取方式，就会先把数据删除，然后在需要的时候再计算存入。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;先操作数据库 vs 先操作缓存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数据和缓存的操作时序，结论是清楚的：先淘汰缓存，再写数据库。&lt;/p&gt;

&lt;p&gt;假设先写数据库，再淘汰缓存,则会出现DB中是新数据，Cache中是旧数据，数据不一致。&lt;/p&gt;

&lt;p&gt;假设先淘汰缓存，再写数据库：第一步淘汰缓存成功，第二步写数据库失败，则只会引发一次Cache miss。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;数据不一致&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1、单库情况下，服务层的并发读写，缓存与数据库的操作交叉进行&lt;/p&gt;

&lt;p&gt;其实在先淘汰缓存，再写数据库中间还是会出现数据不一致的情况：就是在写入数据库之前，查询来一次，将数据存储到来缓存。&lt;/p&gt;

&lt;p&gt;遇到这种情况，我们最常想到的就是使用锁，但是如果使用全局锁的话影响很大，影响并发量，其实锁的思想就是串行化，我们可以通过相同的id走同一个服务实例和db连接来实现串行化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;修改服务Service连接池，id取模选取服务连接，能够保证同一个数据的读写都落在同一个后端服务上&lt;/li&gt;
&lt;li&gt;修改数据库DB连接池，id取模选取DB连接，能够保证同一个数据的读写在数据库层面是串行的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、主从同步，读写分离的情况下，读从库读到旧数据&lt;/p&gt;

&lt;p&gt;还有一种情况就是在主从同步，读写分离的架构情况下，如果查询数据的时候数据库主从同步还没有完成，导致数据不一致，这种架构还是我们常用的架构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请求A发起一个写操作，第一步淘汰了cache，如上图步骤1&lt;/li&gt;
&lt;li&gt;请求A写数据库了，写入了最新的数据，如上图步骤2&lt;/li&gt;
&lt;li&gt;请求B发起一个读操作，读cache，cache miss，如上图步骤3&lt;/li&gt;
&lt;li&gt;请求B继续读DB，读的是从库，此时主从同步还没有完成，读出来一个脏数据，然后脏数据入cache，如上图步4&lt;/li&gt;
&lt;li&gt;最后数据库的主从同步完成了，如上图步骤5&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在这种情况下，我们可以使用&amp;rdquo;缓存双淘汰”法：思想就是淘汰缓存两次，保证数据最新，第二次缓存什么时候淘汰就是一个关键，可以直接暴力的直接1s后再次删除缓存，但是这种方式需要等待，大大降低来并发，业务是接收不了的，所以还是需要异步完成。&lt;/p&gt;

&lt;p&gt;1、想到异步就想到MQ，所以我们可以通过mq来再次删除缓存&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2、还没有使用日志来二次删除缓存，与业务解耦，对业务线完全没有入侵，比较推荐。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;缓存服务的优化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上述缓存架构有一个缺点：业务方需要同时关注缓存与DB，我们可以通过服务化来屏蔽数据的细节，实现解耦。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;加入一个服务层，向上游提供帅气的数据访问接口，向上游屏蔽底层数据存储的细节，这样业务线不需要关注数据是来自于cache还是DB。其实golang中同步的map也是这么一个逻辑。&lt;/p&gt;

&lt;p&gt;还可以通过异步缓存更新来实现&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/cache/cache2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;业务线所有的写操作都走数据库，所有的读操作都总缓存，由一个异步的工具来做数据库与缓存之间数据的同步。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;要有一个init cache的过程，将需要缓存的数据全量写入cache&lt;/li&gt;
&lt;li&gt;如果DB有写操作，异步更新程序读取binlog，更新cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样也可以，但是比较浪费资源，还用同步的逻辑需要好好处理。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;针对这种架构思想，最多实现的就是&lt;a href=&#34;https://kingjcy.github.io/post/database/mysql/redis-mysql/&#34;&gt;mysql+redis&lt;/a&gt;组合了，上面的问题解决方案都可以用到这组实现中。&lt;/p&gt;

&lt;h1 id=&#34;问题&#34;&gt;问题&lt;/h1&gt;

&lt;h2 id=&#34;缓存穿透&#34;&gt;缓存穿透&lt;/h2&gt;

&lt;p&gt;我们在项目中使用缓存通常都是先检查缓存中是否存在，如果存在直接返回缓存内容，如果不存在就直接查询数据库然后再缓存查询结果返回。这个时候如果我们查询的某一个数据在缓存中一直不存在，就会造成每一次请求都查询DB，这样缓存就失去了意义，在流量大时，可能DB就挂掉了。&lt;/p&gt;

&lt;p&gt;那这种问题有什么好办法解决呢？&lt;/p&gt;

&lt;p&gt;要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。&lt;/p&gt;

&lt;p&gt;有一个比较巧妙的作法是，可以将这个不存在的key预先设定一个值。&lt;/p&gt;

&lt;p&gt;比如，”key” , “&amp;amp;&amp;amp;”。&lt;/p&gt;

&lt;p&gt;在返回这个&amp;amp;&amp;amp;
值的时候，我们的应用就可以认为这是不存在的key，那我们的应用就可以决定是否继续等待继续访问，还是放弃掉这次操作。如果继续等待访问，过一个时间轮询点后，再次请求这个key，如果取到的值不再是&amp;amp;&amp;amp;，则可以认为这时候key有值了，从而避免了透传到数据库，从而把大量的类似请求挡在了缓存之中。&lt;/p&gt;

&lt;h2 id=&#34;缓存并发&#34;&gt;缓存并发&lt;/h2&gt;

&lt;p&gt;有时候如果网站并发访问高，一个缓存如果失效，可能出现多个进程同时查询DB，同时设置缓存的情况，如果并发确实很大，这也可能造成DB压力过大，还有缓存频繁更新的问题。&lt;/p&gt;

&lt;p&gt;我现在的想法是对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询。&lt;/p&gt;

&lt;p&gt;这种情况和刚才说的预先设定值问题有些类似，只不过利用锁的方式，会造成部分请求等待。&lt;/p&gt;

&lt;h2 id=&#34;缓存失效&#34;&gt;缓存失效&lt;/h2&gt;

&lt;p&gt;引起这个问题的主要原因还是高并发的时候，平时我们设定一个缓存的过期时间时，可能有一些会设置1分钟啊，5分钟这些，并发很高时可能会出在某一个时间同时生成了很多的缓存，并且过期时间都一样，这个时候就可能引发一当过期时间到后，这些缓存同时失效，请求全部转发到DB，DB可能会压力过重。&lt;/p&gt;

&lt;p&gt;那如何解决这些问题呢？
其中的一个简单方案就时讲缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。&lt;/p&gt;

&lt;p&gt;我们讨论的第二个问题时针对同一个缓存，第三个问题时针对很多缓存。&lt;/p&gt;

&lt;h2 id=&#34;总结-1&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;1、缓存穿透：查询一个必然不存在的数据。比如文章表，查询一个不存在的id，每次都会访问DB，如果有人恶意破坏，很可能直接对DB造成影响。&lt;/p&gt;

&lt;p&gt;2、缓存失效：如果缓存集中在一段时间内失效，DB的压力凸显。这个没有完美解决办法，但可以分析用户行为，尽量让失效时间点均匀分布。
当发生大量的缓存穿透，例如对某个失效的缓存的大并发访问就造成了缓存雪崩。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 冗余表</title>
          <link>https://kingjcy.github.io/post/architecture/redundanttable/</link>
          <pubDate>Fri, 14 Jun 2019 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/redundanttable/</guid>
          <description>&lt;p&gt;冗余表的架构设计就是牺牲空间一份数据存多张表，可以通过不同索引查询提高效率的一种架构思想。&lt;/p&gt;

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;p&gt;互联网很多业务场景的数据量很大，此时数据库架构要进行水平切分，水平切分会有一个patition key，通过patition key的查询能够直接定位到库，但是非patition key上的查询可能就需要扫描多个库了。&lt;/p&gt;

&lt;p&gt;例如订单表，业务上对用户和商家都有订单查询需求，如果用buyer_id来分库，seller_id的查询就需要扫描多库。如果用seller_id来分库，buyer_id的查询就需要扫描多库。这类需求，为了做到高吞吐量低延时的查询，往往使用“数据冗余”的方式来实现，同一个数据，冗余两份，一份以buyer_id来分库，满足买家的查询需求；一份以seller_id来分库，满足卖家的查询需求。&lt;/p&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;h2 id=&#34;服务同步写&#34;&gt;服务同步写&lt;/h2&gt;

&lt;p&gt;顾名思义，由服务层同步写冗余数据，先后向两个表中同时插入数据。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;不复杂，服务层由单次写，变两次写&lt;/li&gt;
&lt;li&gt;数据一致性相对较高（因为双写成功才返回）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请求的处理时间增加（要插入次，时间加倍）&lt;/li&gt;
&lt;li&gt;数据仍可能不一致，例如第二步写入T1完成后服务重启，则数据不会写入T2&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;服务异步写&#34;&gt;服务异步写&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据的双写并不再由服务来完成，服务层异步发出一个消息，通过消息总线发送给一个专门的数据复制服务来写入冗余数据。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;请求处理时间短（只插入1次）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;系统的复杂性增加了，多引入了一个组件（消息总线）和一个服务（专用的数据复制服务）&lt;/li&gt;
&lt;li&gt;因为返回业务线数据插入成功时，数据还不一定插入到T2中，因此数据有一个不一致时间窗口（这个窗口很短，最终是一致的）&lt;/li&gt;
&lt;li&gt;在消息总线丢失消息时，冗余表数据会不一致&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;线下异步写&#34;&gt;线下异步写&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据的双写不再由服务层来完成，而是由线下的一个服务或者任务来读取数据的log来完成。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据双写与业务完全解耦&lt;/li&gt;
&lt;li&gt;请求处理时间短（只插入1次）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;返回业务线数据插入成功时，数据还不一定插入到T2中，因此数据有一个不一致时间窗口（这个窗口很短，最终是一致的）&lt;/li&gt;
&lt;li&gt;数据的一致性依赖于线下服务或者任务的可靠性&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;先写正表还是反表&#34;&gt;先写正表还是反表&lt;/h1&gt;

&lt;p&gt;上述三种方案各有优缺点，但不管哪种方案，都会面临“究竟先写T1还是先写T2”的问题，对于一个不能保证事务性的操作，一定涉及“哪个任务先做，哪个任务后做”的问题，解决这个问题的方向是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;【如果出现不一致】，谁先做对业务的影响较小，就谁先执行，需要根据业务逻辑来做处理。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比如还是对订单的业务，用户下单时，如果“先插入buyer表T1，再插入seller冗余表T2”，当第一步成功、第二步失败时，出现的业务影响是“买家能看到自己的订单，卖家看不到推送的订单”，相反，如果“先插入seller表T2，再插入buyer冗余表T1”，当第一步成功、第二步失败时，出现的业务影响是“卖家能看到推送的订单，卖家看不到自己的订单”，由于这个生成订单的动作是买家发起的，买家如果看不到订单，会觉得非常奇怪，并且无法支付以推动订单状态的流转，此时即使卖家看到有人下单也是没有意义的，因此，在此例中，应该先插入buyer表T1，再插入seller表T2。&lt;/p&gt;

&lt;h1 id=&#34;保证数据的一致性&#34;&gt;保证数据的一致性&lt;/h1&gt;

&lt;p&gt;不管哪种方案，因为两步操作不能保证原子性，总有出现数据不一致的可能，基本解决方案。&lt;/p&gt;

&lt;h2 id=&#34;线下扫面正反冗余表全部数据&#34;&gt;线下扫面正反冗余表全部数据&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;线下启动一个离线的扫描工具，不停的比对正表T1和反表T2，如果发现数据不一致，就进行补偿修复。这个是我们最容易想到的方法，也是最消耗资源的方法。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;比较简单，开发代价小&lt;/li&gt;
&lt;li&gt;线上服务无需修改，修复工具与线上服务解耦&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;扫描效率低，会扫描大量的“已经能够保证一致”的数据&lt;/li&gt;
&lt;li&gt;由于扫描的数据量大，扫描一轮的时间比较长，即数据如果不一致，不一致的时间窗口比较长&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;线下扫描增量数据&#34;&gt;线下扫描增量数据&lt;/h2&gt;

&lt;p&gt;有没有只扫描“可能存在不一致可能性”的数据，而不是每次扫描全部数据，每次只扫描增量的日志数据，就能够极大提高效率，缩短数据不一致的时间窗口。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然，我们还是需要一个离线的日志扫描工具，不停的比对日志log1和日志log2，如果发现数据不一致，就进行补偿修复&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;虽比方法一复杂，但仍然是比较简单的&lt;/li&gt;
&lt;li&gt;数据扫描效率高，只扫描增量数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;线上服务略有修改（代价不高，多写了2条日志）&lt;/li&gt;
&lt;li&gt;虽然比方法一更实时，但时效性还是不高，不一致窗口取决于扫描的周期&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;实时线上-消息对-检测&#34;&gt;实时线上“消息对”检测&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/redundant/redundant4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;有些系统要求比较高，需要实现实时的检测，所以就不能使用日志了，需要使用消息系统。假设正常情况下，msg1和msg2的接收时间应该在3s以内，如果检测服务在收到msg1后没有收到msg2，就尝试检测数据的一致性，不一致时进行补偿修复&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;效率高&lt;/li&gt;
&lt;li&gt;实时性高&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;方案比较复杂，上线引入了消息总线这个组件&lt;/li&gt;
&lt;li&gt;线下多了一个订阅总线的检测服务&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Runtime</title>
          <link>https://kingjcy.github.io/post/golang/go-runtime/</link>
          <pubDate>Thu, 13 Jun 2019 19:39:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-runtime/</guid>
          <description>&lt;p&gt;尽管 Go 编译器产生的是本地可执行代码，这些代码仍旧运行在 Go 的 runtime（这部分的代码可以在 runtime 包中找到）当中。这个 runtime 类似 Java 和 .NET 语言所用到的虚拟机，它负责管理包括内存分配、垃圾回收、栈处理、goroutine、channel、切片（slice）、map 和反射（reflection）等等。&lt;/p&gt;

&lt;h1 id=&#34;runtime&#34;&gt;runtime&lt;/h1&gt;

&lt;p&gt;runtime包含Go运行时的系统交互的操作，例如控制goruntine的功能，还有debug，pprof进行排查问题和运行时性能分析，tracer来抓取异常事件信息，总的来说运行时是调度器包括GC。&lt;/p&gt;

&lt;h2 id=&#34;调度器&#34;&gt;调度器&lt;/h2&gt;

&lt;p&gt;runtime 调度器是个非常有用的东西，关于 runtime 包几个方法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NumCPU：返回当前系统的 CPU 核数量&lt;/li&gt;
&lt;li&gt;GOMAXPROCS：设置最大的可同时使用的 CPU 核数，Golang 默认所有任务都运行在一个 cpu 核里，如果要在 goroutine 中使用多核，可以使用 runtime.GOMAXPROCS 函数修改，当参数小于 1 时使用默认值1。&lt;/li&gt;
&lt;li&gt;Gosched：让当前线程让出 cpu 以让其它线程运行,它不会挂起当前线程，因此当前线程未来会继续执行&lt;/li&gt;
&lt;li&gt;Goexit：退出当前 goroutine(但是defer语句会照常执行)&lt;/li&gt;
&lt;li&gt;NumGoroutine：返回正在执行和排队的任务总数&lt;/li&gt;
&lt;li&gt;GOOS：目标操作系统，可以查看目标操作系统&lt;/li&gt;
&lt;li&gt;runtime.GC：会让运行时系统进行一次强制性的垃圾收集&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;获取goroot和os&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//获取goroot目录：
fmt.Println(&amp;quot;GOROOT--&amp;gt;&amp;quot;,runtime.GOROOT())

//获取操作系统
fmt.Println(&amp;quot;os/platform--&amp;gt;&amp;quot;,runtime.GOOS) // GOOS--&amp;gt; darwin，mac系统
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取CPU数量，和设置CPU数量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init(){
    //1.获取逻辑cpu的数量
    fmt.Println(&amp;quot;逻辑CPU的核数：&amp;quot;,runtime.NumCPU())
    //2.设置go程序执行的最大的：[1,256]
    n := runtime.GOMAXPROCS(runtime.NumCPU())
    fmt.Println(n)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让出cpu&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    go func() {
        for i := 0; i &amp;lt; 5; i++ {
            fmt.Println(&amp;quot;goroutine。。。&amp;quot;)
        }
​
    }()
​
    for i := 0; i &amp;lt; 4; i++ {
        //让出时间片，先让别的协议执行，它执行完，再回来执行此协程
        runtime.Gosched()
        fmt.Println(&amp;quot;main。。&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;终止协程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    //创建新建的协程
    go func() {
        fmt.Println(&amp;quot;goroutine开始。。。&amp;quot;)
​
        //调用了别的函数
        fun()
​
        fmt.Println(&amp;quot;goroutine结束。。&amp;quot;)
    }() //别忘了()
​
    //睡一会儿，不让主协程结束
    time.Sleep(3*time.Second)
}
​
​
​
func fun() {
    defer fmt.Println(&amp;quot;defer。。。&amp;quot;)
​
    //return           //终止此函数
    runtime.Goexit() //终止所在的协程
​
    fmt.Println(&amp;quot;fun函数。。。&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列----VictoriaMetrics</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/</link>
          <pubDate>Thu, 13 Jun 2019 16:19:46 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/</guid>
          <description>&lt;p&gt;VictoriaMetrics是一个高性能的，长期存储的prometheus的远程解决方案，实现集群使用的federation的方式，只不过性能很优秀，包括write和query，聚合数据也解决了查询问题。&lt;/p&gt;

&lt;h1 id=&#34;优势&#34;&gt;优势&lt;/h1&gt;

&lt;p&gt;VictoriaMetrics不仅仅是时序数据库,它的优势主要体现在一下几点:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对外支持Prometheus相关的API，所以它可以直接用于Grafana作为Prometheus数据源使用, 同时扩展了PromQL, 详细使用可参考&lt;a href=&#34;https://github.com/VictoriaMetrics/VictoriaMetrics/wiki/ExtendedPromQL&#34;&gt;https://github.com/VictoriaMetrics/VictoriaMetrics/wiki/ExtendedPromQL&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;针对Prometheus的Metrics插入查询具备高性能和良好的扩展性。甚至性能比InfluxDB和TimescaleDB高出20x&lt;/li&gt;
&lt;li&gt;内存占用方面也做出了优化, 比InfluxDB少10x&lt;/li&gt;
&lt;li&gt;高性能的数据压缩方式,使存入存储的数据量比TimescaleDB多达70x&lt;/li&gt;
&lt;li&gt;优化了高延迟IO和低iops的存储&lt;/li&gt;
&lt;li&gt;操作简单&lt;/li&gt;
&lt;li&gt;支持从第三方时序数据库获取数据源&lt;/li&gt;
&lt;li&gt;异常关闭情况下可以保护存储数据损坏&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;部署&#34;&gt;部署&lt;/h1&gt;

&lt;h2 id=&#34;单点&#34;&gt;单点&lt;/h2&gt;

&lt;h3 id=&#34;编译&#34;&gt;编译&lt;/h3&gt;

&lt;p&gt;1、二进制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make victoria-metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make victoria-metrics-prod
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;启动&#34;&gt;启动&lt;/h3&gt;

&lt;p&gt;直接使用二进制文件进行启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup /opt/victoria-metrics/victoria-metrics-prod -storageDataPath=&amp;quot;/data/victoria&amp;quot; -retentionPeriod=2  &amp;gt;&amp;gt;/opt/promes/victoria-metrics/logs/start.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;-storageDataPath - path to data directory. VictoriaMetrics stores all the data in this directory.&lt;/li&gt;
&lt;li&gt;-retentionPeriod - retention period in months for the data. Older data is automatically deleted.&lt;/li&gt;
&lt;li&gt;-httpListenAddr - TCP address to listen to for http requests. By default it listens port 8428 on all the network interfaces.&lt;/li&gt;
&lt;li&gt;-graphiteListenAddr - TCP and UDP address to listen to for Graphite data. By default it is disabled.&lt;/li&gt;
&lt;li&gt;-opentsdbListenAddr - TCP and UDP address to listen to for OpenTSDB data. By default it is disabled.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见他也是一个时序数据库，支持将prometheus，influxdb，graphite，opentsdb的数据的写入，比如使用的是prometheus，只使用了http的端口，在我们对应的prometheus文件中配置远程写入，将数据写入到victoria-metrics中去，配置如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remote_write:
  - url: http://&amp;lt;victoriametrics-addr&amp;gt;:8428/api/v1/write
    queue_config:
      max_samples_per_send: 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数据存储到victoria-metrics，我们还是通过8428端口来读取，我们在grafana中配置datasource：&lt;a href=&#34;http://victoriametrics-addr-ip:8428&#34;&gt;http://victoriametrics-addr-ip:8428&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;停止&#34;&gt;停止&lt;/h3&gt;

&lt;p&gt;发送SIGINT给进程&lt;/p&gt;

&lt;h3 id=&#34;高可用&#34;&gt;高可用&lt;/h3&gt;

&lt;p&gt;启动多个实例，将prometheus的数据分别写入到这些节点中，加一层负载均衡，就可以实现高可用，解决单点问题，prometheus配置如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remote_write:
  - url: http://&amp;lt;victoriametrics-addr-1&amp;gt;:8428/api/v1/write
    queue_config:
      max_samples_per_send: 10000
  # ...
  - url: http://&amp;lt;victoriametrics-addr-N&amp;gt;:8428/api/v1/write
    queue_config:
      max_samples_per_send: 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边讲一下高可用和水平扩展&lt;/p&gt;

&lt;p&gt;高可用是指多活，解决单点故障，正常就是多个相同的服务同时提供服务，来确保一个节点挂了，就能转移到其他的节点上，不影响外部整体的使用，比如redis的主备切换，sentinel机制，还有上面的virtoria-metrics的方式&lt;/p&gt;

&lt;p&gt;水平扩展是一种分布式的能力，一个节点不能处理，就多个节点一起处理，这样分担一下，整体的量就上去了，比如redis的cluster集群，理论上只要加节点，就可以存储月来越多的数据，实际集群内部交互还是有瓶颈的&lt;/p&gt;

&lt;p&gt;正常的服务，可以说在集群同时解决高可用和水平扩展是很困难的，正常的一个集群的作用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;集群内主节点都获取全部数据，然后其他节点都重主节点复制数据，对外一直提供主节点查询，当主节点出现问题的时候，主备切换，这样实现了高可用，但是有单节点数量瓶颈，不能水平扩展。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群内每个节点获取一部分数据，然后集群内节点相互复制，实现最终一致性，每个节点都保存完整的数据，这个时候一个节点挂了，会出问题，单个节点也会有瓶颈，所以在这个基础上收取前加一层负载均衡，这样当一个节点挂了之后，负载均衡会分配到其他节点上，这样实现了高可用，也实现了水平扩展，但是这个很难实现，而且还是有单节点瓶颈，一般是适用这种数据量很小的需要一致性的服务发现。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群内每个节点获取一部分数据，并且只存储这一部分数据，然后集群内使用一些数据库或者自身实现关系映射，然后对外查询会路由到对应的节点上去查询数据。这种模式就是支持水平扩展的，但是有一个节点
出问题，查询就会出问题，没有实现高可用，所以在这个基础上实现高可用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个单节点的设置主从复制，相互切换&lt;/li&gt;
&lt;li&gt;完成集群间的复制&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后可以说是目前比较好的解决方式。&lt;/p&gt;

&lt;h3 id=&#34;其他操作&#34;&gt;其他操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;可以删除数据&lt;/li&gt;
&lt;li&gt;可以导出数据&lt;/li&gt;
&lt;li&gt;目前不支持Downsampling，但是victoria-metrics的压缩率和查询效率足以使用&lt;/li&gt;
&lt;li&gt;单节点不支持水平扩展，但是单节点足以媲美thanos和M3，timescaleDB的性能，如果还是觉得不够用，可以尝试集群版本&lt;/li&gt;
&lt;li&gt;virtoria-metrics的参数基本不用调整，都是优化后的合理设计，自身也支持prometheus监控&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;测试&#34;&gt;测试&lt;/h3&gt;

&lt;p&gt;启动脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test victoria-metrics]# cat start.sh
nohup /opt/victoria-metrics/victoria-metrics-prod -storageDataPath=&amp;quot;/data/victoria&amp;quot; -retentionPeriod=2  &amp;gt;&amp;gt;/opt/victoria-metrics/logs/start.log 2&amp;gt;&amp;amp;1 &amp;amp;


-storageDataPath=&amp;quot;/data/victoria&amp;quot;：数据存储目录

-retentionPeriod=2：数据存储时间，两个月
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;11天的数据量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test victoria-metrics]# du -sh /data/victoria
20G /data/victoria
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cpu和内存消耗&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
21899 root      20   0 42.9g  26g 6280 S 188.5 20.8  13486:10 victoria-metric
 4560 root      20   0  159g  26g 365m S 1110.1 20.8   9846:56 prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;集群模式是采用的分布式部署，将数据分别存储在不同的节点上，实现了水平扩展，目前还没有relaese版本，需要自己编译，但是解决了数据量的问题，同时在性能方面并没有发生太大的影响。&lt;/p&gt;

&lt;h3 id=&#34;编译-1&#34;&gt;编译&lt;/h3&gt;

&lt;p&gt;直接make就会在bin目录下生成可执行文件vmstorage, vmselect and vminsert。&lt;/p&gt;

&lt;h3 id=&#34;架构原理图&#34;&gt;架构原理图&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/vm/vm.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;组件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;vmstorage - stores the data&lt;/p&gt;

&lt;p&gt;vmstore其实就是我们数据存在的地方，需要先启动，否则insert会找不到插入的节点，导致数据丢失。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vminsert - proxies the ingested data to vmstorage shards using consistent hashing&lt;/p&gt;

&lt;p&gt;vminsert对采集的提供的代理接口，同时选择将数据插入到我们指定的store节点，可以是单节点，也可以是集群上所有的机器都部署，通过nginx来负载均衡，可以减少节点压力，但是并不能解决单点问题。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vmselect - performs incoming queries using the data from vmstorage&lt;/p&gt;

&lt;p&gt;vmselect是给外部进行查询的接口，同时也负责查询数据的聚合功能。负载均衡和vmisert一样，使用nginx。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;http-api&#34;&gt;HTTP api&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;insert&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;vminsert-ip&amp;gt;:8480/insert/&amp;lt;accountID&amp;gt;/&amp;lt;suffix&amp;gt;:

&amp;lt;accountID&amp;gt; is an arbitrary number identifying namespace for data ingestion (aka tenant)
&amp;lt;suffix&amp;gt; may have the following values:

    1.prometheus - for inserting data with Prometheus remote write API
    2.influx/write or influx/api/v2/write - for inserting data with Influx line protocol
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;querying&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;vmselect-ip&amp;gt;:8481/select/&amp;lt;accountID&amp;gt;/prometheus/&amp;lt;suffix&amp;gt;:

&amp;lt;accountID&amp;gt; is an arbitrary number identifying data namespace for the query (aka tenant)
&amp;lt;suffix&amp;gt; may have the following values:

    1.api/v1/query - performs PromQL instant query
    2.api/v1/query_range - performs PromQL range query
    3.api/v1/series - performs series query
    4.api/v1/labels - returns a list of label names
    5.api/v1/label/&amp;lt;label_name&amp;gt;/values - returns values for the given &amp;lt;label_name&amp;gt; according to API
    6.federate - returns federated metrics
    7.api/v1/export - exports raw data. See this article for details
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;delete&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;vmselect-ip&amp;gt;:8481/delete/&amp;lt;accountID&amp;gt;/prometheus/api/v1/admin/tsdb/delete_series?match[]=&amp;lt;timeseries_selector_for_delete&amp;gt;.
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;vmstorage&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;vmstore保留了8482端口，提供一下URL：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/snapshot/create - create instant snapshot, which can be used for backups in background. Snapshots are created in &amp;lt;storageDataPath&amp;gt;/snapshots folder, where &amp;lt;storageDataPath&amp;gt; is the corresponding command-line flag value.
/snapshot/list - list available snasphots.
/snapshot/delete?snapshot=&amp;lt;id&amp;gt; - delete the given snapshot.
/snapshot/delete_all - delete all the snapshots.
Snapshots may be created independently on each vmstorage node. There is no need in synchronizing snapshots&#39; creation across vmstorage nodes.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;扩展&#34;&gt;扩展&lt;/h3&gt;

&lt;p&gt;1、vminsert and vmselect是可扩展的，无状态的，可以随时扩展或者缩容，并不影响，只是需要在负载均衡中将相关节点处理一下&lt;/p&gt;

&lt;p&gt;2、vmstore是有状态的，因为是分布式存储数据的，所以新增节点需要如下步骤&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Start new vmstorage node with the same -retentionPeriod as existing nodes in the cluster.&lt;/li&gt;
&lt;li&gt;Gradually restart all the vmselect nodes with new -storageNode arg containing &lt;new_vmstorage_host&gt;:8401.&lt;/li&gt;
&lt;li&gt;Gradually restart all the vminsert nodes with new -storageNode arg containing &lt;new_vmstorage_host&gt;:8400.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;备份和恢复&#34;&gt;备份和恢复&lt;/h3&gt;

&lt;p&gt;1、主要使用vmstore的url来进行备份&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Create an instant snapshot by navigating to /snapshot/create HTTP handler. It will create snapshot and return its name.
Archive the created snapshot from &amp;lt;-storageDataPath&amp;gt;/snapshots/&amp;lt;snapshot_name&amp;gt; folder using any suitable tool that follows symlinks. For instance, cp -L, rsync -L or scp -r. The archival process doesn&#39;t interfere with vmstorage work, so it may be performed at any suitable time. Incremental backups are possible with rsync --delete, which should remove extraneous files from backup dir.
Delete unused snapshots via /snapshot/delete?snapshot=&amp;lt;snapshot_name&amp;gt; or /snapshot/delete_all in order to free up occupied storage space.
There is no need in synchronizing backups among all the vmstorage nodes.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、恢复&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stop vmstorage node with kill -INT.&lt;/li&gt;
&lt;li&gt;Delete all the contents of the directory pointed by -storageDataPath command-line flag.&lt;/li&gt;
&lt;li&gt;Copy all the contents of the backup directory to -storageDataPath directory.&lt;/li&gt;
&lt;li&gt;Start vmstorage node.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;实现原理&#34;&gt;实现原理&lt;/h1&gt;

&lt;h2 id=&#34;vminsert&#34;&gt;vminsert&lt;/h2&gt;

&lt;p&gt;插入数据就比较简单了，使用了prometheus差不多的数据结构体来存储数据，只要将数据转化为对应的结构体直接存入数据就可以。对于其他的时序数据库比如influxdb都是差不多的数据结构，只要稍微进行转换，就可以将数据存储到存储节点去。&lt;/p&gt;

&lt;h2 id=&#34;vmstore&#34;&gt;vmstore&lt;/h2&gt;

&lt;p&gt;存储数据可以比常规的节省10倍的内存&lt;/p&gt;

&lt;h2 id=&#34;vmselect&#34;&gt;vmselect&lt;/h2&gt;

&lt;p&gt;查询数据很快&lt;/p&gt;

&lt;h2 id=&#34;mergetree&#34;&gt;MergeTree&lt;/h2&gt;

&lt;p&gt;VictoriaMetrics将数据存储在相似于ClickHouse的 MergeTree表 数据结构中。它是用于剖析数据和其余事件流的最快的数据库。在典型的剖析查问上，它的性能要比PostgreSQL和MySQL等传统数据库高10到1000倍。&lt;/p&gt;

&lt;h1 id=&#34;特性&#34;&gt;特性&lt;/h1&gt;

&lt;h2 id=&#34;扩展了promeql&#34;&gt;扩展了promeql&lt;/h2&gt;

&lt;p&gt;1 、模版&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;((node_memory_MemTotal_bytes{instance=~&amp;quot;$node:$port&amp;quot;, job=~&amp;quot;$job&amp;quot;} - node_memory_MemFree_bytes{instance=~&amp;quot;$node:$port&amp;quot;, job=~&amp;quot;$job&amp;quot;}) /
node_memory_MemTotal_bytes{instance=~&amp;quot;$node:$port&amp;quot;, job=~&amp;quot;$job&amp;quot;}) * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用模版&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WITH (
    commonFilters = {instance=~&amp;quot;$node:$port&amp;quot;,job=~&amp;quot;$job&amp;quot;}
)
(node_memory_MemTotal_bytes{commonFilters} - node_memory_MemFree_bytes{commonFilters}) /
    node_memory_MemTotal_bytes{commonFilters} * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;发展&#34;&gt;发展&lt;/h1&gt;

&lt;h2 id=&#34;vmagent&#34;&gt;vmagent&lt;/h2&gt;

&lt;p&gt;vmagent是一个很小巧但优秀的代理，它可以帮助您从各种来源收集指标并将其存储到VictoriaMetrics或任何其他支持remote_write协议的与Prometheus兼容的存储系统。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/vm/vm2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以用作Prometheus的直接替代品，用于抓取目标（例如node_exporter）。&lt;/li&gt;
&lt;li&gt;可以像Prometheus那样，重新添加，删除和修改标签。可以在将数据发送到远程存储之前对其进行过滤。&lt;/li&gt;
&lt;li&gt;支持多种VictoriaMetrics支持的数据格式，比如Influx，OpenTSDB，Graphite，Prometheus等。&lt;/li&gt;
&lt;li&gt;可以将收集的指标同时复制到多个远程存储系统。在与远程存储连接不稳定的环境中工作。如果远程存储不可用，则将收集的指标缓存在-remoteWrite.tmpDataPath中。一旦恢复远程存储的连接，缓冲的metrcis即发送到远程存储。可以通过-remoteWrite.maxDiskUsagePerURL限制缓冲区的最大磁盘使用量。&lt;/li&gt;
&lt;li&gt;与Prometheus相比，使用较少的RAM，CPU，磁盘IO和网络带宽。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前来讲，Prometheus依旧不可或缺。vmagent 还处于开发阶段。但是vmagent有取代prometheus的想法是可以看出来的。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Cortex</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/</link>
          <pubDate>Thu, 13 Jun 2019 14:28:39 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/</guid>
          <description>&lt;p&gt;crotex是一个为了支持prometheus扩展的服务，支持水平扩展，高可用，多租户，长期存储。主要开发者也是promehteus的开发者&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/cortex/architecture.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;组件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Distributor&lt;/p&gt;

&lt;p&gt;Distributor就是负责接收promtheus发送过来的数据，然后将数据分发给lngester。&lt;/p&gt;

&lt;p&gt;Distributor只要和lngester进行交互，使用的是grpc&lt;/p&gt;

&lt;p&gt;Distributor使用一致性hash来将数据分发给哪个lngester实例，consistent hash ring is stored in Consul&lt;/p&gt;

&lt;p&gt;建议使用负载均衡来运行多个distributors实例。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;lngester&lt;/p&gt;

&lt;p&gt;lngester组件主要是接受Distributor发来的数据，然后发送到后段的数据库存储&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ruler&lt;/p&gt;

&lt;p&gt;ruler组件主要是负责处理alertmanager产生的告警&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Query frontend&lt;/p&gt;

&lt;p&gt;Query frontend组件主要是接受http请求，把他们按着tenant ID排列，并且重试一些返回错误的请求，比如large query&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Querier&lt;/p&gt;

&lt;p&gt;Querier组件主要是处理promql&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Chunk store&lt;/p&gt;

&lt;p&gt;Chunk store组件就是长期存储&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;就是我们常用的集群架构：聚合，将所有数据都发送到一个节点，用于存储+查询&lt;/p&gt;

&lt;h1 id=&#34;安装使用&#34;&gt;安装使用&lt;/h1&gt;

&lt;p&gt;编译启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build ./cmd/cortex
$ ./cortex -config.file=./docs/single-process-config.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置prometheus的远程写，将prometheus数据写入到cortex中去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remote_write:
- url: http://localhost:9009/api/prom/push
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup /opt/promes/cortex/cortex -config.file=/opt/promes/cortex/config/single-process-config.yaml -distributor.ingestion-rate-limit=100000 -ring.store=consul -consul.hostname=10.47.182.224:9996 -distributor.replication-factor=2 &amp;gt;&amp;gt;/opt/promes/cortex/logs/start.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;-distributor.ingestion-rate-limit=100000：限制数据量为100000，其实达不到这个量，prometheus默认remote_write的10000个并发，每个包含100个数据，这个时候会大量出错，所以在写入性能上达不到这个量，测试最大每个包含25个数据可以处理，同样的机器上victoria-metrics可以达到10000个数据而不出错。&lt;/li&gt;
&lt;li&gt;-ring.store=consul -consul.hostname=10.47.182.224:9996：一个令牌存储在consul上，用我们现有的consul&lt;/li&gt;
&lt;li&gt;-distributor.replication-factor=2：集群节点的数量，这边主要是高可用，两个节点互相复制，完成一致性哈希&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前没有看到cortex的可扩展的优秀的方面，可能是社区开发还没有完成，等release。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 中台建设</title>
          <link>https://kingjcy.github.io/post/architecture/electronic-commerce/</link>
          <pubDate>Tue, 04 Jun 2019 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/electronic-commerce/</guid>
          <description>&lt;p&gt;中台建设其实就是将一些能够统一的业务进行统一规划，所以系统的接入和流出都是标准化的操作。&lt;/p&gt;

&lt;h1 id=&#34;组织架构的演变&#34;&gt;组织架构的演变&lt;/h1&gt;

&lt;p&gt;其实我们在另一篇文章&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/microservices/&#34;&gt;架构的演变&lt;/a&gt;中已经主要架构的设计思路，其实也是和组织架构相互对应，相互衍生的。&lt;/p&gt;

&lt;h2 id=&#34;中台&#34;&gt;中台&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;前台---中台---后台
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而到了大中台时代，中台的核心价值是在于，在对企业业务有了柔性支撑和贯通的前提下，再形成协同与智慧的运营体系。&lt;/p&gt;

&lt;p&gt;一般企业架构分成了三个层次：前台、中台、后台。中台又分成三大块，业务中台、数据中台和技术中台。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/electroniccommerce/middle&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;技术中台支撑企业业务发展，通过打通企业内异构系统，支持业务中台；&lt;/li&gt;
&lt;li&gt;业务中台围绕公司业务运营进行服务，将获取的多维度数据传递给数据中台，由数据中台分析反馈给业务中台，以优化业务运营。同时数据中台通过BI智能分析，帮助企业管理者更好的做决策分析。三者是相辅相成，相互协作的。
业务中台其实就是把原有的前端的会员中心、营销中心、商品中心，后端的供应链中心、采配中心等重点模块放在业务中台模块，以后前端不管对接多少个第三方，线上线下增加多少家门店，都能进行统一会员、统一商品编码、统一供应链整合，整个系统一体化。真正做到用技术支持业务，通过业务收集大量数据进行决策，统一高效的进行管理。&lt;/li&gt;
&lt;li&gt;数据中台：一般指以数据采集、数据集成、数据治理，指标体系和数据仓库统一建设等数据管理活动为典型特征的中台。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;大中台&#34;&gt;大中台&lt;/h1&gt;

&lt;p&gt;其实中台严格意义上来说，不是一种架构，也不是一种系统，而是一种战略。&lt;/p&gt;

&lt;p&gt;当前最需要建设的中台有两种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;狭义的业务中台：一般指在线业务为典型特征的中台。在OLDI（Online Data-Intensive）时代，越来越多的企业的核心业务都是在线业务，因此把在线业务中台简称为业务中台。&lt;/li&gt;
&lt;li&gt;数据中台：一般指以数据采集、数据集成、数据治理，指标体系和数据仓库统一建设等数据管理活动为典型特征的中台。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对业务中台来说，比较符合的场景主要有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;业务系统研发团队至少大几十人（含外包的），需求多变化快，系统又涉及多个领域（比如做ERP、电商的），业务逻辑比较复杂。&lt;/li&gt;
&lt;li&gt;这时业务中台可以把系统和业务领域划分清楚，提高研发效率。做相似行业的外包项目为主，业务规模也做的比较大的（一年有两位数的项目）。&lt;/li&gt;
&lt;li&gt;这时业务中台可以提升软件复用，降低定制化成本，提高研发效率。如果每个项目都完全不一样，那中台也救不了你。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;支持业务中台的技术体系，包括微服务、DevOps、云原生和分布式事务等。&lt;/p&gt;

&lt;p&gt;将需求设计成微服务架构，然后每个服务使用各种技术栈来开发业务，比如golang的技术栈的高并发的特性来开发web服务等，然后将一些统一的模块进行统一的接入和输出，使用devops的开发模式，在业务中还是需要解决分布式事务等问题。&lt;/p&gt;

&lt;p&gt;比如在网易，是网易轻舟微服务平台，提供微服务应用全生命周期的完整支持，包括下一代微服务Service Mesh支持、经典微服务框架NSF、包括CI/CD的DevOps、分布式事务框架GXTS、APM、API网关、GoAPI全自动化测试以及容器应用管理服务等。&lt;/p&gt;

&lt;p&gt;对数据中台来说，比较符合的场景：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数据产品比较多，每天要看数据如果没数据就不知道怎么工作的运营人员比较多的业务。
比如电商就是典型。尤其是数据产品和运营人员还在多个团队。
用数据的姿势比较复杂，问题比较多，比如经常出现指标不一致、数据出错、想要的数据不知道哪里有等问题。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;支持数据中台的技术体系，包括指标管理、数据服务、元数据管理、数仓开发与管理、数据安全管理、数据资产管理、大数据计算引擎、数据集成/同步/交换引擎等，&lt;/p&gt;

&lt;p&gt;其实数据中台就是将数据进行处理，不同数据资源，统一的输出标准，中间用到大部分就是数据引擎，比如kafka队列，sprak，flink等流式引擎，hadoop，hbase和hive等大数据引擎。&lt;/p&gt;

&lt;p&gt;比如在网易，是以网易猛犸为核心的网易全链路数据中台解决方案。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 广告投放系统</title>
          <link>https://kingjcy.github.io/post/architecture/advertising/</link>
          <pubDate>Tue, 04 Jun 2019 14:38:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/advertising/</guid>
          <description>&lt;p&gt;互联网智能广告系统简易流程与架构。&lt;/p&gt;

&lt;h1 id=&#34;业务简述&#34;&gt;业务简述&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/advertising/advertising.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;广告主在业务端投递广告&lt;/p&gt;

&lt;p&gt;广告主登录业务端后台，进行设置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;•今日投放地域是“北京-上地”
•投放类别是“租房”
•定向人群为“女”，“30岁以下”
•需要推广的广告内容是他发布的一条“房屋出租”的帖子
•竞价设置的是0.2元
•单日预算是20元
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些数据，当然通过业务端存储到了数据层，即数据库和缓存里。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;用户来到了网站，进入了“北京-上地-租房”类别，广告初筛实施&lt;/p&gt;

&lt;p&gt;合适的广告，必须符合“语义相关性”，即基础检索属性（广告属性）必须符合（广告能否满足用户的需求，满足了点击率才高），这个工作是通过BS-basic search检索服务完成的。BS从数据层检索到“北京-上地-租房”的广告帖子。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;用户属性与广告主属性匹配，广告精筛实施&lt;/p&gt;

&lt;p&gt;步骤二中，基础属性初筛了以后，要进行更深层次的策略筛选（用户能否满足广告的需求），此例中，广告主的精准需求为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;•用户性别为“女”
•用户年龄为“30岁以下”
•用户访问IP是“北京”
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;系统将初筛出来的M条广告和用户属性进行匹配筛选，又过滤掉了一部分，最后剩余N条待定广告，这些广告既满足用户的需求（初筛），这些用户也满足广告主的需求（精筛），后者是在AS-advanced search策略服务完成的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;综合排序，并返回Top X的广告&lt;/p&gt;

&lt;p&gt;经过步骤2和步骤3的初筛和精筛之后，待选的N条广告既能满足用户当前的需求，用户亦能满足广告主的筛选需求，但实际情况是，广告位只有3个，怎么办呢？就需要我们对N条广告进行综合打分排序（满足平台的需求，广告平台要多赚钱嘛）。&lt;/p&gt;

&lt;p&gt;出价高，但没人点击，广告平台没有收益；点击率高，但出价低，广告平台还是没有收益。最终应该按照广告的出价与CTR的乘积作为综合打分排序的依据，bid*CTR。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;展现端展示了广告，用户点击了广告&lt;/p&gt;

&lt;p&gt;展示了广告后，展现端js会上报广告展示日志，有部分用户点击了广告，服务端会记录点击日志，这些日志可以作为广告算法实施的数据源，同时，他们经过统计分析之后，会被展示给广告主，让他们能够看到自己广告的展示信息，点击信息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对广告主进行扣费&lt;/p&gt;

&lt;p&gt;用户既然点击了广告，平台就要对投放广告的广告主进行扣费了，扣费前当然要经过反作弊系统的过滤（主要是恶意点击），扣费后信息会实时反映到数据层，费用扣光后，广告就要从数据层下线。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- Distributed config</title>
          <link>https://kingjcy.github.io/post/distributed/distributed-config/</link>
          <pubDate>Sun, 26 May 2019 20:10:41 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/distributed-config/</guid>
          <description>&lt;p&gt;配置信息就是程序加载时候需要设置的信息，一般我们可以通过在代码中设置，在配置文件中设置，在配置中心配置来设置我们需要的信息。&lt;/p&gt;

&lt;h1 id=&#34;配置方式&#34;&gt;配置方式&lt;/h1&gt;

&lt;p&gt;配置方式一般有两种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;本地配置，包括代码中耦合，配置文件加载。&lt;/li&gt;
&lt;li&gt;适用于分布式系统的集中式资源配置。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;本地配置&#34;&gt;本地配置&lt;/h2&gt;

&lt;h3 id=&#34;代码耦合&#34;&gt;代码耦合&lt;/h3&gt;

&lt;p&gt;将配置直接写在代码中很方便，但是对于后期的维护修改十分的不友好，需要重新发布才能修改完成，基本上是不用的。&lt;/p&gt;

&lt;h3 id=&#34;配置文件&#34;&gt;配置文件&lt;/h3&gt;

&lt;p&gt;将配置信息写在配置文件中是最常见的方式，程序自动加载配置文件的信息，然后进行处理，有需要只要修改对应的配置信息就可以，实现了对业务代码的解耦。&lt;/p&gt;

&lt;p&gt;但是随着系统的发展，分布式系统的出现，我们很多实例都是通过集群的方式进行部署，这个时候在集群的每个节点上都会有一份配置文件，随着规模的扩大，修改每个节点上的配置文件又变的容易出错难以维护，因此我们需要一种集中式的配置方式。&lt;/p&gt;

&lt;h2 id=&#34;集中式资源配置&#34;&gt;集中式资源配置&lt;/h2&gt;

&lt;p&gt;在分布式的情况下，我们需要一个共享配置的中心，使用集中式的资源管理配置平台的好处&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置信息统一管理&lt;/li&gt;
&lt;li&gt;动态获取更新配置文件&lt;/li&gt;
&lt;li&gt;降低运维成本&lt;/li&gt;
&lt;li&gt;降低配置出错率&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;分布式配置管理服务的本质是一个发布订阅模式的实现。&lt;/p&gt;

&lt;p&gt;集中式的配置中心一般都是作为服务发现和动态注册的基础，也是我们常用的几个框架，比如zookeeper，consul，etcd等。我们一般的配置平台都是基于这些组件来实现的，目前这一块做的比较好的就是淘宝的diamonds和百度的disconf，还有一些比如携程的apollo等。&lt;/p&gt;

&lt;h3 id=&#34;zookeeper&#34;&gt;zookeeper&lt;/h3&gt;

&lt;p&gt;常规使用一些服务发现动态注册组件做配置中心都是一个思路，我们以zookeeper为例，看下他是如何实现分布式配置管理的&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/zookeeper&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;整体可以分为3部分&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;zookeeper集群
提供了稳定的配置管理服务，对外提供了接口，外部可以添加、修改配置信息，可以监听配置的变化&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置管理中心
需要自己开发，负责维护配置信息&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;各个分布式应用
每个应用只需要调用一下ZK的接口，把自己注册到ZK，就可以自动接收配置的变化信息&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;工作流程原理也很简答&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/zookeeper1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;diamond&#34;&gt;diamond&lt;/h3&gt;

&lt;p&gt;diamond是淘宝内部使用的一个管理持久配置的系统，它的特点是简单、可靠、易用，目前淘宝内部绝大多数系统的配置，由diamond来进行统一管理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/diamond&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;作为一个配置中心，diamond的功能分为发布和订阅两部分。因为diamond存放的是持久数据，这些数据的变化频率不会很高，甚至很低，所以发布采用手工的形式，通过diamond后台管理界面发布；订阅是diamond的核心功能，订阅通过diamond-client的API进行。&lt;/li&gt;
&lt;li&gt;diamond服务端采用mysql加本地文件的形式存放配置数据。发布数据时，数据先写到mysql，再写到本地文件；订阅数据时，直接获取本地文件，不查询数据库，这样可以最大程度减少对数据库的压力。&lt;/li&gt;
&lt;li&gt;diamond服务端是一个集群，集群中的每台机器连接同一个mysql，集群之间的数据同步通过两种方式进行，一是每台server定时去mysqldump数据到本地文件，二是某一台server接收发布数据请求，在更新完mysql和本机的本地文件后，发送一个HTTP请求（通知）到集群中的其他几台server，其他server收到通知，去mysql中将刚刚更新的数据dump到本地文件。&lt;/li&gt;
&lt;li&gt;每一台server前端都有一个nginx，用来做流量控制。&lt;/li&gt;
&lt;li&gt;图中没有将地址服务器画出，地址服务器是一台有域名的机器，上面运行有一个HTTPserver，其中有一个静态文件，存放着diamond服务器的地址列表。客户端启动时，根据自身的域名绑定，连接到地址服务器，取回diamond服务器的地址列表，从中随机选择一台diamond服务器进行连接。&lt;/li&gt;
&lt;li&gt;可以看到，整个diamond的架构非常简单，使用的都是最常用的一些技术以及产品，它之所以表现得非常稳定，跟其架构简单是分不开的，当然，稳定的另一个主要原因是它具备一套比较完善的容灾机制，容灾机制将在下一篇文章中讲述。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;disconf&#34;&gt;disconf&lt;/h3&gt;

&lt;p&gt;disconf是一套完整的基于zookeeper的分布式配置统一解决方案。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/disconf&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每个模块的简单介绍如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Disconf-core

&lt;ul&gt;
&lt;li&gt;分布式通知模块：支持配置更新的实时化通知&lt;/li&gt;
&lt;li&gt;路径管理模块：统一管理内部配置路径URL&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disconf-client

&lt;ul&gt;
&lt;li&gt;配置仓库容器模块：统一管理用户实例中本地配置文件和配置项的内存数据存储&lt;/li&gt;
&lt;li&gt;配置reload模块：监控本地配置文件的变动，并自动reload到指定bean&lt;/li&gt;
&lt;li&gt;扫描模块：支持扫描所有disconf注解的类和域&lt;/li&gt;
&lt;li&gt;下载模块：restful风格的下载配置文件和配置项&lt;/li&gt;
&lt;li&gt;watch模块：监控远程配置文件和配置项的变化&lt;/li&gt;
&lt;li&gt;主备分配模块：主备竞争结束后，统一管理主备分配与主备监控控制&lt;/li&gt;
&lt;li&gt;主备竞争模块：支持分布式环境下的主备竞争&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disconf-web

&lt;ul&gt;
&lt;li&gt;配置存储模块：管理所有配置的存储和读取&lt;/li&gt;
&lt;li&gt;配置管理模块：支持配置的上传、下载、更新&lt;/li&gt;
&lt;li&gt;通知模块：当配置更新后，实时通知使用这些配置的所有实例&lt;/li&gt;
&lt;li&gt;配置自检监控模块：自动定时校验实例本地配置与中心配置是否一致&lt;/li&gt;
&lt;li&gt;权限控制：web的简单权限控制&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disconf-tools

&lt;ul&gt;
&lt;li&gt;context共享模块：提供多实例间context的共享。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;原理&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/config/disconf&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动事件A：以下按顺序发生。

&lt;ul&gt;
&lt;li&gt;A3：扫描静态注解类数据，并注入到配置仓库里。&lt;/li&gt;
&lt;li&gt;A4+A2：根据仓库里的配置文件、配置项，去 disconf-web 平台里下载配置数据。这里会有主备竞争&lt;/li&gt;
&lt;li&gt;A5：将下载得到的配置数据值注入到仓库里。&lt;/li&gt;
&lt;li&gt;A6：根据仓库里的配置文件、配置项，去ZK上监控结点。&lt;/li&gt;
&lt;li&gt;A7+A2：根据XML配置定义，到 disconf-web 平台里下载配置文件，放在仓库里，并监控ZK结点。这里会有主备竞争。&lt;/li&gt;
&lt;li&gt;A8：A1-A6均是处理静态类数据。A7是处理动态类数据，包括：实例化配置的回调函数类；将配置的值注入到配置实体里。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;更新配置事件B：以下按顺序发生。

&lt;ul&gt;
&lt;li&gt;B1：管理员在 Disconf-web 平台上更新配置。&lt;/li&gt;
&lt;li&gt;B2：Disconf-web 平台发送配置更新消息给ZK指定的结点。&lt;/li&gt;
&lt;li&gt;B3：ZK通知 Disconf-cient 模块。&lt;/li&gt;
&lt;li&gt;B4：与A4一样。&lt;/li&gt;
&lt;li&gt;B5：与A5一样。&lt;/li&gt;
&lt;li&gt;B6：基本与A4一样，唯一的区别是，这里还会将配置的新值注入到配置实体里。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;主备机切换事件C：以下按顺序发生。

&lt;ul&gt;
&lt;li&gt;C1：发生主机挂机事件。&lt;/li&gt;
&lt;li&gt;C2：ZK通知所有被影响到的备机。&lt;/li&gt;
&lt;li&gt;C4：与A2一样。&lt;/li&gt;
&lt;li&gt;C5：与A4一样。&lt;/li&gt;
&lt;li&gt;C6：与A5一样。&lt;/li&gt;
&lt;li&gt;C7：与A6一样。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>服务发现系列---- Zookeeper</title>
          <link>https://kingjcy.github.io/post/middleware/serverdiscovery/zookeeper/</link>
          <pubDate>Fri, 19 Apr 2019 09:37:11 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/serverdiscovery/zookeeper/</guid>
          <description>&lt;p&gt;zookeeper是Hadoop的一个子项目，它是分布式系统中的协调系统，可提供的服务主要有：配置服务、名字服务、分布式同步、组服务等。就是提供高可用的数据管理、应用程序协调服务的分布式服务框架，基于对Paxos算法的实现，使该框架保证了分布式环境中数据的强一致性，提供的功能包括：配置维护、统一命名服务、状态同步服务、集群管理等。&lt;/p&gt;

&lt;h1 id=&#34;zookeeper&#34;&gt;Zookeeper&lt;/h1&gt;

&lt;p&gt;Zookeeper的特点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;简单&lt;/p&gt;

&lt;p&gt;Zookeeper的核心是一个精简的文件系统，它支持一些简单的操作和一些抽象操作，例如，排序和通知。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;丰富&lt;/p&gt;

&lt;p&gt;Zookeeper的原语操作是很丰富的，可实现一些协调数据结构和协议。例如，分布式队列、分布式锁和一组同级别节点中的“领导者选举”。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;高可靠&lt;/p&gt;

&lt;p&gt;Zookeeper支持集群模式，可以很容易的解决单点故障问题。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;松耦合交互&lt;/p&gt;

&lt;p&gt;不同进程间的交互不需要了解彼此，甚至可以不必同时存在，某进程在zookeeper中留下消息后，该进程结束后其它进程还可以读这条消息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;资源库&lt;/p&gt;

&lt;p&gt;Zookeeper实现了一个关于通用协调模式的开源共享存储库，能使开发者免于编写这类通用协议。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装并启动&#34;&gt;安装并启动&lt;/h2&gt;

&lt;p&gt;1、配置文件conf/zoo.cfg&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
initLimit=10
syncLimit=5
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;tickTime:指定了ZooKeeper的基本时间单位（以毫秒为单位）。&lt;/li&gt;
&lt;li&gt;dataDir:存储内存数据快照位置。&lt;/li&gt;
&lt;li&gt;clientPort：监听客户端连接端口。&lt;/li&gt;
&lt;li&gt;initLimmit：启动zookeeper，从节点至主节点连接超时时间。（上面为10个tickTime）&lt;/li&gt;
&lt;li&gt;syncLimit:zookeeper正常运行，若主从同步时间超过syncLimit，则丢弃该从节点。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、配置完，启动zookeeper&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、检查是否运行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo ruok | nc localhost 2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、连接到zookeeper&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/zkCli.sh -server 127.0.0.1:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在shell命令中使用help查看命令列表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[zkshell: 0] help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个zookeeper节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create /zk_test my_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取节点数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;get /zk_test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改节点数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set /zk_test junk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;delete /zk_test
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;其实原理简单来说，就是要选举leader，会生成一个zxid，然后分发给所有的server（所以这里一台server可以接受多台server给他发送要选举leader的请求），然后各个server根据发送给自己的zxid，选择一个值最大的，然后将这个选择返回给发送这个zxid的server，只要这个server收到的答复大于等于2/n+1个（也就是超过半数的同意票），则表明自己当选为leader，然后会向所有server广播自己已经成为leader。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeeper数据模型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;类似于文件系统，所有的节点都是绝对路径，将文件和目录抽象成znode。和Unix中的文件系统路径格式很想，但是只支持绝对路径，不支持相对路径，也不支持点号（”.”和”..”）。&lt;/p&gt;

&lt;p&gt;1、znode&lt;/p&gt;

&lt;p&gt;维护了数据和ACL改变的版本号，每一次数据改变版本号增加，当有一个client去执行update和delete时候，必须提供一个数据变更版本号，如果与数据不符合，则更新失败。&lt;/p&gt;

&lt;p&gt;2、存储小数据&lt;/p&gt;

&lt;p&gt;一般不超过1M，大量数据会花费更多的时间和延迟来完成数据拷贝，由于网络的消耗。&lt;/p&gt;

&lt;p&gt;3、瞬时节点&lt;/p&gt;

&lt;p&gt;随着会话结束而删除&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;When the session ends the znode is deleted. Because
of this behavior ephemeral znodes are not allowed to have children.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、操作的原子性&lt;/p&gt;

&lt;p&gt;Znode的数据读写是原子的，要么读或写了完整的数据，要么就失败，不会出现只读或写了部分数据。&lt;/p&gt;

&lt;p&gt;5、顺序znode&lt;/p&gt;

&lt;p&gt;名称中包含Zookeeper指定顺序号的znode。若在创建znode时设置了顺序标识，那么该znode被创建后，名字的后边将会附加一串数字，该数字是由一个单调递增的计数器来生成的。例如，创建节点时传入的path是”/aa/bb”，创建后的则可能是”/aa/bb0002”，再次创建后是”/aa/bb0003”。&lt;/p&gt;

&lt;p&gt;Znode的创建模式CreateMode有四种，分别是：EPHEMERAL（短暂的znode）、EPHEMERAL_SEQUENTIAL（短暂的顺序znode）、PERSISTENT（持久的znode）和PERSISTENT_SEQUENTIAL（持久的顺序znode）。如果您已经看过了上篇博文，那么这里的api调用应该是很好理解的，见：&lt;a href=&#34;http://www.cnblogs.com/leocook/p/zk_0.html。&#34;&gt;http://www.cnblogs.com/leocook/p/zk_0.html。&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeeper中的时间&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Zxid：zookeeper状态的改变都会有事物id自增来维护。&lt;/li&gt;
&lt;li&gt;Version numbers ：znode的数据和ACL变更维护。&lt;/li&gt;
&lt;li&gt;ticks:zookeeper集群部署时的时间单位。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeepr  watches&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以在读操作exists、getChildren和getData上设置观察，在执行写操作create、delete和setData将会触发观察事件，当然，在执行写的操作时，也可以选择是否触发znode上设置的观察器，具体可查看相关的api。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当观察的znode被创建、删除或其数据被更新时，设置在exists上的观察将会被触发；&lt;/li&gt;
&lt;li&gt;当观察的znode被删除或数据被更新时，设置在getData上的观察将会被触发；&lt;/li&gt;
&lt;li&gt;当观察的znode的子节点被创建、删除或znode自身被删除时，设置在getChildren上的观察将会被触发，可通过观察事件的类型来判断被删除的是znode还是它的子节点。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：在收到收到触发事件到执行读操作之间，znode的状态可能会发生状态，这点需要牢记。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ACL&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ACL权限：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CREATE: you can create a child node&lt;/li&gt;
&lt;li&gt;READ: you can get data from a node and list its children.&lt;/li&gt;
&lt;li&gt;WRITE: you can set data for a node&lt;/li&gt;
&lt;li&gt;DELETE: you can delete a child node&lt;/li&gt;
&lt;li&gt;ADMIN: you can set permissions&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;身份验证模式：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;world 无验证&lt;/li&gt;
&lt;li&gt;auth 只能使用sessionID&lt;/li&gt;
&lt;li&gt;digest  username:password 验证&lt;/li&gt;
&lt;li&gt;ip 客户端IP验证&lt;/li&gt;
&lt;li&gt;host 客户端主机名验证&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;添加验证&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可使用zk对象的addAuthInfo()方法来添加验证模式，如使用digest模式进行身份验证：zk.addAuthInfo(“digest”,”username:passwd”.getBytes());&lt;/p&gt;

&lt;p&gt;在zookeeper对象被创建时，初始化会被添加world验证模式。world身份验证模式的验证id是”anyone”。&lt;/p&gt;

&lt;p&gt;若该连接创建了znode，那么他将会被添加auth身份验证模式的验证id是””，即空字符串，这里将使用sessionID进行验证。&lt;/p&gt;

&lt;p&gt;创建自定义验证：创建ACL对象时，可用ACL类的构造方法ACL(int perms, Id id)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeeper  内部原理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、选举领导&lt;/p&gt;

&lt;p&gt;集群中所有的zk实例会选举出来一个“领导实例”（leader），其它实例称之为“随从实例”（follower）。如果leader出现故障，其余的实例会选出一台leader，并一起提供服务，若之前的leader恢复正常，便成为follower。选举follower是一个很快的过程，性能影响不明显。&lt;/p&gt;

&lt;p&gt;Leader主要功能是协调所有实例实现写操作的原子性，即：所有的写操作都会转发给leader，然后leader会将更新广播给所有的follower，当半数以上的实例都完成写操作后，leader才会提交这个写操作，随后客户端会收到写操作执行成功的响应。&lt;/p&gt;

&lt;p&gt;2、原子广播&lt;/p&gt;

&lt;p&gt;上边已经说到：所有的写操作都会转发给leader，然后leader会将更新广播给所有的follower，当半数以上的实例都完成写操作后，leader才会提交这个写操作，随后客户端会收到写操作执行成功的响应。这么来的话，就实现了客户端的写操作的原子性，每个写操作要么成功要么失败。逻辑和数据库的两阶段提交协议很像。&lt;/p&gt;

&lt;p&gt;Znode的每次写操作都相当于数据库里的一次事务提交，每个写操作都有个全局唯一的ID，称为：zxid（ZooKeeper Transaction）。ZooKeeper会根据写操作的zxid大小来对操作进行排序，zxid小的操作会先执行。zk下边的这些特性保证了它的数据一致性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;顺序一致性&lt;/p&gt;

&lt;p&gt;任意客户端的写操作都会按其发送的顺序被提交。如果一个客户端把某znode的值改为a，然后又把值改为b（后面没有其它任何修改），那么任何客户端在读到值为b之后都不会再读到a。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;原子性&lt;/p&gt;

&lt;p&gt;这一点再前面已经说了，写操作只有成功和失败两种状态，不存在只写了百分之多少这么一说。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;单一系统映像&lt;/p&gt;

&lt;p&gt;客户端只会连接host列表中状态最新的那些实例。如果正在连接到的实例挂了，客户端会尝试重新连接到集群中的其他实例，那么此时滞后于故障实例的其它实例都不会接收该连接请求，只有和故障实例版本相同或更新的实例才接收该连接请求。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;持久性&lt;/p&gt;

&lt;p&gt;写操作完成之后将会被持久化存储，不受服务器故障影响。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;及时性&lt;/p&gt;

&lt;p&gt;在对某个znode进行读操作时，应该先执行sync方法，使得读操作的连接所连的zk实例能与leader进行同步，从而能读到最新的类容。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意：sync调用是异步的，无需等待调用的返回，zk服务器会保证所有后续的操作会在sync操作完成之后才执行，哪怕这些操作是在执行sync之前被提交的。&lt;/p&gt;

&lt;h1 id=&#34;zookeeper-典型的应用场景&#34;&gt;ZooKeeper 典型的应用场景&lt;/h1&gt;

&lt;p&gt;Zookeeper 从设计模式角度来看，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些观察者做出相应的反应，从而实现集群中类似 Master/Slave 管理模式，关于 Zookeeper 的详细架构等内部细节可以阅读 Zookeeper 的源码&lt;/p&gt;

&lt;p&gt;下面详细介绍这些典型的应用场景，也就是 Zookeeper 到底能帮我们解决那些问题？下面将给出答案。&lt;/p&gt;

&lt;h2 id=&#34;统一命名服务-name-service&#34;&gt;统一命名服务（Name Service）&lt;/h2&gt;

&lt;p&gt;分布式应用中，通常需要有一套完整的命名规则，既能够产生唯一的名称又便于人识别和记住，通常情况下用树形的名称结构是一个理想的选择，树形的名称结构是一个有层次的目录结构，既对人友好又不会重复。说到这里你可能想到了 JNDI，没错 Zookeeper 的 Name Service 与 JNDI 能够完成的功能是差不多的，它们都是将有层次的目录结构关联到一定资源上，但是 Zookeeper 的 Name Service 更加是广泛意义上的关联，也许你并不需要将名称关联到特定资源上，你可能只需要一个不会重复名称，就像数据库中产生一个唯一的数字主键一样。&lt;/p&gt;

&lt;p&gt;Name Service 已经是 Zookeeper 内置的功能，你只要调用 Zookeeper 的 API 就能实现。如调用 create 接口就可以很容易创建一个目录节点。&lt;/p&gt;

&lt;h2 id=&#34;配置管理-configuration-management&#34;&gt;配置管理（Configuration Management）&lt;/h2&gt;

&lt;p&gt;配置的管理在分布式应用环境中很常见，例如同一个应用系统需要多台 PC Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 PC Server，这样非常麻烦而且容易出错。&lt;/p&gt;

&lt;p&gt;像这样的配置信息完全可以交给 Zookeeper 来管理，将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中.&lt;/p&gt;

&lt;h2 id=&#34;集群管理-group-membership&#34;&gt;集群管理（Group Membership）&lt;/h2&gt;

&lt;p&gt;Zookeeper 能够很容易的实现集群管理的功能，如有多台 Server 组成一个服务集群，那么必须要一个“总管”知道当前集群中每台机器的服务状态，一旦有机器不能提供服务，集群中其它集群必须知道，从而做出调整重新分配服务策略。同样当增加集群的服务能力时，就会增加一台或多台 Server，同样也必须让“总管”知道。&lt;/p&gt;

&lt;p&gt;Zookeeper 不仅能够帮你维护当前的集群中机器的服务状态，而且能够帮你选出一个“总管”，让这个总管来管理集群，这就是 Zookeeper 的另一个功能 Leader Election。&lt;/p&gt;

&lt;p&gt;它们的实现方式都是在 Zookeeper 上创建一个 EPHEMERAL 类型的目录节点，然后每个 Server 在它们创建目录节点的父目录节点上调用 getChildren(String path, boolean watch) 方法并设置 watch 为 true，由于是 EPHEMERAL 目录节点，当创建它的 Server 死去，这个目录节点也随之被删除，所以 Children 将会变化，这时 getChildren上的 Watch 将会被调用，所以其它 Server 就知道已经有某台 Server 死去了。新增 Server 也是同样的原理。&lt;/p&gt;

&lt;p&gt;Zookeeper 如何实现 Leader Election，也就是选出一个 Master Server。和前面的一样每台 Server 创建一个 EPHEMERAL 目录节点，不同的是它还是一个 SEQUENTIAL 目录节点，所以它是个 EPHEMERAL_SEQUENTIAL 目录节点。之所以它是 EPHEMERAL_SEQUENTIAL 目录节点，是因为我们可以给每台 Server 编号，我们可以选择当前是最小编号的 Server 为 Master，假如这个最小编号的 Server 死去，由于是 EPHEMERAL 节点，死去的 Server 对应的节点也被删除，所以当前的节点列表中又出现一个最小编号的节点，我们就选择这个节点为当前 Master。这样就实现了动态选择 Master，避免了传统意义上单 Master 容易出现单点故障的问题。&lt;/p&gt;

&lt;h2 id=&#34;zookeeper-实现-locks&#34;&gt;Zookeeper 实现 Locks&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;加锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ZooKeeper 将按照如下方式实现加锁的操作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ZooKeeper 调用 create （）方法来创建一个路径格式为“ &lt;em&gt;locknode&lt;/em&gt;/lock- ”的节点，此节点类型为sequence （连续）和 ephemeral （临时）。也就是说，创建的节点为临时节点，并且所有的节点连续编号，即“ lock-i ”的格式。&lt;/li&gt;
&lt;li&gt;在创建的锁节点上调用 getChildren （）方法，来获取锁目录下的最小编号节点，并且不设置 watch 。&lt;/li&gt;
&lt;li&gt;步骤 2 中获取的节点恰好是步骤 1 中客户端创建的节点，那么此客户端获得此种类型的锁，然后退出操作。&lt;/li&gt;
&lt;li&gt;客户端在锁目录上调用 exists （）方法，并且设置 watch 来监视锁目录下比自己小一个的连续临时节点的状态。&lt;/li&gt;
&lt;li&gt;如果监视节点状态发生变化，则跳转到第 2 步，继续进行后续的操作，直到退出锁竞争。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;解锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ZooKeeper 解锁操作非常简单，客户端只需要将加锁操作步骤 1 中创建的临时节点删除即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Locks extends TestMainClient {
    public static final Logger logger = Logger.getLogger(Locks.class);
    String myZnode;

    public Locks(String connectString, String root) {
        super(connectString);
        this.root = root;
        if (zk != null) {
            try {
                //创建锁节点，并不设置观察
                Stat s = zk.exists(root, false);
                if (s == null) {
                    zk.create(root, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
                }
            } catch (KeeperException e) {
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }
        }
    }
    void getLock() throws KeeperException, InterruptedException{
        List&amp;lt;String&amp;gt; list = zk.getChildren(root, false);
        String[] nodes = list.toArray(new String[list.size()]);
        //对锁目录下 的所有子节点排序
        Arrays.sort(nodes);
        //判断该zkclient创建的临时顺序节点是否为集群中最小的节点
        if(myZnode.equals(root+&amp;quot;/&amp;quot;+nodes[0])){
            doAction();
        }
        else{
            waitForLock(nodes[0]);
        }
    }
    //创建zk客户端的临时瞬时节点，并尝试获取锁
    void check() throws InterruptedException, KeeperException {
        myZnode = zk.create(root + &amp;quot;/lock_&amp;quot; , new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.EPHEMERAL_SEQUENTIAL);
        getLock();
    }
    void waitForLock(String lower) throws InterruptedException, KeeperException {
        Stat stat = zk.exists(root + &amp;quot;/&amp;quot; + lower,true);
        if(stat != null){   //发现最小的目录节点还未被移除，则等待
            mutex.wait();
        }
        else{
            getLock();
        }
    }
    @Override //发现有节点移除，该等待状态的客户端被notify
    public void process(WatchedEvent event) {
        if(event.getType() == Event.EventType.NodeDeleted){
            System.out.println(&amp;quot;得到通知&amp;quot;);
            super.process(event);
            doAction();
        }
    }
    /**
     * 执行其他任务
     */
    private void doAction(){
        System.out.println(&amp;quot;同步队列已经得到同步，可以开始执行后面的任务了&amp;quot;);
    }

    public static void main(String[] args) {
        TestMainServer.start();
        String connectString = &amp;quot;localhost:&amp;quot;+TestMainServer.CLIENT_PORT;

        Locks lk = new Locks(connectString, &amp;quot;/locks&amp;quot;);
        try {
            lk.check();
        } catch (InterruptedException e) {
            logger.error(e);
        } catch (KeeperException e) {
            logger.error(e);
        }
    }


}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;同步队列&#34;&gt;同步队列&lt;/h2&gt;

&lt;p&gt;同步队列用 Zookeeper 实现的实现思路如下：创建一个父目录 /synchronizing，每个成员都监控标志（Set Watch）位目录 /synchronizing/start 是否存在，然后每个成员都加入这个队列，加入队列的方式就是创建 /synchronizing/member_i 的临时目录节点，然后每个成员获取 / synchronizing 目录的所有目录节点，也就是 member_i。判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /synchronizing/start 的出现，如果已经相等就创建 /synchronizing/start。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列。
 */
public class Synchronizing extends TestMainClient {
    int size;
    String name;
    public static final Logger logger = Logger.getLogger(Synchronizing.class);

    /**
     * 构造函数
     *
     * @param connectString
     *            服务器连接
     * @param root
     *            根目录
     * @param size
     *            队列大小
     */
    Synchronizing(String connectString, String root, int size) {
        super(connectString);
        this.root = root;
        this.size = size;

        if (zk != null) {
            try {
                Stat s = zk.exists(root, false);
                if (s == null) {
                    zk.create(root, new byte[0], Ids.OPEN_ACL_UNSAFE,
                            CreateMode.PERSISTENT);
                }
            } catch (KeeperException e) {
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }
        }
        try {
            name = new String(InetAddress.getLocalHost().getCanonicalHostName()
                    .toString());
        } catch (UnknownHostException e) {
            logger.error(e);
        }

    }

    /**
     * 加入队列
     *
     * @return
     * @throws KeeperException
     * @throws InterruptedException
     */

    void addQueue() throws KeeperException, InterruptedException {
        zk.exists(root + &amp;quot;/start&amp;quot;, true);
        zk.create(root + &amp;quot;/&amp;quot; + name, new byte[0], Ids.OPEN_ACL_UNSAFE,
                CreateMode.EPHEMERAL_SEQUENTIAL);
        synchronized (mutex) {
            List&amp;lt;String&amp;gt; list = zk.getChildren(root, false);
            //如果队列中子节点数小于size，则等待，如果不小于size，则创建start目录，其他client则触发事件，执行doAction
            if (list.size() &amp;lt; size) {
                mutex.wait();
            } else {
                zk.create(root + &amp;quot;/start&amp;quot;, new byte[0], Ids.OPEN_ACL_UNSAFE,
                        CreateMode.PERSISTENT);
            }
        }
    }

    @Override
    public void process(WatchedEvent event) {
        if (event.getPath().equals(root + &amp;quot;/start&amp;quot;)
                &amp;amp;&amp;amp; event.getType() == Event.EventType.NodeCreated) {
            System.out.println(&amp;quot;得到通知&amp;quot;);
            super.process(event);
            doAction();
        }
    }

    /**
     * 执行其他任务
     */
    private void doAction() {
        System.out.println(&amp;quot;同步队列已经得到同步，可以开始执行后面的任务了&amp;quot;);
    }

    public static void main(String args[]) {
        // 启动Server
        TestMainServer.start();
        String connectString = &amp;quot;localhost:&amp;quot; + TestMainServer.CLIENT_PORT;
        int size = 1;
        Synchronizing b = new Synchronizing(connectString, &amp;quot;/synchronizing&amp;quot;,
                size);
        try {
            b.addQueue();
        } catch (KeeperException e) {
            logger.error(e);
        } catch (InterruptedException e) {
            logger.error(e);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fifo队列-生产者-消费者&#34;&gt;FIFO队列(生产者-消费者)&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 队列按照 FIFO 方式进行入队和出队操作，例如实现生产者和消费者模型。
 *
 * 实现的思路也非常简单，就是在特定的目录下创建 SEQUENTIAL 类型的子目录
 * /queue_i，这样就能保证所有成员加入队列时都是有编号的，出队列时通过 getChildren( )
 * 方法可以返回当前所有的队列中的元素，然后消费其中最小的一个，这样就能保证 FIFO。
 */
public class FIFOQueue extends TestMainClient {
    public static final Logger logger = Logger.getLogger(FIFOQueue.class);

    /**
     * Constructor
     *
     * @param connectString
     * @param root
     */
    FIFOQueue(String connectString, String root) {
        super(connectString);
        this.root = root;
        if (zk != null) {
            try {
                Stat s = zk.exists(root, false);
                if (s == null) {
                    zk.create(root, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,
                            CreateMode.PERSISTENT);
                }
            } catch (KeeperException e) {
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }
        }
    }

    /**
     * 生产者
     *
     * @param i
     * @return
     */

    boolean produce(int i) throws KeeperException, InterruptedException {
        ByteBuffer b = ByteBuffer.allocate(4);
        byte[] value;
        b.putInt(i);
        value = b.array();
        zk.create(root + &amp;quot;/element&amp;quot;, value, ZooDefs.Ids.OPEN_ACL_UNSAFE,
                CreateMode.PERSISTENT_SEQUENTIAL);
        return true;
    }

    /**
     * 消费者
     *
     * @return
     * @throws KeeperException
     * @throws InterruptedException
     */
    int consume() throws KeeperException, InterruptedException {
        int retvalue = -1;
        Stat stat = null;
        while (true) {
            synchronized (mutex) {
                // 对root的子节点设置监听
                List&amp;lt;String&amp;gt; list = zk.getChildren(root, true);
                // 如果没有任何子节点，则wait
                if (list.size() == 0) {
                    mutex.wait();
                } else {
                    Integer min = new Integer(list.get(0).substring(7));
                    for (String s : list) {
                        Integer tempValue = new Integer(s.substring(7));
                        if (tempValue &amp;lt; min)
                            min = tempValue;
                    }
                    byte[] b = zk.getData(root + &amp;quot;/element&amp;quot; + min, false, stat);
                    // 获取到子节点数据之后 执行delete，并触发事件，执行所有cliet的notify
                    zk.delete(root + &amp;quot;/element&amp;quot; + min, 0);
                    ByteBuffer buffer = ByteBuffer.wrap(b);
                    retvalue = buffer.getInt();
                    return retvalue;
                }
            }
        }
    }

    @Override
    public void process(WatchedEvent event) {
        super.process(event);
    }

    public static void main(String args[]) {
        // 启动Server
        TestMainServer.start();
        String connectString = &amp;quot;localhost:&amp;quot; + TestMainServer.CLIENT_PORT;

        FIFOQueue q = new FIFOQueue(connectString, &amp;quot;/app1&amp;quot;);
        int i;
        Integer max = new Integer(5);

        System.out.println(&amp;quot;Producer&amp;quot;);
        for (i = 0; i &amp;lt; max; i++)
            try {
                q.produce(10 + i);
            } catch (KeeperException e) {
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }

        for (i = 0; i &amp;lt; max; i++) {
            try {
                int r = q.consume();
                System.out.println(&amp;quot;Item: &amp;quot; + r);
            } catch (KeeperException e) {
                i--;
                logger.error(e);
            } catch (InterruptedException e) {
                logger.error(e);
            }
        }

    }
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus gpu_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/gpu_exporter/</link>
          <pubDate>Mon, 15 Apr 2019 19:21:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/gpu_exporter/</guid>
          <description>&lt;p&gt;随着区块链、人工智能的盛行，越来越多的场景开始使用GPU，而其监控也随之受到重视。目前生产环境中大部分GPU为NVIDIA厂商，今天就聊聊NVIDIA如何进行GPU的监控。&lt;/p&gt;

&lt;h1 id=&#34;nvml&#34;&gt;NVML&lt;/h1&gt;

&lt;p&gt;NVIDIA Management Library 是英伟达提供用于监控、管理GPU的API，底层是用C实现。我们常用的nvidia-smi命令也是基于该库进行的封装。官方同时也提供了perl、python版本的库。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@host104722317 ~]# nvidia-smi
Tue Sep  8 10:16:02 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P40           Off  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0    49W / 250W |   7083MiB / 22912MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P40           Off  | 00000000:88:00.0 Off |                    0 |
| N/A   31C    P0    50W / 250W |  12843MiB / 22912MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      8336      C   ./image_ds                                   193MiB |
|    0     13531      C   ./image_ds                                   193MiB |
|    0     17306      C   ./image_ds                                   193MiB |
|    0     36354      C   ...entCS/imagesearch/cs/cs01_self/image_cs  1809MiB |
|    0     37691      C   ...rc/trandeploy/imagesearch/as00/image_as  1561MiB |
|    0     37714      C   ...rc/trandeploy/imagesearch/as01/image_as  1561MiB |
|    0     37738      C   ...rc/trandeploy/imagesearch/as02/image_as  1561MiB |
|    1      8336      C   ./image_ds                                   411MiB |
|    1     13531      C   ./image_ds                                   411MiB |
|    1     17306      C   ./image_ds                                   411MiB |
|    1     36376      C   ...entCS/imagesearch/cs/cs11_self/image_cs  1809MiB |
|    1     37622      C   ...rc/trandeploy/imagesearch/as10/image_as  1561MiB |
|    1     37645      C   ...rc/trandeploy/imagesearch/as11/image_as  4401MiB |
|    1     37668      C   ...rc/trandeploy/imagesearch/as12/image_as  1561MiB |
|    1     38160      C   ...entDS/commentVideoDS/ds01_self/image_ds  1131MiB |
|    1     38181      C   ...entDS/commentVideoDS/ds11_self/image_ds  1131MiB |
+-----------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;监控项&#34;&gt;监控项&lt;/h1&gt;

&lt;p&gt;常规有如下几个监控项，基本可以覆盖大部分应用场景。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;温度&lt;/li&gt;
&lt;li&gt;GPU利用率&lt;/li&gt;
&lt;li&gt;内存总量&lt;/li&gt;
&lt;li&gt;已分配内存&lt;/li&gt;
&lt;li&gt;内存利用率
内存利用率跟已分配内存是有区别的，我理解只要有任务被分配到GPU就会分配内存，而分配的内存究竟有没有被使用才是内存利用率。根据官方的描述：内存利用率是指在每个采集周期内（可能在1/6至1秒）读写占用的时间百分比。&lt;/li&gt;
&lt;li&gt;电源使用情况&lt;/li&gt;
&lt;li&gt;风扇速度
风扇并不是所有GPU都有&lt;/li&gt;
&lt;li&gt;GPU数量&lt;/li&gt;
&lt;li&gt;GPU平均利用率&lt;/li&gt;
&lt;li&gt;GPU平均内存利用率&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;实现&#34;&gt;实现&lt;/h1&gt;

&lt;p&gt;需要借助第三方库github.com/mindprince/gonvml，利用cgo的特性直接使用官方的API。&lt;/p&gt;

&lt;p&gt;这个方案是在Linux环境下，依赖libnvidia-ml.so.1库。这个库主要包含2个文件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nvml.h的存在让我们不依赖于构建环境中存在的NVML。&lt;/li&gt;
&lt;li&gt;bindings.go是调用NVML函数的cgo桥。 bindings.go中的cgo前导代码使用dlopen动态加载NVML并使其功能可用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;详细的代码直接到github看就可以，不在这里贴出了。&lt;/p&gt;

&lt;p&gt;Agent相关代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package funcs

import (
    &amp;quot;log&amp;quot;

    &amp;quot;github.com/mindprince/gonvml&amp;quot;
    &amp;quot;github.com/open-falcon/falcon-plus/common/model&amp;quot;
)

// 需要load libnvidia-ml.so.1库
func GpuMetrics() (L []*model.MetricValue) {

    if err := gonvml.Initialize(); err != nil {
        log.Println(&amp;quot;Initialize error: &amp;quot;, err)
        return
    }

    defer gonvml.Shutdown()

    count, err := gonvml.DeviceCount()
    if err != nil {
        log.Println(&amp;quot;DeviceCount error: &amp;quot;, err)
        return
    }

    if count == 0 {
        return
    }

    temperature := uint(0)
    totalMemory := uint64(0)
    usedMemory := uint64(0)
    gpuUtilization := uint(0)
    memoryUtilization := uint(0)
    powerUsage := uint(0)
    allUtilization := uint(0)
    allMemoryUtilization := uint(0)

    for i := 0; i &amp;lt; int(count); i++ {
        dev, err := gonvml.DeviceHandleByIndex(uint(i))
        if err != nil {
            log.Println(&amp;quot;DeviceHandleByIndex error:&amp;quot;, err)
            continue
        }

        uuid, err := dev.UUID()
        if err != nil {
            log.Println(&amp;quot;dev.UUID error&amp;quot;, err)
        }

        tag := &amp;quot;uuid=&amp;quot; + uuid

        // 不是所有gpu都有风扇
        fanSpeed, err := dev.FanSpeed()
        if err != nil {
            log.Println(&amp;quot;dev.FanSpeed error: &amp;quot;, err)
        } else {
            L = append(L, GaugeValue(&amp;quot;gpu.fan.speed&amp;quot;, fanSpeed, tag))
        }

        temperature, err = dev.Temperature()
        if err != nil {
            log.Println(&amp;quot;dev.Temperature error: &amp;quot;, err)
            continue
        }

        totalMemory, usedMemory, err = dev.MemoryInfo()
        if err != nil {
            log.Println(&amp;quot;dev.MemoryInfo error: &amp;quot;, err)
            continue
        }

        // 单位换算为兆
        totalBillion := float64(totalMemory / 1024 / 1024)
        usedBillion := float64(usedMemory / 1024 / 1024)

        gpuUtilization, memoryUtilization, err = dev.UtilizationRates()
        if err != nil {
            log.Println(&amp;quot;dev.UtilizationRates error: &amp;quot;, err)
            continue
        }

        allUtilization += gpuUtilization
        allMemoryUtilization += memoryUtilization

        powerUsage, err = dev.PowerUsage()
        if err != nil {
            log.Println(&amp;quot;dev.PowerUsage error: &amp;quot;, err)
        }

        // 单位换算为瓦特
        powerWatt := float64(powerUsage / 1000)

        L = append(L, GaugeValue(&amp;quot;gpu.temperature&amp;quot;, temperature, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.memory.total&amp;quot;, totalBillion, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.memory.used&amp;quot;, usedBillion, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.memory.util&amp;quot;, memoryUtilization, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.util&amp;quot;, gpuUtilization, tag))
        L = append(L, GaugeValue(&amp;quot;gpu.power.usage&amp;quot;, powerWatt, tag))
    }

    L = append(L, GaugeValue(&amp;quot;gpu.count&amp;quot;, count))
    L = append(L, GaugeValue(&amp;quot;gpu.util.avg&amp;quot;, allUtilization/count))
    L = append(L, GaugeValue(&amp;quot;gpu.memory.util.avg&amp;quot;, allMemoryUtilization/count))
    return L
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Process Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/process_exporter/</link>
          <pubDate>Tue, 09 Apr 2019 16:44:29 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/process_exporter/</guid>
          <description>&lt;p&gt;Process-exporter 主要是对进程进行监控。&lt;/p&gt;

&lt;h1 id=&#34;安装使用&#34;&gt;安装使用&lt;/h1&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;wget https://github.com/ncabatoff/process-exporter/releases/download/v0.4.0/process-exporter-0.4.0.linux-amd64.tar.gz 
tar -xvf process-exporter-0.4.0.linux-amd64.tar.gz -C /usr/local/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置&#34;&gt;配置&lt;/h2&gt;

&lt;p&gt;选择要监视的进程并将它的分组，提供命令行参数或者使用YAML配置文件两种方法。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;整体模版&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;process_names:
  - matcher1
  - matcher2
  ...
  - matcherN
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;默认定义全部进程监控&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim conf.yaml
process_names:
  - name: &amp;quot;{{.Comm}}&amp;quot;
    cmdline:
    - &#39;.+&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;name 进程名，可以使用变量模版匹配&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;模版变量&lt;/p&gt;

&lt;p&gt;可用的模板变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{.Comm}} 包含原始可执行文件的basename，换句话说 在/proc/&amp;lt;pid&amp;gt;/stat
{{.ExeBase}} 包含可执行文件的basename，这个是name的默认值
{{.ExeFull}} 包含可执行文件的完全限定路径
{{.Username}} contains the username of the effective user
{{.Matches}} 映射包含应用命令行所产生的所有匹配项
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建配置文件定义进程名监控&lt;/p&gt;

&lt;p&gt;Process-exporter 可以进程名字匹配进程，获取进程信息。匹配规则由name对应的模板变量决定，以下表示监控进程名字为nginx 与 zombie 的进程状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim process-name.yaml
process_names:
  - name: &amp;quot;{{.Matches}}&amp;quot;
    cmdline:
    - &#39;nginx&#39;

  - name: &amp;quot;{{.Matches}}&amp;quot;
    cmdline:
    - &#39;zombie&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;进程选择器（process selectors）&lt;/p&gt;

&lt;p&gt;process_names 中的每个项必须包含一个或者多个选择器(comm，exe &amp;ndash;OR或者 cmdline&amp;ndash;AND) ；如果存在多个选择器，则它们都必须匹配&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;启动服务&#34;&gt;启动服务&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;./process-exporter -config.path process-name.yaml &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;查看数据&#34;&gt;查看数据&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;curl http://localhost:9256/metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;监控项&#34;&gt;监控项&lt;/h1&gt;

&lt;p&gt;常用进程监控项&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;总进程数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(namedprocess_namegroup_states)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;总僵尸进程数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(namedprocess_namegroup_states{state=&amp;quot;Zombie&amp;quot;})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;namedprocess_namegroup_都是以这个开头的&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;num_procs：Number of processes in this group&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;cpu_seconds_total：CPU usage，类似于node_cpu_seconds_total&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;read_bytes_total：io read&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;write_bytes_total：io write&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;major_page_faults_total：Number of major page faults based on /proc/[pid]/stat&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;minor_page_faults_total：Number of minor page faults based on /proc/[pid]/stat&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;context_switches_total：voluntary_ctxt_switches&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;memory_bytes：memory used&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;open_filedesc：Number of file descriptors，基于/proc/[pid]/fd&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;worst_fd_ratio：fd limit&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;oldest_start_time_seconds：the oldest process in the group started&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;num_threads：Sum of number of threads of all process in the group&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;states：Running, Sleeping, Waiting, Zombie, Other：Number of threads in the group&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有线程的一些指标，默认关闭，需要开启&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;读取/proc/[pid]/（stat，fd）等文件的数据进行解析来获取进程的相关状态和资源情况。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- Distributed Lock</title>
          <link>https://kingjcy.github.io/post/distributed/distributed-lock/</link>
          <pubDate>Wed, 03 Apr 2019 19:57:56 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/distributed-lock/</guid>
          <description>&lt;p&gt;锁可以分为正常的进程内锁和分布式的进程间的锁。&lt;/p&gt;

&lt;h1 id=&#34;正常锁&#34;&gt;正常锁&lt;/h1&gt;

&lt;p&gt;我们一般所说的锁，就是指单进程多线程的锁机制。在单进程中，如果有多个线程并发访问某个某个全局资源，存在并发修改的问题。如果要避免这个问题，我们需要对资源进行同步，同步其实就是可以加一个锁来保证同一时刻只有一个线程能操作这个资源。&lt;/p&gt;

&lt;p&gt;具体的锁可以看go的&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-mutex/&#34;&gt;锁&lt;/a&gt;,当然在golang中主要是使用channel来实现进程内通信和共享。&lt;/p&gt;

&lt;h1 id=&#34;分布式锁&#34;&gt;分布式锁&lt;/h1&gt;

&lt;p&gt;涉及到分布式环境，以集群为例，就是多个实例，也就是多个进程，而且这些进程完全可能不在同一个机器上。我们知道多线程可以共享父进程的资源，包括内存。所以多线程可以看见锁，但是多进程之间无法共享资源，甚至都不在一台机器上，所以这时候分布式环境下，就需要其他的方式来让所有进程都可以知道这个锁，来控制对全局资源的并发修改。&lt;/p&gt;

&lt;p&gt;为了解决分布式的问题，我们可以把这个锁放入所有进程都可以访问的地方，比如数据库，redis，memcached或者是zookeeper。这些也是目前实现分布式锁的主要实现方式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基于数据库表实现分布式锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们可以将我们分布式要操作的资源都定义成表，然后对表进行查询数据，如果查到了没有数据，可以进行update，否则，说明该锁被其他线程持有，还没有释放&lt;/p&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;p&gt;更新之前会多一次查询，增加了数据库的操作&lt;/p&gt;

&lt;p&gt;数据库链接资源宝贵，如果并发量太大，数据库的性能有影响&lt;/p&gt;

&lt;p&gt;如果单个数据库存在单点问题，所以最好是高可用的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基于Redis实现分布式锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;通过Redis的setnx key命令，如果不存在某个key就设置值，设置成功表示获取锁。&lt;/p&gt;

&lt;p&gt;缺点：如果设置成功后，还没有释放锁，对应的业务节点就挂掉了，那么这时候锁就没有释放。其他业务节点也无法获取这个锁。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用setnx设置命令成功后，则使用expire命令设置到期时间，就算业务节点还没有释放锁就挂掉了，但是我们还是可以保证这个锁到期就会释放。&lt;/p&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;p&gt;setnx 和 expire不是原子操作，即设置了setnx还没有来得及设置到期时间，业务节点就挂了。&lt;/p&gt;

&lt;p&gt;而且key到期了，业务节点业务还没有执行完，怎么办？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用set命令 我们知道set命令格式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set key value [EX seconds] [PX milliseconds][NX|XX]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即首先可以根据这个key不存在，则设置值，即使用NX。然后可以设置到期时间，EX表示秒数，PX表示毫秒数，这个操作就是原子性的，解决了上述问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set id EX 10 NX
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;zookeeper，memcached&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;利用Memcached的add命令。此命令是原子性操作，只有在key不存在的情况下，才能add成功，也就意味着线程得到了锁。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;利用Zookeeper的顺序临时节点，来实现分布式锁和等待队列。Zookeeper设计的初衷，就是为了实现分布式锁服务的。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;1、简单实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;time&amp;quot;

    &amp;quot;github.com/garyburd/redigo/redis&amp;quot;
)

type Lock struct {
    resource string
    token    string
    conn     redis.Conn
    timeout  int
}

func (lock *Lock) tryLock() (ok bool, err error) {
    _, err = redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(lock.timeout), &amp;quot;NX&amp;quot;))
    if err == redis.ErrNil {
        // The lock was not successful, it already exists.
        return false, nil
    }
    if err != nil {
        return false, err
    }
    return true, nil
}

func (lock *Lock) Unlock() (err error) {
    _, err = lock.conn.Do(&amp;quot;del&amp;quot;, lock.key())
    return
}

func (lock *Lock) key() string {
    return fmt.Sprintf(&amp;quot;redislock:%s&amp;quot;, lock.resource)
}

func (lock *Lock) AddTimeout(ex_time int64) (ok bool, err error) {
    ttl_time, err := redis.Int64(lock.conn.Do(&amp;quot;TTL&amp;quot;, lock.key()))
    fmt.Println(ttl_time)
    if err != nil {
        log.Fatal(&amp;quot;redis get failed:&amp;quot;, err)
    }
    if ttl_time &amp;gt; 0 {
        fmt.Println(11)
        _, err := redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(ttl_time+ex_time)))
        if err == redis.ErrNil {
            return false, nil
        }
        if err != nil {
            return false, err
        }
    }
    return false, nil
}

func TryLock(conn redis.Conn, resource string, token string, DefaulTimeout int) (lock *Lock, ok bool, err error) {
    return TryLockWithTimeout(conn, resource, token, DefaulTimeout)
}

func TryLockWithTimeout(conn redis.Conn, resource string, token string, timeout int) (lock *Lock, ok bool, err error) {
    lock = &amp;amp;Lock{resource, token, conn, timeout}

    ok, err = lock.tryLock()

    if !ok || err != nil {
        lock = nil
    }

    return
}

func main() {
    fmt.Println(&amp;quot;start&amp;quot;)
    DefaultTimeout := 10
    conn, err := redis.Dial(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:6379&amp;quot;)

    lock, ok, err := TryLock(conn, &amp;quot;xiaoru.cc&amp;quot;, &amp;quot;token&amp;quot;, int(DefaultTimeout))
    if err != nil {
        log.Fatal(&amp;quot;Error while attempting lock&amp;quot;)
    }
    if !ok {
        log.Fatal(&amp;quot;Lock&amp;quot;)
    }
    lock.AddTimeout(100)

    time.Sleep(time.Duration(DefaultTimeout) * time.Second)
    fmt.Println(&amp;quot;end&amp;quot;)
    defer lock.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段golang代码运行后的正常结果是:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run lock.go
start
10
11
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果同时起多个进程去测试，会遇到这么一个结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run lock.go
start
2016/03/23 01:23:22 Lock
exit status 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、promes告警事件进行加锁&lt;/p&gt;

&lt;p&gt;后台是分布式的admin程序，分布在几台机器上，同时去消费kafka中的告警信息事件，对于同一个id的有着不同的事件，对于同一个id可能在不同的实例进行事件的处理，比如一个发生事件，一个恢复事件，这个时候就要使用分布式锁了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set id EX 10 NX
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;redis分布式锁实现乐观锁-悲观锁&#34;&gt;Redis分布式锁实现乐观锁、悲观锁&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;乐观锁的实现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;乐观锁实现中的锁就是商品的键值对。使用jedis的watch方法监视商品键值对，如果事务提交exec时发现监视的键值对发生变化，事务将被取消，商品数目不会被改动。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1).multi，开启Redis的事务，置客户端为事务态。

2).exec，提交事务，执行从multi到此命令前的命令队列，置客户端为非事务态。

3).discard，取消事务，置客户端为非事务态。

4).watch,监视键值对，作用时如果事务提交exec时发现监视的监视对发生变化，事务将被取消。
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;悲观锁实现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;悲观锁中的锁是一个唯一标识的锁lockKey和该锁的过期时间。首先确定缓存中有商品，然后在拿数据(商品数目改动)之前先获取到锁，之后对商品数目进行减一操作，操作完成释放锁，一个秒杀操作完成。这个锁是基于redis的setNX操作实现的阻塞式分布式锁。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Rocketmq</title>
          <link>https://kingjcy.github.io/post/middleware/mq/rocketmq/</link>
          <pubDate>Thu, 28 Mar 2019 21:26:44 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/rocketmq/</guid>
          <description>&lt;p&gt;RocketMQ 一个纯java、分布式、队列模型的开源消息中间件，是阿里巴巴在2012年开源的分布式消息中间件，目前已经捐赠给 Apache 软件基金会，并于2017年9月25日成为 Apache 的顶级项目。作为经历过多次阿里巴巴双十一这种“超级工程”的洗礼并有稳定出色表现的国产中间件，以其高性能、低延时和高可靠等特性近年来已经也被越来越多的国内企业使用。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;RocketMQ具有以下特点：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;灵活可扩展性&lt;/p&gt;

&lt;p&gt;RocketMQ 天然支持集群，其核心四组件（Name Server、Broker、Producer、Consumer）每一个都可以在没有单点故障的情况下进行水平扩展。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;海量消息堆积能力&lt;/p&gt;

&lt;p&gt;RocketMQ 采用零拷贝原理实现超大的消息的堆积能力，据说单机已可以支持亿级消息堆积，而且在堆积了这么多消息后依然保持写入低延迟。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;支持顺序消息&lt;/p&gt;

&lt;p&gt;可以保证消息消费者按照消息发送的顺序对消息进行消费。顺序消息分为全局有序和局部有序，一般推荐使用局部有序，即生产者通过将某一类消息按顺序发送至同一个队列来实现。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多种消息过滤方式&lt;/p&gt;

&lt;p&gt;消息过滤分为在服务器端过滤和在消费端过滤。服务器端过滤时可以按照消息消费者的要求做过滤，优点是减少不必要消息传输，缺点是增加了消息服务器的负担，实现相对复杂。消费端过滤则完全由具体应用自定义实现，这种方式更加灵活，缺点是很多无用的消息会传输给消息消费者。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;支持事务消息&lt;/p&gt;

&lt;p&gt;RocketMQ 除了支持普通消息，顺序消息之外还支持事务消息，这个特性对于分布式事务来说提供了又一种解决思路。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;回溯消费&lt;/p&gt;

&lt;p&gt;回溯消费是指消费者已经消费成功的消息，由于业务上需求需要重新消费，RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒，可以向前回溯，也可以向后回溯。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多客户端&lt;/p&gt;

&lt;p&gt;RocketMQ 目前支持 Java、C++、Go 三种语言访问。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;通过下面的基本部署图，我们看一下一些概念&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/rocketmq/20180328.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、生产者(Producer）&lt;/p&gt;

&lt;p&gt;生产者（Producer）负责产生消息，生产者向消息服务器发送由业务应用程序系统生成的消息。 RocketMQ 提供了三种方式发送消息：同步、异步和单向。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;同步发送&lt;/p&gt;

&lt;p&gt;同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。一般用于重要通知消息，例如重要通知邮件、营销短信。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;异步发送&lt;/p&gt;

&lt;p&gt;异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包，一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;单向发送&lt;/p&gt;

&lt;p&gt;单向发送是指只负责发送消息而不等待服务器回应且没有回调函数触发，适用于某些耗时非常短但对可靠性要求并不高的场景，例如日志收集。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2、生产者组（Producer Group）&lt;/p&gt;

&lt;p&gt;生产者组（Producer Group）是一类 Producer 的集合，这类 Producer 通常发送一类消息并且发送逻辑一致，所以将这些 Producer 分组在一起。从部署结构上看生产者通过 Producer Group 的名字来标记自己是一个集群。&lt;/p&gt;

&lt;p&gt;3、消费者（Consumer&lt;/p&gt;

&lt;p&gt;消费者（Consumer）负责消费消息，消费者从消息服务器拉取信息并将其输入用户应用程序。站在用户应用的角度消费者有两种类型：拉取型消费者、推送型消费者。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;拉取型消费者&lt;/p&gt;

&lt;p&gt;拉取型消费者（Pull Consumer）主动从消息服务器拉取信息，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型。这是第一代的模式，典型代表包括Notify、Napoli。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;推送型消费者&lt;/p&gt;

&lt;p&gt;推送型消费者（Push Consumer）封装了消息的拉取、消费进度和其他的内部维护工作，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型，但从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器，当监听器处触发后才开始消费消息。这是第二代的模式，典型代表MetaQ。&lt;/p&gt;

&lt;p&gt;最后第三代是以拉模式为主，兼有推模式低延迟消息引擎RocketMQ，在二代功能特性的基础上，为电商金融领域添加了可靠重试、基于文件存储的分布式事务等特性。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;4、消费者组（Consumer Group）&lt;/p&gt;

&lt;p&gt;消费者组（Consumer Group）一类 Consumer 的集合名称，这类 Consumer 通常消费同一类消息并且消费逻辑一致，所以将这些 Consumer 分组在一起。消费者组与生产者组类似，都是将相同角色的分组在一起并命名，分组是个很精妙的概念设计，RocketMQ 正是通过这种分组机制，实现了天然的消息负载均衡。消费消息时通过 Consumer Group 实现了将消息分发到多个消费者服务器实例，比如某个 Topic 有9条消息，其中一个 Consumer Group 有3个实例（3个进程或3台机器），那么每个实例将均摊3条消息，这也意味着我们可以很方便的通过加机器来实现水平扩展。&lt;/p&gt;

&lt;p&gt;5、消息服务器（Broker）&lt;/p&gt;

&lt;p&gt;消息服务器（Broker）是消息存储中心，主要作用是接收来自 Producer 的消息并存储， Consumer 从这里取得消息。它还存储与消息相关的元数据，包括用户组、消费进度偏移量、队列信息等。从部署结构图中可以看出 Broker 有 Master 和 Slave 两种类型，Master 既可以写又可以读，Slave 不可以写只可以读。从物理结构上看 Broker 的集群部署方式有四种：单 Master 、多 Master 、多 Master 多 Slave（同步刷盘）、多 Master多 Slave（异步刷盘）。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;单 Master&lt;/p&gt;

&lt;p&gt;这种方式一旦 Broker 重启或宕机会导致整个服务不可用，这种方式风险较大，所以显然不建议线上环境使用。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多 Master&lt;/p&gt;

&lt;p&gt;所有消息服务器都是 Master ，没有 Slave 。这种方式优点是配置简单，单个 Master 宕机或重启维护对应用无影响。缺点是单台机器宕机期间，该机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受影响。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多 Master 多 Slave（异步复制）&lt;/p&gt;

&lt;p&gt;每个 Master 配置一个 Slave，所以有多对 Master-Slave，消息采用异步复制方式，主备之间有毫秒级消息延迟。这种方式优点是消息丢失的非常少，且消息实时性不会受影响，Master 宕机后消费者可以继续从 Slave 消费，中间的过程对用户应用程序透明，不需要人工干预，性能同多 Master 方式几乎一样。缺点是 Master 宕机时在磁盘损坏情况下会丢失极少量消息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多 Master 多 Slave（同步双写）&lt;/p&gt;

&lt;p&gt;每个 Master 配置一个 Slave，所以有多对 Master-Slave ，消息采用同步双写方式，主备都写成功才返回成功。这种方式优点是数据与服务都没有单点问题，Master 宕机时消息无延迟，服务与数据的可用性非常高。缺点是性能相对异步复制方式略低，发送消息的延迟会略高。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;6、名称服务器（NameServer）&lt;/p&gt;

&lt;p&gt;名称服务器（NameServer）用来保存 Broker 相关元信息并给 Producer 和 Consumer 查找 Broker 信息。NameServer 被设计成几乎无状态的，可以横向扩展，节点之间相互之间无通信，通过部署多台机器来标记自己是一个伪集群。每个 Broker 在启动的时候会到 NameServer 注册，Producer 在发送消息前会根据 Topic 到 NameServer 获取到 Broker 的路由信息，Consumer 也会定时获取 Topic 的路由信息。所以从功能上看应该是和 ZooKeeper 差不多，据说 RocketMQ 的早期版本确实是使用的 ZooKeeper ，后来改为了自己实现的 NameServer 。&lt;/p&gt;

&lt;p&gt;7、消息（Message）&lt;/p&gt;

&lt;p&gt;消息（Message）就是要传输的信息。一条消息必须有一个主题（Topic），主题可以看做是你的信件要邮寄的地址。一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 key 并在 Broker 上查找此消息以便在开发期间查找问题。&lt;/p&gt;

&lt;p&gt;8、主题（Topic）&lt;/p&gt;

&lt;p&gt;主题（Topic）可以看做消息的规类，它是消息的第一级类型。比如一个电商系统可以分为：交易消息、物流消息等，一条消息必须有一个 Topic 。Topic 与生产者和消费者的关系非常松散，一个 Topic 可以有0个、1个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。一个 Topic 也可以被 0个、1个、多个消费者订阅。&lt;/p&gt;

&lt;p&gt;9、标签（Tag）&lt;/p&gt;

&lt;p&gt;标签（Tag）可以看作子主题，它是消息的第二级类型，用于为用户提供额外的灵活性。使用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识。比如交易消息又可以分为：交易创建消息、交易完成消息等，一条消息可以没有 Tag 。标签有助于保持您的代码干净和连贯，并且还可以为 RocketMQ 提供的查询系统提供帮助。&lt;/p&gt;

&lt;p&gt;10、消息队列（Message Queue）&lt;/p&gt;

&lt;p&gt;消息队列（Message Queue），主题被划分为一个或多个子主题，即消息队列。一个 Topic 下可以设置多个消息队列，发送消息时执行该消息的 Topic ，RocketMQ 会轮询该 Topic 下的所有队列将消息发出去。下图 Broker 内部消息情况：&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/rocketmq/rocketmq&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实在上面已经看过架构图了，我们对集群工作流程做一个整理&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动Namesrv，Namesrv起来后监听端口，等待Broker、Produer、Consumer连上来，相当于一个路由控制中心。&lt;/li&gt;
&lt;li&gt;Broker启动，跟所有的Namesrv保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有topic信息。注册成功后，namesrv集群中就有Topic跟Broker的映射关系。&lt;/li&gt;
&lt;li&gt;收发消息前，先创建topic，创建topic时需要指定该topic要存储在哪些Broker上。也可以在发送消息时自动创建Topic。&lt;/li&gt;
&lt;li&gt;Producer发送消息，启动时先跟Namesrv集群中的其中一台建立长连接，并从Namesrv中获取当前发送的Topic存在哪些Broker上，然后跟对应的Broker建长连接，直接向Broker发消息。&lt;/li&gt;
&lt;li&gt;Consumer跟Producer类似。跟其中一台Namesrv建立长连接，获取当前订阅Topic存在哪些Broker，然后直接跟Broker建立连接通道，开始消费消息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;大体的流程就是这样，其实基本原理也就清楚了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;消息消费模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;消息消费模式有两种：集群消费（Clustering）和广播消费（Broadcasting）。默认情况下就是集群消费，该模式下一个消费者集群共同消费一个主题的多个队列，一个队列只会被一个消费者消费，如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。而广播消费消息会发给消费者组中的每一个消费者进行消费。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;消息顺序&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;消息顺序（Message Order）有两种：顺序消费（Orderly）和并行消费（Concurrently）。顺序消费表示消息消费的顺序同生产者为每个消息队列发送的顺序一致，所以如果正在处理全局顺序是强制性的场景，需要确保使用的主题只有一个消息队列。并行消费不再保证消息顺序，消费的最大并行数量受每个消费者客户端指定的线程池限制。&lt;/p&gt;

&lt;h1 id=&#34;应用&#34;&gt;应用&lt;/h1&gt;

&lt;p&gt;其实也就是一般消息队列的的基本应用。&lt;/p&gt;

&lt;p&gt;1.削峰填谷&lt;/p&gt;

&lt;p&gt;比如如秒杀等大型活动时会带来较高的流量脉冲，如果没做相应的保护，将导致系统超负荷甚至崩溃。如果因限制太过导致请求大量失败而影响用户体验，可以利用MQ 超高性能的消息处理能力来解决。&lt;/p&gt;

&lt;p&gt;2.异步解耦&lt;/p&gt;

&lt;p&gt;通过上、下游业务系统的松耦合设计，比如：交易系统的下游子系统（如积分等）出现不可用甚至宕机，都不会影响到核心交易系统的正常运转。&lt;/p&gt;

&lt;p&gt;3.顺序消息&lt;/p&gt;

&lt;p&gt;与FIFO原理类似，MQ提供的顺序消息即保证消息的先进先出，可以应用于交易系统中的订单创建、支付、退款等流程。&lt;/p&gt;

&lt;p&gt;4.分布式事务消息&lt;/p&gt;

&lt;p&gt;比如阿里的交易系统、支付红包等场景需要确保数据的最终一致性，需要引入 MQ 的分布式事务，既实现了系统之间的解耦，又可以保证最终的数据一致性。&lt;/p&gt;

&lt;p&gt;将大事务拆分成小事务，减少系统间的交互，既高效又可靠。再利用MQ 的可靠传输与多副本技术确保消息不丢，At-Least-Once 特性来最终确保数据的最终一致性。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- M3db</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/</link>
          <pubDate>Wed, 13 Mar 2019 17:13:10 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/</guid>
          <description>&lt;p&gt;Uber开发了指标平台M3和分布式时间序列数据库M3DB。来解决Uber在发展过程当中遇到的问题：使用开源软件后，因为可靠性，成本等问题，在操做密集型方面没法大规模使用这些开源软件。因此Uber逐步构建了本身的指标平台。咱们利用经验来帮助咱们构建本地分布式时间序列数据库，高度动态和高性能的聚合服务，查询引擎以及其余支持基础架构。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;M3包括了以下的组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;M3DB &amp;ndash; M3db是一个使用TSDB（时间数据库），保存全部Prometheus指标，M3db是分布式，高可用性和复制的数据库，它使用Etcd做为共识算法。&lt;/li&gt;
&lt;li&gt;M3Coordinator &amp;ndash; 是Prometheus实例与M3db之间的适配器，它公开了Prometheus用来从数据库中推送和提取数据的读/写端点。&lt;/li&gt;
&lt;li&gt;M3Query &amp;ndash; 众所周知，Prometheus努力处理显示大量数据的查询，而不是从Prometheus提取数据，M3Query实现了相同的PromQL并能够响应此类请求。&lt;/li&gt;
&lt;li&gt;M3Aggregator &amp;ndash; 可选但很重要，此服务将下降指标的采样率，为长期存储作好准备。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总体架构图以下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/m3/m3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关于M3，咱们目前积累了一些生产实践。目前的问题是，社区不够活跃，文档也不够丰富。不少时候遇到问题，只能去研究代码。M3query对PromSql支持的不够，因此M3query并不能生产环境使用。&lt;/p&gt;

&lt;h1 id=&#34;调研&#34;&gt;调研&lt;/h1&gt;

&lt;p&gt;首先，我们想把大量的数据存储到m3中，给prometheus进行查询告警。但是数据量很大，m3db数据插入性能是有必要进行保证的。&lt;/p&gt;

&lt;p&gt;之前写过一个adapter，将数据转化为json调用json api将数据已经插入了m3db中，当时对数据性能没有要求，这次使用json api进行压测的时候，发现性能很差，而且在数据并发达到100个goroutine，一个goroutine发送100条数据，m3db就崩溃了，不接受连接。然后简单的测试了一下得到以下的数据&lt;/p&gt;

&lt;p&gt;这个远远达不到要求啊，于是看看有没有批量操作的接口，官方文档说明，有两种方式插入数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Test RPC
To test out some of the functionality of M3DB there are some user friendly HTTP JSON APIs that you can use. These use the DB node cluster service endpoints.

Note: performance sensitive users are expected to use the more performant endpoints via either the Go src/dbnode/client/Session API, or the GRPC endpoints exposed via src/coordinator.
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;go api（src/dbnode/client/Session），session看代码使用的是apache的thrift rpc。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GRPC（src/coordinator）&lt;/p&gt;

&lt;p&gt;官方提供了benchmark（src/query/benchmark），于是去编译进行测试，但是m3开源的太差了，很多第三方库都是使用的老版本，兼容性很差，各种api对不上，也不把自己的vendor包一同开源，踩了许多坑：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;github.com/thrift &amp;mdash;0.10.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/uber-go/tally&amp;mdash;3.3.7&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;client_golang&amp;mdash;0.8.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/coreos/etcd&amp;mdash;&amp;ndash;3.2.0，还是缺少参数，坑&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;google.golang.org/grpc&amp;ndash;laster&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;golang.org/x/text&amp;mdash;&amp;mdash;laster&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/satori/go.uuid&amp;mdash;-1.2.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/couchbase/vellum&amp;mdash;&amp;ndash;master&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/pilosa/pilosa-最新班都缺少参数&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;还是没有编译完成，后续持续跟进，看看官方有没有继续开源和改进。&lt;/p&gt;

&lt;p&gt;目前得到以下结论&lt;/p&gt;

&lt;p&gt;m3db目前没有发现批量处理的方式&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Coordinator的方式最后还是一条一条的发送（通过查看代码，未能运行）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;session方式，其中一个包github.com/pilosa/pilosa/roaring目前最新版本都没有m3中使用的参数，最终无法编译使用。网上使用session运行成功的，我未能找到他使用了什么版本的github.com/pilosa/pilosa/roaring包。但是通过他运行的结果来看（结合代码api），也是一条一条发送的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;m3中很多都是老版本的库，兼容性很差，api很多不兼容，官方也未推出他使用了什么库，如果继续，应该需要大量的时间去校验和编译&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>算法思想系列---- Raft</title>
          <link>https://kingjcy.github.io/post/algorithm/raft/</link>
          <pubDate>Tue, 12 Mar 2019 16:56:08 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/algorithm/raft/</guid>
          <description>&lt;p&gt;Raft 实际上是一个一致性算法的一种实现，和Paxos等价，但是在实现上，简化了一些，并且更加易用。&lt;/p&gt;

&lt;h1 id=&#34;一致性-consensus&#34;&gt;一致性（consensus）&lt;/h1&gt;

&lt;p&gt;分布式存储系统通常通过维护多个副本来进行容错，提高系统的可用性。要实现此目标，就必须要解决分布式存储系统的最核心问题：维护多个副本的一致性。&lt;/p&gt;

&lt;p&gt;首先需要解释一下什么是一致性（consensus）,它是构建具有容错性（fault-tolerant）的分布式系统的基础。 在一个具有一致性的性质的集群里面，同一时刻所有的结点对存储在其中的某个值都有相同的结果，即对其共享的存储保持一致。集群具有自动恢复的性质，当少数结点失效的时候不影响集群的正常工作，当大多数集群中的结点失效的时候，集群则会停止服务（不会返回一个错误的结果）。&lt;/p&gt;

&lt;p&gt;一致性协议就是用来干这事的，用来保证即使在部分(确切地说是小部分)副本宕机的情况下，系统仍然能正常对外提供服务。一致性协议通常基于replicated state machines，即所有结点都从同一个state出发，都经过同样的一些操作序列（log），最后到达同样的state。&lt;/p&gt;

&lt;p&gt;对于一致性，一致的程度不同大体可以分为强、弱、最终一致性三类。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;强一致性: 对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。比如小明更新V0到V1，那么小华读取的时候也应该是V1。&lt;/li&gt;
&lt;li&gt;弱一致性: 如果能容忍后续的部分或者全部访问不到，则是弱一致性。比如小明更新VO到V1，可以容忍那么小华读取的时候是V0。&lt;/li&gt;
&lt;li&gt;最终一致性: 如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。比如小明更新VO到V1，可以使得小华在一段时间之后读取的时候是V0。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;raft协议实现的是多副本数据的强一致性。&lt;/p&gt;

&lt;h1 id=&#34;rsm复制状态机-replicated-state-machine&#34;&gt;RSM复制状态机(replicated state machine)&lt;/h1&gt;

&lt;p&gt;上面讲到了RSM，我们先来了解一下，一个分布式的复制状态机系统由多个复制单元组成，每个复制单元均是一个状态机，它的状态保存在一组状态变量中，状态机的变量只能通过外部命令来改变。简单理解的话，可以想象成是一组服务器，每个服务器是一个状态机，服务器的运行状态只能通过一行行的命令来改变。每一个状态机存储一个包含一系列指令的日志，严格按照顺序逐条执行日志中的指令，如果所有的状态机都能按照相同的日志执行指令，那么它们最终将达到相同的状态。因此，在复制状态机模型下，只要保证了操作日志的一致性，我们就能保证该分布式系统状态的一致性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在上图中，服务器中的一致性模块(Consensus Modle)接受来自客户端的指令，并写入到自己的日志中，然后通过一致性模块和其他服务器交互，确保每一条日志都能以相同顺序写入到其他服务器的日志中，即便服务器宕机了一段时间。一旦日志命令都被正确的复制，每一台服务器就会顺序的处理命令，并向客户端返回结果。&lt;/p&gt;

&lt;p&gt;系统中每个结点有三个组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;状态机: 当我们说一致性的时候，实际就是在说要保证这个状态机的一致性。状态机会从log里面取出所有的命令，然后执行一遍，得到的结果就是我们对外提供的保证了一致性的数据&lt;/li&gt;
&lt;li&gt;Log: 保存了所有修改记录&lt;/li&gt;
&lt;li&gt;一致性模块: 一致性模块算法就是用来保证写入的log的命令的一致性，这也是raft算法核心内容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了让一致性协议变得简单可理解，Raft协议主要使用了两种策略。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一是将复杂问题进行分解，在Raft协议中，一致性问题被分解为：leader election、log replication、safety三个简单问题；&lt;/li&gt;
&lt;li&gt;二是减少状态空间中的状态数目。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我们详细看一下Raft协议是怎样设计的。&lt;/p&gt;

&lt;h1 id=&#34;基础概念&#34;&gt;基础概念&lt;/h1&gt;

&lt;h2 id=&#34;状态&#34;&gt;状态&lt;/h2&gt;

&lt;p&gt;Raft协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Leader：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步至多个其他副本；&lt;/li&gt;
&lt;li&gt;Follower：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件&lt;/li&gt;
&lt;li&gt;Candidate：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;集群刚启动时，所有节点都是follower，之后在time out信号的驱使下，follower会转变成candidate去拉取选票，获得大多数选票后就会成为leader，这时候如果其他候选人发现了新的leader已经诞生，就会自动转变为follower；而如果另一个time out信号发出时，还没有选举出leader，将会重新开始一次新的选举。可见，time out信号是促使角色转换得关键因素，类似于操作系统中得中断信号。&lt;/p&gt;

&lt;h2 id=&#34;term&#34;&gt;term&lt;/h2&gt;

&lt;p&gt;在Raft协议中，将时间分成了一些任意长度的时间片，称为term，term使用连续递增的编号的进行识别，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每一个term都从新的选举开始，candidate们会努力争取称为leader。一旦获胜，它就会在剩余的term时间内保持leader状态，在某些情况下(如term3)选票可能被多个candidate瓜分，形不成多数派，因此term可能直至结束都没有leader，下一个term很快就会到来重新发起选举。&lt;/p&gt;

&lt;p&gt;term也起到了系统中逻辑时钟的作用，每一个server都存储了当前term编号，在server之间进行交流的时候就会带有该编号，如果一个server的编号小于另一个的，那么它会将自己的编号更新为较大的那一个；如果leader或者candidate发现自己的编号不是最新的了，就会自动转变为follower；如果接收到的请求的term编号小于自己的当前term将会拒绝执行。&lt;/p&gt;

&lt;p&gt;其实就是这个server处于什么时间段的状态，然后用于对比，来处理当前的状态。&lt;/p&gt;

&lt;h2 id=&#34;传输协议&#34;&gt;传输协议&lt;/h2&gt;

&lt;p&gt;server之间的交流是通过RPC进行的。只需要实现两种RPC就能构建一个基本的Raft集群：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RequestVote RPC：它由选举过程中的candidate发起，用于拉取选票&lt;/li&gt;
&lt;li&gt;AppendEntries RPC：它由leader发起，用于复制日志或者发送心跳信号。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft4.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;h2 id=&#34;leader-election&#34;&gt;Leader election&lt;/h2&gt;

&lt;p&gt;Raft通过心跳机制发起leader选举。节点都是从follower状态开始的，如果收到了来自leader或candidate的RPC，那它就保持follower状态，避免争抢成为candidate。Leader会发送空的AppendEntries RPC作为心跳信号来确立自己的地位，如果follower一段时间(election timeout)没有收到心跳，它就会认为leader已经挂了，发起新的一轮选举。&lt;/p&gt;

&lt;p&gt;选举发起后&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Follower将自己维护的current_term_id加1。&lt;/li&gt;
&lt;li&gt;然后将自己的状态转成Candidate&lt;/li&gt;
&lt;li&gt;它会首先投自己一票，然后发送RequestVoteRPC消息(带上current_term_id) 给 其它所有server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个过程会有三种结果：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自己被选成了主。当收到了majority的投票后，状态切成Leader，并且定期给其它的所有server发心跳消息（不带log的AppendEntriesRPC）以告诉对方自己是current_term_id所标识的term的leader。每个term最多只有一个leader，term id作为logical clock，在每个RPC消息中都会带上，用于检测过期的消息。当一个server收到的RPC消息中的rpc_term_id比本地的current_term_id更大时，就更新current_term_id为rpc_term_id，并且如果当前state为leader或者candidate时，将自己的状态切成follower。如果rpc_term_id比本地的current_term_id更小，则拒绝这个RPC消息。&lt;/li&gt;
&lt;li&gt;别人成为了主。如1所述，当Candidator在等待投票的过程中，收到了大于或者等于本地的current_term_id的声明对方是leader的AppendEntriesRPC时，则将自己的state切成follower，并且更新本地的current_term_id。&lt;/li&gt;
&lt;li&gt;没有选出主。当投票被平均瓜分，没有任何一个candidate收到了majority的vote时，没有leader被选出。这种情况下，每个candidate等待的投票的过程就超时了，接着candidates都会将本地的current_term_id再加1，发起RequestVoteRPC进行新一轮的leader election。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;投票策略：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个节点只会给每个term投一票，具体的是否同意和后续的Safety有关。&lt;/li&gt;
&lt;li&gt;当投票被瓜分后，所有的candidate同时超时，然后有可能进入新一轮的票数被瓜分，为了避免这个问题，Raft采用一种很简单的方法：每个Candidate的election timeout从150ms-300ms之间随机取，那么第一个超时的Candidate就可以发起新一轮的leader election，带着最大的term_id给其它所有server发送RequestVoteRPC消息，从而自己成为leader，然后给他们发送心跳消息以告诉他们自己是主。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;log-replication&#34;&gt;Log Replication&lt;/h2&gt;

&lt;p&gt;一旦leader被选举成功，就可以对客户端提供服务了。客户端提交每一条命令都会被按顺序记录到leader的日志中，每一条命令都包含term编号和顺序索引的结构体log entry，然后向其他节点并行发送AppendEntries RPC用以复制命令(如果命令丢失会不断重发)，当复制成功也就是大多数节点成功复制后，leader就会提交命令，即执行该命令并且将执行结果返回客户端，raft保证已经提交的命令最终也会被其他节点成功执行。leader会保存有当前已经提交的最高日志编号。顺序性确保了相同日志索引处的命令是相同的，而且之前的命令也是相同的。当发送AppendEntries RPC时，会包含leader上一条刚处理过的命令，接收节点如果发现上一条命令不匹配，就会拒绝执行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft19&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;日志冲突&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在这个过程中可能会出现一种特殊故障：如果leader崩溃了，它所记录的日志没有完全被复制，会造成日志不一致的情况，follower相比于当前的leader可能会丢失几条日志，也可能会额外多出几条日志，这种情况可能会持续几个term。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在上图中，框内的数字是term编号，a、b丢失了一些命令，c、d多出来了一些命令，e、f既有丢失也有增多，这些情况都有可能发生。比如f可能发生在这样的情况下：f节点在term2时是leader，在此期间写入了几条命令，然后再提交之前崩溃了，在之后的term3种它很快重启并再次成为leader，又写入了几条日志，在提交之前又崩溃了，等他苏醒过来时新的leader来了，就形成了上图情形。&lt;/p&gt;

&lt;p&gt;因此，需要有一种机制来让leader和follower对log达成一致，在Raft中，leader通过强制follower复制自己的日志来解决上述日志不一致的情形，那么冲突的日志将会被重写。为了让日志一致，先找到最新的一致的那条日志(如f中索引为3的日志条目)，然后把follower之后的日志全部删除，leader再把自己在那之后的日志一股脑推送给follower，这样就实现了一致。而寻找该条日志，可以通过AppendEntries RPC，该RPC中包含着下一次要执行的命令索引，如果能和follower的当前索引对上，那就执行，否则拒绝，然后leader将会逐次递减索引，直到找到相同的那条日志。&lt;/p&gt;

&lt;p&gt;leader会为每个follower维护一个nextIndex，表示leader给各个follower发送的下一条log entry在log中的index，初始化为leader的最后一条log entry的下一个位置。leader给follower发送AppendEntriesRPC消息，带着(term_id, (nextIndex-1))， term_id即(nextIndex-1)这个槽位的log entry的term_id，follower接收到AppendEntriesRPC后，会从自己的log中找是不是存在这样的log entry，如果不存在，就给leader回复拒绝消息，然后leader则将nextIndex减1，再重复，知道AppendEntriesRPC消息被接收。&lt;/p&gt;

&lt;p&gt;然而这样也还是会有问题，比如某个follower在leader提交时宕机了，也就是少了几条命令，然后它又经过选举成了新的leader，这样它就会强制其他follower跟自己一样，使得其他节点上刚刚提交的命令被删除，导致客户端提交的一些命令被丢失了，下面一节内容将会解决这个问题。Raft通过为选举过程添加一个限制条件，解决了上面提出的问题，该限制确保leader包含之前term已经提交过的所有命令。Raft通过投票过程确保只有拥有全部已提交日志的candidate能成为leader。由于candidate为了拉选票需要通过RequestVote RPC联系其他节点，而之前提交的命令至少会存在于其中某一个节点上,因此只要candidate的日志至少和其他大部分节点的一样新就可以了, follower如果收到了不如自己新的candidate的RPC,就会将其丢弃.&lt;/p&gt;

&lt;p&gt;还可能会出现另外一个问题, 如果命令已经被复制到了大部分节点上,但是还没来的及提交就崩溃了,这样后来的leader应该完成之前term未完成的提交. Raft通过让leader统计当前term内还未提交的命令已经被复制的数量是否半数以上, 然后进行提交.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;日志压缩&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;随着日志大小的增长，会占用更多的内存空间，处理起来也会耗费更多的时间，对系统的可用性造成影响，因此必须想办法压缩日志大小。Snapshotting是最简单的压缩方法，系统的全部状态会写入一个snapshot保存起来，然后丢弃截止到snapshot时间点之前的所有日志。Raft中的snapshot内容如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每一个server都有自己的snapshot，它只保存当前状态，如上图中的当前状态为x=0,y=9，而last included index和last included term代表snapshot之前最新的命令，用于AppendEntries的状态检查。&lt;/p&gt;

&lt;p&gt;Snapshot中包含以下内容：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日志元数据，最后一条commited log entry的 (log index, last_included_term)。这两个值在Snapshot之后的第一条log entry的AppendEntriesRPC的consistency check的时候会被用上，之前讲过。一旦这个server做完了snapshot，就可以把这条记录的最后一条log index及其之前的所有的log entry都删掉。&lt;/li&gt;
&lt;li&gt;系统状态机：存储系统当前状态（这是怎么生成的呢？）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;虽然每一个server都保存有自己的snapshot，但是当follower严重落后于leader时，leader需要把自己的snapshot发送给follower加快同步，此时用到了一个新的RPC：InstallSnapshot RPC。follower收到snapshot时，需要决定如何处理自己的日志，如果收到的snapshot包含有更新的信息，它将丢弃自己已有的日志，按snapshot更新自己的状态，如果snapshot包含的信息更少，那么它会丢弃snapshot中的内容，但是自己之后的内容会保存下来。RPC的定义如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;safety&#34;&gt;Safety&lt;/h2&gt;

&lt;p&gt;哪些follower有资格成为leader?&lt;/p&gt;

&lt;p&gt;Raft保证被选为新leader的节点拥有所有已提交的log entry，这与ViewStamped Replication不同，后者不需要这个保证，而是通过其他机制从follower拉取自己没有的提交的日志记录
这个保证是在RequestVoteRPC阶段做的，candidate在发送RequestVoteRPC时，会带上自己的最后一条日志记录的term_id和index，其他节点收到消息时，如果发现自己的日志比RPC请求中携带的更新，拒绝投票。日志比较的原则是，如果本地的最后一条log entry的term id更大，则更新，如果term id一样大，则日志更多的更大(index更大)。&lt;/p&gt;

&lt;p&gt;哪些日志记录被认为是commited?&lt;/p&gt;

&lt;p&gt;leader正在replicate当前term（即term 2）的日志记录给其它Follower，一旦leader确认了这条log entry被majority写盘了，这条log entry就被认为是committed。如图a，S1作为当前term即term2的leader，log index为2的日志被majority写盘了，这条log entry被认为是commited
leader正在replicate更早的term的log entry给其它follower。图b的状态是这么出来的。&lt;/p&gt;

&lt;p&gt;对协议的一点修正&lt;/p&gt;

&lt;p&gt;在实际的协议中，需要进行一些微调，这是因为可能会出现下面这种情况：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft9&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在阶段a，term为2，S1是Leader，且S1写入日志（term, index）为(2, 2)，并且日志被同步写入了S2；&lt;/li&gt;
&lt;li&gt;在阶段b，S1离线，触发一次新的选主，此时S5被选为新的Leader，此时系统term为3，且写入了日志（term, index）为（3， 2）;&lt;/li&gt;
&lt;li&gt;S5尚未将日志推送到Followers变离线了，进而触发了一次新的选主，而之前离线的S1经过重新上线后被选中变成Leader，此时系统term为4，此时S1会将自己的日志同步到Followers，按照上图就是将日志（2， 2）同步到了S3，而此时由于该日志已经被同步到了多数节点（S1, S2, S3），因此，此时日志（2，2）可以被commit了（即更新到状态机）；&lt;/li&gt;
&lt;li&gt;在阶段d，S1又很不幸地下线了，系统触发一次选主，而S5有可能被选为新的Leader（这是因为S5可以满足作为主的一切条件：1. term = 3 &amp;gt; 2, 2. 最新的日志index为2，比大多数节点（如S2/S3/S4的日志都新），然后S5会将自己的日志更新到Followers，于是S2、S3中已经被提交的日志（2，2）被截断了，这是致命性的错误，因为一致性协议中不允许出现已经应用到状态机中的日志被截断。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了避免这种致命错误，需要对协议进行一个微调：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;只允许主节点提交包含当前term的日志
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;针对上述情况就是：即使日志（2，2）已经被大多数节点（S1、S2、S3）确认了，但是它不能被Commit，因为它是来自之前term(2)的日志，直到S1在当前term（4）产生的日志（4， 3）被大多数Follower确认，S1方可Commit（4，3）这条日志，当然，根据Raft定义，（4，3）之前的所有日志也会被Commit。此时即使S1再下线，重新选主时S5不可能成为Leader，因为它没有包含大多数节点已经拥有的日志（4，3）。&lt;/p&gt;

&lt;h1 id=&#34;实战&#34;&gt;实战&lt;/h1&gt;

&lt;h2 id=&#34;raft在etcd中的实现&#34;&gt;raft在etcd中的实现&lt;/h2&gt;

&lt;p&gt;etcd 是一个被广泛应用于共享配置和服务发现的分布式、一致性的 kv 存储系统。作为分布式 kv，其底层使用的 是 raft 算法来实现多副本数据的强一致复制，etcd-raft 作为 raft 开源实现的杰出代表，在设计上，将 raft 算法逻辑和持久化、网络、线程等完全抽离出来单独实现，充分解耦，在工程上，实现了诸多性能优化，是 raft 开源实践中较早的工业级的实现，很多后来的 raft 实践者都直接或者间接的参考了 ectd-raft 的设计和实现，算是 raft 实现的一个典范。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft10&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;功能支持：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Election（vote）：选举&lt;/li&gt;
&lt;li&gt;Pre-vote：在发起 election vote 之前，先进行 pre-vote，可以避免在网络分区的情况避免反复的 election 打断当前 leader，触发新的选举造成可用性降低的问题&lt;/li&gt;
&lt;li&gt;Config changes：配置变更，增加，删除节点等&lt;/li&gt;
&lt;li&gt;Leaner：leaner 角色，仅参与 log replication，不参与投票和提交的 Op log entry，增加节点时，复制追赶 使用 leader 角色&lt;/li&gt;
&lt;li&gt;Transfer leader：主动变更 Leader，用于关机维护，leader 负载等&lt;/li&gt;
&lt;li&gt;ReadIndex：优化 raft read 走 Op log 性能问题，每次 read Op，仅记录 commit index，然后发送所有 peers heartbeat 确认 leader 身份，如果 leader 身份确认成功，等到 applied index &amp;gt;= commit index，就可以返回 client read 了&lt;/li&gt;
&lt;li&gt;Lease read：通过 lease 保证 leader 的身份，从而省去了 ReadIndex 每次 heartbeat 确认 leader 身份，性能更好，但是通过时钟维护 lease 本身并不是绝对的安全&lt;/li&gt;
&lt;li&gt;snapshot：raft 主动生成 snapshot，实现 log compact 和加速启动恢复，install snapshot 实现给 follower 拷贝数据等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从功能上，etcd-raft 完备的实现了 raft 几乎所需的功能。&lt;/p&gt;

&lt;p&gt;性能优化：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Batch：网络batch发送、batch持久化 Op log entries到WAL&lt;/li&gt;
&lt;li&gt;Pipeline：Leader 向 Follower 发送 Message 可以 pipeline 发送的（相对的 ping-pong 模式发送和接收）（pipeline 是grpc的一重要特性）&lt;/li&gt;
&lt;li&gt;Append Log Parallelly：Leader 发送 Op log entries message 给 Followers 和 Leader 持久化 Op log entries 是并行的&lt;/li&gt;
&lt;li&gt;Asynchronous Apply：由单独的 coroutine（协程） 负责异步的 Apply&lt;/li&gt;
&lt;li&gt;Asynchronous GC：WAL 和 snapshot 文件会分别开启单独的 coroutine 进行 GC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;etcd-raft 几乎实现了 raft 大论文和工程上该有的性能优化，实际上 ReadIndex 和 Lease Read 本身也算是性能优化。&lt;/p&gt;

&lt;h3 id=&#34;架构设计&#34;&gt;架构设计&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft11&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上图，整个 etcd-server 的整体架构，其主要分为三层：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;网络层&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;图中最上面一层是网络层，负责使用 grpc 收发 etcd-raft 和 client 的各种 messages，etcd-raft 会通过网络层收发各种 message，包括 raft append entries、vote、client 发送过来的 request，以及 response 等等，其都是由 rpc 的 coroutine 完成（PS：可以简单理解所有 messages 都是通过网络模块异步收发的）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;持久化层&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;图中最下面一层是持久化层，其提供了对 raft 各种数据的持久化存储，WAL - 持久化 raft Op log entries；Snapshot - 持久化 raft snapshot；KV - raft apply 的数据就是写入 kv 存储中，因为 etcd 是一个分布式的 kv 存储，所以，对 raft 来说，applied 的数据自然也就是写入到 kv 中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Raft 层&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;中间这一层就是 raft 层，也是 etcd-raft 的核心实现。etcd 设计上将 raft 算法的逻辑和持久化、网络、线程（实际上是 coroutine）等完全解耦成一个单独的模块，拍脑袋思考下，将网络、持久化层抽离出来，并不难，但是如何将 raft 算法逻辑和网络、持久化、coroutine 完成解耦呢 ？&lt;/p&gt;

&lt;p&gt;其核心的思路就是将 raft 所有算法逻辑实现封装成一个 StateMachine，也就是图中的 raft StateMachine，注意和 raft 复制状态机区别，这里 raft StateMachine 只是对 raft 算法的多个状态 （Leader、Follower、Candidate 等），多个阶段的一种代码实现，类似网络处理实现中也会通过一个 StateMachine 来实现网络 message 异步不同阶段的处理。&lt;/p&gt;

&lt;p&gt;为了更加形象，这里以 client 发起 一个 put kv request 为例子，来看看 raft StateMachine 的输入、运转和输出，这里以 Leader 为例，分为如下阶段：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一阶段：client 发送一个 put kv request 给 etcd server，grpc server 解析后，生成一个 Propose Message作为 raft StateMachine 输入，如果你驱动 raft StateMachine 运转，就会生成两个输出，一个需要写入 WAL 的 Op log entry，2 条发送给另外两个副本的 Append entries Msg，输出会封装在 Ready 结构中&lt;/li&gt;
&lt;li&gt;第二阶段：如果把第一阶段的输出 WAL 写到了盘上，并且把 Append entries Msg 发送给了其他两个副本，那么两个副本会收到 Append entries Msg，持久化之后就会给 Leader 返回 Append entries Response Msg，etcd server 收到 Msg 之后，依然作为输入交给 raft StateMachine 处理，驱动 StateMachine 运转，如果超过大多数 response，那么就会产生输出：已经 commit 的 committed entries&lt;/li&gt;
&lt;li&gt;第三阶段：外部将上面 raft StateMachine 输出 committed entries 拿到后，然后就可以返回 client put kv success 的 response 了&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上面的例子展示了 raft StateMachine 输入，输出，运转的情况，尽管已经有了网络层和持久化层，但是，显然还缺少很多其的模块，例如：coroutine 驱动状态机运转，coroutine 将驱动网络发 message 和 持久化写盘等，下面介绍的raft 层的三个小模块就是完成这些事情的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;raft StateMachine&lt;/p&gt;

&lt;p&gt;就是一个 raft 算法的逻辑实现，其输入统一被抽象成了 Msg，输出则统一封装在 Ready 结构中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;node（raft StateMachine 接口 - 输入+运转）&lt;/p&gt;

&lt;p&gt;node 模块提供了如下功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;raft StateMachine 和外界交互的接口，就是提供ectd使用一致性协议算法的接口，供上层向 raft StateMachine 提交 request，也就是输入，已上面例子的 put kv request 为例，就是通过func (n *node) Propose(ctx context.Context, data []byte)接口向 raft StateMachine 提交一个 Propose，这个接口将用户请求转换成 raft StateMachine 认识的 MsgProp Msg，并通过 Channel 传递给驱动 raft StateMachine 运转的 coroutine；&lt;/li&gt;
&lt;li&gt;提供驱动 raft 运转的 coroutine，其负责监听在各个 Msg 输入 Channel 中，一旦收到 Msg 就会调用 raft StateMachine 处理 Msg 接口 func (r *raft) Step(m pb.Message) 得到输出 Ready 结构，并将 Channel 传递给其他 coroutine 处理&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;raftNode（处理 raft StateMachine 输出 Ready）&lt;/p&gt;

&lt;p&gt;raftNode 模块会有一个 coroutine，负责从处理 raft StateMachine 的输出 Ready 结构，该持久化的调用持久化的接口持久化，该发送其他副本的，通过网络接口发送给其他副本，该 apply 的提交给其他 coroutine apply。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;交互架构&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft17&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其中红色虚线框起来的代表一个 coroutine，下面将对各个协程的作用基本的描述&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ticker：golang 的 Ticker struct 会定期触发 Tick 滴答时钟，etcd raft 的都是通过滴答时钟往前推进，从而触发相应的 heartbeat timeout 和 election timeout，从而触发发送心跳和选举。&lt;/li&gt;
&lt;li&gt;ReadLoop：这个 coroutine 主要负责处理 Read request，负责将 Read 请求通过 node 模块的 Propose 提交给 raft StateMachine，然后监听 raft StateMachine，一旦 raft StateMachine 完成 read 请求的处理，会通过 readStateC 通知 ReadLoop coroutine 此 read 的commit index，然后 ReadLoop coroutine 就可以根据当前 applied index 的推进情况，一旦 applied index &amp;gt;= commit index，ReadLoop coroutine 就会 Read 数据并通过网络返回 client read response&lt;/li&gt;
&lt;li&gt;raftNode：raftNode 模块会有一个 coroutine 负责处理 raft StateMachine 的输出 Ready，上文已经描述了，这里不在赘述&lt;/li&gt;
&lt;li&gt;node：node 模块也会有一个 coroutine 负责接收输入，运行状态机和准备输出，上文已经描述，这里不在赘述&lt;/li&gt;
&lt;li&gt;apply：raftNode 模块在 raft StateMachine 输出 Ready 中已经 committed entries 的时候，会将 apply 逻辑放在单独的 coroutine 处理，这就是 Async apply。&lt;/li&gt;
&lt;li&gt;GC：WAL 和 snapshot 的 GC 回收也都是分别在两个单独的 coroutine 中完成的。etcd 会在配置文中分别设置 WAL 和 snapshot 文件最大数量，然后两个 GC 后台异步 GC&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;代码分析&#34;&gt;代码分析&lt;/h3&gt;

&lt;p&gt;Etcd将raft协议实现为一个library，然后本身作为一个应用使用它。当然，可能是为了推广它所实现的这个library，etcd还额外提供了一个叫&lt;a href=&#34;https://github.com/etcd-io/etcd/tree/v3.3.10/contrib/raftexample&#34;&gt;raftexample&lt;/a&gt;的示例程序，向用户展示怎样在它所提供的raft library的基础上构建出一个分布式的KV存储引擎。&lt;/p&gt;

&lt;p&gt;我们来看看raft library，其实也就是raft层的实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tree --dirsfirst -L 1 -I &#39;*test*&#39; -P &#39;*.go&#39;
.
├── raftpb
├── doc.go
├── log.go
├── log_unstable.go
├── logger.go
├── node.go
├── progress.go
├── raft.go
├── rawnode.go
├── read_only.go
├── status.go
├── storage.go
└── util.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来详细说明&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;raftpb&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Raft中的序列化是借助于Protocol Buffer来实现的，这个文件夹就定义了需要序列化的几个数据结构，比如Entry和Message。&lt;/p&gt;

&lt;p&gt;1、Entry&lt;/p&gt;

&lt;p&gt;从整体上来说，一个集群中的每个节点都是一个状态机，而raft管理的就是对这个状态机进行更改的一些操作，这些操作在代码中被封装为一个个Entry。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/raftpb/raft.pb.go#L203
type Entry struct {
    Term             uint64
    Index            uint64
    Type             EntryType
    Data             []byte
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Term：选举任期，每次选举之后递增1。它的主要作用是标记信息的时效性，比方说当一个节点发出来的消息中携带的term是2，而另一个节点携带的term是3，那我们就认为第一个节点的信息过时了。&lt;/li&gt;
&lt;li&gt;Index：当前这个entry在整个raft日志中的位置索引。有了Term和Index之后，一个log entry就能被唯一标识。&lt;/li&gt;
&lt;li&gt;Type：当前entry的类型，目前etcd支持两种类型：EntryNormal和EntryConfChange，EntryNormal代表当前Entry是对状态机的操作，EntryConfChange则代表对当前集群配置进行更改的操作，比如增加或者减少节点。&lt;/li&gt;
&lt;li&gt;Data：一个被序列化后的byte数组，代表当前entry真正要执行的操作，比方说如果上面的Type是EntryNormal，那这里的Data就可能是具体要更改的key-value pair，如果Type是EntryConfChange，那Data就是具体的配置更改项ConfChange。raft算法本身并不关心这个数据是什么，它只是把这段数据当做log同步过程中的payload来处理，具体对这个数据的解析则有上层应用来完成。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、Message&lt;/p&gt;

&lt;p&gt;Raft集群中节点之间的通讯都是通过传递不同的Message来完成的，这个Message结构就是一个非常general的大容器，它涵盖了各种消息所需的字段。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/raftpb/raft.pb.go#L239
type Message struct {
    Type             MessageType
    To               uint64
    From             uint64
    Term             uint64
    LogTerm          uint64
    Index            uint64
    Entries          []Entry
    Commit           uint64
    Snapshot         Snapshot
    Reject           bool
    RejectHint       uint64
    Context          []byte
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Type：当前传递的消息类型，它的取值有很多个，但大致可以分成两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Raft 协议相关的，包括心跳MsgHeartbeat、日志MsgApp、投票消息MsgVote等。&lt;/li&gt;
&lt;li&gt;上层应用触发的（没错，上层应用并不是通过api与raft库交互的，而是通过发消息），比如应用对数据更改的消息MsgProp(osal)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所有的 Msg 在 bp.Message 中详细定义，下面给出所有的 message 类型并且依次介绍：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    const (
        MsgHup            MessageType = 0   // 本地消息：选举，可能会触发 pre-vote 或者 vote
        MsgBeat           MessageType = 1   // 本地消息：心跳，触发放给 peers 的 Msgheartbeat
        MsgProp           MessageType = 2   // 本地消息：Propose，触发 MsgApp
        MsgApp            MessageType = 3   // 非本地：Op log 复制/配置变更 request
        MsgAppResp        MessageType = 4   // 非本地：Op log 复制 response
        MsgVote           MessageType = 5   // 非本地：vote request
        MsgVoteResp       MessageType = 6   // 非本地：vote response
        MsgSnap           MessageType = 7   // 非本地：Leader 向 Follower 拷贝 Snapshot，response Message 就是 MsgAppResp，通过这个值告诉 Leader 继续复制后面的日志
        MsgHeartbeat      MessageType = 8   // 非本地：心跳 request
        MsgHeartbeatResp  MessageType = 9   // 非本地：心跳 response
        MsgUnreachable    MessageType = 10  // 本地消息：EtcdServer 通过这个消息告诉 raft 状态某个 Follower 不可达，让其发送 message方式由 pipeline 切成 ping-pong 模式
        MsgSnapStatus     MessageType = 11  // 本地消息：EtcdServer 通过这个消息告诉 raft 状态机 snapshot 发送成功还是失败
        MsgCheckQuorum    MessageType = 12  // 本地消息：CheckQuorum，用于 Lease read，Leader lease
        MsgTransferLeader MessageType = 13  // 本地消息：可能会触发一个空的 MsgApp 尽快完成日志复制，也有可能是 MsgTimeoutNow 出 Transferee 立即进入选举
        MsgTimeoutNow     MessageType = 14  // 非本地：触发 Transferee 立即进行选举
        MsgReadIndex      MessageType = 15  // 非本地：Read only ReadIndex
        MsgReadIndexResp  MessageType = 16  // 非本地：Read only ReadIndex response
        MsgPreVote        MessageType = 17  // 非本地：pre vote request
        MsgPreVoteResp    MessageType = 18  // 非本地：pre vote response
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不同类型的消息会用到下面不同的字段：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To, From分别代表了这个消息的接受者和发送者。&lt;/li&gt;
&lt;li&gt;Term：这个消息发出时整个集群所处的任期。&lt;/li&gt;
&lt;li&gt;LogTerm：消息发出者所保存的日志中最后一条的任期号，一般MsgVote会用到这个字段。&lt;/li&gt;
&lt;li&gt;Index：日志索引号。如果当前消息是MsgVote的话，代表这个candidate最后一条日志的索引号，它跟上面的LogTerm一起代表这个candidate所拥有的最新日志信息，这样别人就可以比较自己的日志是不是比candidata的日志要新，从而决定是否投票。&lt;/li&gt;
&lt;li&gt;Entries：需要存储的日志。&lt;/li&gt;
&lt;li&gt;Commit：已经提交的日志的索引值，用来向别人同步日志的提交信息。&lt;/li&gt;
&lt;li&gt;Snapshot：一般跟MsgSnap合用，用来放置具体的Snapshot值。&lt;/li&gt;
&lt;li&gt;Reject，RejectHint：代表对方节点拒绝了当前节点的请求(MsgVote/MsgApp/MsgSnap…)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;log_unstable.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;log_unstable顾名思义，unstable数据结构用于还没有被用户层持久化的数据，它维护了两部分内容snapshot和entries:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/log_unstable.go#L23
type unstable struct {
    // the incoming unstable snapshot, if any.
    snapshot *pb.Snapshot
    // all entries that have not yet been written to storage.
    entries []pb.Entry
    offset  uint64

    logger Logger
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;entries代表的是要进行操作的日志&lt;/li&gt;
&lt;li&gt;snapshot代表快照数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两者的关系是相辅相成的，共同组成了全部的数据，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft13&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里的前半部分是快照数据，而后半部分是日志条目组成的数组entries，另外unstable.offset成员保存的是entries数组中的第一条数据在raft日志中的索引，即第i条entries在raft日志中的索引为i + unstable.offset。在同步的时候，如果entries日志不能完全同步，就需要使用到snapshot。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;storage.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个文件定义了一个Storage接口，因为etcd中的raft实现并不负责数据的持久化，所以它希望上面的应用层能实现这个接口，以便提供给它查询log的能力。&lt;/p&gt;

&lt;p&gt;另外，这个文件也提供了Storage接口的一个内存版本的实现MemoryStorage，这个实现同样也维护了snapshot和entries这两部分，他们的排列跟unstable中的类似，也是snapshot在前，entries在后。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;log.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边主要实现raftLog，这个结构体承担了raft日志相关的操作。&lt;/p&gt;

&lt;p&gt;raftLog由以下成员组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;storage Storage：前面提到的存放已经持久化数据的Storage接口。&lt;/li&gt;
&lt;li&gt;unstable unstable：前面分析过的unstable结构体，用于保存应用层还没有持久化的数据。&lt;/li&gt;
&lt;li&gt;committed uint64：保存当前提交的日志数据索引。&lt;/li&gt;
&lt;li&gt;applied uint64：保存当前传入状态机的数据最高索引。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;raftLog结构体中，几部分数据的排列如下图所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft14&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个数据排布的情况，可以从raftLog的初始化函数中看出来：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/log.go#L45
// newLog returns log using the given storage. It recovers the log to the state
// that it just commits and applies the latest snapshot.
func newLog(storage Storage, logger Logger) *raftLog {
    if storage == nil {
        log.Panic(&amp;quot;storage must not be nil&amp;quot;)
    }
    log := &amp;amp;raftLog{
        storage: storage,
        logger:  logger,
    }
    firstIndex, err := storage.FirstIndex()
    if err != nil {
        panic(err) // TODO(bdarnell)
    }
    lastIndex, err := storage.LastIndex()
    if err != nil {
        panic(err) // TODO(bdarnell)
    }
    log.unstable.offset = lastIndex + 1
    log.unstable.logger = logger
    // Initialize our committed and applied pointers to the time of the last compaction.
    log.committed = firstIndex - 1
    log.applied = firstIndex - 1

    return log
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从这里的代码可以看出，raftLog的两部分，持久化存储和非持久化存储，它们之间的分界线就是lastIndex，在此之前都是Storage管理的已经持久化的数据，而在此之后都是unstable管理的还没有持久化的数据。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;progress.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Leader通过Progress这个数据结构来追踪一个follower的状态，并根据Progress里的信息来决定每次同步的日志项。这里介绍三个比较重要的属性：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/progress.go#L37
// Progress represents a follower’s progress in the view of the leader. Leader maintains
// progresses of all followers, and sends entries to the follower based on its progress.
type Progress struct {
    Match, Next uint64

    State ProgressStateType

    ins *inflights
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;用来保存当前follower节点的日志状态的属性

&lt;ul&gt;
&lt;li&gt;Match：保存目前为止，已复制给该follower的日志的最高索引值。如果leader对该follower上的日志情况一无所知的话，这个值被设为0。&lt;/li&gt;
&lt;li&gt;Next：保存下一次leader发送append消息给该follower的日志索引，即下一次复制日志时，leader会从Next开始发送日志。
在正常情况下，Next = Match + 1，也就是下一个要同步的日志应当是对方已有日志的下一条。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;State属性用来保存该节点当前的同步状态，它会有一下几种取值

&lt;ul&gt;
&lt;li&gt;ProgressStateProbe
探测状态，当follower拒绝了最近的append消息时，那么就会进入探测状态，此时leader会试图继续往前追溯该follower的日志从哪里开始丢失的。在probe状态时，leader每次最多append一条日志，如果收到的回应中带有RejectHint信息，则回退Next索引，以便下次重试。在初始时，leader会把所有follower的状态设为probe，因为它并不知道各个follower的同步状态，所以需要慢慢试探。&lt;/li&gt;
&lt;li&gt;ProgressStateReplicate
当leader确认某个follower的同步状态后，它就会把这个follower的state切换到这个状态，并且用pipeline的方式快速复制日志。leader在发送复制消息之后，就修改该节点的Next索引为发送消息的最大索引+1。&lt;/li&gt;
&lt;li&gt;ProgressStateSnapshot
接收快照状态。当leader向某个follower发送append消息，试图让该follower状态跟上leader时，发现此时leader上保存的索引数据已经对不上了，比如leader在index为10之前的数据都已经写入快照中了，但是该follower需要的是10之前的数据，此时就会切换到该状态下，发送快照给该follower。当快照数据同步追上之后，并不是直接切换到Replicate状态，而是首先切换到Probe状态。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ins属性用来做流量控制，因为如果同步请求非常多，再碰上网络分区时，leader可能会累积很多待发送消息，一旦网络恢复，可能会有非常大流量发送给follower，所以这里要做flow control。它的实现有点类似TCP的滑动窗口，这里不再赘述。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Progress其实也是个状态机，下面是它的状态转移图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft15&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;raft.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;raft协议的具体实现就在这个文件里。这其中，大部分的逻辑是由Step函数驱动的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/raft.go#L752
func (r *raft) Step(m pb.Message) error {
    //...
    switch m.Type {
        case pb.MsgHup:
        //...
        case pb.MsgVote, pb.MsgPreVote:
        //...
        default:
            r.step(r, m)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Step的主要作用是处理不同的消息,调用不同的处理函数，step属性是一个函数指针，根据当前节点的不同角色，指向不同的消息处理函数：stepLeader/stepFollower/stepCandidate。与它类似的还有一个tick函数指针，根据角色的不同，也会在tickHeartbeat和tickElection之间来回切换，分别用来触发定时心跳和选举检测。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft16&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;node.go&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;node其实就是消息通道，or-select-channel组成的事件循环。&lt;/p&gt;

&lt;p&gt;node的主要作用是应用层（etcdserver）和共识模块（raft）的衔接。将应用层的消息传递给底层共识模块，并将底层共识模块共识后的结果反馈给应用层。所以它的初始化函数创建了很多用来通信的channel，然后就在另一个goroutine里面开始了事件循环，不停的在各种channel中倒腾数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/node.go#L286
for {
  select {
    case m := &amp;lt;-propc:
        r.Step(m)
    case m := &amp;lt;-n.recvc:
        r.Step(m)
    case cc := &amp;lt;-n.confc:
        // Add/remove/update node according to cc.Type
    case &amp;lt;-n.tickc:
        r.tick()
    case readyc &amp;lt;- rd:
        // Cleaning after result is consumed by application
    case &amp;lt;-advancec:
        // Stablize logs
    case c := &amp;lt;-n.status:
        // Update status
    case &amp;lt;-n.stop:
        close(n.done)
        return
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;输入（msg）：propc和recvc中拿到的是从上层应用传进来的消息，这个消息会被交给raft层的Step函数处理。&lt;/p&gt;

&lt;p&gt;所有的外部处理请求经过 raft StateMachine 处理都会首先被转换成统一抽象的输入 Message（Msg），Msg 会通过 raft.Step(m) 接口完成 raft StateMachine 的处理，Msg 分两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;本地 Msg，term = 0，这种 Msg 并不会经过网络发送给 Peer，只是将 Node 接口的一些请求转换成 raft StateMachine 统一处理的抽象 Msg，这里以 Propose 接口为例，向 raft 提交一个 Op 操作，其会被转换成 MsgProp，通过 raft.Step() 传递给 raft StateMachine，最后可能被转换成给 Peer 复制 Op log 的 MsgApp Msg；（即发送给本地peer的消息）&lt;/li&gt;
&lt;li&gt;非本地 Msg，term 非 0，这种 Msg 会经过网络发送给 Peer；这里以 Msgheartbeat 为例子，就是 Leader 给 Follower 发送的心跳包。但是这个 MsgHeartbeat Msg 是通过 Tick 接口传入的，这个接口会向 raft StateMachine 传递一个 MsgBeat Msg，raft StateMachine 处理这个 MsgBeat 就是向复制组其它 Peer 分别发送一个 MsgHeartbeat Msg&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体的类型我们在上面的结构体定义的时候已经说明过了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;输出（ready）：readyc这个channel对外通知有数据要处理了，并将这些需要外部处理的数据打包到一个Ready结构体中，其实就是底层消息传递到应用层。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于 etcd 的网络、持久化模块和 raft 核心是分离的，所以当 raft 处理到某一些阶段的时候，需要输出一些东西，给外部处理，例如 Op log entries 持久化，Op log entries 复制的 Msg 等；以 heartbeat 为例，输入是 MsgBeat Msg，经过状态机状态化之后，就变成了给复制组所有的 Peer 发送心跳的 MsgHeartbeat Msg；在 ectd 中就是通过一个 Ready 的数据结构来封装当前 Raft state machine 已经准备好的数据和 Msg 供外部处理。&lt;/p&gt;

&lt;p&gt;我们来看看这个Ready结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// https://github.com/etcd-io/etcd/blob/v3.3.10/raft/node.go#L52
// Ready encapsulates the entries and messages that are ready to read,
// be saved to stable storage, committed or sent to other peers.
// All fields in Ready are read-only.
type Ready struct {
    // The current volatile state of a Node.
    // SoftState will be nil if there is no update.
    // It is not required to consume or store SoftState.
    *SoftState

    // The current state of a Node to be saved to stable storage BEFORE
    // Messages are sent.
    // HardState will be equal to empty state if there is no update.
    pb.HardState

    // ReadStates can be used for node to serve linearizable read requests locally
    // when its applied index is greater than the index in ReadState.
    // Note that the readState will be returned when raft receives msgReadIndex.
    // The returned is only valid for the request that requested to read.
    ReadStates []ReadState

    // Entries specifies entries to be saved to stable storage BEFORE
    // Messages are sent.
    Entries []pb.Entry

    // Snapshot specifies the snapshot to be saved to stable storage.
    Snapshot pb.Snapshot

    // CommittedEntries specifies entries to be committed to a
    // store/state-machine. These have previously been committed to stable
    // store.
    CommittedEntries []pb.Entry

    // Messages specifies outbound messages to be sent AFTER Entries are
    // committed to stable storage.
    // If it contains a MsgSnap message, the application MUST report back to raft
    // when the snapshot has been received or has failed by calling ReportSnapshot.
    Messages []pb.Message

    // MustSync indicates whether the HardState and Entries must be synchronously
    // written to disk or if an asynchronous write is permissible.
    MustSync bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ready 是 raft 状态机和外面交互传递的核心数据结构，其包含了一批更新操作&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SoftState：当前 node 的状态信息，主要记录了 Leader 是谁 ？当前 node 处于什么状态，是 Leader，还是 Follower，用于更新 etcd server 的状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// SoftState provides state that is useful for logging and debugging.
// The state is volatile and does not need to be persisted to the WAL.
type SoftState struct {
    Lead      uint64 // must use atomic operations to access; keep 64-bit aligned.
    RaftState StateType
}
​
type StateType uint64
​
var stmap = [...]string{
    &amp;quot;StateFollower&amp;quot;,
    &amp;quot;StateCandidate&amp;quot;,
    &amp;quot;StateLeader&amp;quot;,
    &amp;quot;StatePreCandidate&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pb.HardState: 包含当前节点见过的最大的 term，以及在这个 term 给谁投过票，以及当前节点知道的commit index，这部分数据会持久化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type HardState struct {
    Term             uint64 protobuf:&amp;quot;varint,1,opt,name=term&amp;quot; json:&amp;quot;term&amp;quot;
    Vote             uint64 protobuf:&amp;quot;varint,2,opt,name=vote&amp;quot; json:&amp;quot;vote&amp;quot;
    Commit           uint64 protobuf:&amp;quot;varint,3,opt,name=commit&amp;quot; json:&amp;quot;commit&amp;quot;
    XXX_unrecognized []byte json:&amp;quot;-&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ReadStates：用于返回已经确认 Leader 身份的 read 请求的 commit index&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Messages: 需要广播给所有peers的消息&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CommittedEntries：已经commit了，还没有apply到状态机的日志&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Snapshot：需要持久化的快照&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;应用程序得到这个Ready之后，需要：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将HardState, Entries, Snapshot持久化到storage。&lt;/li&gt;
&lt;li&gt;将Messages广播给其他节点。&lt;/li&gt;
&lt;li&gt;将CommittedEntries（已经commit还没有apply）应用到状态机。&lt;/li&gt;
&lt;li&gt;如果发现CommittedEntries中有成员变更类型的entry，调用node.ApplyConfChange()方法让node知道。&lt;/li&gt;
&lt;li&gt;最后再调用node.Advance()告诉raft，这批状态更新处理完了，状态已经演进了，可以给我下一批Ready让我处理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;基本实现&#34;&gt;基本实现&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;投票流程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、首先，在node的大循环里，有一个会定时输出的tick channel，它来触发raft.tick()函数，根据上面的介绍可知，如果当前节点是follower，那它的tick函数会指向tickElection。tickElection的处理逻辑是给自己发送一个MsgHup的内部消息，Step函数看到这个消息后会调用campaign函数，进入竞选状态。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// tickElection is run by followers and candidates after r.electionTimeout.
func (r *raft) tickElection() {
    r.electionElapsed++

    if r.promotable() &amp;amp;&amp;amp; r.pastElectionTimeout() {
        r.electionElapsed = 0
        r.Step(pb.Message{From: r.id, Type: pb.MsgHup})
    }
}

func (r *raft) Step(m pb.Message) error {
    //...
    switch m.Type {
    case pb.MsgHup:
        r.campaign(campaignElection)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、campaign则会调用becomeCandidate把自己切换到candidate模式，并递增Term值。然后再将自己的Term及日志信息发送给其他的节点，请求投票。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) campaign(t CampaignType) {
    //...
    r.becomeCandidate()
    // Get peer id from progress
    for id := range r.prs {
        //...
        r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、另一方面，其他节点在接受到这个请求后，会首先比较接收到的Term是不是比自己的大，以及接受到的日志信息是不是比自己的要新，从而决定是否投票。在对应节点的step函数中有对投票请求这种消息的处理逻辑。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) Step(m pb.Message) error {
    //...
    switch m.Type {
    case pb.MsgVote, pb.MsgPreVote:
        // We can vote if this is a repeat of a vote we&#39;ve already cast...
        canVote := r.Vote == m.From ||
            // ...we haven&#39;t voted and we don&#39;t think there&#39;s a leader yet in this term...
            (r.Vote == None &amp;amp;&amp;amp; r.lead == None) ||
            // ...or this is a PreVote for a future term...
            (m.Type == pb.MsgPreVote &amp;amp;&amp;amp; m.Term &amp;gt; r.Term)
        // ...and we believe the candidate is up to date.
        if canVote &amp;amp;&amp;amp; r.raftLog.isUpToDate(m.Index, m.LogTerm) {
            r.send(pb.Message{To: m.From, Term: m.Term, Type: voteRespMsgType(m.Type)})
        } else {
            r.send(pb.Message{To: m.From, Term: r.Term, Type: voteRespMsgType(m.Type), Reject: true})
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、最后当candidate节点收到投票回复后，就会计算收到的选票数目是否大于所有节点数的一半，如果大于则自己成为leader，并昭告天下，否则将自己置为follower，同样是在即自己的step的逻辑中处理投票恢复的消息逻辑。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) Step(m pb.Message) error {
    //...
    switch m.Type {
    case myVoteRespType:
        gr := r.poll(m.From, m.Type, !m.Reject)
        switch r.quorum() {
        case gr:
            if r.state == StatePreCandidate {
                r.campaign(campaignElection)
            } else {
                r.becomeLeader()
                r.bcastAppend()
            }
        case len(r.votes) - gr:
            r.becomeFollower(r.Term, None)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;put kv 请求&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft18&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;client 通过 grpc 发送一个 Put kv request，etcd server 的 rpc server 收到这个请求，通过 node 模块的 Propose 接口提交，node 模块的Propose方法将这个 Put kv request 转换成 raft StateMachine 认识的 MsgProp Msg 并通过 propc Channel 传递给 node 模块的 coroutine；&lt;/li&gt;
&lt;li&gt;node 模块 coroutine 监听在 propc Channel 中，收到 MsgProp Msg 之后，通过 raft.Step(Msg) 接口将其提交给 raft StateMachine 处理；&lt;/li&gt;
&lt;li&gt;raft StateMachine 处理完这个 MsgProp Msg 会产生 1 个 Op log entry 和 2 个发送给另外两个副本的 Append entries 的 MsgApp messages，node 模块会将这两个输出打包成 Ready，然后通过 readyc Channel 传递给 raftNode 模块的 coroutine；&lt;/li&gt;
&lt;li&gt;raftNode 模块的 coroutine 通过 readyc 读取到 Ready，首先通过网络层将 2 个 append entries 的 messages 发送给两个副本(PS:这里是异步发送的)；&lt;/li&gt;
&lt;li&gt;raftNode 模块的 coroutine 自己将 Op log entry 通过持久化层的 WAL 接口同步的写入 WAL 文件中&lt;/li&gt;
&lt;li&gt;raftNode 模块的 coroutine 通过 advancec Channel 通知当前 Ready 已经处理完，请给我准备下一个 带出的 raft StateMachine 输出Ready；&lt;/li&gt;
&lt;li&gt;其他副本的返回 Append entries 的 response： MsgAppResp message，会通过 node 模块的接口经过 recevc Channel 提交给 node 模块的 coroutine；&lt;/li&gt;
&lt;li&gt;node 模块 coroutine 从 recev Channel 读取到 MsgAppResp，然后提交给 raft StateMachine 处理。node 模块 coroutine 会驱动 raft StateMachine 得到关于这个 committedEntires，也就是一旦大多数副本返回了就可以 commit 了，node 模块 new 一个新的 Ready其包含了 committedEntries，通过 readyc Channel 传递给 raftNode 模块 coroutine 处理；&lt;/li&gt;
&lt;li&gt;raftNode 模块 coroutine 从 readyc Channel 中读取 Ready结构，然后取出已经 commit 的 committedEntries 通过 applyc 传递给另外一个 etcd server coroutine 处理，其会将每个 apply 任务提交给 FIFOScheduler 调度异步处理，这个调度器可以保证 apply 任务按照顺序被执行，因为 apply 的执行是不能乱的；&lt;/li&gt;
&lt;li&gt;raftNode 模块的 coroutine 通过 advancec Channel 通知当前 Ready 已经处理完，请给我准备下一个待处理的 raft StateMachine 输出Ready；&lt;/li&gt;
&lt;li&gt;FIFOScheduler 调度执行 apply 已经提交的 committedEntries&lt;/li&gt;
&lt;li&gt;AppliedIndex 推进，通知 ReadLoop coroutine，满足 applied index&amp;gt;= commit index 的 read request 可以返回；&lt;/li&gt;
&lt;li&gt;调用网络层接口返回 client 成功。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面主要是leader的一个流程，其实还是有fallow，整体一个流程总结&lt;/p&gt;

&lt;p&gt;1、一个写请求一般会通过调用node.Propose开始，Propose方法将这个写请求封装到一个MsgProp消息里面，发送给自己处理。&lt;/p&gt;

&lt;p&gt;2、消息处理函数Step无法直接处理这个消息，它会调用那个小写的step函数，来根据当前的状态进行处理。&lt;/p&gt;

&lt;p&gt;如果当前是follower，那它会把这个消息转发给leader。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func stepFollower(r *raft, m pb.Message) error {
    switch m.Type {
    case pb.MsgProp:
        //...
        m.To = r.lead
        r.send(m)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Leader收到这个消息后（不管是follower转发过来的还是自己内部产生的）会有两步操作
- 将这个消息添加到自己的log里
- 向其他follower广播这个消息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    func stepLeader(r *raft, m pb.Message) error {
        switch m.Type {
        case pb.MsgProp:
            //...
            if !r.appendEntry(m.Entries...) {
                return ErrProposalDropped
            }
            r.bcastAppend()
            return nil
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、在follower接受完这个log后，会返回一个MsgAppResp消息。&lt;/p&gt;

&lt;p&gt;5、当leader确认已经有足够多的follower接受了这个log后，它首先会commit这个log，然后再广播一次，告诉别人它的commit状态。这里的实现就有点像两阶段提交了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func stepLeader(r *raft, m pb.Message) error {
    switch m.Type {
    case pb.MsgAppResp:
        //...
        if r.maybeCommit() {
            r.bcastAppend()
        }
    }
}

// maybeCommit attempts to advance the commit index. Returns true if
// the commit index changed (in which case the caller should call
// r.bcastAppend).
func (r *raft) maybeCommit() bool {
    //...
    mis := r.matchBuf[:len(r.prs)]
    idx := 0
    for _, p := range r.prs {
        mis[idx] = p.Match
        idx++
    }
    sort.Sort(mis)
    mci := mis[len(mis)-r.quorum()]
    return r.raftLog.maybeCommit(mci, r.Term)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;状态转换&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;etcd-raft StateMachine 封装在 raft struct 中，其状态转换如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/algorithm/raft12&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图中就是对应的状态转化接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) becomeFollower(term uint64, lead uint64)
func (r *raft) becomePreCandidate()
func (r *raft) becomeCandidate()
func (r *raft) becomeLeader()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;etcd将请求数据都转化为msg，然后通过step接口进行处理，我们来看一下step接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raft) Step(m pb.Message) error {
    r.step(r, m)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在调用的step接口中，会针对不同的state调用不同的函数，如下，其中 stepCandidate 会处理 PreCandidate 和 Candidate 两种状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func stepFollower(r *raft, m pb.Message) error
func stepCandidate(r *raft, m pb.Message) error
func stepLeader(r *raft, m pb.Message) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们简单看一下stepCandidate，对各种 Msg 进行处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func stepCandidate(r *raft, m pb.Message) error {
    ......
    switch m.Type {
    case pb.MsgProp:
        r.logger.Infof(&amp;quot;%x no leader at term %d; dropping proposal&amp;quot;, r.id, r.Term)
        return ErrProposalDropped
    case pb.MsgApp:
        r.becomeFollower(m.Term, m.From) // always m.Term == r.Term
        r.handleAppendEntries(m)
    case pb.MsgHeartbeat:
        r.becomeFollower(m.Term, m.From) // always m.Term == r.Term
        r.handleHeartbeat(m)
    case pb.MsgSnap:
        r.becomeFollower(m.Term, m.From) // always m.Term == r.Term
        r.handleSnapshot(m)
    case myVoteRespType:
        ......
    case pb.MsgTimeoutNow:
        r.logger.Debugf(&amp;quot;%x [term %d state %v] ignored MsgTimeoutNow from %x&amp;quot;, r.id, r.Term, r.state, m.From)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;核心模块&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、node 模块，对应一个 coroutine&lt;/p&gt;

&lt;p&gt;其实就是我们说的node的功能，负责raft和应用层的交互，也就是那个for-select-channel的coroutine。&lt;/p&gt;

&lt;p&gt;2、raftNode 模块：也会有一个 coroutine&lt;/p&gt;

&lt;p&gt;主要完成的工作是把 raft StateMachine 处理的阶段性输出 Ready 拿来处理，该持久化的通过持久化接口写入盘中，该发送给 Peer 的通过网络层发送给 Peers 等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *raftNode) start(rh *raftReadyHandler) {
    go func() {
        defer r.onStop()
        islead := false
​
        for {
            select {
            // 监听 Ticker 事件，并通知 raft StateMachine
            case &amp;lt;-r.ticker.C:
                r.tick()
            // 监听待处理的 Ready，并处理
            case rd := &amp;lt;-r.Ready():
                ......
                  // 这部分处理 Ready 的逻辑下面单独文字描述
                ......
                // 通知 raft StateMachine 运转，返回新的待处理的 Ready
                r.Advance()
            case &amp;lt;-r.stopped:
                return
            }
        }
    }()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;raftNode 模块的 cortoutine 核心就是处理 raft StateMachine 的 Ready，下面将用文字单独描述，这里仅考虑Leader 分支，Follower 分支省略：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;取出 Ready.SoftState 更新 EtcdServer 的当前节点身份信息（leader、follower&amp;hellip;.）等&lt;/li&gt;
&lt;li&gt;取出 Ready.ReadStates（保存了 commit index），通过 raftNode.readStateC 通道传递给 EtcdServer 处理 read 的 coroutine&lt;/li&gt;
&lt;li&gt;取出 Ready.CommittedEntires 封装成 apply 结构，通过 raftnode.applyc 通道传递给 EtcdServer 异步 Apply 的 coroutine，并更新 EtcdServer 的 commit index&lt;/li&gt;
&lt;li&gt;取出 Ready.Messages，通过网络模块 raftNode.transport 发送给 Peers&lt;/li&gt;
&lt;li&gt;取出 Ready.HardState 和 Entries，通过 raftNode.storage 持久化到 WAL 中&lt;/li&gt;
&lt;li&gt;（Follower分支）取出 Ready.snapshot（Leader 发送过来的），（1）通过 raftNode.storage 持久化 Snapshot 到盘中的 Snapshot，（2）通知异步 Apply coroutine apply snapshot 到 KV 存储中，（3）Apply snapshot 到 raftNode.raftStorage 中（all raftLog in memory）&lt;/li&gt;
&lt;li&gt;取出 Ready.entries，append 到 raftLog 中&lt;/li&gt;
&lt;li&gt;调用 raftNode.Advance 通知 raft StateMachine coroutine，当前 Ready 已经处理完，可以投递下一个准备好的 Ready 给 raftNode cortouine 处理了（raft StateMachine 中会删除 raftLog 中 unstable 中 log entries 拷贝到 raftLog 的 Memory storage 中）&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Map</title>
          <link>https://kingjcy.github.io/post/golang/go-map/</link>
          <pubDate>Thu, 28 Feb 2019 17:42:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-map/</guid>
          <description>&lt;p&gt;map是我们经常使用的一种数据结构，也是很重要的一种数据结构，我们来详细的了解一下map。&lt;/p&gt;

&lt;h1 id=&#34;map&#34;&gt;map&lt;/h1&gt;

&lt;p&gt;map就是k/v的映射，map持有对底层数据结构的引用。如果将map传递给函数，其对map的内容做了改变，则这些改变对于调用者是可见的。&lt;/p&gt;

&lt;h2 id=&#34;map的实现&#34;&gt;map的实现&lt;/h2&gt;

&lt;p&gt;所有的Map底层一般都是使用数组+链表的hashmap来实现，会借用哈希算法辅助。对于给定的 key，一般先进行 hash 操作，然后相对哈希表的长度取模，将 key 映射到指定的链表中。&lt;/p&gt;

&lt;p&gt;Golang的map正常是使用哈希表作为底层实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;hashtable是一种根据key，key在通过hash函数来对应的数组的数据。&lt;/li&gt;
&lt;li&gt;hashmap是hashtable使用拉链法实现的一种方式，数组+链表的实现方式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;正常情况下，golang使用数组+桶（buckets）来实现，默认每个桶的数量是8个，在超出8个的情况下，新增一个桶，使用链表连接起来，这是时候就变成了hash数组 + 桶 + 溢出的桶链表了。&lt;/p&gt;

&lt;p&gt;其实map不是并发安全的，也是因为hashmap不是并发安全的，实现并发安全的几种方式，可以参考&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;数据结构&#34;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;map数据结构由runtime/map.go/hmap定义:&lt;/p&gt;

&lt;p&gt;hmap&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/map.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type hmap struct {
    count     int
    flags     uint8
    B         uint8
    noverflow uint16
    hash0     uint32

    buckets    unsafe.Pointer
    oldbuckets unsafe.Pointer
    nevacuate  uintptr

    extra *mapextra
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;count 表示当前哈希表中的元素数量；类似于&lt;code&gt;buckets[count]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;B表示当前哈希表持有的buckets数量，但是因为哈希表中桶的数量都2的倍数，所以该字段会存储对数，也就是 len(buckets) == 2^B；类似于&lt;code&gt;buckets[2^B]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;hash0 是哈希的种子，它能为哈希函数的结果引入随机性，这个值在创建哈希表时确定，并在调用哈希函数时作为参数传入；&lt;/li&gt;
&lt;li&gt;oldbuckets 是哈希在扩容时用于保存之前 buckets 的字段，它的大小是当前 buckets 的一半；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;bmap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type bmap struct {
    tophash [8]uint8 //存储哈希值的高8位
    //data和overflow并不是在结构体中显示定义的，而是直接通过指针运算进行访问的。
    data    byte[1]  //key value数据:key/key/key/.../value/value/value...
    overflow *bmap   //溢出bucket的地址
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;tophash是个长度为8的数组，哈希值相同的键（准确的说是哈希值低位相同的键）存入当前bucket时会将哈希值的高位存储在该数组中，以方便后续匹配。&lt;/li&gt;
&lt;li&gt;data区存放的是key-value数据，存放顺序是key/key/key/&amp;hellip;value/value/value，如此存放是为了节省字节对齐带来的空间浪费。&lt;/li&gt;
&lt;li&gt;overflow 指针指向的是下一个bucket，据此将所有冲突的键连接起来。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我们能根据编译期间的 cmd/compile/internal/gc.bmap 函数对它的结构重建：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type bmap struct {
    topbits  [8]uint8
    keys     [8]keytype
    values   [8]valuetype
    pad      uintptr
    overflow uintptr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hmap.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hmap2.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hmap3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;基本机制&#34;&gt;基本机制&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;负载因子和rehash&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;负载因子用于衡量一个哈希表冲突情况，公式为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;负载因子 = 键数量/bucket数量
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如，对于一个bucket数量为4，包含4个键值对的哈希表来说，这个哈希表的负载因子为1.&lt;/p&gt;

&lt;p&gt;哈希表需要将负载因子控制在合适的大小，超过其阀值需要进行rehash，也即键值对重新组织：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;哈希因子过小，说明空间利用率低&lt;/li&gt;
&lt;li&gt;哈希因子过大，说明冲突严重，存取效率低&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个哈希表的实现对负载因子容忍程度不同，比如Redis实现中负载因子大于1时就会触发rehash，而Go则在在负载因子达到6.5时才会触发rehash，因为Redis的每个bucket只能存1个键值对，而Go的bucket可能存8个键值对，所以Go可以容忍更高的负载因子。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map的删除机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;采用的是惰性删除的策略，打上一个empty的标记，实际上并没有删除，也不会释放内存，还可以在后面进行复用，这样做主要为了解决遍历过程的溢出问题，因为是用数组实现的。实现迭代安全&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map的扩容机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;增量扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当负载因子大于6.5也即平均每个bucket存储的键值对达到6.5个，就新建一个bucket，新的bucket长度是原来的2倍，然后旧bucket数据搬迁到新的bucket。&lt;/p&gt;

&lt;p&gt;考虑到如果map存储了数以亿计的key-value，一次性搬迁将会造成比较大的延时，Go采用逐步搬迁策略，即每次访问map时都会触发一次搬迁，每次搬迁2个键值对。&lt;/p&gt;

&lt;p&gt;扩容流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/dilatation.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当前map存储了7个键值对，只有1个bucket。此地负载因子为7。再次插入数据时将会触发扩容操作，扩容之后再将新插入键写入新的bucket。&lt;/p&gt;

&lt;p&gt;当第8个键值对插入时，将会触发扩容，扩容后示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/dilatation2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;hmap数据结构中oldbuckets成员指身原bucket，而buckets指向了新申请的bucket。新的键值对被插入新的bucket中。 后续对map的访问操作会触发迁移，将oldbuckets中的键值对逐步的搬迁过来。当oldbuckets中的键值对全部搬迁完毕后，删除oldbuckets。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/dilatation3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;等量扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实际上并不是扩大容量，buckets数量不变，重新做一遍类似增量扩容的搬迁动作，把松散的键值对重新排列一次，以使bucket的使用率更高，进而保证更快的存取。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map的缩容机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;溢出的桶数量noverflow&amp;gt;=32768(1&amp;lt;&lt;15)或者&gt;=hash数组大小。&lt;/p&gt;

&lt;p&gt;但是缩容并不会释放已经占用的空间，真的要释放空间，就新建一个map进行迁移&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;什么时候转化为红黑树&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang中没有转化为红黑树，就是正常的扩容使用数组，在java中超过8个桶，就会转化为红黑树，和查询次数也即是时间复杂度有关，因为Map中桶的元素初始化是数组保存的，其查找性能是O(n)，而树结构能将查找性能提升到O(log(n))。当数组长度很小的时候，即使遍历，速度也非常快，但是当链表长度不断变长，肯定会对查询性能有一定的影响，所以才需要转成树。有利于减少查询的次数&lt;/p&gt;

&lt;p&gt;8个桶这个其实是概率统计出来的，8最合适。golang中的桶有8个kv应该是也是这个道理。&lt;/p&gt;

&lt;h1 id=&#34;map并发安全&#34;&gt;map并发安全&lt;/h1&gt;

&lt;p&gt;Go 原生的 map 数据类型是非并发安全的，在go1.9开始发布了sync.map是线程安全的。&lt;/p&gt;

&lt;p&gt;我们先看看基于原生map的基础上加mutex实现并发安全。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map+mutex&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang1.9以前，我们都是使用读写锁（sync.RWMutex）来实现并发安全，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package beego

import (
    &amp;quot;sync&amp;quot;
)

type BeeMap struct {
    lock *sync.RWMutex
    bm   map[interface{}]interface{}

}

func NewBeeMap() *BeeMap {
    return &amp;amp;BeeMap{
        lock: new(sync.RWMutex),
        bm:   make(map[interface{}]interface{}),
    }
}

//Get from maps return the k&#39;s value
func (m *BeeMap) Get(k interface{}) interface{} {
    m.lock.RLock()
    defer m.lock.RUnlock()
    if val, ok := m.bm[k]; ok {
        return val
    }
    return nil
}

// Maps the given key and value. Returns false
// if the key is already in the map and changes nothing.
func (m *BeeMap) Set(k interface{}, v interface{}) bool {
    m.lock.Lock()
    defer m.lock.Unlock()
    if val, ok := m.bm[k]; !ok {
        m.bm[k] = v
    } else if val != v {
        m.bm[k] = v
    } else {
        return false
    }
    return true
}

// Returns true if k is exist in the map.
func (m *BeeMap) Check(k interface{}) bool {
    m.lock.RLock()
    defer m.lock.RUnlock()
    if _, ok := m.bm[k]; !ok {
        return false
    }
    return true
}

func (m *BeeMap) Delete(k interface{}) {
    m.lock.Lock()
    defer m.lock.Unlock()
    delete(m.bm, k)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在项目中经常使用的方式：通过数组、map、sync.RWMutex来实现原生map的并发读写（采用map数组，把key hash到相应的map，每个map单独加锁以降低锁的粒度）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;sync.map&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;那么golang sync.map是如何实现并发安全的呢？&lt;/p&gt;

&lt;p&gt;简单总结就是使用了用空间换时间（多存储一份map作为缓存，减少锁的使用）的思想来实现来一个高效的并发安全。主要下面的函数接口实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Load：读取指定 key 返回 value&lt;/li&gt;
&lt;li&gt;Store： 存储（增或改）key-value&lt;/li&gt;
&lt;li&gt;Delete： 删除指定 key&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4种操作：读key、增加key、更新key、删除key的基本流程，其实在代码中只有上面三个函数实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;读key：先到read中读取，如果有则直接返回结果，如果没有就加锁，再次检查read是否存在，因为刚刚是无锁读，如果没有，则到dirty加锁中读取，如果有返回结果并更新miss数（用于数据迁移），这边在read这边设置了一个amended，如果为true则表明当前read只读map的数据不完整，dirty map中包含部分数据，这边决定了刚刚上一步是否执行到dirty中查找&lt;/li&gt;
&lt;li&gt;更新key（增加key）：其实更新和增加使用的是同一个函数store，首先查找read中是否存在，如果存在直接更新，如果没有就加锁，再次检查read是否存在，因为刚刚是无锁读，如果不存在，同样有上面的判断是否是全量数据，不是就继续到dirty中查找，找到了就更新，找不到就新建一个存储，就是新增key&lt;/li&gt;
&lt;li&gt;删除key：先到read中看看有没有，如果有p标记为nil，如果没有则到dirty中直接删除（同样有数据全不全的判断）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;源码详解解读&#34;&gt;源码详解解读&lt;/h2&gt;

&lt;h3 id=&#34;数据结构分析&#34;&gt;数据结构分析&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Map&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Map struct {
    mu Mutex
    // read是一个readOnly的指针，里面包含了一个map结构，就是我们说的只读map对该map的元素的访问
    // 不需要加锁，只需要通过atomic加载最新的指针即可
    read atomic.Value // readOnly

    // dirty包含部分map的键值对，如果要访问需要进行mutex获取
    // 最终dirty中的元素会被全部提升到read里面的map中
    dirty map[interface{}]*entry

   // misses是一个计数器用于记录从read中没有加载到数据
    // 尝试从dirty中进行获取的次数，从而决定将数据从dirty迁移到read的时机
    misses int
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;readOnly&lt;/p&gt;

&lt;p&gt;只读map,对该map元素的访问不需要加锁，但是该map也不会进行元素的增加，元素会被先添加到dirty中然后后续再转移到read只读map中，通过atomic原子操作不需要进行锁操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type readOnly struct {
    // m包含所有只读数据，不会进行任何的数据增加和删除操作
    // 但是可以修改entry的指针因为这个不会导致map的元素移动
    m       map[interface{}]*entry
    // 标志位，如果为true则表明当前read只读map的数据不完整，dirty map中包含部分数据
    amended bool
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;entry&lt;/p&gt;

&lt;p&gt;entry是sync.Map中值得指针，如果当p指针指向expunged这个指针的时候，则表明该元素被删除，但不会立即从map中删除，如果在未删除之前又重新赋值则会重用该元素&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type entry struct {
    // 指向元素实际值得指针
    p unsafe.Pointer // *interface{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;p 有三种状态：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;p == nil: 键值已经被删除，且 m.dirty == nil&lt;/li&gt;
&lt;li&gt;p == expunged: 键值已经被删除，但 m.dirty!=nil 且 m.dirty 不存在该键值（expunged 实际是空接口指针）&lt;/li&gt;
&lt;li&gt;键值对存在，存在于 m.read.m 中，如果 m.dirty!=nil 则也存在于 m.dirty&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;数据存储&#34;&gt;数据存储&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/date-store.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;元素存在read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;元素如果存储在只读map中，则只需要获取entry元素，然后修改其p的指针指向新的元素就可以了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    read, _ := m.read.Load().(readOnly)
    if e, ok := read.m[key]; ok &amp;amp;&amp;amp; e.tryStore(&amp;amp;value) {
        return
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边其实有两点需要特别说明的&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;map不是并发安全的，这边为什么可以直接修改呢，因为这边使用atomic中的CAS的乐观思想来实现乐观锁，实现并发安全。&lt;/li&gt;
&lt;li&gt;cas就是实现了对比并交互，这个操作是用来实现乐观锁这种思路的，乐观锁并不是真正的锁，它用版本好来标记数据是否被修改，如果被修改则重试。&lt;/li&gt;
&lt;li&gt;怎么同步到dirty中去，entry是一个指向实际值的地址，所以read和dirty是共享地址的。所以就一起改了&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;元素存在dirty中&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果元素存在dirty中其实同read map逻辑一样，只需要修改对应元素的指针即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;} else if e, ok := m.dirty[key]; ok {
    // 如果已经在dirty中就会直接存储
    e.storeLocked(&amp;amp;value)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边有两点需要说明&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;read中是有一个标识amended来判断，如果为true则表明当前read只读map的数据不完整，dirty map中包含部分数据,才会进来查询。&lt;/li&gt;
&lt;li&gt;在加锁之后，还是需要重read中查询一次的，查到就直接修改，因为是一开始的查询是无锁读，存在并发安全问题，可能在这段未加锁的时间内数据发生了改变。&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;元素不存在&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果元素之前不存在当前Map中则需要先将其存储在dirty map中，同时将amended标识为true,即当前read中的数据不全，有一部分数据存储在dirty中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 如果当前不是在修正状态
if !read.amended {
    // 新加入的key会先被添加到dirty map中， 并进行read标记为不完整
    // 如果dirty为空则将read中的所有没有被删除的数据都迁移到dirty中
    m.dirtyLocked()
    m.read.Store(readOnly{m: read.m, amended: true})
}
m.dirty[key] = newEntry(value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) Store(key, value interface{}) {
    read, _ := m.read.Load().(readOnly)
    // 如果 read 里存在，则尝试存到 entry 里
    if e, ok := read.m[key]; ok &amp;amp;&amp;amp; e.tryStore(&amp;amp;value) {
        return
    }

    // 如果上一步没执行成功，则要分情况处理
    m.mu.Lock()
    read, _ = m.read.Load().(readOnly)
    // 和 Load 一样，重新从 read 获取一次
    if e, ok := read.m[key]; ok {
        // 情况 1：read 里存在
        if e.unexpungeLocked() {
            // 如果 p == expunged，则需要先将 entry 赋值给 dirty（因为 expunged 数据不会留在 dirty）
            m.dirty[key] = e
        }
        // 用值更新 entry
        e.storeLocked(&amp;amp;value)
    } else if e, ok := m.dirty[key]; ok {
        // 情况 2：read 里不存在，但 dirty 里存在，则用值更新 entry
        e.storeLocked(&amp;amp;value)
    } else {
        // 情况 3：read 和 dirty 里都不存在
        if !read.amended {
            // 如果 amended == false，则调用 dirtyLocked 将 read 拷贝到 dirty（除了被标记删除的数据）
            m.dirtyLocked()
            // 然后将 amended 改为 true
            m.read.Store(readOnly{m: read.m, amended: true})
        }
        // 将新的键值存入 dirty
        m.dirty[key] = newEntry(value)
    }
    m.mu.Unlock()
}

func (e *entry) tryStore(i *interface{}) bool {
    for {
        p := atomic.LoadPointer(&amp;amp;e.p)
        if p == expunged {
            return false
        }
        //原子操作cas
        if atomic.CompareAndSwapPointer(&amp;amp;e.p, p, unsafe.Pointer(i)) {
            return true
        }
    }
}

func (e *entry) unexpungeLocked() (wasExpunged bool) {
    return atomic.CompareAndSwapPointer(&amp;amp;e.p, expunged, nil)
}

func (e *entry) storeLocked(i *interface{}) {
    atomic.StorePointer(&amp;amp;e.p, unsafe.Pointer(i))
}

func (m *Map) dirtyLocked() {
    if m.dirty != nil {
        return
    }

    read, _ := m.read.Load().(readOnly)
    m.dirty = make(map[interface{}]*entry, len(read.m))
    for k, e := range read.m {
        // 判断 entry 是否被删除，否则就存到 dirty 中
        if !e.tryExpungeLocked() {
            m.dirty[k] = e
        }
    }
}

func (e *entry) tryExpungeLocked() (isExpunged bool) {
    p := atomic.LoadPointer(&amp;amp;e.p)
    for p == nil {
        // 如果有 p == nil（即键值对被 delete），则会在这个时机被置为 expunged
        if atomic.CompareAndSwapPointer(&amp;amp;e.p, nil, expunged) {
            return true
        }
        p = atomic.LoadPointer(&amp;amp;e.p)
    }
    return p == expunged
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在数据存储里其实还有数据迁移的逻辑&lt;/p&gt;

&lt;p&gt;当read多次都没有命中数据，达到阈值，表示这个cache命中率太低，这时直接将整个read用dirty替换掉，然后dirty又重新置为nil，下一次再添加一个新key的时候，会触发一次read到dirty的复制，这样二者又保持了一致。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-migration.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在刚初始化和将所有元素迁移到read中后，dirty默认都是nil元素，而此时如果有新的元素增加，则需要先将read map中的所有未删除数据先迁移到dirty中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) dirtyLocked() {
    if m.dirty != nil {
        return
    }

    read, _ := m.read.Load().(readOnly)
    m.dirty = make(map[interface{}]*entry, len(read.m))
    for k, e := range read.m {
        if !e.tryExpungeLocked() {
            m.dirty[k] = e
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-migration2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当持续的从read访问穿透到dirty中后，也就是上面的miss值大于dirty存储数据的长度，就会触发一次从dirty到read的迁移，这也意味着如果我们的元素读写比差比较小，其实就会导致频繁的迁移操作，性能其实可能并不如rwmutex等实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) missLocked() {
    m.misses++
    if m.misses &amp;lt; len(m.dirty) {
        return
    }
    m.read.Store(readOnly{m: m.dirty})
    m.dirty = nil
    m.misses = 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据查询&#34;&gt;数据查询&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-query.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关于amended标识的使用，和存储是一样的&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据在read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Load数据的时候回先从read中获取，如果此时发现元素，则直接返回即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;read, _ := m.read.Load().(readOnly)
e, ok := read.m[key]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;数据在dirty&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加锁后会尝试从read和dirty中读取，同时进行misses计数器的递增，如果满足迁移条件则会进行数据迁移&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;read, _ = m.read.Load().(readOnly)
e, ok = read.m[key]
if !ok &amp;amp;&amp;amp; read.amended {
    e, ok = m.dirty[key]
    // 这里将采取缓慢迁移的策略
    // 只有当misses计数==len(m.dirty)的时候，才会将dirty里面的数据全部晋升到read中
    m.missLocked()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) Load(key interface{}) (value interface{}, ok bool) {
    // 首先尝试从 read 中读取 readOnly 对象
    read, _ := m.read.Load().(readOnly)
    e, ok := read.m[key]

    // 如果不存在则尝试从 dirty 中获取
    if !ok &amp;amp;&amp;amp; read.amended {
        m.mu.Lock()
        // 由于上面 read 获取没有加锁，为了安全再检查一次
        read, _ = m.read.Load().(readOnly)
        e, ok = read.m[key]

        // 确实不存在则从 dirty 获取
        if !ok &amp;amp;&amp;amp; read.amended {
            e, ok = m.dirty[key]
            // 调用 miss 的逻辑
            m.missLocked()
        }
        m.mu.Unlock()
    }

    if !ok {
        return nil, false
    }
    // 从 entry.p 读取值
    return e.load()
}

func (m *Map) missLocked() {
    m.misses++
    if m.misses &amp;lt; len(m.dirty) {
        return
    }
    // 当 miss 积累过多，会将 dirty 存入 read，然后 将 amended = false，且 m.dirty = nil
    m.read.Store(readOnly{m: m.dirty})
    m.dirty = nil
    m.misses = 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据删除&#34;&gt;数据删除&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-query.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果数据在read中，则就直接修改entry的标志位指向删除的指针即可，如果当前read中数据不全，则需要进行dirty里面的元素删除尝试，&lt;/li&gt;
&lt;li&gt;如果存在就直接从dirty中删除即可&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) Delete(key interface{}) {
    m.LoadAndDelete(key)
}

// LoadAndDelete 作用等同于 Delete，并且会返回值与是否存在
func (m *Map) LoadAndDelete(key interface{}) (value interface{}, loaded bool) {
    // 获取逻辑和 Load 类似，read 不存在则查询 dirty
    read, _ := m.read.Load().(readOnly)
    e, ok := read.m[key]
    if !ok &amp;amp;&amp;amp; read.amended {
        m.mu.Lock()
        read, _ = m.read.Load().(readOnly)
        e, ok = read.m[key]
        if !ok &amp;amp;&amp;amp; read.amended {
            e, ok = m.dirty[key]
            m.missLocked()
        }
        m.mu.Unlock()
    }
    // 查询到 entry 后执行删除
    if ok {
        // 将 entry.p 标记为 nil，数据并没有实际删除
        // 真正删除数据并被被置为 expunged，是在 Store 的 tryExpungeLocked 中
        return e.delete()
    }
    return nil, false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;设计思想&#34;&gt;设计思想&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;自动扩缩容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在Mutex和RWMutex实现的并发安全的map中map随着时间和元素数量的增加、删除，容量会不断的递增，在某些情况下比如在某个时间点频繁的进行大量数据的增加，然后又大量的删除，其map的容量并不会随着元素的删除而缩小，而在sync.Map中，当进行元素从dirty进行提升到read map的时候会进行重建，可以实现自动扩缩容。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;读写分离&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;并发访问map读的主要问题其实是在扩容的时候，可能会导致元素被hash到其他的地址，那如果我的读的map不会进行扩容操作，就可以进行并发安全的访问了，而sync.map里面正是采用了这种方式，对增加元素通过dirty来进行保存&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;无锁读&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过read只读和dirty写map将操作分离，其实就只需要通过原子指令对read map来进行读操作而不需要加锁了，从而提高读的性能&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;写加锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面提到增加元素操作可能会先增加大dirty写map中，那针对多个goroutine同时写，其实就需要进行Mutex加锁了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;延迟提升&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面提到了read只读map和dirty写map, 那就会有个问题，默认增加元素都放在dirty中，那后续访问新的元素如果都通过 mutex加锁，那read只读map就失去意义，sync.Map中采用一直延迟提升的策略，进行批量将当前map中的所有元素都提升到read只读map中从而为后续的读访问提供无锁支持&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;惰性删除&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;惰性删除是并发设计中一中常见的设计，比如删除某个个链表元素，如果要删除则需要修改前后元素的指针，而采用惰性删除，则通常只需要给某个标志位设定为删除，然后在后续修改中再进行操作，sync.Map中也采用这种方式，通过给指针指向某个标识删除的指针，从而实现惰性删除&lt;/p&gt;

&lt;p&gt;我觉得最重要的就是实现了读写分离，加锁分离，从而实现了空间换取时间的快速处理。read相当于dirty的缓存，read是原子操作，不需要加锁，快速，dirty可以延迟提升，就和缓存数据库做缓存是一个道理，包括热点数据的提升。&lt;/p&gt;

&lt;h2 id=&#34;应用场景&#34;&gt;应用场景&lt;/h2&gt;

&lt;p&gt;由以上方法可得知，无论是读操作，还是更新操作，亦或者删除操作，都会先从read进行操作，因为read的读取更新不需要锁，是原子操作，这样既做到了并发安全，又做到了尽量减少锁的争用，虽然采用的是空间换时间的策略，通过两个冗余的map，实现了这一点，但是底层存的都是指针类型，所以对于空间占用，也是做到了最大程度的优化。
但是同时也可以得知，当存在大量写操作时，会导致read中读不到数据，依然会频繁加锁，同时dirty升级为read，整体性能就会很低，所以sync.Map更加适合大量读、少量写的场景。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Sync</title>
          <link>https://kingjcy.github.io/post/golang/go-sync/</link>
          <pubDate>Thu, 28 Feb 2019 17:42:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-sync/</guid>
          <description>&lt;p&gt;sync包提供了基本的同步基元，如锁，WaitGroup、Once 和 Cond等同步原语。除了Once和WaitGroup类型，大部分都是适用于普通程序线程，大型并发同步使用channel通信（csp）更好一些。&lt;/p&gt;

&lt;h1 id=&#34;sync&#34;&gt;sync&lt;/h1&gt;

&lt;p&gt;sync同步功能主要提供了once，mutex，cond，并发安全map，安全并发pool，waitgroup。&lt;/p&gt;

&lt;h2 id=&#34;sync-once&#34;&gt;sync.Once&lt;/h2&gt;

&lt;p&gt;sync.Once是一个简单而强大的原语，可确保一个函数仅执行一次。&lt;/p&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Once struct {
        // 非暴露字段
}

func (o *Once) Do(f func())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用前先定义 Once 类型变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var once Once
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用的时候向 Once 类型变量传入函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;once.Do(func() { init() })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;多次调用 once.Do(f) 只会触发一次 f 的执行，即第一次 f 的执行。&lt;/p&gt;

&lt;p&gt;用法实例&lt;/p&gt;

&lt;p&gt;某些操作只需要执行一次（比如一些初始化动作），这时就可使用 Once，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var once sync.Once
    onceBody := func() {
        fmt.Println(&amp;quot;Only once&amp;quot;)
    }
    done := make(chan bool)

    // 创建 10 个 goroutine，但是 onceBody 只会执行 1 次
    for i := 0; i &amp;lt; 10; i++ {
        go func() {
            once.Do(onceBody)
            done &amp;lt;- true
        }()
    }

    // 等待 10 个 goroutine 结束
    for i := 0; i &amp;lt; 10; i++ {
        &amp;lt;-done
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实 Once 的实现非常简单，就是互斥锁+原子变量&lt;/p&gt;

&lt;h2 id=&#34;sync-waitgroup&#34;&gt;sync.WaitGroup&lt;/h2&gt;

&lt;p&gt;sync.WaitGroup拥有一个内部计数器。当计数器等于0时，则Wait()方法会立即返回。否则它将阻塞执行Wait()方法的goroutine直到计数器等于0时为止。&lt;/p&gt;

&lt;p&gt;WaitGroup 的使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type WaitGroup struct {
    // 包含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WaitGroup用于等待一组线程的结束。父线程调用Add方法来设定应等待的线程的数量。每个被等待的线程在结束时应调用Done方法。同时，主线程里可以调用Wait方法阻塞至所有线程结束。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*WaitGroup) Add
func (wg *WaitGroup) Add(delta int)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add方法向内部计数加上delta，delta可以是负数；如果内部计数器变为0，Wait方法阻塞等待的所有线程都会释放，如果计数器小于0，方法panic。注意Add加上正数的调用应在Wait之前，否则Wait可能只会等待很少的线程。一般来说本方法应在创建新的线程或者其他应等待的事件之前调用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*WaitGroup) Done
func (wg *WaitGroup) Done()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done方法减少WaitGroup计数器的值，应在线程的最后执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*WaitGroup) Wait
func (wg *WaitGroup) Wait()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait方法阻塞直到WaitGroup计数器减为0。&lt;/p&gt;

&lt;h2 id=&#34;sync-pool&#34;&gt;sync.Pool&lt;/h2&gt;

&lt;p&gt;sync.Pool是一个并发池，负责安全地保存一组对象。&lt;/p&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Pool struct {
        // 当 Get() 找不到一个对象时，会使用 New() 生成一个对象
        New func() interface{}

        // 剩下的是非暴露字段
}

// 任意从 pool 中挑选一个对象返回给客户端，如果找不到就使用 p.New 生成
func (p *pool) Get() interface{}

// 将对象 x 放回到 pool 中
func (p *pool) Put(x interface{})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 Pool 可以用以管理一些临时对象供多个 package 的客户端使用，客户端对 Pool 的逻辑是无感知的：需要的时候 Get，不需要的时候 Put，而且 Pool 可根据当前负载自动调整对象池的大小。&lt;/p&gt;

&lt;p&gt;一个典型的应用是日志，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var bufPool = sync.Pool{
    New: func() interface{} {
        return new(bytes.Buffer)
    },
}

func Log(w io.Writer, key, val string) {
    // 从对象池中获取 buffer
    b := bufPool.Get().(*bytes.Buffer)
    b.Reset()
    b.WriteString(time.Now().Format(time.RFC3339))
    b.WriteByte(&#39; &#39;)
    b.WriteString(key)
    b.WriteByte(&#39;=&#39;)
    b.WriteString(val)
    w.Write(b.Bytes())
    // 使用完毕，归还 buffer
    bufPool.Put(b)
}

func main() {
    Log(os.Stdout, &amp;quot;path&amp;quot;, &amp;quot;/search?q=flowers&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么什么时候使用sync.Pool？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一个是当我们必须重用共享的和长期存在的对象（例如，数据库连接）时。&lt;/li&gt;
&lt;li&gt;第二个是用于优化内存分配。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sync-mutex&#34;&gt;sync.Mutex&lt;/h2&gt;

&lt;p&gt;sync.Mutex可能是sync包中使用最广泛的原语。它允许在共享资源上互斥访问（不能同时访问），&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/#如何做到并发安全&#34;&gt;锁&lt;/a&gt;在并发安全中有着很重要的作用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mutex := &amp;amp;sync.Mutex{}

mutex.Lock()
// Update共享变量 (比如切片，结构体指针等)
mutex.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sync.RWMutex是一个读写互斥锁，它提供了我们上面的刚刚看到的sync.Mutex的Lock和UnLock方法（因为这两个结构都实现了sync.Locker接口）。但是，它还允许使用RLock和RUnlock方法进行并发读取：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mutex := &amp;amp;sync.RWMutex{}

mutex.Lock()
// Update 共享变量
mutex.Unlock()

mutex.RLock()
// Read 共享变量
mutex.RUnlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只有在频繁读取和不频繁写入的场景里，才应该使用sync.RWMutex。&lt;/p&gt;

&lt;h2 id=&#34;sync-cond&#34;&gt;sync.Cond&lt;/h2&gt;

&lt;p&gt;sync.Cond可能是sync包提供的同步原语中最不常用的一个，它用于发出信号（一对一）或广播信号（一对多）到goroutine。&lt;/p&gt;

&lt;p&gt;条件变量做的事情很简单：让多个 goroutine 等待在某个条件上，如果条件不满足，进入等待状态；如果条件满足，继续运行。&lt;/p&gt;

&lt;p&gt;Cond 内部维护着一个 notifyList，当条件不满足的时候，则将对应的 goroutine 添加到列表上然后进入等待状态。当条件满足时，一般会有其他执行者显式使用 Signal() 或者 Broadcast() 去唤醒 notifyList 上 goroutine。&lt;/p&gt;

&lt;p&gt;当进行条件的判断时，必须使用互斥锁来保证条件的安全，即在判断的时候条件没有被其他人修改。所以 Cond 一般会与一个符合 Lock 接口的 Mutex 一起使用。&lt;/p&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Cond struct {
    // 读写条件状态需要加锁
    L Locker
    // 剩下的是非暴露字段
}

func NewCond(l Locker) *Cond

// 广播所以等待的 goroutine，条件已经满足
func (c *Cond) Broadcast()

// 单播其中一个等待的 goroutine，条件已经满足
func (c *Cond) Signal()

// 如果条件不满足，调用 Wait() 进入等待状态
func (c *Cond) Wait()
此处要特别小心 Wait() 的使用。正如前文所说，条件的判断需要使用互斥锁来确保条件读取前后是一致的，即：

    c.L.Lock() // 进行条件判断，加锁
    if !condition() { // 如果不满足条件，进入 if 中
        c.Wait() // Wait() 内部会自动解锁
    }

    ... 这里可能会对 condition 作出改变 ...
    c.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述代码其实还有一个很严重的问题，为了说明这个问题，让我们来看看 Wait() 的实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cond) Wait() {
    c.checker.check()
    t := runtime_notifyListAdd(&amp;amp;c.notify) // 加入 notifyList
    c.L.Unlock() // 解锁
    runtime_notifyListWait(&amp;amp;c.notify, t) // 进入等待模式
    c.L.Lock() // 运行到此处说明条件已经满足，开始获取互斥锁，如果锁已经被别人用了，开始等待
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的例子可以看出，当 Wait() 返回时（即已经获取到了互斥锁），有可能条件已经被其他先获取互斥锁的 goroutine 改变了，所以此时必须再次判断一下条件，即：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c.L.Lock() // 进行条件判断，加锁
if !condition() { // 如果不满足条件，进入 if 中
    c.Wait() // Wait() 内部会自动解锁
}

if !condition() { // 如果不满足条件，进入 if 中
    c.Wait() // Wait() 内部会自动解锁
}
... 这里可能会对 condition 作出改变 ...
c.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果代码这么写，就太费劲了，上面代码可以简化为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c.L.Lock() // 进行条件判断，加锁
for !condition() { // 如果不满足条件，进入 if 中
    c.Wait() // Wait() 内部会自动解锁
}

... 这里可能会对 condition 作出改变 ...
c.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即将 if 替换为 for，从而当从 Wait() 返回时，再次判断条件是否满足。&lt;/p&gt;

&lt;p&gt;用法实例&lt;/p&gt;

&lt;p&gt;用一个简单的例子来介绍一下 Cond 如何使用，即：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    wakeup    = false
    workerNum = 3
)

func worker(workerID int, c *sync.Cond) {
    fmt.Printf(&amp;quot;Worker [%d] is RUNNING\n&amp;quot;, workerID)
    c.L.Lock()
    for !wakeup {
        fmt.Printf(&amp;quot;Worker [%d] check conditon\n&amp;quot;, workerID)
        c.Wait()
    }
    fmt.Printf(&amp;quot;Worker [%d] wakeup, DO something\n&amp;quot;, workerID)
    // 将唤醒标志改为 false
    // 此时其他已经醒来并抢夺互斥锁的 goroutine 重新判断条件后
    // 将再次进入 wait 状态
    wakeup = false 
    c.L.Unlock()
}

func main() {
    cond := sync.NewCond(&amp;amp;sync.Mutex{})
    for i := 0; i &amp;lt; workerNum; i++ {
        go worker(i, cond)
    }

    time.Sleep(2 * time.Second)
    wakeup = true
    cond.Broadcast() // 向所有 goroutine 进行广播，条件已经满足，即 wakeup = true

    time.Sleep(2 * time.Second)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行后的输出为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Worker [0] is RUNNING
Worker [1] is RUNNING
Worker [0] check conditon
Worker [1] check conditon
Worker [2] is RUNNING
Worker [2] check conditon
Worker [0] wakeup, DO something
Worker [1] check conditon
Worker [2] check conditon
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当 worker0 醒来后，又重新把条件变量进行了修改，从而导致 worker1 和 worker2 获取到互斥锁后重新检查到条件不满足，再次进入 wait 状态。&lt;/p&gt;

&lt;h2 id=&#34;map&#34;&gt;map&lt;/h2&gt;

&lt;p&gt;这是一个重点的数据结构，实现十分经典，具体看&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-map/&#34;&gt;map&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;sync-atomic&#34;&gt;sync/atomic&lt;/h1&gt;

&lt;p&gt;atomic包提供了底层的原子级内存操作，对于同步算法的实现很有用。支持类型共有六种：int32, int64, uint32, uint64, uintptr, unsafe.Pinter，实现的操作共五种：增减，比较并交换，载入，存储，交换。&lt;/p&gt;

&lt;h2 id=&#34;增或减&#34;&gt;增或减&lt;/h2&gt;

&lt;p&gt;顾名思义，原子增或减即可实现对被操作值的增大或减少。因此该操作只能操作数值类型。
　　
被用于进行增或减的原子操作都是以“Add”为前缀，并后面跟针对具体类型的名称。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func AddUint32(addr *uint32, delta uint32) (new uint32)

func AddInt64(addr *int64, delta int64) (new int64)
AddInt64原子性的将val的值添加到*addr并返回新值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;增&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;栗子：（在原来的基础上加n）
atomic.AddUint32(&amp;amp;addr,n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;减&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;栗子：(在原来的基础上加n（n为负数))
atomic.AddUint32(*addr,uint32(int32(n)))
//或
atomic.AddUint32(&amp;amp;addr,^uint32(-n-1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;比较并交换&#34;&gt;比较并交换&lt;/h2&gt;

&lt;p&gt;比较并交换&amp;mdash;-Compare And Swap 简称CAS&lt;/p&gt;

&lt;p&gt;他是假设被操作的值未曾被改变（即与旧值相等），并一旦确定这个假设的真实性就立即进行值替换，这是典型的乐观实现思想。&lt;/p&gt;

&lt;p&gt;如果想安全的并发一些类型的值，我们总是应该优先使用CAS&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子：（如果addr和old相同,就用new代替addr）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ok:=atomic.CompareAndSwapInt32(&amp;amp;addr,old,new)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;载入&#34;&gt;载入&lt;/h2&gt;

&lt;p&gt;如果一个写操作未完成，有一个读操作就已经发生了，这样读操作使很糟糕的。&lt;/p&gt;

&lt;p&gt;为了原子的读取某个值sync/atomic代码包同样为我们提供了一系列的函数。这些函数都以&amp;rdquo;Load&amp;rdquo;为前缀，意为载入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func LoadInt32(addr *int32) (val int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fun addValue(delta int32){
    for{
        v:=atomic.LoadInt32(&amp;amp;addr)
        if atomic.CompareAndSwapInt32(&amp;amp;v,addr,(delta+v)){
            break;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;存储&#34;&gt;存储&lt;/h2&gt;

&lt;p&gt;　　
与读操作对应的是写入操作，sync/atomic也提供了与原子的值载入函数相对应的原子的值存储函数。这些函数的名称均以“Store”为前缀
　　&lt;/p&gt;

&lt;p&gt;在原子的存储某个值的过程中，任何cpu都不会进行针对进行同一个值的读或写操作。如果我们把所有针对此值的写操作都改为原子操作，那么就不会出现针对此值的读操作读操作因被并发的进行而读到修改了一半的情况。
　　
原子操作总会成功，因为他不必关心被操作值的旧值是什么。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func StoreInt32(addr *int32, val int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;atomic.StoreInt32(被操作值的指针,新值)
atomic.StoreInt32(&amp;amp;value,newaddr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;交换&#34;&gt;交换&lt;/h2&gt;

&lt;p&gt;　　
原子交换操作，这类函数的名称都以“Swap”为前缀，与CAS不同，交换操作直接赋予新值，不管旧值。
　　
会返回旧值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func SwapInt32(addr *int32, new int32) (old int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;atomic.SwapInt32(被操作值的指针,新值)（返回旧值）
oldval：=atomic.StoreInt32(&amp;amp;value,newaddr)
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- mutex</title>
          <link>https://kingjcy.github.io/post/golang/go-mutex/</link>
          <pubDate>Thu, 28 Feb 2019 17:42:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-mutex/</guid>
          <description>&lt;p&gt;锁是一种常见的并发控制技术，我们一般会将锁分成乐观锁和悲观锁，即乐观并发控制和悲观并发控制。&lt;/p&gt;

&lt;h1 id=&#34;悲观锁&#34;&gt;悲观锁&lt;/h1&gt;

&lt;p&gt;悲观锁就是我们常用的锁机制，不管它会不会发生，只要存在并发安全问题，就在操作这个资源的时候给他先加上锁。常见的锁有互斥锁，读写锁等。&lt;/p&gt;

&lt;p&gt;golang中除了atomic其他都是悲观锁。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;互斥锁Mutex&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var l sync.Mutex
func foo() {
     l.Lock()
     defer l.Unlock()
     //...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中Mutex为互斥锁，Lock()加锁，Unlock()解锁，如果在使用Unlock()前未加锁，就会引起一个运行错误，使用Lock()加锁后，便不能再次对其进行加锁，直到利用Unlock()解锁对其解锁后，才能再次加锁．适用于读写不确定场景，即读写次数没有明显的区别，并且只允许只有一个读或者写的场景，所以该锁也叫做全局锁。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;读写锁RWMutex&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (rw *RWMutex) Lock()　　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写锁，如果在添加写锁之前已经有其他的读锁和写锁，则lock就会阻塞直到该锁可用，为确保该锁最终可用，已阻塞的 Lock 调用会从获得的锁中排除新的读取器，即写锁权限高于读锁，有写锁时优先进行写锁定&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rw *RWMutex) Unlock()　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写锁解锁，如果没有进行写锁定，则就会引起一个运行时错误&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rw *RWMutex) RLock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读锁，当有写锁时，无法加载读锁，当只有读锁或者没有锁时，可以加载读锁，读锁可以加载多个，所以适用于＂读多写少＂的场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rw *RWMutex)RUnlock()　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读锁解锁，RUnlock 撤销单次RLock 调用，它对于其它同时存在的读取器则没有效果。若 rw 并没有为读取而锁定，调用 RUnlock 就会引发一个运行时错误(注：这种说法在go1.3版本中是不对的，例如下面这个例子)。&lt;/p&gt;

&lt;p&gt;读写锁是针对于读写操作的互斥锁。&lt;/p&gt;

&lt;p&gt;基本遵循两大原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;加读锁后就不能写，允许存在多个读锁，但只能有一把写锁；读锁的时候不能写，可以随便读。多个goroutin同时读。&lt;/li&gt;
&lt;li&gt;加写锁的时候，当写锁未被释放时或此时有正被等待的写锁（只有当全部读锁结束，写锁才可用），读锁不可用；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面再用一个实例来简单介绍一下 RWMutex 的几条规则：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var rw sync.RWMutex

func reader(readerID int) {
    fmt.Printf(&amp;quot;[reader-%d] try to get read lock\n&amp;quot;, readerID)
    rw.RLock()
    fmt.Printf(&amp;quot;[reader-%d] get read lock and sleep\n&amp;quot;, readerID)
    time.Sleep(1 * time.Second)
    fmt.Printf(&amp;quot;[reader-%d] release read lock\n&amp;quot;, readerID)
    rw.RUnlock()
}

func writer(writerID int) {
    fmt.Printf(&amp;quot;[writer-%d] try to get write lock\n&amp;quot;, writerID)
    rw.Lock()
    fmt.Printf(&amp;quot;[writer-%d] get write lock and sleep\n&amp;quot;, writerID)
    time.Sleep(3 * time.Second)
    fmt.Printf(&amp;quot;[writer-%d] release write lock\n&amp;quot;, writerID)
    rw.Unlock()
}

func main() {
    // 启动多个 goroutine 获取 read lock 后 sleep 一段时间
    // 由于此时没有写者，所以两个 reader 都可以同时获取到读锁
    go reader(1)
    go reader(2)

    time.Sleep(500 * time.Millisecond)

    // 写者获取写锁，由于读锁未被释放，所以一开始写者获取不到写锁
    go writer(1)

    time.Sleep(1 * time.Second)

    // 由于写锁还未释放，新的读者获取不到读锁
    go reader(3)
    go reader(4)

    // 主 goroutine 等待足够长时间让所有 goroutine 执行完
    time.Sleep(10 * time.Second)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行后输出为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[reader-2] try to get read lock
[reader-1] try to get read lock
[reader-2] get read lock and sleep
[reader-1] get read lock and sleep
[writer-1] try to get write lock      --&amp;gt; 尝试获取写锁失败，因为读锁未释放
[reader-2] release read lock          --&amp;gt; 读锁释放
[reader-1] release read lock
[writer-1] get write lock and sleep   --&amp;gt; 读锁释放后，获取写锁成功
[reader-4] try to get read lock       --&amp;gt; 获取读锁失败因为写锁未释放
[reader-3] try to get read lock
[writer-1] release write lock         --&amp;gt; 写锁释放
[reader-3] get read lock and sleep    --&amp;gt; 写锁释放后，获取读锁成功
[reader-4] get read lock and sleep
[reader-4] release read lock
[reader-3] release read lock
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;乐观锁&#34;&gt;乐观锁&lt;/h1&gt;

&lt;p&gt;乐观锁并不是一把真正的锁，不像上面的锁一样有api，而是一种并发控制的思想：总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;版本号机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;CAS算法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要读写的内存值 V&lt;/li&gt;
&lt;li&gt;进行比较的值 A&lt;/li&gt;
&lt;li&gt;拟写入的新值 B&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。&lt;/p&gt;

&lt;p&gt;CAS在golang的库sync/atomic中得到了实现。atomic基本使用了乐观锁的原理，但是只是支持int32/int64/uint32/uint64/uintptr这几种数据类型的一些基础操作，操作共五种：增减， 比较并交换， 载入， 存储，交换。&lt;/p&gt;

&lt;p&gt;一般无锁的操作都是使用乐观并发控制思想来实现的。&lt;/p&gt;

&lt;h2 id=&#34;实现方式&#34;&gt;实现方式&lt;/h2&gt;

&lt;p&gt;1、基于数据库的version字段进行实现，在表中新增一个字段version就可以，执行sql的时候加上这个字段的比较&lt;/p&gt;

&lt;p&gt;2、基于缓存数据库实现，比如redis的watch，和memcached的gets和cas&lt;/p&gt;

&lt;h1 id=&#34;锁的原理&#34;&gt;锁的原理&lt;/h1&gt;

&lt;p&gt;锁的实现一般会依赖于信号量，信号量则是一个非负的整数计数器。&lt;/p&gt;

&lt;p&gt;信号量：多线程同步使用的；一个线程完成某个动作后通过信号告诉别的线程，别的线程才可以执行某些动作；信号量可以是多值的，当信号量在0和1之间操作时候就是互斥量&lt;/p&gt;

&lt;p&gt;互斥量：多线程互斥使用的；一个线程占用某个资源，那么别的线程就无法访问，直到该线程离开，其他线程才可以访问该资源；0或1&lt;/p&gt;

&lt;p&gt;具体互斥锁的实现原理可以参考这篇文章：&lt;a href=&#34;https://www.cnblogs.com/sylz/p/6030201.html&#34;&gt;https://www.cnblogs.com/sylz/p/6030201.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;简单来说，就是加锁时，就把信号量减一，如果是零说明加锁成功。释放锁时把信号量加一，如果是一说明释放成功。&lt;/p&gt;

&lt;p&gt;但是在实际应用中大家都使用信号量，因为信号量是多值得，可以通过信号量加等待队列，减少唤醒的次数。&lt;/p&gt;

&lt;p&gt;pv原语&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;P（S）：将信号量S的值减1，即S=S-1；
        如果S&amp;gt;=0，则该进程继续执行；否则该进程置为等待状态，排入等待队列。-----申请资源
V（S）：将信号量S的值加1，即S=S+1；
        如果S&amp;gt;0，则该进程继续执行；否则释放队列中第一个等待信号量的进程。------释放资源
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>数据库系列---- Elasticsearch</title>
          <link>https://kingjcy.github.io/post/database/elasticsearch/</link>
          <pubDate>Thu, 21 Feb 2019 19:28:32 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/database/elasticsearch/</guid>
          <description>&lt;p&gt;开源的 Elasticsearch （以下简称 Elastic）是目前全文搜索引擎的首选。它可以快速地储存、搜索和分析海量数据。并且支持分布式，解决Lucene（支持全文索引的数据库系统）单机问题，目前维基百科、Stack Overflow、Github 都采用它。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;存储结构&#34;&gt;存储结构&lt;/h2&gt;

&lt;p&gt;在ES中，存储结构主要有四种，与传统的关系型数据库对比如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;index（Indices）相当于一个database&lt;/li&gt;
&lt;li&gt;type相当于一个table&lt;/li&gt;
&lt;li&gt;document相当于一个row&lt;/li&gt;
&lt;li&gt;properties（Fields）相当于一个column&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以如下对比&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Relational DB -&amp;gt; Databases -&amp;gt; Tables -&amp;gt; Rows -&amp;gt; Columns
Elasticsearch -&amp;gt; Indices -&amp;gt; Types -&amp;gt; Documents -&amp;gt; Fields
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Node 与 Cluster&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Elastic 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。&lt;/p&gt;

&lt;p&gt;单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Index&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Elastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。&lt;/p&gt;

&lt;p&gt;所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。&lt;/p&gt;

&lt;p&gt;下面的命令可以查看当前节点的所有 Index。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X GET &#39;http://localhost:9200/_cat/indices?v&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Document&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。&lt;/p&gt;

&lt;p&gt;Document 使用 JSON 格式表示，下面是一个例子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;user&amp;quot;: &amp;quot;张三&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
  &amp;quot;desc&amp;quot;: &amp;quot;数据库管理&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Type&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Document 可以分组，比如weather这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。&lt;/p&gt;

&lt;p&gt;不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。&lt;/p&gt;

&lt;p&gt;下面的命令可以列出每个 Index 所包含的 Type。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/_mapping?pretty=true&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据规划，Elastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。&lt;/p&gt;

&lt;h1 id=&#34;基本操作&#34;&gt;基本操作&lt;/h1&gt;

&lt;h2 id=&#34;新建和删除-index&#34;&gt;新建和删除 Index&lt;/h2&gt;

&lt;p&gt;新建 Index，可以直接向 Elastic 服务器发出 PUT 请求。下面的例子是新建一个名叫weather的 Index。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/weather&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务器返回一个 JSON 对象，里面的acknowledged字段表示操作成功。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;acknowledged&amp;quot;:true,
  &amp;quot;shards_acknowledged&amp;quot;:true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，我们发出 DELETE 请求，删除这个 Index。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X DELETE &#39;localhost:9200/weather&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;中文分词设置&#34;&gt;中文分词设置&lt;/h2&gt;

&lt;p&gt;首先，安装中文分词插件。这里使用的是 ik，也可以考虑其他插件（比如 smartcn）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码安装的是5.5.1版的插件，与 Elastic 5.5.1 配合使用。&lt;/p&gt;

&lt;p&gt;接着，重新启动 Elastic，就会自动加载这个新安装的插件。&lt;/p&gt;

&lt;p&gt;然后，新建一个 Index，指定需要分词的字段。这一步根据数据结构而异，下面的命令只针对本文。基本上，凡是需要搜索的中文字段，都要单独设置一下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/accounts&#39; -d &#39;
{
  &amp;quot;mappings&amp;quot;: {
    &amp;quot;person&amp;quot;: {
      &amp;quot;properties&amp;quot;: {
        &amp;quot;user&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
          &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
          &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
        },
        &amp;quot;title&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
          &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
          &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
        },
        &amp;quot;desc&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
          &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
          &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
        }
      }
    }
  }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，首先新建一个名称为accounts的 Index，里面有一个名称为person的 Type。person有三个字段。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user
title
desc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这三个字段都是中文，而且类型都是文本（text），所以需要指定中文分词器，不能使用默认的英文分词器。&lt;/p&gt;

&lt;p&gt;Elastic 的分词器称为 analyzer。我们对每个字段指定分词器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;user&amp;quot;: {
  &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
  &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
  &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，analyzer是字段文本的分词器，search_analyzer是搜索词的分词器。ik_max_word分词器是插件ik提供的，可以对文本进行最大数量的分词。&lt;/p&gt;

&lt;h2 id=&#34;新增记录&#34;&gt;新增记录&lt;/h2&gt;

&lt;p&gt;向指定的 /Index/Type 发送 PUT 请求，就可以在 Index 里面新增一条记录。比如，向/accounts/person发送请求，就可以新增一条人员记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/accounts/person/1&#39; -d &#39;
{
  &amp;quot;user&amp;quot;: &amp;quot;张三&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
  &amp;quot;desc&amp;quot;: &amp;quot;数据库管理&amp;quot;
}&#39; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务器返回的 JSON 对象，会给出 Index、Type、Id、Version 等信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
  &amp;quot;_version&amp;quot;:1,
  &amp;quot;result&amp;quot;:&amp;quot;created&amp;quot;,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:2,&amp;quot;successful&amp;quot;:1,&amp;quot;failed&amp;quot;:0},
  &amp;quot;created&amp;quot;:true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你仔细看，会发现请求路径是/accounts/person/1，最后的1是该条记录的 Id。它不一定是数字，任意字符串（比如abc）都可以。&lt;/p&gt;

&lt;p&gt;新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X POST &#39;localhost:9200/accounts/person&#39; -d &#39;
{
  &amp;quot;user&amp;quot;: &amp;quot;李四&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
  &amp;quot;desc&amp;quot;: &amp;quot;系统管理&amp;quot;
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，向/accounts/person发出一个 POST 请求，添加一个记录。这时，服务器返回的 JSON 对象里面，_id字段就是一个随机字符串。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot;:&amp;quot;AV3qGfrC6jMbsbXb6k1p&amp;quot;,
  &amp;quot;_version&amp;quot;:1,
  &amp;quot;result&amp;quot;:&amp;quot;created&amp;quot;,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:2,&amp;quot;successful&amp;quot;:1,&amp;quot;failed&amp;quot;:0},
  &amp;quot;created&amp;quot;:true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，如果没有先创建 Index（这个例子是accounts），直接执行上面的命令，Elastic 也不会报错，而是直接生成指定的 Index。所以，打字的时候要小心，不要写错 Index 的名称。&lt;/p&gt;

&lt;h2 id=&#34;查看记录&#34;&gt;查看记录&lt;/h2&gt;

&lt;p&gt;向/Index/Type/Id发出 GET 请求，就可以查看这条记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/1?pretty=true&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码请求查看/accounts/person/1这条记录，URL 的参数pretty=true表示以易读的格式返回。&lt;/p&gt;

&lt;p&gt;返回的数据中，found字段表示查询成功，_source字段返回原始记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;_index&amp;quot; : &amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot; : &amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
  &amp;quot;_version&amp;quot; : 1,
  &amp;quot;found&amp;quot; : true,
  &amp;quot;_source&amp;quot; : {
    &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
    &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
    &amp;quot;desc&amp;quot; : &amp;quot;数据库管理&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 Id 不正确，就查不到数据，found字段就是false。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/weather/beijing/abc?pretty=true&#39;

{
  &amp;quot;_index&amp;quot; : &amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot; : &amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot; : &amp;quot;abc&amp;quot;,
  &amp;quot;found&amp;quot; : false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;删除记录&#34;&gt;删除记录&lt;/h2&gt;

&lt;p&gt;删除记录就是发出 DELETE 请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X DELETE &#39;localhost:9200/accounts/person/1&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里先不要删除这条记录，后面还要用到。&lt;/p&gt;

&lt;h2 id=&#34;更新记录&#34;&gt;更新记录&lt;/h2&gt;

&lt;p&gt;更新记录就是使用 PUT 请求，重新发送一次数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/accounts/person/1&#39; -d &#39;
{
    &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
    &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
    &amp;quot;desc&amp;quot; : &amp;quot;数据库管理，软件开发&amp;quot;
}&#39; 

{
  &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
  &amp;quot;_version&amp;quot;:2,
  &amp;quot;result&amp;quot;:&amp;quot;updated&amp;quot;,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:2,&amp;quot;successful&amp;quot;:1,&amp;quot;failed&amp;quot;:0},
  &amp;quot;created&amp;quot;:false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，我们将原始数据从&amp;rdquo;数据库管理&amp;rdquo;改成&amp;rdquo;数据库管理，软件开发&amp;rdquo;。 返回结果里面，有几个字段发生了变化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;_version&amp;quot; : 2,
&amp;quot;result&amp;quot; : &amp;quot;updated&amp;quot;,
&amp;quot;created&amp;quot; : false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，记录的 Id 没变，但是版本（version）从1变成2，操作类型（result）从created变成updated，created字段变成false，因为这次不是新建记录。&lt;/p&gt;

&lt;h2 id=&#34;数据查询&#34;&gt;数据查询&lt;/h2&gt;

&lt;p&gt;返回所有记录&lt;/p&gt;

&lt;p&gt;使用 GET 方法，直接请求/Index/Type/_search，就会返回所有记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;

{
  &amp;quot;took&amp;quot;:2,
  &amp;quot;timed_out&amp;quot;:false,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:5,&amp;quot;successful&amp;quot;:5,&amp;quot;failed&amp;quot;:0},
  &amp;quot;hits&amp;quot;:{
    &amp;quot;total&amp;quot;:2,
    &amp;quot;max_score&amp;quot;:1.0,
    &amp;quot;hits&amp;quot;:[
      {
        &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
        &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
        &amp;quot;_id&amp;quot;:&amp;quot;AV3qGfrC6jMbsbXb6k1p&amp;quot;,
        &amp;quot;_score&amp;quot;:1.0,
        &amp;quot;_source&amp;quot;: {
          &amp;quot;user&amp;quot;: &amp;quot;李四&amp;quot;,
          &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
          &amp;quot;desc&amp;quot;: &amp;quot;系统管理&amp;quot;
        }
      },
      {
        &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
        &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
        &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
        &amp;quot;_score&amp;quot;:1.0,
        &amp;quot;_source&amp;quot;: {
          &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
          &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
          &amp;quot;desc&amp;quot; : &amp;quot;数据库管理，软件开发&amp;quot;
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total：返回记录数，本例是2条。
max_score：最高的匹配程度，本例是1.0。
hits：返回的记录组成的数组。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。&lt;/p&gt;

&lt;h2 id=&#34;全文搜索&#34;&gt;全文搜索&lt;/h2&gt;

&lt;p&gt;Elastic 的查询非常特别，使用自己的查询语法，要求 GET 请求带有数据体。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;软件&amp;quot; }}
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码使用 Match 查询，指定的匹配条件是desc字段里面包含&amp;rdquo;软件&amp;rdquo;这个词。返回结果如下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;took&amp;quot;:3,
  &amp;quot;timed_out&amp;quot;:false,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:5,&amp;quot;successful&amp;quot;:5,&amp;quot;failed&amp;quot;:0},
  &amp;quot;hits&amp;quot;:{
    &amp;quot;total&amp;quot;:1,
    &amp;quot;max_score&amp;quot;:0.28582606,
    &amp;quot;hits&amp;quot;:[
      {
        &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
        &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
        &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
        &amp;quot;_score&amp;quot;:0.28582606,
        &amp;quot;_source&amp;quot;: {
          &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
          &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
          &amp;quot;desc&amp;quot; : &amp;quot;数据库管理，软件开发&amp;quot;
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Elastic 默认一次返回10条结果，可以通过size字段改变这个设置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;管理&amp;quot; }},
  &amp;quot;size&amp;quot;: 1
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码指定，每次只返回一条结果。&lt;/p&gt;

&lt;p&gt;还可以通过from字段，指定位移。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;管理&amp;quot; }},
  &amp;quot;from&amp;quot;: 1,
  &amp;quot;size&amp;quot;: 1
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码指定，从位置1开始（默认是从位置0开始），只返回一条结果。&lt;/p&gt;

&lt;h2 id=&#34;逻辑运算&#34;&gt;逻辑运算&lt;/h2&gt;

&lt;p&gt;如果有多个搜索关键字， Elastic 认为它们是or关系。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;软件 系统&amp;quot; }}
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码搜索的是软件 or 系统。&lt;/p&gt;

&lt;p&gt;如果要执行多个关键词的and搜索，必须使用布尔查询。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot;: {
    &amp;quot;bool&amp;quot;: {
      &amp;quot;must&amp;quot;: [
        { &amp;quot;match&amp;quot;: { &amp;quot;desc&amp;quot;: &amp;quot;软件&amp;quot; } },
        { &amp;quot;match&amp;quot;: { &amp;quot;desc&amp;quot;: &amp;quot;系统&amp;quot; } }
      ]
    }
  }
}&#39;
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus redis Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/redis_exporter/</link>
          <pubDate>Thu, 21 Feb 2019 15:10:33 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/redis_exporter/</guid>
          <description>&lt;p&gt;redis探针主要是监控redis相关情况，比如内存，连接数等。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;1、set log format,支持txt和json两种格式的日志&lt;/p&gt;

&lt;p&gt;2、set log level，支持正常的日志级别&lt;/p&gt;

&lt;p&gt;3、show version 支持option打印版本&lt;/p&gt;

&lt;p&gt;4、解析addr和passwd和alias，这几个参数支持参数配置，也可以重环境变量中获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Environment         Variables
Name                Description
REDIS_ADDR          Address of Redis node(s)
REDIS_PASSWORD      Password to use when authenticating to Redis
REDIS_ALIAS         Alias name of Redis node(s)
REDIS_FILE          Path to file containing Redis node(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、创建一个Exporter结构体，初始化，将上面的信息传进去，并创建对应的指标，checkkey支持对多数据库的查询&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Exporter implements the prometheus.Exporter interface, and exports Redis metrics.
type Exporter struct {
    redis        RedisHost
    namespace    string
    keys         []dbKeyPair
    keyValues    *prometheus.GaugeVec
    keySizes     *prometheus.GaugeVec
    duration     prometheus.Gauge
    scrapeErrors prometheus.Gauge
    totalScrapes prometheus.Counter
    metrics      map[string]*prometheus.GaugeVec
    metricsMtx   sync.RWMutex
    sync.RWMutex
}


// RedisHost represents a set of Redis Hosts to health check.
type RedisHost struct {
    Addrs     []string
    Passwords []string
    Aliases   []string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、然后调用client_golang的库函数进行数据采集，scrape连接数据库，调用命令，然后解析返回参数。完成监控。&lt;/p&gt;

&lt;p&gt;由此可见，redis_exporter只是简单的对redis的单节点进行了监控，并没有对redis的cluster和sentinel进行监控。&lt;/p&gt;

&lt;h1 id=&#34;sentinel监控&#34;&gt;sentinel监控&lt;/h1&gt;

&lt;p&gt;这边需要监控sentinel，这边做一个设计&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;运行天数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_start_time_seconds{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 1.551058403e+09
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标直接解析info信息中的sentinel信息，每次通过执行info命令获取，获取uptime_in_days字段值，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;注册shard数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_masters{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标直接解析info信息中的sentinel信息，每次通过执行info命令获取，获取sentinel_masters字段值，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正常shard的个数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_masters_ok{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标直接解析info信息中的sentinel信息，每次通过执行info命令获取，获取status=ok的个数，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;异常shard的个数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_masters_down{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标直接解析info信息中的sentinel信息，每次通过执行info命令获取，获取status=odown的个数，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;sentinel的连接数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_connects{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标是通过执行netstat命令来统计对应进程的连接数，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;检活&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_status{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标是通过执行redis PING命令来检测返回的是否是PONG，如果是则为1，否则为0，代表着sentinel的死活状态，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;26379端口检测&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis_sentinel_port_isalive{addr=&amp;quot;10.37.152.206:26379&amp;quot;,alias=&amp;quot;&amp;quot;} 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个指标是通过执行netstat命令来统计对应端口为LiSTEN的数量，1代表端口正在监听，0代表没有监听端口，设计标签为通用的addr，alias字段。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前探针融合在redis的探针中，可以自动识别sentinel还是redis，然后放回对应的指标，支持多地址混合配置，可以返回所有需要的指标。&lt;/p&gt;

&lt;h1 id=&#34;连接池&#34;&gt;连接池&lt;/h1&gt;

&lt;p&gt;redis探针目前是短连接，在很多场景下，需要改短连接为长连接，可以只使用一个连接来解决这个问题，这个其实适用于长连接使用的场景，然后考虑长连接可能出来大量连接存在的场景，所以最好直接使用连接池，对连接的数量进行限制，这样就可以完美的解决问题。&lt;/p&gt;

&lt;p&gt;探针启动直接初始化一个连接池，大小为2，正常使用一个长连接来采集高频率的数据，出现异常，可能会使用到第二个长连接。&lt;/p&gt;

&lt;p&gt;验证在高频率的采集下连接数并没有出现增长。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;redis自身支持长连接需要设置参数，tcp_keepalive=1，默认是关闭的，所以都是以timeout时间为准，到时间后redis端会关闭连接，所以探针侧就会close_wait，当然这个连接是可以复用的，当重新请求的时候，又会建立连接在连接池上，按着我们每10s才是一次的频率，redis设置的180s的超时时间，基本上不会出现不断短连接的情形。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Alertmanager</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/alertmanager/</link>
          <pubDate>Tue, 12 Feb 2019 16:00:11 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/alertmanager/</guid>
          <description>&lt;p&gt;Alertmanager主要用于接收 Prometheus 发送的告警信息，它支持丰富的告警通知渠道，而且很容易做到告警信息进行去重，降噪，分组，策略路由，是一款前卫的告警通知系统。&lt;/p&gt;

&lt;h1 id=&#34;安装使用&#34;&gt;安装使用&lt;/h1&gt;

&lt;p&gt;1、获取二进制安装包，直接去prometheus官方去获取就行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export VERSION=0.18.0
wget https://github.com/prometheus/alertmanager/releases/download/v$VERSION/alertmanager-$VERSION.linux-amd64.tar.gz
tar xvf alertmanager-$VERSION.linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、解压成功后，使用 ./alertmanager &amp;ndash;version 来检查是否安装成功&lt;/p&gt;

&lt;p&gt;3、Alertmanager 默认端口为 9093。&lt;/p&gt;

&lt;p&gt;4、配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  resolve_timeout: 2h

route:
  group_by: [&#39;alertname&#39;]
  group_wait: 5s
  group_interval: 10s
  repeat_interval: 1h
  receiver: &#39;webhook&#39;

receivers:
- name: &#39;webhook&#39;
  webhook_configs:
  - url: &#39;http://example.com/xxxx&#39;
    send_resolved: true
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;全局配置（global）：用于定义一些全局的公共参数，如全局的SMTP配置，Slack配置等内容；&lt;/li&gt;
&lt;li&gt;模板（templates）：用于定义告警通知时的模板，如HTML模板，邮件模板等；&lt;/li&gt;
&lt;li&gt;告警路由（route）：根据标签匹配，确定当前告警应该如何处理；&lt;/li&gt;
&lt;li&gt;接收人（receivers）：接收人是一个抽象的概念，它可以是一个邮箱也可以是微信，Slack或者Webhook等，接收人一般配合告警路由使用；&lt;/li&gt;
&lt;li&gt;抑制规则（inhibit_rules）：合理设置抑制规则可以减少垃圾告警的产生&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如使用email告警，配置如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  smtp_smarthost: &#39;smtp.qq.com:587&#39;
  smtp_from: &#39;xxx@qq.com&#39;
  smtp_auth_username: &#39;xxx@qq.com&#39;
  smtp_auth_password: &#39;your_email_password&#39;

route：
  # If an alert has successfully been sent, wait &#39;repeat_interval&#39; to resend them.
  repeat_interval: 10s    
  #  A default receiver
  receiver: team-X-mails  

receivers:
  - name: &#39;team-X-mails&#39;
    email_configs:
    - to: &#39;team-X+alerts@example.org&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、产生告警，在prometheus下添加 alert.rules 文件，指定告警规则&lt;/p&gt;

&lt;p&gt;文件中写入以下简单规则作为示例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALERT memory_high
  IF prometheus_local_storage_memory_series &amp;gt;= 0
  FOR 15s
  ANNOTATIONS {
    summary = &amp;quot;Prometheus using more memory than it should {{ $labels.instance }}&amp;quot;,
    description = &amp;quot;{{ $labels.instance }} has lots of memory man (current value: {{ $value }}s)&amp;quot;,
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 prometheus.yml 文件&lt;/p&gt;

&lt;p&gt;添加以下规则：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rule_files:
  - &amp;quot;alert.rules&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动AlertManager服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./Alertmanager -config.file=simple.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动prometheus服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./prometheus -Alertmanager.url=http://localhost:9093
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据以上步骤设置，此时 “team-X+alerts@example.org” 应该就可以收到 “xxx@qq.com” 发送的告警邮件了。&lt;/p&gt;

&lt;h1 id=&#34;架构原理&#34;&gt;架构原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/alertmanager/alertmanager&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从左上开始，Prometheus 发送的警报到 Alertmanager;&lt;/li&gt;
&lt;li&gt;警报会被存储到 AlertProvider 中，Alertmanager 的内置实现就是包了一个 map，也就是存放在本机内存中，这里可以很容易地扩展其它 Provider;&lt;/li&gt;
&lt;li&gt;Dispatcher 是一个单独的 goroutine，它会不断到 AlertProvider 拉新的警报，并且根据 YAML 配置的 Routing Tree 将警报路由到一个分组中;&lt;/li&gt;
&lt;li&gt;分组会定时进行 flush (间隔为配置参数中的 group_interval), flush 后这组警报会走一个 Notification Pipeline 链式处理;&lt;/li&gt;
&lt;li&gt;Notification Pipeline 为这组警报确定发送目标，并执行抑制逻辑，静默逻辑，去重逻辑，发送与重试逻辑，实现警报的最终投递;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Client_golang</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/</link>
          <pubDate>Tue, 12 Feb 2019 15:59:42 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/</guid>
          <description>&lt;p&gt;client_golang 是Prometheus client的使用，基于golang语言。提供了prometheus的数据规范。&lt;/p&gt;

&lt;h1 id=&#34;库结构&#34;&gt;库结构&lt;/h1&gt;

&lt;p&gt;地址： &lt;a href=&#34;https://github.com/prometheus/client_golang&#34;&gt;https://github.com/prometheus/client_golang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Directories（描述）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;api                     Package api provides clients for the HTTP APIs.
api/prometheus/v1       Package v1 provides bindings to the Prometheus HTTP API v1: http://prometheus.io/docs/querying/api/

examples/random         A simple example exposing fictional RPC latencies with different types of random distributions (uniform, normal, and exponential) as Prometheus metrics.

examples/simple         A minimal example of how to include Prometheus instrumentation.
prometheus              Package prometheus is the core instrumentation package.
prometheus/graphite     Package graphite provides a bridge to push Prometheus metrics to a Graphite server.
prometheus/promauto     Package promauto provides constructors for the usual Prometheus metrics that return them already registered with the global registry (prometheus.DefaultRegisterer).
prometheus/promhttp     Package promhttp provides tooling around HTTP servers and clients.
prometheus/push         Package push provides functions to push metrics to a Pushgateway.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面对client_golang库的结构和使用进行了总结。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;原理解析&#34;&gt;原理解析&lt;/h2&gt;

&lt;p&gt;下面是客户端的UML图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/library/client_golang/client_golang.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、默认调用gocollect这个采集器，在引入package registry包的时候，就会调用init初始化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;registry会调用describe这个接口，实现就是gocollect这个对应的describe&lt;/li&gt;
&lt;li&gt;http.handle会调用registry的gather函数，然后函数调用collect接口，实现就是gocollect这个对应的collect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、上面只是一种特殊类型，其实对应的四种类型分别都有对应的结构体继承vec的基本函数接口，也有对应的接口，会有对应的实现&lt;/p&gt;

&lt;p&gt;然后这个四种类型就是就是四种collecter，同样的流程&lt;/p&gt;

&lt;p&gt;3、可以新建一个struct作为一个collecter，实现describe，collect接口，就可以实现自己的逻辑，最后其实还是调用四种类型，结合使用&lt;/p&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;p&gt;1、四种类型有实现的函数赋值，常用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set（）
WithLabelValues().set()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、注册的几种方式&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;第一种&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;//statement
proxyTodayTrafficIn := prometheus.NewGaugeVec(prometheus.GaugeOpts{
    Name: &amp;quot;&amp;quot;,
    Help: &amp;quot;The today trafficin of proxy.&amp;quot;,
},[]string{&amp;quot;type&amp;quot;,&amp;quot;laststarttime&amp;quot;,&amp;quot;lastclosetime&amp;quot;})
//get value
proxyTodayTrafficIn.With(prometheus.Labels{&amp;quot;type&amp;quot;:v.Type,&amp;quot;laststarttime&amp;quot;:v.LastStartTime,&amp;quot;lastclosetime&amp;quot;:v.LastCloseTime}).Set(float64(v.TodayTrafficIn))

//registry
prometheus.MustRegister(proxyTodayTrafficIn)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;第二种&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;serverBindPort := prometheus.NewGauge(prometheus.GaugeOpts{
    Name: &amp;quot;frps_server_bind_port&amp;quot;,
    Help: &amp;quot;The port of server frps.&amp;quot;,
})

serverBindPort.Set(float64(cfg.BindPort))

//registry
prometheus.MustRegister(serverBindPort)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面两种可以归为一类，都是采用的默认的方式，下面就涉及到自定义结构体，根据上面的原理，我们需要重自定义的结构体中获取到两个结构体的值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *结构体) Describe(ch chan&amp;lt;- *prometheus.Desc) {}----可见这个接口的实现需要将prometheus.Desc放倒channel中去
func (s *结构体) Collect(ch chan&amp;lt;- prometheus.Metric) {}----可见这个接口的实现需要将prometheus.Metric放倒channel中去
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看这两个结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Desc struct {
    // fqName has been built from Namespace, Subsystem, and Name.
    fqName string
    // help provides some helpful information about this metric.
    help string
    // constLabelPairs contains precalculated DTO label pairs based on
    // the constant labels.
    constLabelPairs []*dto.LabelPair
    // VariableLabels contains names of labels for which the metric
    // maintains variable values.
    variableLabels []string
    // id is a hash of the values of the ConstLabels and fqName. This
    // must be unique among all registered descriptors and can therefore be
    // used as an identifier of the descriptor.
    id uint64
    // dimHash is a hash of the label names (preset and variable) and the
    // Help string. Each Desc with the same fqName must have the same
    // dimHash.
    dimHash uint64
    // err is an error that occured during construction. It is reported on
    // registration time.
    err error
}

type Metric interface {
    // Desc returns the descriptor for the Metric. This method idempotently
    // returns the same descriptor throughout the lifetime of the
    // Metric. The returned descriptor is immutable by contract. A Metric
    // unable to describe itself must return an invalid descriptor (created
    // with NewInvalidDesc).
    Desc() *Desc
    // Write encodes the Metric into a &amp;quot;Metric&amp;quot; Protocol Buffer data
    // transmission object.
    //
    // Metric implementations must observe concurrency safety as reads of
    // this metric may occur at any time, and any blocking occurs at the
    // expense of total performance of rendering all registered
    // metrics. Ideally, Metric implementations should support concurrent
    // readers.
    //
    // While populating dto.Metric, it is the responsibility of the
    // implementation to ensure validity of the Metric protobuf (like valid
    // UTF-8 strings or syntactically valid metric and label names). It is
    // recommended to sort labels lexicographically. (Implementers may find
    // LabelPairSorter useful for that.) Callers of Write should still make
    // sure of sorting if they depend on it.
    Write(*dto.Metric) error
    // TODO(beorn7): The original rationale of passing in a pre-allocated
    // dto.Metric protobuf to save allocations has disappeared. The
    // signature of this method should be changed to &amp;quot;Write() (*dto.Metric,
    // error)&amp;quot;.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我看看如何获取这两种值，首先desc，每种数据类型都有一个desc函数可以直接获取，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name := fmt.Sprintf(&amp;quot;%s_%s&amp;quot;, namespace, metricMaps.Name)

gaugeDescription := prometheus.NewGauge(
    prometheus.GaugeOpts{
        Name:      name,
        Help:      metricMaps.Description,
    },
)

ch &amp;lt;- gaugeDescription.Desc()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以直接新建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewDesc(fqName, help string, variableLabels []string, constLabels Labels) *Desc {}

//new desc
desc := prometheus.NewDesc(name, metricMaps.Description, constLabels, nil)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看metrics这个接口，找到其相应的结构体实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MustNewConstMetric(desc *Desc, valueType ValueType, value float64, labelValues ...string) Metric {}

//channel
ch &amp;lt;- prometheus.MustNewConstMetric(desc, vtype, value, labelValue...)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;第三种&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;新建结构体，完成上面方法的使用，就可以了，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    //set var
    vtype := prometheus.CounterValue
    name := fmt.Sprintf(&amp;quot;%s_%s&amp;quot;, namespace, metricMaps.Name)
    log.Debugf(&amp;quot;counter name: %s&amp;quot;, name)

    //new desc
    desc := prometheus.NewDesc(name, metricMaps.Description, constLabels, nil)

    //deal Value
    value, err := dealValue(res[i])
    if err != nil {
        log.Errorf(&amp;quot;parse value error: %s&amp;quot;,err)
        break
    }
    log.Debugf(&amp;quot;counter value: %s&amp;quot;, value)

    //channel
    ch &amp;lt;- prometheus.MustNewConstMetric(desc, vtype, value, labelValue...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Newregistry调用和直接调用prometheus的对应的MustRegister其实是一样的，都是默认new一个registry的结构体&lt;/p&gt;

&lt;p&gt;4、duplicate metrics collector registration attempted&amp;mdash;重复注册&lt;/p&gt;

&lt;h2 id=&#34;collector&#34;&gt;Collector&lt;/h2&gt;

&lt;p&gt;Collector 中 Describe 和 Collect 方法都是无状态的函数，其中 Describe 暴露全部可能的 Metric 描述列表，在注册（Register）或注销（Unregister）Collector 时会调用 Describe 来获取完整的 Metric 列表，用以检测 Metric 定义的冲突，另外在 github.com/prometheus/client_golang/prometheus/promhttp 下的 Instrument Handler 中，也会通过 Describe 获取 Metric 列表，并检查 label 列表（InstrumentHandler 中只支持 code 和 method 两种自定义 label）；而通过 Collect 可以获取采样数据，然后通过 HTTP 接口暴露给 Prom Server。另外，一些临时性的进程，如批处理任务，可以把数据 push 到 Push Gateway，由 Push Gateway 暴露 pull 接口，此处不赘述。&lt;/p&gt;

&lt;p&gt;客户端对数据的收集大多是针对标准数据结构来进行的,如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Counter：收集事件次数等单调递增的数据&lt;/li&gt;
&lt;li&gt;Gauge：收集当前的状态，可增可减，比如数据库连接数&lt;/li&gt;
&lt;li&gt;Histogram：收集随机正态分布数据，比如响应延迟&lt;/li&gt;
&lt;li&gt;Summary：收集随机正态分布数据，和 Histogram 是类似的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每种标准数据结构还对应了 Vec 结构，通过 Vec 可以简洁的定义一组相同性质的 Metric，在采集数据的时候传入一组自定义的 Label/Value 获取具体的 Metric（Counter/Gauge/Histogram/Summary），最终都会落实到基本的数据结构上，这里不再赘述。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Counter 和 Gauge&lt;/p&gt;

&lt;p&gt;Gauge 和 Counter 基本实现上看是一个进程内共享的浮点数，基于 value 结构实现，而 Counter 和 Gauge 仅仅封装了对这个共享浮点数的各种操作和合法性检查逻辑。&lt;/p&gt;

&lt;p&gt;看 Counter 中 Inc 函数的实现&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/library/client_golang/add.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;value.Add 中修改共享数据时采用了“无锁”实现，相比“有锁 (Mutex)”实现可以更充分利用多核处理器的并行计算能力，性能相比加 Mutex 的实现会有很大提升。下图是 Go Benchmark 的测试结果，对比了“有锁”（用 defer 或不用 defer 来释放锁）和“无锁”实现在多核场景下对性能的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/library/client_golang/benchmark.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Histogram&lt;/p&gt;

&lt;p&gt;Histogram 实现了 Observer 接口，用来获取客户端状态初始化（重启）到某个时间点的采样点分布，监控数据常需要服从正态分布。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Summary&lt;/p&gt;

&lt;p&gt;Summary 是标准数据结构中最复杂的一个，用来收集服从正态分布的采样数据。在 Go 客户端 Summary 结构和 Histogram 一样，都实现了 Observer 接口&lt;/p&gt;

&lt;p&gt;这两个比较复杂，使用较少，可以先不研究，使用的时候研究&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;集成注意事项&#34;&gt;集成注意事项&lt;/h2&gt;

&lt;p&gt;Go 客户端为 HTTP 层的集成提供了方便的 API，但使用中需要注意不要使用 github.com/prometheus/client_golang/prometheus 下定义的已经 deprecated 的 Instrument 函数，除了会引入额外（通常不需要）的监控数据，不仅会对程序性能造成不利影响，而且可能存在危险的 race（如计算请求大小时存在 goroutine 并发地访问 Header 逻辑）。&lt;/p&gt;

&lt;h1 id=&#34;prometheus-exporter实例&#34;&gt;Prometheus Exporter实例&lt;/h1&gt;

&lt;p&gt;Exporter是基于Prometheus实施的监控系统中重要的组成部分，承担数据指标的采集工作，官方的exporter列表中已经包含了常见的绝大多数的系统指标监控，比如用于机器性能监控的node_exporter, 用于网络设备监控的snmp_exporter等等。这些已有的exporter对于监控来说，仅仅需要很少的配置工作就能提供完善的数据指标采集。&lt;/p&gt;

&lt;p&gt;有时我们需要自己去写一些与业务逻辑比较相关的指标监控，这些指标无法通过常见的exporter获取到。比如我们需要提供对于DNS解析情况的整体监控，了解如何编写exporter对于业务监控很重要，也是完善监控系统需要经历的一个阶段。接下来我们就介绍如何编写exporter, 本篇内容编写的语言为golang, 官方也提供了python, java等其他的语言实现的库，采集方式其实大同小异。编写exporter的方式也是大同小异，就是集成对应的prometheus库，我们使用golang语言，就是集成client_golang。&lt;/p&gt;

&lt;p&gt;下面我们就使用golang语言集成cleint_golang来开发一个exporter。&lt;/p&gt;

&lt;h2 id=&#34;搭建环境&#34;&gt;搭建环境&lt;/h2&gt;

&lt;p&gt;首先确保机器上安装了go语言(1.7版本以上)，并设置好了对应的GOPATH。接下来我们就可以开始编写代码了。以下是一个简单的exporter&lt;/p&gt;

&lt;p&gt;下载对应的prometheus包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/prometheus/client_golang/prometheus/promhttp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序主函数:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;log&amp;quot;
    &amp;quot;net/http&amp;quot;
    &amp;quot;github.com/prometheus/client_golang/prometheus/promhttp&amp;quot;
)
func main() {
    http.Handle(&amp;quot;/metrics&amp;quot;, promhttp.Handler())
    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个代码中我们仅仅通过http模块指定了一个路径，并将client_golang库中的promhttp.Handler()作为处理函数传递进去后，就可以获取指标信息了,两行代码实现了一个exporter。这里内部其实是使用了一个默认的收集器将通过NewGoCollector采集当前Go运行时的相关信息比如go堆栈使用,goroutine的数据等等。 通过访问&lt;a href=&#34;http://localhost:8080/metrics&#34;&gt;http://localhost:8080/metrics&lt;/a&gt; 即可查看详细的指标参数。&lt;/p&gt;

&lt;p&gt;上面的代码仅仅展示了一个默认的采集器，并且通过接口调用隐藏了太多实施细节，对于下一步开发并没什么作用，为了实现自定义的监控我们需要先了解一些基本概念。&lt;/p&gt;

&lt;h2 id=&#34;指标类别&#34;&gt;指标类别&lt;/h2&gt;

&lt;p&gt;Prometheus中主要使用的四类指标类型，如下所示&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Counter (累加指标)&lt;/li&gt;
&lt;li&gt;Gauge (测量指标)&lt;/li&gt;
&lt;li&gt;Summary (概略图)&lt;/li&gt;
&lt;li&gt;Histogram (直方图)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些我们在上面基本原理中已经介绍过了，这边详细的介绍，并在下面加以使用。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Counter 一个累加指标数据，这个值随着时间只会逐渐的增加，比如程序完成的总任务数量，运行错误发生的总次数。常见的还有交换机中snmp采集的数据流量也属于该类型，代表了持续增加的数据包或者传输字节累加值。&lt;/li&gt;
&lt;li&gt;Gauge代表了采集的一个单一数据，这个数据可以增加也可以减少，比如CPU使用情况，内存使用量，硬盘当前的空间容量等等&lt;/li&gt;
&lt;li&gt;Histogram和Summary使用的频率较少，两种都是基于采样的方式。另外有一些库对于这两个指标的使用和支持程度不同，有些仅仅实现了部分功能。这两个类型对于某一些业务需求可能比较常见，比如查询单位时间内：总的响应时间低于300ms的占比，或者查询95%用户查询的门限值对应的响应时间是多少。 使用Histogram和Summary指标的时候同时会产生多组数据，_count代表了采样的总数，_sum则代表采样值的和。 _bucket则代表了落入此范围的数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是使用historam来定义的一组指标，计算出了平均五分钟内的查询请求小于0.3s的请求占比总量的比例值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  sum(rate(http_request_duration_seconds_bucket{le=&amp;quot;0.3&amp;quot;}[5m])) by (job)
/
  sum(rate(http_request_duration_seconds_count[5m])) by (job)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果需要聚合数据，可以使用histogram. 并且如果对于分布范围有明确的值的情况下（比如300ms），也可以使用histogram。但是如果仅仅是一个百分比的值（比如上面的95%），则使用Summary&lt;/p&gt;

&lt;h2 id=&#34;定义指标&#34;&gt;定义指标&lt;/h2&gt;

&lt;p&gt;这里我们需要引入另一个依赖库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/prometheus/client_golang/prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面先来定义了两个指标数据，一个是Guage类型， 一个是Counter类型。分别代表了CPU温度和磁盘失败次数统计，使用上面的定义进行分类。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cpuTemp = prometheus.NewGauge(prometheus.GaugeOpts{
    Name: &amp;quot;cpu_temperature_celsius&amp;quot;,
    Help: &amp;quot;Current temperature of the CPU.&amp;quot;,
})
hdFailures = prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Name: &amp;quot;hd_errors_total&amp;quot;,
        Help: &amp;quot;Number of hard-disk errors.&amp;quot;,
    },
    []string{&amp;quot;device&amp;quot;},
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;加一个counter的用法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;totalScrapes: prometheus.NewCounter(prometheus.CounterOpts{
            Namespace: namespace,
            Name:      &amp;quot;exporter_scrapes_total&amp;quot;,
            Help:      &amp;quot;Current total redis scrapes.&amp;quot;,
        })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里还可以注册其他的参数，比如上面的磁盘失败次数统计上，我们可以同时传递一个device设备名称进去，这样我们采集的时候就可以获得多个不同的指标。每个指标对应了一个设备的磁盘失败次数统计。&lt;/p&gt;

&lt;h2 id=&#34;注册指标&#34;&gt;注册指标&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    // Metrics have to be registered to be exposed:
    prometheus.MustRegister(cpuTemp)
    prometheus.MustRegister(hdFailures)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用prometheus.MustRegister是将数据直接注册到Default Registry，就像上面的运行的例子一样，这个Default Registry不需要额外的任何代码就可以将指标传递出去。注册后既可以在程序层面上去使用该指标了，这里我们使用之前定义的指标提供的API（Set和With().Inc）去改变指标的数据内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    cpuTemp.Set(65.3)
    hdFailures.With(prometheus.Labels{&amp;quot;device&amp;quot;:&amp;quot;/dev/sda&amp;quot;}).Inc()

    // The Handler function provides a default handler to expose metrics
    // via an HTTP server. &amp;quot;/metrics&amp;quot; is the usual endpoint for that.
    http.Handle(&amp;quot;/metrics&amp;quot;, promhttp.Handler())
    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中With函数是传递到之前定义的label=”device”上的值，也就是生成指标类似于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cpu_temperature_celsius 65.3
hd_errors_total{&amp;quot;device&amp;quot;=&amp;quot;/dev/sda&amp;quot;} 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然我们写在main函数中的方式是有问题的，这样这个指标仅仅改变了一次，不会随着我们下次采集数据的时候发生任何变化，我们希望的是每次执行采集的时候，程序都去自动的抓取指标并将数据通过http的方式传递给我们。&lt;/p&gt;

&lt;p&gt;到这里，一套基本的采集流程也就完成了，这是最基本的使用方式，当然其中也还是封装了很多过程，比如采集器等，如果需要自定义一些东西，就要了解这些封装的过程，完成重写，下面我们自定义exporter。&lt;/p&gt;

&lt;h2 id=&#34;自定义exporter&#34;&gt;自定义exporter&lt;/h2&gt;

&lt;p&gt;counter数据采集实例，重写collecter&lt;/p&gt;

&lt;p&gt;下面是一个采集Counter类型数据的实例，这个例子中实现了一个自定义的，满足采集器(Collector)接口的结构体，并手动注册该结构体后，使其每次查询的时候自动执行采集任务。&lt;/p&gt;

&lt;p&gt;我们先来看下采集器Collector接口的实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Collector interface {
    // 用于传递所有可能的指标的定义描述符
    // 可以在程序运行期间添加新的描述，收集新的指标信息
    // 重复的描述符将被忽略。两个不同的Collector不要设置相同的描述符
    Describe(chan&amp;lt;- *Desc)

    // Prometheus的注册器调用Collect执行实际的抓取参数的工作，
    // 并将收集的数据传递到Channel中返回
    // 收集的指标信息来自于Describe中传递，可以并发的执行抓取工作，但是必须要保证线程的安全。
    Collect(chan&amp;lt;- Metric)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;了解了接口的实现后，我们就可以写自己的实现了，先定义结构体，这是一个集群的指标采集器，每个集群都有自己的Zone,代表集群的名称。另外两个是保存的采集的指标。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ClusterManager struct {
    Zone         string
    OOMCountDesc *prometheus.Desc
    RAMUsageDesc *prometheus.Desc
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来实现一个采集工作,放到了ReallyExpensiveAssessmentOfTheSystemState函数中实现，每次执行的时候，返回一个按照主机名作为键采集到的数据，两个返回值分别代表了OOM错误计数，和RAM使用指标信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *ClusterManager) ReallyExpensiveAssessmentOfTheSystemState() (
    oomCountByHost map[string]int, ramUsageByHost map[string]float64,
) {
    oomCountByHost = map[string]int{
        &amp;quot;foo.example.org&amp;quot;: int(rand.Int31n(1000)),
        &amp;quot;bar.example.org&amp;quot;: int(rand.Int31n(1000)),
    }
    ramUsageByHost = map[string]float64{
        &amp;quot;foo.example.org&amp;quot;: rand.Float64() * 100,
        &amp;quot;bar.example.org&amp;quot;: rand.Float64() * 100,
    }
    return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现Describe接口，传递指标描述符到channel&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Describe simply sends the two Descs in the struct to the channel.
func (c *ClusterManager) Describe(ch chan&amp;lt;- *prometheus.Desc) {
    ch &amp;lt;- c.OOMCountDesc
    ch &amp;lt;- c.RAMUsageDesc
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Collect函数将执行抓取函数并返回数据，返回的数据传递到channel中，并且传递的同时绑定原先的指标描述符。以及指标的类型（一个Counter和一个Guage）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *ClusterManager) Collect(ch chan&amp;lt;- prometheus.Metric) {
    oomCountByHost, ramUsageByHost := c.ReallyExpensiveAssessmentOfTheSystemState()
    for host, oomCount := range oomCountByHost {
        ch &amp;lt;- prometheus.MustNewConstMetric(
            c.OOMCountDesc,
            prometheus.CounterValue,
            float64(oomCount),
            host,
        )
    }
    for host, ramUsage := range ramUsageByHost {
        ch &amp;lt;- prometheus.MustNewConstMetric(
            c.RAMUsageDesc,
            prometheus.GaugeValue,
            ramUsage,
            host,
        )
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建结构体及对应的指标信息,NewDesc参数第一个为指标的名称，第二个为帮助信息，显示在指标的上面作为注释，第三个是定义的label名称数组，第四个是定义的Labels&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewClusterManager(zone string) *ClusterManager {
    return &amp;amp;ClusterManager{
        Zone: zone,
        OOMCountDesc: prometheus.NewDesc(
            &amp;quot;clustermanager_oom_crashes_total&amp;quot;,
            &amp;quot;Number of OOM crashes.&amp;quot;,
            []string{&amp;quot;host&amp;quot;},
            prometheus.Labels{&amp;quot;zone&amp;quot;: zone},
        ),
        RAMUsageDesc: prometheus.NewDesc(
            &amp;quot;clustermanager_ram_usage_bytes&amp;quot;,
            &amp;quot;RAM usage as reported to the cluster manager.&amp;quot;,
            []string{&amp;quot;host&amp;quot;},
            prometheus.Labels{&amp;quot;zone&amp;quot;: zone},
        ),
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行主程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    workerDB := NewClusterManager(&amp;quot;db&amp;quot;)
    workerCA := NewClusterManager(&amp;quot;ca&amp;quot;)

    // Since we are dealing with custom Collector implementations, it might
    // be a good idea to try it out with a pedantic registry.
    reg := prometheus.NewPedanticRegistry()
    reg.MustRegister(workerDB)
    reg.MustRegister(workerCA)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果直接执行上面的参数的话，不会获取任何的参数，因为程序将自动推出，我们并未定义http接口去暴露数据出来，因此数据在执行的时候还需要定义一个httphandler来处理http请求。&lt;/p&gt;

&lt;p&gt;添加下面的代码到main函数后面，即可实现数据传递到http接口上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gatherers := prometheus.Gatherers{
    prometheus.DefaultGatherer,
    reg,
}

h := promhttp.HandlerFor(gatherers,
    promhttp.HandlerOpts{
        ErrorLog:      log.NewErrorLogger(),
        ErrorHandling: promhttp.ContinueOnError,
    })
http.HandleFunc(&amp;quot;/metrics&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
    h.ServeHTTP(w, r)
})
log.Infoln(&amp;quot;Start server at :8080&amp;quot;)
if err := http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil); err != nil {
    log.Errorf(&amp;quot;Error occur when start server %v&amp;quot;, err)
    os.Exit(1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中prometheus.Gatherers用来定义一个采集数据的收集器集合，可以merge多个不同的采集数据到一个结果集合，这里我们传递了缺省的DefaultGatherer，所以他在输出中也会包含go运行时指标信息。同时包含reg是我们之前生成的一个注册对象，用来自定义采集数据。&lt;/p&gt;

&lt;p&gt;promhttp.HandlerFor()函数传递之前的Gatherers对象，并返回一个httpHandler对象，这个httpHandler对象可以调用其自身的ServHTTP函数来接手http请求，并返回响应。其中promhttp.HandlerOpts定义了采集过程中如果发生错误时，继续采集其他的数据。&lt;/p&gt;

&lt;p&gt;尝试刷新几次浏览器获取最新的指标信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clustermanager_oom_crashes_total{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 364
clustermanager_oom_crashes_total{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 90
clustermanager_oom_crashes_total{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 844
clustermanager_oom_crashes_total{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 801
# HELP clustermanager_ram_usage_bytes RAM usage as reported to the cluster manager.
# TYPE clustermanager_ram_usage_bytes gauge
clustermanager_ram_usage_bytes{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 10.738111282075208
clustermanager_ram_usage_bytes{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 19.003276633920805
clustermanager_ram_usage_bytes{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 79.72085409108028
clustermanager_ram_usage_bytes{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 13.041384617379178
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每次刷新的时候，我们都会获得不同的数据，类似于实现了一个数值不断改变的采集器。当然，具体的指标和采集函数还需要按照需求进行修改，满足实际的业务需求。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/</link>
          <pubDate>Tue, 12 Feb 2019 15:59:42 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/</guid>
          <description>&lt;p&gt;Exporter 本质上就是将收集的数据，转化为对应的文本格式，并提供 http 请求。&lt;/p&gt;

&lt;h1 id=&#34;文本格式&#34;&gt;文本格式&lt;/h1&gt;

&lt;p&gt;Exporter 收集的数据转化的文本内容以行 (\n) 为单位，空行将被忽略, 文本内容最后一行为空行。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;注释&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文本内容，如果以 # 开头通常表示注释。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;以 # HELP 开头表示 metric 帮助说明。&lt;/li&gt;
&lt;li&gt;以 # TYPE 开头表示定义 metric 类型，包含 counter, gauge, histogram, summary, 和 untyped 类型。&lt;/li&gt;
&lt;li&gt;其他表示一般注释，供阅读使用，将被 Prometheus 忽略。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;采样数据的样式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;内容如果不以 # 开头，表示采样数据。它通常紧挨着类型定义行，满足以下格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metric_name [
  &amp;quot;{&amp;quot; label_name &amp;quot;=&amp;quot; `&amp;quot;` label_value `&amp;quot;` { &amp;quot;,&amp;quot; label_name &amp;quot;=&amp;quot; `&amp;quot;` label_value `&amp;quot;` } [ &amp;quot;,&amp;quot; ] &amp;quot;}&amp;quot;
] value [ timestamp ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面是一个完整的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# HELP http_requests_total The total number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{method=&amp;quot;post&amp;quot;,code=&amp;quot;200&amp;quot;} 1027 1395066363000
http_requests_total{method=&amp;quot;post&amp;quot;,code=&amp;quot;400&amp;quot;}    3 1395066363000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个 exporter 就是将收集的数据转化为文本格式，并提供 http 请求即可，那很容自己实现一个，prometheus官方提供了client各种语言的库，比如go语言的clent-golang，只要集成使用就能输出对应的指标。&lt;/p&gt;

&lt;h1 id=&#34;client-golang&#34;&gt;client_golang&lt;/h1&gt;

&lt;p&gt;我们开发探针都是基于官方提供的语言库，最多使用的还是golang，我们需要重点了解&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/&#34;&gt;client_golang&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;exporter&#34;&gt;exporter&lt;/h1&gt;

&lt;p&gt;通过各种client库，我们可以开发各种采集探针，将数据转为固定的文本格式来给监控提供数据。不同的监控通过不同的探针来实现，下面是一些常见探针的使用说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/node-exporter/&#34;&gt;Node_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/grok_exporter/&#34;&gt;grok_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/log/mtail/&#34;&gt;mtail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/server/nginx&#34;&gt;nginx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/blackbox_exporter/&#34;&gt;blackbox_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/gpu_exporter/&#34;&gt;gpu_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/mysqld-exporter/&#34;&gt;mysqld_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/postgres_exporter/&#34;&gt;postgres_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/process_exporter/&#34;&gt;process_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/redis_exporter/&#34;&gt;redis_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/snmp_exporter/&#34;&gt;snmp_Exporter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;其他探针&#34;&gt;其他探针&lt;/h2&gt;

&lt;p&gt;还有其他很多采集器，可以直接去prometheus官方网站看目前发布的&lt;a href=&#34;https://prometheus.io/docs/instrumenting/exporters/&#34;&gt;https://prometheus.io/docs/instrumenting/exporters/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;开发注意&#34;&gt;开发注意&lt;/h1&gt;

&lt;p&gt;1、探针日志注意事项：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;正常情况下应该不输出日志，只会在有错误的情况下输出错误日志&lt;/li&gt;
&lt;li&gt;如果输出日志了，应该设置定时清理脚本&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个也适用于很多服务器下面的情况，毕竟如果程序如果跑在了别人的机器上，你是不容易操作的。&lt;/p&gt;

&lt;p&gt;2、探针的管理&lt;/p&gt;

&lt;p&gt;prometheus的exporter都是独立的，简单几个使用还是不错，解耦还开箱即用，但是数量多了，运维的压力变大了，例如探针管理升级，运行情况的检查等，有几种方案解决&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;做一个管理平台，类似于后台系统，专门对exporter进行管理&lt;/li&gt;
&lt;li&gt;用一个主进程整合几个探针，每个探针依旧是原来的版本&lt;/li&gt;
&lt;li&gt;用telegraf来支持各种类型的input，all in one&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Sort</title>
          <link>https://kingjcy.github.io/post/golang/go-sort/</link>
          <pubDate>Fri, 01 Feb 2019 11:54:17 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-sort/</guid>
          <description>&lt;p&gt;golang中也实现了排序算法的包sort包．&lt;/p&gt;

&lt;h1 id=&#34;sort&#34;&gt;sort&lt;/h1&gt;

&lt;p&gt;sort包中实现了4种基本的排序算法：插入排序．归并排序，快排和堆排序．和其他语言中一样，这四种方式都是不公开的，他们只在sort包内部使用．所以用户在使用sort包进行排序时无需考虑使用那种排序方式，sort包会根据实际数据自动选择高效的排序算法，只要实现sort.Interface接口就行。&lt;/p&gt;

&lt;h1 id=&#34;接口&#34;&gt;接口&lt;/h1&gt;

&lt;p&gt;sort.Interface定义的三个方法：获取数据集合长度的Len()方法、比较两个元素大小的Less()方法和交换两个元素位置的Swap()方法，就可以顺利对数据集合进行排序。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Interface interface {

    Len() int    // Len 为集合内元素的总数

    Less(i, j int) bool　//如果index为i的元素小于index为j的元素，则返回true，否则返回false

    Swap(i, j int)  // Swap 交换索引为 i 和 j 的元素
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;任何实现了 sort.Interface 的类型（一般为集合），均可使用该包中的方法进行排序。&lt;/p&gt;

&lt;h1 id=&#34;实例&#34;&gt;实例&lt;/h1&gt;

&lt;p&gt;golang自身实现的interface有三种，Float64Slice，IntSlice，StringSlice，具体如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Float64Slice

type Float64Slice []float64
Float64Slice 针对 []float6 实现接口的方法，以升序排列。
func (p Float64Slice) Len() int　　　　//求长度
func (p Float64Slice) Less(i, j int) bool　//比大小
func (p Float64Slice) Search(x float64) int　//查找
func (p Float64Slice) Sort()　　　　　　//排序
func (p Float64Slice) Swap(i, j int)　　　//交换位置



type IntSlice

type IntSlice []int
IntSlice 针对 []int 实现接口的方法，以升序排列。

func (p IntSlice) Len() int
func (p IntSlice) Less(i, j int) bool
func (p IntSlice) Search(x int) int
func (p IntSlice) Sort()
func (p IntSlice) Swap(i, j int)


type StringSlice

type StringSlice []string
StringSlice 针对 []string 实现接口的方法，以升序排列。

func (p StringSlice) Len() int
func (p StringSlice) Less(i, j int) bool
func (p StringSlice) Search(x string) int
func (p StringSlice) Sort()
func (p StringSlice) Swap(i, j int)


func Reverse(data Interface) Interface
func Reverse(data Interface) Interface
Reverse实现对data的逆序排列
package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sort&amp;quot;
)

func main() {
    a := []int{1, 2, 5, 3, 4}
    fmt.Println(a)        // [1 2 5 3 4]
    sort.Sort(sort.Reverse(sort.IntSlice(a)))
    fmt.Println(a)        // [5 4 3 2 1]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内部已经实现的float，int，string等排序，这些方法要求集合内列出元素的索引为整数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Float64s(a []float64)     //Float64s将类型为float64的slice a以升序方式进行排序
func Float64sAreSorted(a []float64) bool　　//判定是否已经进行排序func Ints(a []int)

func Ints(a []int)                       //Ints 以升序排列 int 切片。
func IntsAreSorted(a []int) bool　　　  //IntsAreSorted 判断 int 切片是否已经按升序排列。
func IsSorted(data Interface) bool    IsSorted 判断数据是否已经排序。包括各种可sort的数据类型的判断．

func Strings(a []string)//Strings 以升序排列 string 切片。
func StringsAreSorted(a []string) bool//StringsAreSorted 判断 string 切片是否已经按升序排列。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sort&amp;quot;
)

//定义interface{},并实现sort.Interface接口的三个方法
type IntSlice []int

func (c IntSlice) Len() int {
    return len(c)
}
func (c IntSlice) Swap(i, j int) {
    c[i], c[j] = c[j], c[i]
}
func (c IntSlice) Less(i, j int) bool {
    return c[i] &amp;lt; c[j]
}

func main() {
    a := IntSlice{1, 3, 5, 7, 2}
    b := []float64{1.1, 2.3, 5.3, 3.4}
    c := []int{1, 3, 5, 4, 2}
    fmt.Println(sort.IsSorted(a)) //false
    if !sort.IsSorted(a) {
        sort.Sort(a) 
    }

    if !sort.Float64sAreSorted(b) {
        sort.Float64s(b)
    }
    if !sort.IntsAreSorted(c) {
        sort.Ints(c)
    }
    fmt.Println(a)//[1 2 3 5 7]
    fmt.Println(b)//[1.1 2.3 3.4 5.3]
    fmt.Println(c)// [1 2 3 4 5]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;search&#34;&gt;search&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;func Search(n int, f func(int) bool) int   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;search使用二分法进行查找，Search()方法回使用“二分查找”算法来搜索某指定切片[0:n]，并返回能够使f(i)=true的最小的i（0&amp;lt;=i&amp;lt;n）值，并且会假定，如果f(i)=true，则f(i+1)=true，即对于切片[0:n]，i之前的切片元素会使f()函数返回false，i及i之后的元素会使f()函数返回true。但是，当在切片中无法找到时f(i)=true的i时（此时切片元素都不能使f()函数返回true），Search()方法会返回n（而不是返回-1）。&lt;/p&gt;

&lt;p&gt;Search 常用于在一个已排序的，可索引的数据结构中寻找索引为 i 的值 x，例如数组或切片。这种情况下，实参 f，一般是一个闭包，会捕获所要搜索的值，以及索引并排序该数据结构的方式。&lt;/p&gt;

&lt;p&gt;为了查找某个值，而不是某一范围的值时，如果slice以升序排序，则　f func中应该使用＞＝,如果slice以降序排序，则应该使用&amp;lt;=. 例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sort&amp;quot;
)

func main() {
    a := []int{1, 2, 3, 4, 5}
    b := sort.Search(len(a), func(i int) bool { return a[i] &amp;gt;= 30 })
    fmt.Println(b)　　　　　　　//5，查找不到，返回a slice的长度５，而不是-1
    c := sort.Search(len(a), func(i int) bool { return a[i] &amp;lt;= 3 })
    fmt.Println(c)                             //0，利用二分法进行查找，返回符合条件的最左边数值的index，即为０
    d := sort.Search(len(a), func(i int) bool { return a[i] == 3 })
    fmt.Println(d)                          //2　　　
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;官网上面有趣的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func GuessingGame() {
    var s string
    fmt.Printf(&amp;quot;Pick an integer from 0 to 100.\n&amp;quot;)
    answer := sort.Search(100, func(i int) bool {
        fmt.Printf(&amp;quot;Is your number &amp;lt;= %d? &amp;quot;, i)
        fmt.Scanf(&amp;quot;%s&amp;quot;, &amp;amp;s)
        return s != &amp;quot;&amp;quot; &amp;amp;&amp;amp; s[0] == &#39;y&#39;
    })
    fmt.Printf(&amp;quot;Your number is %d.\n&amp;quot;, answer)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还实现了指定类型的search&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func SearchFloat64s(a []float64, x float64) int　　//SearchFloat64s 在float64s切片中搜索x并返回索引如Search函数所述. 返回可以插入x值的索引位置，如果x不存在，返回数组a的长度切片必须以升序排列
func SearchInts(a []int, x int) int  //SearchInts 在ints切片中搜索x并返回索引如Search函数所述. 返回可以插入x值的索引位置，如果x不存在，返回数组a的长度切片必须以升序排列
func SearchStrings(a []string, x string) int//SearchFloat64s 在strings切片中搜索x并返回索引如Search函数所述. 返回可以插入x值的索引位置，如果x不存在，返回数组a的长度切片必须以升序排列
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中需要注意的是，以上三种search查找方法，其对应的slice必须按照升序进行排序，否则会出现奇怪的结果．&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sort&amp;quot;
)

func main() {
    a := []string{&amp;quot;a&amp;quot;, &amp;quot;c&amp;quot;}
    i := sort.SearchStrings(a, &amp;quot;b&amp;quot;)
    fmt.Println(i) //1
    b := []string{&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;}
    i = sort.SearchStrings(b, &amp;quot;b&amp;quot;)
    fmt.Println(i) //1
    c := []string{&amp;quot;d&amp;quot;, &amp;quot;c&amp;quot;}
    i = sort.SearchStrings(c, &amp;quot;b&amp;quot;)
    fmt.Println(i) //0
    d := []string{&amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;b&amp;quot;}
    i = sort.SearchStrings(d, &amp;quot;b&amp;quot;)
    fmt.Println(i) //0，由于d不是以升序方式排列，所以出现奇怪的结果，这可以根据SearchStrings的定义进行解释．见下方．
}


func SearchStrings(a []string, x string) int {
return Search(len(a), func(i int) bool { return a[i] &amp;gt;= x })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由此可见，为了精确查找，必须对[]string　以升序方式进行排序。&lt;/p&gt;

&lt;h1 id=&#34;其他函数&#34;&gt;其他函数&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;func Sort(data Interface)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sort 对 data 进行排序。它调用一次 data.Len 来决定排序的长度 n，调用 data.Less 和 data.Swap 的开销为O(n*log(n))。此排序为不稳定排序。他根据不同形式决定使用不同的排序方式（插入排序，堆排序，快排）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Stable(data Interface)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stable对data进行排序，不过排序过程中，如果data中存在相等的元素，则他们原来的顺序不会改变，即如果有两个相等元素num,他们的初始index分别为i和j，并且i&amp;lt;j，则利用Stable对data进行排序后，i依然小于ｊ．直接利用sort进行排序则不能够保证这一点。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Math</title>
          <link>https://kingjcy.github.io/post/golang/go-math/</link>
          <pubDate>Fri, 01 Feb 2019 11:37:51 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-math/</guid>
          <description>&lt;p&gt;math包实现的就是数学函数计算。&lt;/p&gt;

&lt;h1 id=&#34;math&#34;&gt;math&lt;/h1&gt;

&lt;p&gt;提供了三角函数,幂次函数,特殊函数,类型转化函数等还有其他函数。&lt;/p&gt;

&lt;h2 id=&#34;三角函数&#34;&gt;三角函数&lt;/h2&gt;

&lt;p&gt;正弦函数，反正弦函数，双曲正弦，反双曲正弦&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;func Sin(x float64) float64&lt;/li&gt;
&lt;li&gt;func Asin(x float64) float64&lt;/li&gt;
&lt;li&gt;func Sinh(x float64) float64&lt;/li&gt;
&lt;li&gt;func Asinh(x float64) float64&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一次性返回 sin,cos&lt;/p&gt;

&lt;p&gt;func Sincos(x float64) (sin, cos float64)&lt;/p&gt;

&lt;p&gt;余弦函数，反余弦函数，双曲余弦，反双曲余弦&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;func Cos(x float64) float64&lt;/li&gt;
&lt;li&gt;func Acos(x float64) float64&lt;/li&gt;
&lt;li&gt;func Cosh(x float64) float64&lt;/li&gt;
&lt;li&gt;func Acosh(x float64) float64&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;正切函数，反正切函数，双曲正切，反双曲正切&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;func Tan(x float64) float64&lt;/li&gt;
&lt;li&gt;func Atan(x float64) float64 和 func Atan2(y, x float64) float64&lt;/li&gt;
&lt;li&gt;func Tanh(x float64) float64&lt;/li&gt;
&lt;li&gt;func Atanh(x float64) float64&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;幂次函数&#34;&gt;幂次函数&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;func Cbrt(x float64) float64 // 立方根函数&lt;/li&gt;
&lt;li&gt;func Pow(x, y float64) float64  // x 的幂函数&lt;/li&gt;
&lt;li&gt;func Pow10(e int) float64 // 10 根的幂函数&lt;/li&gt;
&lt;li&gt;func Sqrt(x float64) float64 // 平方根&lt;/li&gt;
&lt;li&gt;func Log(x float64) float64 // 对数函数&lt;/li&gt;
&lt;li&gt;func Log10(x float64) float64 // 10 为底的对数函数&lt;/li&gt;
&lt;li&gt;func Log2(x float64) float64  // 2 为底的对数函数&lt;/li&gt;
&lt;li&gt;func Log1p(x float64) float64 // log(1 + x)&lt;/li&gt;
&lt;li&gt;func Logb(x float64) float64 // 相当于 log2(x) 的绝对值&lt;/li&gt;
&lt;li&gt;func Ilogb(x float64) int // 相当于 log2(x) 的绝对值的整数部分&lt;/li&gt;
&lt;li&gt;func Exp(x float64) float64 // 指数函数&lt;/li&gt;
&lt;li&gt;func Exp2(x float64) float64 // 2 为底的指数函数&lt;/li&gt;
&lt;li&gt;func Expm1(x float64) float64 // Exp(x) - 1&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;特殊函数&#34;&gt;特殊函数&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;func Inf(sign int) float64  // 正无穷&lt;/li&gt;
&lt;li&gt;func IsInf(f float64, sign int) bool // 是否正无穷&lt;/li&gt;
&lt;li&gt;func NaN() float64 // 无穷值&lt;/li&gt;
&lt;li&gt;func IsNaN(f float64) (is bool) // 是否是无穷值&lt;/li&gt;
&lt;li&gt;func Hypot(p, q float64) float64 // 计算直角三角形的斜边长&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;类型转化函数&#34;&gt;类型转化函数&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;func Float32bits(f float32) uint32  // float32 和 unit32 的转换&lt;/li&gt;
&lt;li&gt;func Float32frombits(b uint32) float32 // uint32 和 float32 的转换&lt;/li&gt;
&lt;li&gt;func Float64bits(f float64) uint64 // float64 和 uint64 的转换&lt;/li&gt;
&lt;li&gt;func Float64frombits(b uint64) float64 // uint64 和 float64 的转换&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;其他函数&#34;&gt;其他函数&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;func Abs(x float64) float64 // 绝对值函数&lt;/li&gt;
&lt;li&gt;func Ceil(x float64) float64  // 向上取整&lt;/li&gt;
&lt;li&gt;func Floor(x float64) float64 // 向下取整&lt;/li&gt;
&lt;li&gt;func Mod(x, y float64) float64 // 取模&lt;/li&gt;
&lt;li&gt;func Modf(f float64) (int float64, frac float64) // 分解 f，以得到 f 的整数和小数部分&lt;/li&gt;
&lt;li&gt;func Frexp(f float64) (frac float64, exp int) // 分解 f，得到 f 的位数和指数&lt;/li&gt;
&lt;li&gt;func Max(x, y float64) float64  // 取大值&lt;/li&gt;
&lt;li&gt;func Min(x, y float64) float64  // 取小值&lt;/li&gt;
&lt;li&gt;func Dim(x, y float64) float64 // 复数的维数&lt;/li&gt;
&lt;li&gt;func J0(x float64) float64  // 0 阶贝塞尔函数&lt;/li&gt;
&lt;li&gt;func J1(x float64) float64  // 1 阶贝塞尔函数&lt;/li&gt;
&lt;li&gt;func Jn(n int, x float64) float64 // n 阶贝塞尔函数&lt;/li&gt;
&lt;li&gt;func Y0(x float64) float64  // 第二类贝塞尔函数 0 阶&lt;/li&gt;
&lt;li&gt;func Y1(x float64) float64  // 第二类贝塞尔函数 1 阶&lt;/li&gt;
&lt;li&gt;func Yn(n int, x float64) float64 // 第二类贝塞尔函数 n 阶&lt;/li&gt;
&lt;li&gt;func Erf(x float64) float64 // 误差函数&lt;/li&gt;
&lt;li&gt;func Erfc(x float64) float64 // 余补误差函数&lt;/li&gt;
&lt;li&gt;func Copysign(x, y float64) float64 // 以 y 的符号返回 x 值&lt;/li&gt;
&lt;li&gt;func Signbit(x float64) bool // 获取 x 的符号&lt;/li&gt;
&lt;li&gt;func Gamma(x float64) float64 // 伽玛函数&lt;/li&gt;
&lt;li&gt;func Lgamma(x float64) (lgamma float64, sign int) // 伽玛函数的自然对数&lt;/li&gt;
&lt;li&gt;func Ldexp(frac float64, exp int) float64 // value 乘以 2 的 exp 次幂&lt;/li&gt;
&lt;li&gt;func Nextafter(x, y float64) (r float64) // 返回参数 x 在参数 y 方向上可以表示的最接近的数值，若 x 等于 y，则返回 x&lt;/li&gt;
&lt;li&gt;func Nextafter32(x, y float32) (r float32) // 返回参数 x 在参数 y 方向上可以表示的最接近的数值，若 x 等于 y，则返回 x&lt;/li&gt;
&lt;li&gt;func Remainder(x, y float64) float64 // 取余运算&lt;/li&gt;
&lt;li&gt;func Trunc(x float64) float64 // 截取函数&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;math包中定义的常量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math&amp;quot;
)

func main() {

    fmt.Printf(&amp;quot;float64的最大值是:%.f\n&amp;quot;, math.MaxFloat64)
    fmt.Printf(&amp;quot;float64的最小值是:%.f\n&amp;quot;, math.SmallestNonzeroFloat64)
    fmt.Printf(&amp;quot;float32的最大值是:%.f\n&amp;quot;, math.MaxFloat32)
    fmt.Printf(&amp;quot;float32的最小值是:%.f\n&amp;quot;, math.SmallestNonzeroFloat32)
    fmt.Printf(&amp;quot;Int8的最大值是:%d\n&amp;quot;, math.MaxInt8)
    fmt.Printf(&amp;quot;Int8的最小值是:%d\n&amp;quot;, math.MinInt8)
    fmt.Printf(&amp;quot;Uint8的最大值是:%d\n&amp;quot;, math.MaxUint8)
    fmt.Printf(&amp;quot;Int16的最大值是:%d\n&amp;quot;, math.MaxInt16)
    fmt.Printf(&amp;quot;Int16的最小值是:%d\n&amp;quot;, math.MinInt16)
    fmt.Printf(&amp;quot;Uint16的最大值是:%d\n&amp;quot;, math.MaxUint16)
    fmt.Printf(&amp;quot;Int32的最大值是:%d\n&amp;quot;, math.MaxInt32)
    fmt.Printf(&amp;quot;Int32的最小值是:%d\n&amp;quot;, math.MinInt32)
    fmt.Printf(&amp;quot;Uint32的最大值是:%d\n&amp;quot;, math.MaxUint32)
    fmt.Printf(&amp;quot;Int64的最大值是:%d\n&amp;quot;, math.MaxInt64)
    fmt.Printf(&amp;quot;Int64的最小值是:%d\n&amp;quot;, math.MinInt64)
    //fmt.Println(&amp;quot;Uint64的最大值是:&amp;quot;, math.MaxUint64)
    fmt.Printf(&amp;quot;圆周率默认为:%.200f\n&amp;quot;, math.Pi)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math&amp;quot;
)

func main() {
    /*
        取绝对值,函数签名如下:
            func Abs(x float64) float64
    */
    fmt.Printf(&amp;quot;[-3.14]的绝对值为:[%.2f]\n&amp;quot;, math.Abs(-3.14))

    /*
        取x的y次方，函数签名如下:
            func Pow(x, y float64) float64
    */
    fmt.Printf(&amp;quot;[2]的16次方为:[%.f]\n&amp;quot;, math.Pow(2, 16))

    /*
        取余数，函数签名如下:
            func Pow10(n int) float64
    */
    fmt.Printf(&amp;quot;10的[3]次方为:[%.f]\n&amp;quot;, math.Pow10(3))

    /*
        取x的开平方，函数签名如下:
            func Sqrt(x float64) float64
    */
    fmt.Printf(&amp;quot;[64]的开平方为:[%.f]\n&amp;quot;, math.Sqrt(64))

    /*
        取x的开立方，函数签名如下:
            func Cbrt(x float64) float64
    */
    fmt.Printf(&amp;quot;[27]的开立方为:[%.f]\n&amp;quot;, math.Cbrt(27))

    /*
        向上取整，函数签名如下:
            func Ceil(x float64) float64
    */
    fmt.Printf(&amp;quot;[3.14]向上取整为:[%.f]\n&amp;quot;, math.Ceil(3.14))

    /*
        向下取整，函数签名如下:
            func Floor(x float64) float64
    */
    fmt.Printf(&amp;quot;[8.75]向下取整为:[%.f]\n&amp;quot;, math.Floor(8.75))

    /*
        取余数，函数签名如下:
            func Floor(x float64) float64
    */
    fmt.Printf(&amp;quot;[10/3]的余数为:[%.f]\n&amp;quot;, math.Mod(10, 3))

    /*
        分别取整数和小数部分,函数签名如下:
            func Modf(f float64) (int float64, frac float64)
    */
    Integer, Decimal := math.Modf(3.14159265358979)
    fmt.Printf(&amp;quot;[3.14159265358979]的整数部分为:[%.f],小数部分为:[%.14f]\n&amp;quot;, Integer, Decimal)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;rand&#34;&gt;rand&lt;/h1&gt;

&lt;p&gt;rand包实现了伪随机数生成器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import &amp;quot;math/rand&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;随机数从资源生成。包水平的函数都使用的默认的公共资源。该资源会在程序每次运行时都产生确定的序列。如果需要每次运行产生不同的序列，应使用Seed函数进行初始化。如果不调用sned函数，默认都是采用sned（1），默认资源可以安全的用于多go程并发。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/rand&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    rand.Seed(time.Now().UnixNano())
    for i := 0; i &amp;lt; 10; i++ {
        x := rand.Intn(100)
        fmt.Println(x)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上例子是打印10个100以内（0-99）的随机数字&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rand.Intn(n)。int代表整数，后面的n代表范围，其他类型类似&lt;/li&gt;
&lt;li&gt;rand.Seed 设置随机数种子&lt;/li&gt;
&lt;li&gt;time.Now().UnixNano() 种子，也就是随机因子，相同的种子产生的随机数是一样的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;自定义生成rand结构体-设置随机数种子&#34;&gt;自定义生成Rand结构体，设置随机数种子&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func NewSource(seed int64) Source
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用给定的种子创建一个伪随机资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func New(src Source) *Rand
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个使用src生产的随机数来生成其他各种分布的随机数值的*Rand。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/rand&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    source := rand.NewSource(time.Now().UnixNano()) // 使用当前的纳秒生成一个随机源，也就是随机种子
    ran := rand.New(source) // 生成一个rand
    fmt.Println(rand.Int())
    fmt.Println(rand.Int31())
    fmt.Println(rand.Intn(5))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;其它生成随机数的方法&#34;&gt;其它生成随机数的方法&lt;/h2&gt;

&lt;p&gt;Rand生成随机数当然不只这三个方法，还有其它生成随机数的方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Int63() int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个int64类型的非负的63位伪随机数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Uint32() uint32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个uint32类型的非负的32位伪随机数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Int31n(n int32) int32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个取值范围在[0,n)的伪随机int32值，如果n&amp;lt;=0会panic。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Int63n(n int64) int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个取值范围在[0, n)的伪随机int64值，如果n&amp;lt;=0会panic。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Float32() float32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个取值范围在[0.0, 1.0)的伪随机float32值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Float64() float64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个取值范围在[0.0, 1.0)的伪随机float64值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Rand) Perm(n int) []int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个有n个元素的，[0,n)范围内整数的伪随机排列的切片。&lt;/p&gt;

&lt;h2 id=&#34;crypto-rand&#34;&gt;crypto/rand&lt;/h2&gt;

&lt;p&gt;这个rand包实现了用于加解密的更安全的随机数生成器,主要应用场景：生成随机加密串。&lt;/p&gt;

&lt;p&gt;这边简单使用一个实例说明，具体可以看&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-crypto/&#34;&gt;crypto&lt;/a&gt;包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;crypto/rand&amp;quot;
    &amp;quot;encoding/base64&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/big&amp;quot;
)

func main() {
    //1、Int
    n, err := rand.Int(rand.Reader, big.NewInt(128))
    if err == nil {
        fmt.Println(&amp;quot;rand.Int：&amp;quot;, n, n.BitLen())
    }
    //2、Prime
    p, err := rand.Prime(rand.Reader, 5)
    if err == nil {
        fmt.Println(&amp;quot;rand.Prime：&amp;quot;, p)
    }
    //3、Read
    b := make([]byte, 32)
    m, err := rand.Read(b)
    if err == nil {
        fmt.Println(&amp;quot;rand.Read：&amp;quot;, b[:m])
        fmt.Println(&amp;quot;rand.Read：&amp;quot;, base64.URLEncoding.EncodeToString(b))
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rand.Int： 92 7
rand.Prime： 29
rand.Read： [207 47 241 208 190 84 109 134 86 106 87 223 111 113 203 155 44 118 71 20 186 62 66 130 244 98 97 184 8 179 6 230]
rand.Read： zy_x0L5UbYZWalffb3HLmyx2RxS6PkKC9GJhuAizBuY=
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;big&#34;&gt;big&lt;/h1&gt;

&lt;p&gt;大数处理，可以用golang的math/big包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/big&amp;quot;
)

func main() {
    //设置一个大于int64的数
    a := new(big.Int)
    a, ok := a.SetString(&amp;quot;9122322238215458478512545454878168716584545412154785452142499999&amp;quot;, 10)
    if !ok {
        panic(&amp;quot;error&amp;quot;)
    }
    //String方法可以转换成字符串输出
    fmt.Println(a.String())

    //大数相加
    b:=big.NewInt(2)
    b=b.Add(a,b) //  Mod 取模、Add 加、Sub 减、Mul 乘、Div 除
    fmt.Println(b.String())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;cmplx&#34;&gt;cmplx&lt;/h1&gt;

&lt;p&gt;cmplx 包为复数提供基本的常量和数学函数。&lt;/p&gt;

&lt;h1 id=&#34;bits&#34;&gt;bits&lt;/h1&gt;

&lt;p&gt;bits主要打包字节为预先声明的无符号整数类型实现位计数和操作函数。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 文件存储系统Fastdfs</title>
          <link>https://kingjcy.github.io/post/distributed/store/fs/fastfs/</link>
          <pubDate>Wed, 16 Jan 2019 20:32:58 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/fs/fastfs/</guid>
          <description>&lt;p&gt;fastdfs是一个开源的轻量级分布式文件系统，是纯C语言开发的。它对文件进行管理，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等，FastDFS 针对大量小文件存储有优势。&lt;/p&gt;

&lt;h1 id=&#34;安装-v5-08&#34;&gt;安装（v5.08）&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;172.32.148.127 client&lt;/p&gt;

&lt;p&gt;172.32.148.128 storage&lt;/p&gt;

&lt;p&gt;172.32.148.129 tracker storage&lt;/p&gt;

&lt;p&gt;172.32.148.130 storage&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;依赖libfastcommon安装&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;v5版本已经不需要单独安装libevent，但是需要安装项目中的libfastcommon，所有机器都要安装，下载地址&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/happyfish100/libfastcommon.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后用自带的脚本make.sh编译安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./make.sh
./make.sh install
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;fastdfs安装&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;依赖安装好就开始安装fastdfs，下载压缩包fastdfs-5.08.tar.gz，所有机器进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./make.sh
./make.sh install
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;配置文件与进程&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;fastdfs安装好后在/etc/fdfs/下面会有对应的配置文件，这个项目的配置文件注释还是比较全面的，对它进行修改&lt;/p&gt;

&lt;p&gt;1、tracker.conf&lt;/p&gt;

&lt;p&gt;监控进程使用的配置文件，主要是配置端口，日志数据存储路径，端口一般使用配置文件默认的22122，在172.32.148.129上操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# the tracker server port
port=22122

# the base path to store data and log files
base_path=/home/jcy/fastdfs/fastdfs_tracker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动监控进程，到对应的路径下看日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fdfs_trackerd /etc/fdfs/tracker.conf start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以ps看一下进程启动状况，也可以到日志看启动状况，也可以通过netstat来看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netstat -tupln|grep tracker
#可以看到如下：
tcp  0   0   0.0.0.0:22122   0.0.0.0:*   LISTEN   18559/fdfs_trackerd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、storage.conf&lt;/p&gt;

&lt;p&gt;存储进程使用的配置文件，主要是指定tracker服务的地址和端口，因为存储进程要定时给监控进程发送数据信息，同group内的storage进程会相互connect，来同步文件。这些进程在172.32.148.128，172.32.148.129，172.32.148.130上进行操作。同时它也有自己的group_name和开发的端口，还有对应的日志数据路径。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# the name of the group this storage server belongs to
#
# comment or remove this item for fetching from tracker server,
# in this case, use_storage_id must set to true in tracker.conf,
# and storage_ids.conf must be configed correctly.
group_name=group1

# the storage server port
port=23000

# the base path to store data and log files
base_path=/home/jcy/fastdfs/fastdfs_storage

# store_path#, based 0, if store_path0 not exists, it&#39;s value is base_path
# the paths must be exist
store_path0=/home/jcy/fastdfs/fastdfs_storage

# tracker_server can ocur more than once, and tracker_server format is
#  &amp;quot;host:port&amp;quot;, host can be hostname or ip address
tracker_server=172.32.148.129:22122
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动存储进程，查看对应的情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fdfs_storaged /etc/fdfs/storage.conf start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、client.conf&lt;/p&gt;

&lt;p&gt;客户端进行文件上传的测试,首先是对配置文件的tracker服务器地址进行配置，然后就是一些基本配置看注释就可以&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;测试文件上传&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在客户端机器172.32.148.127上进行操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[jcy@pdapp17 fastdfs]$ fdfs_upload_file /etc/fdfs/client.conf a.txt
group1/M00/00/00/rCCUgVh8lF-ATRZpAAAADw_r4o4559.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上传成功，到对应的storage进程指定的path下的data目录就可以找到对应的文件，文件会同时同步的同一个group下的所有storage上，完成备份。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;FastDFS服务端有三个角色：跟踪服务器（tracker server）、存储服务器（storage server）和客户端（client）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;tracker server：跟踪服务器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要做调度工作，起负载均衡的作用。在内存中记录集群中所有存储组和存储服务器的状态信息，是客户端和数据服务器交互的枢纽。相比GFS中的master更为精简，不记录文件索引信息，占用的内存量很少。&lt;/p&gt;

&lt;p&gt;Tracker是FastDFS的协调者，负责管理所有的storage server和group，每个storage在启动后会连接Tracker，告知自己所属的group等信息，并保持周期性的心跳，tracker根据storage的心跳信息，建立group==&amp;gt;[storage server list]的映射表。&lt;/p&gt;

&lt;p&gt;Tracker需要管理的元信息很少，会全部存储在内存中；另外tracker上的元信息都是由storage汇报的信息生成的，本身不需要持久化任何数据，这样使得tracker非常容易扩展，直接增加tracker机器即可扩展为tracker cluster来服务，cluster里每个tracker之间是完全对等的，所有的tracker都接受stroage的心跳信息，生成元数据信息来提供读写服务。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;storage server：存储服务器（又称：存储节点或数据服务器）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文件和文件属性（meta data）都保存到存储服务器上。Storage server直接利用OS的文件系统调用管理文件。&lt;/p&gt;

&lt;p&gt;Storage server（后简称storage）以组（卷，group或volume）为单位组织，一个group内包含多台storage机器，数据互为备份，存储空间以group内容量最小的storage为准，所以建议group内的多个storage尽量配置相同，以免造成存储空间的浪费。&lt;/p&gt;

&lt;p&gt;以group为单位组织存储能方便的进行应用隔离、负载均衡、副本数定制（group内storage server数量即为该group的副本数），比如将不同应用数据存到不同的group就能隔离应用数据，同时还可根据应用的访问特性来将应用分配到不同的group来做负载均衡；缺点是group的容量受单机存储容量的限制，同时当group内有机器坏掉时，数据恢复只能依赖group内地其他机器，使得恢复时间会很长。&lt;/p&gt;

&lt;p&gt;group内每个storage的存储依赖于本地文件系统，storage可配置多个数据存储目录，比如有10块磁盘，分别挂载在/data/disk1-/data/disk10，则可将这10个目录都配置为storage的数据存储目录。&lt;/p&gt;

&lt;p&gt;storage接受到写文件请求时，会根据配置好的规则（后面会介绍），选择其中一个存储目录来存储文件。为了避免单个目录下的文件数太多，在storage第一次启动时，会在每个数据存储目录里创建2级子目录，每级256个，总共65536个文件，新写的文件会以hash的方式被路由到其中某个子目录下，然后将文件数据直接作为一个本地文件存储到该目录中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;client：客户端&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;作为业务请求的发起方，通过专有接口，使用TCP/IP协议与跟踪器服务器或存储节点进行数据交互。FastDFS向使用者提供基本文件访问接口，比如upload、download、append、delete等，以客户端库的方式提供给用户使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;group&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;组， 也可称为卷。 同组内服务器上的文件是完全相同的 ，同一组内的storage server之间是对等的， 文件上传、 删除等操作可以在任意一台storage server上进行 。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;meta data&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;文件相关属性，键值对（ Key Value Pair） 方式，如：width=1024,heigth=768 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/fastdfs&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;Tracker相当于FastDFS的大脑，不论是上传还是下载都是通过tracker来分配资源；客户端一般可以使用ngnix等静态服务器来调用或者做一部分的缓存；存储服务器内部分为卷（或者叫做组），卷于卷之间是平行的关系，可以根据资源的使用情况随时增加，卷内服务器文件相互同步备份，以达到容灾的目的。&lt;/p&gt;

&lt;h2 id=&#34;上传机制&#34;&gt;上传机制&lt;/h2&gt;

&lt;p&gt;首先客户端请求Tracker服务获取到存储服务器的ip地址和端口，然后客户端根据返回的IP地址和端口号请求上传文件，存储服务器接收到请求后生产文件，并且将文件内容写入磁盘并返回给客户端file_id、路径信息、文件名等信息，客户端保存相关信息上传完毕。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/upload&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;内部机制如下：&lt;/p&gt;

&lt;p&gt;1、选择tracker server&lt;/p&gt;

&lt;p&gt;当集群中不止一个tracker server时，由于tracker之间是完全对等的关系，客户端在upload文件时可以任意选择一个trakcer。 选择存储的group 当tracker接收到upload file的请求时，会为该文件分配一个可以存储该文件的group，支持如下选择group的规则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、Round robin，所有的group间轮询&lt;/li&gt;
&lt;li&gt;2、Specified group，指定某一个确定的group&lt;/li&gt;
&lt;li&gt;3、Load balance，剩余存储空间多多group优先&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、选择storage server&lt;/p&gt;

&lt;p&gt;当选定group后，tracker会在group内选择一个storage server给客户端，支持如下选择storage的规则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、Round robin，在group内的所有storage间轮询&lt;/li&gt;
&lt;li&gt;2、First server ordered by ip，按ip排序&lt;/li&gt;
&lt;li&gt;3、First server ordered by priority，按优先级排序（优先级在storage上配置）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、选择storage path&lt;/p&gt;

&lt;p&gt;当分配好storage server后，客户端将向storage发送写文件请求，storage将会为文件分配一个数据存储目录，支持如下规则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、Round robin，多个存储目录间轮询&lt;/li&gt;
&lt;li&gt;2、剩余存储空间最多的优先&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、生成Fileid&lt;/p&gt;

&lt;p&gt;选定存储目录之后，storage会为文件生一个Fileid，由storage server ip、文件创建时间、文件大小、文件crc32和一个随机数拼接而成，然后将这个二进制串进行base64编码，转换为可打印的字符串。 选择两级目录 当选定存储目录之后，storage会为文件分配一个fileid，每个存储目录下有两级256*256的子目录，storage会按文件fileid进行两次hash（猜测），路由到其中一个子目录，然后将文件以fileid为文件名存储到该子目录下。&lt;/p&gt;

&lt;p&gt;5、生成文件名&lt;/p&gt;

&lt;p&gt;当文件存储到某个子目录后，即认为该文件存储成功，接下来会为该文件生成一个文件名，文件名由group、存储目录、两级子目录、fileid、文件后缀名（由客户端指定，主要用于区分文件类型）拼接而成。&lt;/p&gt;

&lt;h2 id=&#34;下载机制&#34;&gt;下载机制&lt;/h2&gt;

&lt;p&gt;客户端带上文件名信息请求Tracker服务获取到存储服务器的ip地址和端口，然后客户端根据返回的IP地址和端口号请求下载文件，存储服务器接收到请求后返回文件给客户端。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/store/download&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;跟upload file一样，在download file时客户端可以选择任意tracker server。tracker发送download请求给某个tracker，必须带上文件名信息，tracke从文件名中解析出文件的group、大小、创建时间等信息，然后为该请求选择一个storage用来服务读请求。由于group内的文件同步时在后台异步进行的，所以有可能出现在读到时候，文件还没有同步到某些storage server上，为了尽量避免访问到这样的storage，tracker按照如下规则选择group内可读的storage。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、该文件上传到的源头storage - 源头storage只要存活着，肯定包含这个文件，源头的地址被编码在文件名中。&lt;/li&gt;
&lt;li&gt;2、文件创建时间戳==storage被同步到的时间戳 且(当前时间-文件创建时间戳) &amp;gt; 文件同步最大时间（如5分钟) - 文件创建后，认为经过最大同步时间后，肯定已经同步到其他storage了。&lt;/li&gt;
&lt;li&gt;3、文件创建时间戳 &amp;lt; storage被同步到的时间戳。 - 同步时间戳之前的文件确定已经同步了&lt;/li&gt;
&lt;li&gt;4、(当前时间-文件创建时间戳) &amp;gt; 同步延迟阀值（如一天）。 - 经过同步延迟阈值时间，认为文件肯定已经同步了。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Bufio</title>
          <link>https://kingjcy.github.io/post/golang/go-bufio/</link>
          <pubDate>Tue, 25 Dec 2018 14:27:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-bufio/</guid>
          <description>&lt;p&gt;bufio 包实现了缓存IO。它包装了 io.Reader 和 io.Writer 对象，创建了另外的Reader和Writer对象，它们也实现了 io.Reader 和 io.Writer 接口，不过它们是有缓存的。该包同时为文本I/O提供了一些便利操作。&lt;/p&gt;

&lt;h1 id=&#34;类型和方法&#34;&gt;类型和方法&lt;/h1&gt;

&lt;h2 id=&#34;reader-类型和方法&#34;&gt;Reader 类型和方法&lt;/h2&gt;

&lt;h3 id=&#34;类型&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;bufio.Reader 结构包装了一个 io.Reader 对象，提供缓存功能，同时实现了 io.Reader 接口。&lt;/p&gt;

&lt;p&gt;Reader 结构没有任何导出的字段，结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader struct {
    buf          []byte        // 缓存
    rd           io.Reader    // 底层的io.Reader
    // r:从buf中读走的字节（偏移）；w:buf中填充内容的偏移；
    // w - r 是buf中可被读的长度（缓存数据的大小），也是Buffered()方法的返回值
    r, w         int
    err          error        // 读过程中遇到的错误
    lastByte     int        // 最后一次读到的字节（ReadByte/UnreadByte)
    lastRuneSize int        // 最后一次读到的Rune的大小 (ReadRune/UnreadRune)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;bufio 包提供了两个实例化 bufio.Reader 对象的函数：NewReader 和 NewReaderSize。其中，NewReader 函数是调用 NewReaderSize 函数实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(rd io.Reader) *Reader {
    // 默认缓存大小：defaultBufSize=4096
    return NewReaderSize(rd, defaultBufSize)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下NewReaderSize的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReaderSize(rd io.Reader, size int) *Reader {
    // 已经是bufio.Reader类型，且缓存大小不小于 size，则直接返回
    b, ok := rd.(*Reader)
    if ok &amp;amp;&amp;amp; len(b.buf) &amp;gt;= size {
        return b
    }
    // 缓存大小不会小于 minReadBufferSize （16字节）
    if size &amp;lt; minReadBufferSize {
        size = minReadBufferSize
    }
    // 构造一个bufio.Reader实例
    return &amp;amp;Reader{
        buf:          make([]byte, size),
        rd:           rd,
        lastByte:     -1,
        lastRuneSize: -1,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见需要一个id.reader的实例来进行初始化，一般我们都是使用string或者[]byte的reader类型来创建。&lt;/p&gt;

&lt;h3 id=&#34;方法&#34;&gt;方法&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadSlice、ReadBytes、ReadString 和 ReadLine 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之所以将这几个方法放在一起，是因为他们有着类似的行为。事实上，后三个方法最终都是调用ReadSlice来实现的。所以，我们先来看看ReadSlice方法(感觉这一段直接看源码较好)。&lt;/p&gt;

&lt;p&gt;1.ReadSlice方法签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadSlice(delim byte) (line []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ReadSlice 从输入中读取，直到遇到第一个界定符（delim）为止，返回一个指向缓存中字节的 slice，在下次调用读操作（read）时，这些字节会无效。举例说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := bufio.NewReader(strings.NewReader(&amp;quot;http://studygolang.com. \nIt is the home of gophers&amp;quot;))
line, _ := reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
// 这里可以换上任意的 bufio 的 Read/Write 操作
n, _ := reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
fmt.Println(string(n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;the line:http://studygolang.com.

the line:It is the home of gophers
It is the home of gophers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从结果可以看出，第一次ReadSlice的结果（line），在第二次调用读操作后，内容发生了变化。也就是说，ReadSlice 返回的 []byte 是指向 Reader 中的 buffer ，而不是 copy 一份返回。正因为ReadSlice 返回的数据会被下次的 I/O 操作重写，因此许多的客户端会选择使用 ReadBytes 或者 ReadString 来代替。读者可以将上面代码中的 ReadSlice 改为 ReadBytes 或 ReadString ，看看结果有什么不同。&lt;/p&gt;

&lt;p&gt;注意，这里的界定符可以是任意的字符，可以将上面代码中的&amp;rsquo;\n&amp;rsquo;改为&amp;rsquo;m&amp;rsquo;试试。同时，返回的结果是包含界定符本身的，上例中，输出结果有一空行就是&amp;rsquo;\n&amp;rsquo;本身(line携带一个&amp;rsquo;\n&amp;rsquo;,printf又追加了一个&amp;rsquo;\n&amp;rsquo;)。&lt;/p&gt;

&lt;p&gt;如果 ReadSlice 在找到界定符之前遇到了 error ，它就会返回缓存中所有的数据和错误本身（经常是 io.EOF）。如果在找到界定符之前缓存已经满了，ReadSlice 会返回 bufio.ErrBufferFull 错误。当且仅当返回的结果（line）没有以界定符结束的时候，ReadSlice 返回err != nil，也就是说，如果ReadSlice 返回的结果 line 不是以界定符 delim 结尾，那么返回的 er r也一定不等于 nil（可能是bufio.ErrBufferFull或io.EOF）。 例子代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := bufio.NewReaderSize(strings.NewReader(&amp;quot;http://studygolang.com&amp;quot;),16)
line, err := reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;line:%s\terror:%s\n&amp;quot;, line, err)
line, err = reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;line:%s\terror:%s\n&amp;quot;, line, err)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line:http://studygola    error:bufio: buffer full
line:ng.com    error:EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.ReadBytes方法签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadBytes(delim byte) (line []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方法的参数和返回值类型与 ReadSlice 都一样。 ReadBytes 从输入中读取直到遇到界定符（delim）为止，返回的 slice 包含了从当前到界定符的内容 （包括界定符）。如果 ReadBytes 在遇到界定符之前就捕获到一个错误，它会返回遇到错误之前已经读取的数据，和这个捕获到的错误（经常是 io.EOF）。跟 ReadSlice 一样，如果 ReadBytes 返回的结果 line 不是以界定符 delim 结尾，那么返回的 err 也一定不等于 nil（可能是bufio.ErrBufferFull 或 io.EOF）。&lt;/p&gt;

&lt;p&gt;从这个说明可以看出，ReadBytes和ReadSlice功能和用法都很像，那他们有什么不同呢？&lt;/p&gt;

&lt;p&gt;在讲解ReadSlice时说到，它返回的 []byte 是指向 Reader 中的 buffer，而不是 copy 一份返回，也正因为如此，通常我们会使用 ReadBytes 或 ReadString。很显然，ReadBytes 返回的 []byte 不会是指向 Reader 中的 buffer，通过查看源码可以证实这一点。&lt;/p&gt;

&lt;p&gt;还是上面的例子，我们将 ReadSlice 改为 ReadBytes：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := bufio.NewReader(strings.NewReader(&amp;quot;http://studygolang.com. \nIt is the home of gophers&amp;quot;))
line, _ := reader.ReadBytes(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
// 这里可以换上任意的 bufio 的 Read/Write 操作
n, _ := reader.ReadBytes(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
fmt.Println(string(n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;the line:http://studygolang.com.

the line:http://studygolang.com.

It is the home of gophers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.ReadString方法&lt;/p&gt;

&lt;p&gt;看一下该方法的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadString(delim byte) (line string, err error) {
    bytes, err := b.ReadBytes(delim)
    return string(bytes), err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它调用了 ReadBytes 方法，并将结果的 []byte 转为 string 类型。&lt;/p&gt;

&lt;p&gt;4.ReadLine方法签名如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadLine() (line []byte, isPrefix bool, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ReadLine 是一个底层的原始行读取命令。许多调用者或许会使用 ReadBytes(&amp;rsquo;\n&amp;rsquo;) 或者 ReadString(&amp;rsquo;\n&amp;rsquo;) 来代替这个方法。&lt;/p&gt;

&lt;p&gt;ReadLine 尝试返回单独的行，不包括行尾的换行符。如果一行大于缓存，isPrefix 会被设置为 true，同时返回该行的开始部分（等于缓存大小的部分）。该行剩余的部分就会在下次调用的时候返回。当下次调用返回该行剩余部分时，isPrefix 将会是 false 。跟 ReadSlice 一样，返回的 line 只是 buffer 的引用，在下次执行IO操作时，line 会无效。可以将 ReadSlice 中的例子该为 ReadLine 试试。&lt;/p&gt;

&lt;p&gt;注意，返回值中，要么 line 不是 nil，要么 err 非 nil，两者不会同时非 nil。ReadLine 返回的文本不会包含行结尾（&amp;rdquo;\r\n&amp;rdquo;或者&amp;rdquo;\n&amp;rdquo;）。如果输入中没有行尾标识符，不会返回任何指示或者错误。&lt;/p&gt;

&lt;p&gt;个人建议可以这么实现读取一行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line, err := reader.ReadBytes(&#39;\n&#39;)
line = bytes.TrimRight(line, &amp;quot;\r\n&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样既读取了一行，也去掉了行尾结束符（当然，如果你希望留下行尾结束符，只用ReadBytes即可）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Peek 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从方法的名称可以猜到，该方法只是“窥探”一下 Reader 中没有读取的 n 个字节。好比栈数据结构中的取栈顶元素，但不出栈。&lt;/p&gt;

&lt;p&gt;方法的签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) Peek(n int) ([]byte, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同上面介绍的 ReadSlice一样，返回的 []byte 只是 buffer 中的引用，在下次IO操作后会无效，可见该方法（以及ReadSlice这样的，返回buffer引用的方法）对多 goroutine 是不安全的，也就是在多并发环境下，不能依赖其结果。&lt;/p&gt;

&lt;p&gt;我们通过例子来证明一下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;bufio&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;strings&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    reader := bufio.NewReaderSize(strings.NewReader(&amp;quot;http://studygolang.com.\t It is the home of gophers&amp;quot;), 14)
    go Peek(reader)
    go reader.ReadBytes(&#39;\t&#39;)
    time.Sleep(1e8)
}

func Peek(reader *bufio.Reader) {
    line, _ := reader.Peek(14)
    fmt.Printf(&amp;quot;%s\n&amp;quot;, line)
    // time.Sleep(1)
    fmt.Printf(&amp;quot;%s\n&amp;quot;, line)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygo
http://studygo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果和预期的一致。然而，这是由于目前的 goroutine 调度方式导致的结果。如果我们将例子中注释掉的 time.Sleep(1) 取消注释（这样调度其他 goroutine 执行），再次运行，得到的结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygo
ng.com.     It is
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，Reader 的 Peek 方法如果返回的 []byte 长度小于 n，这时返回的 err != nil ，用于解释为啥会小于 n。如果 n 大于 reader 的 buffer 长度，err 会是 ErrBufferFull。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Reader 的其他方法都是实现了 io 包中的接口，它们的使用方法在io包中都有介绍，在此不赘述。&lt;/p&gt;

&lt;p&gt;这些方法包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) Read(p []byte) (n int, err error)
func (b *Reader) ReadByte() (c byte, err error)
func (b *Reader) ReadRune() (r rune, size int, err error)
func (b *Reader) UnreadByte() error
func (b *Reader) UnreadRune() error
func (b *Reader) WriteTo(w io.Writer) (n int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你应该知道它们都是哪个接口的方法吧。&lt;/p&gt;

&lt;h2 id=&#34;scanner-类型和方法&#34;&gt;Scanner 类型和方法&lt;/h2&gt;

&lt;h3 id=&#34;类型-1&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;对于简单的读取一行，在 Reader 类型中，感觉没有让人特别满意的方法。于是，Go1.1增加了一个类型：Scanner。官方关于Go1.1增加该类型的说明如下：&lt;/p&gt;

&lt;p&gt;在 bufio 包中有多种方式获取文本输入，ReadBytes、ReadString 和独特的 ReadLine，对于简单的目的这些都有些过于复杂了。在 Go 1.1 中，添加了一个新类型，Scanner，以便更容易的处理如按行读取输入序列或空格分隔单词等，这类简单的任务。它终结了如输入一个很长的有问题的行这样的输入错误，并且提供了简单的默认行为：基于行的输入，每行都剔除分隔标识。这里的代码展示一次输入一行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scanner := bufio.NewScanner(os.Stdin)
for scanner.Scan() {
    fmt.Println(scanner.Text()) // Println will add back the final &#39;\n&#39;
}
if err := scanner.Err(); err != nil {
    fmt.Fprintln(os.Stderr, &amp;quot;reading standard input:&amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输入的行为可以通过一个函数控制，来控制输入的每个部分（参阅 SplitFunc 的文档），但是对于复杂的问题或持续传递错误的，可能还是需要原有接口。&lt;/p&gt;

&lt;p&gt;Scanner 类型和 Reader 类型一样，没有任何导出的字段，同时它也包装了一个 io.Reader 对象，但它没有实现 io.Reader 接口。&lt;/p&gt;

&lt;p&gt;Scanner 的结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Scanner struct {
    r            io.Reader // The reader provided by the client.
    split        SplitFunc // The function to split the tokens.
    maxTokenSize int       // Maximum size of a token; modified by tests.
    token        []byte    // Last token returned by split.
    buf          []byte    // Buffer used as argument to split.
    start        int       // First non-processed byte in buf.
    end          int       // End of data in buf.
    err          error     // Sticky error.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里 split、maxTokenSize 和 token 需要讲解一下。&lt;/p&gt;

&lt;h4 id=&#34;split&#34;&gt;split&lt;/h4&gt;

&lt;p&gt;split对应的类型是SplitFunc ，我们需要了解一下SplitFunc类型，定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SplitFunc func(data []byte, atEOF bool) (advance int, token []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SplitFunc 定义了 用于对输入进行分词的 split 函数的签名。参数 data 是还未处理的数据，atEOF 标识 Reader 是否还有更多数据（是否到了EOF）。返回值 advance 表示从输入中读取的字节数，token 表示下一个结果数据，err 则代表可能的错误。&lt;/p&gt;

&lt;p&gt;举例说明一下这里的 token 代表的意思：&lt;/p&gt;

&lt;p&gt;有数据 &amp;ldquo;studygolang\tpolaris\tgolangchina&amp;rdquo;，通过&amp;rdquo;\t&amp;rdquo;进行分词，那么会得到三个token，它们的内容分别是：studygolang、polaris 和 golangchina。而 SplitFunc 的功能是：进行分词，并返回未处理的数据中第一个 token。对于这个数据，就是返回 studygolang。
如果 data 中没有一个完整的 token，例如，在扫描行（scanning lines）时没有换行符，SplitFunc 会返回(0,nil,nil)通知 Scanner 读取更多数据到 slice 中。&lt;/p&gt;

&lt;p&gt;如果 err != nil，扫描停止，同时该错误会返回。&lt;/p&gt;

&lt;p&gt;如果参数 data 为空的 slice，除非 atEOF 为 true，否则该函数永远不会被调用。如果 atEOF 为 true，这时 data 可以非空，这时的数据是没有处理的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;SplitFunc 的实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 bufio 包中预定义了一些 split 函数，也就是说，在 Scanner 结构中的 split 字段，可以通过这些预定义的 split 赋值，同时 Scanner 类型的 Split 方法也可以接收这些预定义函数作为参数。所以，我们可以说，这些预定义 split 函数都是 SplitFunc 类型的实例。这些函数包括：ScanBytes、ScanRunes、ScanWords 和 ScanLines。（由于都是 SplitFunc 的实例，自然这些函数的签名都和 SplitFunc 一样）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ScanBytes 返回单个字节作为一个 token。&lt;/li&gt;
&lt;li&gt;ScanRunes 返回单个 UTF-8 编码的 rune 作为一个 token。返回的 rune 序列（token）和 range string类型 返回的序列是等价的，也就是说，对于无效的 UTF-8 编码会解释为 U+FFFD = &amp;ldquo;\xef\xbf\xbd&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;ScanWords 返回通过“空格”分词的单词。如：study golang，调用会返回study。注意，这里的“空格”是 unicode.IsSpace()，即包括：&amp;rsquo;\t&amp;rsquo;, &amp;lsquo;\n&amp;rsquo;, &amp;lsquo;\v&amp;rsquo;, &amp;lsquo;\f&amp;rsquo;, &amp;lsquo;\r&amp;rsquo;, &amp;lsquo; &amp;lsquo;, U+0085 (NEL), U+00A0 (NBSP)。&lt;/li&gt;
&lt;li&gt;ScanLines 返回一行文本，不包括行尾的换行符。这里的换行包括了Windows下的&amp;rdquo;\r\n&amp;rdquo;和Unix下的&amp;rdquo;\n&amp;rdquo;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般地，我们不会单独使用这些函数，而是提供给 Scanner 实例使用。&lt;/p&gt;

&lt;h4 id=&#34;maxtokensize&#34;&gt;maxTokenSize&lt;/h4&gt;

&lt;p&gt;maxTokenSize 字段 表示通过 split 分词后的一个 token 允许的最大长度。在该包中定义了一个常量 MaxScanTokenSize = 64 * 1024，这是允许的最大 token 长度（64k）。&lt;/p&gt;

&lt;h3 id=&#34;方法-1&#34;&gt;方法&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Scanner 没有导出任何字段，而它需要有外部的 io.Reader 对象，因此，我们不能直接实例化 Scanner 对象，必须通过 bufio 包提供的实例化函数来实例化。实例化函数签名以及内部实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewScanner(r io.Reader) *Scanner {
    return &amp;amp;Scanner{
        r:            r,
        split:        ScanLines,
        maxTokenSize: MaxScanTokenSize,
        buf:          make([]byte, 4096), // Plausible starting size; needn&#39;t be large.
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，返回的 Scanner 实例默认的 split 函数是 ScanLines。这边实例化也是需要一个id.reader的实例，所以我们一般也是使用string或者[]byte的实例来做创建参数使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Split 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;前面我们提到过可以通过 Split 方法为 Scanner 实例设置分词行为。由于 Scanner 实例的默认 split 总是 ScanLines，如果我们想要用其他的 split，可以通过 Split 方法做到。&lt;/p&gt;

&lt;p&gt;比如，我们想要统计一段英文有多少个单词（不排除重复），我们可以这么做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const input = &amp;quot;This is The Golang Standard Library.\nWelcome you!&amp;quot;
scanner := bufio.NewScanner(strings.NewReader(input))
scanner.Split(bufio.ScanWords)
count := 0
for scanner.Scan() {
    count++
}
if err := scanner.Err(); err != nil {
    fmt.Fprintln(os.Stderr, &amp;quot;reading input:&amp;quot;, err)
}
fmt.Println(count)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们实例化 Scanner 后，通过调用 scanner.Split(bufio.ScanWords) 来更改 split 函数。注意，我们应该在调用 Scan 方法之前调用 Split 方法。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Scan 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;该方法好比 iterator 中的 Next 方法，它用于将 Scanner 获取下一个 token，以便 Bytes 和 Text 方法可用。当扫描停止时，它返回false，这时候，要么是到了输入的末尾要么是遇到了一个错误。注意，当 Scan 返回 false 时，通过 Err 方法可以获取第一个遇到的错误（但如果错误是 io.EOF，Err 方法会返回 nil）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Bytes 和 Text 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这两个方法的行为一致，都是返回最近的 token，无非 Bytes 返回的是 []byte，Text 返回的是 string。该方法应该在 Scan 调用后调用，而且，下次调用 Scan 会覆盖这次的 token。比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scanner := bufio.NewScanner(strings.NewReader(&amp;quot;http://studygolang.com. \nIt is the home of gophers&amp;quot;))
if scanner.Scan() {
    scanner.Scan()
    fmt.Printf(&amp;quot;%s&amp;quot;, scanner.Text())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回的是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;It is the home of gophers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而不是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygolang.com.
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Err 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;前面已经提到，通过 Err 方法可以获取第一个遇到的错误（但如果错误是 io.EOF，Err 方法会返回 nil）。&lt;/p&gt;

&lt;h3 id=&#34;完整实例&#34;&gt;完整实例&lt;/h3&gt;

&lt;p&gt;我们经常会有这样的需求：读取文件中的数据，一次读取一行。在学习了 Reader 类型，我们可以使用它的 ReadBytes 或 ReadString来实现，甚至使用 ReadLine 来实现。然而，在 Go1.1 中，我们可以使用 Scanner 来做这件事，而且更简单好用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file, err := os.Create(&amp;quot;scanner.txt&amp;quot;)
if err != nil {
    panic(err)
}
defer file.Close()
file.WriteString(&amp;quot;http://studygolang.com.\nIt is the home of gophers.\nIf you are studying golang, welcome you!&amp;quot;)
// 将文件 offset 设置到文件开头
file.Seek(0, os.SEEK_SET)
scanner := bufio.NewScanner(file)
for scanner.Scan() {
    fmt.Println(scanner.Text())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygolang.com.
It is the home of gophers.
If you are studying golang, welcome you!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;writer-类型和方法&#34;&gt;Writer 类型和方法&lt;/h2&gt;

&lt;h3 id=&#34;类型-2&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;bufio.Writer 结构包装了一个 io.Writer 对象，提供缓存功能，同时实现了 io.Writer 接口。&lt;/p&gt;

&lt;p&gt;Writer 结构没有任何导出的字段，结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Writer struct {
    err error        // 写过程中遇到的错误
    buf []byte        // 缓存
    n   int            // 当前缓存中的字节数
    wr  io.Writer    // 底层的 io.Writer 对象
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相比 bufio.Reader, bufio.Writer 结构定义简单很多。&lt;/p&gt;

&lt;p&gt;注意：如果在写数据到 Writer 的时候出现了一个错误，不会再允许有数据被写进来了，并且所有随后的写操作都会返回该错误。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;和 Reader 类型一样，bufio 包提供了两个实例化 bufio.Writer 对象的函数：NewWriter 和 NewWriterSize。其中，NewWriter 函数是调用 NewWriterSize 函数实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewWriter(wr io.Writer) *Writer {
    // 默认缓存大小：defaultBufSize=4096
    return NewWriterSize(wr, defaultBufSize)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下 NewWriterSize 的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewWriterSize(wr io.Writer, size int) *Writer {
    // 已经是 bufio.Writer 类型，且缓存大小不小于 size，则直接返回
    b, ok := wr.(*Writer)
    if ok &amp;amp;&amp;amp; len(b.buf) &amp;gt;= size {
        return b
    }
    if size &amp;lt;= 0 {
        size = defaultBufSize
    }
    return &amp;amp;Writer{
        buf: make([]byte, size),
        wr:  w,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;方法-2&#34;&gt;方法&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Available 和 Buffered 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Available 方法获取缓存中还未使用的字节数（缓存大小 - 字段 n 的值）；&lt;/p&gt;

&lt;p&gt;Buffered 方法获取写入当前缓存中的字节数（字段 n 的值）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Flush 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;该方法将缓存中的所有数据写入底层的 io.Writer 对象中。使用 bufio.Writer 时，在所有的 Write 操作完成之后，应该调用 Flush 方法使得缓存都写入 io.Writer 对象中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Writer 类型其他方法是一些实际的写方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 实现了 io.ReaderFrom 接口
func (b *Writer) ReadFrom(r io.Reader) (n int64, err error)

// 实现了 io.Writer 接口
func (b *Writer) Write(p []byte) (nn int, err error)

// 实现了 io.ByteWriter 接口
func (b *Writer) WriteByte(c byte) error

// io 中没有该方法的接口，它用于写入单个 Unicode 码点，返回写入的字节数（码点占用的字节），内部实现会根据当前 rune 的范围调用 WriteByte 或 WriteString
func (b *Writer) WriteRune(r rune) (size int, err error)

// 写入字符串，如果返回写入的字节数比 len(s) 小，返回的error会解释原因
func (b *Writer) WriteString(s string) (int, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些写方法在缓存满了时会调用 Flush 方法。另外，这些写方法源码开始处，有这样的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if b.err != nil {
    return b.err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说，只要写的过程中遇到了错误，再次调用写操作会直接返回该错误。&lt;/p&gt;

&lt;h2 id=&#34;readwriter-类型和实例化&#34;&gt;ReadWriter 类型和实例化&lt;/h2&gt;

&lt;h3 id=&#34;类型-3&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;ReadWriter 结构存储了 bufio.Reader 和 bufio.Writer 类型的指针（内嵌），它实现了 io.ReadWriter 结构。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ReadWriter struct {
    *Reader
    *Writer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;实例&#34;&gt;实例&lt;/h3&gt;

&lt;p&gt;ReadWriter 的实例化可以跟普通结构类型一样，也可以通过调用 bufio.NewReadWriter 函数来实现：只是简单的实例化 ReadWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReadWriter(r *Reader, w *Writer) *ReadWriter {
    return &amp;amp;ReadWriter{r, w}
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus snmp Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/snmp_exporter/</link>
          <pubDate>Fri, 09 Nov 2018 14:29:56 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/snmp_exporter/</guid>
          <description>&lt;p&gt;Prometheus exporter for snmp server metrics.&lt;/p&gt;

&lt;h1 id=&#34;源码原理解析&#34;&gt;源码原理解析&lt;/h1&gt;

&lt;p&gt;1、解析配置文件到结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Config map[string]*Module
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、metrics中可以指定snmp对应的版本&lt;/p&gt;

&lt;p&gt;3、支持配置重载&lt;/p&gt;

&lt;p&gt;4、重置url：snmp的处理函数handler&lt;/p&gt;

&lt;p&gt;5、获取两个必要调用参数：target和module（默认是if_mib）&lt;/p&gt;

&lt;p&gt;6、生成结构体collector重写describe和collect&lt;/p&gt;

&lt;p&gt;7、colletcd中调用github.com/soniah/gosnmp这个snmp clinet lib来获取数据&lt;/p&gt;

&lt;p&gt;8、解析数据放入到对应配置文件中的指标进行赋值输出&lt;/p&gt;

&lt;p&gt;9、这边就是直接使用了snmp协议，所以还是要了解mib对应的东西，包括格式与语法&lt;/p&gt;

&lt;p&gt;下面就是看我们默认的if_mib采集是哪些数据，是否符合要求，是否要换去其他模版&lt;/p&gt;

&lt;p&gt;針對普通網絡設備的端口，MIB的相關定義是Interface組，主要管理如下信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ifIndex 端口索引號
ifDescr 端口描述
ifType 端口類型
ifMtu 最大傳輸包字節數
ifSpeed 端口速度
ifPhysAddress 物理地址
ifOperStatus 操作狀態
ifLastChange 上次狀態更新時間
ifInOctets 輸入字節數
*ifInUcastPkts 輸入非廣播包數
*ifInNUcastPkts 輸入廣播包數
*ifInDiscards 輸入包丟棄數
*ifInErrors 輸入包錯誤數
*ifInUnknownProtos 輸入未知協議包數
*ifOutOctets 輸出字節數
*ifOutUcastPkts 輸出非廣播包數
*ifOutNUcastPkts 輸出廣播包數
*ifOutDiscards 輸出包丟棄數
*ifOutErrors 輸出包錯誤數
ifOutQLen 輸出隊長 其中， 號標識的是與網絡流量有關的信息。
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;獲取CISCO2900端口1的上行總流量 snmpwalk -v 1 -c public 192.168.1.254 IF-MIB::ifInOctets.1 返回結果 IF-MIB::ifInOctets.1 = Counter32: 4861881&lt;/li&gt;
&lt;li&gt;五秒後再獲取一次 snmpwalk -v 1 -c public 192.168.1.254 IF-MIB::ifInOctets.1 返回結果 IF-MIB::ifInOctets.1 = Counter32: 4870486 3、計算結果 （後值48704863-前值4861881）/ 5＝1721b/s （應該是BYTE）&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;generator原理&#34;&gt;generator原理&lt;/h1&gt;

&lt;p&gt;将generator.yml转化为snmp.yml文件，通过解析generator.yml中配置的mib module也就是walk中的数组，在mibs中都有现成的指标定义，然后听过netsnmp解析成我们需要的snmp.yml文件进行采集&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控系列---- 监控总纲</title>
          <link>https://kingjcy.github.io/post/monitor/monitor/</link>
          <pubDate>Fri, 02 Nov 2018 19:51:38 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/monitor/</guid>
          <description>&lt;p&gt;几乎所有的系统（我们通常都是APM：应用系统监控）都可以通过是三个方面来构建三维一体立体化监控体系。&lt;/p&gt;

&lt;h1 id=&#34;总体架构&#34;&gt;总体架构&lt;/h1&gt;

&lt;p&gt;立体化监控分三个维度,三种相互辅助的完整监控体系。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Metrics：   可以用于服务告警&lt;/li&gt;
&lt;li&gt;Logging：   用于调试发现问题&lt;/li&gt;
&lt;li&gt;Tracing：   用于调试发现问题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/monitor&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;不同的指标表示不同维度的监控，构成一个完成的监控体系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/monitor1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;监控分类&#34;&gt;监控分类&lt;/h1&gt;

&lt;h2 id=&#34;metrics&#34;&gt;Metrics&lt;/h2&gt;

&lt;p&gt;metrics就是指标监控，通过定义的指标来表示资源的使用情况或者状态等。针对这一类监控，我们通常使用时序数据库来做图表面展示，我们常用的监控系统有&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/metrics/&#34;&gt;prometheus生态,zabbix&lt;/a&gt;等，目前来说时序更加符合监控的需求，无论重数据的存储到数据的使用上都是时序数据库有优势。&lt;/p&gt;

&lt;h2 id=&#34;logging&#34;&gt;Logging&lt;/h2&gt;

&lt;p&gt;log就是日志监控，采集日志，进行处理，存储，展示，重日志中获取到运行的信息，我们通常用的日志系统有&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/log-scheme/&#34;&gt;EFK生态&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;tracing&#34;&gt;Tracing&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/trace/trace/&#34;&gt;trace&lt;/a&gt;就是调用链监控，即一次完整的事务调用请求。比方说一个用户的下单请求，经过层层服务预处理，到支付服务成功，数据落库，成功返回，这就是一条完整的 Trace 。Trace 最大的特点就是它含有上下文环境，通常来说会由一个唯一的 ID 来进行标识。一个 Trace 内可能有多个不同的事务 (Transaction) 以及标志事件 (Event) 组成。我们通常使用的是[Skywalking]()，CNCF推荐的是[jaeger]()。&lt;/p&gt;

&lt;h2 id=&#34;对比结合&#34;&gt;对比结合&lt;/h2&gt;

&lt;p&gt;三种对比&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;metrics监控前期的搭建难度适中，后期维护比较简单，在出现问题的时候，灵敏度比较高，比较容易发现问题，发出告警，但是在排查问题的只有表面的数据，不能找到具体的根因。&lt;/li&gt;
&lt;li&gt;log监控前期的搭建难度比较低，现在搭建一套ELK已经很成熟了，后期维护比较高，因为日志的数据量都是巨大的，在出现问题的时候，灵敏度比较适中，不是太容易发现问题，发出告警，主要在排查问题的进行查看。&lt;/li&gt;
&lt;li&gt;trace监控前期的搭建难度很高，后期维护也比较适中，在出现问题的时候，灵敏度很低，不容易发现问题，发出告警，但是在排查问题的时候能找到具体的根因，主要是用于排查问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结合&lt;/p&gt;

&lt;p&gt;一般系统出问题的时候，都是metrics先发现问题，发出告警，然后我们去查看日志查看问题，查看trace来定位到具体的原因。&lt;/p&gt;

&lt;h1 id=&#34;监控分层&#34;&gt;监控分层&lt;/h1&gt;

&lt;p&gt;在不同的层面都可以使用上面三种监控类型进行监控，结合起来完成每一层完整的监控体系。&lt;/p&gt;

&lt;p&gt;从底到上分为：基础设施监控、系统层监控、应用层监控、业务层监控、端用户体验监控&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/monitor2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;端侧监控主要是用调用链监控，来监控一些调用延迟，错误等，比如听云。&lt;/li&gt;
&lt;li&gt;业务层监控主要是用metrics监控，日志监控，来监控注册登陆转化订单数据，这些业务的错误情况下还需要通过日志来进行排查。&lt;/li&gt;
&lt;li&gt;应用层其实也就是我们的服务层了，需要通过三位一体化的监控，形成完整的监控体系，来监控一些调用延迟错误。&lt;/li&gt;
&lt;li&gt;系统层就是底层基础设施的监控，一些系统的指标日志。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控系列---- log</title>
          <link>https://kingjcy.github.io/post/monitor/log/log-scheme/</link>
          <pubDate>Mon, 13 Aug 2018 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/log-scheme/</guid>
          <description>&lt;p&gt;日志是设备或者程序对自身状态和运作行为的记录，日志监控平台是包括日志采集，存储，分析，索引查询，告警以及各种流程管理的一站式日志服务，日志监控是监控体系中核心的建设，而且可以说是量最大的一项监控。&lt;/p&gt;

&lt;h1 id=&#34;日志&#34;&gt;日志&lt;/h1&gt;

&lt;p&gt;日志是设备或者程序对自身状态和运作行为的记录。日志记录了事件，通过日志就可以看到设备和程序运行的历史信息，通过这些信息，可以了解设备和程序运行情况的变化，以更好的对于设备和程序进行维护。主要是在系统出现问题的时候，通过对于运行过程中发生的历史事件，可以查找问题出现的原因。&lt;/p&gt;

&lt;p&gt;我们可以通过下图来对日志有一个直观的概念&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/log/log&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;日志平台&#34;&gt;日志平台&lt;/h1&gt;

&lt;p&gt;业内最常见的日志采集方案就是 ELK，在 ELK 出来之前，日志管理基本上都是通过登陆日志所在机器然后使用 Linux 命令或人为查看和统计 ，这样是非常没有效率的。&lt;/p&gt;

&lt;h2 id=&#34;架构&#34;&gt;架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/log/log1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这是一个最简化版的日志收集架构，很多基于ELK的日志架构是从它演化而来，比如中加上kafka等队列缓存，核心的问题就是日志数据都保存到ElasticSearch中。其实核心的是四大模块&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据采集模块：负责从各节点上实时采集数据，建议选用filebeat来实现。&lt;/li&gt;
&lt;li&gt;数据接入模块：由于采集数据的速度和数据处理的速度不一定同步，因此添加一个消息中间件来作为缓冲，建议选用Kafka来实现。&lt;/li&gt;
&lt;li&gt;存储计算模块：对采集到的数据进行实时存储分析，建议选用ES来实现。&lt;/li&gt;
&lt;li&gt;数据输出模块：对分析后的结果展示，一般使用kibana。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;采集&#34;&gt;采集&lt;/h2&gt;

&lt;p&gt;在日志采集方面，可以说是有很多项目的支持，从以一开始的logstash，Rsyslog到后来Flume，Fluentd，Filebeat等。采集越来越倾向于轻量级，性能越来越高。容器日志采集和寻常的采集也不一样，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/collect-scheme/&#34;&gt;不同的方案&lt;/a&gt;有不同的适用场景。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.02.20&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;今天调研了grafana推出的loki也是处理日志,这边采集是promtail，具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/loki/loki/&#34;&gt;loki调研&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;存储&#34;&gt;存储&lt;/h2&gt;

&lt;p&gt;在日志存储索引查询方面，目前只有&lt;a href=&#34;https://kingjcy.github.io/post/database/elasticsearch/&#34;&gt;ES&lt;/a&gt;一个核心技术站，并没有过多的选择。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.02.20&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;今天调研了grafana推出的loki也是处理日志，借鉴了prometheus的label和metrics理念，通过label完成检索，具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/loki/loki/&#34;&gt;loki调研&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;展示&#34;&gt;展示&lt;/h2&gt;

&lt;p&gt;在数据展示报表方面，目前对日志也没有什么选择，只有kibana。Kibana主要负责读取ElasticSearch中的数据，并进行可视化展示。并且，它还自带Tool，可以方便调用ElasticSearch的Rest API。在日志平台中，我们通过Kibana查看日志。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控系列---- trace</title>
          <link>https://kingjcy.github.io/post/monitor/trace/trace/</link>
          <pubDate>Mon, 13 Aug 2018 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/trace/trace/</guid>
          <description>&lt;p&gt;全链路监控系统 - APM（Application Performance Managemen）主要用于调用链路追踪，对每一次调用都做性能分析。&lt;/p&gt;

&lt;p&gt;当我们需要解决以下的问题的时候，我们就需要引入APM了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;上线发布后，如何确认服务一切正常？&lt;/li&gt;
&lt;li&gt;客户端收到了错误的提示，但是到底是哪个服务抛出的这个错误？&lt;/li&gt;
&lt;li&gt;程序性能有问题，但是具体是哪个环节成了性能的瓶颈？&lt;/li&gt;
&lt;li&gt;接口响应很慢，到底是网络问题还是代码问题？&lt;/li&gt;
&lt;li&gt;服务调用链路长，每个环节都可能是一个出问题的风险点？&lt;/li&gt;
&lt;li&gt;做技术优化，如何丈量我们的服务质量呢？&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;调用链&#34;&gt;调用链&lt;/h1&gt;

&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;

&lt;p&gt;调用链中的核心概念&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trace 一次分布式调用的链路&lt;/li&gt;
&lt;li&gt;Span 一次本地或者远程方法的调用&lt;/li&gt;
&lt;li&gt;Annotation 附加在 Span 上的日志信息&lt;/li&gt;
&lt;li&gt;Sampling 采样率（客户端按照比例将埋点信息提交给服务端）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来通过实例来具体展示以下对应的概念&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/trace&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上我们图示了一次分布式调用的全过程，图中有三个分布式服务 Service A、Service B、Service B，每个方法的调用链信息中涉及到如下三个标记：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;tid：tid 即为 trace id，代表该次调用的唯一 ID，在一次分布式调用中，所有方法的 tid 都相同。如上图中的 Tid:1。&lt;/li&gt;
&lt;li&gt;sid：sid 即为 span id， 代表的是一个本地/远程方法调用的唯一 ID。如上图中每个绿色框代表的是一次方法调用，每次调用都有自己的 sid。&lt;/li&gt;
&lt;li&gt;pid：pid 其实也是一个 span id，但是它代表的是当前方法的父级方法的 span id。如上图中第一方法调用由客户端发起，是没有 pid 的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在上图中所示的调用链中总共包含了 7 个方法（本地/远程）调用，依次如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用客户端发起调用请求后，首先请求进入到 A 服务。此时会产生调用链信息 tid:1, sid:1 。&lt;/li&gt;
&lt;li&gt;接着发生了一次远程调用 Tid:1, pid:1, sid:2，pid 为 1 代表父级方法的 span id 为 1 即为 sid = 1 的方法，同理本次 redis 远程调用的 span id 为 2。&lt;/li&gt;
&lt;li&gt;Redis 远程调用结束后发生了对 Service B 的远程调用 Tid:1, pid:1, sid:3，与方法 2 类似，不同的是本次方法调用的 span id 为 3。&lt;/li&gt;
&lt;li&gt;在 Service B 中，首先是一个本地方法调用 Tid:1, pid:3, sid:4，从 pid = 3 可以得出它的父级方法正是方法 3。&lt;/li&gt;
&lt;li&gt;接着发生了一次对 Mysql 的远程调用 Tid:1, pid:4, sid:5，pid = 4 代表父级方法为方法 4，span id 为 5。&lt;/li&gt;
&lt;li&gt;Mysql 远程调用结束后，Service B 对 Service C 进行了一次远程调用 Tid:1, pid:4, sid:6，同样通过 pid 和 tid 我们可以将本次方法调用与整个调用链关联起来。&lt;/li&gt;
&lt;li&gt;最后是 Service C 的一个本地方法调用 Tid:1, pid:6, sid:7，至此整个调用链到达最远端，这是本次分布式调用链最深处的一个方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;发展&#34;&gt;发展&lt;/h2&gt;

&lt;p&gt;2010 年 Google Dapper 问世，其实早在 2002 年 eBay 已经有了自己的调用链监控产品，它叫 CAL（Central Application Logging）。当时 eBay 中国研发中心的一位资深工程师作为 CAL 的核心维护人员，对 CAL 的方方面面都非常熟悉。&lt;/p&gt;

&lt;p&gt;后来他去了美团点评，在 2011 年的时候，他带领团队研发出了 CAT（Central Application Tracing），CAT 继承了 CAL 的优点，也增加了很多自己的特色功能，并且它已经在 GitHub 开源，也在美团点评经受了大流量，高并发应用的检验，是目前业界应用比较广泛和成熟的生产级别调用链监控产品。&lt;/p&gt;

&lt;p&gt;随后由于 Google Dapper 论文的发布，也伴随着互联网产品的迅速发展，各个大厂依据 Dapper 纷纷实现了自己的调用链监控产品。在 2012 年就诞生了 3 款产品，携程的 CTrace，韩国公司 Naver 的 PinPoint，Twitter 的 Zipkin。&lt;/p&gt;

&lt;p&gt;随后在 2014 年，阿里研发了 Eagleye，京东研发了 Hydra。接着诞生了调用链监控的标准规范 Open Tracing，面对各大厂的调用链监控产品，他们使用不兼容的 API 来实现各自的应用需求。尽管这些分布式追踪系统有着相似的 API 语法，但各种语言的开发人员依然很难将他们各自的系统（使用不同的语言和技术）和特定的分布式追踪系统进行整合，Open Tracing 希望可以解决这个问题，因此颁布了这套标准。于此也诞生了 Uber 的 Jaeger，国人吴晟做的 SkyWalking（现在已经捐赠给了 Apache）均实现了 Open Tracing 标准&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/trace1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;各大调用链产品分为了典型的三类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CAT 类：鼻祖 CAL，侵入式埋点，国内公司使用较广。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/trace/zipkin&#34;&gt;Zipkin&lt;/a&gt; 类：鼻祖 Google Dapper，侵入式埋点，国内使用不广(s)。&lt;/li&gt;
&lt;li&gt;PinPiont类：鼻祖 Google Dapper，非侵入式卖点，采用字节码增强技术。&lt;/li&gt;
&lt;li&gt;opentacing：&lt;a href=&#34;https://kingjcy.github.io/post/monitor/trace/jaeger&#34;&gt;jaeger&lt;/a&gt;，skywalking&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/trace/trace2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;es&#34;&gt;ES&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/database/elasticsearch/&#34;&gt;ES&lt;/a&gt;数据库很重要，不管在日志还是在调用链都是存储数据的核心模块，也是提供聚合查询的基础。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus postgresql_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/postgres_exporter/</link>
          <pubDate>Thu, 09 Aug 2018 14:29:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/postgres_exporter/</guid>
          <description>&lt;p&gt;Prometheus exporter for PostgreSQL server metrics.&lt;/p&gt;

&lt;h1 id=&#34;源码原理解析&#34;&gt;源码原理解析&lt;/h1&gt;

&lt;p&gt;1、使用kingpin来解析启动参数，包含web.listen-address，web.telemetry-path，disable-default-metrics，extend.query-path，dumpmaps，version&lt;/p&gt;

&lt;p&gt;2、如果设置参数dumpmaps，则直接把默认查询的查询map结构输出，主要查询一些系统信息，最后结束探针不运行&lt;/p&gt;

&lt;p&gt;3、启动时设置环境变量DATA_SOURCE_NAME作为探测pg的地址链接，代码中通过获取环境变量的值来获取datasource&lt;/p&gt;

&lt;p&gt;4、创建一个exporter的结构体，并初始化，包含了默认采集指标&lt;/p&gt;

&lt;p&gt;5、针对可配的路由web.telemetry-path来做对应的数据处理，默认是/metrics，根目录则显示到可配置的目录下获取指标数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1)根据获取的dsn来连接数据库
2）检查版本是否需要更新，就是看pg的版本是否和我们目前支持的采集兼容，低于最低可探测版本则不可用，然后对可探测的对应的版本默认指标进行调整。
3）解析配置文件
4）根据默认语句和配置文件查询语句来进行查询，将查询结果放到对应的指标变量中去。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、启动监听地址和端口，可配置&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系统---- Thanos</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/</link>
          <pubDate>Fri, 13 Jul 2018 17:14:15 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/</guid>
          <description>&lt;p&gt;Thanos，一组通过跨集群联合、跨集群无限存储和全局查询为Prometheus 增加高可用性的组件。&lt;/p&gt;

&lt;h1 id=&#34;基本功能&#34;&gt;基本功能&lt;/h1&gt;

&lt;p&gt;prometheus单点能够支持百万的metrics，但是在规模越来越大的系统中，已经不能满足要求，需要集群的功能来处理更加庞大的数据，基于这个情况，thanos诞生了，thanos的主要功能：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;去重，单点问题，可以让prometheus高可用，实现多采集情况下的数据查询，query是无状态的，可以使用负载均衡&lt;/li&gt;
&lt;li&gt;聚合，实现不同prometheus的数据的聚合，匹配prometheus的hashmode功能，实现集群的方式&lt;/li&gt;
&lt;li&gt;数据备份，主要是基于s3的，相当于远程存储，我们没有使用，直接将数据写入到了kafka&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;基本组件&#34;&gt;基本组件&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Sidecar&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sidecar作为一个单独的进程和已有的Prometheus实例运行在一个server上，互不影响。Sidecar可以视为一个Proxy组件，所有对Prometheus的访问都通过Sidecar来代理进行。通过Sidecar还可以将采集到的数据直接备份到云端对象存储服务器。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Querier&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有的Sidecar与Querier直连，同时Querier实现了一套Prometheus官方的HTTP API从而保证对外提供与Prometheus一致的数据源接口，Grafana可以通过同一个查询接口请求不同集群的数据，Querier负责找到对应的集群并通过Sidecar获取数据。Querier本身无状态的也是水平可扩展的，因而可以实现高可部署，而且Querier可以实现对高可部署的Prometheus的数据进行合并从而保证多次查询结果的一致性，从而解决全局视图和高可用的问题。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Store&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Store实现了一套和Sidecar完全一致的API提供给Querier用于查询Sidecar备份到云端对象存储的数据。因为Sidecar在完成数据备份后，Prometheus会清理掉本地数据保证本地空间可用。所以当监控人员需要调取历史数据时只能去对象存储空间获取，而Store就提供了这样一个接口。Store Gateway只会缓存对象存储的基本信息，例如存储块的索引，从而保证实现快速查询的同时占用较少本地空间。&lt;/p&gt;

&lt;p&gt;store和sidecar都提供了相同gprc的api，给外部client进行查询，其实是一回事。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Comactor&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Compactor主要用于对采集到的数据进行压缩，实现将数据存储至对象存储时节省空间。单独使用，和集群没有什么关系。主要是将对象存储 Bucket 中的多个小 的相同的Block 合并成 大 Block&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;h2 id=&#34;sidecar&#34;&gt;sidecar&lt;/h2&gt;

&lt;p&gt;sidecar部署在prometheus机器上,直接使用二进制文件配置不同的启动参数来启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/promes/thanos-sidecar/thanos sidecar --log.level=debug --tsdb.path=/data --prometheus.url=http://localhost:9099
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;query&#34;&gt;query&lt;/h2&gt;

&lt;p&gt;query用于查询，单独部署，然后和prometheus一样使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/promes/thanos-query/thanos query --query.timeout=15s --store.response-timeout=15s --log.level=debug --store=10.243.53.96:19091 --store=10.243.53.100:19091 --store=10.243.53.101:19091 --store=10.243.53.186:19091
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sd&#34;&gt;sd&lt;/h2&gt;

&lt;p&gt;thanos有三种sd的方式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Static Flags&lt;/p&gt;

&lt;p&gt;最简单的就是在参数中配置列表，就是我们上面使用的方式&lt;/p&gt;

&lt;p&gt;&amp;ndash;store参数指定的是每个sidecar的grpc端口，query会根据&amp;ndash;store参数列表找到对应的prometheus进行查询，所有组件的端口都是有默认值的，如果需要修改则指定参数&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Interface&lt;/th&gt;
&lt;th&gt;Port&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sidecar&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10901&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sidecar&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10902&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Query&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10903&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Query&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10904&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Store&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10905&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Store&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10906&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Receive&lt;/td&gt;
&lt;td&gt;gRPC (store API)&lt;/td&gt;
&lt;td&gt;10907&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Receive&lt;/td&gt;
&lt;td&gt;HTTP (remote write API)&lt;/td&gt;
&lt;td&gt;10908&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Receive&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10909&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rule&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10910&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rule&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10911&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Compact&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10912&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;File SD&lt;/p&gt;

&lt;p&gt;&amp;ndash;store.sd-files=&lt;path&gt;和 &amp;ndash;store.sd-interval=&lt;5m&gt;来获取对应的prometheus列表&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;DNS SD&lt;/p&gt;

&lt;p&gt;&amp;ndash;store=dns+stores.thanos.mycompany.org:9090
&amp;ndash;store=dnssrv+_thanosstores._tcp.mycompany.org
&amp;ndash;store=dnssrvnoa+_thanosstores._tcp.mycompany.org&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;Thanos 在每一台 Prometheus 服务器上运行一个sidecar组件，并提供了一个用于处理 PromQL 查询的中央 Querier 组件，因而在所有服务器之间引入了一个中央查询层。这些组件构成了一个 Thanos 部署，并基于 memberlist gossip 协议实现组件间通信。Querier 可以水平扩展，因为它是无状态的，并且可充当智能逆向代理，将请求转发给sidecar，汇总它们的响应，并对 PromQL 查询进行评估。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实现细节&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;Thanos 通过使用后端的对象存储来解决数据保留问题。Prometheus 在将数据写入磁盘时，sidecar的 StoreAPI 组件会检测到，并将数据上传到对象存储器中。Store 组件还可以作为一个基于 gossip 协议的检索代理，让 Querier 组件与它进行通信以获取数据。&lt;/li&gt;
&lt;li&gt;我们使用基本的过滤器（基于时间范围和外部标签）过滤掉不会提供所需数据的 StoreAPI（叶子），然后执行剩余的查询。然后将来自不同来源的数据按照时间顺序追加的方式合并在一起。&lt;/li&gt;
&lt;li&gt;Querier 组件可以基于用户规模自动调整密度（例如 5 分钟、1 小时或 24 小时）&lt;/li&gt;
&lt;li&gt;StoreAPI 组件了解 Prometheus 的数据格式，因此它可以优化查询执行计划，并缓存数据块的特定索引，以对用户查询做出足够快的响应，避免了缓存大量数据的必要。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;我们通过为所有 Prometheus+ sidecar实例提供唯一的外部标签来解决多个边车试图将相同的数据块上传到对象存储的问题，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;First:

&amp;quot;cluster&amp;quot;: &amp;quot;prod1&amp;quot;

&amp;quot;replica&amp;quot;: &amp;quot;0&amp;quot;

Second:

&amp;quot;cluster&amp;quot;:&amp;quot;prod1&amp;quot;

&amp;quot;replica&amp;quot;: &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于标签集是唯一的，所以不会有什么问题。不过，如果指定了副本，查询层可以在运行时通过“replica”标签进行除重操作。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thanos 还提供了时间序列数据的压缩和降采样（downsample）存储。Prometheus 提供了一个内置的压缩​​模型，现有较小的数据块被重写为较大的数据块，并进行结构重组以提高查询性能。Thanos 在Compactor 组件（作为批次作业运行）中使用了相同的机制，并压缩对象存储数据。Płotka 说，Compactor 也对数据进行降采样，“目前降采样时间间隔不可配置，不过我们选择了一些合理的时间间隔——5 分钟和1 小时”。压缩也是其他时间序列数据库（如 InfluxDB 和 OpenTSDB ）的常见功能。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;遇到的问题&#34;&gt;遇到的问题&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;查询速度较慢，在数据量基本特别大的时候，查询会超时。&lt;/li&gt;
&lt;li&gt;thanos 目前还不能支持默认查询lookback时间，promehteus可以设置默认查询时间，thanos默认是根据规模自动调整的，目前发现有10m，20m等，这边可以暂时表达式加时间处理这个问题。&lt;/li&gt;
&lt;li&gt;sidecar的启动参数&amp;ndash;cluster-peers是什么作用&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;在prometheus的聚合和集群发展中，出现了很多的相同的项目，大部分都是使用了远程存储的概念，比如&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/&#34;&gt;cortex&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/&#34;&gt;M3DB&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;victoriametrics&lt;/a&gt;，thanos在调研落地的过程中，各方面还是相对做到比较好的，适合做为prometheus的扩展和聚合方案。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/thanos/thanos&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2019.9.9&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;victoriametrics&lt;/a&gt;在存储和查询上更加的优秀，目前比较推荐victoriametrics。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus mysqld_exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/mysqld-exporter/</link>
          <pubDate>Mon, 09 Jul 2018 14:29:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/mysqld-exporter/</guid>
          <description>&lt;p&gt;mysql监控指标采集探针:Prometheus exporter for MySQL server metrics.&lt;/p&gt;

&lt;h1 id=&#34;源码原理解析&#34;&gt;源码原理解析&lt;/h1&gt;

&lt;p&gt;1、collector&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scraperFlags := map[collector.Scraper]*bool{}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记录需要采集的collector，是interface Scraper的实现的结构体，并且支持所有采集 启动项可配置。&lt;/p&gt;

&lt;p&gt;2、连接数据库重环境变量或者mysql的配置文件中获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DATA_SOURCE_NAME／my.cnf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后采集一下数据库状态，每个scrapers启动一个协程去调用接口中Scrape的实现&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- Filebeat</title>
          <link>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat/</link>
          <pubDate>Sun, 08 Jul 2018 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat/</guid>
          <description>&lt;p&gt;Filebeat 是使用 Golang 实现的轻量型日志采集器，是基于原先 logstash-forwarder 的源码改造出来的，没有任何依赖，可以单独存在的搞性能采集工具。&lt;/p&gt;

&lt;h1 id=&#34;认识beats&#34;&gt;认识beats&lt;/h1&gt;

&lt;p&gt;Beats是轻量级（资源高效，无依赖性，小型）采集程序的集合。这些可以是日志文件（Filebeat），网络数据（Packetbeat），服务器指标（Metricbeat）等，Beats建立在名为libbeat的Go框架之上，该框架主要用于数据转发，Elastic和社区开发的越来越多的Beats可以收集的任何其他类型的数据，收集后，数据将直接发送到Elasticsearch或Logstash中进行其他处理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Filebeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;顾名思义，Filebeat用于收集和传送日志文件，它也是最常用的Beat。 Filebeat如此高效的事实之一就是它处理背压的方式，如果Logstash繁忙，Filebeat会减慢其读取速率，并在减速结束后加快节奏。
Filebeat几乎可以安装在任何操作系统上，包括作为Docker容器安装，还随附用于特定平台（例如Apache，MySQL，Docker等）的内部模块，其中包含这些平台的默认配置和Kibana对象。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Packetbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;网络数据包分析器Packetbeat是第一个引入的beat。 Packetbeat捕获服务器之间的网络流量，因此可用于应用程序和性能监视。
Packetbeat可以安装在受监视的服务器上，也可以安装在其专用服务器上。 Packetbeat跟踪网络流量，解码协议并记录每笔交易的数据。 Packetbeat支持的协议包括：DNS，HTTP，ICMP，Redis，MySQL，MongoDB，Cassandra等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Metricbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Metricbeat是一种非常受欢迎的beat，它收集并报告各种系统和平台的各种系统级度量。 Metricbeat还支持用于从特定平台收集统计信息的内部模块。您可以使用这些模块和称为指标集的metricsets来配置Metricbeat收集指标的频率以及要收集哪些特定指标。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Heartbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Heartbeat是用于“uptime monitoring”的。本质上，Heartbeat是探测服务以检查它们是否可访问的功能，例如，它可以用来验证服务的正常运行时间是否符合您的SLA。 您要做的就是为Heartbeat提供URL和正常运行时间指标的列表。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Auditbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Auditbeat可用于操作Linux服务器上的用户和进程活动。 与其他传统的系统工具（systemd，auditd）类似，Auditbeat可用于识别安全漏洞-文件更改，配置更改，恶意行为等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Winlogbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Winlogbeat仅会引起Windows系统管理员或工程师的兴趣，因为它是专门为收集Windows事件日志而设计的。 它可用于分析安全事件，已安装的更新等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Functionbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Functionbeat主要是为“serverless”而设计，可以将其部署为收集数据并将其发送到ELK堆栈的功能。 Functionbeat专为监视云环境而设计，目前已针对Amazon设置量身定制，可以部署为Amazon Lambda函数，以从Amazon CloudWatch，Kinesis和SQS收集数据。&lt;/p&gt;

&lt;h1 id=&#34;filebeat&#34;&gt;filebeat&lt;/h1&gt;

&lt;p&gt;为什么选择filebeat&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;性能好&lt;/li&gt;
&lt;li&gt;基于golang的技术站，对于容器生态友好&lt;/li&gt;
&lt;li&gt;使用部署方便，功能齐全&lt;/li&gt;
&lt;li&gt;其他技术方案，主要是社区的fluentd，使用的ruby+c，使用起来很复杂，各种功能都需要写插件来完成。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;基本使用&#34;&gt;基本使用&lt;/h2&gt;

&lt;h3 id=&#34;下载&#34;&gt;下载&lt;/h3&gt;

&lt;p&gt;直接去github上可以下载二进制文件，可以直接运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.10.2-linux-x86_64.tar.gz
tar xzvf filebeat-7.10.2-linux-x86_64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以使用源码编译&lt;/p&gt;

&lt;p&gt;After installing Go, set the GOPATH environment variable to point to your workspace location, and make sure $GOPATH/bin is in your PATH.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p ${GOPATH}/src/github.com/elastic
git clone https://github.com/elastic/beats ${GOPATH}/src/github.com/elastic/beats
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have multiple go paths, use ${GOPATH%%:*} instead of ${GOPATH}.&lt;/p&gt;

&lt;p&gt;Then you can compile a particular Beat by using the Makefile. For example, for Packetbeat:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd beats/packetbeat
make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of the Beats might have extra development requirements, in which case you’ll find a CONTRIBUTING.md file in the Beat directory.&lt;/p&gt;

&lt;p&gt;We use an EditorConfig file in the beats repository to standardise how different editors handle whitespace, line endings, and other coding styles in our files. Most popular editors have a plugin for EditorConfig and we strongly recommend that you install it.&lt;/p&gt;

&lt;h3 id=&#34;配置文件&#34;&gt;配置文件&lt;/h3&gt;

&lt;p&gt;FileBeat 的配置文件定义了在读取文件的位置，输出流的位置以及相应的性能参数，本实例是以 Kafka 消息中间件作为缓冲，所有的日志收集器都向 Kafka 输送日志流，相应的最简单的配置项如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim fileat.yml

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /wls/applogs/rtlog/app.log
  fields:
    log_topic: appName
  multiline:
        # pattern for error log, if start with space or cause by
        pattern: &#39;^[[:space:]]+(at|\.{3})\b|^Caused by:&#39;
        negate:  false
        match:   after

output.kafka:
   enabled: true
   hosts: [&amp;quot;kafka-1:9092&amp;quot;,&amp;quot;kafka-2:9092&amp;quot;]
   topic: applog
   version: &amp;quot;0.10.2.0&amp;quot;
   compression: gzip

processors:
- drop_fields:
   fields: [&amp;quot;beat&amp;quot;, &amp;quot;input&amp;quot;, &amp;quot;source&amp;quot;, &amp;quot;offset&amp;quot;]

logging.level: error
name: app-server-ip
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;paths:定义了日志文件路径，可以采用模糊匹配模式，如*.log&lt;/li&gt;
&lt;li&gt;fields：topic 对应的消息字段或自定义增加的字段。&lt;/li&gt;
&lt;li&gt;output.kafka：filebeat 支持多种输出，支持向 kafka，logstash，elasticsearch 输出数据，此处设置数据输出到 kafka。&lt;/li&gt;
&lt;li&gt;enabled：这个启动这个模块。&lt;/li&gt;
&lt;li&gt;topic：指定要发送数据给 kafka 集群的哪个 topic，若指定的 topic 不存在，则会自动创建此 topic。&lt;/li&gt;
&lt;li&gt;version：指定 kafka 的版本。&lt;/li&gt;
&lt;li&gt;drop_fields：舍弃字段，filebeat 会 json 日志信息，适当舍弃无用字段节省空间资源。&lt;/li&gt;
&lt;li&gt;name：收集日志中对应主机的名字，建议 name 这里设置为 IP，便于区分多台主机的日志信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每一个不同的输出都用不同的配置项，还有很多功能性能的配置项，比如&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;合并规则&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;multiline:
        pattern: &#39;^[[:space:]]+(at|\.{3})\b|^Caused by:&#39;
        negate:  false
        match:   after
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用的合并规则&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[0-9]{4}-[0-9]{2}-[0-9]{2} 按照yyyy-mm-dd时间戳合并，日志不是以这个时间戳开头的都合并
[[0-9]{4}-[0-9]{2}-[0-9]{2} 按照[yyyy-mm-dd时间戳合并，日志不是以这个时间戳开头的都合并
* 不合并----不合并最后处理出来的结果就是没有这一段的配置
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;过滤规则&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;include_lines: [&amp;quot;FATAL&amp;quot;,&amp;quot;ERROR&amp;quot;,&amp;quot;WARN&amp;quot;,&amp;quot;INFO&amp;quot;,&amp;quot;fatal&amp;quot;,&amp;quot;error&amp;quot;,&amp;quot;warn&amp;quot;,&amp;quot;info&amp;quot;,&amp;quot;Fatal&amp;quot;,&amp;quot;Error&amp;quot;,&amp;quot;Warn&amp;quot;,&amp;quot;Info&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常用于日志级别的选择，比如上面只是采集包含这些字段的日志，其实也就是INFO级别的日志的采集。当然还是使用exclude_lines，不包含，和白名单黑名单一样的概念。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;资源限制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;logging.level: debug    日志级别
max_procs: 2            cpu核数限制

queue:
      mem:
        events: 32768               pipeline队列长度
        flush.min_events: 1024
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;采集配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;close_eof: false
close_inactive: 5m（5分钟没有活跃，就会停止采集）
close_removed: false
close_renamed: false
ignore_older: 48h（即将开启的采集文件如果大于48H，就不要采集了）
# State options
clean_removed: true
clean_inactive: 72h（如果文件72h没有活跃就删除采集记录）
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;输出配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;output.kafka:
      topic: &amp;quot;%{[topic]}&amp;quot;
      version: &amp;quot;0.8.2.2&amp;quot;
      codec.format:
        ignoreNotFound: true
        string: &#39;%{[message]}&#39;
      metadata:
        retry.max: 2
        full: true
      worker: 10（The number of concurrent load-balanced Kafka output workers.）
      channel_buffer_size: 30000（每一个连接可以缓存消息的长度，默认是256）
      ##bulk_max_size: 20480（一次发送kafka请求最多的事件数量，默认是2048）
      #keep_alive: 0（是否保持连接，默认0，不保持）
      ##required_acks: 0（代表kafka是否需要等待回复，有1，0，-1，默认是1，需要等待主节点回复）
      compression: none（代表压缩级别，none代表不压缩，默认压缩是gzip）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多详细的配置可以去查看完整的配置文件说明。&lt;/p&gt;

&lt;h3 id=&#34;启动&#34;&gt;启动&lt;/h3&gt;

&lt;p&gt;调试模式下采用：终端启动（退出终端或 ctrl+c 会退出运行）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./filebeat -e -c filebeat.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;线上环境配合 error 级别使用：以后台守护进程启动启动 filebeats&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup ./filebeat -e -c filebeat.yml &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;零输出启动（不推荐）：将所有标准输出及标准错误输出到/dev/null空设备，即没有任何输出信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup ./filebeat -e -c filebeat.yml &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止运行 FileBeat 进程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ps -ef | grep filebeat
Kill -9 线程号
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;特性&#34;&gt;特性&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;采集路径可以使用正则表达式，来采集当前目录下所有的文件，包括子目录下，比如/k8s_log/*&lt;em&gt;/&lt;/em&gt;.log*&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可以动态加载，可以在配置文件中配置reload的时间，filebeat本身自动加载，但是这个加载不能更新output&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filebeat本身日志支持备份切换，默认一个文件10M，保留8个文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filebaet支持句柄保持和checkpoint功能。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filebeat输出到kafka可以不支持多个kafka集群，可以改造多pipeline来发送到不同的Kafka集群。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;h3 id=&#34;filebeat创建一个beater&#34;&gt;filebeat创建一个beater&lt;/h3&gt;

&lt;p&gt;filebaet创建了一个beater实例启动&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.启动了一个Crawler，用于

&lt;ul&gt;
&lt;li&gt;1.启动静态的 input (写在主配置里的)&lt;/li&gt;
&lt;li&gt;2.启动 reloader，动态的 input 由 reloader 管理。&lt;/li&gt;
&lt;li&gt;3.启动Registrar&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2.每个input又启动了Harvester，Harvester就是负责采集日志&lt;/li&gt;
&lt;li&gt;3.Harvester 连接 pipeline 时，调用 outlet factory 创建一个 outleter，Outleter 封装了 pipeline 的 producer，调用 outleter OnEvent 方法发送数据到 pipeline&lt;/li&gt;
&lt;li&gt;4.Registrar 负责 checkpoint 文件的更新&lt;/li&gt;
&lt;li&gt;5.启动Pipeline 模块，Pipeline 是一个大的功能模块，包含 &lt;code&gt;queue&lt;/code&gt;, &lt;code&gt;outputController&lt;/code&gt;, &lt;code&gt;consumer&lt;/code&gt;, &lt;code&gt;output&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;1.Queue (memqueue)
真正的 queue 对象时 Broker。创建 broker 时启动事件循环，负责处理 publish 和 consume 等各种请求

&lt;ul&gt;
&lt;li&gt;创建 broker&lt;/li&gt;
&lt;li&gt;事件循环&lt;/li&gt;
&lt;li&gt;Consumer&lt;/li&gt;
&lt;li&gt;Producer 真实创建的是 ackProducer&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2.Output Controller
负责管理 consumer。Consumer 负责从 queue 中获取 batch，然后发送到 workQueue(chan publisher.Batch)。&lt;/li&gt;
&lt;li&gt;3.Consumer
Consumer 会启动多个 outputWorker。outputWorker 负责从 workQueue 获取 batch，然后调用 output client 的 Publish 方法发送出去。&lt;/li&gt;
&lt;li&gt;4.Retryer
Retry 负责重试发送失败的请求&lt;/li&gt;
&lt;li&gt;5.Output(kafka)
Connect 调用 sarama 库，创建一个 kafka AsyncProducer。连接时会启动两个循环，处理成功响应和失败响应。&lt;/li&gt;
&lt;li&gt;6.msgRef
msgRef 注入到发送给 AsyncProducer 的事件中。在处理响应的时候回调。如果成功就调用 batch.ACK()。失败就调用 batch.OnRetry 重试。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;核心代码模块&#34;&gt;核心代码模块&lt;/h3&gt;

&lt;h4 id=&#34;filebeat-1&#34;&gt;filebeat&lt;/h4&gt;

&lt;p&gt;beater 启动
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/beater/filebeat.go#L273&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/beater/filebeat.go#L273&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Crawler 启动时加载配置文件中的 inputs。启动 reloader，定期加载动态配置文件目录中的 inputs。启动Registrar，负责checkpoint
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/crawler/crawler.go#L33&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/crawler/crawler.go#L33&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动静态的 input (写在主配置里的)&lt;/li&gt;
&lt;li&gt;启动 reloader，动态的 input 由 reloader 管理&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;InputInput 是一个用来包装 &lt;code&gt;harvester&lt;/code&gt; 的数据结构，对外提供生命周期管理接口。Input 在建立起来时，会调用 pipeline 的 &lt;code&gt;ConnectWith&lt;/code&gt; 方法获取一个 client，用于发送 events。
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/input.go#L35&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/input.go#L35&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Harvester
负责采集日志
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/log/harvester.go#L88&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/log/harvester.go#L88&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Outleter：
Harvester 连接 pipeline 时，调用 outlet factory 创建一个 outleter
Outleter 封装了 pipeline 的 producer，调用 outleter OnEvent 方法发送数据到 pipeline&lt;/p&gt;

&lt;p&gt;Outleter 创建
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/channel/factory.go#L70&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/channel/factory.go#L70&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Registrar
负责 checkpoint 文件的更新
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/registrar/registrar.go#L19&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/registrar/registrar.go#L19&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;libbeat&#34;&gt;Libbeat&lt;/h4&gt;

&lt;p&gt;Pipeline 模块启动
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L30&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L30&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;加载 output &lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L85&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L85&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Pipeline
Pipeline 包含 queue, output controller, consumer, output&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/pipeline.go#L135&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/pipeline.go#L135&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Output Controller

&lt;ul&gt;
&lt;li&gt;负责管理 consumer。Consumer 负责从 queue 中获取 batch，然后发送到 workQueue(chan publisher.Batch)。&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/controller.go#L13&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/controller.go#L13&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Consumer

&lt;ul&gt;
&lt;li&gt;Consumer 会启动多个 outputWorker。outputWorker 负责从 workQueue 获取 batch，然后调用 output client 的 Publish 方法发送出去。
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/consumer.go#L43&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/consumer.go#L43&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Retryer

&lt;ul&gt;
&lt;li&gt;Retry 负责重试发送失败的请求
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/retry.go#L14&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/retry.go#L14&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Output(kafka)

&lt;ul&gt;
&lt;li&gt;创建 &lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/kafka.go#L114&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/kafka.go#L114&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Connect
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L68&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L68&lt;/a&gt;
调用 sarama 库，创建一个 kafka AsyncProducer。连接时会启动两个循环，处理成功响应和失败响应。&lt;/li&gt;
&lt;li&gt;msgRef
msgRef 注入到发送给 AsyncProducer 的事件中。在处理响应的时候回调。如果成功就调用 batch.ACK()。失败就调用 batch.OnRetry 重试。
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L222&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L222&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Queue (memqueue)
真正的 queue 对象时 Broker。创建 broker 时启动事件循环，负责处理 publish 和 consume 等各种请求&lt;/li&gt;
&lt;li&gt;创建 broker
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/broker.go#L63&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/broker.go#L63&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;事件循环
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/eventloop.go#L30&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/eventloop.go#L30&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consumer
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/consume.go#L12&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/consume.go#L12&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Producer
真实创建的是 ackProducer
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/produce.go#L39&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/produce.go#L39&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;工作机制:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;队列容量为 &lt;code&gt;Events&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;当队列中的 events 数量大于 &lt;code&gt;FlushMinEvents&lt;/code&gt; 开始 flush&lt;/li&gt;
&lt;li&gt;当队列中有 events 并且离上一次 flush 过了 &lt;code&gt;FlushTimeout&lt;/code&gt; 时间，开始 flush&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这边简单梳理了filebeat的模块，核心的原理包括一些beats的原理可以看我写的另一篇&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/&#34;&gt;filebeat原理&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- Filebeat原理</title>
          <link>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/</link>
          <pubDate>Sun, 08 Jul 2018 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/</guid>
          <description>&lt;p&gt;Filebeat 是使用 Golang 实现的轻量型日志采集器，也是 Elasticsearch stack 里面的一员。本质上是一个 agent，可以安装在各个节点上，根据配置读取对应位置的日志，并上报到相应的地方去。&lt;/p&gt;

&lt;p&gt;filebeat源码归属于beats项目，而beats项目的设计初衷是为了采集各类的数据，所以beats抽象出了一个libbeat库，基于libbeat我们可以快速的开发实现一个采集的工具，除了filebeat，还有像metricbeat、packetbeat等官方的项目也是在beats工程中。libbeat已经实现了内存缓存队列memqueue、几种output日志发送客户端，数据的过滤处理processor,配置解析、日志打印、事件处理和发送等通用功能，而filebeat只需要实现日志文件的读取等和日志相关的逻辑即可。&lt;/p&gt;

&lt;h1 id=&#34;beats&#34;&gt;beats&lt;/h1&gt;

&lt;p&gt;对于任一种beats来说，主要逻辑都包含两个部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;收集数据并转换成事件&lt;/li&gt;
&lt;li&gt;发送事件到指定的输出&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中第二点已由libbeat实现，因此各个beats实际只需要关心如何收集数据并生成事件后发送给libbeat的Publisher。&lt;/p&gt;

&lt;h1 id=&#34;filebeat整体架构&#34;&gt;filebeat整体架构&lt;/h1&gt;

&lt;h2 id=&#34;架构图&#34;&gt;架构图&lt;/h2&gt;

&lt;p&gt;下图是 Filebeat 官方提供的架构图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/filebeat.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;下图是看代码的一些模块组合&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/filebeat1.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实我个人觉得这一幅图是最形象的说明了filebeat的功能&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/filebeat2.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;模块&#34;&gt;模块&lt;/h2&gt;

&lt;p&gt;除了图中提到的各个模块，整个 filebeat 主要包含以下重要模块：&lt;/p&gt;

&lt;p&gt;1.filebeat主要模块&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Crawler: 负责管理和启动各个Input,管理所有Input收集数据并发送事件到libbeat的Publisher
Input: 负责管理和解析输入源的信息，以及为每个文件启动 Harvester。可由配置文件指定输入源信息。
    Harvester: 负责读取一个文件的数据,对应一个输入源，是收集数据的实际工作者。配置中，一个具体的Input可以包含多个输入源（Harvester）
module: 简化了一些常见程序日志（比如nginx日志）收集、解析、可视化（kibana dashboard）配置项
    fileset: module下具体的一种Input定义（比如nginx包括access和error log），包含：
        1）输入配置；
        2）es ingest node pipeline定义；
        3）事件字段定义；
        4）示例kibana dashboard
Registrar：接收libbeat反馈回来的ACK, 作相应的持久化，管理记录每个文件处理状态，包括偏移量、文件名等信息。当 Filebeat 启动时，会从 Registrar 恢复文件处理状态。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.libbeat主要模块&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Pipeline（publisher）: 负责管理缓存、Harvester 的信息写入以及 Output 的消费等，是 Filebeat 最核心的组件。
    client: 提供Publish接口让filebeat将事件发送到Publisher。在发送到队列之前，内部会先调用processors（包括input 内部的processors和全局processors）进行处理。
    processor: 事件处理器，可对事件按照配置中的条件进行各种处理（比如删除事件、保留指定字段，过滤添加字段，多行合并等）。配置项
    queue: 事件队列，有memqueue（基于内存）和spool（基于磁盘文件）两种实现。配置项
    outputs: 事件的输出端，比如ES、Logstash、kafka等。配置项
    acker: 事件确认回调，在事件发送成功后进行回调
autodiscover：用于自动发现容器并将其作为输入源
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;filebeat 的整个生命周期，几个组件共同协作，完成了日志从采集到上报的整个过程。&lt;/p&gt;

&lt;h1 id=&#34;基本原理-源码解析&#34;&gt;基本原理（源码解析）&lt;/h1&gt;

&lt;h2 id=&#34;文件目录组织&#34;&gt;文件目录组织&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;├── autodiscover        # 包含filebeat的autodiscover适配器（adapter），当autodiscover发现新容器时创建对应类型的输入
├── beater              # 包含与libbeat库交互相关的文件
├── channel             # 包含filebeat输出到pipeline相关的文件
├── config              # 包含filebeat配置结构和解析函数
├── crawler             # 包含Crawler结构和相关函数
├── fileset             # 包含module和fileset相关的结构
├── harvester           # 包含Harvester接口定义、Reader接口及实现等
├── input               # 包含所有输入类型的实现（比如: log, stdin, syslog）
├── inputsource         # 在syslog输入类型中用于读取tcp或udp syslog
├── module              # 包含各module和fileset配置
├── modules.d           # 包含各module对应的日志路径配置文件，用于修改默认路径
├── processor           # 用于从容器日志的事件字段source中提取容器id
├── prospector          # 包含旧版本的输入结构Prospector，现已被Input取代
├── registrar           # 包含Registrar结构和方法
└── util                # 包含beat事件和文件状态的通用结构Data
└── ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这些目录中还有一些重要的文件&lt;/p&gt;

&lt;p&gt;/beater：包含与libbeat库交互相关的文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acker.go: 包含在libbeat设置的ack回调函数，事件成功发送后被调用
channels.go: 包含在ack回调函数中被调用的记录者（logger），包括：
    registrarLogger: 将已确认事件写入registrar运行队列
    finishedLogger: 统计已确认事件数量
filebeat.go: 包含实现了beater接口的filebeat结构，接口函数包括：
    New：创建了filebeat实例
    Run：运行filebeat
    Stop: 停止filebeat运行
signalwait.go：基于channel实现的等待函数，在filebeat中用于：
    等待fileebat结束
    等待确认事件被写入registry文件
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/channel：filebeat输出（到pipeline）相关的文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;factory.go: 包含OutletFactory，用于创建输出器Outleter对象
interface.go: 定义输出接口Outleter
outlet.go: 实现Outleter，封装了libbeat的pipeline client，其在harvester中被调用用于将事件发送给pipeline
util.go: 定义ack回调的参数结构data，包含beat事件和文件状态
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/input：包含Input接口及各种输入类型的Input和Harvester实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input：对应配置中的一个Input项，同个Input下可包含多个输入源（比如文件）
Harvester：每个输入源对应一个Harvester，负责实际收集数据、并发送事件到pipeline
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/harvester：包含Harvester接口定义、Reader接口及实现等&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;forwarder.go: Forwarder结构（包含outlet）定义，用于转发事件
harvester.go: Harvester接口定义，具体实现则在/input目录下
registry.go: Registry结构，用于在Input中管理多个Harvester（输入源）的启动和停止
source.go: Source接口定义，表示输入源。目前仅有Pipe一种实现（包含os.File），用在log、stdin和docker输入类型中。btw，这三种输入类型都是用的log input的实现。
/reader目录: Reader接口定义和各种Reader实现
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;重要数据结构&#34;&gt;重要数据结构&lt;/h2&gt;

&lt;p&gt;beats通用事件结构(libbeat/beat/event.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Event struct {
    Timestamp time.Time     // 收集日志时记录的时间戳，对应es文档中的@timestamp字段
    Meta      common.MapStr // meta信息，outpus可选的将其作为事件字段输出。比如输出为es且指定了pipeline时，其pipeline id就被包含在此字段中
    Fields    common.MapStr // 默认输出字段定义在field.yml，其他字段可以在通过fields配置项指定
    Private   interface{} // for beats private use
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Crawler(filebeat/crawler/crawler.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Crawler 负责抓取日志并发送到libbeat pipeline
type Crawler struct {
    inputs          map[uint64]*input.Runner // 包含所有输入的runner
    inputConfigs    []*common.Config
    out             channel.Factory
    wg              sync.WaitGroup
    InputsFactory   cfgfile.RunnerFactory
    ModulesFactory  cfgfile.RunnerFactory
    modulesReloader *cfgfile.Reloader
    inputReloader   *cfgfile.Reloader
    once            bool
    beatVersion     string
    beatDone        chan struct{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;log类型Input(filebeat/input/log/input.go)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Input contains the input and its config
type Input struct {
    cfg           *common.Config
    config        config
    states        *file.States
    harvesters    *harvester.Registry   // 包含Input所有Harvester
    outlet        channel.Outleter      // Input共享的Publisher client
    stateOutlet   channel.Outleter
    done          chan struct{}
    numHarvesters atomic.Uint32
    meta          map[string]string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;log类型Harvester(filebeat/input/log/harvester.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Harvester struct {
    id     uuid.UUID
    config config
    source harvester.Source // the source being watched

    // shutdown handling
    done     chan struct{}
    stopOnce sync.Once
    stopWg   *sync.WaitGroup
    stopLock sync.Mutex

    // internal harvester state
    state  file.State
    states *file.States
    log    *Log

    // file reader pipeline
    reader          reader.Reader
    encodingFactory encoding.EncodingFactory
    encoding        encoding.Encoding

    // event/state publishing
    outletFactory OutletFactory
    publishState  func(*util.Data) bool

    onTerminate func()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Registrar(filebeat/registrar/registrar.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Registrar struct {
    Channel      chan []file.State
    out          successLogger
    done         chan struct{}
    registryFile string      // Path to the Registry File
    fileMode     os.FileMode // Permissions to apply on the Registry File
    wg           sync.WaitGroup

    states               *file.States // Map with all file paths inside and the corresponding state
    gcRequired           bool         // gcRequired is set if registry state needs to be gc&#39;ed before the next write
    gcEnabled            bool         // gcEnabled indictes the registry contains some state that can be gc&#39;ed in the future
    flushTimeout         time.Duration
    bufferedStateUpdates int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;libbeat Pipeline(libbeat/publisher/pipeline/pipeline.go)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Pipeline struct {
    beatInfo beat.Info

    logger *logp.Logger
    queue  queue.Queue
    output *outputController

    observer observer

    eventer pipelineEventer

    // wait close support
    waitCloseMode    WaitCloseMode
    waitCloseTimeout time.Duration
    waitCloser       *waitCloser

    // pipeline ack
    ackMode    pipelineACKMode
    ackActive  atomic.Bool
    ackDone    chan struct{}
    ackBuilder ackBuilder // pipelineEventsACK
    eventSema  *sema

    processors pipelineProcessors
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;启动&#34;&gt;启动&lt;/h2&gt;

&lt;p&gt;filebeat启动流程图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/f1.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每个 beat 的构建是独立的。从 filebeat 的入口文件filebeat/main.go可以看到，它向libbeat传递了名字、版本和构造函数来构造自身。跟着走到libbeat/beater/beater.go，我们可以看到程序的启动时的主要工作都是在这里完成的，包括命令行参数的处理、通用配置项的解析，以及最为重要的：调用象征一个beat的生命周期的若干方法&lt;/p&gt;

&lt;p&gt;我们来看filebeat的启动过程。&lt;/p&gt;

&lt;p&gt;1、执行root命令&lt;/p&gt;

&lt;p&gt;在filebeat/main.go文件中，main函数调用了cmd.RootCmd.Execute()，而RootCmd则是在cmd/root.go中被init函数初始化，其中就注册了filebeat.go:New函数以创建实现了beater接口的filebeat实例&lt;/p&gt;

&lt;p&gt;对于任意一个beats来说，都需要有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实现Beater接口的具体Beater（如Filebeat）;&lt;/li&gt;
&lt;li&gt;创建该具体Beater的(New)函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;beater接口定义（beat/beat.go）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Beater interface {
    // The main event loop. This method should block until signalled to stop by an
    // invocation of the Stop() method.
    Run(b *Beat) error

    // Stop is invoked to signal that the Run method should finish its execution.
    // It will be invoked at most once.
    Stop()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、初始化和运行Filebeat&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建libbeat/cmd/instance/beat.go:Beat结构&lt;/li&gt;
&lt;li&gt;执行(*Beat).launch方法

&lt;ul&gt;
&lt;li&gt;(*Beat).Init() 初始化Beat：加载beats公共config&lt;/li&gt;
&lt;li&gt;(*Beat).createBeater&lt;/li&gt;
&lt;li&gt;registerTemplateLoading: 当输出为es时，注册加载es模板的回调函数&lt;/li&gt;
&lt;li&gt;pipeline.Load: 创建Pipeline：包含队列、事件处理器、输出等&lt;/li&gt;
&lt;li&gt;setupMetrics: 安装监控&lt;/li&gt;
&lt;li&gt;filebeat.New: 解析配置(其中输入配置包括配置文件中的Input和module Input)等&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;loadDashboards 加载kibana dashboard&lt;/li&gt;
&lt;li&gt;(*Filebeat).Run: 运行filebeat&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、Filebeat运行&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;设置加载es pipeline的回调函数&lt;/li&gt;
&lt;li&gt;初始化registrar和crawler&lt;/li&gt;
&lt;li&gt;设置事件完成的回调函数&lt;/li&gt;
&lt;li&gt;启动Registrar、启动Crawler、启动Autodiscover&lt;/li&gt;
&lt;li&gt;等待filebeat运行结束&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们再重代码看一下这个启动过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;main.go

    package main
    import (
        &amp;quot;os&amp;quot;
        &amp;quot;github.com/elastic/beats/filebeat/cmd&amp;quot;
    )
    func main() {
        if err := cmd.RootCmd.Execute(); err != nil {
            os.Exit(1)
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入到filebeat/cmd执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package cmd

import (
    &amp;quot;flag&amp;quot;

    &amp;quot;github.com/spf13/pflag&amp;quot;

    &amp;quot;github.com/elastic/beats/filebeat/beater&amp;quot;

    cmd &amp;quot;github.com/elastic/beats/libbeat/cmd&amp;quot;
)

// Name of this beat
var Name = &amp;quot;filebeat&amp;quot;

// RootCmd to handle beats cli
var RootCmd *cmd.BeatsRootCmd

func init() {
    var runFlags = pflag.NewFlagSet(Name, pflag.ExitOnError)
    runFlags.AddGoFlag(flag.CommandLine.Lookup(&amp;quot;once&amp;quot;))
    runFlags.AddGoFlag(flag.CommandLine.Lookup(&amp;quot;modules&amp;quot;))

    RootCmd = cmd.GenRootCmdWithRunFlags(Name, &amp;quot;&amp;quot;, beater.New, runFlags)
    RootCmd.PersistentFlags().AddGoFlag(flag.CommandLine.Lookup(&amp;quot;M&amp;quot;))
    RootCmd.TestCmd.Flags().AddGoFlag(flag.CommandLine.Lookup(&amp;quot;modules&amp;quot;))
    RootCmd.SetupCmd.Flags().AddGoFlag(flag.CommandLine.Lookup(&amp;quot;modules&amp;quot;))
    RootCmd.AddCommand(cmd.GenModulesCmd(Name, &amp;quot;&amp;quot;, buildModulesManager))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RootCmd 在这一句初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RootCmd = cmd.GenRootCmdWithRunFlags(Name, &amp;quot;&amp;quot;, beater.New, runFlags)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;beater.New跟进去看到是filebeat.go，这个函数会在后面进行调用，来创建filebeat结构体，传递filebeat相关的配置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func New(b beat.Beat, rawConfig common.Config) (beat.Beater, error) {...}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在进入GenRootCmdWithRunFlags方法，一路跟进去到GenRootCmdWithSettings，真正的初始化是在这个方法里面。&lt;/p&gt;

&lt;p&gt;忽略前面的一段初始化值方法，看到RunCmd的初始化在：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rootCmd.RunCmd = genRunCmd(settings, beatCreator, runFlags)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入getRunCmd，看到执行代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err := instance.Run(settings, beatCreator)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;跟到\elastic\beats\libbeat\cmd\instance\beat.go的Run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b, err := NewBeat(name, idxPrefix, version)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里新建了beat结构体，同时将filebeat的New方法也传递了进来，就是参数beatCreator，我们可以看到在beat通过launch函数创建了filebeat结构体类型的beater&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return b.launch(settings, bt)---&amp;gt;beater, err := b.createBeater(bt)---&amp;gt;beater, err := bt(&amp;amp;b.Beat, sub)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入launch后，还做了很多的事情&lt;/p&gt;

&lt;p&gt;1、还初始化了配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err := b.InitWithSettings(settings)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、&lt;a href=&#34;#pipeline初始化&#34;&gt;pipeline的初始化&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pipeline, err := pipeline.Load(b.Info,
        pipeline.Monitors{
            Metrics:   reg,
            Telemetry: monitoring.GetNamespace(&amp;quot;state&amp;quot;).GetRegistry(),
            Logger:    logp.L().Named(&amp;quot;publisher&amp;quot;),
        },
        b.Config.Pipeline,
        b.processing,
        b.makeOutputFactory(b.Config.Output),
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在launch的末尾，还调用了beater启动方法，也就是filebeat的run函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return beater.Run(&amp;amp;b.Beat)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为启动的是filebeat，我们到filebeat.go的Run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (fb *Filebeat) Run(b *beat.Beat) error {
       var err error
       config := fb.config

       if !fb.moduleRegistry.Empty() {
              err = fb.loadModulesPipelines(b)
              if err != nil {
                     return err
              }
       }

       waitFinished := newSignalWait()
       waitEvents := newSignalWait()

       // count active events for waiting on shutdown
       wgEvents := &amp;amp;eventCounter{
              count: monitoring.NewInt(nil, &amp;quot;filebeat.events.active&amp;quot;),
              added: monitoring.NewUint(nil, &amp;quot;filebeat.events.added&amp;quot;),
              done:  monitoring.NewUint(nil, &amp;quot;filebeat.events.done&amp;quot;),
       }
       finishedLogger := newFinishedLogger(wgEvents)

       // Setup registrar to persist state
       registrar, err := registrar.New(config.RegistryFile, config.RegistryFilePermissions, config.RegistryFlush, finishedLogger)
       if err != nil {
              logp.Err(&amp;quot;Could not init registrar: %v&amp;quot;, err)
              return err
       }

       // Make sure all events that were published in
       registrarChannel := newRegistrarLogger(registrar)

       err = b.Publisher.SetACKHandler(beat.PipelineACKHandler{
              ACKEvents: newEventACKer(finishedLogger, registrarChannel).ackEvents,
       })
       if err != nil {
              logp.Err(&amp;quot;Failed to install the registry with the publisher pipeline: %v&amp;quot;, err)
              return err
       }

       outDone := make(chan struct{}) // outDone closes down all active pipeline connections
       crawler, err := crawler.New(
              channel.NewOutletFactory(outDone, wgEvents).Create,
              config.Inputs,
              b.Info.Version,
              fb.done,
              *once)
       if err != nil {
              logp.Err(&amp;quot;Could not init crawler: %v&amp;quot;, err)
              return err
       }

       // The order of starting and stopping is important. Stopping is inverted to the starting order.
       // The current order is: registrar, publisher, spooler, crawler
       // That means, crawler is stopped first.

       // Start the registrar
       err = registrar.Start()
       if err != nil {
              return fmt.Errorf(&amp;quot;Could not start registrar: %v&amp;quot;, err)
       }

       // Stopping registrar will write last state
       defer registrar.Stop()

       // Stopping publisher (might potentially drop items)
       defer func() {
              // Closes first the registrar logger to make sure not more events arrive at the registrar
              // registrarChannel must be closed first to potentially unblock (pretty unlikely) the publisher
              registrarChannel.Close()
              close(outDone) // finally close all active connections to publisher pipeline
       }()

       // Wait for all events to be processed or timeout
       defer waitEvents.Wait()

       // Create a ES connection factory for dynamic modules pipeline loading
       var pipelineLoaderFactory fileset.PipelineLoaderFactory
       if b.Config.Output.Name() == &amp;quot;elasticsearch&amp;quot; {
              pipelineLoaderFactory = newPipelineLoaderFactory(b.Config.Output.Config())
       } else {
              logp.Warn(pipelinesWarning)
       }

       if config.OverwritePipelines {
              logp.Debug(&amp;quot;modules&amp;quot;, &amp;quot;Existing Ingest pipelines will be updated&amp;quot;)
       }

       err = crawler.Start(b.Publisher, registrar, config.ConfigInput, config.ConfigModules, pipelineLoaderFactory, config.OverwritePipelines)
       if err != nil {
              crawler.Stop()
              return err
       }

       // If run once, add crawler completion check as alternative to done signal
       if *once {
              runOnce := func() {
                     logp.Info(&amp;quot;Running filebeat once. Waiting for completion ...&amp;quot;)
                     crawler.WaitForCompletion()
                     logp.Info(&amp;quot;All data collection completed. Shutting down.&amp;quot;)
              }
              waitFinished.Add(runOnce)
       }

       // Register reloadable list of inputs and modules
       inputs := cfgfile.NewRunnerList(management.DebugK, crawler.InputsFactory, b.Publisher)
       reload.Register.MustRegisterList(&amp;quot;filebeat.inputs&amp;quot;, inputs)

       modules := cfgfile.NewRunnerList(management.DebugK, crawler.ModulesFactory, b.Publisher)
       reload.Register.MustRegisterList(&amp;quot;filebeat.modules&amp;quot;, modules)

       var adiscover *autodiscover.Autodiscover
       if fb.config.Autodiscover != nil {
              adapter := fbautodiscover.NewAutodiscoverAdapter(crawler.InputsFactory, crawler.ModulesFactory)
              adiscover, err = autodiscover.NewAutodiscover(&amp;quot;filebeat&amp;quot;, b.Publisher, adapter, config.Autodiscover)
              if err != nil {
                     return err
              }
       }
       adiscover.Start()

       // Add done channel to wait for shutdown signal
       waitFinished.AddChan(fb.done)
       waitFinished.Wait()

       // Stop reloadable lists, autodiscover -&amp;gt; Stop crawler -&amp;gt; stop inputs -&amp;gt; stop harvesters
       // Note: waiting for crawlers to stop here in order to install wgEvents.Wait
       //       after all events have been enqueued for publishing. Otherwise wgEvents.Wait
       //       or publisher might panic due to concurrent updates.
       inputs.Stop()
       modules.Stop()
       adiscover.Stop()
       crawler.Stop()

       timeout := fb.config.ShutdownTimeout
       // Checks if on shutdown it should wait for all events to be published
       waitPublished := fb.config.ShutdownTimeout &amp;gt; 0 || *once
       if waitPublished {
              // Wait for registrar to finish writing registry
              waitEvents.Add(withLog(wgEvents.Wait,
                     &amp;quot;Continue shutdown: All enqueued events being published.&amp;quot;))
              // Wait for either timeout or all events having been ACKed by outputs.
              if fb.config.ShutdownTimeout &amp;gt; 0 {
                     logp.Info(&amp;quot;Shutdown output timer started. Waiting for max %v.&amp;quot;, timeout)
                     waitEvents.Add(withLog(waitDuration(timeout),
                            &amp;quot;Continue shutdown: Time out waiting for events being published.&amp;quot;))
              } else {
                     waitEvents.AddChan(fb.done)
              }
       }

       return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构造了&lt;a href=&#34;#registry和ack-机制&#34;&gt;registrar&lt;/a&gt;和crawler，用于监控文件状态变更和数据采集。然后&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err = crawler.Start(b.Publisher, registrar, config.ConfigInput, config.ConfigModules, pipelineLoaderFactory, config.OverwritePipelines)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;crawler开始启动采集数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for _, inputConfig := range c.inputConfigs {
       err := c.startInput(pipeline, inputConfig, r.GetStates())
       if err != nil {
              return err
       }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;crawler的Start方法里面根据每个配置的输入调用一次startInput&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Crawler) startInput(
       pipeline beat.Pipeline,
       config *common.Config,
       states []file.State,
) error {
       if !config.Enabled() {
              return nil
       }

       connector := channel.ConnectTo(pipeline, c.out)
       p, err := input.New(config, connector, c.beatDone, states, nil)
       if err != nil {
              return fmt.Errorf(&amp;quot;Error in initing input: %s&amp;quot;, err)
       }
       p.Once = c.once

       if _, ok := c.inputs[p.ID]; ok {
              return fmt.Errorf(&amp;quot;Input with same ID already exists: %d&amp;quot;, p.ID)
       }

       c.inputs[p.ID] = p

       p.Start()

       return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据配置的input，构造log/input&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Input) Run() {
       logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Start next scan&amp;quot;)

       // TailFiles is like ignore_older = 1ns and only on startup
       if p.config.TailFiles {
              ignoreOlder := p.config.IgnoreOlder

              // Overwrite ignore_older for the first scan
              p.config.IgnoreOlder = 1
              defer func() {
                     // Reset ignore_older after first run
                     p.config.IgnoreOlder = ignoreOlder
                     // Disable tail_files after the first run
                     p.config.TailFiles = false
              }()
       }
       p.scan()

       // It is important that a first scan is run before cleanup to make sure all new states are read first
       if p.config.CleanInactive &amp;gt; 0 || p.config.CleanRemoved {
              beforeCount := p.states.Count()
              cleanedStates, pendingClean := p.states.Cleanup()
              logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;input states cleaned up. Before: %d, After: %d, Pending: %d&amp;quot;,
                     beforeCount, beforeCount-cleanedStates, pendingClean)
       }

       // Marking removed files to be cleaned up. Cleanup happens after next scan to make sure all states are updated first
       if p.config.CleanRemoved {
              for _, state := range p.states.GetStates() {
                     // os.Stat will return an error in case the file does not exist
                     stat, err := os.Stat(state.Source)
                     if err != nil {
                            if os.IsNotExist(err) {
                                   p.removeState(state)
                                   logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Remove state for file as file removed: %s&amp;quot;, state.Source)
                            } else {
                                   logp.Err(&amp;quot;input state for %s was not removed: %s&amp;quot;, state.Source, err)
                            }
                     } else {
                            // Check if existing source on disk and state are the same. Remove if not the case.
                            newState := file.NewState(stat, state.Source, p.config.Type, p.meta)
                            if !newState.FileStateOS.IsSame(state.FileStateOS) {
                                   p.removeState(state)
                                   logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Remove state for file as file removed or renamed: %s&amp;quot;, state.Source)
                            }
                     }
              }
       }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;input开始根据配置的输入路径扫描所有符合的文件，并启动harvester&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Input) scan() {
       var sortInfos []FileSortInfo
       var files []string

       paths := p.getFiles()

       var err error

       if p.config.ScanSort != &amp;quot;&amp;quot; {
              sortInfos, err = getSortedFiles(p.config.ScanOrder, p.config.ScanSort, getSortInfos(paths))
              if err != nil {
                     logp.Err(&amp;quot;Failed to sort files during scan due to error %s&amp;quot;, err)
              }
       }

       if sortInfos == nil {
              files = getKeys(paths)
       }

       for i := 0; i &amp;lt; len(paths); i++ {

              var path string
              var info os.FileInfo

              if sortInfos == nil {
                     path = files[i]
                     info = paths[path]
              } else {
                     path = sortInfos[i].path
                     info = sortInfos[i].info
              }

              select {
              case &amp;lt;-p.done:
                     logp.Info(&amp;quot;Scan aborted because input stopped.&amp;quot;)
                     return
              default:
              }

              newState, err := getFileState(path, info, p)
              if err != nil {
                     logp.Err(&amp;quot;Skipping file %s due to error %s&amp;quot;, path, err)
              }

              // Load last state
              lastState := p.states.FindPrevious(newState)

              // Ignores all files which fall under ignore_older
              if p.isIgnoreOlder(newState) {
                     err := p.handleIgnoreOlder(lastState, newState)
                     if err != nil {
                            logp.Err(&amp;quot;Updating ignore_older state error: %s&amp;quot;, err)
                     }
                     continue
              }

              // Decides if previous state exists
              if lastState.IsEmpty() {
                     logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Start harvester for new file: %s&amp;quot;, newState.Source)
                     err := p.startHarvester(newState, 0)
                     if err == errHarvesterLimit {
                            logp.Debug(&amp;quot;input&amp;quot;, harvesterErrMsg, newState.Source, err)
                            continue
                     }
                     if err != nil {
                            logp.Err(harvesterErrMsg, newState.Source, err)
                     }
              } else {
                     p.harvestExistingFile(newState, lastState)
              }
       }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在harvest的Run看到一个死循环读取message，预处理之后交由forwarder发送到目标输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;message, err := h.reader.Next()
h.sendEvent(data, forwarder)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，整个filebeat的启动到发送数据就理完了&lt;/p&gt;

&lt;h2 id=&#34;配置文件解析&#34;&gt;配置文件解析&lt;/h2&gt;

&lt;p&gt;在libbeat中实现了通用的配置文件解析，在启动的过程中，在每次createbeater时候就会进行config。&lt;/p&gt;

&lt;p&gt;调用 cfgfile.Load方法解析到cfg对象，进入load方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Load(path string, beatOverrides *common.Config) (*common.Config, error) {
       var config *common.Config
       var err error

       cfgpath := GetPathConfig()

       if path == &amp;quot;&amp;quot; {
              list := []string{}
              for _, cfg := range configfiles.List() {
                     if !filepath.IsAbs(cfg) {
                            list = append(list, filepath.Join(cfgpath, cfg))
                     } else {
                            list = append(list, cfg)
                     }
              }
              config, err = common.LoadFiles(list...)
       } else {
              if !filepath.IsAbs(path) {
                     path = filepath.Join(cfgpath, path)
              }
              config, err = common.LoadFile(path)
       }
       if err != nil {
              return nil, err
       }

       if beatOverrides != nil {
              config, err = common.MergeConfigs(
                     defaults,
                     beatOverrides,
                     config,
                     overwrites,
              )
              if err != nil {
                     return nil, err
              }
       } else {
              config, err = common.MergeConfigs(
                     defaults,
                     config,
                     overwrites,
              )
       }

       config.PrintDebugf(&amp;quot;Complete configuration loaded:&amp;quot;)
       return config, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果不输入配置文件，使用configfiles定义文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;configfiles = common.StringArrFlag(nil, &amp;quot;c&amp;quot;, &amp;quot;beat.yml&amp;quot;, &amp;quot;Configuration file, relative to path.config&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果输入配置文件进入else分支&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config, err = common.LoadFile(path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据配置文件构造config对象，使用的是yaml解析库。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c, err := yaml.NewConfigWithFile(path, configOpts...)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pipeline初始化&#34;&gt;pipeline初始化&lt;/h2&gt;

&lt;p&gt;pipeline的初始化是在libbeat的创建对于的filebeat 的结构体的时候进行的在func (b *Beat) createBeater(bt beat.Creator) (beat.Beater, error) {}函数中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pipeline, err := pipeline.Load(b.Info,
    pipeline.Monitors{
        Metrics:   reg,
        Telemetry: monitoring.GetNamespace(&amp;quot;state&amp;quot;).GetRegistry(),
        Logger:    logp.L().Named(&amp;quot;publisher&amp;quot;),
    },
    b.Config.Pipeline,
    b.processing,
    b.makeOutputFactory(b.Config.Output),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看load函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load uses a Config object to create a new complete Pipeline instance with
// configured queue and outputs.
func Load(
    beatInfo beat.Info,
    monitors Monitors,
    config Config,
    processors processing.Supporter,
    makeOutput func(outputs.Observer) (string, outputs.Group, error),
) (*Pipeline, error) {
    log := monitors.Logger
    if log == nil {
        log = logp.L()
    }

    if publishDisabled {
        log.Info(&amp;quot;Dry run mode. All output types except the file based one are disabled.&amp;quot;)
    }

    name := beatInfo.Name
    settings := Settings{
        WaitClose:     0,
        WaitCloseMode: NoWaitOnClose,
        Processors:    processors,
    }

    queueBuilder, err := createQueueBuilder(config.Queue, monitors)
    if err != nil {
        return nil, err
    }

    out, err := loadOutput(monitors, makeOutput)
    if err != nil {
        return nil, err
    }

    p, err := New(beatInfo, monitors, queueBuilder, out, settings)
    if err != nil {
        return nil, err
    }

    log.Infof(&amp;quot;Beat name: %s&amp;quot;, name)
    return p, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要是初始化queue，output，并创建对应的pipeline。&lt;/p&gt;

&lt;p&gt;1、queue&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;queueBuilder, err := createQueueBuilder(config.Queue, monitors)
    if err != nil {
        return nil, err
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入createQueueBuilder&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func createQueueBuilder(
    config common.ConfigNamespace,
    monitors Monitors,
) (func(queue.Eventer) (queue.Queue, error), error) {
    queueType := defaultQueueType
    if b := config.Name(); b != &amp;quot;&amp;quot; {
        queueType = b
    }

    queueFactory := queue.FindFactory(queueType)
    if queueFactory == nil {
        return nil, fmt.Errorf(&amp;quot;&#39;%v&#39; is no valid queue type&amp;quot;, queueType)
    }

    queueConfig := config.Config()
    if queueConfig == nil {
        queueConfig = common.NewConfig()
    }

    if monitors.Telemetry != nil {
        queueReg := monitors.Telemetry.NewRegistry(&amp;quot;queue&amp;quot;)
        monitoring.NewString(queueReg, &amp;quot;name&amp;quot;).Set(queueType)
    }

    return func(eventer queue.Eventer) (queue.Queue, error) {
        return queueFactory(eventer, monitors.Logger, queueConfig)
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据queueType（有默认类型mem）找到创建的方法，一般mem就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    queue.RegisterType(&amp;quot;mem&amp;quot;, create)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下create函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func create(eventer queue.Eventer, logger *logp.Logger, cfg *common.Config) (queue.Queue, error) {
    config := defaultConfig
    if err := cfg.Unpack(&amp;amp;config); err != nil {
        return nil, err
    }

    if logger == nil {
        logger = logp.L()
    }

    return NewBroker(logger, Settings{
        Eventer:        eventer,
        Events:         config.Events,
        FlushMinEvents: config.FlushMinEvents,
        FlushTimeout:   config.FlushTimeout,
    }), nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建了一个broker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewBroker creates a new broker based in-memory queue holding up to sz number of events.
// If waitOnClose is set to true, the broker will block on Close, until all internal
// workers handling incoming messages and ACKs have been shut down.
func NewBroker(
    logger logger,
    settings Settings,
) *Broker {
    // define internal channel size for producer/client requests
    // to the broker
    chanSize := 20

    var (
        sz           = settings.Events
        minEvents    = settings.FlushMinEvents
        flushTimeout = settings.FlushTimeout
    )

    if minEvents &amp;lt; 1 {
        minEvents = 1
    }
    if minEvents &amp;gt; 1 &amp;amp;&amp;amp; flushTimeout &amp;lt;= 0 {
        minEvents = 1
        flushTimeout = 0
    }
    if minEvents &amp;gt; sz {
        minEvents = sz
    }

    if logger == nil {
        logger = logp.NewLogger(&amp;quot;memqueue&amp;quot;)
    }

    b := &amp;amp;Broker{
        done:   make(chan struct{}),
        logger: logger,

        // broker API channels
        events:    make(chan pushRequest, chanSize),
        requests:  make(chan getRequest),
        pubCancel: make(chan producerCancelRequest, 5),

        // internal broker and ACK handler channels
        acks:          make(chan int),
        scheduledACKs: make(chan chanList),

        waitOnClose: settings.WaitOnClose,

        eventer: settings.Eventer,
    }

    var eventLoop interface {
        run()
        processACK(chanList, int)
    }

    if minEvents &amp;gt; 1 {
        eventLoop = newBufferingEventLoop(b, sz, minEvents, flushTimeout)
    } else {
        eventLoop = newDirectEventLoop(b, sz)
    }

    b.bufSize = sz
    ack := newACKLoop(b, eventLoop.processACK)

    b.wg.Add(2)
    go func() {
        defer b.wg.Done()
        eventLoop.run()
    }()
    go func() {
        defer b.wg.Done()
        ack.run()
    }()

    return b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;broker就是我们的queue，同时创建了一个eventLoop（根据是否有缓存创建不同的结构体，根据配置min_event是否大于1创建BufferingEventLoop或者DirectEventLoop，一般默认都是BufferingEventLoop，即带缓冲的队列。）和ack，调用他们的run函数进行监听&lt;/p&gt;

&lt;p&gt;这边特别说明一下eventLoop的new，我们看带缓存的newBufferingEventLoop&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBufferingEventLoop(b *Broker, size int, minEvents int, flushTimeout time.Duration) *bufferingEventLoop {
    l := &amp;amp;bufferingEventLoop{
        broker:       b,
        maxEvents:    size,
        minEvents:    minEvents,
        flushTimeout: flushTimeout,

        events:    b.events,
        get:       nil,
        pubCancel: b.pubCancel,
        acks:      b.acks,
    }
    l.buf = newBatchBuffer(l.minEvents)

    l.timer = time.NewTimer(flushTimeout)
    if !l.timer.Stop() {
        &amp;lt;-l.timer.C
    }

    return l
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把broker的值很多都赋给了bufferingEventLoop，不知道为什么这么做。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go func() {
    defer b.wg.Done()
    eventLoop.run()
}()
go func() {
    defer b.wg.Done()
    ack.run()
}()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下有缓存的事件处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) run() {
    var (
        broker = l.broker
    )

    for {
        select {
        case &amp;lt;-broker.done:
            return

        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)

        case req := &amp;lt;-l.pubCancel: // producer cancelling active events
            l.handleCancel(&amp;amp;req)

        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)

        case l.schedACKS &amp;lt;- l.pendingACKs:
            l.schedACKS = nil
            l.pendingACKs = chanList{}

        case count := &amp;lt;-l.acks:
            l.handleACK(count)

        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里就可以监听队列中的事件了，BufferingEventLoop是一个实现了Broker、带有各种channel的结构，主要用于将日志发送至consumer消费。 BufferingEventLoop的run方法中，同样是一个无限循环，这里可以认为是一个日志事件的调度中心。&lt;/p&gt;

&lt;p&gt;再来看看ack的调度&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *ackLoop) run() {
    var (
        // log = l.broker.logger

        // Buffer up acked event counter in acked. If acked &amp;gt; 0, acks will be set to
        // the broker.acks channel for sending the ACKs while potentially receiving
        // new batches from the broker event loop.
        // This concurrent bidirectionally communication pattern requiring &#39;select&#39;
        // ensures we can not have any deadlock between the event loop and the ack
        // loop, as the ack loop will not block on any channel
        acked int
        acks  chan int
    )

    for {
        select {
        case &amp;lt;-l.broker.done:
            // TODO: handle pending ACKs?
            // TODO: panic on pending batches?
            return

        case acks &amp;lt;- acked:
            acks, acked = nil, 0

        case lst := &amp;lt;-l.broker.scheduledACKs:
            count, events := lst.count()
            l.lst.concat(&amp;amp;lst)

            // log.Debug(&amp;quot;ACK List:&amp;quot;)
            // for current := l.lst.head; current != nil; current = current.next {
            //  log.Debugf(&amp;quot;  ack entry(seq=%v, start=%v, count=%v&amp;quot;,
            //      current.seq, current.start, current.count)
            // }

            l.batchesSched += uint64(count)
            l.totalSched += uint64(events)

        case &amp;lt;-l.sig:
            acked += l.handleBatchSig()
            if acked &amp;gt; 0 {
                acks = l.broker.acks
            }
        }

        // log.Debug(&amp;quot;ackloop INFO&amp;quot;)
        // log.Debug(&amp;quot;ackloop:   total events scheduled = &amp;quot;, l.totalSched)
        // log.Debug(&amp;quot;ackloop:   total events ack = &amp;quot;, l.totalACK)
        // log.Debug(&amp;quot;ackloop:   total batches scheduled = &amp;quot;, l.batchesSched)
        // log.Debug(&amp;quot;ackloop:   total batches ack = &amp;quot;, l.batchesACKed)

        l.sig = l.lst.channel()
        // if l.sig == nil {
        //  log.Debug(&amp;quot;ackloop: no ack scheduled&amp;quot;)
        // } else {
        //  log.Debug(&amp;quot;ackloop: schedule ack: &amp;quot;, l.lst.head.seq)
        // }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有处理信号就发送给regestry进行记录，关于&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/#registry和ack-机制&#34;&gt;registry在下面详细说明&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;2、output&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;out, err := loadOutput(monitors, makeOutput)
if err != nil {
    return nil, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入loadOutput&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func loadOutput(
    monitors Monitors,
    makeOutput OutputFactory,
) (outputs.Group, error) {
    log := monitors.Logger
    if log == nil {
        log = logp.L()
    }

    if publishDisabled {
        return outputs.Group{}, nil
    }

    if makeOutput == nil {
        return outputs.Group{}, nil
    }

    var (
        metrics  *monitoring.Registry
        outStats outputs.Observer
    )
    if monitors.Metrics != nil {
        metrics = monitors.Metrics.GetRegistry(&amp;quot;output&amp;quot;)
        if metrics != nil {
            metrics.Clear()
        } else {
            metrics = monitors.Metrics.NewRegistry(&amp;quot;output&amp;quot;)
        }
        outStats = outputs.NewStats(metrics)
    }

    outName, out, err := makeOutput(outStats)
    if err != nil {
        return outputs.Fail(err)
    }

    if metrics != nil {
        monitoring.NewString(metrics, &amp;quot;type&amp;quot;).Set(outName)
    }
    if monitors.Telemetry != nil {
        telemetry := monitors.Telemetry.GetRegistry(&amp;quot;output&amp;quot;)
        if telemetry != nil {
            telemetry.Clear()
        } else {
            telemetry = monitors.Telemetry.NewRegistry(&amp;quot;output&amp;quot;)
        }
        monitoring.NewString(telemetry, &amp;quot;name&amp;quot;).Set(outName)
    }

    return out, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边是根据load传进来的makeOutput函数来进行创建的，我们看一下load这个参数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Beat) makeOutputFactory(
    cfg common.ConfigNamespace,
) func(outputs.Observer) (string, outputs.Group, error) {
    return func(outStats outputs.Observer) (string, outputs.Group, error) {
        out, err := b.createOutput(outStats, cfg)
        return cfg.Name(), out, err
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建createOutput&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Beat) createOutput(stats outputs.Observer, cfg common.ConfigNamespace) (outputs.Group, error) {
    if !cfg.IsSet() {
        return outputs.Group{}, nil
    }

    return outputs.Load(b.IdxSupporter, b.Info, stats, cfg.Name(), cfg.Config())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看load&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load creates and configures a output Group using a configuration object..
func Load(
    im IndexManager,
    info beat.Info,
    stats Observer,
    name string,
    config *common.Config,
) (Group, error) {
    factory := FindFactory(name)
    if factory == nil {
        return Group{}, fmt.Errorf(&amp;quot;output type %v undefined&amp;quot;, name)
    }

    if stats == nil {
        stats = NewNilObserver()
    }
    return factory(im, info, stats, config)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见根据配置文件的配置的output的类型进行创建，比如我们用kafka做为output，我们看一下创建函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    sarama.Logger = kafkaLogger{}

    outputs.RegisterType(&amp;quot;kafka&amp;quot;, makeKafka)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是makeKafka&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeKafka(
    _ outputs.IndexManager,
    beat beat.Info,
    observer outputs.Observer,
    cfg *common.Config,
) (outputs.Group, error) {
    debugf(&amp;quot;initialize kafka output&amp;quot;)

    config, err := readConfig(cfg)
    if err != nil {
        return outputs.Fail(err)
    }

    topic, err := outil.BuildSelectorFromConfig(cfg, outil.Settings{
        Key:              &amp;quot;topic&amp;quot;,
        MultiKey:         &amp;quot;topics&amp;quot;,
        EnableSingleOnly: true,
        FailEmpty:        true,
    })
    if err != nil {
        return outputs.Fail(err)
    }

    libCfg, err := newSaramaConfig(config)
    if err != nil {
        return outputs.Fail(err)
    }

    hosts, err := outputs.ReadHostList(cfg)
    if err != nil {
        return outputs.Fail(err)
    }

    codec, err := codec.CreateEncoder(beat, config.Codec)
    if err != nil {
        return outputs.Fail(err)
    }

    client, err := newKafkaClient(observer, hosts, beat.IndexPrefix, config.Key, topic, codec, libCfg)
    if err != nil {
        return outputs.Fail(err)
    }

    retry := 0
    if config.MaxRetries &amp;lt; 0 {
        retry = -1
    }
    return outputs.Success(config.BulkMaxSize, retry, client)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接创建了kafka的client给发送的时候使用。&lt;/p&gt;

&lt;p&gt;最后利用上面的两个构建函数来创建我们的pipeline&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p, err := New(beatInfo, monitors, queueBuilder, out, settings)
if err != nil {
    return nil, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实上面函数有的调用是在这个new中进行的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New create a new Pipeline instance from a queue instance and a set of outputs.
// The new pipeline will take ownership of queue and outputs. On Close, the
// queue and outputs will be closed.
func New(
    beat beat.Info,
    monitors Monitors,
    queueFactory queueFactory,
    out outputs.Group,
    settings Settings,
) (*Pipeline, error) {
    var err error

    if monitors.Logger == nil {
        monitors.Logger = logp.NewLogger(&amp;quot;publish&amp;quot;)
    }

    p := &amp;amp;Pipeline{
        beatInfo:         beat,
        monitors:         monitors,
        observer:         nilObserver,
        waitCloseMode:    settings.WaitCloseMode,
        waitCloseTimeout: settings.WaitClose,
        processors:       settings.Processors,
    }
    p.ackBuilder = &amp;amp;pipelineEmptyACK{p}
    p.ackActive = atomic.MakeBool(true)

    if monitors.Metrics != nil {
        p.observer = newMetricsObserver(monitors.Metrics)
    }
    p.eventer.observer = p.observer
    p.eventer.modifyable = true

    if settings.WaitCloseMode == WaitOnPipelineClose &amp;amp;&amp;amp; settings.WaitClose &amp;gt; 0 {
        p.waitCloser = &amp;amp;waitCloser{}

        // waitCloser decrements counter on queue ACK (not per client)
        p.eventer.waitClose = p.waitCloser
    }

    p.queue, err = queueFactory(&amp;amp;p.eventer)
    if err != nil {
        return nil, err
    }

    if count := p.queue.BufferConfig().Events; count &amp;gt; 0 {
        p.eventSema = newSema(count)
    }

    maxEvents := p.queue.BufferConfig().Events
    if maxEvents &amp;lt;= 0 {
        // Maximum number of events until acker starts blocking.
        // Only active if pipeline can drop events.
        maxEvents = 64000
    }
    p.eventSema = newSema(maxEvents)

    p.output = newOutputController(beat, monitors, p.observer, p.queue)
    p.output.Set(out)

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个output的控制器outputController&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newOutputController(
    beat beat.Info,
    monitors Monitors,
    observer outputObserver,
    b queue.Queue,
) *outputController {
    c := &amp;amp;outputController{
        beat:     beat,
        monitors: monitors,
        observer: observer,
        queue:    b,
    }

    ctx := &amp;amp;batchContext{}
    c.consumer = newEventConsumer(monitors.Logger, b, ctx)
    c.retryer = newRetryer(monitors.Logger, observer, nil, c.consumer)
    ctx.observer = observer
    ctx.retryer = c.retryer

    c.consumer.sigContinue()

    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时初始化了eventConsumer和retryer。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventConsumer(
    log *logp.Logger,
    queue queue.Queue,
    ctx *batchContext,
) *eventConsumer {
    c := &amp;amp;eventConsumer{
        logger: log,
        done:   make(chan struct{}),
        sig:    make(chan consumerSignal, 3),
        out:    nil,

        queue:    queue,
        consumer: queue.Consumer(),
        ctx:      ctx,
    }

    c.pause.Store(true)
    go c.loop(c.consumer)
    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在eventConsumer中启动了一个监听程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *eventConsumer) loop(consumer queue.Consumer) {
    log := c.logger

    log.Debug(&amp;quot;start pipeline event consumer&amp;quot;)

    var (
        out    workQueue
        batch  *Batch
        paused = true
    )

    handleSignal := func(sig consumerSignal) {
        switch sig.tag {
        case sigConsumerCheck:

        case sigConsumerUpdateOutput:
            c.out = sig.out

        case sigConsumerUpdateInput:
            consumer = sig.consumer
        }

        paused = c.paused()
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; batch != nil {
            out = c.out.workQueue
        } else {
            out = nil
        }
    }

    for {
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; consumer != nil &amp;amp;&amp;amp; batch == nil {
            out = c.out.workQueue
            queueBatch, err := consumer.Get(c.out.batchSize)
            if err != nil {
                out = nil
                consumer = nil
                continue
            }
            if queueBatch != nil {
                batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
            }

            paused = c.paused()
            if paused || batch == nil {
                out = nil
            }
        }

        select {
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
            continue
        default:
        }

        select {
        case &amp;lt;-c.done:
            log.Debug(&amp;quot;stop pipeline event consumer&amp;quot;)
            return
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
        case out &amp;lt;- batch:
            batch = nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用于消费队列中的事件event，并将其构建成Batch，放到处理队列中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBatch(ctx *batchContext, original queue.Batch, ttl int) *Batch {
    if original == nil {
        panic(&amp;quot;empty batch&amp;quot;)
    }

    b := batchPool.Get().(*Batch)
    *b = Batch{
        original: original,
        ctx:      ctx,
        ttl:      ttl,
        events:   original.Events(),
    }
    return b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看retryer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newRetryer(
    log *logp.Logger,
    observer outputObserver,
    out workQueue,
    c *eventConsumer,
) *retryer {
    r := &amp;amp;retryer{
        logger:     log,
        observer:   observer,
        done:       make(chan struct{}),
        sig:        make(chan retryerSignal, 3),
        in:         retryQueue(make(chan batchEvent, 3)),
        out:        out,
        consumer:   c,
        doneWaiter: sync.WaitGroup{},
    }
    r.doneWaiter.Add(1)
    go r.loop()
    return r
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样启动了监听程序，用于重试。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *retryer) loop() {
    defer r.doneWaiter.Done()
    var (
        out             workQueue
        consumerBlocked bool

        active     *Batch
        activeSize int
        buffer     []*Batch
        numOutputs int

        log = r.logger
    )

    for {
        select {
        case &amp;lt;-r.done:
            return
        case evt := &amp;lt;-r.in:
            var (
                countFailed  int
                countDropped int
                batch        = evt.batch
                countRetry   = len(batch.events)
            )

            if evt.tag == retryBatch {
                countFailed = len(batch.events)
                r.observer.eventsFailed(countFailed)

                decBatch(batch)

                countRetry = len(batch.events)
                countDropped = countFailed - countRetry
                r.observer.eventsDropped(countDropped)
            }

            if len(batch.events) == 0 {
                log.Info(&amp;quot;Drop batch&amp;quot;)
                batch.Drop()
            } else {
                out = r.out
                buffer = append(buffer, batch)
                out = r.out
                active = buffer[0]
                activeSize = len(active.events)
                if !consumerBlocked {
                    consumerBlocked = blockConsumer(numOutputs, len(buffer))
                    if consumerBlocked {
                        log.Info(&amp;quot;retryer: send wait signal to consumer&amp;quot;)
                        r.consumer.sigWait()
                        log.Info(&amp;quot;  done&amp;quot;)
                    }
                }
            }

        case out &amp;lt;- active:
            r.observer.eventsRetry(activeSize)

            buffer = buffer[1:]
            active, activeSize = nil, 0

            if len(buffer) == 0 {
                out = nil
            } else {
                active = buffer[0]
                activeSize = len(active.events)
            }

            if consumerBlocked {
                consumerBlocked = blockConsumer(numOutputs, len(buffer))
                if !consumerBlocked {
                    log.Info(&amp;quot;retryer: send unwait-signal to consumer&amp;quot;)
                    r.consumer.sigUnWait()
                    log.Info(&amp;quot;  done&amp;quot;)
                }
            }

        case sig := &amp;lt;-r.sig:
            switch sig.tag {
            case sigRetryerUpdateOutput:
                r.out = sig.channel
            case sigRetryerOutputAdded:
                numOutputs++
            case sigRetryerOutputRemoved:
                numOutputs--
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后对out进行了设置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *outputController) Set(outGrp outputs.Group) {
    // create new outputGroup with shared work queue
    clients := outGrp.Clients
    queue := makeWorkQueue()
    worker := make([]outputWorker, len(clients))
    for i, client := range clients {
        worker[i] = makeClientWorker(c.observer, queue, client)
    }
    grp := &amp;amp;outputGroup{
        workQueue:  queue,
        outputs:    worker,
        timeToLive: outGrp.Retry + 1,
        batchSize:  outGrp.BatchSize,
    }

    // update consumer and retryer
    c.consumer.sigPause()
    if c.out != nil {
        for range c.out.outputs {
            c.retryer.sigOutputRemoved()
        }
    }
    c.retryer.updOutput(queue)
    for range clients {
        c.retryer.sigOutputAdded()
    }
    c.consumer.updOutput(grp)

    // close old group, so events are send to new workQueue via retryer
    if c.out != nil {
        for _, w := range c.out.outputs {
            w.Close()
        }
    }

    c.out = grp

    // restart consumer (potentially blocked by retryer)
    c.consumer.sigContinue()

    c.observer.updateOutputGroup()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边就是对上面创建的kafka的每个client创建一个监控程序makeClientWorker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeClientWorker(observer outputObserver, qu workQueue, client outputs.Client) outputWorker {
    if nc, ok := client.(outputs.NetworkClient); ok {
        c := &amp;amp;netClientWorker{observer: observer, qu: qu, client: nc}
        go c.run()
        return c
    }
    c := &amp;amp;clientWorker{observer: observer, qu: qu, client: client}
    go c.run()
    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就用于监控workQueue的数据，有数据就通过client的push发送到kafka，到这边pipeline的初始化也就结束了。&lt;/p&gt;

&lt;h2 id=&#34;日志收集&#34;&gt;日志收集&lt;/h2&gt;

&lt;p&gt;Filebeat 不仅支持普通文本日志的作为输入源，还内置支持了 redis 的慢查询日志、stdin、tcp 和 udp 等作为输入源。&lt;/p&gt;

&lt;p&gt;本文只分析下普通文本日志的处理方式，对于普通文本日志，可以按照以下配置方式，指定 log 的输入源信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/*.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 Input 也可以指定多个, 每个 Input 下的 Log 也可以指定多个。&lt;/p&gt;

&lt;p&gt;从收集日志、到发送事件到publisher，其数据流如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/collect.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;filebeat 启动时会开启 Crawler，filebeat抽象出一个Crawler的结构体，对于配置中的每条 Input，Crawler 都会启动一个 Input 进行处理，代码如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Crawler) Start(...){
    ...
    for _, inputConfig := range c.inputConfigs {
        err := c.startInput(pipeline, inputConfig, r.GetStates())
        if err != nil {
            return err
        }
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是创建input，比如我们采集的是log类型的，就是调用log的NewInput来创建，并且启动，定时进行扫描&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p, err := input.New(config, connector, c.beatDone, states, nil)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会根据采集日志的类型来进行注册调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewInput instantiates a new Log
func NewInput(
    cfg *common.Config,
    outlet channel.Connector,
    context input.Context,
) (input.Input, error) {
    cleanupNeeded := true
    cleanupIfNeeded := func(f func() error) {
        if cleanupNeeded {
            f()
        }
    }

    // Note: underlying output.
    //  The input and harvester do have different requirements
    //  on the timings the outlets must be closed/unblocked.
    //  The outlet generated here is the underlying outlet, only closed
    //  once all workers have been shut down.
    //  For state updates and events, separate sub-outlets will be used.
    out, err := outlet.ConnectWith(cfg, beat.ClientConfig{
        Processing: beat.ProcessingConfig{
            DynamicFields: context.DynamicFields,
        },
    })
    if err != nil {
        return nil, err
    }
    defer cleanupIfNeeded(out.Close)

    // stateOut will only be unblocked if the beat is shut down.
    // otherwise it can block on a full publisher pipeline, so state updates
    // can be forwarded correctly to the registrar.
    stateOut := channel.CloseOnSignal(channel.SubOutlet(out), context.BeatDone)
    defer cleanupIfNeeded(stateOut.Close)

    meta := context.Meta
    if len(meta) == 0 {
        meta = nil
    }

    p := &amp;amp;Input{
        config:      defaultConfig,
        cfg:         cfg,
        harvesters:  harvester.NewRegistry(),
        outlet:      out,
        stateOutlet: stateOut,
        states:      file.NewStates(),
        done:        context.Done,
        meta:        meta,
    }

    if err := cfg.Unpack(&amp;amp;p.config); err != nil {
        return nil, err
    }
    if err := p.config.resolveRecursiveGlobs(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to resolve recursive globs in config: %v&amp;quot;, err)
    }
    if err := p.config.normalizeGlobPatterns(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to normalize globs patterns: %v&amp;quot;, err)
    }

    // Create empty harvester to check if configs are fine
    // TODO: Do config validation instead
    _, err = p.createHarvester(file.State{}, nil)
    if err != nil {
        return nil, err
    }

    if len(p.config.Paths) == 0 {
        return nil, fmt.Errorf(&amp;quot;each input must have at least one path defined&amp;quot;)
    }

    err = p.loadStates(context.States)
    if err != nil {
        return nil, err
    }

    logp.Info(&amp;quot;Configured paths: %v&amp;quot;, p.config.Paths)

    cleanupNeeded = false
    go p.stopWhenDone()

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后会调用这个结构体的run函数进行扫描，主要是调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.scan()

// Scan starts a scanGlob for each provided path/glob
func (p *Input) scan() {
    var sortInfos []FileSortInfo
    var files []string

    paths := p.getFiles()

    var err error

    if p.config.ScanSort != &amp;quot;&amp;quot; {
        sortInfos, err = getSortedFiles(p.config.ScanOrder, p.config.ScanSort, getSortInfos(paths))
        if err != nil {
            logp.Err(&amp;quot;Failed to sort files during scan due to error %s&amp;quot;, err)
        }
    }

    if sortInfos == nil {
        files = getKeys(paths)
    }

    for i := 0; i &amp;lt; len(paths); i++ {

        var path string
        var info os.FileInfo

        if sortInfos == nil {
            path = files[i]
            info = paths[path]
        } else {
            path = sortInfos[i].path
            info = sortInfos[i].info
        }

        select {
        case &amp;lt;-p.done:
            logp.Info(&amp;quot;Scan aborted because input stopped.&amp;quot;)
            return
        default:
        }

        newState, err := getFileState(path, info, p)
        if err != nil {
            logp.Err(&amp;quot;Skipping file %s due to error %s&amp;quot;, path, err)
        }



        // Load last state
        lastState := p.states.FindPrevious(newState)

        // Ignores all files which fall under ignore_older
        if p.isIgnoreOlder(newState) {
            logp.Debug(&amp;quot;input&amp;quot;,&amp;quot;ignore&amp;quot;)
            err := p.handleIgnoreOlder(lastState, newState)
            if err != nil {
                logp.Err(&amp;quot;Updating ignore_older state error: %s&amp;quot;, err)
            }
            //close(p.done)
            continue
        }

        // Decides if previous state exists
        if lastState.IsEmpty() {
            logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Start harvester for new file: %s&amp;quot;, newState.Source)
            err := p.startHarvester(newState, 0)
            if err == errHarvesterLimit {
                logp.Debug(&amp;quot;input&amp;quot;, harvesterErrMsg, newState.Source, err)
                continue
            }
            if err != nil {
                logp.Err(harvesterErrMsg, newState.Source, err)
            }
        } else {
            p.harvestExistingFile(newState, lastState)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进行扫描过滤。由于指定的 paths 可以配置多个，而且可以是 Glob 类型，因此 Filebeat 将会匹配到多个配置文件。&lt;/p&gt;

&lt;p&gt;根据配置获取匹配的日志文件，需要注意的是，这里的匹配方式并非正则，而是采用linux glob的规则，和正则还是有一些区别。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;matches, err := filepath.Glob(path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取到了所有匹配的日志文件之后，会经过一些复杂的过滤，例如如果配置了exclude_files则会忽略这类文件，同时还会查询文件的状态，如果文件的最近一次修改时间大于ignore_older的配置，也会不去采集该文件。&lt;/p&gt;

&lt;p&gt;还会对文件进行处理，获取每个文件的状态，构建新的state结构，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;newState, err := getFileState(path, info, p)
if err != nil {
    logp.Err(&amp;quot;Skipping file %s due to error %s&amp;quot;, path, err)
}

func getFileState(path string, info os.FileInfo, p *Input) (file.State, error) {
    var err error
    var absolutePath string
    absolutePath, err = filepath.Abs(path)
    if err != nil {
        return file.State{}, fmt.Errorf(&amp;quot;could not fetch abs path for file %s: %s&amp;quot;, absolutePath, err)
    }
    logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Check file for harvesting: %s&amp;quot;, absolutePath)
    // Create new state for comparison
    newState := file.NewState(info, absolutePath, p.config.Type, p.meta, p.cfg.GetField(&amp;quot;brokerlist&amp;quot;), p.cfg.GetField(&amp;quot;topic&amp;quot;))
    return newState, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时在已经存在的状态中进行对比，如果获取到对于的状态就不重新启动协程进行采集，如果获取一个新的状态就开启新的协程进行采集。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load last state
lastState := p.states.FindPrevious(newState)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Input对象创建时会从registry读取文件状态(主要是offset)， 对于每个匹配到的文件，都会开启一个 Harvester 进行逐行读取，每个 Harvester 都工作在自己的的 goroutine 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err := p.startHarvester(newState, 0)
if err == errHarvesterLimit {
    logp.Debug(&amp;quot;input&amp;quot;, harvesterErrMsg, newState.Source, err)
    continue
}
if err != nil {
    logp.Err(harvesterErrMsg, newState.Source, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看startHarvester&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// startHarvester starts a new harvester with the given offset
// In case the HarvesterLimit is reached, an error is returned
func (p *Input) startHarvester(state file.State, offset int64) error {
    if p.numHarvesters.Inc() &amp;gt; p.config.HarvesterLimit &amp;amp;&amp;amp; p.config.HarvesterLimit &amp;gt; 0 {
        p.numHarvesters.Dec()
        harvesterSkipped.Add(1)
        return errHarvesterLimit
    }
    // Set state to &amp;quot;not&amp;quot; finished to indicate that a harvester is running
    state.Finished = false
    state.Offset = offset

    // Create harvester with state
    h, err := p.createHarvester(state, func() { p.numHarvesters.Dec() })
    if err != nil {
        p.numHarvesters.Dec()
        return err
    }

    err = h.Setup()
    if err != nil {
        p.numHarvesters.Dec()
        return fmt.Errorf(&amp;quot;error setting up harvester: %s&amp;quot;, err)
    }

    // Update state before staring harvester
    // This makes sure the states is set to Finished: false
    // This is synchronous state update as part of the scan
    h.SendStateUpdate()

    if err = p.harvesters.Start(h); err != nil {
        p.numHarvesters.Dec()
    }
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先创建了Harvester&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// createHarvester creates a new harvester instance from the given state
func (p *Input) createHarvester(state file.State, onTerminate func()) (*Harvester, error) {
    // Each wraps the outlet, for closing the outlet individually
    h, err := NewHarvester(
        p.cfg,
        state,
        p.states,
        func(state file.State) bool {
            return p.stateOutlet.OnEvent(beat.Event{Private: state})
        },
        subOutletWrap(p.outlet),
    )
    if err == nil {
        h.onTerminate = onTerminate
    }
    return h, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;harvester启动时会通过Setup方法创建一系列reader形成读处理链&lt;/p&gt;

&lt;p&gt;关于log类型的reader处理链，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/read.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;opt表示根据配置决定是否创建该reader&lt;/p&gt;

&lt;p&gt;Reader包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Line: 包含os.File，用于从指定offset开始读取日志行。虽然位于处理链的最内部，但其Next函数中实际的处理逻辑（读文件行）却是最新被执行的。&lt;/li&gt;
&lt;li&gt;Encode: 包含Line Reader，将其读取到的行生成Message结构后返回&lt;/li&gt;
&lt;li&gt;JSON, DockerJSON: 将json形式的日志内容decode成字段&lt;/li&gt;
&lt;li&gt;StripNewLine：去除日志行尾部的空白符&lt;/li&gt;
&lt;li&gt;Multiline: 用于读取多行日志&lt;/li&gt;
&lt;li&gt;Limit: 限制单行日志字节数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了Line Reader外，这些reader都实现了Reader接口：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader interface {
    Next() (Message, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reader通过内部包含Reader对象的方式，使Reader形成一个处理链，其实这就是设计模式中的责任链模式。&lt;/p&gt;

&lt;p&gt;各Reader的Next方法的通用形式像是这样：Next方法调用内部Reader对象的Next方法获取Message，然后处理后返回。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *SomeReader) Next() (Message, error) {
    message, err := r.reader.Next()
    if err != nil {
        return message, err
    }

    // do some processing...

    return message, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实Harvester 的工作流程非常简单，harvester从registry记录的文件位置开始读取，就是逐行读取文件，并更新该文件暂时在 Input 中的文件偏移量（注意，并不是 Registrar 中的偏移量），读取完成（读到文件的EOF末尾），组装成事件（beat.Event）后发给Publisher。主要是调用了Harvester的run方法，部分如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    select {
    case &amp;lt;-h.done:
        return nil
    default:
    }

    message, err := h.reader.Next()
    if err != nil {
        switch err {
        case ErrFileTruncate:
            logp.Info(&amp;quot;File was truncated. Begin reading file from offset 0: %s&amp;quot;, h.state.Source)
            h.state.Offset = 0
            filesTruncated.Add(1)
        case ErrRemoved:
            logp.Info(&amp;quot;File was removed: %s. Closing because close_removed is enabled.&amp;quot;, h.state.Source)
        case ErrRenamed:
            logp.Info(&amp;quot;File was renamed: %s. Closing because close_renamed is enabled.&amp;quot;, h.state.Source)
        case ErrClosed:
            logp.Info(&amp;quot;Reader was closed: %s. Closing.&amp;quot;, h.state.Source)
        case io.EOF:
            logp.Info(&amp;quot;End of file reached: %s. Closing because close_eof is enabled.&amp;quot;, h.state.Source)
        case ErrInactive:
            logp.Info(&amp;quot;File is inactive: %s. Closing because close_inactive of %v reached.&amp;quot;, h.state.Source, h.config.CloseInactive)
        case reader.ErrLineUnparsable:
            logp.Info(&amp;quot;Skipping unparsable line in file: %v&amp;quot;, h.state.Source)
            //line unparsable, go to next line
            continue
        default:
            logp.Err(&amp;quot;Read line error: %v; File: %v&amp;quot;, err, h.state.Source)
        }
        return nil
    }

    // Get copy of state to work on
    // This is important in case sending is not successful so on shutdown
    // the old offset is reported
    state := h.getState()
    startingOffset := state.Offset
    state.Offset += int64(message.Bytes)

    // Stop harvester in case of an error
    if !h.onMessage(forwarder, state, message, startingOffset) {
        return nil
    }

    // Update state of harvester as successfully sent
    h.state = state
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，reader.Next()方法会不停的读取日志，如果没有返回异常，则发送日志数据到缓存队列中。&lt;/p&gt;

&lt;p&gt;返回的异常有几种类型，除了读取到EOF外，还会有例如文件一段时间不活跃等情况发生会使harvester goroutine退出，不再采集该文件，并关闭文件句柄。 filebeat为了防止占据过多的采集日志文件的文件句柄，默认的close_inactive参数为5min，如果日志文件5min内没有被修改，上面代码会进入ErrInactive的case，之后该harvester goroutine会被关闭。 这种场景下还需要注意的是，如果某个文件日志采集中被移除了，但是由于此时被filebeat保持着文件句柄，文件占据的磁盘空间会被保留直到harvester goroutine结束。&lt;/p&gt;

&lt;p&gt;不同的harvester goroutine采集到的日志数据都会发送至一个全局的队列queue中，queue的实现有两种：基于内存和基于磁盘的队列，目前基于磁盘的队列还是处于alpha阶段，filebeat默认启用的是基于内存的缓存队列。 每当队列中的数据缓存到一定的大小或者超过了定时的时间（默认1s)，会被注册的client从队列中消费，发送至配置的后端。目前可以设置的client有kafka、elasticsearch、redis等。&lt;/p&gt;

&lt;p&gt;同时，我们需要考虑到，日志型的数据其实是在不断增长和变化的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;会有新的日志在不断产生
可能一个日志文件对应的 Harvester 退出后，又再次有了内容更新。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了解决这两个情况，filebeat 采用了 Input 定时扫描的方式。代码如下，可以看出，Input 扫描的频率是由用户指定的 scan_frequency 配置来决定的 (默认 10s 扫描一次)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Runner) Run() {
    p.input.Run()

    if p.Once {
        return
    }

    for {
        select {
        case &amp;lt;-p.done:
            logp.Info(&amp;quot;input ticker stopped&amp;quot;)
            return
        case &amp;lt;-time.After(p.config.ScanFrequency): // 定时扫描
            logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Run input&amp;quot;)
            p.input.Run()
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外，如果用户启动时指定了 –once 选项，则扫描只会进行一次，就退出了。&lt;/p&gt;

&lt;p&gt;使用一个简单的流程图可以这样表示&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/collect1.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;处理文件重命名，删除，截断&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取文件信息时会获取文件的device id + indoe作为文件的唯一标识;&lt;/li&gt;
&lt;li&gt;文件收集进度会被持久化，这样当创建Harvester时，首先会对文件作openFile, 以 device id + inode为key在持久化文件中查看当前文件是否被收集过，收集到了什么位置，然后断点续传&lt;/li&gt;
&lt;li&gt;在读取过程中，如果文件被截断，认为文件已经被同名覆盖，将从头开始读取文件&lt;/li&gt;
&lt;li&gt;如果文件被删除，因为原文件已被打开，不影响继续收集，但如果设置了CloseRemoved， 则不会再继续收集&lt;/li&gt;
&lt;li&gt;如果文件被重命名，因为原文件已被打开，不影响继续收集，但如果设置了CloseRenamed ， 则不会再继续收集&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;pipeline调度&#34;&gt;pipeline调度&lt;/h2&gt;

&lt;p&gt;至此，我们可以清楚的知道，Filebeat 是如何采集日志文件。而日志采集过程，Harvest 会将数据写到 Pipeline 中。我们接下来看下数据是如何写入到 Pipeline 中的。&lt;/p&gt;

&lt;p&gt;Haveseter 会将数据写入缓存中，而另一方面 Output 会从缓存将数据读走。整个生产消费的过程都是由 Pipeline 进行调度的，而整个调度过程也非常复杂。&lt;/p&gt;

&lt;p&gt;不同的harvester goroutine采集到的日志数据都会发送至一个全局的队列queue中，Filebeat 的缓存queue目前分为 memqueue 和 spool。memqueue 顾名思义就是内存缓存，spool 则是将数据缓存到磁盘中。本文将基于 memqueue 讲解整个调度过程。&lt;/p&gt;

&lt;p&gt;在下面的pipeline的写入和消费中，在client.go在(*client) publish方法中我们可以看到，事件是通过调用c.producer.Publish(pubEvent)被实际发送的，而producer则通过具体Queue的Producer方法生成。&lt;/p&gt;

&lt;p&gt;队列对象被包含在pipeline.go:Pipeline结构中，其接口的定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Queue interface {
    io.Closer
    BufferConfig() BufferConfig
    Producer(cfg ProducerConfig) Producer
    Consumer() Consumer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要的，Producer方法生成Producer对象，用于向队列中push事件；Consumer方法生成Consumer对象，用于从队列中取出事件。Producer和Consumer接口定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Producer interface {
    Publish(event publisher.Event) bool
    TryPublish(event publisher.Event) bool
    Cancel() int
}

type Consumer interface {
    Get(sz int) (Batch, error)
    Close() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在配置中没有指定队列配置时，默认使用了memqueue作为队列实现，下面我们来看看memqueue及其对应producer和consumer定义：&lt;/p&gt;

&lt;p&gt;Broker结构(memqueue在代码中实际对应的结构名是Broker)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Broker struct {
    done chan struct{}

    logger logger

    bufSize int
    // buf         brokerBuffer
    // minEvents   int
    // idleTimeout time.Duration

    // api channels
    events    chan pushRequest
    requests  chan getRequest
    pubCancel chan producerCancelRequest

    // internal channels
    acks          chan int
    scheduledACKs chan chanList

    eventer queue.Eventer

    // wait group for worker shutdown
    wg          sync.WaitGroup
    waitOnClose bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据是否需要ack分为forgetfullProducer和ackProducer两种producer：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type forgetfullProducer struct {
    broker    *Broker
    openState openState
}

type ackProducer struct {
    broker    *Broker
    cancel    bool
    seq       uint32
    state     produceState
    openState openState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;consumer结构:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type consumer struct {
    broker *Broker
    resp   chan getResponse

    done   chan struct{}
    closed atomic.Bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;queue、producer、consumer三者关系的运作方式如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/queue.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Producer通过Publish或TryPublish事件放入Broker的队列，即结构中的channel对象evetns&lt;/li&gt;
&lt;li&gt;Broker的主事件循环EventLoop将（请求）事件从events channel取出，放入自身结构体对象ringBuffer中。主事件循环有两种类型：

&lt;ul&gt;
&lt;li&gt;直接（不带buffer）事件循环结构directEventLoop：收到事件后尽可能快的转发；&lt;/li&gt;
&lt;li&gt;带buffer事件循环结构bufferingEventLoop：当buffer满或刷新超时时转发。具体使用哪一种取决于memqueue配置项flush.min_events，大于1时使用后者，否则使用前者。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;eventConsumer调用Consumer的Get方法获取事件：

&lt;ul&gt;
&lt;li&gt;首先将获取事件请求（包括请求事件数和用于存放其响应事件的channel resp）放入Broker的请求队列requests中，等待主事件循环EventLoop处理后将事件放入resp；&lt;/li&gt;
&lt;li&gt;获取resp的事件，组装成batch结构后返回&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;eventConsumer将事件放入output对应队列中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;这部分关于事件在队列中各种channel间的流转，笔者认为是比较消耗性能的，但不清楚设计者这样设计的考量是什么。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;另外值得思考的是，在多个go routine使用队列交互的场景下，libbeat中都使用了go语言channel作为其底层的队列，它是否可以完全替代加锁队列的使用呢？&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;pipeline-的写入&#34;&gt;Pipeline 的写入&lt;/h3&gt;

&lt;p&gt;在Crawler收集日志并转换成事件后，我们继续发送数据，其就会通过调用Publisher对应client的Publish接口将事件送到Publisher，后续的处理流程也都将由libbeat完成，事件的流转如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/event.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们首先看一下事件处理器processor&lt;/p&gt;

&lt;p&gt;在harvester调用client.Publish接口时，其内部会使用配置中定义的processors对事件进行处理，然后才将事件发送到Publisher队列。&lt;/p&gt;

&lt;p&gt;processor包含两种：在Input内定义作为局部（Input独享）的processor，其只对该Input产生的事件生效；在顶层配置中定义作为全局processor，其对全部事件生效。其对应的代码实现方式是： filebeat在使用libbeat pipeline的ConnectWith接口创建client时（factory.go中(*OutletFactory)Create函数），会将Input内部的定义processor作为参数传递给ConnectWith接口。而在ConnectWith实现中，会将参数中的processor和全局processor（在创建pipeline时生成）合并。从这里读者也可以发现，实际上每个Input都独享一个client，其包含一些Input自身的配置定义逻辑。&lt;/p&gt;

&lt;p&gt;任一Processor都实现了Processor接口：Run函数包含处理逻辑，String返回Processor名。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Processor interface {
    Run(event *beat.Event) (*beat.Event, error)
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再看下 Haveseter 是如何将数据写入缓存中的，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/produce-to-pipeline.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Harvester 通过 pipeline 提供的 pipelineClient 将数据写入到 pipeline 中，Haveseter 会将读到的数据会包装成一个 Event 结构体，再递交给 pipeline。&lt;/p&gt;

&lt;p&gt;在 Filebeat 的实现中，pipelineClient 并不直接操作缓存，而是将 event 先写入一个 events channel 中。&lt;/p&gt;

&lt;p&gt;同时，有一个 eventloop 组件，会监听 events channel 的事件到来，等 event 到达时，eventloop 会将其放入缓存中。&lt;/p&gt;

&lt;p&gt;当缓存满的时候，eventloop 直接移除对该 channel 的监听。&lt;/p&gt;

&lt;p&gt;每次 event ACK 或者取消后，缓存不再满了，则 eventloop 会重新监听 events channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// onMessage processes a new message read from the reader.
// This results in a state update and possibly an event would be send.
// A state update first updates the in memory state held by the prospector,
// and finally sends the file.State indirectly to the registrar.
// The events Private field is used to forward the file state update.
//
// onMessage returns &#39;false&#39; if it was interrupted in the process of sending the event.
// This normally signals a harvester shutdown.
func (h *Harvester) onMessage(
    forwarder *harvester.Forwarder,
    state file.State,
    message reader.Message,
    messageOffset int64,
) bool {
    if h.source.HasState() {
        h.states.Update(state)
    }

    text := string(message.Content)
    if message.IsEmpty() || !h.shouldExportLine(text) {
        // No data or event is filtered out -&amp;gt; send empty event with state update
        // only. The call can fail on filebeat shutdown.
        // The event will be filtered out, but forwarded to the registry as is.
        err := forwarder.Send(beat.Event{Private: state})
        return err == nil
    }

    fields := common.MapStr{
        &amp;quot;log&amp;quot;: common.MapStr{
            &amp;quot;offset&amp;quot;: messageOffset, // Offset here is the offset before the starting char.
            &amp;quot;file&amp;quot;: common.MapStr{
                &amp;quot;path&amp;quot;: state.Source,
            },
        },
    }
    fields.DeepUpdate(message.Fields)

    // Check if json fields exist
    var jsonFields common.MapStr
    if f, ok := fields[&amp;quot;json&amp;quot;]; ok {
        jsonFields = f.(common.MapStr)
    }

    var meta common.MapStr
    timestamp := message.Ts
    if h.config.JSON != nil &amp;amp;&amp;amp; len(jsonFields) &amp;gt; 0 {
        id, ts := readjson.MergeJSONFields(fields, jsonFields, &amp;amp;text, *h.config.JSON)
        if !ts.IsZero() {
            // there was a `@timestamp` key in the event, so overwrite
            // the resulting timestamp
            timestamp = ts
        }

        if id != &amp;quot;&amp;quot; {
            meta = common.MapStr{
                &amp;quot;id&amp;quot;: id,
            }
        }
    } else if &amp;amp;text != nil {
        if fields == nil {
            fields = common.MapStr{}
        }
        fields[&amp;quot;message&amp;quot;] = text
    }

    err := forwarder.Send(beat.Event{
        Timestamp: timestamp,
        Fields:    fields,
        Meta:      meta,
        Private:   state,
    })
    return err == nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将数据包装成event直接通过send方法将数据发出去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Send updates the input state and sends the event to the spooler
// All state updates done by the input itself are synchronous to make sure no states are overwritten
func (f *Forwarder) Send(event beat.Event) error {
    ok := f.Outlet.OnEvent(event)
    if !ok {
        logp.Info(&amp;quot;Input outlet closed&amp;quot;)
        return errors.New(&amp;quot;input outlet closed&amp;quot;)
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用Outlet.OnEvent发送data&lt;/p&gt;

&lt;p&gt;点进去发现是一个接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Outlet interface {
       OnEvent(data *util.Data) bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过调试观察，elastic\beats\filebeat\channel\outlet.go实现了这个接口&lt;/p&gt;

&lt;p&gt;outlet在Harvester的run一开始就创建了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;outlet := channel.CloseOnSignal(h.outletFactory(), h.done)
forwarder := harvester.NewForwarder(outlet)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以调用的OnEvent&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (o *outlet) OnEvent(event beat.Event) bool {
    if !o.isOpen.Load() {
        return false
    }

    if o.wg != nil {
        o.wg.Add(1)
    }

    o.client.Publish(event)

    // Note: race condition on shutdown:
    //  The underlying beat.Client is asynchronous. Without proper ACK
    //  handler we can not tell if the event made it &#39;through&#39; or the client
    //  close has been completed before sending. In either case,
    //  we report &#39;false&#39; here, indicating the event eventually being dropped.
    //  Returning false here, prevents the harvester from updating the state
    //  to the most recently published events. Therefore, on shutdown the harvester
    //  might report an old/outdated state update to the registry, overwriting the
    //  most recently
    //  published offset in the registry on shutdown.
    return o.isOpen.Load()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过client.Publish发送数据，client也是一个接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Client interface {
       Publish(Event)
       PublishAll([]Event)
       Close() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调试之后，client使用的是elastic\beats\libbeat\publisher\pipeline\client.go的client对象&lt;/p&gt;

&lt;p&gt;我们来看一下这个client是通过Harvester的参数outletFactory来初始化的，我们来看一下NewHarvester初始化的时候也就是在createHarvester的时候传递的参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// createHarvester creates a new harvester instance from the given state
func (p *Input) createHarvester(state file.State, onTerminate func()) (*Harvester, error) {
    // Each wraps the outlet, for closing the outlet individually
    h, err := NewHarvester(
        p.cfg,
        state,
        p.states,
        func(state file.State) bool {
            return p.stateOutlet.OnEvent(beat.Event{Private: state})
        },
        subOutletWrap(p.outlet),
    )
    if err == nil {
        h.onTerminate = onTerminate
    }
    return h, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是subOutletWrap中的参数p.outlet那就要看以下Input初始化的的时候NewInput传递的参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewInput instantiates a new Log
func NewInput(
    cfg *common.Config,
    outlet channel.Connector,
    context input.Context,
) (input.Input, error) {
    cleanupNeeded := true
    cleanupIfNeeded := func(f func() error) {
        if cleanupNeeded {
            f()
        }
    }

    // Note: underlying output.
    //  The input and harvester do have different requirements
    //  on the timings the outlets must be closed/unblocked.
    //  The outlet generated here is the underlying outlet, only closed
    //  once all workers have been shut down.
    //  For state updates and events, separate sub-outlets will be used.
    out, err := outlet.ConnectWith(cfg, beat.ClientConfig{
        Processing: beat.ProcessingConfig{
            DynamicFields: context.DynamicFields,
        },
    })
    if err != nil {
        return nil, err
    }
    defer cleanupIfNeeded(out.Close)

    // stateOut will only be unblocked if the beat is shut down.
    // otherwise it can block on a full publisher pipeline, so state updates
    // can be forwarded correctly to the registrar.
    stateOut := channel.CloseOnSignal(channel.SubOutlet(out), context.BeatDone)
    defer cleanupIfNeeded(stateOut.Close)

    meta := context.Meta
    if len(meta) == 0 {
        meta = nil
    }

    p := &amp;amp;Input{
        config:      defaultConfig,
        cfg:         cfg,
        harvesters:  harvester.NewRegistry(),
        outlet:      out,
        stateOutlet: stateOut,
        states:      file.NewStates(),
        done:        context.Done,
        meta:        meta,
    }

    if err := cfg.Unpack(&amp;amp;p.config); err != nil {
        return nil, err
    }
    if err := p.config.resolveRecursiveGlobs(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to resolve recursive globs in config: %v&amp;quot;, err)
    }
    if err := p.config.normalizeGlobPatterns(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to normalize globs patterns: %v&amp;quot;, err)
    }

    // Create empty harvester to check if configs are fine
    // TODO: Do config validation instead
    _, err = p.createHarvester(file.State{}, nil)
    if err != nil {
        return nil, err
    }

    if len(p.config.Paths) == 0 {
        return nil, fmt.Errorf(&amp;quot;each input must have at least one path defined&amp;quot;)
    }

    err = p.loadStates(context.States)
    if err != nil {
        return nil, err
    }

    logp.Info(&amp;quot;Configured paths: %v&amp;quot;, p.config.Paths)

    cleanupNeeded = false
    go p.stopWhenDone()

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要看outlet，这个是在Crawler的startInput的时候进行初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Crawler) startInput(
    pipeline beat.Pipeline,
    config *common.Config,
    states []file.State,
) error {
    if !config.Enabled() {
        return nil
    }

    connector := c.out(pipeline)
    p, err := input.New(config, connector, c.beatDone, states, nil)
    if err != nil {
        return fmt.Errorf(&amp;quot;Error while initializing input: %s&amp;quot;, err)
    }
    p.Once = c.once

    if _, ok := c.inputs[p.ID]; ok {
        return fmt.Errorf(&amp;quot;Input with same ID already exists: %d&amp;quot;, p.ID)
    }

    c.inputs[p.ID] = p

    p.Start()

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看out就是crawler创建new的时候传递的值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;crawler, err := crawler.New(
        channel.NewOutletFactory(outDone, wgEvents, b.Info).Create,
        config.Inputs,
        b.Info.Version,
        fb.done,
        *once)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是create返回的pipelineConnector结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *OutletFactory) Create(p beat.Pipeline) Connector {
    return &amp;amp;pipelineConnector{parent: f, pipeline: p}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看pipelineConnector的ConnectWith函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *pipelineConnector) ConnectWith(cfg *common.Config, clientCfg beat.ClientConfig) (Outleter, error) {
    config := inputOutletConfig{}
    if err := cfg.Unpack(&amp;amp;config); err != nil {
        return nil, err
    }

    procs, err := processorsForConfig(c.parent.beatInfo, config, clientCfg)
    if err != nil {
        return nil, err
    }

    setOptional := func(to common.MapStr, key string, value string) {
        if value != &amp;quot;&amp;quot; {
            to.Put(key, value)
        }
    }

    meta := clientCfg.Processing.Meta.Clone()
    fields := clientCfg.Processing.Fields.Clone()

    serviceType := config.ServiceType
    if serviceType == &amp;quot;&amp;quot; {
        serviceType = config.Module
    }

    setOptional(meta, &amp;quot;pipeline&amp;quot;, config.Pipeline)
    setOptional(fields, &amp;quot;fileset.name&amp;quot;, config.Fileset)
    setOptional(fields, &amp;quot;service.type&amp;quot;, serviceType)
    setOptional(fields, &amp;quot;input.type&amp;quot;, config.Type)
    if config.Module != &amp;quot;&amp;quot; {
        event := common.MapStr{&amp;quot;module&amp;quot;: config.Module}
        if config.Fileset != &amp;quot;&amp;quot; {
            event[&amp;quot;dataset&amp;quot;] = config.Module + &amp;quot;.&amp;quot; + config.Fileset
        }
        fields[&amp;quot;event&amp;quot;] = event
    }

    mode := clientCfg.PublishMode
    if mode == beat.DefaultGuarantees {
        mode = beat.GuaranteedSend
    }

    // connect with updated configuration
    clientCfg.PublishMode = mode
    clientCfg.Processing.EventMetadata = config.EventMetadata
    clientCfg.Processing.Meta = meta
    clientCfg.Processing.Fields = fields
    clientCfg.Processing.Processor = procs
    clientCfg.Processing.KeepNull = config.KeepNull
    client, err := c.pipeline.ConnectWith(clientCfg)
    if err != nil {
        return nil, err
    }

    outlet := newOutlet(client, c.parent.wgEvents)
    if c.parent.done != nil {
        return CloseOnSignal(outlet, c.parent.done), nil
    }
    return outlet, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边获取到了pipeline的客户端client&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// ConnectWith create a new Client for publishing events to the pipeline.
// The client behavior on close and ACK handling can be configured by setting
// the appropriate fields in the passed ClientConfig.
// If not set otherwise the defaut publish mode is OutputChooses.
func (p *Pipeline) ConnectWith(cfg beat.ClientConfig) (beat.Client, error) {
    var (
        canDrop      bool
        dropOnCancel bool
        eventFlags   publisher.EventFlags
    )

    err := validateClientConfig(&amp;amp;cfg)
    if err != nil {
        return nil, err
    }

    p.eventer.mutex.Lock()
    p.eventer.modifyable = false
    p.eventer.mutex.Unlock()

    switch cfg.PublishMode {
    case beat.GuaranteedSend:
        eventFlags = publisher.GuaranteedSend
        dropOnCancel = true
    case beat.DropIfFull:
        canDrop = true
    }

    waitClose := cfg.WaitClose
    reportEvents := p.waitCloser != nil

    switch p.waitCloseMode {
    case NoWaitOnClose:

    case WaitOnClientClose:
        if waitClose &amp;lt;= 0 {
            waitClose = p.waitCloseTimeout
        }
    }

    processors, err := p.createEventProcessing(cfg.Processing, publishDisabled)
    if err != nil {
        return nil, err
    }

    client := &amp;amp;client{
        pipeline:     p,
        closeRef:     cfg.CloseRef,
        done:         make(chan struct{}),
        isOpen:       atomic.MakeBool(true),
        eventer:      cfg.Events,
        processors:   processors,
        eventFlags:   eventFlags,
        canDrop:      canDrop,
        reportEvents: reportEvents,
    }

    acker := p.makeACKer(processors != nil, &amp;amp;cfg, waitClose, client.unlink)
    producerCfg := queue.ProducerConfig{
        // Cancel events from queue if acker is configured
        // and no pipeline-wide ACK handler is registered.
        DropOnCancel: dropOnCancel &amp;amp;&amp;amp; acker != nil &amp;amp;&amp;amp; p.eventer.cb == nil,
    }

    if reportEvents || cfg.Events != nil {
        producerCfg.OnDrop = func(event beat.Event) {
            if cfg.Events != nil {
                cfg.Events.DroppedOnPublish(event)
            }
            if reportEvents {
                p.waitCloser.dec(1)
            }
        }
    }

    if acker != nil {
        producerCfg.ACK = acker.ackEvents
    } else {
        acker = newCloseACKer(nilACKer, client.unlink)
    }

    client.acker = acker
    client.producer = p.queue.Producer(producerCfg)

    p.observer.clientConnected()

    if client.closeRef != nil {
        p.registerSignalPropagation(client)
    }

    return client, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的就是client的Publish函数来发送数据，publish方法即发送日志的方法，如果需要在发送前改造日志格式，可在这里添加代码，如下面的解析日志代码。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) Publish(e beat.Event) {
    c.mutex.Lock()
    defer c.mutex.Unlock()

    c.publish(e)
}

func (c *client) publish(e beat.Event) {
    var (
        event   = &amp;amp;e
        publish = true
        log     = c.pipeline.monitors.Logger
    )

    c.onNewEvent()

    if !c.isOpen.Load() {
        // client is closing down -&amp;gt; report event as dropped and return
        c.onDroppedOnPublish(e)
        return
    }

    if c.processors != nil {
        var err error

        event, err = c.processors.Run(event)
        publish = event != nil
        if err != nil {
            // TODO: introduce dead-letter queue?

            log.Errorf(&amp;quot;Failed to publish event: %v&amp;quot;, err)
        }
    }

    if event != nil {
        e = *event
    }

    open := c.acker.addEvent(e, publish)
    if !open {
        // client is closing down -&amp;gt; report event as dropped and return
        c.onDroppedOnPublish(e)
        return
    }

    if !publish {
        c.onFilteredOut(e)
        return
    }

    e = *event
    pubEvent := publisher.Event{
        Content: e,
        Flags:   c.eventFlags,
    }

    if c.reportEvents {
        c.pipeline.waitCloser.inc()
    }

    var published bool
    if c.canDrop {
        published = c.producer.TryPublish(pubEvent)
    } else {
        published = c.producer.Publish(pubEvent)
    }

    if published {
        c.onPublished()
    } else {
        c.onDroppedOnPublish(e)
        if c.reportEvents {
            c.pipeline.waitCloser.dec(1)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上面创建clinet的时候，创建了队列的生产者，也就是之前broker的Producer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) Producer(cfg queue.ProducerConfig) queue.Producer {
    return newProducer(b, cfg.ACK, cfg.OnDrop, cfg.DropOnCancel)
}

func newProducer(b *Broker, cb ackHandler, dropCB func(beat.Event), dropOnCancel bool) queue.Producer {
    openState := openState{
        log:    b.logger,
        isOpen: atomic.MakeBool(true),
        done:   make(chan struct{}),
        events: b.events,
    }

    if cb != nil {
        p := &amp;amp;ackProducer{broker: b, seq: 1, cancel: dropOnCancel, openState: openState}
        p.state.cb = cb
        p.state.dropCB = dropCB
        return p
    }
    return &amp;amp;forgetfulProducer{broker: b, openState: openState}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是forgetfulProducer结构体，调用这个的Publish函数来发送数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *forgetfulProducer) Publish(event publisher.Event) bool {
    return p.openState.publish(p.makeRequest(event))
}

func (st *openState) publish(req pushRequest) bool {
    select {
    case st.events &amp;lt;- req:
        return true
    case &amp;lt;-st.done:
        st.events = nil
        return false
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将数据放到了forgetfulProducer的openState的events中。到此数据就算发送到pipeline中了。&lt;/p&gt;

&lt;p&gt;上文在pipeline的初始化的时候，queue初始化一般默认都是BufferingEventLoop，即带缓冲的队列。BufferingEventLoop是一个实现了Broker、带有各种channel的结构，主要用于将日志发送至consumer消费。 BufferingEventLoop的run方法中，同样是一个无限循环，这里可以认为是一个日志事件的调度中心。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
        select {
        case &amp;lt;-broker.done:
            return
        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)
        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)
        case count := &amp;lt;-l.acks:
            l.handleACK(count)
        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上文中harvester goroutine每次读取到日志数据之后，最终会被发送至bufferingEventLoop中的events chan pushRequest 的channel中，然后触发上面req := &amp;lt;-l.events的case，handleInsert方法会把数据添加至bufferingEventLoop的buf中，buf即memqueue实际缓存日志数据的队列，如果buf长度超过配置的最大值或者bufferingEventLoop中的timer定时器（默认1S）触发了case &amp;lt;-l.idleC，均会调用flushBuffer()方法。
flushBuffer()又会触发req := &amp;lt;-l.get的case，然后运行handleConsumer方法，该方法中最重要的是这一句代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;req.resp &amp;lt;- getResponse{ackChan, events}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里获取到了consumer消费者的response channel，然后发送数据给这个channel。真正到这，才会触发consumer对memqueue的消费。所以，其实memqueue并非一直不停的在被consumer消费，而是在memqueue通知consumer的时候才被消费，我们可以理解为一种脉冲式的发送&lt;/p&gt;

&lt;p&gt;简单的来说就是，每当队列中的数据缓存到一定的大小或者超过了定时的时间（默认1s)，会被注册的client从队列中消费，发送至配置的后端。&lt;/p&gt;

&lt;p&gt;以上是 Pipeline 的写入过程，此时 event 已被写入到了缓存中。&lt;/p&gt;

&lt;p&gt;但是 Output 是如何从缓存中拿到 event 数据的？&lt;/p&gt;

&lt;h3 id=&#34;pipeline-的消费过程&#34;&gt;Pipeline 的消费过程&lt;/h3&gt;

&lt;p&gt;在上文已经提到过，filebeat初始化的时候，就已经创建了一个eventConsumer并在loop无限循环方法里试图从Broker中其实也就是上面的resp中获取日志数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; consumer != nil &amp;amp;&amp;amp; batch == nil {
            out = c.out.workQueue
            queueBatch, err := consumer.Get(c.out.batchSize)
            ...
            batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
        }
        ...
        select {
        case &amp;lt;-c.done:
            return
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
        case out &amp;lt;- batch:
            batch = nil
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面consumer.Get就是消费者consumer从Broker中获取日志数据，然后发送至out的channel中被output client发送，我们看一下Get方法里的核心代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select {
    case c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:
    case &amp;lt;-c.done:
        return nil, io.EOF
    }

    // if request has been send, we do have to wait for a response
    resp := &amp;lt;-c.resp
    return &amp;amp;batch{
        consumer: c,
        events:   resp.buf,
        ack:      resp.ack,
        state:    batchActive,
    }, nil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;getRequest的结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type getRequest struct {
    sz   int              // request sz events from the broker
    resp chan getResponse // channel to send response to
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;getResponse的结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type getResponse struct {
    ack *ackChan
    buf []publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;getResponse里包含了日志的数据，而getRequest包含了一个发送至消费者的channel。
在上文bufferingEventLoop缓冲队列的handleConsumer方法里接收到的参数为getRequest，里面包含了consumer请求的getResponse channel。
如果handleConsumer不发送数据，consumer.Get方法会一直阻塞在select中，直到flushBuffer，consumer的getResponse channel才会接收到日志数据。&lt;/p&gt;

&lt;p&gt;我们来看看bufferingEventLoop的调度中心&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) run() {
    var (
        broker = l.broker
    )

    for {
        select {
        case &amp;lt;-broker.done:
            return

        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)

        case req := &amp;lt;-l.pubCancel: // producer cancelling active events
            l.handleCancel(&amp;amp;req)

        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)

        case l.schedACKS &amp;lt;- l.pendingACKs:
            l.schedACKS = nil
            l.pendingACKs = chanList{}

        case count := &amp;lt;-l.acks:
            l.handleACK(count)

        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}时候，l.get会得到信息，为什么l.get会得到信息，因为l.get = l.broker.requests&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) flushBuffer() {
    l.buf.flushed = true

    if l.buf.length() == 0 {
        panic(&amp;quot;flushing empty buffer&amp;quot;)
    }

    l.flushList.add(l.buf)
    l.get = l.broker.requests
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看handleConsumer如何处理信息的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) handleConsumer(req *getRequest) {
    buf := l.flushList.head
    if buf == nil {
        panic(&amp;quot;get from non-flushed buffers&amp;quot;)
    }

    count := buf.length()
    if count == 0 {
        panic(&amp;quot;empty buffer in flush list&amp;quot;)
    }

    if sz := req.sz; sz &amp;gt; 0 {
        if sz &amp;lt; count {
            count = sz
        }
    }

    if count == 0 {
        panic(&amp;quot;empty batch returned&amp;quot;)
    }

    events := buf.events[:count]
    clients := buf.clients[:count]
    ackChan := newACKChan(l.ackSeq, 0, count, clients)
    l.ackSeq++

    req.resp &amp;lt;- getResponse{ackChan, events}
    l.pendingACKs.append(ackChan)
    l.schedACKS = l.broker.scheduledACKs

    buf.events = buf.events[count:]
    buf.clients = buf.clients[count:]
    if buf.length() == 0 {
        l.advanceFlushList()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;处理req，并且将数据发送给req的resp，在发送的时候c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:就将c.resp赋值给了req的resp。所以可以获得返回值getResponse，组装成batch发送出去，其实就是放到type workQueue chan *Batch这个barch类型的channel中。&lt;/p&gt;

&lt;p&gt;看一下getResponse&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type getResponse struct {
    ack *ackChan
    buf []publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见ack就是channel，buf就是发送的日志。&lt;/p&gt;

&lt;p&gt;整个消费的过程非常复杂，数据会在多个 channel 之间传递流转，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/consume-from-pipeline.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;首先再介绍两个角色：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;consumer： pipeline 在创建的时候，会同时创建一个 consumer。consumer 负责从缓存中取数据
client worker：负责接收 consumer 传来的数据，并调用 Output 的 Publish 函数进行上报。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;与 producer 类似，consumer 也不直接操作缓存，而是会向 get channel 中写入消费请求。&lt;/p&gt;

&lt;p&gt;consumer 本身是个后台 loop 的过程，这个消费请求会不断进行。&lt;/p&gt;

&lt;p&gt;eventloop 监听 get channel, 拿到之后会从缓存中取数据。并将数据写入到 resp channel 中。&lt;/p&gt;

&lt;p&gt;consumer 从 resp channel 中拿到 event 数据后，又会将其写入到 workQueue。&lt;/p&gt;

&lt;p&gt;workQueue 也是个 channel。client worker 会监听该 channel 上的数据到来，将数据交给 Output client 进行 Publish 上报。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type workQueue chan *Batch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而且，Output 收到的是 Batch Events，即会一次收到一批 Events。BatchSize 由各个 Output 自行决定。&lt;/p&gt;

&lt;p&gt;至此，消息已经递交给了 Output 组件。&lt;/p&gt;

&lt;h3 id=&#34;output&#34;&gt;output&lt;/h3&gt;

&lt;p&gt;Filebeat 并不依赖于 Elasticsearch，可以单独存在。我们可以单独使用 Filebeat 进行日志的上报和搜集。filebeat 内置了常用的 Output 组件, 例如 kafka、Elasticsearch、redis 等。出于调试考虑，也可以输出到 console 和 file。我们可以利用现有的 Output 组件，将日志进行上报。&lt;/p&gt;

&lt;p&gt;当然，我们也可以自定义 Output 组件，让 Filebeat 将日志转发到我们想要的地方。&lt;/p&gt;

&lt;p&gt;在上文提到过，在pipeline初始化的时候，就会设置output的clinet，会创建一个clientWorker或者netClientWorker（可重连，默认就是这个），clientWorker的run方法中，会不停的从consumer发送的channel（就是上面的workQueue）里读取日志数据，然后调用client.Publish批量发送日志。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (w *netClientWorker) run() {
    for !w.closed.Load() {
        reconnectAttempts := 0

        // start initial connect loop from first batch, but return
        // batch to pipeline for other outputs to catch up while we&#39;re trying to connect
        for batch := range w.qu {
            batch.Cancelled()

            if w.closed.Load() {
                logp.Info(&amp;quot;Closed connection to %v&amp;quot;, w.client)
                return
            }

            if reconnectAttempts &amp;gt; 0 {
                logp.Info(&amp;quot;Attempting to reconnect to %v with %d reconnect attempt(s)&amp;quot;, w.client, reconnectAttempts)
            } else {
                logp.Info(&amp;quot;Connecting to %v&amp;quot;, w.client)
            }

            err := w.client.Connect()
            if err != nil {
                logp.Err(&amp;quot;Failed to connect to %v: %v&amp;quot;, w.client, err)
                reconnectAttempts++
                continue
            }

            logp.Info(&amp;quot;Connection to %v established&amp;quot;, w.client)
            reconnectAttempts = 0
            break
        }

        // send loop
        for batch := range w.qu {
            if w.closed.Load() {
                if batch != nil {
                    batch.Cancelled()
                }
                return
            }

            err := w.client.Publish(batch)
            if err != nil {
                logp.Err(&amp;quot;Failed to publish events: %v&amp;quot;, err)
                // on error return to connect loop
                break
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;libbeats库中包含了kafka、elasticsearch、logstash等几种client，它们均实现了client接口：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Client interface {
    Close() error
    Publish(publisher.Batch) error
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然最重要的是实现Publish接口，然后将日志发送出去。比如我们看一下kafka的Publish接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) Publish(batch publisher.Batch) error {
    events := batch.Events()
    c.observer.NewBatch(len(events))

    ref := &amp;amp;msgRef{
        client: c,
        count:  int32(len(events)),
        total:  len(events),
        failed: nil,
        batch:  batch,
    }

    ch := c.producer.Input()
    for i := range events {
        d := &amp;amp;events[i]
        msg, err := c.getEventMessage(d)
        if err != nil {
            logp.Err(&amp;quot;Dropping event: %v&amp;quot;, err)
            ref.done()
            c.observer.Dropped(1)
            continue
        }

        msg.ref = ref
        msg.initProducerMessage()
        ch &amp;lt;- &amp;amp;msg.msg
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是基本的kafka客户端的使用方法，到此为止，数据也就发送的kakfa了。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;其实在pipleline调度的时候就说明了queue的生产消费的关系，数据在各个channel中进行传输，整个日志数据流转的过程还是表复杂的，在各个channel中进行流转，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/datastream.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;registry和ack-机制&#34;&gt;registry和Ack 机制&lt;/h2&gt;

&lt;p&gt;Filebeat 的可靠性很强，可以保证日志 At least once 的上报，同时也考虑了日志搜集中的各类问题，例如日志断点续读、文件名更改、日志 Truncated 等。&lt;/p&gt;

&lt;p&gt;filebeat 之所以可以保证日志可以 at least once 的上报，就是基于其 Ack 机制。&lt;/p&gt;

&lt;p&gt;简单来说，Ack 机制就是，当 Output Publish 成功之后会调用 ACK，最终 Registrar 会收到 ACK，并修改偏移量。&lt;/p&gt;

&lt;p&gt;而且, Registrar 只会在 Output 调用 batch 的相关信号时，才改变文件偏移量。其中 Batch 对外提供了这些信号：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Batch interface {
    Events() []Event

    // signals
    ACK()
    Drop()
    Retry()
    RetryEvents(events []Event)
    Cancelled()
    CancelledEvents(events []Event)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output 在 Publish 之后，无论失败，必须调用这些函数中的其中一个。&lt;/p&gt;

&lt;p&gt;以下是 Output Publish 成功后调用 Ack 的流程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/ack.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到其中起核心作用的组件是 Ackloop。AckLoop 中有一个 ackChanList，其中每一个 ackChan，对应于转发给 Output 的一个 Batch。
每次新建一个 Batch，同时会建立一个 ackChan，该 ackChan 会被 append 到 ackChanList 中。&lt;/p&gt;

&lt;p&gt;而 AckLoop 每次只监听处于 ackChanList 最头部的 ackChan。&lt;/p&gt;

&lt;p&gt;当 Batch 被 Output 调用 Ack 后，AckLoop 会收到对应 ackChan 上的事件，并将其最终转发给 Registrar。同时，ackChanList 将会 pop 头部的 ackChan，继续监听接下来的 Ack 事件。&lt;/p&gt;

&lt;p&gt;由于 FileBeat 是 At least once 的上报，但并不保证 Exactly once, 因此一条数据可能会被上报多次，所以接收端需要自行进行去重过滤。&lt;/p&gt;

&lt;p&gt;上面状态的修改，主要是filebeat维护了一个registry文件在本地的磁盘，该registry文件维护了所有已经采集的日志文件的状态。 实际上，每当日志数据发送至后端成功后，会返回ack事件。filebeat启动了一个独立的registry协程负责监听该事件，接收到ack事件后会将日志文件的State状态更新至registry文件中，State中的Offset表示读取到的文件偏移量，所以filebeat会保证Offset记录之前的日志数据肯定被后端的日志存储接收到。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;pipeline初始化的ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先是pipeline初始化只有一次，在这个时候只是简单的初始化了pipeline的ack相关信息，这边也创建的一个queue，原始是使用一个queue，这边初始化broker的时候会创建ack.run()来监听，后来改造多kafka发送后这一条queue
是不用的，而且每次连接kafka的时候创建一个新queue的时候，会都会创建一个ack.run()来监听，流程是一样的，改造可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/#支持多kafka的发送&#34;&gt;支持多kafka的发送&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.ackBuilder = &amp;amp;pipelineEmptyACK{p}
p.ackActive = atomic.MakeBool(true)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是在创建queue的时候，默认是使用mem的queue，会创建ack。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ack := newACKLoop(b, eventLoop.processACK)

b.wg.Add(2)
go func() {
    defer b.wg.Done()
    eventLoop.run()
}()
go func() {
    defer b.wg.Done()
    ack.run()
}()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在创建newBufferingEventLoop队列的同时，会newACKLoop并且调用相应结构体的run函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newACKLoop(b *Broker, processACK func(chanList, int)) *ackLoop {
    l := &amp;amp;ackLoop{broker: b}
    l.processACK = processACK
    return l
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ackLoop struct {
    broker *Broker
    sig    chan batchAckMsg
    lst    chanList

    totalACK   uint64
    totalSched uint64

    batchesSched uint64
    batchesACKed uint64

    processACK func(chanList, int)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看一下run函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *ackLoop) run() {
    var (
        // log = l.broker.logger

        // Buffer up acked event counter in acked. If acked &amp;gt; 0, acks will be set to
        // the broker.acks channel for sending the ACKs while potentially receiving
        // new batches from the broker event loop.
        // This concurrent bidirectionally communication pattern requiring &#39;select&#39;
        // ensures we can not have any deadlock between the event loop and the ack
        // loop, as the ack loop will not block on any channel
        acked int
        acks  chan int
    )

    for {
        select {
        case &amp;lt;-l.broker.done:
            // TODO: handle pending ACKs?
            // TODO: panic on pending batches?
            return

        case acks &amp;lt;- acked:
            acks, acked = nil, 0

        case lst := &amp;lt;-l.broker.scheduledACKs:
            count, events := lst.count()
            l.lst.concat(&amp;amp;lst)

            // log.Debug(&amp;quot;ACK List:&amp;quot;)
            // for current := l.lst.head; current != nil; current = current.next {
            //  log.Debugf(&amp;quot;  ack entry(seq=%v, start=%v, count=%v&amp;quot;,
            //      current.seq, current.start, current.count)
            // }

            l.batchesSched += uint64(count)
            l.totalSched += uint64(events)

        case &amp;lt;-l.sig:
            acked += l.handleBatchSig()
            if acked &amp;gt; 0 {
                acks = l.broker.acks
            }
        }

        // log.Debug(&amp;quot;ackloop INFO&amp;quot;)
        // log.Debug(&amp;quot;ackloop:   total events scheduled = &amp;quot;, l.totalSched)
        // log.Debug(&amp;quot;ackloop:   total events ack = &amp;quot;, l.totalACK)
        // log.Debug(&amp;quot;ackloop:   total batches scheduled = &amp;quot;, l.batchesSched)
        // log.Debug(&amp;quot;ackloop:   total batches ack = &amp;quot;, l.batchesACKed)

        l.sig = l.lst.channel()
        // if l.sig == nil {
        //  log.Debug(&amp;quot;ackloop: no ack scheduled&amp;quot;)
        // } else {
        //  log.Debug(&amp;quot;ackloop: schedule ack: &amp;quot;, l.lst.head.seq)
        // }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边其实就是对ack信号的调度处理中心。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;registrar初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后就是启动filebeat的时候可能是要初始化registrar&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Setup registrar to persist state
registrar, err := registrar.New(config.Registry, finishedLogger)
if err != nil {
    logp.Err(&amp;quot;Could not init registrar: %v&amp;quot;, err)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;config.Registry就是registry文件的配置信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Registry struct {
    Path         string        `config:&amp;quot;path&amp;quot;`
    Permissions  os.FileMode   `config:&amp;quot;file_permissions&amp;quot;`
    FlushTimeout time.Duration `config:&amp;quot;flush&amp;quot;`
    MigrateFile  string        `config:&amp;quot;migrate_file&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建方法很简单，就是对文件的一些描述赋值给了这个结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New creates a new Registrar instance, updating the registry file on
// `file.State` updates. New fails if the file can not be opened or created.
func New(cfg config.Registry, out successLogger) (*Registrar, error) {
    home := paths.Resolve(paths.Data, cfg.Path)
    migrateFile := cfg.MigrateFile
    if migrateFile != &amp;quot;&amp;quot; {
        migrateFile = paths.Resolve(paths.Data, migrateFile)
    }

    err := ensureCurrent(home, migrateFile, cfg.Permissions)
    if err != nil {
        return nil, err
    }

    dataFile := filepath.Join(home, &amp;quot;filebeat&amp;quot;, &amp;quot;data.json&amp;quot;)
    r := &amp;amp;Registrar{
        registryFile: dataFile,
        fileMode:     cfg.Permissions,
        done:         make(chan struct{}),
        states:       file.NewStates(),
        Channel:      make(chan []file.State, 1),
        flushTimeout: cfg.FlushTimeout,
        out:          out,
        wg:           sync.WaitGroup{},
    }
    return r, r.Init()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后返回Registrar结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Registrar struct {
    Channel      chan []file.State
    out          successLogger
    done         chan struct{}
    registryFile string      // Path to the Registry File
    fileMode     os.FileMode // Permissions to apply on the Registry File
    wg           sync.WaitGroup

    states               *file.States // Map with all file paths inside and the corresponding state
    gcRequired           bool         // gcRequired is set if registry state needs to be gc&#39;ed before the next write
    gcEnabled            bool         // gcEnabled indicates the registry contains some state that can be gc&#39;ed in the future
    flushTimeout         time.Duration
    bufferedStateUpdates int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还进行了初始化，主要是对文件进行了一些检查。然后就是启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Start the registrar
err = registrar.Start()
if err != nil {
    return fmt.Errorf(&amp;quot;Could not start registrar: %v&amp;quot;, err)
}

func (r *Registrar) Start() error {
    // Load the previous log file locations now, for use in input
    err := r.loadStates()
    if err != nil {
        return fmt.Errorf(&amp;quot;Error loading state: %v&amp;quot;, err)
    }

    r.wg.Add(1)
    go r.Run()

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先就是加载了目前存在的文件的状态来赋值给结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// loadStates fetches the previous reading state from the configure RegistryFile file
// The default file is `registry` in the data path.
func (r *Registrar) loadStates() error {
    f, err := os.Open(r.registryFile)
    if err != nil {
        return err
    }

    defer f.Close()

    logp.Info(&amp;quot;Loading registrar data from %s&amp;quot;, r.registryFile)

    states, err := readStatesFrom(f)
    if err != nil {
        return err
    }
    r.states.SetStates(states)
    logp.Info(&amp;quot;States Loaded from registrar: %+v&amp;quot;, len(states))

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后开始监听来更新文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Registrar) Run() {
    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Starting Registrar&amp;quot;)
    // Writes registry on shutdown
    defer func() {
        r.writeRegistry()
        r.wg.Done()
    }()

    var (
        timer  *time.Timer
        flushC &amp;lt;-chan time.Time
    )

    for {
        select {
        case &amp;lt;-r.done:
            logp.Info(&amp;quot;Ending Registrar&amp;quot;)
            return
        case &amp;lt;-flushC:
            flushC = nil
            timer.Stop()
            r.flushRegistry()
        case states := &amp;lt;-r.Channel:
            r.onEvents(states)
            if r.flushTimeout &amp;lt;= 0 {
                r.flushRegistry()
            } else if flushC == nil {
                timer = time.NewTimer(r.flushTimeout)
                flushC = timer.C
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当接受到Registrar中channel的发来的文件状态，就更新结构体的值，如果到时间了就将内存中的值刷新到本地文件中，如果没有就定一个timeout时间后刷新到本地文件中。&lt;/p&gt;

&lt;p&gt;我们可以简单的看一下这个channel&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Channel:      make(chan []file.State, 1),
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是一个文件状态的channel，关于文件状态的结构体如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// State is used to communicate the reading state of a file
type State struct {
    Id          string            `json:&amp;quot;-&amp;quot;` // local unique id to make comparison more efficient
    Finished    bool              `json:&amp;quot;-&amp;quot;` // harvester state
    Fileinfo    os.FileInfo       `json:&amp;quot;-&amp;quot;` // the file info
    Source      string            `json:&amp;quot;source&amp;quot;`
    Offset      int64             `json:&amp;quot;offset&amp;quot;`
    Timestamp   time.Time         `json:&amp;quot;timestamp&amp;quot;`
    TTL         time.Duration     `json:&amp;quot;ttl&amp;quot;`
    Type        string            `json:&amp;quot;type&amp;quot;`
    Meta        map[string]string `json:&amp;quot;meta&amp;quot;`
    FileStateOS file.StateOS
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记录在registry文件中的数据大致如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[{&amp;quot;source&amp;quot;:&amp;quot;/tmp/aa.log&amp;quot;,&amp;quot;offset&amp;quot;:48,&amp;quot;timestamp&amp;quot;:&amp;quot;2019-07-03T13:54:01.298995+08:00&amp;quot;,&amp;quot;ttl&amp;quot;:-1,&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;meta&amp;quot;:null,&amp;quot;FileStateOS&amp;quot;:{&amp;quot;inode&amp;quot;:7048952,&amp;quot;device&amp;quot;:16777220}}]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于文件可能会被改名或移动，filebeat会根据inode和设备号来标志每个日志文件。&lt;/p&gt;

&lt;p&gt;到这边registrar启动也结束了，下面就是监控registrar中channel的数据，在启动的时候还做了一件事情，那就是把channel设置到pipeline中去。&lt;/p&gt;

&lt;p&gt;在构建registrar的时候，通过registrar中channel构建一个结构体registrarLogger&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type registrarLogger struct {
    done chan struct{}
    ch   chan&amp;lt;- []file.State
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是用来交互的结构体,这个结构体中的channel获取的文件状态就是给上面的监听程序进行处理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Make sure all events that were published in
registrarChannel := newRegistrarLogger(registrar)

func newRegistrarLogger(reg *registrar.Registrar) *registrarLogger {
    return &amp;amp;registrarLogger{
        done: make(chan struct{}),
        ch:   reg.Channel,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过这个registrarLogger结构体，做了如下的调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err = b.Publisher.SetACKHandler(beat.PipelineACKHandler{
    ACKEvents: newEventACKer(finishedLogger, registrarChannel).ackEvents,
})
if err != nil {
    logp.Err(&amp;quot;Failed to install the registry with the publisher pipeline: %v&amp;quot;, err)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先看一下newEventACKer(finishedLogger, registrarChannel)这个结构体的ackEvents函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACKer(stateless statelessLogger, stateful statefulLogger) *eventACKer {
    return &amp;amp;eventACKer{stateless: stateless, stateful: stateful, log: logp.NewLogger(&amp;quot;acker&amp;quot;)}
}

func (a *eventACKer) ackEvents(data []interface{}) {
    stateless := 0
    states := make([]file.State, 0, len(data))
    for _, datum := range data {
        if datum == nil {
            stateless++
            continue
        }

        st, ok := datum.(file.State)
        if !ok {
            stateless++
            continue
        }

        states = append(states, st)
    }

    if len(states) &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateful ack&amp;quot;, &amp;quot;count&amp;quot;, len(states))
        a.stateful.Published(states)
    }

    if stateless &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateless ack&amp;quot;, &amp;quot;count&amp;quot;, stateless)
        a.stateless.Published(stateless)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见是一个eventACKer结构体的函数赋值给了beat.PipelineACKHandler的成员函数，我们再来看一下beat.PipelineACKHandler&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// PipelineACKHandler configures some pipeline-wide event ACK handler.
type PipelineACKHandler struct {
    // ACKCount reports the number of published events recently acknowledged
    // by the pipeline.
    ACKCount func(int)

    // ACKEvents reports the events recently acknowledged by the pipeline.
    // Only the events &#39;Private&#39; field will be reported.
    ACKEvents func([]interface{})

    // ACKLastEvent reports the last ACKed event per pipeline client.
    // Only the events &#39;Private&#39; field will be reported.
    ACKLastEvents func([]interface{})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是一个这样的结构体作为参数，最后我们来看一下SetACKHandler这个函数的调用，首先b的就是libbeat中创建的beat，其中的Publisher就是对应的初始化的Pipeline，看一下Pipeline的SetACKHandler方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// SetACKHandler sets a global ACK handler on all events published to the pipeline.
// SetACKHandler must be called before any connection is made.
func (p *Pipeline) SetACKHandler(handler beat.PipelineACKHandler) error {
    p.eventer.mutex.Lock()
    defer p.eventer.mutex.Unlock()

    if !p.eventer.modifyable {
        return errors.New(&amp;quot;can not set ack handler on already active pipeline&amp;quot;)
    }

    // TODO: check only one type being configured

    cb, err := newPipelineEventCB(handler)
    if err != nil {
        return err
    }

    if cb == nil {
        p.ackBuilder = &amp;amp;pipelineEmptyACK{p}
        p.eventer.cb = nil
        return nil
    }

    p.eventer.cb = cb
    if cb.mode == countACKMode {
        p.ackBuilder = &amp;amp;pipelineCountACK{
            pipeline: p,
            cb:       cb.onCounts,
        }
    } else {
        p.ackBuilder = &amp;amp;pipelineEventsACK{
            pipeline: p,
            cb:       cb.onEvents,
        }
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;newPipelineEventCB是根据传递的不同函数，创建不同mode的pipelineEventCB结构体，启动goroutine来work。我们这边传递的是ACKEvents，设置mode为eventsACKMode&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newPipelineEventCB(handler beat.PipelineACKHandler) (*pipelineEventCB, error) {
    mode := noACKMode
    if handler.ACKCount != nil {
        mode = countACKMode
    }
    if handler.ACKEvents != nil {
        if mode != noACKMode {
            return nil, errors.New(&amp;quot;only one callback can be set&amp;quot;)
        }
        mode = eventsACKMode
    }
    if handler.ACKLastEvents != nil {
        if mode != noACKMode {
            return nil, errors.New(&amp;quot;only one callback can be set&amp;quot;)
        }
        mode = lastEventsACKMode
    }

    // yay, no work
    if mode == noACKMode {
        return nil, nil
    }

    cb := &amp;amp;pipelineEventCB{
        acks:          make(chan int),
        mode:          mode,
        handler:       handler,
        events:        make(chan eventsDataMsg),
        droppedEvents: make(chan eventsDataMsg),
    }
    go cb.worker()
    return cb, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看worker工作协程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) worker() {
    defer close(p.acks)
    defer close(p.events)
    defer close(p.droppedEvents)

    for {
        select {
        case count := &amp;lt;-p.acks:
            exit := p.collect(count)
            if exit {
                return
            }

            // short circuit dropped events, but have client block until all events
            // have been processed by pipeline ack handler
        case msg := &amp;lt;-p.droppedEvents:
            p.reportEventsData(msg.data, msg.total)
            if msg.sig != nil {
                close(msg.sig)
            }

        case &amp;lt;-p.done:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时对于不同的mode对p.ackBuilder进行了重新构建，因为是代码设置mode为eventsACKMode&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.ackBuilder = &amp;amp;pipelineEventsACK{
    pipeline: p,
    cb:       cb.onEvents,
}

type pipelineEventsACK struct {
    pipeline *Pipeline
    cb       func([]interface{}, int)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里启动就结束了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;input初始化的ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;crawler启动后构建新的input构建的时候，需要获取到pipeline的client，在使用ConnectWith进行构建的时候，会构建client的acker，第一次参数是processors != nil，影响后的结构体的创建，一般是true&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acker := p.makeACKer(processors != nil, &amp;amp;cfg, waitClose, client.unlink)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下makeACKer这个函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Pipeline) makeACKer(
    canDrop bool,
    cfg *beat.ClientConfig,
    waitClose time.Duration,
    afterClose func(),
) acker {
    var (
        bld   = p.ackBuilder
        acker acker
    )

    sema := p.eventSema
    switch {
    case cfg.ACKCount != nil:
        acker = bld.createCountACKer(canDrop, sema, cfg.ACKCount)
    case cfg.ACKEvents != nil:
        acker = bld.createEventACKer(canDrop, sema, cfg.ACKEvents)
    case cfg.ACKLastEvent != nil:
        cb := lastEventACK(cfg.ACKLastEvent)
        acker = bld.createEventACKer(canDrop, sema, cb)
    default:
        if waitClose &amp;lt;= 0 {
            acker = bld.createPipelineACKer(canDrop, sema)
        } else {
            acker = bld.createCountACKer(canDrop, sema, func(_ int) {})
        }
    }

    if waitClose &amp;lt;= 0 {
        return newCloseACKer(acker, afterClose)
    }
    return newWaitACK(acker, waitClose, afterClose)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要使用p.ackBuilder的create函数，我们在上面SetACKHandler的时候构建了p.ackBuilder，根据cfg配置调用，默认调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acker = bld.createEventACKer(canDrop, sema, cfg.ACKEvents)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *pipelineEventsACK) createEventACKer(canDrop bool, sema *sema, fn func([]interface{})) acker {
    return buildClientEventACK(b.pipeline, canDrop, sema, func(guard *clientACKer) func([]interface{}, int) {
        return func(data []interface{}, acked int) {
            b.cb(data, acked)
            if guard.Active() {
                fn(data)
            }
        }
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用函数buildClientEventACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func buildClientEventACK(
    pipeline *Pipeline,
    canDrop bool,
    sema *sema,
    mk func(*clientACKer) func([]interface{}, int),
) acker {
    guard := &amp;amp;clientACKer{}
    guard.lift(newEventACK(pipeline, canDrop, sema, mk(guard)))
    return guard
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下返回值clientACKer结构体，其成员acker的赋值就是eventDataACK。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACK(
    pipeline *Pipeline,
    canDrop bool,
    sema *sema,
    fn func([]interface{}, int),
) *eventDataACK {
    a := &amp;amp;eventDataACK{pipeline: pipeline, fn: fn}
    a.acker = makeCountACK(pipeline, canDrop, sema, a.onACK)

    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个eventDataACK的结构体，fn就是mk(guard)就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func(data []interface{}, acked int) {
    b.cb(data, acked)
    if guard.Active() {
        fn(data)
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后调用makeCountACK来赋值给eventDataACK的acker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeCountACK(pipeline *Pipeline, canDrop bool, sema *sema, fn func(int, int)) acker {
    if canDrop {
        return newBoundGapCountACK(pipeline, sema, fn)
    }
    return newCountACK(pipeline, fn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;canDrop之前说过了，就是ture，所以创建newBoundGapCountACK，将eventDataACK的onACK当参数传递进来。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *eventDataACK) onACK(total, acked int) {
    n := total

    a.mutex.Lock()
    data := a.data[:n]
    a.data = a.data[n:]
    a.mutex.Unlock()

    if len(data) &amp;gt; 0 &amp;amp;&amp;amp; a.pipeline.ackActive.Load() {
        a.fn(data, acked)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;继续newBoundGapCountACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBoundGapCountACK(
    pipeline *Pipeline,
    sema *sema,
    fn func(total, acked int),
) *boundGapCountACK {
    a := &amp;amp;boundGapCountACK{active: true, sema: sema, fn: fn}
    a.acker.init(pipeline, a.onACK)
    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建了一个boundGapCountACK，调用初始化函数，将这个结构体的onACK传进去就是fn&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) init(pipeline *Pipeline, fn func(int, int)) {
    *a = gapCountACK{
        pipeline: pipeline,
        fn:       fn,
        done:     make(chan struct{}),
        drop:     make(chan struct{}),
        acks:     make(chan int, 1),
    }

    init := &amp;amp;gapInfo{}
    a.lst.head = init
    a.lst.tail = init

    go a.ackLoop()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) ackLoop() {
    // close channels, as no more events should be ACKed:
    // - once pipeline is closed
    // - all events of the closed client have been acked/processed by the pipeline

    acks, drop := a.acks, a.drop
    closing := false

    for {
        select {
        case &amp;lt;-a.done:
            closing = true
            a.done = nil
            if a.events.Load() == 0 {
                // stop worker, if all events accounted for have been ACKed.
                // If new events are added after this acker won&#39;t handle them, which may
                // result in duplicates
                return
            }

        case &amp;lt;-a.pipeline.ackDone:
            return

        case n := &amp;lt;-acks:
            empty := a.handleACK(n)
            if empty &amp;amp;&amp;amp; closing &amp;amp;&amp;amp; a.events.Load() == 0 {
                // stop worker, if and only if all events accounted for have been ACKed
                return
            }

        case &amp;lt;-drop:
            // TODO: accumulate multiple drop events + flush count with timer
            a.events.Sub(1)
            a.fn(1, 0)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边启动了一个acker的监听，然后使用这个clientACKer结构体又构建了一个新的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newWaitACK(acker acker, timeout time.Duration, afterClose func()) *waitACK {
    return &amp;amp;waitACK{
        acker:      acker,
        signalAll:  make(chan struct{}, 1),
        signalDone: make(chan struct{}),
        waitClose:  timeout,
        active:     atomic.MakeBool(true),
        afterClose: afterClose,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里连接创建就结束了，创建的acker就是waitACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;client.acker = acker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以pipeline的client的acker就是waitACK。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;publish数据的ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面是有数据的时候将数据发送到pipeline，调用的client的publish函数，在发送数据的时候调用了addEvent。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open := c.acker.addEvent(e, publish)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c.acker也就是上面waitACK的addEvent函数，e就是对应发送的事件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *waitACK) addEvent(event beat.Event, published bool) bool {
    if published {
        a.events.Inc()
    }
    return a.acker.addEvent(event, published)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的是结构体成员acker的addEvent，也就是eventDataACK的addEvent。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *eventDataACK) addEvent(event beat.Event, published bool) bool {
    a.mutex.Lock()
    active := a.pipeline.ackActive.Load()
    if active {
        a.data = append(a.data, event.Private)
    }
    a.mutex.Unlock()

    if active {
        return a.acker.addEvent(event, published)
    }
    return false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边就是将数据传输到了data中，同时调用了其对应的acker的addEvent函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACK(
    pipeline *Pipeline,
    canDrop bool,
    sema *sema,
    fn func([]interface{}, int),
) *eventDataACK {
    a := &amp;amp;eventDataACK{pipeline: pipeline, fn: fn}
    a.acker = makeCountACK(pipeline, canDrop, sema, a.onACK)

    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看到a.acker = makeCountACK(pipeline, canDrop, sema, a.onACK)，再看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeCountACK(pipeline *Pipeline, canDrop bool, sema *sema, fn func(int, int)) acker {
    if canDrop {
        return newBoundGapCountACK(pipeline, sema, fn)
    }
    return newCountACK(pipeline, fn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;canDrop上面说明过了，所以是newBoundGapCountACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBoundGapCountACK(
    pipeline *Pipeline,
    sema *sema,
    fn func(total, acked int),
) *boundGapCountACK {
    a := &amp;amp;boundGapCountACK{active: true, sema: sema, fn: fn}
    a.acker.init(pipeline, a.onACK)
    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以返回的结构体是boundGapCountACK，调用的也是这个结构体的addEvent&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *boundGapCountACK) addEvent(event beat.Event, published bool) bool {
    a.sema.inc()
    return a.acker.addEvent(event, published)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有acker.addEvent，再看boundGapCountACK的acker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type boundGapCountACK struct {
    active bool
    fn     func(total, acked int)

    acker gapCountACK
    sema  *sema
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;是一个gapCountACK的结构体，调用初始化a.acker.init(pipeline, a.onACK)来赋值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) init(pipeline *Pipeline, fn func(int, int)) {
    *a = gapCountACK{
        pipeline: pipeline,
        fn:       fn,
        done:     make(chan struct{}),
        drop:     make(chan struct{}),
        acks:     make(chan int, 1),
    }

    init := &amp;amp;gapInfo{}
    a.lst.head = init
    a.lst.tail = init

    go a.ackLoop()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时启动了一个监听程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) ackLoop() {
    // close channels, as no more events should be ACKed:
    // - once pipeline is closed
    // - all events of the closed client have been acked/processed by the pipeline

    acks, drop := a.acks, a.drop
    closing := false

    for {
        select {
        case &amp;lt;-a.done:
            closing = true
            a.done = nil
            if a.events.Load() == 0 {
                // stop worker, if all events accounted for have been ACKed.
                // If new events are added after this acker won&#39;t handle them, which may
                // result in duplicates
                return
            }

        case &amp;lt;-a.pipeline.ackDone:
            return

        case n := &amp;lt;-acks:
            empty := a.handleACK(n)
            if empty &amp;amp;&amp;amp; closing &amp;amp;&amp;amp; a.events.Load() == 0 {
                // stop worker, if and only if all events accounted for have been ACKed
                return
            }

        case &amp;lt;-drop:
            // TODO: accumulate multiple drop events + flush count with timer
            a.events.Sub(1)
            a.fn(1, 0)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当drop获取到信号的时候，就会调用fn也就是boundGapCountACK的onACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *boundGapCountACK) onACK(total, acked int) {
    a.sema.release(total)
    a.fn(total, acked)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边会调用到fn也就是eventDataACK的onACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *eventDataACK) onACK(total, acked int) {
    n := total

    a.mutex.Lock()
    data := a.data[:n]
    a.data = a.data[n:]
    a.mutex.Unlock()

    if len(data) &amp;gt; 0 &amp;amp;&amp;amp; a.pipeline.ackActive.Load() {
        a.fn(data, acked)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边会调用fn也就是我们创建的ackBuilder的cb成员也就是我们的ackBuilder结构的cb函数，也就是我们的onEvents&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) onEvents(data []interface{}, acked int) {
    p.pushMsg(eventsDataMsg{data: data, total: len(data), acked: acked})
}

func (p *pipelineEventCB) onCounts(total, acked int) {
    p.pushMsg(eventsDataMsg{total: total, acked: acked})
}

func (p *pipelineEventCB) pushMsg(msg eventsDataMsg) {
    if msg.acked == 0 {
        p.droppedEvents &amp;lt;- msg
    } else {
        msg.sig = make(chan struct{})
        p.events &amp;lt;- msg
        &amp;lt;-msg.sig
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取到了file的具体数据。到这边就是继续监听，我们先看addEvent，published肯定是true，正常都是有事件的publish = event != nil&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) addEvent(_ beat.Event, published bool) bool {
    // if gapList is empty and event is being dropped, forward drop event to ack
    // loop worker:

    a.events.Inc()
    if !published {
        a.addDropEvent()
    } else {
        a.addPublishedEvent()
    }

    return true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以看addPublishedEvent，只是给结构体成员send加一&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) addPublishedEvent() {
    // event is publisher -&amp;gt; add a new gap list entry if gap is present in current
    // gapInfo

    a.lst.Lock()

    current := a.lst.tail
    current.Lock()

    if current.dropped &amp;gt; 0 {
        tmp := &amp;amp;gapInfo{}
        a.lst.tail.next = tmp
        a.lst.tail = tmp

        current.Unlock()
        tmp.Lock()
        current = tmp
    }

    a.lst.Unlock()

    current.send++
    current.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边调用的addevent就结束了，下面就是等待output的publish后的返回调用。上面已经有四个相关ack的监听，一个queue消费的监听，一个registry监听，一个是pipeline的监听，一个gapCountACK的ackLoop。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;publish后回调ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后就是output的publish的时候进行回调了，我们使用的是kafka，kafka在connect的时候会新建两个协程，来监听发送的情况，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) Connect() error {
    c.mux.Lock()
    defer c.mux.Unlock()

    debugf(&amp;quot;connect: %v&amp;quot;, c.hosts)

    // try to connect
    producer, err := sarama.NewAsyncProducer(c.hosts, &amp;amp;c.config)
    if err != nil {
        logp.Err(&amp;quot;Kafka connect fails with: %v&amp;quot;, err)
        return err
    }

    c.producer = producer

    c.wg.Add(2)
    go c.successWorker(producer.Successes())
    go c.errorWorker(producer.Errors())

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们一共可以看到两个处理方式，一个成功一个失败，producer.Successes()和producer.Errors()为这个producer的成功和错误返回channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) successWorker(ch &amp;lt;-chan *sarama.ProducerMessage) {
    defer c.wg.Done()
    defer debugf(&amp;quot;Stop kafka ack worker&amp;quot;)

    for libMsg := range ch {
        msg := libMsg.Metadata.(*message)
        msg.ref.done()
    }
}

func (c *client) errorWorker(ch &amp;lt;-chan *sarama.ProducerError) {
    defer c.wg.Done()
    defer debugf(&amp;quot;Stop kafka error handler&amp;quot;)

    for errMsg := range ch {
        msg := errMsg.Msg.Metadata.(*message)
        msg.ref.fail(msg, errMsg.Err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下成功的响应，失败也是一样的，只不过多了一个错误处理，有兴趣可以自己看一下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *msgRef) done() {
    r.dec()
}

func (r *msgRef) dec() {
    i := atomic.AddInt32(&amp;amp;r.count, -1)
    if i &amp;gt; 0 {
        return
    }

    debugf(&amp;quot;finished kafka batch&amp;quot;)
    stats := r.client.observer

    err := r.err
    if err != nil {
        failed := len(r.failed)
        success := r.total - failed
        r.batch.RetryEvents(r.failed)

        stats.Failed(failed)
        if success &amp;gt; 0 {
            stats.Acked(success)
        }

        debugf(&amp;quot;Kafka publish failed with: %v&amp;quot;, err)
    } else {
        r.batch.ACK()
        stats.Acked(r.total)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在else中也就是接受到成功发送信号后调用了batch.ACK()。我们来看一下batch，首先是msg的类型转化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;msg := errMsg.Msg.Metadata.(*message)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;转化为我们定义的kafka的message的结构体message&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type message struct {
    msg sarama.ProducerMessage

    topic string
    key   []byte
    value []byte
    ref   *msgRef
    ts    time.Time

    hash      uint32
    partition int32

    data publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在kafka的client使用publish的时候初始化了ref，给batch赋值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ref := &amp;amp;msgRef{
        client: c,
        count:  int32(len(events)),
        total:  len(events),
        failed: nil,
        batch:  batch,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个batch就是重netClientWorker的qu workQueue中获取的，看一下这个channel是bantch类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type workQueue chan *Batch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个batch就是我们需要找的结构，发送kafka成功后就是调用这个结构他的ACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Batch struct {
    original queue.Batch
    ctx      *batchContext
    ttl      int
    events   []publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下Batch的ACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Batch) ACK() {
    b.ctx.observer.outBatchACKed(len(b.events))
    b.original.ACK()
    releaseBatch(b)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边继续调用ACK，我们需要看一下b.original赋值，赋值都会调用newBatch函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBatch(ctx *batchContext, original queue.Batch, ttl int) *Batch {
    if original == nil {
        panic(&amp;quot;empty batch&amp;quot;)
    }

    b := batchPool.Get().(*Batch)
    *b = Batch{
        original: original,
        ctx:      ctx,
        ttl:      ttl,
        events:   original.Events(),
    }
    return b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只在eventConsumer消费的时候调用了newBatch，通过get方法获取的queueBatch给了他&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;queueBatch, err := consumer.Get(c.out.batchSize)
if err != nil {
    out = nil
    consumer = nil
    continue
}
if queueBatch != nil {
    batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先consumer是基于mem的，看一下get方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *consumer) Get(sz int) (queue.Batch, error) {
    // log := c.broker.logger

    if c.closed.Load() {
        return nil, io.EOF
    }

    select {
    case c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:
    case &amp;lt;-c.done:
        return nil, io.EOF
    }

    // if request has been send, we do have to wait for a response
    resp := &amp;lt;-c.resp
    return &amp;amp;batch{
        consumer: c,
        events:   resp.buf,
        ack:      resp.ack,
        state:    batchActive,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到返回使用的是结构体batch如下，我们简单看一下需要先向队列请求channel发送getRequest结构体，等待resp的返回来创建下面的结构体。具体的处理逻辑可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/#pipeline-的消费过程&#34;&gt;pipeline的消费&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type batch struct {
    consumer     *consumer
    events       []publisher.Event
    clientStates []clientState
    ack          *ackChan
    state        ackState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下batch的ACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *batch) ACK() {
    if b.state != batchActive {
        switch b.state {
        case batchACK:
            panic(&amp;quot;Can not acknowledge already acknowledged batch&amp;quot;)
        default:
            panic(&amp;quot;inactive batch&amp;quot;)
        }
    }

    b.report()
}

func (b *batch) report() {
    b.ack.ch &amp;lt;- batchAckMsg{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是给batch的ack也就是getResponse的ack的ch发送了一个信号batchAckMsg{}。这个ch接收到信号，牵涉到一个完整的消费的调度过程。&lt;/p&gt;

&lt;p&gt;我们先看一下正常的消费调度，在上面说过，首先在有数据发送到queue的时候，consumer会获取这个数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; consumer != nil &amp;amp;&amp;amp; batch == nil {
            out = c.out.workQueue
            queueBatch, err := consumer.Get(c.out.batchSize)
            ...
            batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
        }
        ...
        select {
        case &amp;lt;-c.done:
            return
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
        case out &amp;lt;- batch:
            batch = nil
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面consumer.Get就是消费者consumer从Broker中获取日志数据，然后发送至out的channel中被output client发送，我们看一下Get方法里的核心代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *consumer) Get(sz int) (queue.Batch, error) {
    // log := c.broker.logger

    if c.closed.Load() {
        return nil, io.EOF
    }

    select {
    case c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:
    case &amp;lt;-c.done:
        return nil, io.EOF
    }

    // if request has been send, we do have to wait for a response
    resp := &amp;lt;-c.resp
    return &amp;amp;batch{
        consumer: c,
        events:   resp.buf,
        ack:      resp.ack,
        state:    batchActive,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}时候,我们需要看一下bufferingEventLoop的调度中心的响应。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) run() {
    var (
        broker = l.broker
    )

    for {
        select {
        case &amp;lt;-broker.done:
            return

        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)

        case req := &amp;lt;-l.pubCancel: // producer cancelling active events
            l.handleCancel(&amp;amp;req)

        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)

        case l.schedACKS &amp;lt;- l.pendingACKs:
            l.schedACKS = nil
            l.pendingACKs = chanList{}

        case count := &amp;lt;-l.acks:
            l.handleACK(count)

        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;l.get会得到信息，为什么l.get会得到信息，因为l.get = l.broker.requests&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) flushBuffer() {
    l.buf.flushed = true

    if l.buf.length() == 0 {
        panic(&amp;quot;flushing empty buffer&amp;quot;)
    }

    l.flushList.add(l.buf)
    l.get = l.broker.requests
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;l.get得到信息后handleConsumer如何处理信息的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) handleConsumer(req *getRequest) {
    buf := l.flushList.head
    if buf == nil {
        panic(&amp;quot;get from non-flushed buffers&amp;quot;)
    }

    count := buf.length()
    if count == 0 {
        panic(&amp;quot;empty buffer in flush list&amp;quot;)
    }

    if sz := req.sz; sz &amp;gt; 0 {
        if sz &amp;lt; count {
            count = sz
        }
    }

    if count == 0 {
        panic(&amp;quot;empty batch returned&amp;quot;)
    }

    events := buf.events[:count]
    clients := buf.clients[:count]
    ackChan := newACKChan(l.ackSeq, 0, count, clients)
    l.ackSeq++

    req.resp &amp;lt;- getResponse{ackChan, events}
    l.pendingACKs.append(ackChan)
    l.schedACKS = l.broker.scheduledACKs

    buf.events = buf.events[count:]
    buf.clients = buf.clients[count:]
    if buf.length() == 0 {
        l.advanceFlushList()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到获取到数据封装getResponse发送给output，我们这边不看这个数据具体发送到workqueue，而是关心的是ack。&lt;/p&gt;

&lt;p&gt;先看ack构建的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newACKChan(seq uint, start, count int, states []clientState) *ackChan {
    ch := ackChanPool.Get().(*ackChan)
    ch.next = nil
    ch.seq = seq
    ch.start = start
    ch.count = count
    ch.states = states
    return ch
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将这个ackChan新增到chanlist的链表中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;l.pendingACKs.append(ackChan)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将l.schedACKS = l.broker.scheduledACKs，一开始调度的时候l.schedACKS是阻塞的，l.pendingACKs不能写入到l.schedACKS，但是这边进行赋值后就是将l.pendingACKs写入到l.broker.scheduledACKs，这个在初始化的时候是有缓存的。就直接写入了，然后bufferingEventLoop调度中心将这个数据清空，bufferingEventLoop的ack的调度获取到这个chanenl&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *ackLoop) run() {
    var (
        // log = l.broker.logger

        // Buffer up acked event counter in acked. If acked &amp;gt; 0, acks will be set to
        // the broker.acks channel for sending the ACKs while potentially receiving
        // new batches from the broker event loop.
        // This concurrent bidirectionally communication pattern requiring &#39;select&#39;
        // ensures we can not have any deadlock between the event loop and the ack
        // loop, as the ack loop will not block on any channel
        acked int
        acks  chan int
    )

    for {
        select {
        case &amp;lt;-l.broker.done:
            // TODO: handle pending ACKs?
            // TODO: panic on pending batches?
            return

        case acks &amp;lt;- acked:
            acks, acked = nil, 0

        case lst := &amp;lt;-l.broker.scheduledACKs:
            count, events := lst.count()
            l.lst.concat(&amp;amp;lst)

            // log.Debug(&amp;quot;ACK List:&amp;quot;)
            // for current := l.lst.head; current != nil; current = current.next {
            //  log.Debugf(&amp;quot;  ack entry(seq=%v, start=%v, count=%v&amp;quot;,
            //      current.seq, current.start, current.count)
            // }

            l.batchesSched += uint64(count)
            l.totalSched += uint64(events)

        case &amp;lt;-l.sig:
            acked += l.handleBatchSig()
            if acked &amp;gt; 0 {
                acks = l.broker.acks
            }
        }

        // log.Debug(&amp;quot;ackloop INFO&amp;quot;)
        // log.Debug(&amp;quot;ackloop:   total events scheduled = &amp;quot;, l.totalSched)
        // log.Debug(&amp;quot;ackloop:   total events ack = &amp;quot;, l.totalACK)
        // log.Debug(&amp;quot;ackloop:   total batches scheduled = &amp;quot;, l.batchesSched)
        // log.Debug(&amp;quot;ackloop:   total batches ack = &amp;quot;, l.batchesACKed)

        l.sig = l.lst.channel()
        // if l.sig == nil {
        //  log.Debug(&amp;quot;ackloop: no ack scheduled&amp;quot;)
        // } else {
        //  log.Debug(&amp;quot;ackloop: schedule ack: &amp;quot;, l.lst.head.seq)
        // }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;lst := &amp;lt;-l.broker.scheduledACKs就是获取到那个请求是新建的ackchan的channel，也就是获取到了batch回调的ack的函数的信号。也就是&amp;lt;-l.sig获取到了信号，调用handleBatchSig&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// handleBatchSig collects and handles a batch ACK/Cancel signal. handleBatchSig
// is run by the ackLoop.
func (l *ackLoop) handleBatchSig() int {
    lst := l.collectAcked()

    count := 0
    for current := lst.front(); current != nil; current = current.next {
        count += current.count
    }

    if count &amp;gt; 0 {
        if e := l.broker.eventer; e != nil {
            e.OnACK(count)
        }

        // report acks to waiting clients
        l.processACK(lst, count)
    }

    for !lst.empty() {
        releaseACKChan(lst.pop())
    }

    // return final ACK to EventLoop, in order to clean up internal buffer
    l.broker.logger.Debug(&amp;quot;ackloop: return ack to broker loop:&amp;quot;, count)

    l.totalACK += uint64(count)
    l.broker.logger.Debug(&amp;quot;ackloop:  done send ack&amp;quot;)
    return count
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;l.broker.eventer我们可以追溯一下，broker就是创建queue的是newACKLoop的时候传递的，broker也是在这个时候初始化的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b := &amp;amp;Broker{
        done:   make(chan struct{}),
        logger: logger,

        // broker API channels
        events:    make(chan pushRequest, chanSize),
        requests:  make(chan getRequest),
        pubCancel: make(chan producerCancelRequest, 5),

        // internal broker and ACK handler channels
        acks:          make(chan int),
        scheduledACKs: make(chan chanList),

        waitOnClose: settings.WaitOnClose,

        eventer: settings.Eventer,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;eventer是create的时候传递的，create回传的时候是create的方法，真正调用是在pipeline初始化的时候new新建pipeline的时候&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.queue, err = queueFactory(&amp;amp;p.eventer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以说l.broker.eventer就是p.eventer也就是结构体pipelineEventer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type pipelineEventer struct {
    mutex      sync.Mutex
    modifyable bool

    observer  queueObserver
    waitClose *waitCloser
    cb        *pipelineEventCB
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在pipeline中设置registry的时候p.eventer.cb就是我们创建的pipelineEventCB：p.eventer.cb = cb&lt;/p&gt;

&lt;p&gt;到这边可以看出e.OnACK(count)就是调用pipelineEventer的成员函数OnACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (e *pipelineEventer) OnACK(n int) {
    e.observer.queueACKed(n)

    if wc := e.waitClose; wc != nil {
        wc.dec(n)
    }
    if e.cb != nil {
        e.cb.reportQueueACK(n)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候就调用我们最初用的pipelineEventCB的reportQueueACK函数，就是将acked发送到了p.acks &amp;lt;- acked中，这个时候pipelineEventCB的监听程序监听到acks信号。&lt;/p&gt;

&lt;p&gt;收到ack的channel信息，调用collect函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) collect(count int) (exit bool) {
    var (
        signalers []chan struct{}
        data      []interface{}
        acked     int
        total     int
    )

    for acked &amp;lt; count {
        var msg eventsDataMsg
        select {
        case msg = &amp;lt;-p.events:
        case msg = &amp;lt;-p.droppedEvents:
        case &amp;lt;-p.done:
            exit = true
            return
        }

        if msg.sig != nil {
            signalers = append(signalers, msg.sig)
        }
        total += msg.total
        acked += msg.acked

        if count-acked &amp;lt; 0 {
            panic(&amp;quot;ack count mismatch&amp;quot;)
        }

        switch p.mode {
        case eventsACKMode:
            data = append(data, msg.data...)

        case lastEventsACKMode:
            if L := len(msg.data); L &amp;gt; 0 {
                data = append(data, msg.data[L-1])
            }
        }
    }

    // signal clients we processed all active ACKs, as reported by queue
    for _, sig := range signalers {
        close(sig)
    }
    p.reportEventsData(data, total)
    return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重pipelineEventCB中的events和droppedEvents中读取数据信息，然后进行上报reportEventsData&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) reportEventsData(data []interface{}, total int) {
    // report ACK back to the beat
    switch p.mode {
    case countACKMode:
        p.handler.ACKCount(total)
    case eventsACKMode:
        p.handler.ACKEvents(data)
    case lastEventsACKMode:
        p.handler.ACKLastEvents(data)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边就调用到一开始的ACKEvents函数，对数据进行处理，其实这些数据就是[]file.State文件信息。在创建pipelineEventCB的时候，也就是在pipeline使用set函数的时候，我们这边传递的是ACKEvents，所以调用的是newEventACKer(finishedLogger, registrarChannel)这个结构体的ackEvents函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACKer(stateless statelessLogger, stateful statefulLogger) *eventACKer {
    return &amp;amp;eventACKer{stateless: stateless, stateful: stateful, log: logp.NewLogger(&amp;quot;acker&amp;quot;)}
}

func (a *eventACKer) ackEvents(data []interface{}) {
    stateless := 0
    states := make([]file.State, 0, len(data))
    for _, datum := range data {
        if datum == nil {
            stateless++
            continue
        }

        st, ok := datum.(file.State)
        if !ok {
            stateless++
            continue
        }

        states = append(states, st)
    }

    if len(states) &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateful ack&amp;quot;, &amp;quot;count&amp;quot;, len(states))
        a.stateful.Published(states)
    }

    if stateless &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateless ack&amp;quot;, &amp;quot;count&amp;quot;, stateless)
        a.stateless.Published(stateless)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的这个registrarLogger的Published来完成文件状态的推送&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *registrarLogger) Published(states []file.State) {
    select {
    case &amp;lt;-l.done:
        // set ch to nil, so no more events will be send after channel close signal
        // has been processed the first time.
        // Note: nil channels will block, so only done channel will be actively
        //       report &#39;closed&#39;.
        l.ch = nil
    case l.ch &amp;lt;- states:
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;文件持久化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;registrarLogger的channel获取的信息是如何处理的？其实是Registrar的channel接受到了信息，在一开始Registrar就启动了监听channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    select {
    case &amp;lt;-r.done:
        logp.Info(&amp;quot;Ending Registrar&amp;quot;)
        return
    case &amp;lt;-flushC:
        flushC = nil
        timer.Stop()
        r.flushRegistry()
    case states := &amp;lt;-r.Channel:
        r.onEvents(states)
        if r.flushTimeout &amp;lt;= 0 {
            r.flushRegistry()
        } else if flushC == nil {
            timer = time.NewTimer(r.flushTimeout)
            flushC = timer.C
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接收到文件状态后调用onEvents&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// onEvents processes events received from the publisher pipeline
func (r *Registrar) onEvents(states []file.State) {
    r.processEventStates(states)
    r.bufferedStateUpdates += len(states)

    // check if we need to enable state cleanup
    if !r.gcEnabled {
        for i := range states {
            if states[i].TTL &amp;gt;= 0 || states[i].Finished {
                r.gcEnabled = true
                break
            }
        }
    }

    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Registrar state updates processed. Count: %v&amp;quot;, len(states))

    // new set of events received -&amp;gt; mark state registry ready for next
    // cleanup phase in case gc&#39;able events are stored in the registry.
    r.gcRequired = r.gcEnabled
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过processEventStates来处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// processEventStates gets the states from the events and writes them to the registrar state
func (r *Registrar) processEventStates(states []file.State) {
    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Processing %d events&amp;quot;, len(states))

    ts := time.Now()
    for i := range states {
        r.states.UpdateWithTs(states[i], ts)
        statesUpdate.Add(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是states的更新UpdateWithTs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// UpdateWithTs updates a state, assigning the given timestamp.
// If previous state didn&#39;t exist, new one is created
func (s *States) UpdateWithTs(newState State, ts time.Time) {
    s.Lock()
    defer s.Unlock()

    id := newState.ID()
    index := s.findPrevious(id)
    newState.Timestamp = ts

    if index &amp;gt;= 0 {
        s.states[index] = newState
    } else {
        // No existing state found, add new one
        s.idx[id] = len(s.states)
        s.states = append(s.states, newState)
        logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;New state added for %s&amp;quot;, newState.Source)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边states的状态就发生变化，再来看看states的初始化操作，其实就是在Registrar的New的时候调用了NewStates进行了初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r := &amp;amp;Registrar{
    registryFile: dataFile,
    fileMode:     cfg.Permissions,
    done:         make(chan struct{}),
    states:       file.NewStates(),
    Channel:      make(chan []file.State, 1),
    flushTimeout: cfg.FlushTimeout,
    out:          out,
    wg:           sync.WaitGroup{},
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Registrar的for循环的时候，定时会对状态进行写文件操作，调用flushRegistry的writeRegistry来完成文件的持久化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Registrar) flushRegistry() {
    if err := r.writeRegistry(); err != nil {
        logp.Err(&amp;quot;Writing of registry returned error: %v. Continuing...&amp;quot;, err)
    }

    if r.out != nil {
        r.out.Published(r.bufferedStateUpdates)
    }
    r.bufferedStateUpdates = 0
}

// writeRegistry writes the new json registry file to disk.
func (r *Registrar) writeRegistry() error {
    // First clean up states
    r.gcStates()
    states := r.states.GetStates()
    statesCurrent.Set(int64(len(states)))

    registryWrites.Inc()

    tempfile, err := writeTmpFile(r.registryFile, r.fileMode, states)
    if err != nil {
        registryFails.Inc()
        return err
    }

    err = helper.SafeFileRotate(r.registryFile, tempfile)
    if err != nil {
        registryFails.Inc()
        return err
    }

    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Registry file updated. %d states written.&amp;quot;, len(states))
    registrySuccess.Inc()

    return nil
}

func writeTmpFile(baseName string, perm os.FileMode, states []file.State) (string, error) {
    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Write registry file: %s (%v)&amp;quot;, baseName, len(states))

    tempfile := baseName + &amp;quot;.new&amp;quot;
    f, err := os.OpenFile(tempfile, os.O_RDWR|os.O_CREATE|os.O_TRUNC|os.O_SYNC, perm)
    if err != nil {
        logp.Err(&amp;quot;Failed to create tempfile (%s) for writing: %s&amp;quot;, tempfile, err)
        return &amp;quot;&amp;quot;, err
    }

    defer f.Close()

    encoder := json.NewEncoder(f)

    if err := encoder.Encode(states); err != nil {
        logp.Err(&amp;quot;Error when encoding the states: %s&amp;quot;, err)
        return &amp;quot;&amp;quot;, err
    }

    // Commit the changes to storage to avoid corrupt registry files
    if err = f.Sync(); err != nil {
        logp.Err(&amp;quot;Error when syncing new registry file contents: %s&amp;quot;, err)
        return &amp;quot;&amp;quot;, err
    }

    return tempfile, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对以上的过程最一个简单的总结&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/registry.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;特殊情况&#34;&gt;特殊情况&lt;/h3&gt;

&lt;p&gt;1.如果filebeat异常重启，每次采集harvester启动的时候都会读取registry文件，从上次记录的状态继续采集，确保不会从头开始重复发送所有的日志文件。
当然，如果日志发送过程中，还没来得及返回ack，filebeat就挂掉，registry文件肯定不会更新至最新的状态，那么下次采集的时候，这部分的日志就会重复发送，所以这意味着filebeat只能保证at least once，无法保证不重复发送。
还有一个比较异常的情况是，linux下如果老文件被移除，新文件马上创建，很有可能它们有相同的inode，而由于filebeat根据inode来标志文件记录采集的偏移，会导致registry里记录的其实是被移除的文件State状态，这样新的文件采集却从老的文件Offset开始，从而会遗漏日志数据。
为了尽量避免inode被复用的情况，同时防止registry文件随着时间增长越来越大，建议使用clean_inactive和clean_remove配置将长时间未更新或者被删除的文件State从registry中移除。&lt;/p&gt;

&lt;p&gt;2.在harvester读取日志中，会更新registry的状态处理一些异常场景。例如，如果一个日志文件被清空，filebeat会在下一次Reader.Next方法中返回ErrFileTruncate异常，将inode标志文件的Offset置为0，结束这次harvester，重新启动新的harvester，虽然文件不变，但是registry中的Offset为0，采集会从头开始。&lt;/p&gt;

&lt;p&gt;3.如果使用容器部署filebeat，需要将registry文件挂载到宿主机上，否则容器重启后registry文件丢失，会使filebeat从头开始重复采集日志文件。&lt;/p&gt;

&lt;h4 id=&#34;日志重复&#34;&gt;日志重复&lt;/h4&gt;

&lt;p&gt;Filebeat对于收集到的数据（即event）的传输保证的是&amp;rdquo;at least once&amp;rdquo;，而不是&amp;rdquo;exactly once&amp;rdquo;，也就是Filebeat传输的数据是有可能有重复的。这里我们讨论一下可能产生重复数据的一些场景，我大概将其分为两类。&lt;/p&gt;

&lt;p&gt;第一类：Filebeat重传导致数据重复。重传是因为Filebeat要保证数据至少发送一次，进而避免数据丢失。具体来说就是每条event发送到output后都要等待ack，只有收到ack了才会认为数据发送成功，然后将状态记录到registry。当然实际操作的时候为了高效是批量发送，批量确认的。而造成重传的场景（也就是没有收到ack）非常多，而且很多都不可避免，比如后端不可达、网络传输失败、程序突然挂掉等等。&lt;/p&gt;

&lt;p&gt;第二类：配置不当或操作不当导致文件重复收集。Filebeat感知文件有没有被收集过靠的是registry文件里面记录的状态，如果一个文件已经被收集过了，但因为各种原因它的状态从registry文件中被移除了，而恰巧这个文件还在收集范围内，那就会再收集一次。&lt;/p&gt;

&lt;p&gt;对于第一类产生的数据重复一般不可避免，而第二类可以避免，但总的来说，Filebeat提供的是at least once的机制，所以我们在使用时要明白数据是可能重复的。如果业务上不能接受数据重复，那就要在Filebeat之后的流程中去重。&lt;/p&gt;

&lt;h4 id=&#34;数据丢失&#34;&gt;数据丢失&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;inode重用的问题&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果一个文件达到了限制（比如大小），不是重新创建一个新的文件写，而是将这个文件truncate掉继续复用（当然实际中这种场景好像比较少，但也并非没有），Filebeat下次来检查这个文件是否有变动的时候，这个文件的大小如果大于之前记录的offset，也会发生上面的情况。这个问题在github上面是有issue的，但目前还没有解决，官方回复是Filebeat的整个机制在重构中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;还有一些其它情况，比如文件数太多，Filebeat的处理能力有限，在还没来得及处理的时候这些文件就被删掉了（比如rotate给老化掉了）也会造成数据丢失。还有就是后端不可用，所以Filebeat还在重试，但源文件被删了，那数据也就丢了。因为Filebeat的重试并非一直发送已经收集到内存里面的event，必要的时候会重新从源文件读，比如程序重启。这些情况的话，只要不限制Filebeat的收集能力，同时保证后端的可用性，网络的可用性，一般问题不大。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;总结-1&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;其实重数据发送到内存队列中这一套完整的功能就是由libbeat完成的，正常流程如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/output.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;日志采集状态监控&#34;&gt;日志采集状态监控&lt;/h2&gt;

&lt;p&gt;我们之前讲到 Registrar 会记录每个文件的状态，当 Filebeat 启动时，会从 Registrar 恢复文件处理状态。&lt;/p&gt;

&lt;p&gt;其实在 filebeat 运行过程中，Input 组件也记录了文件状态。不一样的是，Registrar 是持久化存储，而 Input 中的文件状态仅表示当前文件的读取偏移量，且修改时不会同步到磁盘中。&lt;/p&gt;

&lt;p&gt;每次，Filebeat 刚启动时，Input 都会载入 Registrar 中记录的文件状态，作为初始状态。Input 中的状态有两个非常重要：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;offset: 代表文件当前读取的 offset，从 Registrar 中初始化。Harvest 读取文件后，会同时修改 offset。
finished: 代表该文件对应的 Harvester 是否已经结束，Harvester 开始时置为 false，结束时置为 true。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于每次定时扫描到的文件，概括来说，会有三种大的情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input 找不到该文件状态的记录, 说明是新增文件，则开启一个 Harvester，从头开始解析该文件
如果可以找到文件状态，且 finished 等于 false。这个说明已经有了一个 Harvester 在处理了，这种情况直接忽略就好了。
如果可以找到文件状态，且 finished 等于 true。说明之前有 Harvester 处理过，但已经处理结束了。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于这种第三种情况，我们需要考虑到一些异常情况，Filebeat 是这么处理的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;如果 offset 大于当前文件大小：说明文件被 Truncate 过，此时按做一个新文件处理，直接从头开始解析该文件
如果 offset 小于当前文件大小，说明文件内容有新增，则从上次 offset 处继续读即可。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于第二种情况，Filebeat 似乎有一个逻辑上的问题: 如果文件被 Truncate 过，后来又新增了数据，且文件大小也比之前 offset 大，那么 Filebeat 是检查不出来这个问题的。&lt;/p&gt;

&lt;h2 id=&#34;句柄保持&#34;&gt;句柄保持&lt;/h2&gt;

&lt;p&gt;Filebeat 甚至可以处理文件名修改的问题。即使一个日志的文件名被修改过，Filebeat 重启后，也能找到该文件，从上次读过的地方继续读。&lt;/p&gt;

&lt;p&gt;这是因为 Filebeat 除了在 Registrar 存储了文件名，还存储了文件的唯一标识。对于 Linux 来说，这个文件的唯一标识就是该文件的 inode ID + device ID。&lt;/p&gt;

&lt;h2 id=&#34;重载&#34;&gt;重载&lt;/h2&gt;

&lt;p&gt;重载是在Crawler中启动的，首先是新建的InputsFactory的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c.InputsFactory = input.NewRunnerFactory(c.out, r, c.beatDone)

// RunnerFactory is a factory for registrars
type RunnerFactory struct {
    outlet    channel.Factory
    registrar *registrar.Registrar
    beatDone  chan struct{}
}

// NewRunnerFactory instantiates a new RunnerFactory
func NewRunnerFactory(outlet channel.Factory, registrar *registrar.Registrar, beatDone chan struct{}) *RunnerFactory {
    return &amp;amp;RunnerFactory{
        outlet:    outlet,
        registrar: registrar,
        beatDone:  beatDone,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后如果配置了重载，就会新建Reloader结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Reloader is used to register and reload modules
type Reloader struct {
    pipeline beat.Pipeline
    config   DynamicConfig
    path     string
    done     chan struct{}
    wg       sync.WaitGroup
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新建的过程如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if configInputs.Enabled() {
    c.inputReloader = cfgfile.NewReloader(pipeline, configInputs)
    if err := c.inputReloader.Check(c.InputsFactory); err != nil {
        return err
    }

    go func() {
        c.inputReloader.Run(c.InputsFactory)
    }()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动reloader的run方法并且将RunnerFactory作为参数传递进去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Run runs the reloader
func (rl *Reloader) Run(runnerFactory RunnerFactory) {
    logp.Info(&amp;quot;Config reloader started&amp;quot;)

    list := NewRunnerList(&amp;quot;reload&amp;quot;, runnerFactory, rl.pipeline)

    rl.wg.Add(1)
    defer rl.wg.Done()

    // Stop all running modules when method finishes
    defer list.Stop()

    gw := NewGlobWatcher(rl.path)

    // If reloading is disable, config files should be loaded immediately
    if !rl.config.Reload.Enabled {
        rl.config.Reload.Period = 0
    }

    overwriteUpdate := true

    for {
        select {
        case &amp;lt;-rl.done:
            logp.Info(&amp;quot;Dynamic config reloader stopped&amp;quot;)
            return

        case &amp;lt;-time.After(rl.config.Reload.Period):
            debugf(&amp;quot;Scan for new config files&amp;quot;)
            configReloads.Add(1)

            //扫描所有的配置文件
            files, updated, err := gw.Scan()
            if err != nil {
                // In most cases of error, updated == false, so will continue
                // to next iteration below
                logp.Err(&amp;quot;Error fetching new config files: %v&amp;quot;, err)
            }

            // no file changes
            if !updated &amp;amp;&amp;amp; !overwriteUpdate {
                overwriteUpdate = false
                continue
            }

            // Load all config objects 加载所有配置文件
            configs, _ := rl.loadConfigs(files)

            debugf(&amp;quot;Number of module configs found: %v&amp;quot;, len(configs))

            //启动加载程序
            if err := list.Reload(configs); err != nil {
                // Make sure the next run also updates because some runners were not properly loaded
                overwriteUpdate = true
            }
        }

        // Path loading is enabled but not reloading. Loads files only once and then stops.
        if !rl.config.Reload.Enabled {
            logp.Info(&amp;quot;Loading of config files completed.&amp;quot;)
            select {
            case &amp;lt;-rl.done:
                logp.Info(&amp;quot;Dynamic config reloader stopped&amp;quot;)
                return
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见有一个定时的循环程序来获取所有的配置文件，交给RunnerList的reload的来加载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Reload the list of runners to match the given state
func (r *RunnerList) Reload(configs []*reload.ConfigWithMeta) error {
    r.mutex.Lock()
    defer r.mutex.Unlock()

    var errs multierror.Errors

    startList := map[uint64]*reload.ConfigWithMeta{}
    //获取正在运行的runner到stopList
    stopList := r.copyRunnerList()

    r.logger.Debugf(&amp;quot;Starting reload procedure, current runners: %d&amp;quot;, len(stopList))

    // diff current &amp;amp; desired state, create action lists
    for _, config := range configs {
        hash, err := HashConfig(config.Config)
        if err != nil {
            r.logger.Errorf(&amp;quot;Unable to hash given config: %s&amp;quot;, err)
            errs = append(errs, errors.Wrap(err, &amp;quot;Unable to hash given config&amp;quot;))
            continue
        }

        //如果配置文件还在，就重stopList中删除，继续采集，剩下的在stopList中的下面会停止采集
        if _, ok := stopList[hash]; ok {
            delete(stopList, hash)
        } else {
            //如果不在stopList中，说明是新的文件，如果不是重复的就加入到startList中，下来开始采集
            if _,ok := r.runners[hash]; !ok{
                startList[hash] = config
            }
        }
    }

    r.logger.Debugf(&amp;quot;Start list: %d, Stop list: %d&amp;quot;, len(startList), len(stopList))

    // Stop removed runners
    for hash, runner := range stopList {
        r.logger.Debugf(&amp;quot;Stopping runner: %s&amp;quot;, runner)
        delete(r.runners, hash)
        go runner.Stop()
    }

    // Start new runners
    for hash, config := range startList {
        // Pass a copy of the config to the factory, this way if the factory modifies it,
        // that doesn&#39;t affect the hash of the original one.
        c, _ := common.NewConfigFrom(config.Config)
        runner, err := r.factory.Create(r.pipeline, c, config.Meta)
        if err != nil {
            r.logger.Errorf(&amp;quot;Error creating runner from config: %s&amp;quot;, err)
            errs = append(errs, errors.Wrap(err, &amp;quot;Error creating runner from config&amp;quot;))
            continue
        }

        r.logger.Debugf(&amp;quot;Starting runner: %s&amp;quot;, runner)
        r.runners[hash] = runner
        runner.Start()
    }

    return errs.Err()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边的create就是上面传进来的InputsFactory的结构体的成员函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Create creates a input based on a config
func (r *RunnerFactory) Create(
    pipeline beat.Pipeline,
    c *common.Config,
    meta *common.MapStrPointer,
) (cfgfile.Runner, error) {
    connector := r.outlet(pipeline)
    p, err := New(c, connector, r.beatDone, r.registrar.GetStates(), meta)
    if err != nil {
        // In case of error with loading state, input is still returned
        return p, err
    }

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看看new函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New instantiates a new Runner
func New(
    conf *common.Config,
    connector channel.Connector,
    beatDone chan struct{},
    states []file.State,
    dynFields *common.MapStrPointer,
) (*Runner, error) {
    input := &amp;amp;Runner{
        config:   defaultConfig,
        wg:       &amp;amp;sync.WaitGroup{},
        done:     make(chan struct{}),
        Once:     false,
        beatDone: beatDone,
    }

    var err error
    if err = conf.Unpack(&amp;amp;input.config); err != nil {
        return nil, err
    }

    var h map[string]interface{}
    conf.Unpack(&amp;amp;h)
    input.ID, err = hashstructure.Hash(h, nil)
    if err != nil {
        return nil, err
    }

    var f Factory
    f, err = GetFactory(input.config.Type)
    if err != nil {
        return input, err
    }

    context := Context{
        States:        states,
        Done:          input.done,
        BeatDone:      input.beatDone,
        DynamicFields: dynFields,
        Meta:          nil,
    }
    var ipt Input
    ipt, err = f(conf, connector, context)
    if err != nil {
        return input, err
    }
    input.input = ipt

    return input, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看见就是新建流程中的新建runner，下面调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;runner.Start()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是正常的采集流程，到这边重载也就结束了。&lt;/p&gt;

&lt;h1 id=&#34;基本使用与特性&#34;&gt;基本使用与特性&lt;/h1&gt;

&lt;h2 id=&#34;filebeat自动reload更新&#34;&gt;filebeat自动reload更新&lt;/h2&gt;

&lt;p&gt;目前filebeat支持reload input配置，module配置，但reload的机制只有定时更新。&lt;/p&gt;

&lt;p&gt;在配置中打开reload.enable之后，还可以配置reload.period表示自动reload配置的时间间隔。&lt;/p&gt;

&lt;p&gt;filebeat在启动时，会创建一个专门用于reload的协程。对于每个正在运行的harvester，filebeat会将其加入一个全局的Runner列表，每次到了定时的间隔后，会触发一次配置文件的diff判断，如果是需要停止的加入stopRunner列表，然后逐个关闭，新的则加入startRunner列表，启动新的Runner。&lt;/p&gt;

&lt;h2 id=&#34;filebeat对kubernetes的支持&#34;&gt;filebeat对kubernetes的支持&lt;/h2&gt;

&lt;p&gt;filebeat官方文档提供了在kubernetes下基于daemonset的部署方式，最主要的一个配置如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- type: docker
      containers.ids:
      - &amp;quot;*&amp;quot;
      processors:
        - add_kubernetes_metadata:
            in_cluster: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即设置输入input为docker类型。由于所有的容器的标准输出日志默认都在节点的/var/lib/docker/containers/&lt;containerId&gt;/*-json.log路径，所以本质上采集的是这类日志文件。&lt;/p&gt;

&lt;p&gt;和传统的部署方式有所区别的是，如果服务部署在kubernetes上，我们查看和检索日志的维度不能仅仅局限于节点和服务，还需要有podName，containerName等，所以每条日志我们都需要打标增加kubernetes的元信息才发送至后端。&lt;/p&gt;

&lt;p&gt;filebeat会在配置中增加了add_kubernetes_metadata的processor的情况下，启动监听kubernetes的watch服务，监听所有kubernetes pod的变更，然后将归属本节点的pod最新的事件同步至本地的缓存中。&lt;/p&gt;

&lt;p&gt;节点上一旦发生容器的销毁创建，/var/lib/docker/containers/下会有目录的变动，filebeat根据路径提取出containerId，再根据containerId从本地的缓存中找到pod信息，从而可以获取到podName、label等数据，并加到日志的元信息fields中。&lt;/p&gt;

&lt;p&gt;filebeat还有一个beta版的功能autodiscover，autodiscover的目的是把分散到不同节点上的filebeat配置文件集中管理。目前也支持kubernetes作为provider，本质上还是监听kubernetes事件然后采集docker的标准输出文件。&lt;/p&gt;

&lt;p&gt;大致架构如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/k8slog.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但是在实际生产环境使用中，仅采集容器的标准输出日志还是远远不够，我们往往还需要采集容器挂载出来的自定义日志目录，还需要控制每个服务的日志采集方式以及更多的定制化功能。&lt;/p&gt;

&lt;h2 id=&#34;性能分析与调优&#34;&gt;性能分析与调优&lt;/h2&gt;

&lt;p&gt;虽然beats系列主打轻量级，虽然用golang写的filebeat的内存占用确实比较基于jvm的logstash等好太多，但是事实告诉我们其实没那么简单。&lt;/p&gt;

&lt;p&gt;正常启动filebeat，一般确实只会占用3、40MB内存，但是在轻舟容器云上偶发性的我们也会发现某些节点上的filebeat容器内存占用超过配置的pod limit限制（一般设置为200MB），并且不停的触发的OOM。&lt;/p&gt;

&lt;p&gt;究其原因，一般容器化环境中，特别是裸机上运行的容器个数可能会比较多，导致创建大量的harvester去采集日志。如果没有很好的配置filebeat，会有较大概率导致内存急剧上升。
当然，filebeat内存占据较大的部分还是memqueue，所有采集到的日志都会先发送至memqueue聚集，再通过output发送出去。每条日志的数据在filebeat中都被组装为event结构，filebeat默认配置的memqueue缓存的event个数为4096，可通过queue.mem.events设置。默认最大的一条日志的event大小限制为10MB，可通过max_bytes设置。4096 * 10MB = 40GB，可以想象，极端场景下，filebeat至少占据40GB的内存。特别是配置了multiline多行模式的情况下，如果multiline配置有误，单个event误采集为上千条日志的数据，很可能导致memqueue占据了大量内存，致使内存爆炸。&lt;/p&gt;

&lt;p&gt;所以，合理的配置日志文件的匹配规则，限制单行日志大小，根据实际情况配置memqueue缓存的个数，才能在实际使用中规避filebeat的内存占用过大的问题。&lt;/p&gt;

&lt;p&gt;有些文章说filebeat内存消耗很少,不会超过100M, 这简直是不负责任的胡说,假如带着这样的认识把filebeat部署到生产服务器上就等着哭吧.&lt;/p&gt;

&lt;p&gt;那怎么样才能避免以上内存灾难呢?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;每个日志生产环境生产的日志大小,爆发量都不一样, 要根据自己的日志特点设定合适的event值;什么叫合适,至少能避免内存&amp;gt;200MB的灾难;&lt;/li&gt;
&lt;li&gt;在不知道日志实际情况(单条大小,爆发量), 务必把event设置上,建议128或者256;&lt;/li&gt;
&lt;li&gt;合理的配置日志文件的匹配规则，是否因为通配符的原因，造成同时监控数量巨大的文件，这种情况应该避免用通配符监控无用的文件。&lt;/li&gt;
&lt;li&gt;规范日志，限制单行日志大小，是否文件的单行内容巨大，确定是否需要改造文件内容，或者将其过滤&lt;/li&gt;
&lt;li&gt;限制cpu&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面的一系列操作可以做如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max_procs: 2
queue:
  mem:
    events: 512
    flush.min_events: 256
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;限制cpu为2core，内存最大为512*10M～=5G&lt;/p&gt;

&lt;p&gt;限制cpu的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max_procs，限制filebeat的进程数量，其实是内核数，建议手动设为1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;限制内存的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;queue.mem.events消息队列的大小，默认值是4096，这个参数在6.0以前的版本是spool-size，通过命令行，在启动时进行配置
max_message_bytes 单条消息的大小, 默认值是10M
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;filebeat最大的可能占用的内存是max_message_bytes * queue.mem.events = 40G，考虑到这个queue是用于存储encode过的数据，raw数据也是要存储的，所以，在没有对内存进行限制的情况下，最大的内存占用情况是可以达到超过80G。&lt;/p&gt;

&lt;h2 id=&#34;内存使用过多的情况&#34;&gt;内存使用过多的情况&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;非常频繁的rotate日志&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于实时大量产生内容的文件，比如日志，常用的做法往往是将日志文件进行rotate，根据策略的不同，每隔一段时间或者达到固定大小之后，将日志rotate。
这样，在文件目录下可能会产生大量的日志文件。
如果我们使用通配符的方式，去监控该目录，则filebeat会启动大量的harvester实例去采集文件。但是，请记住，我这里不是说这样一定会产生内存泄漏，只是在这里观测到了内存泄漏而已，不是说这是造成内存泄漏的原因。&lt;/p&gt;

&lt;p&gt;当filebeat运行了几个月之后，占用了超过10个G的内存。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;因为multiline导致内存占用过多&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;multiline.pattern: &amp;lsquo;^[[:space:]]+|^Caused by:|^.+Exception:|^\d+\serror，比如这个配置，认为空格或者制表符开头的line是上一行的附加内容，需要作为多行模式，存储到同一个event当中。当你监控的文件刚巧在文件的每一行带有一个空格时，会错误的匹配多行，造成filebeat解析过后，单条event的行数达到了上千行，大小达到了10M，并且在这过程中使用的是正则表达式，每一条event的处理都会极大的消耗内存。因为大多数的filebeat output是需应答的，buffer这些event必然会大量的消耗内存。&lt;/p&gt;

&lt;h2 id=&#34;解读日志中的监控数据&#34;&gt;解读日志中的监控数据&lt;/h2&gt;

&lt;p&gt;其实filebeat的日志，已经包含了很多参数用于实时观测filebeat的资源使用情况，（下面是6.0版本的，6.5版本之后，整个日志格式变了，从kv格式变成了json对象格式）&lt;/p&gt;

&lt;p&gt;里面的参数主要分成三个部分：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;beat.*，包含memstats.gc_next，memstats.memory_alloc，memstats.memory_total，这个是所有beat组件都有的指标，是filebeat继承来的，主要是内存相关的，我们这里特别关注memstats.memory_alloc，alloc的越多，占用内存越大
filebeat.*，这部分是filebeat特有的指标，通过event相关的指标，我们知道吞吐，通过harvester，我们知道正在监控多少个文件，未消费event堆积的越多，havester创建的越多，消耗内存越大
libbeat.*，也是beats组件通用的指标，包含outputs和pipeline等信息。这里要主要当outputs发生阻塞的时候，会直接影响queue里面event的消费，造成内存堆积
registrar，filebeat将监控文件的状态放在registry文件里面，当监控文件非常多的时候，比如10万个，而且没有合理的设置close_inactive参数，这个文件能达到100M，载入内存后，直接占用内存
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在6.5之后都是json，但也是kv结构，可以对应查看。&lt;/p&gt;

&lt;h2 id=&#34;如何对filebeat进行扩展开发&#34;&gt;如何对filebeat进行扩展开发&lt;/h2&gt;

&lt;p&gt;一般情况下filebeat可满足大部分的日志采集需求，但是仍然避免不了一些特殊的场景需要我们对filebeat进行定制化开发，当然filebeat本身的设计也提供了良好的扩展性。
beats目前只提供了像elasticsearch、kafka、logstash等几类output客户端，如果我们想要filebeat直接发送至其他后端，需要定制化开发自己的output。同样，如果需要对日志做过滤处理或者增加元信息，也可以自制processor插件。
无论是增加output还是写个processor，filebeat提供的大体思路基本相同。一般来讲有3种方式：&lt;/p&gt;

&lt;p&gt;1.直接fork filebeat，在现有的源码上开发。output或者processor都提供了类似Run、Stop等的接口，只需要实现该类接口，然后在init方法中注册相应的插件初始化方法即可。当然，由于golang中init方法是在import包时才被调用，所以需要在初始化filebeat的代码中手动import。&lt;/p&gt;

&lt;p&gt;2.filebeat还提供了基于golang plugin的插件机制，需要把自研的插件编译成.so共享链接库，然后在filebeat启动参数中通过-plugin指定库所在路径。不过实际上一方面golang plugin还不够成熟稳定，一方面自研的插件依然需要依赖相同版本的libbeat库，而且还需要相同的golang版本编译，坑可能更多，不太推荐。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- 容器日志采集方案</title>
          <link>https://kingjcy.github.io/post/monitor/log/collect/collect-scheme/</link>
          <pubDate>Sun, 08 Jul 2018 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/collect/collect-scheme/</guid>
          <description>&lt;p&gt;容器由于其特殊性，在日志采集上有着不同的解决方案，目前主要还是以探针采集为主。&lt;/p&gt;

&lt;h1 id=&#34;日志采集演进&#34;&gt;日志采集演进&lt;/h1&gt;

&lt;p&gt;容器日志采集方案一直不断的演进，纵览当前容器日志收集的场景，无非就是两种方式：一是直接采集Docker标准输出，容器内的服务将日志信息写到标准输出，这样通过Docker的log driver可以发送到相应的收集程序中；二是延续传统的日志写入方式，容器内的服务将日志直接写到普通文件中，通过Docker volume将日志文件映射到Host上，日志采集程序就可以收集它。&lt;/p&gt;

&lt;h2 id=&#34;docker-log-driver&#34;&gt;docker log driver&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;docker logs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;docker logs edc-k8s-demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，Docker的日志会发送到容器的标准输出设备（STDOUT）和标准错误设备（STDERR），其中STDOUT和STDERR实际上就是容器的控制台终端。如果想要持续看到新打印出的日志信息，那么可以加上 -f 参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker logs -f edc-k8s-demo
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Docker logging driver&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Docker还提供了其他的一些机制允许我们从运行的容器中提取日志，这些机制统称为 logging driver。&lt;/p&gt;

&lt;p&gt;对Docker而言，其默认的logging driver是json-file，如果在启动时没有特别指定，都会使用这个默认的logging driver。json-file会将我们在控制台通过docker logs命名看到的日志都保存在一个json文件中，我们可以在服务器Host上的容器目录中找到这个json文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;容器日志路径：/var/lib/docker/containers/&amp;lt;container-id&amp;gt;/&amp;lt;container-id&amp;gt;-json.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了json-file，Docker还支持以下多种logging dirver&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;none  No logs are available for the container and docker logs does not return any output.&lt;/li&gt;
&lt;li&gt;local Logs are stored in a custom format designed for minimal overhead.&lt;/li&gt;
&lt;li&gt;json-file The logs are formatted as JSON. The default logging driver for Docker.&lt;/li&gt;
&lt;li&gt;syslog    Writes logging messages to the syslog facility. The syslog daemon must be running on the host machine.&lt;/li&gt;
&lt;li&gt;journald  Writes log messages to journald. The journald daemon must be running on the host machine.&lt;/li&gt;
&lt;li&gt;gelf  Writes log messages to a Graylog Extended Log Format (GELF) endpoint such as Graylog or Logstash.&lt;/li&gt;
&lt;li&gt;fluentd   Writes log messages to fluentd (forward input). The fluentd daemon must be running on the host machine.&lt;/li&gt;
&lt;li&gt;awslogs   Writes log messages to Amazon CloudWatch Logs.&lt;/li&gt;
&lt;li&gt;splunk    Writes log messages to splunk using the HTTP Event Collector.&lt;/li&gt;
&lt;li&gt;etwlogs   Writes log messages as Event Tracing for Windows (ETW) events. Only available on Windows platforms.&lt;/li&gt;
&lt;li&gt;gcplogs   Writes log messages to Google Cloud Platform (GCP) Logging.&lt;/li&gt;
&lt;li&gt;logentries    Writes log messages to Rapid7 Logentries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以在容器启动时通过加上 &amp;ndash;log-driver 来指定使用哪个具体的 logging driver，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --log-driver=syslog ......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想要设置默认的logging driver，那么则需要修改Docker daemon的启动脚本，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;log-driver&amp;quot;: &amp;quot;json-file&amp;quot;,
  &amp;quot;log-opts&amp;quot;: {
    &amp;quot;labels&amp;quot;: &amp;quot;production_status&amp;quot;,
    &amp;quot;env&amp;quot;: &amp;quot;os,customer&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个logging driver都有一些自己特定的log-opt，使用时可以参考具体官方文档。&lt;/p&gt;

&lt;p&gt;可见，第一种方式足够简单，直接配置相关的Log Driver就可以，但是这种方式也有些劣势：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当主机的容器密度比较高的时候，对Docker Engine的压力比较大，毕竟容器标准输出都要通过Docker Engine来处理。&lt;/li&gt;
&lt;li&gt;尽管原则上，我们希望遵循一容器部署一个服务的原则，但是有时候特殊情况不可避免容器内有多个业务服务，这时候很难做到所有服务都向标准输出写日志，这就需要用到前面所说的第二种场景模式。&lt;/li&gt;
&lt;li&gt;虽然我们可以先选择很多种Log Driver，但是有些Log Driver会破坏Docker原生的体验，比如docker logs无法直接看到容器日志。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;docker-volume&#34;&gt;docker volume&lt;/h2&gt;

&lt;p&gt;通过对第一种方案的摸索，存在着很多的问题与不方便，所以目前我们大多数采集还是使用第二种方案，文件采集的方式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;第三方采集方案&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面都是将日志文件落到STDOUT和STDERR，我们采集都是基于这个，其实在我们应用编程的时候，完全可以将日志文件落到容器的对应的目录下，落盘然后使用第三方采集组件比如filebeat、fluentd等采集，统一管理。&lt;/p&gt;

&lt;h1 id=&#34;容器日志采集方案&#34;&gt;容器日志采集方案&lt;/h1&gt;

&lt;p&gt;根据上面的基本描述，容器日志采集有很多种方式，每种方式都用不同实现方案，适用于不同的场景。&lt;/p&gt;

&lt;h2 id=&#34;logdriver&#34;&gt;LogDriver&lt;/h2&gt;

&lt;p&gt;DockerEngine 本身具有 LogDriver 功能，可通过配置不同的 LogDriver 将容器的 stdout 通过 DockerEngine 写入到远端存储，以此达到日志采集的目的。这种方式的可定制化、灵活性、资源隔离性都很低，一般不建议在生产环境中使用，上面我们已经说明不使用的原因。&lt;/p&gt;

&lt;h2 id=&#34;http&#34;&gt;http&lt;/h2&gt;

&lt;p&gt;业务直写是在应用中集成日志采集的 SDK，通过 SDK 直接将日志发送到服务端。这种方式省去了落盘采集的逻辑，也不需要额外部署 Agent，对于系统的资源消耗最低，但由于业务和日志 SDK 强绑定，整体灵活性很低，一般只有日志量极大的场景中使用，这是一种特殊的场景，我们会在特殊情况下使用。&lt;/p&gt;

&lt;h2 id=&#34;deamonset模式&#34;&gt;deamonset模式&lt;/h2&gt;

&lt;p&gt;DaemonSet 方式在每个 node 节点上只运行一个日志 agent(&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat/&#34;&gt;filebeat&lt;/a&gt;,fluentd,flume,fluentbit)，采集这个节点上所有的日志。DaemonSet 相对资源占用要小很多，但扩展性、租户隔离性受限，比较适用于功能单一或业务不是很多的集群；&lt;/p&gt;

&lt;p&gt;正常规模的采集可以适应，日志分类明确、功能较单一的集群，大规模的集群采集速度就跟不上了，而且没有办法做到垂直扩展无上限。&lt;/p&gt;

&lt;p&gt;当然完整的方案还是有很多需要做的工作，比如如何做发现，如何动态变更，这就是一个完成的平台建设了，这些每个公司都有自己的建设，就不太好说了。&lt;/p&gt;

&lt;h2 id=&#34;sidecar模式&#34;&gt;sidecar模式&lt;/h2&gt;

&lt;p&gt;Sidecar 方式为每个 POD 单独部署日志 agent，这个 agent 只负责一个业务应用的日志采集。Sidecar 相对资源占用较多，但灵活性以及多租户隔离性较强，建议大型的 K8s 集群或作为 PaaS 平台为多个业务方服务的集群使用该方式。&lt;/p&gt;

&lt;p&gt;适用于大型、混合型、PAAS型集群的日志采集，是一种水平扩展消耗更多资源来增加采集速度的方案，但是方案就比较复杂。&lt;/p&gt;

&lt;p&gt;当然完整的方案还是有很多需要做的工作，比如如何做发现，如何动态变更，这就是一个完成的平台建设了，这些每个公司都有自己的建设，也就不太好说了。&lt;/p&gt;

&lt;h1 id=&#34;网络采集性能数据&#34;&gt;网络采集性能数据&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;有赞&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;重flume发展到自研rsyslog-hub和http服务&lt;/p&gt;

&lt;p&gt;17年平均每秒产生日志1.1万条，峰值1.5万条，每天的日志量约9亿条，占用空间2.4T左右&lt;/p&gt;

&lt;p&gt;19年每天都会产生百亿级别的日志量（据统计，平均每秒产生 50 万条日志，峰值每秒可达 80 万条）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;七牛云&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;自研logkit&lt;/p&gt;

&lt;p&gt;17年现在日均数据流入量超 250 TB，3650 亿条，其中最大的客户日均数据流入量超过 45 TB。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;b站&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;17年目前集群规模20台机器，接入业务200+，单日日志量10T+。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;阿里云&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;自研logtail（重内核都得到的优化和充分利用）&lt;/p&gt;

&lt;p&gt;速度达到160M/s&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Nginx</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/server/nginx/</link>
          <pubDate>Fri, 29 Jun 2018 16:55:15 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/server/nginx/</guid>
          <description>&lt;p&gt;nginx [engine x] is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server.关于&lt;a href=&#34;https://kingjcy.github.io/post/middleware/proxy/nginx&#34;&gt;nginx&lt;/a&gt;的介绍就不多谈了，这里主要聊下如何打造nginx集群的监控系统。&lt;/p&gt;

&lt;h1 id=&#34;监控演化&#34;&gt;监控演化&lt;/h1&gt;

&lt;p&gt;目前国内大多数互联网都是选择nginx构建web转发集群，那么如何构建nginx集群的监控系统？&lt;/p&gt;

&lt;p&gt;下面这个监控方案的历程，大体也说明了nginx的进化发展。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2011年左右&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当时并没有合适的开源工具，只能自写python脚本，一方面是通过nginx的status模块获取少量指标如处理请求数，连接数等等，另一方面是通过定期跑nginx日志生成监控数据，写入mysql进行存储，然后通过自研的监控系统展示相关监控指标。其中，指标的主要维度是域名。这个方案除了跑nginx日志消耗过多资源以及需要大量开发工作外，没啥大问题，就是有新的监控需求时很头疼，得改脚本。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2013年左右&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这时候阿里的tengine开始发力，开发各种插件并在阿里内部推广使用，其中就包括了更为强大的status模块。只是tengine魔改了大量nginx的内部实现，导致大多数模块与nginx并不兼容。于是就有了这个项目hnlq715/status-nginx-module，完美支持nginx，并成功重构了当时上百台nginx，日请求量几十亿的的监控系统，此时能满足绝大多数监控需求。指标的主要维度仍然是域名。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2015年左右&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这时候注意到了prometheus的出现，感叹于其强大，于是考虑基于prometheus实现nginx的监控。而此时，vozlt/nginx-module-vts已经使用很广，于是创建了这个项目hnlq715/nginx-vts-exporter，用于将vts的输出内容转化为prometheus的格式，便于prometheus抓取。基于grafana提供监控查询及UI展示，这时候算是步入真正的现代化监控系统，此时的指标维度已经细化到具体的URI。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2017年左右&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然而上一个方案还是有个短板，那就是nginx-module-vts对响应时间的处理太过粗糙，只给了一个平均值，无法对P99，P90或是P50给出直观的数据。于是基于lua内嵌在nginx里跑监控指标，demo在这hnlq715/nginx-prometheus-metrics，这种方式可以直接用prometheus的histogram类型统计响应时间等指标，并在prometheus层聚合。此时，才算是真正的现代化监控系统。指标维度可以完全自定义，且更加多维化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;基于prometheus聚合nginx的监控数据是非常好的选择。上述项目能够在一定程度上帮助实现，不用手写代码或是通过极少的lua代码实现一套现代化的nginx监控系统。当然，也可以聚合其他诸如redis、mysql、node等各类系统的监控数据到prometheus进行统一管理。&lt;/p&gt;

&lt;h1 id=&#34;监控&#34;&gt;监控&lt;/h1&gt;

&lt;h2 id=&#34;status&#34;&gt;status&lt;/h2&gt;

&lt;p&gt;原版的 NGINX 会在一个简单的状态页面上显示几个与服务器状态有关的基本指标，它们由你启用的 HTTP stub status module 所提供。&lt;/p&gt;

&lt;p&gt;你可以浏览状态页看到你的指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Active connections: 24 
server accepts handled requests
1156958 1156958 4491319
Reading: 0 Writing: 18 Waiting : 6 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下表是 Nginx 提供的监控参数及其简单释义。&lt;/p&gt;

&lt;p&gt;参数名称    参数描述&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Active connections  当前活跃的用户连接(包含Waiting状态)
accepts 接收到的用户连接总数
handled Nginx处理的用户连接总数
requests    用户请求总数
Reading 当前连接中Nginx读取请求首部的个数
Writing 当前连接中Nginx写返回给用户的个数
Waiting 当前没有请求的活跃用户连接数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些提供了我们简单的指标。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当用户请求连接Nginx服务器时，accepts计数器会加一。且当服务器处理该连接请求时，handled计数器同样会加一。一般而言，两者的值是相等的，除非达到了某些资源极限（如worker_connection的限制）。&lt;/li&gt;
&lt;li&gt;用户连接请求被处理，就会进入 active 状态。如果该连接没有其他 request，则进入 waiting 的子状态；如果有 request，nginx 会读取 request 的 header，计数器 request 加一，进入 reading 的子状态。 reading 状态持续时间非常短，header 被读取后就会进入 writing 状态。事实上，直到服务器将响应结果返回给用户之前，该连接会一直保持 writing 状态。所以说，writing 状态一般会被长时间占用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;监控需求&#34;&gt;监控需求&lt;/h2&gt;

&lt;p&gt;三类指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;基本活动指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Accepts（接受）、Handled（已处理）、Requests（请求数）是一直在增加的计数器。Active（活跃）、Waiting（等待）、Reading（读）、Writing（写）随着请求量而增减。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;提醒指标: 丢弃连接
被丢弃的连接数目等于 Accepts 和 Handled 之差（NGINX 中），或是可直接得到的标准指标（NGINX Plus 中）。在正常情况下，丢弃连接数应该是零。如果在每个单位时间内丢弃连接的速度开始上升，那么应该看看是否资源饱和了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;提醒指标: 每秒请求数
按固定时间间隔采样你的请求数据（开源 NGINX 的requests或者 NGINX Plus 中total） 会提供给你单位时间内（通常是分钟或秒）所接受的请求数量。监测这个指标可以查看进入的 Web 流量尖峰，无论是合法的还是恶意的，或者突然的下降，这通常都代表着出现了问题。每秒请求数若发生急剧变化可以提醒你的环境出现问题了，即使它不能告诉你确切问题的位置所在。请注意，所有的请求都同样计数，无论 URL 是什么。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;错误指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NGINX 错误指标告诉你服务器是否经常返回错误而不是正常工作。客户端错误返回4XX状态码，服务器端错误返回5XX状态码。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;提醒指标: 服务器错误率&lt;/p&gt;

&lt;p&gt;服务器错误率等于在单位时间（通常为一到五分钟）内5xx错误状态代码的总数除以状态码（1XX，2XX，3XX，4XX，5XX）的总数。如果你的错误率随着时间的推移开始攀升，调查可能的原因。如果突然增加，可能需要采取紧急行动，因为客户端可能收到错误信息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;收集错误度量
配置 NGINX 的日志模块将响应码写入访问日志&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;性能指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;提醒指标: 请求处理时间
请求处理时间指标记录了 NGINX 处理每个请求的时间，从读到客户端的第一个请求字节到完成请求。较长的响应时间说明问题在上游。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;

&lt;p&gt;目前prometheus还没有官方的exporter。目前有两种采集的办法&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;nginx-lua-prometheus&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个是以lua插件的形式暴露出一些基础连接信息&lt;/p&gt;

&lt;p&gt;下载地址：&lt;a href=&#34;https://github.com/knyar/nginx-lua-prometheus&#34;&gt;https://github.com/knyar/nginx-lua-prometheus&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;nginx-lua-prometheus测试&lt;/p&gt;

&lt;p&gt;解压源码包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf nginx-1.6.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开始进行编译。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test nginx-1.6.0]# pwd
/opt/nginx-1.6.0
[root@test nginx-1.6.0]# ./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-http_realip_module --add-module=./nginx-sticky-module-1.1 --add-module=./nginx_upstream_check_module-master --add-module=./nginx_upstream_hash-0.3.1 --add-module=./lua-nginx-module-0.9.10 --add-module=./nginx-concat-module
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看编译后安装的模块&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test sbin]# ./nginx -V
nginx version: nginx/1.6.0
built by gcc 4.1.2 20080704 (Red Hat 4.1.2-48)
TLS SNI support disabled
configure arguments: --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-http_realip_module --add-module=./nginx-sticky-module-1.1 --add-module=./nginx_upstream_check_module-master --add-module=./nginx_upstream_hash-0.3.1 --add-module=./lua-nginx-module-0.9.10 --add-module=./nginx-concat-module
[root@test sbin]# pwd
/usr/local/nginx/sbin
[root@test sbin]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后make &amp;amp; make install&lt;/p&gt;

&lt;p&gt;启动nginx&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test sbin]# ./nginx
[root@test sbin]# ps -ef |grep nginx
root     15585     1  0 15:45 ?        00:00:00 nginx: master process ./nginx
nobody   15586 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15587 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15588 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15589 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15590 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15591 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15592 15585  0 15:45 ?        00:00:00 nginx: worker process
nobody   15593 15585  0 15:45 ?        00:00:00 nginx: worker process
root     15597  3959  0 15:45 pts/2    00:00:00 grep nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问&lt;/p&gt;

&lt;p&gt;开始安装对应监控模块&lt;/p&gt;

&lt;p&gt;下载最新的release版本nginx-lua-prometheus-0.1-20170610.tar.gz&lt;/p&gt;

&lt;p&gt;创建目录&lt;/p&gt;

&lt;p&gt;/usr/local/nginx/lua&lt;/p&gt;

&lt;p&gt;上传gz包，并解压&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test nginx-lua-prometheus-0.1-20170610]# pwd
/usr/local/nginx/lua/nginx-lua-prometheus-0.1-20170610
[root@test nginx-lua-prometheus-0.1-20170610]# ls
nginx-lua-prometheus-0.1-20170610.rockspec  prometheus.lua  prometheus_test.lua  README.md
[root@test nginx-lua-prometheus-0.1-20170610]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改nginx.conf&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lua_shared_dict prometheus_metrics 10M;
lua_package_path &amp;quot;/usr/local/nginx/lua/nginx-lua-prometheus-0.1-20170610/?.lua&amp;quot;;
init_by_lua &#39;
  prometheus = require(&amp;quot;prometheus&amp;quot;).init(&amp;quot;prometheus_metrics&amp;quot;)
  metric_requests = prometheus:counter(
    &amp;quot;nginx_http_requests_total&amp;quot;, &amp;quot;Number of HTTP requests&amp;quot;, {&amp;quot;host&amp;quot;, &amp;quot;status&amp;quot;})
  metric_latency = prometheus:histogram(
    &amp;quot;nginx_http_request_duration_seconds&amp;quot;, &amp;quot;HTTP request latency&amp;quot;, {&amp;quot;host&amp;quot;})
  metric_connections = prometheus:gauge(
    &amp;quot;nginx_http_connections&amp;quot;, &amp;quot;Number of HTTP connections&amp;quot;, {&amp;quot;state&amp;quot;})
&#39;;
log_by_lua &#39;
  local host = ngx.var.host:gsub(&amp;quot;^www.&amp;quot;, &amp;quot;&amp;quot;)
  metric_requests:inc(1, {host, ngx.var.status})
  metric_latency:observe(ngx.now() - ngx.req.start_time(), {host})
&#39;;

server {
  listen 9145;
  #allow 0.0.0.0/16;
   #deny all;
  location /metrics {
    content_by_lua &#39;
      metric_connections:set(ngx.var.connections_reading, {&amp;quot;reading&amp;quot;})
      metric_connections:set(ngx.var.connections_waiting, {&amp;quot;waiting&amp;quot;})
      metric_connections:set(ngx.var.connections_writing, {&amp;quot;writing&amp;quot;})
      prometheus:collect()
    &#39;;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新加载nginx&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test conf]# ../sbin/nginx -s reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问nginx的页面&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://10.19.250.191:9145/metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;nginx-vts-exporter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;目前的版本是nginx-vts-exporter-0.8.3.linux-amd64.tar.gz&lt;/p&gt;

&lt;p&gt;下载地址是：&lt;a href=&#34;https://github.com/hnlq715/nginx-vts-exporter/releases&#34;&gt;https://github.com/hnlq715/nginx-vts-exporter/releases&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;nginx-vts-exporter测试&lt;/p&gt;

&lt;p&gt;由于nginx-vts-exporter依赖于Nginx的nginx-module-vts模块，所以这里需要重新编译下Nginx。之后再安装这个exporter&lt;/p&gt;

&lt;p&gt;VTS安装步骤&lt;/p&gt;

&lt;p&gt;1、  下载nginx-module-vts&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://github.com/vozlt/nginx-module-vts/releases/tag/v0.1.15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解压后目录为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/nginx-module-vts-0.1.15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、  重新编译nginx&lt;/p&gt;

&lt;p&gt;说明 由于 nginx_upstream_check_module-master 模块有问题。&lt;/p&gt;

&lt;p&gt;所以编译的时候的配置语句为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./configure  --with-http_stub_status_module --with-http_ssl_module --with-http_realip_module --add-module=./nginx-sticky-module-1.1  --add-module=./lua-nginx-module-0.9.10  --add-module=./nginx-concat-module   --add-module=/opt/nginx-module-vts-0.1.15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、  开始安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make &amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、  修改配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http {
    vhost_traffic_status_zone;

    ...

server {
      listen 8088;
      location /status {
            vhost_traffic_status_display;
            vhost_traffic_status_display_format html;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、  重新加载配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test conf]# ../sbin/nginx -s reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问页面&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://10.19.250.191:8088/status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/server/20190315.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见，vts的本身监控就是比较全面的，就是为监控而生。&lt;/p&gt;

&lt;p&gt;安装启动探针&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test nginx-vts-exporter-0.8.3.linux-amd64]# nohup /opt/nginx-vts-exporter-0.8.3.linux-amd64/./nginx-vts-exporter  -nginx.scrape_uri=&amp;quot;http://10.19.250.191:8088/status/format/json&amp;quot; 2&amp;gt;&amp;amp;1 &amp;amp;

[root@test nginx-vts-exporter-0.8.3.linux-amd64]# nohup: appending output to `nohup.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置访问&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -XPUT http://10.27.136.227:9996/v1/agent/service/register   -d &#39;
{
    &amp;quot;id&amp;quot;: &amp;quot;prometheus-exporter11&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;promether-exporter&amp;quot;,
    &amp;quot;address&amp;quot;: &amp;quot;10.19.250.191&amp;quot;,
    &amp;quot;port&amp;quot;: 9913,
    &amp;quot;tags&amp;quot;: [
          &amp;quot;SNMON&amp;quot;,
                &amp;quot;NJXZ&amp;quot;,
                &amp;quot;DEV&amp;quot;,
                &amp;quot;10.19.250.191&amp;quot;,
                &amp;quot;nginx-9913&amp;quot;
    ],
    &amp;quot;checks&amp;quot;: [
        {
            &amp;quot;script&amp;quot;: &amp;quot;curl http://10.19.250.9913/metrics &amp;gt;/dev/null 2&amp;gt;&amp;amp;1&amp;quot;,
            &amp;quot;interval&amp;quot;: &amp;quot;10s&amp;quot;
        }
    ]
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://10.27.136.227:9099/targets
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看Metric&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://10.19.250.191:9913/metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;综合来说，vts的指标本来就比较全面，结合prometheus，更加匹配我们的监控需求，但是要重新编译nginx比较麻烦，最终还是使用vts。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Infrastructure监控方案</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/infrastructure-base/</link>
          <pubDate>Wed, 13 Jun 2018 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/infrastructure-base/</guid>
          <description>&lt;p&gt;一个完整的监控体系包括：采集数据、分析存储数据、展示数据、告警以及自动化处理、监控工具自身的安全机制，我们来看看如何使用prometheus进行基础设施监控架构。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;首先可以看一下官方给出的架构方案&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20190315.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这边对架构正常使用做一个补充说明：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;grafana是无状态的，可以多部署几个通过nginx来负载均衡，通过不同的端口去访问不同的thanos-query。&lt;/li&gt;
&lt;li&gt;thanos-query查询对应的prometheus集群，获取数据。这边使用thanos来完成了prometheus集群功能，具体看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos的实现&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;prometheus使用hashmod来实现对数据采集的分片，把数据放到不同的prometheus的节点上，实现集群，这个没有使用federation，因为联合在大规模的情况下，瓶颈比较严重，还有目前只是单采集模式，可以设置双采，使用vip+keepalive来实现主备切换。&lt;/li&gt;
&lt;li&gt;prometheus去采集数据并不是直接使用target连接，使用nginx进行了转发采集，使用prometheus的relabel来设置&lt;strong&gt;address&lt;/strong&gt;,使得所有采集都连接nginx，然后使用target作为参数，最终访问target地址，获取数据，这样可以将nginx的网络打通，就能实现跨机房跨区域采集了，后面有网络问题也是处理nginx所在的机器就好。这两步具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus&#34;&gt;prometheus的实现&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;具体的采集&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/infrastructure-base/#监控内容&#34;&gt;监控内容&lt;/a&gt;可以通过探针的部署使用来区分。&lt;/li&gt;
&lt;li&gt;prometheus的数据可以提供给第三方使用，可以直接将数据通过&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus/#adapter&#34;&gt;adapter&lt;/a&gt;推送的kafka，给其他使用方消费&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/#remote-storage&#34;&gt;远程存储&lt;/a&gt;这一块，直接使用了remote read／write，当原生数据库不支持的时候，需要使用adapter进行转化发送。其实大部分我们使用的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos&#34;&gt;prometheus的扩展&lt;/a&gt;来做存储查询。&lt;/li&gt;
&lt;li&gt;告警直接根据rule文件推送的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/alertmanager/&#34;&gt;alartmanager&lt;/a&gt;，这个alartmanager是一个分布式的，在prometheus的yaml文件中都要配置上，alartmanager也可以将数据推送给mq（需要改造），正常可以使用kafka，给一些告警平台进行消费使用。rule文件直接使用consul的注册信息生成，注册信息是后台管理的。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/middleware/serverdiscovery/sd/&#34;&gt;动态注册，服务发现&lt;/a&gt;这一块，可以看见，使用的是consul+consul-template,使用consul注册，并且保存注册信息，这边使用了consul的k/v模式（为什么使用这个下面有说明），然后使用consul-template这个工具将注册的信息生成json文件给prometheus的file_sd_condig使用。使用crontab 来定时更新文件，实现配置文件的自动加载。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;大体架构如此，就可以实现一套物理环境的基础设施监控。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.08.08&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;NOTE：基于thanos查询慢，VM优秀的写入和查询性能，已经使用远程存储将数据都存储到VM进行查询，所以prometheus将数据都写到VM&lt;/strong&gt;，VM具体查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM的实现&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;设计原则&#34;&gt;设计原则&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;监控的是基础设施，目的是为了解决问题，没有必要朝着大而全的方向去做，对于没有必要采集的指标，不浪费资源。&lt;/li&gt;
&lt;li&gt;需要处理的告警才发出来，发出来的是必须要处理的告警&lt;/li&gt;
&lt;li&gt;业务系统和监控分离，哪怕业务系统挂了，监控也不能挂，监控挂了，不影响业务系统的运行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;监控内容&#34;&gt;监控内容&lt;/h1&gt;

&lt;p&gt;具体见监控内容可以查看每个探针，我们来看一下prometheus的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/&#34;&gt;exporter&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;采集组件&#34;&gt;采集组件&lt;/h2&gt;

&lt;p&gt;prometheus的exporter都是独立的，简单几个使用还是不错，解耦还开箱即用，但是数量多了，运维的压力变大了，例如探针管理升级，运行情况的检查等，有几种方案解决&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;做一个管理平台，类似于我们的后台系统，专门对exporter进行管理&lt;/li&gt;
&lt;li&gt;用一个主进程整合几个探针，每个探针依旧是原来的版本&lt;/li&gt;
&lt;li&gt;用telegraf来支持各种类型的input，all in one&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;prometheus使用总结&#34;&gt;prometheus使用总结&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;consul的service的瓶颈问题&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之前使用consul的services注册job的服务信息，然后使用consul-template动态生成prometheus的配置文件。然后prometheus通过查询consul中注册的信息正则匹配来完成prometheus的采集操作
但是这样当job量很大的时候，比如有20组job，一组job130的target的的时候，就会出现consul请求api瓶颈&lt;/p&gt;

&lt;p&gt;现在使用consul的k/v格式进行注册，直接通过IP：port作为key，对应的label作为vaule，然后使用consul-template动态生成discovery的json文件，然后prometheus使用file sd来发现这个json文件，相当于将对应的json的内容写到了prometheus的配置文件中去，这个时候五分钟consul-template动态生成一次，不会每次都去请求，这样consul的压力就几乎没有了，经过测试可以达到5000个target，prometheus的shard极限，对consul依旧没有什么压力，现在主要瓶颈在于json文件大小，filesd的压力，可以继续优化成多个文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;自动刷新配置文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;由于Prometheus是“拉”的方式主动监测，所以需要在server端指定被监控节点的列表。当被监控的节点增多之后，每次增加节点都需要更改配置文件，非常麻烦，我这里用consul-template+consul动态生成配置文件，这种方式同样适用于其他需要频繁更改配置文件的服务。另外一种解决方案是etcd+confd，基本现在主流的动态配置系统分这两大阵营。consul-template的定位和confd差不多，不过它是consul自家推出的模板系统。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- prometheus监控方案</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/monitor-scheme/</link>
          <pubDate>Wed, 13 Jun 2018 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/monitor-scheme/</guid>
          <description>&lt;p&gt;prometheus生态可以构建一个完整的监控平台，包括采集数据、分析存储数据、展示数据、告警等一系列操作，我们来看看他在原始的基础设施监控和新兴的容器监控中如何架构落地。&lt;/p&gt;

&lt;h1 id=&#34;infrastructure监控实现&#34;&gt;Infrastructure监控实现&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/infrastructure-base/&#34;&gt;KVM监控方案&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;k8s监控实现&#34;&gt;k8s监控实现&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s-base/&#34;&gt;容器监控方案&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Operator</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/</link>
          <pubDate>Tue, 12 Jun 2018 16:57:05 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/</guid>
          <description>&lt;p&gt;Prometheus-Operator是一套为了方便整合prometheus和kubernetes的开源方案，使用Prometheus-Operator可以非常简单的在kubernetes集群中部署Prometheus服务，用户能够使用简单的声明性配置来配置和管理Prometheus实例，这些配置将响应、创建、配置和管理Prometheus监控实例。&lt;/p&gt;

&lt;h1 id=&#34;operator&#34;&gt;operator&lt;/h1&gt;

&lt;p&gt;Operator是由CoreOS公司开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的一些专业知识，比如创建一个数据库的Operator，则必须对创建的数据库的各种运维方式非常了解，创建Operator的关键是CRD（自定义资源）的设计。&lt;/p&gt;

&lt;p&gt;CRD是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在YAML文件里定义的那些spec都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。&lt;/p&gt;

&lt;p&gt;Operator是将运维人员对软件操作的知识给代码化，同时利用 Kubernetes 强大的抽象来管理大规模的软件应用。目前CoreOS官方提供了几种Operator的实现，其中就包括我们今天的主角：Prometheus Operator，Operator的核心实现就是基于 Kubernetes 的以下两个概念：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;资源：对象的状态定义&lt;/li&gt;
&lt;li&gt;控制器：观测、分析和行动，以调节资源的分布&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然我们如果有对应的需求也完全可以自己去实现一个Operator，接下来我们就来给大家详细介绍下Prometheus-Operator的使用方法&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;基本架构&#34;&gt;基本架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/operator/operator.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、Operator： 根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心，也就是我们常用的控制器，可见operater让prometheus更加k8s。&lt;/p&gt;

&lt;p&gt;2、Prometheus：声明 Prometheus deployment 期望的状态，Operator 确保这个 deployment 运行时一直与定义保持一致。这边是一个资源类型，和下一个具体的prometheus是有区别的。&lt;/p&gt;

&lt;p&gt;3、Prometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。&lt;/p&gt;

&lt;p&gt;4、ServiceMonitor：声明指定监控的服务，描述了一组被 Prometheus 监控的目标列表。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。&lt;/p&gt;

&lt;p&gt;5、Service：简单的说就是 Prometheus 监控的对象。&lt;/p&gt;

&lt;p&gt;6、Alertmanager：定义 AlertManager deployment 期望的状态，Operator 确保这个 deployment 运行时一直与定义保持一致。&lt;/p&gt;

&lt;p&gt;这边涉及来operater定义的几种crd类型。&lt;/p&gt;

&lt;h2 id=&#34;crd&#34;&gt;CRD&lt;/h2&gt;

&lt;p&gt;Prometheus Operater 定义了如下的六类自定义资源（CRD）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Prometheus：部署prometheus
ServiceMonitor：服务发现拉去列表基于service
Alertmanager：部署alertmanager
PrometheusRule：告警规则
ThanosRuler：部署thanos
PodMonitor：服务发现拉去列表基于pod
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;prometheus&#34;&gt;Prometheus&lt;/h3&gt;

&lt;p&gt;Prometheus 自定义资源（CRD）声明了在 Kubernetes 集群中运行的 Prometheus 的期望设置。包含了副本数量，持久化存储，以及 Prometheus 实例发送警告到的 Alertmanagers等配置选项。&lt;/p&gt;

&lt;p&gt;每一个 Prometheus 资源，Operator 都会在相同 namespace 下部署成一个正确配置的 StatefulSet，Prometheus 的 Pod 都会挂载一个名为 &lt;prometheus-name&gt; 的 Secret，里面包含了 Prometheus 的配置。Operator 根据包含的 ServiceMonitor 生成配置，并且更新含有配置的 Secret。无论是对 ServiceMonitors 或者 Prometheus 的修改，都会持续不断的被按照前面的步骤更新。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Prometheus
metadata: # 略
spec:
  alerting:
    alertmanagers:
    - name: prometheus-prometheus-oper-alertmanager # 定义该 Prometheus 对接的 Alertmanager 集群的名字, 在 default 这个 namespace 中
      namespace: default
      pathPrefix: /
      port: web
  baseImage: quay.io/prometheus/prometheus
  replicas: 2 # 定义该 Proemtheus “集群”有两个副本，说是集群，其实 Prometheus 自身不带集群功能，这里只是起两个完全一样的 Prometheus 来避免单点故障
  ruleSelector: # 定义这个 Prometheus 需要使用带有 prometheus=k8s 且 role=alert-rules 标签的 PrometheusRule
    matchLabels:
      prometheus: k8s
      role: alert-rules
  serviceMonitorNamespaceSelector: {} # 定义这些 Prometheus 在哪些 namespace 里寻找 ServiceMonitor
  serviceMonitorSelector: # 定义这个 Prometheus 需要使用带有 k8s-app=node-exporter 标签的 ServiceMonitor，不声明则会全部选中
    matchLabels:
      k8s-app: node-exporter
  version: v2.10.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;servicemonitor&#34;&gt;ServiceMonitor&lt;/h3&gt;

&lt;p&gt;ServiceMonitor 自定义资源(CRD)能够声明如何监控一组动态服务的定义。它使用标签选择定义一组需要被监控的服务。主要通过Selector来依据 Labels 选取对应的Service的endpoints，并让 Prometheus Server 通过 Service 进行拉取指标。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: ServiceMonitor
metadata:
  labels:
    k8s-app: node-exporter # 这个 ServiceMonitor 对象带有 k8s-app=node-exporter 标签，因此会被 Prometheus 选中
  name: node-exporter
  namespace: default
spec:
  selector:
    matchLabels: # 定义需要监控的 Endpoints，带有 app=node-exporter 且 k8s-app=node-exporter标签的 Endpoints 会被选中
      app: node-exporter
      k8s-app: node-exporter
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    interval: 30s # 定义这些 Endpoints 需要每 30 秒抓取一次
    targetPort: 9100 # 定义这些 Endpoints 的指标端口为 9100
    scheme: https
  jobLabel: k8s-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spec 的 endpoints 部分用于配置需要收集 metrics 的 Endpoints 的端口和其他参数。endpoints（小写）是 ServiceMonitor CRD 中的一个字段，而 Endpoints（大写）是 Kubernetes 资源类型。&lt;/p&gt;

&lt;p&gt;Spec 下的 namespaceSelector 可以现在允许发现 Endpoints 对象的命名空间。要发现所有命名空间下的目标，namespaceSelector 必须为空。&lt;/p&gt;

&lt;h3 id=&#34;alertmanager&#34;&gt;Alertmanager&lt;/h3&gt;

&lt;p&gt;Alertmanager 自定义资源(CRD)声明在 Kubernetes 集群中运行的 Alertmanager 的期望设置。它也提供了配置副本集和持久化存储的选项。&lt;/p&gt;

&lt;p&gt;每一个 Alertmanager 资源，Operator 都会在相同 namespace 下部署成一个正确配置的 StatefulSet。Alertmanager pods 配置挂载一个名为 &lt;alertmanager-name&gt; 的 Secret， 使用 alertmanager.yaml key 对作为配置文件。&lt;/p&gt;

&lt;p&gt;当有两个或更多配置的副本时，Operator 可以高可用性模式运行Alertmanager实例。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Alertmanager #  一个 Alertmanager 对象
metadata:
  name: prometheus-prometheus-oper-alertmanager
spec:
  baseImage: quay.io/prometheus/alertmanager
  replicas: 3      # 定义该 Alertmanager 集群的节点数为 3
  version: v0.17.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;prometheusrule&#34;&gt;PrometheusRule&lt;/h3&gt;

&lt;p&gt;PrometheusRule CRD 声明一个或多个 Prometheus 实例需要的 Prometheus rule。&lt;/p&gt;

&lt;p&gt;Alerts 和 recording rules 可以保存并应用为 yaml 文件，可以被动态加载而不需要重启。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PrometheusRule
metadata:
  labels: # 定义该 PrometheusRule 的 label, 显然它会被 Prometheus 选中
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
spec:
  groups:
  - name: k8s.rules
    rules: # 定义了一组规则，其中只有一条报警规则，用来报警 kubelet 是不是挂了
    - alert: KubeletDown
      annotations:
        message: Kubelet has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job=&amp;quot;kubelet&amp;quot;} == 1)
      for: 15m
      labels:
        severity: critical
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;thanosruler&#34;&gt;ThanosRuler&lt;/h3&gt;

&lt;p&gt;ThanosRuler 自定义资源(CRD)声明在 Kubernetes 集群中运行的 thanos 的期望设置。它也提供了配置副本集和持久化存储的选项。&lt;/p&gt;

&lt;h3 id=&#34;podmonitor&#34;&gt;PodMonitor&lt;/h3&gt;

&lt;p&gt;直接对接pod。&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;h2 id=&#34;部署prometheus-operater&#34;&gt;部署prometheus-operater&lt;/h2&gt;

&lt;p&gt;operater安装直接使用yaml安装就好了，先clone项目&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/coreos/prometheus-operator.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在项目目录下有一个bundle.yaml定义了各种crd资源和operater的镜像启动配置清单，直接运行就好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:prometheus-operator chunyinjiang$ kubectl apply -f bundle.yaml
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
serviceaccount/prometheus-operator created
service/prometheus-operator created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见在default的namespace下创建了prometheus-operator的sa，service，deployment应用，还有授权role以及CRD。&lt;/p&gt;

&lt;p&gt;最新的版本官方将资源&lt;a href=&#34;https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus&#34;&gt;https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus&lt;/a&gt; 迁移到了独立的git仓库中：&lt;a href=&#34;https://github.com/coreos/kube-prometheus.git，&#34;&gt;https://github.com/coreos/kube-prometheus.git，&lt;/a&gt; 我们也可以直接使用这里面setup的yaml文件来部署prometheus-operater，这个项目中还有prometheus相关生态的部署yaml，可以参考使用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/coreos/kube-prometheus.git
cd manifests/setup
$ ls
00namespace-namespace.yaml                                         node-exporter-clusterRole.yaml
0prometheus-operator-0alertmanagerCustomResourceDefinition.yaml    node-exporter-daemonset.yaml
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后直接部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:setup chunyinjiang$ kubectl apply -f .
namespace/monitoring created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
service/prometheus-operator created
serviceaccount/prometheus-operator created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get all -n monitoring
NAME                                       READY   STATUS    RESTARTS   AGE
pod/prometheus-operator-6f98f66b89-dggn6   2/2     Running   0          20h

NAME                          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
service/prometheus-operator   ClusterIP   None         &amp;lt;none&amp;gt;        8443/TCP   20h

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/prometheus-operator   1/1     1            1           20h

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/prometheus-operator-6f98f66b89   1         1         1       20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看crd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get crd | grep monitoring
alertmanagers.monitoring.coreos.com     2020-06-19T11:34:06Z
podmonitors.monitoring.coreos.com       2020-06-19T11:34:06Z
prometheuses.monitoring.coreos.com      2020-06-19T11:34:06Z
prometheusrules.monitoring.coreos.com   2020-06-19T11:34:06Z
servicemonitors.monitoring.coreos.com   2020-06-19T11:34:07Z
thanosrulers.monitoring.coreos.com      2020-06-19T11:34:07Z
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;部署prometheus生态&#34;&gt;部署prometheus生态&lt;/h2&gt;

&lt;p&gt;直接使用kube-prometheus的yaml进行部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl apply -f .
alertmanager.monitoring.coreos.com/main created
secret/alertmanager-main created
service/alertmanager-main created
serviceaccount/alertmanager-main created
servicemonitor.monitoring.coreos.com/alertmanager created
secret/grafana-datasources created
configmap/grafana-dashboard-apiserver created
configmap/grafana-dashboard-cluster-total created
configmap/grafana-dashboard-controller-manager created
configmap/grafana-dashboard-k8s-resources-cluster created
configmap/grafana-dashboard-k8s-resources-namespace created
configmap/grafana-dashboard-k8s-resources-node created
configmap/grafana-dashboard-k8s-resources-pod created
configmap/grafana-dashboard-k8s-resources-workload created
configmap/grafana-dashboard-k8s-resources-workloads-namespace created
configmap/grafana-dashboard-kubelet created
configmap/grafana-dashboard-namespace-by-pod created
configmap/grafana-dashboard-namespace-by-workload created
configmap/grafana-dashboard-node-cluster-rsrc-use created
configmap/grafana-dashboard-node-rsrc-use created
configmap/grafana-dashboard-nodes created
configmap/grafana-dashboard-persistentvolumesusage created
configmap/grafana-dashboard-pod-total created
configmap/grafana-dashboard-prometheus-remote-write created
configmap/grafana-dashboard-prometheus created
configmap/grafana-dashboard-proxy created
configmap/grafana-dashboard-scheduler created
configmap/grafana-dashboard-statefulset created
configmap/grafana-dashboard-workload-total created
configmap/grafana-dashboards created
deployment.apps/grafana created
service/grafana created
serviceaccount/grafana created
servicemonitor.monitoring.coreos.com/grafana created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
deployment.apps/kube-state-metrics created
service/kube-state-metrics created
serviceaccount/kube-state-metrics created
servicemonitor.monitoring.coreos.com/kube-state-metrics created
clusterrole.rbac.authorization.k8s.io/node-exporter created
clusterrolebinding.rbac.authorization.k8s.io/node-exporter created
daemonset.apps/node-exporter created
service/node-exporter created
serviceaccount/node-exporter created
servicemonitor.monitoring.coreos.com/node-exporter created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io configured
clusterrole.rbac.authorization.k8s.io/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created
clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created
clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created
configmap/adapter-config created
deployment.apps/prometheus-adapter created
rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created
service/prometheus-adapter created
serviceaccount/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/prometheus-k8s created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus-operator created
prometheus.monitoring.coreos.com/k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s-config created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
prometheusrule.monitoring.coreos.com/prometheus-k8s-rules created
service/prometheus-k8s created
serviceaccount/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus created
servicemonitor.monitoring.coreos.com/kube-apiserver created
servicemonitor.monitoring.coreos.com/coredns created
servicemonitor.monitoring.coreos.com/kube-controller-manager created
servicemonitor.monitoring.coreos.com/kube-scheduler created
servicemonitor.monitoring.coreos.com/kubelet created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get all -n monitoring
NAME                                       READY   STATUS              RESTARTS   AGE
pod/alertmanager-main-0                    0/2     ContainerCreating   0          3m16s
pod/alertmanager-main-1                    0/2     ContainerCreating   0          3m16s
pod/alertmanager-main-2                    0/2     ContainerCreating   0          3m16s
pod/grafana-5c55845445-7tdhk               0/1     ContainerCreating   0          3m15s
pod/kube-state-metrics-957fd6c75-sqntg     0/3     ContainerCreating   0          3m14s
pod/node-exporter-tnftm                    0/2     ContainerCreating   0          3m14s
pod/prometheus-adapter-5cdcdf9c8d-xpxz4    1/1     Running             0          3m15s
pod/prometheus-k8s-0                       0/3     ContainerCreating   0          3m13s
pod/prometheus-k8s-1                       0/3     ContainerCreating   0          3m13s
pod/prometheus-operator-6f98f66b89-dggn6   2/2     Running             0          20h

NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
service/alertmanager-main       ClusterIP   10.106.202.8    &amp;lt;none&amp;gt;        9093/TCP                     3m17s
service/alertmanager-operated   ClusterIP   None            &amp;lt;none&amp;gt;        9093/TCP,9094/TCP,9094/UDP   3m17s
service/grafana                 ClusterIP   10.98.82.99     &amp;lt;none&amp;gt;        3000/TCP                     3m16s
service/kube-state-metrics      ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP,9443/TCP            3m16s
service/node-exporter           ClusterIP   None            &amp;lt;none&amp;gt;        9100/TCP                     3m15s
service/prometheus-adapter      ClusterIP   10.98.119.241   &amp;lt;none&amp;gt;        443/TCP                      3m15s
service/prometheus-k8s          ClusterIP   10.104.199.30   &amp;lt;none&amp;gt;        9090/TCP                     3m14s
service/prometheus-operated     ClusterIP   None            &amp;lt;none&amp;gt;        9090/TCP                     3m15s
service/prometheus-operator     ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP                     20h

NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/node-exporter   1         1         0       1            0           kubernetes.io/os=linux   3m15s

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana               0/1     1            0           3m16s
deployment.apps/kube-state-metrics    0/1     1            0           3m16s
deployment.apps/prometheus-adapter    1/1     1            1           3m15s
deployment.apps/prometheus-operator   1/1     1            1           20h

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-5c55845445               1         1         0       3m16s
replicaset.apps/kube-state-metrics-957fd6c75     1         1         0       3m16s
replicaset.apps/prometheus-adapter-5cdcdf9c8d    1         1         1       3m15s
replicaset.apps/prometheus-operator-6f98f66b89   1         1         1       20h

NAME                                 READY   AGE
statefulset.apps/alertmanager-main   0/3     3m17s
statefulset.apps/prometheus-k8s      0/2     3m15s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到资源正在创建，拉去镜像可能需要一点事件，其中 alertmanager 和 prometheus 是用 StatefulSet 控制器管理的。&lt;/p&gt;

&lt;p&gt;可以看到上面针对 grafana 和 prometheus 都创建了一个类型为 ClusterIP 的 Service，当然如果我们想要在外网访问这两个服务的话可以通过创建对应的 Ingress 对象或者使用 NodePort 类型的 Service，我们这里为了简单，直接使用 NodePort 类型的服务即可，编辑 grafana 和 prometheus-k8s 这两个 Service，将服务类型更改为 NodePort:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type: ClusterIp   --&amp;gt; NodePort

MacBook-Pro:manifests chunyinjiang$ kubectl edit svc prometheus-k8s -n monitoring
service/prometheus-k8s edited
MacBook-Pro:manifests chunyinjiang$ kubectl edit svc grafana -n monitoring
service/grafana edited
MacBook-Pro:manifests chunyinjiang$ kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.106.85.214   &amp;lt;none&amp;gt;        9093/TCP                     95m
alertmanager-operated   ClusterIP   None            &amp;lt;none&amp;gt;        9093/TCP,9094/TCP,9094/UDP   95m
grafana                 NodePort    10.106.80.1     &amp;lt;none&amp;gt;        3000:30267/TCP               95m
kube-state-metrics      ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP,9443/TCP            95m
node-exporter           ClusterIP   None            &amp;lt;none&amp;gt;        9100/TCP                     95m
prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      95m
prometheus-k8s          NodePort    10.103.177.44   &amp;lt;none&amp;gt;        9090:31174/TCP               95m
prometheus-operated     ClusterIP   None            &amp;lt;none&amp;gt;        9090/TCP                     95m
prometheus-operator     ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP                     22h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更改完成后，我们就可以通过NodeIP:NodePort去访问上面的两个服务了，比如查看 prometheus 的 targets 页面：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/operator/operater.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;至此基本的prometheus生态组件就部署好了，但是可以看到kube-controller-manager 和 kube-scheduler 这两个系统组件并没有taeget，这就和 ServiceMonitor 的定义有关系了，我们刚好研究一下ServiceMonitor&lt;/p&gt;

&lt;p&gt;我们查看ServiceMonitor这种crd的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get servicemonitors.monitoring.coreos.com -n monitoring
NAME                      AGE
alertmanager              101m
coredns                   101m
grafana                   101m
kube-apiserver            101m
kube-controller-manager   101m
kube-scheduler            101m
kube-state-metrics        101m
kubelet                   101m
node-exporter             101m
prometheus                101m
prometheus-operator       101m
MacBook-Pro:manifests chunyinjiang$ kubectl describe servicemonitors.monitoring.coreos.com kube-scheduler -n monitoring
Name:         kube-scheduler
Namespace:    monitoring
Labels:       k8s-app=kube-scheduler
Annotations:  API Version:  monitoring.coreos.com/v1
Kind:         ServiceMonitor
Metadata:
  Creation Timestamp:  2020-06-20T08:04:50Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:kubectl.kubernetes.io/last-applied-configuration:
        f:labels:
          .:
          f:k8s-app:
      f:spec:
        .:
        f:endpoints:
        f:jobLabel:
        f:namespaceSelector:
          .:
          f:matchNames:
        f:selector:
          .:
          f:matchLabels:
            .:
            f:k8s-app:
    Manager:         kubectl
    Operation:       Update
    Time:            2020-06-20T08:04:50Z
  Resource Version:  862846
  Self Link:         /apis/monitoring.coreos.com/v1/namespaces/monitoring/servicemonitors/kube-scheduler
  UID:               07132145-1db1-4847-a2a1-347cc014a80e
Spec:
  Endpoints:
    Interval:  30s
    Port:      http-metrics
  Job Label:   k8s-app
  Namespace Selector:
    Match Names:
      kube-system
  Selector:
    Match Labels:
      k8s-app:  kube-scheduler
Events:         &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到每个监控的应用都是使用ServiceMonitor部署了。我们再来看看对应的资源配置清单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-serviceMonitorKubeScheduler.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: kube-scheduler
  name: kube-scheduler
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s
    port: http-metrics
  jobLabel: k8s-app
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      k8s-app: kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是一个典型的 ServiceMonitor 资源文件的声明方式，上面我们通过selector.matchLabels在 kube-system 这个命名空间下面匹配具有k8s-app=kube-scheduler这样的 Service，我们来看看service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc -n kube-system -L k8s-app
NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE    K8S-APP
kube-dns         ClusterIP   10.96.0.10      &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP         11d    kube-dns
kubelet          ClusterIP   None            &amp;lt;none&amp;gt;        10250/TCP,10255/TCP,4194/TCP   26h    kubelet
metrics-server   ClusterIP   10.111.196.64   &amp;lt;none&amp;gt;        443/TCP                        2d6h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是我们系统中根本就没有对应的 Service，所以我们需要手动创建一个 Service：（prometheus-kubeSchedulerService.yaml）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-scheduler
  labels:
    k8s-app: kube-scheduler
spec:
  selector:
    component: kube-scheduler
  ports:
  - name: http-metrics
    port: 10251
    targetPort: 10251
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10251是kube-scheduler组件 metrics 数据所在的端口，10252是kube-controller-manager组件的监控数据所在端口。&lt;/p&gt;

&lt;p&gt;其中最重要的是上面 labels 和 selector 部分，labels 区域的配置必须和我们上面的 ServiceMonitor 对象中的 selector 保持一致，selector下面配置的是component=kube-scheduler，为什么会是这个 label 标签呢，我们可以去 describe 下 kube-scheduelr 这个 Pod：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pod kube-scheduler-minikube -n kube-system
Name:                 kube-scheduler-minikube
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 minikube/192.168.99.101
Start Time:           Sat, 20 Jun 2020 17:25:48 +0800
Labels:               component=kube-scheduler
                      tier=control-plane
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到这个 Pod 具有component=kube-scheduler和tier=control-plane这两个标签，而前面这个标签具有更唯一的特性，所以使用前面这个标签较好，这样上面创建的 Service 就可以和我们的 Pod 进行关联了，直接创建即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl apply -f prometheus-kubeSchedulerService.yaml
service/kube-scheduler created
MacBook-Pro:manifests chunyinjiang$ kubectl get svc -n kube-system -L k8s-app
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                        AGE    K8S-APP
kube-dns         ClusterIP   10.96.0.10       &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP         14d    kube-dns
kube-scheduler   ClusterIP   10.105.229.159   &amp;lt;none&amp;gt;        10251/TCP                      3m5s   kube-scheduler
kubelet          ClusterIP   None             &amp;lt;none&amp;gt;        10250/TCP,10255/TCP,4194/TCP   4d1h   kubelet
metrics-server   ClusterIP   10.111.196.64    &amp;lt;none&amp;gt;        443/TCP                        5d4h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 targets 下面 kube-scheduler 的状态，可以看到已经发现了，并且有数据了。&lt;/p&gt;

&lt;p&gt;kube-controller-manager 也是一样的操作。下面我们就可以通过自定义的grafana视图来进行监控了。&lt;/p&gt;

&lt;h2 id=&#34;部署详情&#34;&gt;部署详情&lt;/h2&gt;

&lt;h3 id=&#34;prometheus-1&#34;&gt;prometheus&lt;/h3&gt;

&lt;p&gt;prometheus的所有信息都能重prometheus的ui界面进行查看，主要查看status的状态，我们重容器中查看一下。&lt;/p&gt;

&lt;p&gt;上面我们知道使用有状态的statefulset部署两个prometheus，我们来看一下他们的具体情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get pod -n monitoring | grep prometheus-k8s
prometheus-k8s-0                       3/3     Running   17         3d1h
prometheus-k8s-1                       3/3     Running   17         3d1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个pod中有三个容器，可以使用 kubectl describe pod prometheus-k8s-0 -n monitoring来查看这三个容器分别是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus：quay.io/prometheus/prometheus:v2.17.2
prometheus-config-reloader：quay.io/coreos/prometheus-config-reloader:v0.39.0
rules-configmap-reloader：jimmidyson/configmap-reload:v0.3.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们到prometheus中看看，可以理解这个就是启动了prometheus的实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl exec -ti prometheus-k8s-0 -c prometheus -n monitoring -- sh
/prometheus $ ps -ef | grep prome
    1 1000     11:13 /bin/prometheus --web.console.templates=/etc/prometheus/consoles --web.console.libraries=/etc/prometheus/console_libraries --config.file=/etc/prometheus/config_out/prometheus.env.yaml --storage.tsdb.path=/prometheus --storage.tsdb.retention.time=24h --web.enable-lifecycle --storage.tsdb.no-lockfile --web.route-prefix=/
   52 1000      0:00 grep prome
/prometheus $ cat /etc/prometheus/config_out/prometheus.env.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在节点中可以看到&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以看到每个job就是监控的一个组件，也就是serviceMonitor。主要监听组件alertmanager，coredns，grafana，kube-apiserver，kube-controller-manager，kube-scheduler，kube-state-metrics，kubelet，node-exporter，prometheus，prometheus-operator。&lt;/li&gt;
&lt;li&gt;在配置文件中并没有使用hash的模式来分集群进行采集，这边两个prometheus节点是双采，解决来单点问题&lt;/li&gt;
&lt;li&gt;使用的是kubernetes_sd_configs的服务发现模式&lt;/li&gt;
&lt;li&gt;数据存储24h，存储在/prometheus目录下&lt;/li&gt;
&lt;li&gt;监听端口9090，我们可以使用nodeport或者ingress来对外暴露&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个简单的部署只适合简单的小集群的使用，使用大集群的监控还需要将数据进行分片采集，远程存储聚合等方案。&lt;/p&gt;

&lt;p&gt;我们再看看config的container是做什么的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti prometheus-k8s-0 -c prometheus-config-reloader -n monitoring -- sh
/ $ ps -ef
PID   USER     TIME  COMMAND
    1 1000      0:01 /bin/prometheus-config-reloader --log-format=logfmt --reload-url=http://localhost:9090/-/reload --config-file=/etc/prometheus/config/prometheus.yaml.gz --config-envsubst-file=/etc/prometheus/config_out/pro
$ kubectl exec -ti prometheus-k8s-0 -c rules-configmap-reloader -n monitoring -- sh
/ $ ps -ef
\PID   USER     TIME  COMMAND
    1 1000      0:00 /configmap-reload --webhook-url=http://localhost:9090/-/reload --volume-dir=/etc/prometheus/rules/prometheus-k8s-rulefiles-0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实就是对配置文件和rule文件进行热加载。&lt;/p&gt;

&lt;h3 id=&#34;grafana&#34;&gt;grafana&lt;/h3&gt;

&lt;p&gt;grafana也是直接在k8s中用deployment进行部署的，只有一个节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod -n monitoring | grep grafana
grafana-5c55845445-bnln8               1/1     Running   2          3d3h
MacBook-Pro:manifests chunyinjiang$ kubectl exec -ti grafana-5c55845445-bnln8 -n monitoring -- sh
/usr/share/grafana $ ps -ef | grep grafana
    1 nobody    4:36 grafana-server --homepath=/usr/share/grafana --config=/etc/grafana/grafana.ini --packaging=docker cfg:default.log.mode=console cfg:default.paths.data=/var/lib/grafana cfg:default.paths.logs=/var/log/grafana cfg:default.paths.plugins=/var/lib/grafana/plugins cfg:default.paths.provisioning=/etc/grafana/provisioning
   30 nobody    0:00 grep grafana
/usr/share/grafana $ cat /etc/grafana/grafana.ini
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到一些内容&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;监听端口3000，我们可以使用nodeport或者ingress来对外暴露&lt;/li&gt;
&lt;li&gt;没有使用mysql数据库，使用sqlite数据库&lt;/li&gt;
&lt;li&gt;直接通过域名访问prometheus：&lt;a href=&#34;http://prometheus-k8s.monitoring.svc:9090&#34;&gt;http://prometheus-k8s.monitoring.svc:9090&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;alertmanager-1&#34;&gt;alertmanager&lt;/h3&gt;

&lt;p&gt;alertmanager使用的也是statefulset的方式进行部署的，我们看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get pod -n monitoring | grep alert
alertmanager-main-0                    2/2     Running   6          3d3h
alertmanager-main-1                    2/2     Running   7          3d3h
alertmanager-main-2                    2/2     Running   5          3d3h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个pod也有两个container。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;alertmanager：quay.io/prometheus/alertmanager:v0.20.0
config-reloader：jimmidyson/configmap-reload:v0.3.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;config-reloader肯定就是配置加载，我们看看alertmanager的实例吧&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti alertmanager-main-0 -c alertmanager -n monitoring -- sh
/alertmanager $ ps
PID   USER     TIME  COMMAND
    1 1000      2:12 /bin/alertmanager --config.file=/etc/alertmanager/config/alertmanager.yaml --cluster.listen-address=[172.17.0.35]:9094 --storage.path=/alertmanager --data.retention=120h --web.listen-address=:9093 --web.ro
   21 1000      0:00 sh
   26 1000      0:00 ps
/alertmanager $ cat /etc/alertmanager/config/alertmanager.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;adapter&#34;&gt;adapter&lt;/h3&gt;

&lt;p&gt;kubernetes apiserver 提供了两种 api 用于监控指标相关的操作，&lt;/p&gt;

&lt;p&gt;k8s-prometheus-adapter是将prometheus的metrics数据格式转换成k8s API接口能识别的格式，同时通过apiservice扩展的模式（声明apiservice）注册到kube-apiserver来给k8s进行调用。&lt;/p&gt;

&lt;p&gt;查看adapter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get all -n monitoring | grep adapter
pod/prometheus-adapter-66b9c9dd58-6bdbm    1/1     Running   0          14h
service/prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      8d
deployment.apps/prometheus-adapter    1/1     1            1           8d
replicaset.apps/prometheus-adapter-5cdcdf9c8d    0         0         0       8d
replicaset.apps/prometheus-adapter-66b9c9dd58    1         1         1       14h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用deployment来部署了两个副本的k8s-prometheus-adapter，然后启动了一个service。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ps -ef | grep adapter | grep -v grep
dbus     23281 23263  0 00:13 ?        00:00:20 /adapter --cert-dir=/var/run/serving-cert --config=/etc/adapter/config.yaml --logtostderr=true --metrics-relist-interval=1m --prometheus-url=http://192.168.99.101:31174/ --secure-port=6443
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--lister-kubeconfig=&amp;lt;path-to-kubeconfig&amp;gt;: 指定通信的kubeconfig
--metrics-relist-interval=&amp;lt;duration&amp;gt;: 获取指标的间隔，应该大于prometheus的采集间隔时间。
--prometheus-url=&amp;lt;url&amp;gt;: 连接到Prometheus的URL。
--config=&amp;lt;yaml-file&amp;gt; (-c): 配置文件，主要是Prometheus指标和关联的Kubernetes资源，以及如何在自定义指标API中显示这些指标。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再来看看配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rules:
- seriesQuery: &#39;nginx_vts_server_requests_total&#39;
  seriesFilters: []
  resources:
    overrides:
      kubernetes_namespace:
        resource: namespace
      kubernetes_pod_name:
        resource: pod
  name:
    matches: &amp;quot;^(.*)_total&amp;quot;
    as: &amp;quot;${1}_per_second&amp;quot;
  metricsQuery: (sum(rate(&amp;lt;&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}[1m])) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个带参数的 Prometheus 查询，其中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;seriesQuery：查询 Prometheus 的语句，通过这个查询语句查询到的所有指标都可以用于 HPA
seriesFilters：查询到的指标可能会存在不需要的，可以通过它过滤掉。
resources：通过 seriesQuery 查询到的只是指标，如果需要查询某个 Pod 的指标，肯定要将它的名称和所在的命名空间作为指标的标签进行查询，resources 就是将指标的标签和 k8s 的资源类型关联起来，最常用的就是 pod 和 namespace。有两种添加标签的方式，一种是 overrides，另一种是 template。

    overrides：它会将指标中的标签和 k8s 资源关联起来。上面示例中就是将指标中的 pod 和 namespace 标签和 k8s 中的 pod 和 namespace 关联起来，因为 pod 和 namespace 都属于核心 api 组，所以不需要指定 api 组。当我们查询某个 pod 的指标时，它会自动将 pod 的名称和名称空间作为标签加入到查询条件中。比如 nginx: {group: &amp;quot;apps&amp;quot;, resource: &amp;quot;deployment&amp;quot;} 这么写表示的就是将指标中 nginx 这个标签和 apps 这个 api 组中的 deployment 资源关联起来；
    template：通过 go 模板的形式。比如template: &amp;quot;kube_&amp;lt;&amp;lt;.Group&amp;gt;&amp;gt;_&amp;lt;&amp;lt;.Resource&amp;gt;&amp;gt;&amp;quot; 这么写表示，假如 &amp;lt;&amp;lt;.Group&amp;gt;&amp;gt; 为 apps，&amp;lt;&amp;lt;.Resource&amp;gt;&amp;gt; 为 deployment，那么它就是将指标中 kube_apps_deployment 标签和 deployment 资源关联起来。

name：用来给指标重命名的，之所以要给指标重命名是因为有些指标是只增的，比如以 total 结尾的指标。这些指标拿来做 HPA 是没有意义的，我们一般计算它的速率，以速率作为值，那么此时的名称就不能以 total 结尾了，所以要进行重命名。

    matches：通过正则表达式来匹配指标名，可以进行分组
    as：默认值为 $1，也就是第一个分组。as 为空就是使用默认值的意思。

metricsQuery：这就是 Prometheus 的查询语句了，前面的 seriesQuery 查询是获得 HPA 指标。当我们要查某个指标的值时就要通过它指定的查询语句进行了。可以看到查询语句使用了速率和分组，这就是解决上面提到的只增指标的问题。

    Series：表示指标名称
    LabelMatchers：附加的标签，目前只有 pod 和 namespace 两种，因此我们要在之前使用 resources 进行关联
    GroupBy：就是 pod 名称，同样需要使用 resources 进行关联。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看一下项目的的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-adapter-configMap.yaml
apiVersion: v1
data:
  config.yaml: |-
    &amp;quot;resourceRules&amp;quot;:
      &amp;quot;cpu&amp;quot;:
        &amp;quot;containerLabel&amp;quot;: &amp;quot;container&amp;quot;
        &amp;quot;containerQuery&amp;quot;: &amp;quot;sum(irate(container_cpu_usage_seconds_total{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;,container!=\&amp;quot;POD\&amp;quot;,container!=\&amp;quot;\&amp;quot;,pod!=\&amp;quot;\&amp;quot;}[5m])) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;nodeQuery&amp;quot;: &amp;quot;sum(1 - irate(node_cpu_seconds_total{mode=\&amp;quot;idle\&amp;quot;}[5m]) * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;resources&amp;quot;:
          &amp;quot;overrides&amp;quot;:
            &amp;quot;namespace&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;namespace&amp;quot;
            &amp;quot;node&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;node&amp;quot;
            &amp;quot;pod&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;pod&amp;quot;
      &amp;quot;memory&amp;quot;:
        &amp;quot;containerLabel&amp;quot;: &amp;quot;container&amp;quot;
        &amp;quot;containerQuery&amp;quot;: &amp;quot;sum(container_memory_working_set_bytes{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;,container!=\&amp;quot;POD\&amp;quot;,container!=\&amp;quot;\&amp;quot;,pod!=\&amp;quot;\&amp;quot;}) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;nodeQuery&amp;quot;: &amp;quot;sum(node_memory_MemTotal_bytes{job=\&amp;quot;node-exporter\&amp;quot;,&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;} - node_memory_MemAvailable_bytes{job=\&amp;quot;node-exporter\&amp;quot;,&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;resources&amp;quot;:
          &amp;quot;overrides&amp;quot;:
            &amp;quot;instance&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;node&amp;quot;
            &amp;quot;namespace&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;namespace&amp;quot;
            &amp;quot;pod&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;pod&amp;quot;
      &amp;quot;window&amp;quot;: &amp;quot;5m&amp;quot;
kind: ConfigMap
metadata:
  name: adapter-config
  namespace: monitoring
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置了两个规则，可以到prometheus的界面查看一下是可以查到的，下面我们需要提供给k8s，我们就需要使用聚合api，先注册apiservice&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl api-versions  | grep metrics
metrics.k8s.io/v1beta1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们就可以重/apis/metrics.k8s.io/v1beta1这个URL来获取指标，这是给resource metrics 使用的,主要是提供核心指标，这边是指向k8s-prometheus-adapter，所以是prometheus的采集的指标。&lt;/p&gt;

&lt;p&gt;还有是自定义指标，只要是prometheus采集的指标，都可以在上面的配置文件配置，然后都可以通过这个接口查询到，这种情况其实一般使用custom.metrics.k8s.io api接口来操作，&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-autoscaler/#基于prometheus&#34;&gt;具体使用可以在hpa场景下查看&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get --raw=&amp;quot;/apis/metrics.k8s.io/v1beta1&amp;quot;
{
    &amp;quot;kind&amp;quot;:&amp;quot;APIResourceList&amp;quot;,
    &amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,
    &amp;quot;groupVersion&amp;quot;:&amp;quot;metrics.k8s.io/v1beta1&amp;quot;,
    &amp;quot;resources&amp;quot;:[
        {
            &amp;quot;name&amp;quot;:&amp;quot;nodes&amp;quot;,
            &amp;quot;singularName&amp;quot;:&amp;quot;&amp;quot;,
            &amp;quot;namespaced&amp;quot;:false,
            &amp;quot;kind&amp;quot;:&amp;quot;NodeMetrics&amp;quot;,
            &amp;quot;verbs&amp;quot;:[
                &amp;quot;get&amp;quot;,
                &amp;quot;list&amp;quot;
            ]
        },
        {
            &amp;quot;name&amp;quot;:&amp;quot;pods&amp;quot;,
            &amp;quot;singularName&amp;quot;:&amp;quot;&amp;quot;,
            &amp;quot;namespaced&amp;quot;:true,
            &amp;quot;kind&amp;quot;:&amp;quot;PodMetrics&amp;quot;,
            &amp;quot;verbs&amp;quot;:[
                &amp;quot;get&amp;quot;,
                &amp;quot;list&amp;quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看出来这个接口主要获取了核心资源指标，比如nodes，pods，可以具体去查看一下。&lt;/p&gt;

&lt;h3 id=&#34;其他组件&#34;&gt;其他组件&lt;/h3&gt;

&lt;p&gt;主要是给proemtheus的采集的探针&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kube-state-metrics主要是为了暴露集群的一些状态
node-exporter主要是获取主机信息
其他组件集成prometheus的库，通过端口直接暴露metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;servicemonitor-1&#34;&gt;serviceMonitor&lt;/h3&gt;

&lt;p&gt;将所有组件的监控通过serviceMonitor来暴露采集。&lt;/p&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;h2 id=&#34;自定义servicemonitor&#34;&gt;自定义servicemonitor&lt;/h2&gt;

&lt;p&gt;添加一个自定义监控的步骤&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;第一步建立一个 ServiceMonitor 对象，用于 Prometheus 添加监控项
第二步为 ServiceMonitor 对象关联 metrics 数据接口的一个 Service 对象
第三步确保 Service 对象可以正确获取到 metrics 数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们以监控etcd为实例&lt;/p&gt;

&lt;p&gt;etcd 集群一般情况下，为了安全都会开启 https 证书认证的方式，所以要想让 Prometheus 访问到 etcd 集群的监控数据，就需要提供相应的证书校验。查看证书&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pod etcd-minikube  -n kube-system
...
    Command:
      etcd
      --advertise-client-urls=https://192.168.99.101:2379
      --cert-file=/var/lib/minikube/certs/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/minikube/etcd
      --initial-advertise-peer-urls=https://192.168.99.101:2380
      --initial-cluster=minikube=https://192.168.99.101:2380
      --key-file=/var/lib/minikube/certs/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://192.168.99.101:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://192.168.99.101:2380
      --name=minikube
      --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/var/lib/minikube/certs/etcd/peer.key
      --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到证书都在/var/lib/minikube/certs/etcd/下面，我们也可以通过kubectl get pod etcd-minikube  -n kube-system -o yaml来获取对应的配置，我们再来看看这个目录下的证书&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -lrth /var/lib/minikube/certs/etcd/
total 32K
-rw------- 1 root root 1.7K Jun  9 01:38 ca.key
-rw-r--r-- 1 root root 1017 Jun  9 01:38 ca.crt
-rw------- 1 root root 1.7K Jun  9 01:38 server.key
-rw-r--r-- 1 root root 1.2K Jun  9 01:38 server.crt
-rw------- 1 root root 1.7K Jun  9 01:38 peer.key
-rw-r--r-- 1 root root 1.2K Jun  9 01:38 peer.crt
-rw------- 1 root root 1.7K Jun  9 01:38 healthcheck-client.key
-rw-r--r-- 1 root root 1.1K Jun  9 01:38 healthcheck-client.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们需要将证书放到secret中给promehteus使用验证，创建secret就需要把这些证书拉到本地来进行创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret generic etcd-certs -n monitoring --from-file=./healthcheck-client.crt --from-file=./healthcheck-client.key --from-file=./ca.crt
secret/etcd-certs created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将上面创建的 etcd-certs 对象配置到 prometheus 资源对象中，直接更新 prometheus 资源对象即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get prometheus -n monitoring
NAME   VERSION   REPLICAS   AGE
k8s    v2.17.2   2          3d22h
$ kubectl edit prometheus k8s -n monitoring
prometheus.monitoring.coreos.com/k8s edited
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要在spec中新增secret给prometheus使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nodeSelector:
  beta.kubernetes.io/os: linux
replicas: 2
secrets:
- etcd-certs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新完成后，我们就可以在 Prometheus 的 Pod 中获取到上面创建的 etcd 证书文件了，具体的路径我们可以进入 Pod 中查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -it prometheus-k8s-0 -c prometheus /bin/sh -n monitoring
/prometheus $ ls -lrth /etc/prometheus/secrets/etcd-certs/
total 0
lrwxrwxrwx    1 root     root          29 Jun 24 06:48 healthcheck-client.key -&amp;gt; ..data/healthcheck-client.key
lrwxrwxrwx    1 root     root          29 Jun 24 06:48 healthcheck-client.crt -&amp;gt; ..data/healthcheck-client.crt
lrwxrwxrwx    1 root     root          13 Jun 24 06:48 ca.crt -&amp;gt; ..data/ca.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面就是创建ServiceMonitor资源配置清单prometheus-serviceMonitorEtcd.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: etcd-k8s
  namespace: monitoring
  labels:
    k8s-app: etcd-k8s
spec:
  jobLabel: k8s-app
  endpoints:
  - port: port
    interval: 30s
    scheme: https
    tlsConfig:
      caFile: /etc/prometheus/secrets/etcd-certs/ca.crt
      certFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.crt
      keyFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.key
      insecureSkipVerify: true
  selector:
    matchLabels:
      k8s-app: etcd
  namespaceSelector:
    matchNames:
    - kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在 monitoring 命名空间下面创建了名为 etcd-k8s 的 ServiceMonitor 对象&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;匹配 kube-system 这个命名空间下面的具有 k8s-app=etcd 这个 label 标签的 Service&lt;/li&gt;
&lt;li&gt;jobLabel 表示用于检索 job 任务名称的标签&lt;/li&gt;
&lt;li&gt;和前面不太一样的地方是 endpoints 属性的写法，配置上访问 etcd 的相关证书，endpoints 属性下面可以配置很多抓取的参数，比如 relabel、proxyUrl，tlsConfig 表示用于配置抓取监控数据端点的 tls 认证，由于证书 serverName 和 etcd 中签发的可能不匹配，所以加上了 insecureSkipVerify=true&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后就是创建来这个资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f prometheus-serviceMonitorEtcd.yaml
servicemonitor.monitoring.coreos.com &amp;quot;etcd-k8s&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候promehteus的配置文件中就新增一个job为etcd的监控，target是etcd的service，但是现在还没有关联的对应的 Service 对象，所以需要我们去手动创建一个 Service 对象prometheus-etcdService.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: port
    port: 2379
    protocol: TCP

---
apiVersion: v1
kind: Endpoints
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
subsets:
- addresses:
  - ip: 10.151.30.57
    nodeName: etc-master
  ports:
  - name: port
    port: 2379
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们这里创建的 Service 没有采用前面通过 label 标签的形式去匹配 Pod 的做法，因为前面我们说过很多时候我们创建的 etcd 集群是独立于集群之外的，这种情况下面我们就需要自定义一个 Endpoints，要注意 metadata 区域的内容要和 Service 保持一致，Service 的 clusterIP 设置为 None&lt;/p&gt;

&lt;p&gt;Endpoints 的 subsets 中填写 etcd 集群的地址即可&lt;/p&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f prometheus-etcdService.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面讲解的是独立于k8s之外的监控访问，前提是需要把网络打通，也是可以直接使用endpoint进行配置的，当然在集群内的监控常规就是匹配的pod的label。&lt;/p&gt;

&lt;p&gt;比如我们把etcd运行在k8s上，我们创建的service就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-etcdService.yaml
apiVersion: v1
kind: Service
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
spec:
  selector:
    component: etcd
  ports:
  - name: port
    port: 2379
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体在上面的schduler讲解过，不多说了，到此一个完整的监控就新增好了。&lt;/p&gt;

&lt;h2 id=&#34;自定义告警规则&#34;&gt;自定义告警规则&lt;/h2&gt;

&lt;p&gt;我们首先查看prometheus部署的时候的alert的配置，可以在prometheus的ui界面的config下查到&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;alerting:
  alert_relabel_configs:
  - separator: ;
    regex: prometheus_replica
    replacement: $1
    action: labeldrop
  alertmanagers:
  - kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - monitoring
    scheme: http
    path_prefix: /
    timeout: 10s
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name]
      separator: ;
      regex: alertmanager-main
      replacement: $1
      action: keep
    - source_labels: [__meta_kubernetes_endpoint_port_name]
      separator: ;
      regex: web
      replacement: $1
      action: keep
rule_files:
- /etc/prometheus/rules/prometheus-k8s-rulefiles-0/*.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过角色为 endpoints 的 kubernetes 的服务发现机制来知道需要发送的alert的地址&lt;/li&gt;
&lt;li&gt;匹配的是服务名为 alertmanager-main，端口名为 web 的 Service 服务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来看一下operator部署的svc&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.106.85.214   &amp;lt;none&amp;gt;        9093/TCP                     4d
alertmanager-operated   ClusterIP   None            &amp;lt;none&amp;gt;        9093/TCP,9094/TCP,9094/UDP   4d
grafana                 NodePort    10.106.80.1     &amp;lt;none&amp;gt;        3000:30267/TCP               4d
kube-state-metrics      ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP,9443/TCP            4d
node-exporter           ClusterIP   None            &amp;lt;none&amp;gt;        9100/TCP                     4d
prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      4d
prometheus-k8s          NodePort    10.103.177.44   &amp;lt;none&amp;gt;        9090:31174/TCP               4d
prometheus-operated     ClusterIP   None            &amp;lt;none&amp;gt;        9090/TCP                     4d
prometheus-operator     ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP                     4d20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确实有一个alertmanager-main的svc，我们查看一下他的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe svc alertmanager-main -n monitoring
Name:              alertmanager-main
Namespace:         monitoring
Labels:            alertmanager=main
Annotations:       Selector:  alertmanager=main,app=alertmanager
Type:              ClusterIP
IP:                10.106.85.214
Port:              web  9093/TCP
TargetPort:        web/TCP
Endpoints:         172.17.0.22:9093,172.17.0.7:9093,172.17.0.9:9093
Session Affinity:  ClientIP
Events:            &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到服务名正是 alertmanager-main，Port 定义的名称也是 web，符合上面的规则，所以 Prometheus 和 AlertManager 组件就正确关联上了。我们就可以将告警发送到对应的alertmanaager了。&lt;/p&gt;

&lt;p&gt;再来看告警规则在/etc/prometheus/rules/prometheus-k8s-rulefiles-0/目录下面所有的 YAML 文件。我们在部署prometheus的时候有一个规则资源配置清单prometheus-rules.yaml，就是我们现在看到的告警规则。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
  namespace: monitoring
spec:
  groups:
  - name: k8s.rules
    rules:
    - expr: |
        sum(rate(container_cpu_usage_seconds_total{job=&amp;quot;kubelet&amp;quot;, image!=&amp;quot;&amp;quot;, container_name!=&amp;quot;&amp;quot;}[5m])) by (namespace)
      record: namespace:container_cpu_usage_seconds_total:sum_rate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PrometheusRule 的 name 为 prometheus-k8s-rules，namespace 为 monitoring，我们可以猜想到我们创建一个 PrometheusRule 资源对象后，会自动在上面的 prometheus-k8s-rulefiles-0 目录下面生成一个对应的&lt;namespace&gt;-&lt;name&gt;.yaml文件，所以如果以后我们需要自定义一个报警选项的话，只需要定义一个 PrometheusRule 资源对象即可。&lt;/p&gt;

&lt;p&gt;至于为什么 Prometheus 能够识别这个 PrometheusRule 资源对象呢？&lt;/p&gt;

&lt;p&gt;创建的 prometheus 这个资源对象里面有非常重要的一个属性 ruleSelector，用来匹配 rule 规则的过滤器，要求匹配具有 prometheus=k8s 和 role=alert-rules 标签的 PrometheusRule 资源对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruleSelector:
  matchLabels:
    prometheus: k8s
    role: alert-rules
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以自定义一个报警规则，还需要需要创建一个具有 prometheus=k8s 和 role=alert-rules 标签的 PrometheusRule 对象。&lt;/p&gt;

&lt;p&gt;我们以etcd为例来自定义一个告警：如果不可用的 etcd 数量超过了一半那么就触发报警&lt;/p&gt;

&lt;p&gt;创建文件 prometheus-etcdRules.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: etcd-rules
  namespace: monitoring
spec:
  groups:
  - name: etcd
    rules:
    - alert: EtcdClusterUnavailable
      annotations:
        summary: etcd cluster small
        description: If one more etcd peer goes down the cluster will be unavailable
      expr: |
        count(up{job=&amp;quot;etcd&amp;quot;} == 0) &amp;gt; (count(up{job=&amp;quot;etcd&amp;quot;}) / 2 - 1)
      for: 3m
      labels:
        severity: critical
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建好了，我们就可以看到在对应目录下生成了一份yaml告警规则文件。&lt;/p&gt;

&lt;h2 id=&#34;配置告警方式&#34;&gt;配置告警方式&lt;/h2&gt;

&lt;p&gt;我们可以通过 AlertManager 的配置文件去配置各种报警接收器，首先我们将 alertmanager-main 这个 Service 改为 NodePort 类型的 Service，和前面的修改是一样的操作，修改完成后我们可以在页面上的 status 路径下面查看 AlertManager 的配置信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  resolve_timeout: 5m
  http_config: {}
  smtp_hello: localhost
  smtp_require_tls: true
  pagerduty_url: https://events.pagerduty.com/v2/enqueue
  hipchat_api_url: https://api.hipchat.com/
  opsgenie_api_url: https://api.opsgenie.com/
  wechat_api_url: https://qyapi.weixin.qq.com/cgi-bin/
  victorops_api_url: https://alert.victorops.com/integrations/generic/20131114/alert/
route:
  receiver: Default
  group_by:
  - namespace
  routes:
  - receiver: Watchdog
    match:
      alertname: Watchdog
  - receiver: Critical
    match:
      severity: critical
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
inhibit_rules:
- source_match:
    severity: critical
  target_match_re:
    severity: warning|info
  equal:
  - namespace
  - alertname
- source_match:
    severity: warning
  target_match_re:
    severity: info
  equal:
  - namespace
  - alertname
receivers:
- name: Default
- name: Watchdog
- name: Critical
templates: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些配置信息实际上是来自于我们之前在prometheus-operator/contrib/kube-prometheus/manifests目录下面创建的 alertmanager-secret.yaml 文件，将文件中 alertmanager.yaml 对应的 value 值做一个 base64 解码，内容和上面查看的配置信息是一致的。&lt;/p&gt;

&lt;p&gt;果我们想要添加自己的接收器，或者模板消息，我们就可以更改这个文件，比如我们添加了两个接收器，默认的通过邮箱进行发送，对于 CoreDNSDown 这个报警我们通过 webhook 来进行发送。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  resolve_timeout: 5m
  smtp_smarthost: &#39;smtp.163.com:25&#39;
  smtp_from: &#39;ych_1024@163.com&#39;
  smtp_auth_username: &#39;ych_1024@163.com&#39;
  smtp_auth_password: &#39;&amp;lt;邮箱密码&amp;gt;&#39;
  smtp_hello: &#39;163.com&#39;
  smtp_require_tls: false
route:
  group_by: [&#39;job&#39;, &#39;severity&#39;]
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: default
  routes:
  - receiver: webhook
    match:
      alertname: CoreDNSDown
receivers:
- name: &#39;default&#39;
  email_configs:
  - to: &#39;517554016@qq.com&#39;
    send_resolved: true
- name: &#39;webhook&#39;
  webhook_configs:
  - url: &#39;http://dingtalk-hook.kube-ops:5000&#39;
    send_resolved: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上面文件保存为 alertmanager.yaml，然后使用这个文件创建一个 Secret 对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 先将之前的 secret 对象删除
$ kubectl delete secret alertmanager-main -n monitoring
secret &amp;quot;alertmanager-main&amp;quot; deleted
$ kubectl create secret generic alertmanager-main --from-file=alertmanager.yaml -n monitoring
secret &amp;quot;alertmanager-main&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以完成发送的配置了。&lt;/p&gt;

&lt;h2 id=&#34;自动发现配置&#34;&gt;自动发现配置&lt;/h2&gt;

&lt;p&gt;如果在我们的 Kubernetes 集群中有了很多的 Service/Pod，那么我们都需要一个一个的去建立一个对应的 ServiceMonitor 对象来进行监控吗？这样岂不是又变得麻烦起来了？&lt;/p&gt;

&lt;p&gt;我们可以通过添加额外的配置来进行服务发现进行自动监控。配置如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;kubernetes-service-endpoints&#39;
  kubernetes_sd_configs:
  - role: endpoints
  relabel_configs:
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
    action: replace
    target_label: __scheme__
    regex: (https?)
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
    action: replace
    target_label: __metrics_path__
    regex: (.+)
  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
    action: replace
    target_label: __address__
    regex: ([^:]+)(?::\d+)?;(\d+)
    replacement: $1:$2
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    action: replace
    target_label: kubernetes_name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上面文件直接保存为 prometheus-additional.yaml，然后通过这个文件创建一个对应的 Secret 对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret generic additional-configs --from-file=prometheus-additional.yaml -n monitoring
secret &amp;quot;additional-configs&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建完成后，会将上面配置信息进行 base64 编码后作为 prometheus-additional.yaml 这个 key 对应的值存在&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get secret additional-configs -n monitoring -o yaml
apiVersion: v1
data:
  prometheus-additional.yaml: LSBqb2JfbmFtZTogJ2t1YmVybmV0ZXMtc2VydmljZS1lbmRwb2ludHMnCiAga3ViZXJuZXRlc19zZF9jb25maWdzOgogIC0gcm9sZTogZW5kcG9pbnRzCiAgcmVsYWJlbF9jb25maWdzOgogIC0gc291cmNlX2xhYmVsczogW19fbWV0YV9rdWJlcm5ldGVzX3NlcnZpY2VfYW5ub3RhdGlvbl9wcm9tZXRoZXVzX2lvX3NjcmFwZV0KICAgIGFjdGlvbjoga2VlcAogICAgcmVnZXg6IHRydWUKICAtIHNvdXJjZV9sYWJlbHM6IFtfX21ldGFfa3ViZXJuZXRlc19zZXJ2aWNlX2Fubm90YXRpb25fcHJvbWV0aGV1c19pb19zY2hlbWVdCiAgICBhY3Rpb246IHJlcGxhY2UKICAgIHRhcmdldF9sYWJlbDogX19zY2hlbWVfXwogICAgcmVnZXg6IChodHRwcz8pCiAgLSBzb3VyY2VfbGFiZWxzOiBbX19tZXRhX2t1YmVybmV0ZXNfc2VydmljZV9hbm5vdGF0aW9uX3Byb21ldGhldXNfaW9fcGF0aF0KICAgIGFjdGlvbjogcmVwbGFjZQogICAgdGFyZ2V0X2xhYmVsOiBfX21ldHJpY3NfcGF0aF9fCiAgICByZWdleDogKC4rKQogIC0gc291cmNlX2xhYmVsczogW19fYWRkcmVzc19fLCBfX21ldGFfa3ViZXJuZXRlc19zZXJ2aWNlX2Fubm90YXRpb25fcHJvbWV0aGV1c19pb19wb3J0XQogICAgYWN0aW9uOiByZXBsYWNlCiAgICB0YXJnZXRfbGFiZWw6IF9fYWRkcmVzc19fCiAgICByZWdleDogKFteOl0rKSg/OjpcZCspPzsoXGQrKQogICAgcmVwbGFjZW1lbnQ6ICQxOiQyCiAgLSBhY3Rpb246IGxhYmVsbWFwCiAgICByZWdleDogX19tZXRhX2t1YmVybmV0ZXNfc2VydmljZV9sYWJlbF8oLispCiAgLSBzb3VyY2VfbGFiZWxzOiBbX19tZXRhX2t1YmVybmV0ZXNfbmFtZXNwYWNlXQogICAgYWN0aW9uOiByZXBsYWNlCiAgICB0YXJnZXRfbGFiZWw6IGt1YmVybmV0ZXNfbmFtZXNwYWNlCiAgLSBzb3VyY2VfbGFiZWxzOiBbX19tZXRhX2t1YmVybmV0ZXNfc2VydmljZV9uYW1lXQogICAgYWN0aW9uOiByZXBsYWNlCiAgICB0YXJnZXRfbGFiZWw6IGt1YmVybmV0ZXNfbmFtZQo=
kind: Secret
metadata:
  creationTimestamp: 2018-12-20T14:50:35Z
  name: additional-configs
  namespace: monitoring
  resourceVersion: &amp;quot;41814998&amp;quot;
  selfLink: /api/v1/namespaces/monitoring/secrets/additional-configs
  uid: 9bbe22c5-0466-11e9-a777-525400db4df7
type: Opaque
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们只需要在声明创建 prometheus 的资源对象文件中添加上这个额外的配置，其实就是通过secret将这段配置挂载到prometheus上去，作为prometheus的配置文件的一部分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  additionalScrapeConfigs:
    name: additional-configs
    key: prometheus-additional.yaml
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以自动发现service信息了，这边还有一个权限的问题，Prometheus 绑定了一个名为 prometheus-k8s 的 ServiceAccount 对象，而这个对象绑定的是一个名为 prometheus-k8s 的 ClusterRole，有对 Service 或者 Pod 的 list 权限，所以报错了，要解决这个问题，我们只需要添加上需要的权限即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - nodes
  - services
  - endpoints
  - pods
  - nodes/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - configmaps
  - nodes/metrics
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就有权限了，只需要我们在 Service 的annotation区域添加prometheus.io/scrape=true的声明，就会在服务创建的时候被自动发现。&lt;/p&gt;

&lt;h2 id=&#34;数据持久化&#34;&gt;数据持久化&lt;/h2&gt;

&lt;p&gt;查看生成的 Prometheus Pod 的挂载情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod prometheus-k8s-0 -n monitoring -o yaml
......
    volumeMounts:
    - mountPath: /etc/prometheus/config_out
      name: config-out
      readOnly: true
    - mountPath: /prometheus
      name: prometheus-k8s-db
......
  volumes:
......
  - emptyDir: {}
    name: prometheus-k8s-db
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到 Prometheus 的数据目录 /prometheus 实际上是通过 emptyDir 进行挂载的，我们知道 emptyDir 挂载的数据的生命周期和 Pod 生命周期一致的，所以如果 Pod 挂掉了，数据也就丢失了。&lt;/p&gt;

&lt;p&gt;对应线上的监控数据肯定需要做数据的持久化的，们的 Prometheus 最终是通过 Statefulset 控制器进行部署的，所以我们这里需要通过 storageclass 来做数据持久化，首先创建一个 StorageClass 对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: prometheus-data-db
provisioner: fuseim.pri/ifs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 provisioner=fuseim.pri/ifs，则是因为我们集群中使用的是 nfs 作为存储后端，将该文件保存为 prometheus-storageclass.yaml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f prometheus-storageclass.yaml
storageclass.storage.k8s.io &amp;quot;prometheus-data-db&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在 prometheus 的 CRD 资源对象中添加如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;storage:
  volumeClaimTemplate:
    spec:
      storageClassName: prometheus-data-db
      resources:
        requests:
          storage: 10Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意这里的 storageClassName 名字为上面我们创建的 StorageClass 对象名称，然后更新 prometheus 这个 CRD 资源。更新完成后会自动生成两个 PVC 和 PV 资源对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pvc -n monitoring
NAME                                 STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE
prometheus-k8s-db-prometheus-k8s-0   Bound     pvc-0cc03d41-047a-11e9-a777-525400db4df7   10Gi       RWO            prometheus-data-db   8m
prometheus-k8s-db-prometheus-k8s-1   Bound     pvc-1938de6b-047b-11e9-a777-525400db4df7   10Gi       RWO            prometheus-data-db   1m
$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                           STORAGECLASS         REASON    AGE
pvc-0cc03d41-047a-11e9-a777-525400db4df7   10Gi       RWO            Delete           Bound       monitoring/prometheus-k8s-db-prometheus-k8s-0   prometheus-data-db             2m
pvc-1938de6b-047b-11e9-a777-525400db4df7   10Gi       RWO            Delete           Bound       monitoring/prometheus-k8s-db-prometheus-k8s-1   prometheus-data-db             1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在我们再去看 Prometheus Pod 的数据目录就可以看到是关联到一个 PVC 对象上了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod prometheus-k8s-0 -n monitoring -o yaml
......
    volumeMounts:
    - mountPath: /etc/prometheus/config_out
      name: config-out
      readOnly: true
    - mountPath: /prometheus
      name: prometheus-k8s-db
......
  volumes:
......
  - name: prometheus-k8s-db
    persistentVolumeClaim:
      claimName: prometheus-k8s-db-prometheus-k8s-0
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在即使我们的 Pod 挂掉了，数据也不会丢失了。&lt;/p&gt;

&lt;h1 id=&#34;使用场景&#34;&gt;使用场景&lt;/h1&gt;

&lt;p&gt;什么时候prometheus使用的物理机部署的集群？&lt;/p&gt;

&lt;p&gt;1、取决于生产环境，比如要求prometheus不光需要监控k8s还需要监控kvm的机器&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;k8s的服务发现主要是通过定义serviceMonitor，或者service配置备注做自动发现，本质其实就是通过service来暴露指标，但是对于kvm没有这些机制，如何对kvm环境下的机器做服务发现就是一个问题&lt;/li&gt;
&lt;li&gt;k8s和kvm的网络打通也是一个问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、监控规模，数据量导致的稳定性&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当集群规模小，监控数据少的情况下部署单点的prometheus是够用的，但是如果监控规模扩展，数据量很大的时候，对资源，比如cpu和memory的要求比较高，对k8s来说是一个很重的应用，对于本身的稳定性也是一个很重要的考验。&lt;/li&gt;
&lt;li&gt;当规模达到一定的时候，需要分布式集群来处理，在k8s部署分布式集群也是有着很多的问题&lt;/li&gt;
&lt;li&gt;当规模扩大时候，分组也是一个很大的问题，单个prometheus的的采集分配也是问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以如果只是监控k8s使用operator部署k8s上都可以优化，但是如果加上kvm是很有必要在物理机上部署prometheus的监控的，需要设计完成的架构和实现方案。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Principle</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/</link>
          <pubDate>Sun, 13 May 2018 17:56:03 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/</guid>
          <description>&lt;p&gt;本篇文章主要是对prometheus的一些原理进行解析。&lt;/p&gt;

&lt;h1 id=&#34;启动流程解析&#34;&gt;启动流程解析&lt;/h1&gt;

&lt;p&gt;Prometheus 启动过程中，主要包含服务组件初始化，服务组件配置应用及启动各个服务组件三个部分，下面基于版本 v2.7.1，详细分析这三部分内容。&lt;/p&gt;

&lt;h2 id=&#34;服务组件初始化&#34;&gt;服务组件初始化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Storage组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prometheus的Storage组件是时序数据库，包含两个：localStorage和remoteStorage。localStorage当前版本指TSDB，用于对metrics的本地存储存储，remoteStorage用于metrics的远程存储，其中fanoutStorage作为localStorage和remoteStorage的读写代理服务器。&lt;/p&gt;

&lt;p&gt;初始化流程如下，代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;localStorage  = &amp;amp;tsdb.ReadyStorage{} //本地存储
remoteStorage = remote.NewStorage(log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;remote&amp;quot;), //远端存储 localStorage.StartTime, time.Duration(cfg.RemoteFlushDeadline))
fanoutStorage = storage.NewFanout(logger, localStorage, remoteStorage) //读写代理服务器
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;notifier 组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;notifier组件用于发送告警信息给AlertManager，通过方法notifier.NewManager完成初始化&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;notifierManager = notifier.NewManager(&amp;amp;cfg.notifier, log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;notifier&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;discoveryManagerScrape组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;discoveryManagerScrape组件用于服务发现，当前版本支持多种服务发现系统，比如kuberneters等，通过方法discovery.NewManager完成初始化。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;discoveryManagerScrape  = discovery.NewManager(ctxScrape, log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;discovery manager scrape&amp;quot;), discovery.Name(&amp;quot;scrape&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;discoveryManagerNotify组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;discoveryManagerNotify组件用于告警通知服务发现，比如AlertManager服务．也是通过方法discovery.NewManager完成初始化，不同的是，discoveryManagerNotify服务于notify，而discoveryManagerScrape服务于scrape。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;discoveryManagerNotify  = discovery.NewManager(ctxNotify, log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;discovery manager notify&amp;quot;), discovery.Name(&amp;quot;notify&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;scrapeManager组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrapeManager组件利用discoveryManagerScrape组件发现的targets，抓取对应targets的所有metrics，并将抓取的metrics存储到fanoutStorage中，通过方法scrape.NewManager完成初始化。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrapeManager = scrape.NewManager(log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;scrape manager&amp;quot;), fanoutStorage)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;queryEngine组件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;queryEngine组件用于rules查询和计算，通过方法promql.NewEngine完成初始化。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;opts = promql.EngineOpts{
    Logger:        log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;query engine&amp;quot;),
    Reg:           prometheus.DefaultRegisterer,
    MaxConcurrent: cfg.queryConcurrency,　　　　　　　//最大并发查询个数
    MaxSamples:    cfg.queryMaxSamples,
    Timeout:       time.Duration(cfg.queryTimeout),　//查询超时时间
}
queryEngine = promql.NewEngine(opts)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;ruleManager组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ruleManager组件通过方法rules.NewManager完成初始化．其中rules.NewManager的参数涉及多个组件：存储，queryEngine和notifier，整个流程包含rule计算和发送告警。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruleManager = rules.NewManager(&amp;amp;rules.ManagerOptions{
    Appendable:      fanoutStorage,                        //存储器
    TSDB:            localStorage,　　　　　　　　　　　　　　//本地时序数据库TSDB
    QueryFunc:       rules.EngineQueryFunc(queryEngine, fanoutStorage), //rules计算
    NotifyFunc:      sendAlerts(notifierManager, cfg.web.ExternalURL.String()),　//告警通知
    Context:         ctxRule,　//用于控制ruleManager组件的协程
    ExternalURL:     cfg.web.ExternalURL,　//通过Web对外开放的URL
    Registerer:      prometheus.DefaultRegisterer,
    Logger:          log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;rule manager&amp;quot;),
    OutageTolerance: time.Duration(cfg.outageTolerance), //当prometheus重启时，保持alert状态（https://ganeshvernekar.com/gsoc-2018/persist-for-state/）
    ForGracePeriod:  time.Duration(cfg.forGracePeriod),
    ResendDelay:     time.Duration(cfg.resendDelay),
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Web组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Web组件用于为Storage组件，queryEngine组件，scrapeManager组件， ruleManager组件和notifier 组件提供外部HTTP访问方式，也就是我们经常访问的prometheus的界面。&lt;/p&gt;

&lt;p&gt;初始化代码如下，代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfg.web.Context = ctxWeb
cfg.web.TSDB = localStorage.Get
cfg.web.Storage = fanoutStorage
cfg.web.QueryEngine = queryEngine
cfg.web.ScrapeManager = scrapeManager
cfg.web.RuleManager = ruleManager
cfg.web.Notifier = notifierManager

cfg.web.Version = &amp;amp;web.PrometheusVersion{
    Version:   version.Version,
    Revision:  version.Revision,
    Branch:    version.Branch,
    BuildUser: version.BuildUser,
    BuildDate: version.BuildDate,
    GoVersion: version.GoVersion,
}

cfg.web.Flags = map[string]string{}

// Depends on cfg.web.ScrapeManager so needs to be after cfg.web.ScrapeManager = scrapeManager
webHandler := web.New(log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;web&amp;quot;), &amp;amp;cfg.web)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;服务组件配置应用&#34;&gt;服务组件配置应用&lt;/h2&gt;

&lt;p&gt;除了服务组件ruleManager用的方法是Update，其他服务组件的在匿名函数中通过各自的ApplyConfig方法，实现配置的管理。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reloaders := []func(cfg *config.Config) error{
    remoteStorage.ApplyConfig, //存储配置
    webHandler.ApplyConfig,    //web配置
    notifierManager.ApplyConfig, //notifier配置
    scrapeManager.ApplyConfig,　　//scrapeManger配置
　　//从配置文件中提取Section:scrape_configs
    func(cfg *config.Config) error {
        c := make(map[string]sd_config.ServiceDiscoveryConfig)
        for _, v := range cfg.ScrapeConfigs {
            c[v.JobName] = v.ServiceDiscoveryConfig
        }
        return discoveryManagerScrape.ApplyConfig(c)
    },
    //从配置文件中提取Section:alerting
    func(cfg *config.Config) error {
        c := make(map[string]sd_config.ServiceDiscoveryConfig)
        for _, v := range cfg.AlertingConfig.AlertmanagerConfigs {
            // AlertmanagerConfigs doesn&#39;t hold an unique identifier so we use the config hash as the identifier.
            b, err := json.Marshal(v)
            if err != nil {
                return err
            }
            c[fmt.Sprintf(&amp;quot;%x&amp;quot;, md5.Sum(b))] = v.ServiceDiscoveryConfig
        }
        return discoveryManagerNotify.ApplyConfig(c)
    },
    //从配置文件中提取Section:rule_files
    func(cfg *config.Config) error {
        // Get all rule files matching the configuration paths.
        var files []string
        for _, pat := range cfg.RuleFiles {
            fs, err := filepath.Glob(pat)
            if err != nil {
                // The only error can be a bad pattern.
                return fmt.Errorf(&amp;quot;error retrieving rule files for %s: %s&amp;quot;, pat, err)
            }
            files = append(files, fs...)
        }
        return ruleManager.Update(time.Duration(cfg.GlobalConfig.EvaluationInterval), files)
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务组件remoteStorage，webHandler，notifierManager和ScrapeManager的ApplyConfig方法，参数cfg *config.Config中传递的配置文件，是整个文件prometheus.yml&lt;/p&gt;

&lt;p&gt;代码文件prometheus/scrape/manager.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg *config.Config) error {
   .......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务组件discoveryManagerScrape和discoveryManagerNotify的ApplyConfig方法，参数中传递的配置文件，是文件中的一个Section&lt;/p&gt;

&lt;p&gt;代码文件prometheus/discovery/manager.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg map[string]sd_config.ServiceDiscoveryConfig) error {
     ......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以，需要利用匿名函数提前处理下，取出对应的Section。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//从配置文件中提取Section:scrape_configs
func(cfg *config.Config) error {
    c := make(map[string]sd_config.ServiceDiscoveryConfig)
    for _, v := range cfg.ScrapeConfigs {
        c[v.JobName] = v.ServiceDiscoveryConfig
    }
    return discoveryManagerScrape.ApplyConfig(c)
},
//从配置文件中提取Section:alerting
func(cfg *config.Config) error {
    c := make(map[string]sd_config.ServiceDiscoveryConfig)
    for _, v := range cfg.AlertingConfig.AlertmanagerConfigs {
        // AlertmanagerConfigs doesn&#39;t hold an unique identifier so we use the config hash as the identifier.
        b, err := json.Marshal(v)
        if err != nil {
            return err
        }
        c[fmt.Sprintf(&amp;quot;%x&amp;quot;, md5.Sum(b))] = v.ServiceDiscoveryConfig
    }
    return discoveryManagerNotify.ApplyConfig(c)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务组件ruleManager，在匿名函数中提取出Section:rule_files&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//从配置文件中提取Section:rule_files
func(cfg *config.Config) error {
    // Get all rule files matching the configuration paths.
    var files []string
    for _, pat := range cfg.RuleFiles {
        fs, err := filepath.Glob(pat)
        if err != nil {
            // The only error can be a bad pattern.
            return fmt.Errorf(&amp;quot;error retrieving rule files for %s: %s&amp;quot;, pat, err)
        }
        files = append(files, fs...)
    }
    return ruleManager.Update(time.Duration(cfg.GlobalConfig.EvaluationInterval), files)
},
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;利用该组件内置的Update方法完成配置管理。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/rules/manager.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Update(interval time.Duration, files []string) error {
  .......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，通过reloadConfig方法，加载各个服务组件的配置项&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func reloadConfig(filename string, logger log.Logger, rls ...func(*config.Config) error) (err error) {
    level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Loading configuration file&amp;quot;, &amp;quot;filename&amp;quot;, filename)

    defer func() {
        if err == nil {
            configSuccess.Set(1)
            configSuccessTime.SetToCurrentTime()
        } else {
            configSuccess.Set(0)
        }
    }()

    conf, err := config.LoadFile(filename)
    if err != nil {
        return fmt.Errorf(&amp;quot;couldn&#39;t load configuration (--config.file=%q): %v&amp;quot;, filename, err)
    }

    failed := false
　　//通过一个for循环，加载各个服务组件的配置项
    for _, rl := range rls {
        if err := rl(conf); err != nil {
            level.Error(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Failed to apply configuration&amp;quot;, &amp;quot;err&amp;quot;, err)
            failed = true
        }
    }
    if failed {
        return fmt.Errorf(&amp;quot;one or more errors occurred while applying the new configuration (--config.file=%q)&amp;quot;, filename)
    }
    promql.SetDefaultEvaluationInterval(time.Duration(conf.GlobalConfig.EvaluationInterval))
    level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Completed loading of configuration file&amp;quot;, &amp;quot;filename&amp;quot;, filename)
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;服务组件启动&#34;&gt;服务组件启动&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里引用了github.com/oklog/oklog/pkg/group包，实例化一个对象g&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// &amp;quot;github.com/oklog/oklog/pkg/group&amp;quot;
var g group.Group
{
　　......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对象g中包含各个服务组件的入口，通过调用Add方法把把这些入口添加到对象g中，以组件scrapeManager为例。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    // Scrape manager.
　　//通过方法Add，把ScrapeManager组件添加到g中
    g.Add(
        func() error {
            // When the scrape manager receives a new targets list
            // it needs to read a valid config for each job.
            // It depends on the config being in sync with the discovery manager so
            // we wait until the config is fully loaded.
            &amp;lt;-reloadReady.C
　　　　　　　//ScrapeManager组件的启动函数
            err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
            level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape manager stopped&amp;quot;)
            return err
        },
        func(err error) {
            // Scrape manager needs to be stopped before closing the local TSDB
            // so that it doesn&#39;t try to write samples to a closed storage.
            level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Stopping scrape manager...&amp;quot;)
            scrapeManager.Stop()
        },
    )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;run&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过对象g，调用方法run，启动所有服务组件&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if err := g.Run(); err != nil {
    level.Error(logger).Log(&amp;quot;err&amp;quot;, err)
    os.Exit(1)
}
level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;See you next time!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，Prometheus的启动过程分析完成。&lt;/p&gt;

&lt;h1 id=&#34;内部结构&#34;&gt;内部结构&lt;/h1&gt;

&lt;p&gt;Prometheus的内部主要分为三大块，Retrieval是负责定时去暴露的目标页面上去抓取采样指标数据，Storage是负责将采样数据写磁盘，PromQL是Prometheus提供的查询语言模块。当然还有其他的一些组件，可以参考上面组件的初始化，比如一些web，notify等。&lt;/p&gt;

&lt;h2 id=&#34;retrieval&#34;&gt;Retrieval&lt;/h2&gt;

&lt;h3 id=&#34;采集实现&#34;&gt;采集实现&lt;/h3&gt;

&lt;p&gt;​Prometheus采集数据使用pull模式，通过HTTP协议去采集指标，只要应用系统能够提供HTTP接口就可以接入监控系统。&lt;/p&gt;

&lt;p&gt;​拉取目标称之为scrape，一个scrape一般对应一个进程，如下为scrape相关的配置。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;配置文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;scrape_interval:     15s
scrape_configs:
  - job_name: &#39;test_server_name&#39;
    static_configs:
    - targets: [&#39;localhost:8886&#39;]
      labels:
        project: &#39;test_server&#39;
        environment: &#39;test&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该配置描述：每15秒去拉取一次上报数据，拉取目标为localhost:8886。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;读取配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ScrapeConfig的结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ScrapeConfig struct {
   //  作业名称
   JobName string `yaml:&amp;quot;job_name&amp;quot;`
   // 同名lable，是否覆盖处理
   HonorLabels bool `yaml:&amp;quot;honor_labels,omitempty&amp;quot;`
   HonorTimestamps bool `yaml:&amp;quot;honor_timestamps&amp;quot;`
   // 采集目标url参数
   Params url.Values `yaml:&amp;quot;params,omitempty&amp;quot;`
   // 采集周期
   ScrapeInterval model.Duration `yaml:&amp;quot;scrape_interval,omitempty&amp;quot;`
   // 采集超时时间
   ScrapeTimeout model.Duration `yaml:&amp;quot;scrape_timeout,omitempty&amp;quot;`
   // 目标 URl path
   MetricsPath string `yaml:&amp;quot;metrics_path,omitempty&amp;quot;`
   Scheme string `yaml:&amp;quot;scheme,omitempty&amp;quot;`
   SampleLimit uint `yaml:&amp;quot;sample_limit,omitempty&amp;quot;`
   // 服务发现配置
   ServiceDiscoveryConfig sd_config.ServiceDiscoveryConfig `yaml:&amp;quot;,inline&amp;quot;`
   // 客户端http client 配置
   HTTPClientConfig       config_util.HTTPClientConfig     `yaml:&amp;quot;,inline&amp;quot;`
   // 目标重置规则
   RelabelConfigs []*relabel.Config `yaml:&amp;quot;relabel_configs,omitempty&amp;quot;`
   // 指标重置规则
   MetricRelabelConfigs []*relabel.Config `yaml:&amp;quot;metric_relabel_configs,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读取配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg *config.Config) error {
    m.mtxScrape.Lock()
    defer m.mtxScrape.Unlock()
    // 初始化map结构，用于保存配置
    c := make(map[string]*config.ScrapeConfig)
    for _, scfg := range cfg.ScrapeConfigs {
    // 配置读取维度
        c[scfg.JobName] = scfg
    }
    m.scrapeConfigs = c
    // 设置 所有时间序列和警告与外部通信时用的外部标签 external_labels
    if err := m.setJitterSeed(cfg.GlobalConfig.ExternalLabels); err != nil {
        return err
    }

    // 如果配置已经更改，清理历史配置，重新加载到池子中
    var failed bool
    for name, sp := range m.scrapePools {
    // 如果当前job不存在，则删除
        if cfg, ok := m.scrapeConfigs[name]; !ok {
            sp.stop()
            delete(m.scrapePools, name)
        } else if !reflect.DeepEqual(sp.config, cfg) {
      // 如果配置变更，重新启动reload，进行加载
            err := sp.reload(cfg)
            if err != nil {
                level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error reloading scrape pool&amp;quot;, &amp;quot;err&amp;quot;, err, &amp;quot;scrape_pool&amp;quot;, name)
                failed = true
            }
        }
    }
    // 失败 return
    if failed {
        return errors.New(&amp;quot;failed to apply the new configuration&amp;quot;)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus 中，将任意一个独立的数据源（target）称之为实例（instance）。包含相同类型的实例的集合称之为作业（job)，从读取配置中，我们也能看到，以job为key。所以注意job在业务侧的使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Scrape Manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;​
添加Scrape Manager 到 run.Group启动。reloadReady.C的作用是当Manager接收到一组数据采集目标(target)的时候，他需要为每个job读取有效的配置。因此这里等待所有配置加载完成，进行下一步。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;g.Add(
            func() error {
        // 当所有配置都准备好
                &amp;lt;-reloadReady.C
                // 启动scrapeManager
                err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
                level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape manager stopped&amp;quot;)
                return err
            },
            func(err error) {
        // 失败处理
                level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Stopping scrape manager...&amp;quot;)
                scrapeManager.Stop()
            },
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;加载Targets&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加载targets，如果targets更新，会触发重新加载，reloader的加载发生在后台，所以并不会影响target的更新，(配置文件中配置的target是依赖discoveryManagerScrape.ApplyConfig&amp;copy;进行加载的，后面分析target服务发现的时候详细分析)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Run(tsets &amp;lt;-chan map[string][]*targetgroup.Group) error {
    go m.reloader()
    for {
        select {
    // 触发重新加载目标。添加新增
        case ts := &amp;lt;-tsets:
            m.updateTsets(ts)

            select {
      // 关闭 Scrape Manager 处理信号
            case m.triggerReload &amp;lt;- struct{}{}:
            default:
            }

        case &amp;lt;-m.graceShut:
            return nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;scrape pool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;顺着Run继续阅读，reload为每一组tatget生成一个对应的scrape pool管理targets集合，scrapePool结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type scrapePool struct {
   appendable Appendable
   logger     log.Logger
     // 读写锁
   mtx    sync.RWMutex
   // Scrape 配置
   config *config.ScrapeConfig
   // http client
   client *http.Client

   // 正在运行的target
   activeTargets  map[uint64]*Target
   // 无效的target
   droppedTargets []*Target
   // 所有运行的loop
   loops          map[uint64]loop
   // 取消
   cancel         context.CancelFunc
     // 创建loop
   newLoop func(scrapeLoopOptions) loop
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;执行reload&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;m.reloade的流程也很简单，setName指我们配置中的job，如果scrapePools不存在该job，则添加，添加前也是先校验该job的配置是否存在，不存在则报错，创建scrape pool。总结看就是为每个job创建与之对应的scrape pool&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) reload() {
   //加锁
   m.mtxScrape.Lock()
   var wg sync.WaitGroup
   for setName, groups := range m.targetSets {
       //检查该scrape是否存在scrapePools，不存在则创建
      if _, ok := m.scrapePools[setName]; !ok {
         //读取该scrape的配置
         scrapeConfig, ok := m.scrapeConfigs[setName]
         if !ok {
            // 未读取到该scrape的配置打印错误
            level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error reloading target set&amp;quot;, &amp;quot;err&amp;quot;, &amp;quot;invalid config id:&amp;quot;+setName)
            // 跳出
            continue
         }
         // 创建该scrape的scrape pool
         sp, err := newScrapePool(scrapeConfig, m.append, m.jitterSeed, log.With(m.logger, &amp;quot;scrape_pool&amp;quot;, setName))
         if err != nil {
            level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error creating new scrape pool&amp;quot;, &amp;quot;err&amp;quot;, err, &amp;quot;scrape_pool&amp;quot;, setName)
            continue
         }
         // 保存
         m.scrapePools[setName] = sp
      }

      wg.Add(1)
            // 并行运行，提升性能。
      go func(sp *scrapePool, groups []*targetgroup.Group) {
         sp.Sync(groups)
         wg.Done()
      }(m.scrapePools[setName], groups)

   }
   // 释放锁
   m.mtxScrape.Unlock()
   // 阻塞，等待所有pool运行完毕
   wg.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;创建scrape pool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrape pool利用newLoop去为该job下的所有target生成对应的loop：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newScrapePool(cfg *config.ScrapeConfig, app Appendable, jitterSeed uint64, logger log.Logger) (*scrapePool, error) {
    targetScrapePools.Inc()
    if logger == nil {
        logger = log.NewNopLogger()
    }
  // 创建http client，用于执行数据抓取
    client, err := config_util.NewClientFromConfig(cfg.HTTPClientConfig, cfg.JobName)
    if err != nil {
        targetScrapePoolsFailed.Inc()
        return nil, errors.Wrap(err, &amp;quot;error creating HTTP client&amp;quot;)
    }
    // 设置buffers
    buffers := pool.New(1e3, 100e6, 3, func(sz int) interface{} { return make([]byte, 0, sz) })
  // 设置scrapePool的一些基础属性
    ctx, cancel := context.WithCancel(context.Background())
    sp := &amp;amp;scrapePool{
        cancel:        cancel,
        appendable:    app,
        config:        cfg,
        client:        client,
        activeTargets: map[uint64]*Target{},
        loops:         map[uint64]loop{},
        logger:        logger,
    }
  // newLoop用于生层loop，主要处理对应的target，可以理解为，每个target对应一个loop。
    sp.newLoop = func(opts scrapeLoopOptions) loop {
        // Update the targets retrieval function for metadata to a new scrape cache.
        cache := newScrapeCache()
        opts.target.setMetadataStore(cache)

        return newScrapeLoop(
            ctx,
            opts.scraper,
            log.With(logger, &amp;quot;target&amp;quot;, opts.target),
            buffers,
            func(l labels.Labels) labels.Labels {
                return mutateSampleLabels(l, opts.target, opts.honorLabels, opts.mrc)
            },
            func(l labels.Labels) labels.Labels { return mutateReportSampleLabels(l, opts.target) },
            func() storage.Appender {
                app, err := app.Appender()
                if err != nil {
                    panic(err)
                }
                return appender(app, opts.limit)
            },
            cache,
            jitterSeed,
            opts.honorTimestamps,
        )
    }

    return sp, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;group转化为target&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrape pool创建完成后，则通过sp.Sync执行，使用该job对应的pool遍历Group，使其转换为target&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go func(sp *scrapePool, groups []*targetgroup.Group) {
   sp.Sync(groups)
   wg.Done()
}(m.scrapePools[setName], groups)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sync函数解读如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sp *scrapePool) Sync(tgs []*targetgroup.Group) {
   start := time.Now()

   var all []*Target
   // 加锁
   sp.mtx.Lock()
   sp.droppedTargets = []*Target{}
   // 遍历所有Group
   for _, tg := range tgs {
        // 转化对应 targets
      targets, err := targetsFromGroup(tg, sp.config)
      if err != nil {
         level.Error(sp.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;creating targets failed&amp;quot;, &amp;quot;err&amp;quot;, err)
         continue
      }
      // 将所有有效targets添加到all，等待处理
      for _, t := range targets {
         // 检查该target的lable是否有效
         if t.Labels().Len() &amp;gt; 0 {
            // 添加到all队列中
            all = append(all, t)
         } else if t.DiscoveredLabels().Len() &amp;gt; 0 {
            // 记录无效target
            sp.droppedTargets = append(sp.droppedTargets, t)
         }
      }
   }
   // 解锁
   sp.mtx.Unlock()
   // 处理all队列，执行scarape同步操作
   sp.sync(all)

   targetSyncIntervalLength.WithLabelValues(sp.config.JobName).Observe(
      time.Since(start).Seconds(),
   )
   targetScrapePoolSyncsCounter.WithLabelValues(sp.config.JobName).Inc()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;生成loop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在sync最后，调用了当前scrape pool的sync去处理all队列中的target，添加新的target，删除失效的target。实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sp *scrapePool) sync(targets []*Target) {
  // 加锁
    sp.mtx.Lock()
    defer sp.mtx.Unlock()

    var (
    // target 标记
        uniqueTargets   = map[uint64]struct{}{}
    // 采集周期
        interval        = time.Duration(sp.config.ScrapeInterval)
    // 采集超时时间
        timeout         = time.Duration(sp.config.ScrapeTimeout)
        limit           = int(sp.config.SampleLimit)
    // 重复lable是否覆盖
        honorLabels     = sp.config.HonorLabels
        honorTimestamps = sp.config.HonorTimestamps
        mrc             = sp.config.MetricRelabelConfigs
    )
    // 遍历all队列中的所有target
    for _, t := range targets {
    // 赋值，避免range的坑
        t := t
    // 生成对应的hash（对该hash算法感兴趣可以看下这里的源码）
        hash := t.hash()
    // 标记
        uniqueTargets[hash] = struct{}{}
        // 判断该taget是否已经在运行了。如果没有则运行该target对应的loop，将该loop加入activeTargets中
        if _, ok := sp.activeTargets[hash]; !ok {
            s := &amp;amp;targetScraper{Target: t, client: sp.client, timeout: timeout}
            l := sp.newLoop(scrapeLoopOptions{
                target:          t,
                scraper:         s,
                limit:           limit,
                honorLabels:     honorLabels,
                honorTimestamps: honorTimestamps,
                mrc:             mrc,
            })

            sp.activeTargets[hash] = t
            sp.loops[hash] = l
            // 启动该loop
            go l.run(interval, timeout, nil)
        } else {
      // 该target对应的loop已经运行，设置最新的标签信息
            sp.activeTargets[hash].SetDiscoveredLabels(t.DiscoveredLabels())
        }
    }

    var wg sync.WaitGroup

  // 停止并且移除无效的targets与对应的loops
  // 遍历activeTargets正在执行的Target
    for hash := range sp.activeTargets {
    // 检查该hash对应的标记是否存在，放过不存在执行清除逻辑
        if _, ok := uniqueTargets[hash]; !ok {
            wg.Add(1)
      // 异步清除
            go func(l loop) {
                // 停止该loop
                l.stop()
                // 执行完成
                wg.Done()
            }(sp.loops[hash])
            // 从loops中删除该hash对应的loop
            delete(sp.loops, hash)
      // 从activeTargets中删除该hash对应的target
            delete(sp.activeTargets, hash)
        }
    }
  // 等待所有执行完成
    wg.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;运行loop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrape pool对应的sync的实现中可以看到，如果该target没有运行，则启动该target对应的loop，执行l.run，通过一个goroutine来执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sl *scrapeLoop) run(interval, timeout time.Duration, errc chan&amp;lt;- error) {
  // 偏移量相关设置
    select {
    case &amp;lt;-time.After(sl.scraper.offset(interval, sl.jitterSeed)):
        // Continue after a scraping offset.
    case &amp;lt;-sl.scrapeCtx.Done():
        close(sl.stopped)
        return
    }

    var last time.Time
    // 根据interval设置定时器
    ticker := time.NewTicker(interval)
    defer ticker.Stop()

mainLoop:
    for {
        select {
        case &amp;lt;-sl.ctx.Done():
            close(sl.stopped)
            return
        case &amp;lt;-sl.scrapeCtx.Done():
            break mainLoop
        default:
        }

        var (
            start             = time.Now()
            scrapeCtx, cancel = context.WithTimeout(sl.ctx, timeout)
        )

        // 记录第一次
        if !last.IsZero() {
            targetIntervalLength.WithLabelValues(interval.String()).Observe(
                time.Since(last).Seconds(),
            )
        }
        // 根据上次拉取数据的大小，设置buffer空间
        b := sl.buffers.Get(sl.lastScrapeSize).([]byte)
        buf := bytes.NewBuffer(b)
        // 读取数据，设置到buffer中
        contentType, scrapeErr := sl.scraper.scrape(scrapeCtx, buf)
    // 取消，结束scrape
        cancel()

        if scrapeErr == nil {
            b = buf.Bytes()
            if len(b) &amp;gt; 0 {
        // 记录本次Scrape大小
                sl.lastScrapeSize = len(b)
            }
        } else {
      // 错误处理
            level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape failed&amp;quot;, &amp;quot;err&amp;quot;, scrapeErr.Error())
            if errc != nil {
                errc &amp;lt;- scrapeErr
            }
        }

        // 生成数据，存储指标
        total, added, seriesAdded, appErr := sl.append(b, contentType, start)
        if appErr != nil {
            level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;append failed&amp;quot;, &amp;quot;err&amp;quot;, appErr)

            if _, _, _, err := sl.append([]byte{}, &amp;quot;&amp;quot;, start); err != nil {
                level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;append failed&amp;quot;, &amp;quot;err&amp;quot;, err)
            }
        }
        // 对象复用
        sl.buffers.Put(b)

        if scrapeErr == nil {
            scrapeErr = appErr
        }
        // 上报指标，进行统计
        if err := sl.report(start, time.Since(start), total, added, seriesAdded, scrapeErr); err != nil {
            level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;appending scrape report failed&amp;quot;, &amp;quot;err&amp;quot;, err)
        }
    // 重置时间位置
        last = start

        select {
        case &amp;lt;-sl.ctx.Done():
            close(sl.stopped)
            return
        case &amp;lt;-sl.scrapeCtx.Done():
            break mainLoop
        case &amp;lt;-ticker.C:
        }
    }

    close(sl.stopped)

    sl.endOfRunStaleness(last, ticker, interval)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;拉取数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;依赖scrape实现数据的抓取，使用GET方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *targetScraper) scrape(ctx context.Context, w io.Writer) (string, error) {
    if s.req == nil {
    // 新建Http Request
        req, err := http.NewRequest(&amp;quot;GET&amp;quot;, s.URL().String(), nil)
        if err != nil {
            return &amp;quot;&amp;quot;, err
        }
    // 设置请求头
        req.Header.Add(&amp;quot;Accept&amp;quot;, acceptHeader)
        req.Header.Add(&amp;quot;Accept-Encoding&amp;quot;, &amp;quot;gzip&amp;quot;)
        req.Header.Set(&amp;quot;User-Agent&amp;quot;, userAgentHeader)
        req.Header.Set(&amp;quot;X-Prometheus-Scrape-Timeout-Seconds&amp;quot;, fmt.Sprintf(&amp;quot;%f&amp;quot;, s.timeout.Seconds()))

        s.req = req
    }
    // 发起请求
    resp, err := s.client.Do(s.req.WithContext(ctx))
    if err != nil {
        return &amp;quot;&amp;quot;, err
    }
    defer func() {
        io.Copy(ioutil.Discard, resp.Body)
        resp.Body.Close()
    }()
    // 错误处理
    if resp.StatusCode != http.StatusOK {
        return &amp;quot;&amp;quot;, errors.Errorf(&amp;quot;server returned HTTP status %s&amp;quot;, resp.Status)
    }
    // 检查Content-Encoding
    if resp.Header.Get(&amp;quot;Content-Encoding&amp;quot;) != &amp;quot;gzip&amp;quot; {
    // copy buffer到w
        _, err = io.Copy(w, resp.Body)
        if err != nil {
            return &amp;quot;&amp;quot;, err
        }
        return resp.Header.Get(&amp;quot;Content-Type&amp;quot;), nil
    }
    if s.gzipr == nil {
        s.buf = bufio.NewReader(resp.Body)
        s.gzipr, err = gzip.NewReader(s.buf)
        if err != nil {
            return &amp;quot;&amp;quot;, err
        }
    } else {
        s.buf.Reset(resp.Body)
        if err = s.gzipr.Reset(s.buf); err != nil {
            return &amp;quot;&amp;quot;, err
        }
    }

    _, err = io.Copy(w, s.gzipr)
    s.gzipr.Close()
    if err != nil {
        return &amp;quot;&amp;quot;, err
    }
    return resp.Header.Get(&amp;quot;Content-Type&amp;quot;), nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每一个job有一个与之对应的scrape pool，每一个target有一个与之对应的loop，每个loop内部执 Http Get请求拉取数据。通过一些控制参数，控制采集周期以及结束等逻辑。&lt;/p&gt;

&lt;h3 id=&#34;数据规范&#34;&gt;数据规范&lt;/h3&gt;

&lt;h4 id=&#34;数据模型&#34;&gt;数据模型&lt;/h4&gt;

&lt;p&gt;Prometheus与其他主流时序数据库一样，在数据模型定义上，也会包含metric name、一个或多个labels（同tags）以及metric value&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/datamodel.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图是所有数据点分布的一个简单视图，横轴是时间，纵轴是时间线，区域内每个点就是数据点。Prometheus每次接收数据，收到的是图中区域内纵向的一条线。这个表述很形象，在同一时刻，每条时间线只会产生一个数据点，但同时会有多条时间线产生数据，把这些数据点连在一起，就是一条竖线。这个特征很重要，影响数据写入和压缩的优化策略。&lt;/p&gt;

&lt;h3 id=&#34;探针数据&#34;&gt;探针数据&lt;/h3&gt;

&lt;p&gt;都体现在client_golang的库中，直接去&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/&#34;&gt;client_golang&lt;/a&gt;文章中参考。&lt;/p&gt;

&lt;h2 id=&#34;promql&#34;&gt;PromQL&lt;/h2&gt;

&lt;p&gt;PromQL 是 Prom 中的查询语言，提供了简洁的、贴近自然语言的语法实现时序数据的分析计算。&lt;/p&gt;

&lt;h3 id=&#34;原理&#34;&gt;原理&lt;/h3&gt;

&lt;p&gt;PromQL 表达式输入是一段文本，Prom 会解析这段文本，将它转化为一个结构化的语法树对象，进而实现相应的数据计算逻辑。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(avg_over_time(go_goroutines{job=&amp;quot;prometheus&amp;quot;}[5m])) by (instance)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述表达式可以从外往内分解为三层：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(…) by (instance)：序列纵向分组合并序列（包含相同的 instance 会分配到一组）
avg_over_time(…)
go_goroutines{job=&amp;quot;prometheus&amp;quot;}[5m]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;时间点对象MatrixSelector 对象，是获取时序数据的基础结构&lt;/li&gt;
&lt;li&gt;获取时间段里面的数据，通过iterator 是序列筛选结果的顺序访问接口，获取某个时间点往前的一段历史数据，这是一个二维矩阵 (matrix)，进而由外层函数将这段历史数据汇总成一个 vector&lt;/li&gt;
&lt;li&gt;实现对一段数据的汇总，然后求平均值&lt;/li&gt;
&lt;li&gt;最后来看关键字（keyword）sum 的实现，这里注意 sum 不是函数（Function）而是关键字。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;sum(avg_over_time(go_goroutines{job=&amp;ldquo;prometheus&amp;rdquo;}[5m])) by (instance) 计算过程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/promql.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;PromQL 有三个很简单的原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;任意 PromQL 返回的结果都不是原始数据，即使查询一个具体的 Metric（如 go_goroutines），结果也不是原始数据&lt;/li&gt;
&lt;li&gt;任意 Metrics 经过 Function 计算后会丢失&lt;code&gt;__name__&lt;/code&gt;Label&lt;/li&gt;
&lt;li&gt;子序列间具备完全相同的 Label/Value 键值对（可以有不同的&lt;code&gt;__name__&lt;/code&gt;）才能进行代数运算&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;storage&#34;&gt;storage&lt;/h2&gt;

&lt;h3 id=&#34;源码解读&#34;&gt;源码解读&lt;/h3&gt;

&lt;p&gt;真正存储指标的是storage.Appender，在scrape与storage之间有一层缓存。缓存主要的作用是过滤错误的指标。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type scrapeCache struct {
  iter uint64                           // scrape批次
    successfulCount int                   // 成功保存的元数据数
    series map[string]*cacheEntry         // 缓存解析的相关数据
    droppedSeries map[string]*uint64      // 缓存无效指标
    seriesCur  map[uint64]labels.Labels     // 本次采集指标
    seriesPrev map[uint64]labels.Labels   // 上次采集指标

    metaMtx  sync.Mutex
    metadata map[string]*metaEntry
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建scrapeCache，调用newScrapeLoop，初始化scrapeLoop，会判断scrapeCache是否为空，如果为nil，调用newScrapeCache对cache进行初始化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if cache == nil {
        cache = newScrapeCache()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;newScrapeCache()如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newScrapeCache() *scrapeCache {
    return &amp;amp;scrapeCache{
        series:        map[string]*cacheEntry{},
        droppedSeries: map[string]*uint64{},
        seriesCur:     map[uint64]labels.Labels{},
        seriesPrev:    map[uint64]labels.Labels{},
        metadata:      map[string]*metaEntry{},
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;scrapeCache 方法介绍，这里简介各个fun的作用，详细代码不做注解。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 根据met信息，获取对应的cacheEntry
func (c *scrapeCache) get(met string) (*cacheEntry, bool)
// 根据met创建cacheEntry节点
func (c *scrapeCache) addRef(met string, ref uint64, lset labels.Labels, hash uint64)
// 添加无效指标，met作为key
func (c *scrapeCache) addDropped(met string)
// 根据met，检查该指标是否有效
func (c *scrapeCache) getDropped(met string) bool
// 添加当前采集指标
func (c *scrapeCache) trackStaleness(hash uint64, lset labels.Labels)
// 检查指标状态
func (c *scrapeCache) forEachStale(f func(labels.Labels) bool)
// 缓存清理
func (c *scrapeCache) iterDone(flushCache bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;存储过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;分析scrapeLoop.append是如何实现存储数据的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sl *scrapeLoop)  append(b []byte, contentType string, ts time.Time) (total, added, seriesAdded int, err error) {
    var (
    // 获取指标存储组件
        app            = sl.appender()
    // 获取解析组件
        p              = textparse.New(b, contentType)
        defTime        = timestamp.FromTime(ts)
        numOutOfOrder  = 0
        numDuplicates  = 0
        numOutOfBounds = 0
    )
    var sampleLimitErr error

loop:
    for {
        var et textparse.Entry
    // 开始遍历，遍历到EOF(字节流尾部)，终止遍历
        if et, err = p.Next(); err != nil {
            if err == io.EOF {
                err = nil
            }
            break
        }
    // 以下Entry类型跳过
        switch et {
        case textparse.EntryType:
            sl.cache.setType(p.Type())
            continue
        case textparse.EntryHelp:
            sl.cache.setHelp(p.Help())
            continue
        case textparse.EntryUnit:
            sl.cache.setUnit(p.Unit())
            continue
        case textparse.EntryComment:
            continue
        default:
        }
        total++

        t := defTime
    // 获取指标label，时间戳（如果设置了），当前样本值
        met, tp, v := p.Series()
    // 如果设置了honorTimestamps，时间戳设置为nil
        if !sl.honorTimestamps {
            tp = nil
        }
    // 如果时间戳不为空，更新当前t
        if tp != nil {
            t = *tp
        }
        // 检查该指标值是否有效，无效则直接跳过当前处理
        if sl.cache.getDropped(yoloString(met)) {
            continue
        }
    // 根据当前met获取对应的cacheEntry结构
        ce, ok := sl.cache.get(yoloString(met))
    // 如果从缓存中获取，则执行指标的存储操作
        if ok {
      // 指标存储
            switch err = app.AddFast(ce.lset, ce.ref, t, v); err {
            case nil:
        // 如果不带时间戳
                if tp == nil {
          // 存储该不带时间戳的指标到seriesCur中。
                    sl.cache.trackStaleness(ce.hash, ce.lset)
                }
       // 未找到错误，重置ok为false，执行!ok逻辑
            case storage.ErrNotFound:
                ok = false
      // 乱序样本
            case storage.ErrOutOfOrderSample:
        // 乱序样本错误记录，并上报
                numOutOfOrder++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of order sample&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfOrder.Inc()
                continue
      // 重复样本
            case storage.ErrDuplicateSampleForTimestamp:
        // 重复样本错误记录，并上报
                numDuplicates++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Duplicate sample for timestamp&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleDuplicate.Inc()
                continue
      // 存储越界
            case storage.ErrOutOfBounds:
        // 存储越界错误记录，并上报
                numOutOfBounds++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of bounds metric&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfBounds.Inc()
                continue
      // 超出样本限制错误
            case errSampleLimit:
        // 如果我们达到上限也要继续解析输出，所以我们要上报正确的样本总量
                sampleLimitErr = err
                added++
                continue
      // 未知情况，终止loop
            default:
                break loop
            }
        }
    // 在缓存中未查找到，
        if !ok {
            var lset labels.Labels
            // 生成mets
            mets := p.Metric(&amp;amp;lset)
      // 生成hash值
            hash := lset.Hash()

            // 根据配置重置label set
            lset = sl.sampleMutator(lset)

            // 如果label set为空，则表明该mets为非法指标
            if lset == nil {
        // 添加mets到无效指标字典中
                sl.cache.addDropped(mets)
                continue
            }

            var ref uint64
      // 存储指标
            ref, err = app.Add(lset, t, v)

      // 错误处理同上，不重复描述
            switch err {
            case nil:
      // 乱序样本
            case storage.ErrOutOfOrderSample:
                err = nil
                numOutOfOrder++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of order sample&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfOrder.Inc()
                continue
      // 重复样本
            case storage.ErrDuplicateSampleForTimestamp:
                err = nil
                numDuplicates++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Duplicate sample for timestamp&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleDuplicate.Inc()
                continue
      // 存储越界
            case storage.ErrOutOfBounds:
                err = nil
                numOutOfBounds++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of bounds metric&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfBounds.Inc()
                continue
      // 样本限制
            case errSampleLimit:
                sampleLimitErr = err
                added++
                continue
            default:
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;unexpected error&amp;quot;, &amp;quot;series&amp;quot;, string(met), &amp;quot;err&amp;quot;, err)
                break loop
            }
            if tp == nil {
                // 存储该不带时间戳的指标到seriesCur中。
                sl.cache.trackStaleness(hash, lset)
            }
        // 缓存该指标到series中
            sl.cache.addRef(mets, ref, lset, hash)
            seriesAdded++
        }
        added++
    }
  // 错误相关处理，不做分析。
    if sampleLimitErr != nil {
        if err == nil {
            err = sampleLimitErr
        }
        targetScrapeSampleLimit.Inc()
    }
    if numOutOfOrder &amp;gt; 0 {
        level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Error on ingesting out-of-order samples&amp;quot;, &amp;quot;num_dropped&amp;quot;, numOutOfOrder)
    }
    if numDuplicates &amp;gt; 0 {
        level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Error on ingesting samples with different value but same timestamp&amp;quot;, &amp;quot;num_dropped&amp;quot;, numDuplicates)
    }
    if numOutOfBounds &amp;gt; 0 {
        level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Error on ingesting samples that are too old or are too far into the future&amp;quot;, &amp;quot;num_dropped&amp;quot;, numOutOfBounds)
    }
    if err == nil {
    // 指标状态检查。
        sl.cache.forEachStale(func(lset labels.Labels) bool {
            // 标记存储中的过期指标
            _, err = app.Add(lset, defTime, math.Float64frombits(value.StaleNaN))
            switch err {
      // 以下错误不做处理
            case storage.ErrOutOfOrderSample, storage.ErrDuplicateSampleForTimestamp:
                err = nil
            }
            return err == nil
        })
    }
    if err != nil {
    // 出现错误，存储组件进行回滚
        app.Rollback()
        return total, added, seriesAdded, err
    }
  // 存储提交
    if err := app.Commit(); err != nil {
        return total, added, seriesAdded, err
    }

    // 执行缓存清理相关工作
    sl.cache.iterDone(len(b) &amp;gt; 0)

    return total, added, seriesAdded, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;整个存储逻辑都围绕着过滤无效指标进行。特殊点在于存储的时候指标分为有时间戳与无时间戳两种情况。&lt;/p&gt;

&lt;p&gt;1、有时间戳&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;解析指标数据通过Series()&lt;/li&gt;
&lt;li&gt;利用getDropped判断指标是否有效，无效则跳出处理&lt;/li&gt;
&lt;li&gt;通过get查找对应cacheEntry，如果找到利用app.AddFast直接存储样本值。如果未找到，使用sampleMutator进行解析重置，判断lset是否为空，为空则使用addDropped添加到无效字典中，跳出当前处理，如果有效则使用app.Add存储指标。(可以看到，通过get找到使用AddFast存储，未找到使用Add存储，感兴趣可以看下两个fun实现的区别)&lt;/li&gt;
&lt;li&gt;通过forEachStale检查指标是否过期。&lt;/li&gt;
&lt;li&gt;app.Add标记过期指标&lt;/li&gt;
&lt;li&gt;调用iterDone进行相关缓存清理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、无时间戳&lt;/p&gt;

&lt;p&gt;每次存储后，如果不带时间戳都会调用trackStaleness，存储指标到seriesCur中&lt;/p&gt;

&lt;p&gt;这里seriesCur与seriesPrev的作用就是处理指标label是否过期的。forEachStale实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *scrapeCache) forEachStale(f func(labels.Labels) bool) {
    for h, lset := range c.seriesPrev {
        if _, ok := c.seriesCur[h]; !ok {
            if !f(lset) {
                break
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果seriesPrev中的指标(label)存在于seriesPrev，则不处理，如果不存在，则说明过期。其中在iterDone中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 交换seriesPrev与seriesCur
c.seriesPrev, c.seriesCur = c.seriesCur, c.seriesPrev

// 清空当前指标缓存列表
for k := range c.seriesCur {
    delete(c.seriesCur, k)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以，每次存储处理后，都会交换seriesPrev与seriesCur，然后清空seriesCur。下次存储在做比较。如果命中过期规则，则标记该样本值为StaleNaN。&lt;/p&gt;

&lt;h3 id=&#34;local-storage&#34;&gt;local storage&lt;/h3&gt;

&lt;h4 id=&#34;v2&#34;&gt;v2&lt;/h4&gt;

&lt;p&gt;Prometheus 1.0版本的TSDB（V2存储引擎）使用了G家的LevelDB来做索引(PromSQL重度依赖LevelDB)，并且使用了和Facebook Gorilla一样的压缩算法，能够将16个字节的数据点压缩到平均1.37个字节。&lt;/p&gt;

&lt;p&gt;V2存储引擎对于大量的采样数据有自己的存储层，Prometheus为每个时序数据创建一个本地文件，以1024byte大小的chunk来组织。写到head chunk，写满1KB，就再生成新的块，完成的块，是不可再变更的 , 根据配置文件的设置，有一部份chunk会被保留在内存里，按照LRU算法，定期将块写进磁盘文件内。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;缺陷&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;文件数会随着时间线的数量同比增长，慢慢会耗尽inode。&lt;/li&gt;
&lt;li&gt;即便使用了Chunk写优化，若一次写入涉及的时间线过多，IOPS要求还是会很高。&lt;/li&gt;
&lt;li&gt;每个文件不可能会时刻保持open状态，一次查询可能需要重新打开大量文件，增大查询延迟。&lt;/li&gt;
&lt;li&gt;数据回收需要从大量文件扫描和重写数据，耗时较长。&lt;/li&gt;
&lt;li&gt;数据需要在内存中积累一定时间以Chunk写，V2会采用定时写Checkpoint的机制来尽量保证内存中数据不丢失。但通常记录Checkpoint的时间大于能承受的数据丢失的时间窗口，并且在节点恢复时从checkpoint restore数据的时间也会很长。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;优化策略&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Chunk写，热数据内存缓存&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prometheus一次性接收到的数据是一条竖线，包含很多的数据点，但是这些数据点属于不同的时间线。而当前的设计是一条时间线对应一个独立的文件，所以每次写入都会需要向很多不同的文件写入极少量的数据。针对这个问题，V2存储引擎的优化策略是Chunk写，针对单个时间线的写入必须是批量写，那就需要数据在时间线维度累积一定时间后才能凑到一定量的数据点。Chunk写策略带来的好处除了批量写外，还能优化热数据查询效率以及数据压缩率。V2存储引擎使用了和Facebook Gorilla一样的压缩算法，能够将16个字节的数据点压缩到平均1.37个字节，节省12倍内存和空间。Chunk写就要求数据一定要在服务器内存里积累一定的时间，即热数据基本都在内存中，查询效率很高。&lt;/p&gt;

&lt;h4 id=&#34;v3&#34;&gt;v3&lt;/h4&gt;

&lt;p&gt;Prometheus 2.0版本引入了全新的V3存储引擎，提供了更高的写入和查询性能。&lt;/p&gt;

&lt;p&gt;V3引擎完全重新设计，但是也延续了v2的一些优化策略，也来解决V2引擎中存在的这些问题。V3引擎可以看做是一个简单版、针对时序数据场景优化过后的LSM，可以带着LSM的设计思想来理解，先看一下V3引擎中数据的文件目录结构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/data.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;data目录下存放所有的数据，data目录的下一级目录是以&amp;rsquo;b-&amp;lsquo;为前缀，顺序自增的ID为后缀的目录，代表Block。每个Block下有chunks、index和meta.json，chunks目录下存放chunk的数据。这个chunk和V2的chunk是一个概念，唯一的不同是一个chunk内会包含很多的时间线，而不再只是一条。index是这个block下对chunk的索引，可以支持根据某个label快速定位到时间线以及数据所在的chunk。meta.json是一个简单的关于block数据和状态的一个描述文件。要理解V3引擎的设计思想，只需要搞明白几个问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk文件的存储格式？&lt;/li&gt;
&lt;li&gt;index的存储格式，如何实现快速查找？&lt;/li&gt;
&lt;li&gt;为何最后一个block没有chunk目录但有一个wal目录？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/block.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Prometheus将数据按时间维度切分为多个block，每个block被认为是独立的一个数据库，覆盖不同的时间范围的数据，完全没有交叉。每个Block下chunk内的数据dump到文件后即不可再修改，只有最近的一个block允许接收新数据。最新的block内数据写入会先写到一个内存的结构，为了保证数据不丢失，会先写一份WAL（write ahead log）。&lt;/p&gt;

&lt;p&gt;V3完全借鉴了LSM的设计思想，针对时序数据特征做了一些优化，带来很多好处：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当查询一个时间范围的数据时，可快速排除无关的block。每个block有独立的index，能够有效解决V2内遇到的『无效时间线 Series Churn』的问题。&lt;/li&gt;
&lt;li&gt;内存数据dump到chunk file，可高效采用大块数据顺序写，对SSD和HDD都很友好。&lt;/li&gt;
&lt;li&gt;和V2一样，最近的数据在内存内，最近的数据也是最热的数据，在内存可支持最高效的查询。&lt;/li&gt;
&lt;li&gt;老数据的回收变得非常简单和高效，只需要删除少量目录。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/block1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;V3内block以两个小时的跨度来切割，这个时间跨度不能太大，也不能太小。太大的话若内存中要保留两个小时数据，则内存占用会比较大。太小的话会导致太多的block，查询时需要对更多的文件做查询。所以两个小时是一个综合考虑后决定的值，但是当查询大跨度时间范围时，仍不可避免需要跨多个文件，例如查询一周时间跨度需要84个文件。V3也是采用了LSM一样的compaction策略来做查询优化，把小的block合并为大的block，compaction期间也可做其他一些事，例如删除过期数据或重构chunk数据以支持更高效的查询。InfluxDB也有多种不同的compaction策略，在不同的时刻使用。&lt;/p&gt;

&lt;p&gt;prometheus重2.0版本开始使用了V3引擎，V3没有和V2一样采用LevelDB，在已经持久化的Block，Index已经固定下来，不可修改。而对于最新的还在写数据的block，V3则会把所有的索引全部hold在内存，维护一个内存结构，等到这个block被关闭，再持久化到文件。这样做会比较简单一点，内存里维护时间线到ID的映射以及label到ID列表的映射，查询效率会很高。而且Prometheus对Label的基数会有一个假设：『a real-world dataset of ~4.4 million series with about 12 labels each has less than 5,000 unique labels』，这个全部保存在内存也是一个很小的量级，完全没有问题。InfluxDB采用的是类似的策略，而其他一些TSDB则直接使用ElasticSearch作为索引引擎。&lt;/p&gt;

&lt;p&gt;针对时序数据这种写多读少的场景，类LSM的存储引擎还是有不少优势的。有些TSDB直接基于开源的LSM引擎分布式数据库例如Hbase或Cassandra，也有自己基于LevelDB/RocksDB研发，或者再像InfluxDB和Prometheus一样纯自研，因为时序数据这一特定场景还是可以做更多的优化，例如索引、compaction策略等。Prometheus V3引擎的设计思想和InfluxDB真的很像，优化思路高度一致，后续在有新的需求的出现后，会有更多变化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prometheus将Timeseries数据按2小时一个block进行存储。每个block由一个目录组成，该目录里包含：一个或者多个chunk文件（保存timeseries数据）、一个metadata文件、一个index文件（通过metric name和labels查找timeseries数据在chunk文件的位置）。最新写入的数据保存在内存block中，达到2小时后写入磁盘。为了防止程序崩溃导致数据丢失，实现了WAL（write-ahead-log）机制，将timeseries原始数据追加写入log中进行持久化。删除timeseries时，删除条目会记录在独立的tombstone文件中，而不是立即从chunk文件删除。启动时会以写入日志(WAL)的方式来实现重播，从而恢复数据。&lt;/p&gt;

&lt;p&gt;这些2小时的block会在后台压缩成更大的block，数据压缩合并成更高level的block文件后删除低level的block文件。这个和leveldb、rocksdb等LSM树的思路一致。&lt;/p&gt;

&lt;p&gt;这些设计和Gorilla的设计高度相似，所以Prometheus几乎就是等于一个缓存TSDB。它本地存储的特点决定了它不能用于long-term数据存储，只能用于短期窗口的timeseries数据保存和查询，并且不具有高可用性（宕机会导致历史数据无法读取）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;具体形式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、磁盘文件结构&lt;/p&gt;

&lt;p&gt;内存中的block&lt;/p&gt;

&lt;p&gt;内存中的block数据未刷盘时，block目录下面主要保存wal文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./data/01BKGV7JBM69T2G1BGBGM6KB12
./data/01BKGV7JBM69T2G1BGBGM6KB12/meta.json
./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000002
./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;持久化的block&lt;/p&gt;

&lt;p&gt;持久化的block目录下wal文件被删除，timeseries数据保存在chunk文件里。index用于索引timeseries在wal文件里的位置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./data/01BKGV7JC0RY8A6MACW02A2PJD
./data/01BKGV7JC0RY8A6MACW02A2PJD/meta.json
./data/01BKGV7JC0RY8A6MACW02A2PJD/index
./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks
./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks/000001
./data/01BKGV7JC0RY8A6MACW02A2PJD/tombstones
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、mmap&lt;/p&gt;

&lt;p&gt;使用mmap读取压缩合并后的大文件（不占用太多句柄），建立进程虚拟地址和文件偏移的映射关系，只有在查询读取对应的位置时才将数据真正读到物理内存。绕过文件系统page cache，减少了一次数据拷贝。查询结束后，对应内存由Linux系统根据内存压力情况自动进行回收，在回收之前可用于下一次查询命中。因此使用mmap自动管理查询所需的的内存缓存，具有管理简单，处理高效的优势。
从这里也可以看出，它并不是完全基于内存的TSDB，和Gorilla的区别在于查询历史数据需要读取磁盘文件。&lt;/p&gt;

&lt;p&gt;3、Compaction&lt;/p&gt;

&lt;p&gt;Compaction主要操作包括合并block、删除过期数据、重构chunk数据。其中合并多个block成为更大的block，可以有效减少block个数，当查询覆盖的时间范围较长时，避免需要合并很多block的查询结果。
为提高删除效率，删除时序数据时，会记录删除的位置，只有block所有数据都需要删除时，才将block整个目录删除。因此block合并的大小也需要进行限制，避免保留了过多已删除空间（额外的空间占用）。比较好的方法是根据数据保留时长，按百分比（如10%）计算block的最大时长。&lt;/p&gt;

&lt;p&gt;4、Inverted Index&lt;/p&gt;

&lt;p&gt;Inverted Index（倒排索引）基于其内容的子集提供数据项的快速查找。简而言之，我可以查看所有标签为app=“nginx”的数据，而不必遍历每一个timeseries，并检查是否包含该标签。
为此，每个时间序列key被分配一个唯一的ID，通过它可以在恒定的时间内检索，在这种情况下，ID就是正向索引。
举个栗子：如ID为9,10,29的series包含label app=&amp;ldquo;nginx&amp;rdquo;，则lable &amp;ldquo;nginx&amp;rdquo;的倒排索引为[9,10,29]用于快速查询包含该label的series。&lt;/p&gt;

&lt;p&gt;5、存储配置&lt;/p&gt;

&lt;p&gt;对于本地存储，prometheus提供了一些配置项，主要包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--storage.tsdb.path: 存储数据的目录，默认为data/，如果要挂外部存储，可以指定该目录
--storage.tsdb.retention.time: 数据过期清理时间，默认保存15天
--storage.tsdb.retention.size: 实验性质，声明数据块的最大值，不包括wal文件，如512MB
--storage.tsdb.retention: 已被废弃，改为使用storage.tsdb.retention.time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus将所有当前使用的块保留在内存中。此外，它将最新使用的块保留在内存中，最大内存可以通过storage.local.memory-chunks标志配置。&lt;/p&gt;

&lt;p&gt;监测当前使用的内存量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_chunks
process_resident_memory_bytes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监测当前使用的存储指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_series: 时间序列持有的内存当前块数量
prometheus_local_storage_memory_chunks: 在内存中持久块的当前数量


prometheus_local_storage_chunks_to_persist: 当前仍然需要持久化到磁盘的的内存块数量
prometheus_local_storage_persistence_urgency_score: 紧急程度分数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、性能&lt;/p&gt;

&lt;p&gt;在文章Writing a Time Series Database from Scratch里，作者给出了benchmark测试结果为Macbook Pro上写入达到2000万每秒。这个数据比Gorilla论文中的目标7亿次写入每分钟（1000千多万每秒）提供了更高的单机性能。&lt;/p&gt;

&lt;h3 id=&#34;remote-storage&#34;&gt;remote storage&lt;/h3&gt;

&lt;p&gt;Prometheus 的设计者非常看重监控系统自身的稳定性，所以 Prometheus 仅仅依赖了本地文件系统，而这就决定了 Prometheus 自身并不适合存储长期数据。本地存储的优势就是运维简单,缺点就是无法海量的metrics持久化和数据存在丢失的风险，我们在实际使用过程中，出现过几次wal文件损坏，无法再写入的问题。&lt;/p&gt;

&lt;p&gt;所以 Prometheus 提供了 remote read 和 remote write 的接口，让用户自己去实现对接，prometheus以两种方式与远程存储系统集成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prometheus可以以标准格式将其提取的样本写入远程URL。&lt;/li&gt;
&lt;li&gt;Prometheus可以以标准格式从远程URL读取（返回）样本数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/20190315.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Adapter 是一个中间组件，Prometheus 与 Adapter 之间通过由 Prometheus 定义的标准格式发送和接收数据，Adapter 与外部存储系统之间的通信可以自定义，目前 Prometheus 和 Adapter 之间通过 grpc 通信。Prometheus 将 samples 发送到 Adapter。为了提高效率，samples 会在队列中先缓存，再打包发送给 Adapter，所以一个读请求中包含了 start_timestamp，end_timestamp 和 label_matchers，response 则包含所有 match 到的 time series，也就是说，Prometheus 仅通过 Adapter 来获取时间序列，进一步的处理都在 Prometheus 中完成。&lt;/p&gt;

&lt;p&gt;remote read 和 remote write 的配置还没有稳定，我们从代码中来一探究竟，HTTPClientConfig 可以用来配置 HTTP 相关的 auth 信息，proxy 方式，以及 tls。WriteRelabelConfigs 用在发送过程中对 timeseries 进行 relabel。QueueConfig 定义了发送队列的 batch size，queue 数量，发送失败时的重试次数与等待时间等参数。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;write&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prometheus 默认定义了 1000 个 queue，batch size 为 100，预期可以达到 1M samples/s 的发送速率。Prometheus 输出了一些 queue 相关的指标，例如 failed_samples_total, dropped_samples_total，如果这两个指标的 rate 大于 0，就需要说明 Remote Storage 出现了问题导致发送失败，或者队列满了导致 samples 被丢弃掉。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReadRecent 如果为 false，Prometheus 会在处理查询时比较本地存储中最早的数据的 timestamp 与 query 的 start timestamp，如果发现需要的数据都在本地存储中，则会跳过对 Remote Storage 的查询。&lt;/p&gt;

&lt;p&gt;社区中支持prometheus远程读写的方案&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AppOptics: write
Chronix: write
Cortex: read and write
CrateDB: read and write
Elasticsearch: write
Gnocchi: write
Graphite: write
InfluxDB: read and write
OpenTSDB: write
PostgreSQL/TimescaleDB: read and write
SignalFx: write
clickhouse: read and write
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前远程存储使用最多的是influxdb（收费），opentsdb（依赖hbase），m3db（不稳定）,VM(很优秀的存储查询性能)。&lt;/p&gt;

&lt;h2 id=&#34;服务发现&#34;&gt;服务发现&lt;/h2&gt;

&lt;p&gt;Prometheus目前支持以下平台的动态发现能力：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;容器编排系统：kubernetes&lt;/li&gt;
&lt;li&gt;云平台：EC2、Azure、OpenStack&lt;/li&gt;
&lt;li&gt;服务发现：DNS、Zookeeper、Consul 等。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;加载配置
​
ServiceDiscoveryConfig配置结构如下：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type ServiceDiscoveryConfig struct {
    // 静态服务发现配置
    StaticConfigs []*targetgroup.Group `yaml:&amp;quot;static_configs,omitempty&amp;quot;`
    // DNS服务发现配置
    DNSSDConfigs []*dns.SDConfig `yaml:&amp;quot;dns_sd_configs,omitempty&amp;quot;`
    // 配置文件服务发现配置
    FileSDConfigs []*file.SDConfig `yaml:&amp;quot;file_sd_configs,omitempty&amp;quot;`
    // Consul服务发现配置
    ConsulSDConfigs []*consul.SDConfig `yaml:&amp;quot;consul_sd_configs,omitempty&amp;quot;`
    // zookeeper Serverset 服务发现配置
    ServersetSDConfigs []*zookeeper.ServersetSDConfig `yaml:&amp;quot;serverset_sd_configs,omitempty&amp;quot;`
    // zookeeper Nerve 服务发现配置
    NerveSDConfigs []*zookeeper.NerveSDConfig `yaml:&amp;quot;nerve_sd_configs,omitempty&amp;quot;`
    // 根据Marathon API 服务发现配置
    MarathonSDConfigs []*marathon.SDConfig `yaml:&amp;quot;marathon_sd_configs,omitempty&amp;quot;`
    // 根据Kubernetes API 服务发现配置
    KubernetesSDConfigs []*kubernetes.SDConfig `yaml:&amp;quot;kubernetes_sd_configs,omitempty&amp;quot;`
    // GCE 服务发现配置
    GCESDConfigs []*gce.SDConfig `yaml:&amp;quot;gce_sd_configs,omitempty&amp;quot;`
    // EC2服务发现配置
    EC2SDConfigs []*ec2.SDConfig `yaml:&amp;quot;ec2_sd_configs,omitempty&amp;quot;`
    // Openstack 服务发现配置
    OpenstackSDConfigs []*openstack.SDConfig `yaml:&amp;quot;openstack_sd_configs,omitempty&amp;quot;`
    // Azure 服务发现配置
    AzureSDConfigs []*azure.SDConfig `yaml:&amp;quot;azure_sd_configs,omitempty&amp;quot;`
    // Triton 服务发现配置
    TritonSDConfigs []*triton.SDConfig `yaml:&amp;quot;triton_sd_configs,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​
在Prometheus初始化过程中，通过执行discoveryManagerScrape.ApplyConfig进行服务发现相关配置的加载。&lt;/p&gt;

&lt;p&gt;移除目前正在运行的providers，根据新的provided 配置，启动新的providers。。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg map[string]sd_config.ServiceDiscoveryConfig) error {
    // 加锁
    m.mtx.Lock()
  // 函数结束后 解锁
    defer m.mtx.Unlock()
  // 遍历已存在target
    for pk := range m.targets {
        if _, ok := cfg[pk.setName]; !ok {
      // 删除标签
            discoveredTargets.DeleteLabelValues(m.name, pk.setName)
        }
    }
  // 取消所有Discoverer
    m.cancelDiscoverers()
    for name, scfg := range cfg {
    // 根据scfg，注册服务发现实例
        m.registerProviders(scfg, name)
    // 设置标签
        discoveredTargets.WithLabelValues(m.name, name).Set(0)
    }
    for _, prov := range m.providers {
    // 启动服务发现实例
        m.startProvider(m.ctx, prov)
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;注册Providers&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中m.registerProviders的主要作用就是根据cfg（配置）注册所有provider实例，保存在m.providers&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) registerProviders(cfg sd_config.ServiceDiscoveryConfig, setName string) {
    // 标签
    var added bool
  // 加载Providers的add方法
    add := func(cfg interface{}, newDiscoverer func() (Discoverer, error)) {
    // 读取cfg类型
        t := reflect.TypeOf(cfg).String()
        for _, p := range m.providers {
      // 检查该cfg是否加载过
            if reflect.DeepEqual(cfg, p.config) {
        // 如果加载过，记录该job
                p.subs = append(p.subs, setName)
        // 变更标签状态
                added = true
        // 跳出
                return
            }
        }
        // 创建一个Discoverer实例
        d, err := newDiscoverer()
        if err != nil {
            level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Cannot create service discovery&amp;quot;, &amp;quot;err&amp;quot;, err, &amp;quot;type&amp;quot;, t)
            failedConfigs.WithLabelValues(m.name).Inc()
            return
        }
        // 创建一个provider
        provider := provider{
      // 生成provider名称
            name:   fmt.Sprintf(&amp;quot;%s/%d&amp;quot;, t, len(m.providers)),
      // 关联对应的Discoverer实例（比如DNS、zk等）
            d:      d,
      // 关联配置
            config: cfg,
      // 关联job
            subs:   []string{setName},
        }
    // 添加该provider到m.providers队列中
        m.providers = append(m.providers, &amp;amp;provider)
        // 更新标签
        added = true
    }
    // 遍历DNS配置，生成该Discoverer
    for _, c := range cfg.DNSSDConfigs {
        add(c, func() (Discoverer, error) {
            return dns.NewDiscovery(*c, log.With(m.logger, &amp;quot;discovery&amp;quot;, &amp;quot;dns&amp;quot;)), nil
        })
    }
  .
  .
  .
  .
  .
  .
  // 类似配置遍历省略，感兴趣可以阅读源码查看
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;启动Provider&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在ApplyConfig，执行m.startProvider(m.ctx, prov)启动provider。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) startProvider(ctx context.Context, p *provider) {
    level.Info(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Starting provider&amp;quot;, &amp;quot;provider&amp;quot;, p.name, &amp;quot;subs&amp;quot;, fmt.Sprintf(&amp;quot;%v&amp;quot;, p.subs))
    ctx, cancel := context.WithCancel(ctx)
  // 记录发现的服务
    updates := make(chan []*targetgroup.Group)
    // 添加取消方法
    m.discoverCancel = append(m.discoverCancel, cancel)
    // 执行run  每个服务发现都有自己的run方法。
    go p.d.Run(ctx, updates)
  // 更新发现的服务
    go m.updater(ctx, p, updates)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里分析DNS 服务发现对应的Run方法。需要标注下，DNS对应的Discovery其实是refresh中的Discovery的Run实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d.Discovery = refresh.NewDiscovery(
        logger,
        &amp;quot;dns&amp;quot;,
        time.Duration(conf.RefreshInterval),
        d.refresh,
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (d *Discovery) Run(ctx context.Context, ch chan&amp;lt;- []*targetgroup.Group) {
  // 首次进入，执行更新
    tgs, err := d.refresh(ctx)
    if err != nil {
        level.Error(d.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Unable to refresh target groups&amp;quot;, &amp;quot;err&amp;quot;, err.Error())
    } else {
        select {
        case ch &amp;lt;- tgs:
        case &amp;lt;-ctx.Done():
            return
        }
    }
    // 创建定时器
    ticker := time.NewTicker(d.interval)
    defer ticker.Stop()

    for {
        select {
        case &amp;lt;-ticker.C:
      // 定时执行更新，如果发现变化，通过ch发出更新信息
            tgs, err := d.refresh(ctx)
            if err != nil {
                level.Error(d.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Unable to refresh target groups&amp;quot;, &amp;quot;err&amp;quot;, err.Error())
                continue
            }

            select {
      // 发送 变化的targets
            case ch &amp;lt;- tgs:
            case &amp;lt;-ctx.Done():
                return
            }
        case &amp;lt;-ctx.Done():
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;更新服务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当服务发现变化的targets时，通过updates chan进行更新。最终更新Discovery Manager的targets。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) updater(ctx context.Context, p *provider, updates chan []*targetgroup.Group) {
   for {

      select {
      case &amp;lt;-ctx.Done():
         return
      // 接收updates数据
      case tgs, ok := &amp;lt;-updates:
         receivedUpdates.WithLabelValues(m.name).Inc()
         if !ok {
            level.Debug(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;discoverer channel closed&amp;quot;, &amp;quot;provider&amp;quot;, p.name)
            return
         }
                 // 更新targets
         for _, s := range p.subs {
            m.updateGroup(poolKey{setName: s, provider: p.name}, tgs)
         }

         select {
         // 发送更新通知
         case m.triggerSend &amp;lt;- struct{}{}:
         default:
         }
      }
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;启动discovery manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加载完配置，并且完成注册、启动、更新操作后，开始执行discoveryManagerScrape.Run方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Run() error {
  // 后台处理
    go m.sender()
    for range m.ctx.Done() {
        m.cancelDiscoverers()
        return m.ctx.Err()
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定时执行，当接收到服务发现的更新通知，通过m.allGroups()同步服务快照信息到scrapeManager&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) sender() {
  // 创建定时器
    ticker := time.NewTicker(m.updatert)
    defer ticker.Stop()

    for {
        select {
        case &amp;lt;-m.ctx.Done():
            return
        case &amp;lt;-ticker.C:
            select {
      // 检测到更新
            case &amp;lt;-m.triggerSend:
                sentUpdates.WithLabelValues(m.name).Inc()
                select {
        // 通过allGroups同步服务快照信息到scrapeManager
                case m.syncCh &amp;lt;- m.allGroups():
                default:
                    delayedUpdates.WithLabelValues(m.name).Inc()
                    level.Debug(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;discovery receiver&#39;s channel was full so will retry the next cycle&amp;quot;)
                    select {
                    case m.triggerSend &amp;lt;- struct{}{}:
                    default:
                    }
                }
            default:
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;关联ScrapeManager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关联&lt;/p&gt;

&lt;p&gt;在ScrapeManager在启动的时候会关联discoveryManagerScrape.SyncCh()。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func() error {
                &amp;lt;-reloadReady.C
                // 关联 discoveryManagerScrape 的 syncCh
                err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
                level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape manager stopped&amp;quot;)
                return err
            },
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新&lt;/p&gt;

&lt;p&gt;更新ScrapeManager的targets&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Run(tsets &amp;lt;-chan map[string][]*targetgroup.Group) error {
    go m.reloader()
    for {
        select {
    // 收到更新targets
        case ts := &amp;lt;-tsets:
            m.updateTsets(ts)

            select {
            case m.triggerReload &amp;lt;- struct{}{}:
            default:
            }

        case &amp;lt;-m.graceShut:
            return nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行更新&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) updateTsets(tsets map[string][]*targetgroup.Group) {
    m.mtxScrape.Lock()
  // 替换新的 tagets
    m.targetSets = tsets
    m.mtxScrape.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;discoveryManager在加载配置的时候，顺便完成provider的注册、启动、以及discovery的自更新通知操作。discoveryManager与ScrapeManager通过discoveryManager的syncCh通道来关联同步。&lt;/p&gt;

&lt;p&gt;整个服务发现的流程很值得学习，尤其是discoveryManager支持多种服务发现的扩展配置的相关设设计很值得学习。&lt;/p&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;Prometheus有两个比较著名的扩展版一个是cortex，另一个是thanos。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos&lt;/a&gt;在github的简介是『Highly available Prometheus setup with long term storage capabilities』，它基于Prometheus的最大改进是底层存储可扩展支持对象存储，例如AWS的S3，使得单机容量可扩展。这个得益于Prometheus 2.0中V3引擎的特性，持久化的Chunk文件是immutable的，所以能够很容易迁移到对象存储上。从它的设计文档里可以看出，它引入了一个Sidecar节点，与Prometheus server结对部署，主要作用将本地数据backup到远端的对象存储。当然数据被切割到本地和对象存储内后，为了支持统一的查询接口，又引入了Store层。Store层支持标准查询接口，屏蔽了底层是对象存储的细节，同时做了一些查询优化例如对Index的缓存。thanos中包含多个类型的节点，包括Prometheus Server、Sidecar、Store node、Rule node、compactor和query layer，其中只有query layer能水平扩展，因为其是无状态的。也就是说，单个实例的Prometheus其写入能力还是会有瓶颈，cotex相比它则在scalability上改进了更多。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/&#34;&gt;cortex&lt;/a&gt;在github的简介是『A multitenant, horizontally scalable Prometheus as a Service』，几个关键词：多租户、水平扩展及服务化。&lt;/p&gt;

&lt;p&gt;现在还可以使用远程存储聚合的方式来实现集群，比如做的比较好的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- K8s监控方案</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s-base/</link>
          <pubDate>Sat, 12 May 2018 17:02:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s-base/</guid>
          <description>&lt;p&gt;一个完整的监控体系包括：采集数据、分析存储数据、展示数据、告警以及自动化处理、监控工具自身的安全机制。我们来看看使用prometheus进行kubernetes的容器监控。&lt;/p&gt;

&lt;h1 id=&#34;物理部署promehteus监控k8s&#34;&gt;物理部署promehteus监控K8s&lt;/h1&gt;

&lt;h2 id=&#34;集群监控&#34;&gt;集群监控&lt;/h2&gt;

&lt;p&gt;k8s的集群的监控主要分为以下三个方面，当然还有一些k8s扩展使用的组件都是由对应的监控的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;k8s的物理机监控&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;架构图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20180512-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;直接使用prometheus的node-exporter就可以来获取数据的。Node-exporter会部署在每一个节点上，获取当前物理机的指标，当k8s的node节点数多的时候需要分组进行采集，并且k8s使用的网络支持固定ip，所以直接采用将ip：port注册到consul中，然后prometheus获取注册信息直接采集数据，物理机监控主要是使用node_exporter探针来获取物理机的cpu，内存，磁盘空间和i/o等指标。&lt;/p&gt;

&lt;p&gt;node_exporter可以直接采用的是k8s的daemonset部署的方式，先将node-exporter打成镜像，部署在k8s pod上&lt;/p&gt;

&lt;p&gt;物理监控我们必须要关心一下我们常用的指标&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;物理机层面&lt;/p&gt;

&lt;p&gt;cpu的使用率／已经使用／总量／request&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用率：1- avg(irate(node_cpu_seconds_total{mode=&amp;ldquo;idle&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}[2m]))&lt;/li&gt;
&lt;li&gt;已经使用：(count(node_cpu_seconds_total{mode=&amp;ldquo;system&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})-sum(irate(node_cpu_seconds_total{mode=&amp;ldquo;idle&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}[5m])))&lt;/li&gt;
&lt;li&gt;总量：count(node_cpu_seconds_total{mode=&amp;ldquo;system&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;memory的使用率／已经使用／总量／request&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用率：((sum(node_memory_MemTotal_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}) - sum(node_memory_MemFree_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}) - sum(node_memory_Cached_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})) / sum(node_memory_MemTotal_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})) * 100&lt;/li&gt;
&lt;li&gt;已经使用：sum(node_memory_MemTotal_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}) - sum(node_memory_MemFree_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;}) - sum(node_memory_Cached_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})&lt;/li&gt;
&lt;li&gt;总量：sum(node_memory_MemTotal_bytes{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;})&lt;/li&gt;
&lt;li&gt;request：sum(kube_pod_container_resource_requests_memory_bytes{cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,node=~&amp;ldquo;$node_name&amp;rdquo;,instance=&amp;ldquo;$instance&amp;rdquo;})&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;disk和disk io&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用率：(sum(node_filesystem_size_bytes{device!=&amp;ldquo;rootfs&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,fstype !=&amp;ldquo;fuse.ossfs.ossInode&amp;rdquo;}) - sum(node_filesystem_free_bytes{device!=&amp;ldquo;rootfs&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,fstype !=&amp;ldquo;fuse.ossfs.ossInode&amp;rdquo;})) / sum(node_filesystem_size_bytes{device!=&amp;ldquo;rootfs&amp;rdquo;,ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,fstype !=&amp;ldquo;fuse.ossfs.ossInode&amp;rdquo;}) * 100&lt;/li&gt;
&lt;li&gt;io-read：sum(rate(node_disk_read_bytes_total{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,device=&amp;ldquo;sdb&amp;rdquo;}[5m]))&lt;/li&gt;
&lt;li&gt;io-write：sum(rate(node_disk_written_bytes_total{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,device=&amp;ldquo;sdb&amp;rdquo;}[5m]))&lt;/li&gt;
&lt;li&gt;io-time：sum(rate(node_disk_io_time_seconds_total{ip=~&amp;ldquo;$node&amp;rdquo;,cluster_name=~&amp;ldquo;$cluster&amp;rdquo;,device=&amp;ldquo;sdb&amp;rdquo;}[5m]))&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;指标可以直接查看对应的grafana的json文件，这边就不一一列举了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;k8s本身指标的监控&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要是k8s自身使用的组件的指标监控&lt;/p&gt;

&lt;p&gt;架构图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20180512-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见都是通过k8s自身组件来暴露指标给prometheus进行采集的。直接将集群的机器的IP：port注册到consul中去给prometheus拉去探测。&lt;/p&gt;

&lt;p&gt;这边有一个不同的地方，就是每个pod的性能情况都是通过cadvisor统一获取，不需要对每一个pod进行按着探针来监控，pod的注册也是为了业务监控的需要，和自身的监控指标并木有关系。&lt;/p&gt;

&lt;p&gt;在k8s中安装kube-state-metrics组件用来采集kubernetes的各种组件状态信息。&lt;/p&gt;

&lt;p&gt;Kubernetes集群上Pod, DaemonSet, Deployment, Job, CronJob等各种资源对象的状态需要监控数据可以被cadvisor采集。cadvisor集成在kubelet中，不需要单独部署。&lt;/p&gt;

&lt;p&gt;下面是一些监控的组件的端口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Node需要注册target，包括kubelet, cadvisor集成在kubelet中和kubelet同时暴露出来，但是使用不同的url, node-exporter, (node_list包含master节点和node节点)
For node in node_list
http://node_ip:9100/metrics


kube-scheduler监控
For node in masters
http://node_ip:10251/metrics

kube-controller-manager监控
For node in masters
http://node_ip:10252/metrics

kube-apiserver监控：需要权限的prometheus带着证书去访问
https://vip:6443/metrics
kube-state-metrics监控
https://vip:10241/metrics

这个vip会把请求转成master上的apiserver或者kube-state-metrics

etcd：需要权限的prometheus带着证书去访问
https://etcd-ip:2379/metrics
https://etcd-event-ip:2382/metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指标都是重上面的组件中要么采集，要么暴露出来的，主要监控项（表达式可以去json文件中去看）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;pod和container的cpu，memory&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pod的数量和状态&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pod的disk&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;etcd等各个组件的状态&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;应用的监控&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要是对一些中间件的监控，架构设计如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20180512-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;K8s内部部署采用单pod单容器多进程的模式，先把镜像打好，然后在启动应用的时候把探针放进去一起启动。&lt;/p&gt;

&lt;p&gt;直接通过ip:port来访问探针指标，使用外部的prometheus来采集探针提供的指标，最后在grafana上进行展示。&lt;/p&gt;

&lt;p&gt;这边注明一下使用单容器多进程的方式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.09.09&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面的方式会互相影响，所以使用sidecar的控制器，可以自动启停增加删除容器，所以最好使用sidecar的模式进行监控。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;sidecar的模式，单pod双容器&lt;/p&gt;

&lt;p&gt;这种模式，探针和应用分离开来，互不影响，便于更新，还在同一个pod下，可以共享网络
但是这种模式，单独启动了一个容器占用了一部分资源，两个镜像比较麻烦，需要管理。&lt;/p&gt;

&lt;p&gt;个人比较推荐sidecar模式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;单pod单容器多进程模式&lt;/p&gt;

&lt;p&gt;这种模式，不能实现解耦，一个应用挂了，监控也跟着挂了，
但是不占用资源，不用管理。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多pod单容器模式&lt;/p&gt;

&lt;p&gt;这种模式，网络需要打通，还是新建pod浪费资源，目前48C256G的机器没个节点上最多要求不能超过一百个pod。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;容器的设计模式&#34;&gt;容器的设计模式&lt;/h2&gt;

&lt;p&gt;这边讲解一些容器的设计模式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/monitor-scheme/20180512-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;重这个对比图中可以看出&lt;/p&gt;

&lt;p&gt;1)单容器管理模式；&lt;/p&gt;

&lt;p&gt;一个pod一个容器的模式，管理简单清晰。直接使用基本命令就可以运行，当然在一个容器中可以运行多个进程，互相协作。&lt;/p&gt;

&lt;p&gt;2)单节点多容器模式；&lt;/p&gt;

&lt;p&gt;多容器才可以体现k8s的强大，Pod是一个轻量级的节点，同一个Pod中的容器可以共享同一块存储空间和同一个网络地址空间，这使得我们可以实现一些组合多个容器在同一节点工作的模式。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;挎斗模式（Sidecar pattern）&lt;/p&gt;

&lt;p&gt;这种模式主要是利用在同一Pod中的容器可以共享存储空间的能力。比如一个往文件系统中写文件，一个容器重文件系统中读取文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;外交官模式(Ambassador pattern)&lt;/p&gt;

&lt;p&gt;这种模式主要利用同一Pod中的容器可以共享网络地址空间的特性。比如一个容器开启一个proxy，给外部访问，类似于网关（外交官），然后这个容器来对转发请求到内部其他容器中处理，然后proxy容器和内部其他容器共享内部网络，直接使用localhost访问就好了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;适配器模式（Adapter pattern）&lt;/p&gt;

&lt;p&gt;分布地执行和存储，统一的监控和管理。比如业务逻辑容器的pod中运行一个exporter容器，对外统一暴露指标，适配prometheus。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其实这三种只是根据使用不同特性区分了而已，其实就是pod内部共享。&lt;/p&gt;

&lt;p&gt;3)多节点多容器模式；&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;多节点选举模式&lt;/p&gt;

&lt;p&gt;多节点选举在分布式系统中是一种重要的模式，特别是对有状态服务来说。在分布式系统中，一般来说，无状态服务，可以随意的水平伸缩，只要把运行业务逻辑的实例复制出去运行就可以，这也就是K8s里ReplicationController和ReplicaSet所做的事情。&lt;/p&gt;

&lt;p&gt;对于有状态服务，人们也希望能够水平的扩展，但因为每个实例有自己的持久化状态，而这个持久化状态必须要延续它的生命，因此，有状态服务的水平伸缩模式就是状态的分片，其中机制跟数据库的分片是一致的。那么对于一个原生为分布式系统设计的有状态服务，每个实例与分片数据的对应关系，就成为这个有状态服务的全局信息。对于任何服务，多个实例的全局信息都需要一个保存的地方。&lt;/p&gt;

&lt;p&gt;一个简单的办法是保存在外部的一个代理服务器上，这也就是MariaDB的Galera解决方案的做法，也是所以代理服务器为后端服务器所做的事情。但这种方式的问题在于，系统要依赖外部代理服务器，而代理服务器本身的高可用和水平伸缩还是没有解决的问题。&lt;/p&gt;

&lt;p&gt;所以对于要原生自己解决高可用和水平伸缩问题的系统，例如Etcd和ElasticSearch，一定要有原生的主控节点选举机制。这样这个分布式系统就不需要依赖外部的系统来维护自己的状态了。对于一个分布式系统，最主要的系统全局信息，就是集群中有哪些节点，Master节点是哪个，每个节点对应哪个分片。主控节点的任务，就是保存和分发这些信息。&lt;/p&gt;

&lt;p&gt;在K8s集群中，一个微服务实例Pod可以有多个容器。这一特性很好地提高了多节点选举机制的可重用性。它使得我们可以专门开发一个用于选举的容器镜像，在实际部署中，将选举容器和普通应用容器组合起来，应用容器只需要从本地的选举容器读取状态，就可以得到选举结果。这样，使得应用容器可以只关注自身业务逻辑相关的代码。&lt;/p&gt;

&lt;p&gt;这就是多节点多容器的选举模式，是一种使用方式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;工作队列模式&lt;/p&gt;

&lt;p&gt;分布式系统的一个重要作用是能够充分利用多个物理计算资源的能力，特别是在动态按需调动计算资源完成计算任务。设想如果有大量的需要处理的任务随机的到来，对计算资源需要的容量是不确定地；显然，按照最大可能计算量和最小可能计算量设置计算节点都是不合理的。所以可以启动多个节点多个容器来处理队列中的任务，也是一种使用方式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;分散收集模式&lt;/p&gt;

&lt;p&gt;根节点接受到来自客户端的服务请求，将服务请求分配给不同的业务模块分散处理，处理结果收集到根节点，经过一定的汇聚合并运算，产生一个合并的结果交付给客户端。也就是启动多个节点多个容器来协调处理，再通过代理合并返回，也是一种使用方式。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其实可以看见，容器的模式都是来源于物理的使用方式，也是一些常用的架构，只不过环境换成了容器，有了对应的制约和方便管理。&lt;/p&gt;

&lt;h2 id=&#34;探针组件&#34;&gt;探针组件&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Kube-state-metrics&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将Kube-state-metrics使用镜像在k8s中运行，运行在master节点上，可以获取到kube相关的所有指标，也就是具体的各种资源对象的状态指标。&lt;/p&gt;

&lt;p&gt;对应的ymal文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources:
  - configmaps
  - secrets
  - nodes
  - pods
  - services
  - resourcequotas
  - replicationcontrollers
  - limitranges
  - persistentvolumeclaims
  - persistentvolumes
  - namespaces
  - endpoints
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups: [&amp;quot;extensions&amp;quot;]
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups: [&amp;quot;apps&amp;quot;]
  resources:
  - statefulsets
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups: [&amp;quot;batch&amp;quot;]
  resources:
  - cronjobs
  - jobs
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups: [&amp;quot;autoscaling&amp;quot;]
  resources:
  - horizontalpodautoscalers
  verbs: [&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: kube-system
  name: kube-state-metrics-resizer
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources:
  - pods
  verbs: [&amp;quot;get&amp;quot;]
- apiGroups: [&amp;quot;extensions&amp;quot;]
  resources:
  - deployments
  resourceNames: [&amp;quot;kube-state-metrics&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;update&amp;quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kube-state-metrics
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kube-state-metrics-resizer
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-state-metrics
  namespace: kube-system
spec:
  template:
    metadata:
      labels:
        k8s-app: kube-state-metrics
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      serviceAccountName: kube-state-metrics
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/master.node: &amp;quot;&amp;quot;
      containers:
      - name: kube-state-metrics
        image: xgharborsit01.sncloud.com/sncloud/kube-state-metrics:1.4.0
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 200m
            memory: 128Mi
        env:
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: KUBERNETES_SERVICE_HOST
          value: 10.243.129.252
        - name: KUBERNETES_SERVICE_PORT
          value: &amp;quot;6443&amp;quot;
        volumeMounts:
        - mountPath: /opt/kube-state-metrics/logs
          subPath: kube-state-metrics
          name: k8slog
        ports:
        - name: http-metrics
          containerPort: 10241
        - name: telemetry
          containerPort: 10242
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10241
          initialDelaySeconds: 5
          timeoutSeconds: 5
      volumes:
      - hostPath:
          path: /var/run/docker.sock
        name: docker-sock
      - hostPath:
          path: /k8s_log
        name: k8slog
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Node-exporter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Node-exporter也是使用镜像运行在k8s的每个节点上，用于获取k8s部署节点的物理机器资源信息，并不能获取对应的k8s集群的信息。&lt;/p&gt;

&lt;p&gt;对应的yaml文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: kube-system
spec:
  template:
    metadata:
      labels:
        app: node-exporter
      name: node-exporter
      namespace: kube-system
    spec:
      containers:
      - image: xgharborsit01.sncloud.com:443/sncloud/node_exporter:0.16.0
        imagePullPolicy: IfNotPresent
        name: node-exporter
        resources:
          limits:
            memory: 256Mi
            cpu: 200m
          requests:
            memory: 128Mi
            cpu: 100m
        env:
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        ports:
        - containerPort: 9100
          hostPort: 9100
          name: scrape
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - mountPath: /opt/node-exporter/logs
          subPath: node-exporter
          name: k8slog
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - hostPath:
            path: /k8s_log
            type: &amp;quot;&amp;quot;
          name: k8slog
      hostNetwork: true
      hostPID: true
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;cadvisor-proxy&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;cadvisor-proxy对cadvisor的指标进行过滤处理。这个组件也是部署在k8s上运行。&lt;/p&gt;

&lt;p&gt;对应的ymal部署文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: cadvisor-proxy
  namespace: kube-system
spec:
  template:
    metadata:
      labels:
        app: cadvisor-proxy
    spec:
      hostNetwork: true
      containers:
      - name: proxy
        image: xgharborsit01.sncloud.com:443/sncloud/cadvisor-proxy:1.0.0
        imagePullPolicy: Always
        args:
        - --log.path=/opt/cadvisor-proxy/logs
        resources:
          limits:
            memory: 100Mi
            cpu: 500m
          requests:
            memory: 100Mi
            cpu: 100m
        volumeMounts:
        - mountPath: /var/run/docker.sock
          name: docker-sock
          readOnly: true
        - mountPath: /opt/cadvisor-proxy/logs
          subPath: cadvisor-proxy
          name: k8slog
        env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
      volumes:
      - hostPath:
          path: /var/run/docker.sock
        name: docker-sock
      - hostPath:
          path: /k8s_log
        name: k8slog
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;各种应用的exporter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边监控应用都是将对应的探针和对应的应用一起打在一个镜像里，也就是一个容器运行了两个程序，直接获取对应的指标。也有使用sidecar的模式在一个pod中运行两个容器，获取指标。下面会具体说明sidecar和这种模式的相关差异使用&lt;/p&gt;

&lt;h3 id=&#34;组件的区别&#34;&gt;组件的区别&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;cAdvisor&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;cAdvisor是Google开源的容器资源监控和性能分析工具，它是专门为容器而生，本身也支持 Docker 容器。&lt;/p&gt;

&lt;p&gt;Cadvisor可以直接部署运行在vm或者docker上，监控当前环境下docker运行的容器的资源情况。&lt;/p&gt;

&lt;p&gt;在 Kubernetes 中，我们不需要单独去安装，cAdvisor 已经集成在kubelet中，自己暴露指标，作为 kubelet 内置的一部分程序可以直接使用。&lt;/p&gt;

&lt;p&gt;cAdvisor主要监控数据是容器的资源性能数据，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Kube-state-metrics&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;kube-state-metrics通过监听 API Server 生成有关资源对象的状态指标，比如 Deployment、Node、Pod，需要注意的是 kube-state-metrics 只是简单提供一个 metrics 数据，并不会存储这些指标数据，所以我们可以使用 Prometheus 来抓取这些数据然后存储。&lt;/p&gt;

&lt;p&gt;kube-state-metrics主要监控数据主要是k8s集群有关资源的状态。比如pod的状态，副本数，重启次数等资源状态进行监控。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;metrics-server&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;metrics-server 也是一个集群范围内的资源数据聚合工具，是 Heapster 的替代品，Heapster现在已经停止维护和使用，同样的，metrics-server 也只是显示数据，并不提供数据存储服务。&lt;/p&gt;

&lt;p&gt;metrics-server定时从Kubelet的Summary API(类似/ap1/v1/nodes/nodename/stats/summary)采集指标信息。&lt;/p&gt;

&lt;p&gt;metrics-server主要监控数据主要是kubelet和集成的cadvisor中暴露的容器和集群节点的资源情况，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况，以及kubelet对于同期的维护情况。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kube-state-metrics 和metrics-server和prometheus&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1.kube-state-metrics中监控的数据，metrics-server包括其他组件都无法提供。比如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;我调度了多少个replicas？现在可用的有几个？&lt;/li&gt;
&lt;li&gt;多少个Pod是running/stopped/terminated状态？&lt;/li&gt;
&lt;li&gt;Pod重启了多少次？&lt;/li&gt;
&lt;li&gt;我有多少job在运行中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;等这一系列资源状态的数据。&lt;/p&gt;

&lt;p&gt;2.两者其实没有太大的可比性，本质不一样。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Metrics-server是官方废弃heapster项目，新开的一个项目，就是为了将核心资源监控作为一等公民对待，即像pod、service那样直接通过api-server或者client直接访问，不再是安装一个hepater来汇聚且由heapster单独管理。从 Kubernetes1.8 开始，Metrics-server就作为一个 Deployment对象默认部署在由kube-up.sh脚本创建的集群中，这样就可以直接暴露相关聚合的数据。如果形象的说的话，Metrics-server实质上是一个监控系统。&lt;/li&gt;
&lt;li&gt;kube-state-metrics关注于获取k8s各种资源的最新状态，类似于监控系统中的一个探针。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见两种着力点的方向不一样。&lt;/p&gt;

&lt;p&gt;3.Prometheus和Metrics-server&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prometheus监控系统不用Metrics-server&lt;/li&gt;
&lt;li&gt;Prometheus监控系统一般不用Metrics-server，因为他们都是自己做指标收集、集成的。可以说Prometheus包含了metric-server的能力，且prometheus更加强大，比如Prometheus可以监控metric-server本身组件的监控状态并适时报警，这里的监控就可以通过kube-state-metrics来实现，如metric-server的pod的运行状态。&lt;/li&gt;
&lt;li&gt;当然也可以使用Metrics-server，Metrics-server从 Kubelet、cAdvisor 等获取核心数据，再由prometheus从 metrics-server 获取核心度量，从其他数据源（如 Node Exporter 等）获取非核心度量，再基于它们构建监控告警系统。但是这边新增了一层，在原来的不新增的情况下也是能实现的。所以正常prometheus不使用Metrics-server。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;问题处理&#34;&gt;问题处理&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;监控支持多k8s集群场景&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在prometheus采集的时候对集群打标签，一个集群统一一个标签&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;应用关联k8s&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;应用监控时，我们需要知道当前应用是跑在哪个pod上的，这样就需要唯一标志，因为我可以有pod_IP并且在pod创建后podip就固定了，所以可以根据pod ip到cavisor中获取对应的pod name，然后根据pod name来获取对应pod的指标，包括到kube-state-metrics匹配对应的状态指标。&lt;/p&gt;

&lt;p&gt;这样就可以实现，用户到应用，应用对应的在哪个pod，pod在哪个node，以及pod的相关信息，这种一层层的监控结构。&lt;/p&gt;

&lt;p&gt;后面对探针进行改造，对于每一个暴露出来的指标，加上pod ip和pod name的label，然后以这两个纬度进行监控。&lt;/p&gt;

&lt;h1 id=&#34;k8s部署promehteus监控k8s&#34;&gt;k8s部署promehteus监控K8s&lt;/h1&gt;

&lt;p&gt;探针这一块后端部署都是上面的物理部署一样的，包括cadvisor-proxy，node-exporter，kube-state-metrics等，因为这些本来就是部署在k8s上面的，使用的也是一样的，这这边要解决的就是上面部署在物理机上的组件，包含prometheus，thanos，grafana，alertmanager等还有服务发现的方式。&lt;/p&gt;

&lt;h2 id=&#34;手动部署&#34;&gt;手动部署&lt;/h2&gt;

&lt;h3 id=&#34;部署prometheus&#34;&gt;部署prometheus&lt;/h3&gt;

&lt;p&gt;在k8s上部署Prometheus十分简单，只需要下面4个文件：prometheus.rbac.yml, prometheus.config.yml, prometheus.deploy.yml, prometheus.svc.yml。&lt;/p&gt;

&lt;p&gt;下面给的例子中将Prometheus部署到kube-system命名空间。&lt;/p&gt;

&lt;p&gt;prometheus.rbac.yml定义了Prometheus容器访问k8s apiserver所需的ServiceAccount和ClusterRole及ClusterRoleBinding&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
- nonResourceURLs: [&amp;quot;/metrics&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus.config.yml configmap中的prometheus的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
    scrape_configs:

    - job_name: &#39;kubernetes-apiservers&#39;
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

    - job_name: &#39;kubernetes-nodes&#39;
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics

    - job_name: &#39;kubernetes-cadvisor&#39;
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

    - job_name: &#39;kubernetes-service-endpoints&#39;
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name

    - job_name: &#39;kubernetes-services&#39;
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module: [http_2xx]
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
        action: keep
        regex: true
      - source_labels: [__address__]
        target_label: __param_target
      - target_label: __address__
        replacement: blackbox-exporter.example.com:9115
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        target_label: kubernetes_name

    - job_name: &#39;kubernetes-ingresses&#39;
      kubernetes_sd_configs:
      - role: ingress
      relabel_configs:
      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]
        regex: (.+);(.+);(.+)
        replacement: ${1}://${2}${3}
        target_label: __param_target
      - target_label: __address__
        replacement: blackbox-exporter.example.com:9115
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_ingress_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_ingress_name]
        target_label: kubernetes_name

    - job_name: &#39;kubernetes-pods&#39;
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus.deploy.yml定义Prometheus的部署：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  labels:
    name: prometheus-deployment
  name: prometheus
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - image: harbor.frognew.com/prom/prometheus:2.0.0
        name: prometheus
        command:
        - &amp;quot;/bin/prometheus&amp;quot;
        args:
        - &amp;quot;--config.file=/etc/prometheus/prometheus.yml&amp;quot;
        - &amp;quot;--storage.tsdb.path=/prometheus&amp;quot;
        - &amp;quot;--storage.tsdb.retention=24h&amp;quot;
        ports:
        - containerPort: 9090
          protocol: TCP
        volumeMounts:
        - mountPath: &amp;quot;/prometheus&amp;quot;
          name: data
        - mountPath: &amp;quot;/etc/prometheus&amp;quot;
          name: config-volume
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
          limits:
            cpu: 500m
            memory: 2500Mi
      serviceAccountName: prometheus
      imagePullSecrets:
        - name: regsecret
      volumes:
      - name: data
        emptyDir: {}
      - name: config-volume
        configMap:
          name: prometheus-config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus.svc.yml定义Prometheus的Servic，需要将Prometheus以NodePort, LoadBalancer或使用Ingress暴露到集群外部，这样外部的Prometheus才能访问它：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: prometheus
  name: prometheus
  namespace: kube-system
spec:
  type: NodePort
  ports:
  - port: 9090
    targetPort: 9090
    nodePort: 30003
  selector:
    app: prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面就完成了prometheus在k8s的集群中部署，当然只是部署了prometheus，整体的架构还是可以参考物理架构部署&lt;/p&gt;

&lt;h3 id=&#34;部署kube-state-metrics&#34;&gt;部署kube-state-metrics&lt;/h3&gt;

&lt;p&gt;kube-state-metrics已经给出了在Kubernetes部署的manifest定义文件，直接部署，上面物理部署的时候也已经说明了&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s-base/#探针组件&#34;&gt;部署详情&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;将kube-state-metrics部署到Kubernetes上之后，就会发现Kubernetes集群中的Prometheus会在kubernetes-service-endpoints这个job下自动服务发现kube-state-metrics，并开始拉取metrics，当然集群外部的Prometheus也能从集群中的Prometheus拉取到这些数据了。这是因为上2.2中prometheus.config.yml中Prometheus的配置文件job kubernetes-service-endpoints的配置。而部署kube-state-metrics的manifest定义文件kube-state-metrics-service.yaml对kube-state-metricsService的定义包含annotation prometheus.io/scrape: &amp;lsquo;true&amp;rsquo;，因此kube-state-metrics的endpoint可以被Prometheus自动服务发现。&lt;/p&gt;

&lt;h2 id=&#34;peometheus-operator&#34;&gt;peometheus-operator&lt;/h2&gt;

&lt;p&gt;还有很多组件需要k8s部署，但是现在已经不需要这样一个个去手动写yaml文件部署，k8s推出了&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/&#34;&gt;operator的模式&lt;/a&gt;进行promehteus的部署，可以快速的部署使用。&lt;/p&gt;

&lt;h2 id=&#34;使用k8s的服务发现&#34;&gt;使用k8s的服务发现&lt;/h2&gt;

&lt;p&gt;prometheus自身提供了自动发现kubernetes的监控目标的功能，首先直接上官方的prometheus配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# A scrape configuration for running Prometheus on a Kubernetes cluster.
# This uses separate scrape configs for cluster components (i.e. API server, node)
# and services to allow each to use different authentication configs.
#
# Kubernetes labels will be added as Prometheus labels on metrics via the
# `labelmap` relabeling action.
#
# If you are using Kubernetes 1.7.2 or earlier, please take note of the comments
# for the kubernetes-cadvisor job; you will need to edit or remove this job.

# Scrape config for API servers.
#
# Kubernetes exposes API servers as endpoints to the default/kubernetes
# service so this uses `endpoints` role and uses relabelling to only keep
# the endpoints associated with the default/kubernetes service using the
# default named port `https`. This works for single API server deployments as
# well as HA API server deployments.
scrape_configs:
- job_name: &#39;kubernetes-apiservers&#39;

  kubernetes_sd_configs:
  - role: endpoints

  # Default to scraping over https. If required, just disable this or change to
  # `http`.
  scheme: https

  # This TLS &amp;amp; bearer token file config is used to connect to the actual scrape
  # endpoints for cluster components. This is separate to discovery auth
  # configuration because discovery &amp;amp; scraping are two separate concerns in
  # Prometheus. The discovery auth config is automatic if Prometheus runs inside
  # the cluster. Otherwise, more config options have to be provided within the
  # &amp;lt;kubernetes_sd_config&amp;gt;.
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    # If your node certificates are self-signed or use a different CA to the
    # master CA, then disable certificate verification below. Note that
    # certificate verification is an integral part of a secure infrastructure
    # so this should only be disabled in a controlled environment. You can
    # disable certificate verification by uncommenting the line below.
    #
    # insecure_skip_verify: true
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

  # Keep only the default/kubernetes service endpoints for the https port. This
  # will add targets for each API server which Kubernetes adds an endpoint to
  # the default/kubernetes service.
  relabel_configs:
  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
    action: keep
    regex: default;kubernetes;https

# Scrape config for nodes (kubelet).
#
# Rather than connecting directly to the node, the scrape is proxied though the
# Kubernetes apiserver.  This means it will work if Prometheus is running out of
# cluster, or can&#39;t connect to nodes for some other reason (e.g. because of
# firewalling).
- job_name: &#39;kubernetes-nodes&#39;

  # Default to scraping over https. If required, just disable this or change to
  # `http`.
  scheme: https

  # This TLS &amp;amp; bearer token file config is used to connect to the actual scrape
  # endpoints for cluster components. This is separate to discovery auth
  # configuration because discovery &amp;amp; scraping are two separate concerns in
  # Prometheus. The discovery auth config is automatic if Prometheus runs inside
  # the cluster. Otherwise, more config options have to be provided within the
  # &amp;lt;kubernetes_sd_config&amp;gt;.
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

  kubernetes_sd_configs:
  - role: node

  relabel_configs:
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)
  - target_label: __address__
    replacement: kubernetes.default.svc:443
  - source_labels: [__meta_kubernetes_node_name]
    regex: (.+)
    target_label: __metrics_path__
    replacement: /api/v1/nodes/${1}/proxy/metrics

# Scrape config for Kubelet cAdvisor.
#
# This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
# (those whose names begin with &#39;container_&#39;) have been removed from the
# Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
# retrieve those metrics.
#
# In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
# HTTP endpoint; use &amp;quot;replacement: /api/v1/nodes/${1}:4194/proxy/metrics&amp;quot;
# in that case (and ensure cAdvisor&#39;s HTTP server hasn&#39;t been disabled with
# the --cadvisor-port=0 Kubelet flag).
#
# This job is not necessary and should be removed in Kubernetes 1.6 and
# earlier versions, or it will cause the metrics to be scraped twice.
- job_name: &#39;kubernetes-cadvisor&#39;

  # Default to scraping over https. If required, just disable this or change to
  # `http`.
  scheme: https

  # This TLS &amp;amp; bearer token file config is used to connect to the actual scrape
  # endpoints for cluster components. This is separate to discovery auth
  # configuration because discovery &amp;amp; scraping are two separate concerns in
  # Prometheus. The discovery auth config is automatic if Prometheus runs inside
  # the cluster. Otherwise, more config options have to be provided within the
  # &amp;lt;kubernetes_sd_config&amp;gt;.
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

  kubernetes_sd_configs:
  - role: node

  relabel_configs:
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)
  - target_label: __address__
    replacement: kubernetes.default.svc:443
  - source_labels: [__meta_kubernetes_node_name]
    regex: (.+)
    target_label: __metrics_path__
    replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

# Scrape config for service endpoints.
#
# The relabeling allows the actual service scrape endpoint to be configured
# via the following annotations:
#
# * `prometheus.io/scrape`: Only scrape services that have a value of `true`
# * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
# to set this to `https` &amp;amp; most likely set the `tls_config` of the scrape config.
# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
# * `prometheus.io/port`: If the metrics are exposed on a different port to the
# service then set this appropriately.
- job_name: &#39;kubernetes-service-endpoints&#39;

  kubernetes_sd_configs:
  - role: endpoints

  relabel_configs:
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
    action: replace
    target_label: __scheme__
    regex: (https?)
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
    action: replace
    target_label: __metrics_path__
    regex: (.+)
  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
    action: replace
    target_label: __address__
    regex: ([^:]+)(?::\d+)?;(\d+)
    replacement: $1:$2
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    action: replace
    target_label: kubernetes_name

# Example scrape config for probing services via the Blackbox Exporter.
#
# The relabeling allows the actual service scrape endpoint to be configured
# via the following annotations:
#
# * `prometheus.io/probe`: Only probe services that have a value of `true`
- job_name: &#39;kubernetes-services&#39;

  metrics_path: /probe
  params:
    module: [http_2xx]

  kubernetes_sd_configs:
  - role: service

  relabel_configs:
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
    action: keep
    regex: true
  - source_labels: [__address__]
    target_label: __param_target
  - target_label: __address__
    replacement: blackbox-exporter.example.com:9115
  - source_labels: [__param_target]
    target_label: instance
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    target_label: kubernetes_name

# Example scrape config for probing ingresses via the Blackbox Exporter.
#
# The relabeling allows the actual ingress scrape endpoint to be configured
# via the following annotations:
#
# * `prometheus.io/probe`: Only probe services that have a value of `true`
- job_name: &#39;kubernetes-ingresses&#39;

  metrics_path: /probe
  params:
    module: [http_2xx]

  kubernetes_sd_configs:
    - role: ingress

  relabel_configs:
    - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]
      regex: (.+);(.+);(.+)
      replacement: ${1}://${2}${3}
      target_label: __param_target
    - target_label: __address__
      replacement: blackbox-exporter.example.com:9115
    - source_labels: [__param_target]
      target_label: instance
    - action: labelmap
      regex: __meta_kubernetes_ingress_label_(.+)
    - source_labels: [__meta_kubernetes_namespace]
      target_label: kubernetes_namespace
    - source_labels: [__meta_kubernetes_ingress_name]
      target_label: kubernetes_name

# Example scrape config for pods
#
# The relabeling allows the actual pod scrape endpoint to be configured via the
# following annotations:
#
# * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
# * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
# pod&#39;s declared ports (default is a port-free target if none are declared).
- job_name: &#39;kubernetes-pods&#39;

  kubernetes_sd_configs:
  - role: pod

  relabel_configs:
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
    action: replace
    target_label: __metrics_path__
    regex: (.+)
  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
    action: replace
    regex: ([^:]+)(?::\d+)?;(\d+)
    replacement: $1:$2
    target_label: __address__
  - action: labelmap
    regex: __meta_kubernetes_pod_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_pod_name]
    action: replace
    target_label: kubernetes_pod_name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然该配置文件，是在prometheus部署在k8s中生效的,即in-cluster模式。下面我们详细说明一下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;&amp;lt;scrape_config&amp;gt;&lt;/code&gt;:定义收集规则。 在一般情况下，一个scrape配置指定一个job。 在高级配置中，这可能会改变。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;relabel_configs允许在抓取之前对任何目标及其标签进行修改。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-apiservers&lt;/p&gt;

&lt;p&gt;该项主要是让prometheus程序可以访问kube-apiserver，进而进行服务发现。看一下服务发现的代码可以看出，主要服务发现：node，service，ingress，pod。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;switch d.role {
case &amp;quot;endpoints&amp;quot;:
    var wg sync.WaitGroup

    for _, namespace := range namespaces {
        elw := cache.NewListWatchFromClient(rclient, &amp;quot;endpoints&amp;quot;, namespace, nil)
        slw := cache.NewListWatchFromClient(rclient, &amp;quot;services&amp;quot;, namespace, nil)
        plw := cache.NewListWatchFromClient(rclient, &amp;quot;pods&amp;quot;, namespace, nil)
        eps := NewEndpoints(
            log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;endpoint&amp;quot;),
            cache.NewSharedInformer(slw, &amp;amp;apiv1.Service{}, resyncPeriod),
            cache.NewSharedInformer(elw, &amp;amp;apiv1.Endpoints{}, resyncPeriod),
            cache.NewSharedInformer(plw, &amp;amp;apiv1.Pod{}, resyncPeriod),
        )
        go eps.endpointsInf.Run(ctx.Done())
        go eps.serviceInf.Run(ctx.Done())
        go eps.podInf.Run(ctx.Done())

        for !eps.serviceInf.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        for !eps.endpointsInf.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        for !eps.podInf.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        wg.Add(1)
        go func() {
            defer wg.Done()
            eps.Run(ctx, ch)
        }()
    }
    wg.Wait()
case &amp;quot;pod&amp;quot;:
    var wg sync.WaitGroup
    for _, namespace := range namespaces {
        plw := cache.NewListWatchFromClient(rclient, &amp;quot;pods&amp;quot;, namespace, nil)
        pod := NewPod(
            log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;pod&amp;quot;),
            cache.NewSharedInformer(plw, &amp;amp;apiv1.Pod{}, resyncPeriod),
        )
        go pod.informer.Run(ctx.Done())

        for !pod.informer.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        wg.Add(1)
        go func() {
            defer wg.Done()
            pod.Run(ctx, ch)
        }()
    }
    wg.Wait()
case &amp;quot;service&amp;quot;:
    var wg sync.WaitGroup
    for _, namespace := range namespaces {
        slw := cache.NewListWatchFromClient(rclient, &amp;quot;services&amp;quot;, namespace, nil)
        svc := NewService(
            log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;service&amp;quot;),
            cache.NewSharedInformer(slw, &amp;amp;apiv1.Service{}, resyncPeriod),
        )
        go svc.informer.Run(ctx.Done())

        for !svc.informer.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        wg.Add(1)
        go func() {
            defer wg.Done()
            svc.Run(ctx, ch)
        }()
    }
    wg.Wait()
case &amp;quot;ingress&amp;quot;:
    var wg sync.WaitGroup
    for _, namespace := range namespaces {
        ilw := cache.NewListWatchFromClient(reclient, &amp;quot;ingresses&amp;quot;, namespace, nil)
        ingress := NewIngress(
            log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;ingress&amp;quot;),
            cache.NewSharedInformer(ilw, &amp;amp;extensionsv1beta1.Ingress{}, resyncPeriod),
        )
        go ingress.informer.Run(ctx.Done())

        for !ingress.informer.HasSynced() {
            time.Sleep(100 * time.Millisecond)
        }
        wg.Add(1)
        go func() {
            defer wg.Done()
            ingress.Run(ctx, ch)
        }()
    }
    wg.Wait()
case &amp;quot;node&amp;quot;:
    nlw := cache.NewListWatchFromClient(rclient, &amp;quot;nodes&amp;quot;, api.NamespaceAll, nil)
    node := NewNode(
        log.With(d.logger, &amp;quot;role&amp;quot;, &amp;quot;node&amp;quot;),
        cache.NewSharedInformer(nlw, &amp;amp;apiv1.Node{}, resyncPeriod),
    )
    go node.informer.Run(ctx.Done())

    for !node.informer.HasSynced() {
        time.Sleep(100 * time.Millisecond)
    }
    node.Run(ctx, ch)

default:
    level.Error(d.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;unknown Kubernetes discovery kind&amp;quot;, &amp;quot;role&amp;quot;, d.role)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-nodes&lt;/p&gt;

&lt;p&gt;发现node以后，通过/api/v1/nodes/${1}/proxy/metrics来获取node的metrics。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-cadvisor&lt;/p&gt;

&lt;p&gt;cadvisor已经被集成在kubelet中，所以发现了node就相当于发现了cadvisor。通过 /api/v1/nodes/${1}/proxy/metrics/cadvisor采集容器指标。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-services和kubernetes-ingresses&lt;/p&gt;

&lt;p&gt;该两种资源监控方式差不多，都是需要安装black-box，然后类似于探针去定时访问，根据返回的http状态码来判定service和ingress的服务可用性。&lt;/p&gt;

&lt;p&gt;PS：不过我自己在这里和官方的稍微有点区别，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- target_label: __address__
  replacement: blackbox-exporter.example.com:9115
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;官方大致是需要我们要创建black-box 的ingress从外部访问，这样从效率和安全性都不是最合适的。所以我一般都是直接内部dns访问。如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- target_label: __address__
  replacement: blackbox-exporter.kube-system:9115
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然看源码可以发现，并不是所有的service和ingress都会健康监测，如果需要将服务进行健康监测，那么你部署应用的yaml文件加一些注解。例如：&lt;/p&gt;

&lt;p&gt;对于service和ingress：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;需要加注解：prometheus.io/scrape: &#39;true&#39;

apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: &#39;true&#39;
  name: prometheus-node-exporter
  namespace: kube-system
  labels:
    app: prometheus
    component: node-exporter
spec:
  clusterIP: None
  ports:
    - name: prometheus-node-exporter
      port: 9100
      protocol: TCP
  selector:
    app: prometheus
    component: node-exporter
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-pods&lt;/p&gt;

&lt;p&gt;对于pod的监测也是需要加注解：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus.io/scrape，为true则会将pod作为监控目标。
prometheus.io/path，默认为/metrics
prometheus.io/port , 端口
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以看到此处可以看出，该job并不是监控pod的指标，pod已经通过前面的cadvisor采集。此处是对pod中应用的监控。写过exporter的人应该对这个概念非常清楚。通俗讲，就是你pod中的应用提供了prometheus的监控功能，加上对应的注解，那么该应用的metrics会定时被采集走。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubernetes-service-endpoints&lt;/p&gt;

&lt;p&gt;对于服务的终端节点，也需要加注解：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus.io/scrape，为true则会将pod作为监控目标。
prometheus.io/path，默认为/metrics
prometheus.io/port , 端口
prometheus.io/scheme 默认http，如果为了安全设置了https，此处需要改为https
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个基本上同上的。采集service-endpoints的metrics。&lt;/p&gt;

&lt;p&gt;个人认为：如果某些部署应用只有pod没有service，那么这种情况只能在pod上加注解，通过kubernetes-pods采集metrics。如果有service，那么就无需在pod加注解了，直接在service上加即可。毕竟service-endpoints最终也会落到pod上。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;配置项总结&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kubernetes-service-endpoints和kubernetes-pods采集应用中metrics，当然并不是所有的都提供了metrics接口。&lt;/li&gt;
&lt;li&gt;kubernetes-ingresses 和kubernetes-services 健康监测服务和ingress健康的状态&lt;/li&gt;
&lt;li&gt;kubernetes-cadvisor 和 kubernetes-nodes，通过发现node，监控node 和容器的cpu等指标&lt;/li&gt;
&lt;li&gt;自动发现源码，可以参考client-go和prometheus自动发现k8s，这种监听k8s集群中资源的变化，使用informer实现，不要轮询kube-apiserver接口。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;物理部署和k8s的区别和结合&#34;&gt;物理部署和k8s的区别和结合&lt;/h1&gt;

&lt;p&gt;物理部署下的consul走的文件服务发现和k8s的服务发现完全是两种方式，在小规模的情况下使用k8s确实方便，但是在规模较大的情况，就需要分片，目前thanos也是支持k8s，所以应该也是可以分片的，也可以将数据聚合到vm中去。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Mq Compare</title>
          <link>https://kingjcy.github.io/post/middleware/mq/mq-compare/</link>
          <pubDate>Sat, 21 Apr 2018 09:54:56 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/mq-compare/</guid>
          <description>&lt;p&gt;消息队列（MQ）是一种不同应用程序之间(跨进程)的通信方法，用于上下游应用程序之间传递消息。&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;消息队列（MQ）我们拆分来看：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;消息：应用程序通过写入和检索出入列队的数据（消息）来通信。&lt;/li&gt;
&lt;li&gt;队列：除去了接收和发送应用程序同时执行的要求。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样就实现了上游与下游之间的解耦，上游向MQ发送消息，下游从MQ接收消息，上游下游互不依赖，它们只依赖MQ。因为有队列的存在，MQ可在上下游之间进行缓冲，把上游信息先缓存起来，下游根据自己的能力从MQ中拉去信息，起到削峰的作用。&lt;/p&gt;

&lt;p&gt;所有的MQ的基本逻辑架构都是如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/mq&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们在设计和实现mq的时候主要要考虑以下的因素&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/mq2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;主要的使用场景&lt;/p&gt;

&lt;p&gt;可以使用MQ的场景有很多，最常用是业务解耦/最终一致性/广播/错峰削峰流控等。反之，如果需要强一致性，关注业务逻辑的处理结果，则RPC显得更为合适。主要是用来解决异步处理的耗时操作，否则就会增加系统的负载。&lt;/p&gt;

&lt;h1 id=&#34;对比&#34;&gt;对比&lt;/h1&gt;

&lt;p&gt;目前mq的相关产品可以说是百花齐放，有&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/activemq/&#34;&gt;activemq&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/kafka/&#34;&gt;kafka&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/nsq/&#34;&gt;nsq&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/rabbitmq/&#34;&gt;rabbitmq&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/rocketmq/&#34;&gt;rocketmq&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/emq/&#34;&gt;emq&lt;/a&gt;等，我们对其在支持的功能结合上面设计mq的因素来做一个简单的对比(图片来自网络)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/20180421.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/mq3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见目前比较性能好，使用比较多的就是kafka和阿里云的rocketmq。&lt;/p&gt;

&lt;p&gt;这边提一下redis，首先Redis的设计是用来做缓存的，但是由于它自身的某种特性使得他可以用来做消息队列(Redis的List数据结构比较适合做MQ)。它有几个阻塞式的API可以使用，正是这些阻塞式的API让他有做消息队列的能力。 另外做消息队列的其他特性，例如FIFO也很容易实现，只需要一个list对象从头取数据，从尾部塞数据即可实现。 Redis能做消息队列得益于它的list对象blpop brpop接口以及Pub/Sub(发布/订阅)的某些接口。他们都是阻塞版的，所以可以用来做消息队列。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Rabbitmq</title>
          <link>https://kingjcy.github.io/post/middleware/mq/rabbitmq/</link>
          <pubDate>Tue, 20 Mar 2018 19:27:58 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/rabbitmq/</guid>
          <description>&lt;p&gt;RabbitMQ是一个在AMQP（Advanced Message Queuing Protocol ）基础上实现的，由Erlang开发，可复用的企业消息系统。它可以用于大型软件系统各个模块之间的高效通信，支持高并发，支持可扩展。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;1、Broker:消息队列服务器实体&lt;/p&gt;

&lt;p&gt;2、messages（消息）：每个消息都有一个路由键(routing key)的属性。就是一个简单的字符串。&lt;/p&gt;

&lt;p&gt;3、connection：应用程序与broker的网络连接。&lt;/p&gt;

&lt;p&gt;4、channel:几乎所有的操作都在channel中进行，channel是进行消息读写的通道。客户端可建立多个channel，每个channel代表一个会话任务。&lt;/p&gt;

&lt;p&gt;5、exchange（交换机）：消息交换机，它指定消息按什么规则，路由到哪个队列。&lt;/p&gt;

&lt;p&gt;6、Routing Key：路由关键字，exchange根据这个关键字进行消息投递&lt;/p&gt;

&lt;p&gt;7、binding（绑定）：它的作用就是把exchange和queue按照路由规则绑定起来。一个绑定就是基于路由键将交换机和队列连接起来的路由规则，所以交换机不过就是一个由绑定构成的路由表。&lt;/p&gt;

&lt;p&gt;比如一个具有路由键“key1”的消息要发送到两个队列，queueA和queueB。要做到这点就要建立两个绑定，每个绑定连接一个交换机和一个队列。两者都是由路由键“key1”触发，这种情况，交换机会复制一份消息并把它们分别发送到两个队列中。&lt;/p&gt;

&lt;p&gt;8、queues（队列）：消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。&lt;/p&gt;

&lt;p&gt;9、vhost：虚拟主机，一个broker里可以开设多个vhost，用作不同用户的权限分离。&lt;/p&gt;

&lt;p&gt;10、producer：消息生产者，就是投递消息的程序。&lt;/p&gt;

&lt;p&gt;11、consumer：消息消费者，就是接受消息的程序。&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;先看rabbitmq的一个使用组件流程图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/rabbitmq/20180320.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;AMQP模型中，消息在producer中产生，发送到MQ的exchange上，exchange根据配置的路由方式发到相应的Queue上，Queue又将消息发送给consumer，消息从queue到consumer有push和pull两种方式。&lt;/p&gt;

&lt;p&gt;整理大概如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;客户端连接到消息队列服务器，打开一个channel。&lt;/li&gt;
&lt;li&gt;客户端声明一个exchange，并设置相关属性。&lt;/li&gt;
&lt;li&gt;客户端声明一个queue，并设置相关属性。&lt;/li&gt;
&lt;li&gt;客户端使用routing key，在exchange和queue之间建立好绑定关系。&lt;/li&gt;
&lt;li&gt;客户端投递消息到exchange。&lt;/li&gt;
&lt;li&gt;exchange接收到消息后，就根据消息的key和已经设置的binding，进行消息路由，将消息投递到一个或多个队列里。exchange也有几个类型，完全根据key进行投递的叫做Direct交换机，例如，绑定时设置了routing key为”abc”，那么客户端提交的消息，只有设置了key为”abc”的才会投递到队列。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;rabbitmq的核心就是exchange构成的路由规则和queues组成的队列，下面我们核心讲解一下这两个&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;exchange（交换机）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;交换机基本有3种类型：direct，topic，fanout。&lt;/p&gt;

&lt;p&gt;为什么不创建一种交换机来处理所有类型的路由规则？因为每种规则匹配时的CPU开销是不同的，所以根据不同需求选择合适交换机。&lt;/p&gt;

&lt;p&gt;举例：一个&amp;rdquo;topic&amp;rdquo;类型的交换机会将消息的路由键与类似“dog.*”的模式进行匹配。一个“direct”类型的交换机会将路由键与“dogs”进行比较。匹配末端通配符比直接比较消耗更多的cpu,所以如果用不到“topic”类型交换机带来的灵活性，就通过“direct”类型交换机获得更高的处理效率。&lt;/p&gt;

&lt;p&gt;1、Direct交换机：转发消息到routingKey指定队列（完全匹配，单播）。&lt;/p&gt;

&lt;p&gt;routingKey与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发routingkey标记为dog的消息，不会转发dog.puppy，也不会转发dog.guard等。&lt;/p&gt;

&lt;p&gt;2、Topic交换机：按规则转发消息（最灵活，组播）&lt;/p&gt;

&lt;p&gt;Topic类型交换机通过模式匹配分配消息的routing-key属性。将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。&lt;/p&gt;

&lt;p&gt;它将routing-key和binding-key的字符串切分成单词。这些单词之间用点隔开。它同样也会识别两个通配符：符号“#”和符号“*”。#匹配0个或多个单词，*匹配不多不少一个单词。&lt;/p&gt;

&lt;p&gt;例如，binding key:*.stock.#匹配routing key: usd.stock和eur.stock.db，但是不匹配stock.nana。&lt;/p&gt;

&lt;p&gt;例如，“audit.#”能够匹配到“audit.irs.corporate”，但是“audit.*”只会匹配到“audit.irs”。&lt;/p&gt;

&lt;p&gt;3、Fanout交换机：转发消息到所有绑定队列（最快，广播）&lt;/p&gt;

&lt;p&gt;fanout交换机不处理路由键，简单的将队列绑定到交换机上，每个发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。&lt;/p&gt;

&lt;p&gt;很像子网广播，每台子网内的主机都获得了一份复制的消息。Fanout交换机转发消息是最快的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果没有队列绑定在交换机上，则发送到该交换机上的消息会丢失。&lt;/p&gt;

&lt;p&gt;一个交换机可以绑定多个队列，一个队列可以被多个交换机绑定。&lt;/p&gt;

&lt;p&gt;因为交换机是命名实体，声明一个已经存在的交换机，但是试图赋予不同类型是会导致错误。客户端需要删除这个已经存在的交换机，然后重新声明并且赋予新的类型。&lt;/p&gt;

&lt;p&gt;交换机的属性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;持久性：如果启用，交换机将会在server重启前都有效。&lt;/li&gt;
&lt;li&gt;自动删除：如果启用，那么交换机将会在其绑定的队列都被删掉之后删除自身。&lt;/li&gt;
&lt;li&gt;惰性:如果没有声明交换机，那么在执行到使用的时候会导致异常，并不会主动声明。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;queues（队列）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;队列的属性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;持久性：如果启用，队列将在Server服务重启前都有效。&lt;/li&gt;
&lt;li&gt;自动删除：如果启用，那么队列将会在所有的消费者停止使用之后自动删除自身。&lt;/li&gt;
&lt;li&gt;惰性：如果没有声明队列，那么在执行到使用的时候会导致异常，并不会主动声明。&lt;/li&gt;
&lt;li&gt;排他性：如果启用，队列只能被声明它的消费者使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;部署使用&#34;&gt;部署使用&lt;/h1&gt;

&lt;p&gt;1、启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmq-server &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、队列重置（清空队列、用户等）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl stop_app
rabbitmqctl reset
rabbitmqctl stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、关闭&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、列举出所有用户&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl list_users
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5、列举出所有队列&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl list_queues
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、添加用户&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl add_user user_name user_passwd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7、设置用户角色为管理员&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl set_user_tags user administrator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8、权限设置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl set_permissions -p / user &amp;quot;.*&amp;quot; &amp;quot;.*&amp;quot; &amp;quot;.*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用户和权限设置&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;添加用户：rabbitmqctl add_user username password&lt;/li&gt;
&lt;li&gt;分配角色：rabbitmqctl set_user_tags username administrator&lt;/li&gt;
&lt;li&gt;新增虚拟主机：rabbitmqctl add_vhost  vhost_name&lt;/li&gt;
&lt;li&gt;将新虚拟主机授权给新用户：rabbitmqctl set_permissions -p vhost_name username “.*” “.*” “.*”(后面三个”*”代表用户拥有配置、写、读全部权限)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;角色说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;超级管理员(administrator)
可登陆管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。&lt;/li&gt;
&lt;li&gt;监控者(monitoring)
可登陆管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等)&lt;/li&gt;
&lt;li&gt;策略制定者(policymaker)
可登陆管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。&lt;/li&gt;
&lt;li&gt;普通管理者(management)
仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。&lt;/li&gt;
&lt;li&gt;其他
无法登陆管理控制台，通常就是普通的生产者和消费者。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;9、查看状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmqctl status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10、安装RabbitMQWeb管理插件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rabbitmq-plugins enable rabbitmq_management 
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Activemq</title>
          <link>https://kingjcy.github.io/post/middleware/mq/activemq/</link>
          <pubDate>Mon, 19 Mar 2018 19:54:48 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/activemq/</guid>
          <description>&lt;p&gt;ActiveMQ是Apache软件基金下的一个开源软件，它遵循JMS规范（Java Message Service），是消息驱动中间件软件（MOM）。它为企业消息传递提供高可用，出色性能，可扩展，稳定和安全保障。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;1、Broker，消息代理，表示消息队列服务器实体，接受客户端连接，提供消息通信的核心服务。&lt;/p&gt;

&lt;p&gt;2、Producer，消息生产者，业务的发起方，负责生产消息并传输给 Broker 。&lt;/p&gt;

&lt;p&gt;3、Consumer，消息消费者，业务的处理方，负责从 Broker 获取消息并进行业务逻辑处理。&lt;/p&gt;

&lt;p&gt;4、Topic，主题，发布订阅模式下的消息统一汇集地，不同生产者向 Topic 发送消息，由 Broker 分发到不同的订阅者，实现消息的广播。&lt;/p&gt;

&lt;p&gt;5、Queue，队列，点对点模式下特定生产者向特定队列发送消息，消费者订阅特定队列接收消息并进行业务逻辑处理。&lt;/p&gt;

&lt;p&gt;6、Message，消息体，根据不同通信协议定义的固定格式进行编码的数据包，来封装业务 数据，实现消息的传输。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;主要特点&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、多种语言和协议编写客户端。语言: Java, C, C++, C#, Ruby, Perl, Python, PHP。应用协议: OpenWire,Stomp REST,WSNotification,XMPP,AMQP&lt;/p&gt;

&lt;p&gt;2、完全支持JMS1.1和J2EE 1.4规范 (持久化,XA消息,事务)&lt;/p&gt;

&lt;p&gt;3、对Spring的支持,ActiveMQ可以很容易内嵌到使用Spring的系统里面去,而且也支持Spring2.0的特性&lt;/p&gt;

&lt;p&gt;4、通过了常见J2EE服务器(如 Geronimo,JBoss 4,GlassFish,WebLogic)的测试,其中通过JCA 1.5resource adaptors的配置,可以让ActiveMQ可以自动的部署到任何兼容J2EE 1.4 商业服务器上&lt;/p&gt;

&lt;p&gt;5、支持多种传送协议:in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA&lt;/p&gt;

&lt;p&gt;6、支持通过JDBC和journal提供高速的消息持久化&lt;/p&gt;

&lt;p&gt;7、从设计上保证了高性能的集群,客户端-服务器,点对点&lt;/p&gt;

&lt;p&gt;8、支持Ajax&lt;/p&gt;

&lt;p&gt;9、支持与Axis的整合&lt;/p&gt;

&lt;p&gt;10、可以很容易得调用内嵌JMS provider,进行测试&lt;/p&gt;

&lt;p&gt;因为java编写可见对java使用还是很友好的。&lt;/p&gt;

&lt;h1 id=&#34;activemq的安装&#34;&gt;ActiveMQ的安装&lt;/h1&gt;

&lt;p&gt;进入&lt;a href=&#34;http://activemq.apache.org/&#34;&gt;http://activemq.apache.org/&lt;/a&gt; 下载ActiveMQ。&lt;/p&gt;

&lt;p&gt;安装环境：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、需要jdk
2、安装Linux系统。生产环境都是Linux系统。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装步骤&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;第一步： 把ActiveMQ 的压缩包上传到Linux系统。
第二步：解压缩。
第三步：启动。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用bin目录下的activemq命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;启动：[root@localhost bin]# ./activemq start

关闭：[root@localhost bin]# ./activemq stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost bin]# ./activemq status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入管理后台：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://192.168.25.168:8161/admin

用户名：admin
密码：admin
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;activemq类别及开发流程&#34;&gt;ActiveMQ类别及开发流程&lt;/h1&gt;

&lt;p&gt;1、Point-to-Point (点对点)消息模式开发流程 ：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;生产者（producer）开发流程（ProducerTool.java）： 

&lt;ul&gt;
&lt;li&gt;创建Connection： 根据url，user和password创建一个jms Connection。 &lt;/li&gt;
&lt;li&gt;创建Session： 在connection的基础上创建一个session，同时设置是否支持事务和ACKNOWLEDGE标识。 &lt;/li&gt;
&lt;li&gt;创建Destination对象： 需指定其对应的主题（subject）名称，producer和consumer将根据subject来发送/接收对应的消息。 &lt;/li&gt;
&lt;li&gt;创建MessageProducer： 根据Destination创建MessageProducer对象，同时设置其持久模式。 &lt;/li&gt;
&lt;li&gt;发送消息到队列（Queue）： 封装TextMessage消息，使用MessageProducer的send方法将消息发送出去。 &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;消费者（consumer）开发流程（ConsumerTool.java）： 

&lt;ul&gt;
&lt;li&gt;实现MessageListener接口： 消费者类必须实现MessageListener接口，然后在onMessage()方法中监听消息的到达并处理。 &lt;/li&gt;
&lt;li&gt;创建Connection： 根据url，user和password创建一个jms Connection，如果是durable模式，还需要给connection设置一个clientId。 
- 创建Session和Destination： 与ProducerTool.java中的流程类似，不再赘述。 &lt;/li&gt;
&lt;li&gt;创建replyProducer【可选】：可以用来将消息处理结果发送给producer。 &lt;/li&gt;
&lt;li&gt;创建MessageConsumer：  根据Destination创建MessageConsumer对象。 &lt;/li&gt;
&lt;li&gt;消费message：  在onMessage()方法中接收producer发送过来的消息进行处理，并可以通过replyProducer反馈信息给producer &lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、当然还是支持发布／订阅的广播模式。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Producer&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建ConnectionFactory对象，需要指定服务端ip及端口号。&lt;/li&gt;
&lt;li&gt;使用ConnectionFactory对象创建一个Connection对象。&lt;/li&gt;
&lt;li&gt;开启连接，调用Connection对象的start方法。&lt;/li&gt;
&lt;li&gt;使用Connection对象创建一个Session对象。&lt;/li&gt;
&lt;li&gt;使用Session对象创建一个Destination对象（topic、queue），此处创建一个Topic对象。&lt;/li&gt;
&lt;li&gt;使用Session对象创建一个Producer对象。&lt;/li&gt;
&lt;li&gt;创建一个Message对象，创建一个TextMessage对象。&lt;/li&gt;
&lt;li&gt;使用Producer对象发送消息。&lt;/li&gt;
&lt;li&gt;关闭资源。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Consumer消费者：接收消息。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建一个ConnectionFactory对象。&lt;/li&gt;
&lt;li&gt;从ConnectionFactory对象中获得一个Connection对象。&lt;/li&gt;
&lt;li&gt;开启连接。调用Connection对象的start方法。&lt;/li&gt;
&lt;li&gt;使用Connection对象创建一个Session对象。&lt;/li&gt;
&lt;li&gt;使用Session对象创建一个Destination对象。和发送端保持一致topic，并且话题的名称一致。&lt;/li&gt;
&lt;li&gt;使用Session对象创建一个Consumer对象。&lt;/li&gt;
&lt;li&gt;接收消息。&lt;/li&gt;
&lt;li&gt;打印消息。&lt;/li&gt;
&lt;li&gt;关闭资源&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基本就是这两种使用方式。我们可以使用golang来编写生产者和消费者，使用现成的库，然后当着一个mq来传递数据即可。这也是我们最常使用，跨语言进行使用。使得业务和技术实现解耦，也是我们所说的中间件的概念。这样可以跨语言来使用优秀的开源产品，只要知道原理，使用方法，就可以直接使用。其他的mq比如kafka，也是这么个使用道理。所以要理解架构才是道理。&lt;/p&gt;

&lt;h1 id=&#34;性能&#34;&gt;性能&lt;/h1&gt;

&lt;p&gt;ActiveMQ，在赛扬（2.40GHz）机器上能够达到2000/s，消息大小为1-2k。好一些的服务器可以达到2万以上/秒。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Node Exporter</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/node-exporter/</link>
          <pubDate>Mon, 19 Mar 2018 16:51:51 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/exporter/node-exporter/</guid>
          <description>&lt;p&gt;node_exporter 主要用于 LINUX 系统监控, 用 Golang 编写，是我们最常用于监控服务器资源的探针。&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;1、二进制&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/prometheus/node_exporter/releases/download/v0.14.0/node_exporter-0.14.0.linux-amd64.tar.gz&#34;&gt;https://github.com/prometheus/node_exporter/releases/download/v0.14.0/node_exporter-0.14.0.linux-amd64.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;我们可以直接使用 ./node_exporter -h 查看运行选项，./node_exporter 运行 Node Exporter&lt;/p&gt;

&lt;p&gt;2、docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -p 9100:9100 \
  -v &amp;quot;/proc:/host/proc:ro&amp;quot; \
  -v &amp;quot;/sys:/host/sys:ro&amp;quot; \
  -v &amp;quot;/:/rootfs:ro&amp;quot; \
  --net=&amp;quot;host&amp;quot; \
  quay.io/prometheus/node-exporter \
    -collector.procfs /host/proc \
    -collector.sysfs /host/sys \
    -collector.filesystem.ignored-mount-points &amp;quot;^/(sys|proc|dev|host|etc)($|/)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;源码原理解析&#34;&gt;源码原理解析&lt;/h1&gt;

&lt;p&gt;node探针进程启动的时候，会调用collector的package，就会初始化所有的collect注册到&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;factories      = make(map[string]func() (Collector, error))
collectorState = make(map[string]*bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个中，根据初始化中传递的参数，确定采集是否开启，处理函数是什么&lt;/p&gt;

&lt;p&gt;然后会创建一个结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type nodeCollector struct {
    Collectors map[string]Collector
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将开启采集的数据，创建对应的结构体，其实也是一个实现interface的结构，最后存储在这个结构中&lt;/p&gt;

&lt;p&gt;然后将这些collector进行注册，启动监听端口和一些其他设置&lt;/p&gt;

&lt;p&gt;然后就是调用client_golang中的describe和collect的重写函数&lt;/p&gt;

&lt;p&gt;在collect中启动多协程进行每个类型的处理&lt;/p&gt;

&lt;p&gt;调用update函数实现数据赋值与采集&lt;/p&gt;

&lt;h1 id=&#34;开启默认关闭的采集&#34;&gt;开启默认关闭的采集&lt;/h1&gt;

&lt;p&gt;我们可以使用 &amp;ndash;collectors.enabled 运行参数指定 node_exporter 收集的功能模块, 如果不指定，将使用默认模块。&lt;/p&gt;

&lt;p&gt;比如开启ntp采集&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./node_exporter --collector.ntp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出相关指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# HELP node_ntp_leap NTPD leap second indicator, 2 bits.
# TYPE node_ntp_leap gauge
node_ntp_leap 3
# HELP node_ntp_offset_seconds ClockOffset between NTP and local clock.
# TYPE node_ntp_offset_seconds gauge
node_ntp_offset_seconds -1.6239e-05
# HELP node_ntp_reference_timestamp_seconds NTPD ReferenceTime, UNIX timestamp.
# TYPE node_ntp_reference_timestamp_seconds gauge
node_ntp_reference_timestamp_seconds 0
# HELP node_ntp_root_delay_seconds NTPD RootDelay.
# TYPE node_ntp_root_delay_seconds gauge
node_ntp_root_delay_seconds 0
# HELP node_ntp_root_dispersion_seconds NTPD RootDispersion.
# TYPE node_ntp_root_dispersion_seconds gauge
node_ntp_root_dispersion_seconds 0.060363769
# HELP node_ntp_rtt_seconds RTT to NTPD.
# TYPE node_ntp_rtt_seconds gauge
node_ntp_rtt_seconds 8.6646e-05
# HELP node_ntp_sanity NTPD sanity according to RFC5905 heuristics and configured limits.
# TYPE node_ntp_sanity gauge
node_ntp_sanity 0
# HELP node_ntp_stratum NTPD stratum.
# TYPE node_ntp_stratum gauge
node_ntp_stratum 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;计算表达式&#34;&gt;计算表达式&lt;/h1&gt;

&lt;p&gt;收集到 node_exporter 的数据后，我们可以使用 PromQL 进行一些业务查询和监控，下面是一些比较常见的查询&lt;/p&gt;

&lt;p&gt;CPU 各 mode 占比率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;avg by (instance, mode) (irate(node_cpu_seconds_total{instance=&amp;quot;xxx&amp;quot;}[5m])) * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cpu的使用率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;avg(irate(node_cpu_seconds_total{ip=&amp;quot;$host&amp;quot;}[1m]))by (mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node_cpu_seconds_total采集出来的是cpu使用时间的累加，在linux机器上cpu的使用就是这样记录的，所以可以使用速率的方式来求使用率，GPU貌似不是这样记录的。并不是递增的。因为可能是多核的，所以要使用avg平均出所有核。&lt;/p&gt;

&lt;p&gt;pod的cpu使用率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(rate(container_cpu_usage_seconds_total{image!=&amp;quot;&amp;quot;, container_name!=&amp;quot;POD&amp;quot;}[5m])) by (pod_name,sn_pod_ip,container_name,namespace) / sum(container_spec_cpu_quota{image!=&amp;quot;&amp;quot;, container_name!=&amp;quot;POD&amp;quot;}/container_spec_cpu_period{image!=&amp;quot;&amp;quot;, container_name!=&amp;quot;POD&amp;quot;}) by (pod_name,sn_pod_ip,container_name,namespace)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;机器平均负载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;node_load1{instance=&amp;quot;xxx&amp;quot;} // 1分钟负载
node_load5{instance=&amp;quot;xxx&amp;quot;} // 5分钟负载
node_load15{instance=&amp;quot;xxx&amp;quot;} // 15分钟负载
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内存使用率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;100 - ((node_memory_MemFree{instance=&amp;quot;xxx&amp;quot;}+node_memory_Cached{instance=&amp;quot;xxx&amp;quot;}+node_memory_Buffers{instance=&amp;quot;xxx&amp;quot;})/node_memory_MemTotal) * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;磁盘使用率&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;100 - node_filesystem_free{instance=&amp;quot;xxx&amp;quot;,fstype!~&amp;quot;rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs|udev|none|devpts|sysfs|debugfs|fuse.*&amp;quot;} / node_filesystem_size{instance=&amp;quot;xxx&amp;quot;,fstype!~&amp;quot;rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs|udev|none|devpts|sysfs|debugfs|fuse.*&amp;quot;} * 100
或者你也可以直接使用 {fstype=&amp;quot;xxx&amp;quot;} 来指定想查看的磁盘信息
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;网络 IO&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 上行带宽
sum by (instance) (irate(node_network_receive_bytes{instance=&amp;quot;xxx&amp;quot;,device!~&amp;quot;bond.*?|lo&amp;quot;}[5m])/128)

// 下行带宽
sum by (instance) (irate(node_network_transmit_bytes{instance=&amp;quot;xxx&amp;quot;,device!~&amp;quot;bond.*?|lo&amp;quot;}[5m])/128)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;网卡出/入包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 入包量
sum by (instance) (rate(node_network_receive_bytes{instance=&amp;quot;xxx&amp;quot;,device!=&amp;quot;lo&amp;quot;}[5m]))

// 出包量
sum by (instance) (rate(node_network_transmit_bytes{instance=&amp;quot;xxx&amp;quot;,device!=&amp;quot;lo&amp;quot;}[5m]))
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 微服务</title>
          <link>https://kingjcy.github.io/post/architecture/microservices/microservices/</link>
          <pubDate>Mon, 05 Mar 2018 19:11:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/microservices/microservices/</guid>
          <description>&lt;p&gt;微服务其实就是服务化的一种概念，由过去单体架构演变成分布式系统的一个产物。&lt;/p&gt;

&lt;h1 id=&#34;微服务架构演变由来&#34;&gt;微服务架构演变由来&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/architecture-change&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;微服务&#34;&gt;微服务&lt;/h1&gt;

&lt;p&gt;微服务是一种软件架构思想，它将一个大且聚合的业务项目拆解为多个小且独立的业务模块，模块即服务，各服务间使用高效的协议（protobuf、JSON 等）相互调用即是 RPC。这种拆分代码库的方式有以下特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个服务应作为小规模的、独立的业务模块在运行，类似 Unix 的 Do one thing and do it well，这就是微服务参考了unix的设计哲学，包括独立开发，自动化测试（细致的错误检查和处理）和（分布式）部署，不影响其他服务。同时也加快了开发交付周期，降低代码耦合度导致的沟通成本。&lt;/li&gt;
&lt;li&gt;从依赖库到依赖服务，增加了开发者选择的自由（语言，框架，库），提高了复用效率，只要符合服务 API 契约，开发人员可以自由选择开发技术。这就意味着开发人员可以采用新技术编写或重构服务，由于服务相对较小，所以这并不会对整体应用造成太大影响。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见微服务也是分布式系统，但是微服务注重的注册中心，和服务发现，可以实现简单的扩缩容的方式，这也是微服务的一大重大特点。也就是服务治理，其实后面出现的各大框架也是着重解决这个问题。&lt;/p&gt;

&lt;p&gt;为什么要服务化？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;随着模块越来越多，相互调用会产生很多的冗余，所以需要服务化架构（SOA）。&lt;/li&gt;
&lt;li&gt;解耦，解决冲突与臃肿。比如sql在专门的sql服务运行，统一检测修改。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;第一代微服务-服务化&#34;&gt;第一代微服务&amp;mdash;服务化&lt;/h2&gt;

&lt;p&gt;微服务落地目前存在的主要困难有如下几方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务间通信：单体应用拆分为分布式系统后，进程间的通讯机制和故障处理措施变的更加复杂。&lt;/li&gt;
&lt;li&gt;分布式事务：系统微服务化后，一个看似简单的功能，内部可能需要调用多个服务并操作多个数据库实现，服务调用的分布式事务问题变的非常突出。&lt;/li&gt;
&lt;li&gt;大规模部署：微服务数量众多，其测试、部署、监控等都变的更加困难。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;随着RPC框架的成熟，第一个问题已经逐渐得到解决。例如springcloud可以非常好的支持restful调用，dubbo可以支持多种通讯协议，包括现在k8s上istio都志在解决服务间通信。&lt;/p&gt;

&lt;p&gt;对于第三个问题，随着docker、devops技术的发展以及各公有云paas平台自动化运维工具的推出，微服务的测试、部署与运维会变得越来越容易。&lt;/p&gt;

&lt;p&gt;而对于第二个问题，现在还没有通用方案很好的解决微服务产生的事务问题。分布式事务已经成为微服务落地最大的阻碍，也是最具挑战性的一个技术难题,目前&lt;a href=&#34;https://kingjcy.github.io/post/distributed/distributed-event/&#34;&gt;主要解决方案&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;当然微服务架构的还有很多缺点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;微服务强调了服务大小，但实际上这并没有一个统一的标准：业务逻辑应该按照什么规则划分为微服务，这本身就是一个经验工程。有些开发者主张 10-100 行代码就应该建立一个微服务。虽然建立小型服务是微服务架构崇尚的，但要记住，微服务是达到目的的手段，而不是目标。微服务的目标是充分分解应用程序，以促进敏捷开发和持续集成部署。&lt;/li&gt;
&lt;li&gt;微服务的分布式特点带来的复杂性：开发人员需要基于 RPC 或者消息实现微服务之间的调用和通信，而这就使得服务之间的发现、服务调用链的跟踪和质量问题变得的相当棘手。微服务的精髓之一就是服务之间传递的是可序列化消息，而不是对象和引用，这个思想是和 DCOM 及 EJB 完全相反的。只有数据，不包含逻辑；这个设计的好处不用我多说也很好理解，参考 CSP&lt;/li&gt;
&lt;li&gt;分区的数据库体系和分布式事务：更新多个业务实体的业务交易相当普遍，不同服务可能拥有不同的数据库。CAP 原理的约束，使得我们不得不放弃传统的强一致性，而转而追求最终一致性，这个对开发人员来说是一个挑战。&lt;/li&gt;
&lt;li&gt;测试挑战：传统的单体WEB应用只需测试单一的 REST API 即可，而对微服务进行测试，需要启动它依赖的所有其他服务。这种复杂性不可低估。&lt;/li&gt;
&lt;li&gt;跨多个服务的更改：比如在传统单体应用中，若有 A、B、C 三个服务需要更改，A 依赖 B，B 依赖 C。我们只需更改相应的模块，然后一次性部署即可。但是在微服务架构中，我们需要仔细规划和协调每个服务的变更部署。我们需要先更新 C，然后更新 B，最后更新 A。&lt;/li&gt;
&lt;li&gt;部署复杂：微服务由不同的大量服务构成。每种服务可能拥有自己的配置、应用实例数量以及基础服务地址。这里就需要不同的配置、部署、扩展和监控组件。此外，我们还需要服务发现机制，以便服务可以发现与其通信的其他服务的地址。因此，成功部署微服务应用需要开发人员有更好地部署策略和高度自动化的水平。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总的来说（问题和挑战）：API Gateway、服务间调用、服务发现、服务容错、服务部署、数据调用。其实大部分也是我们一开始的说的微服务落地的三大难题。&lt;/p&gt;

&lt;p&gt;不过，现在很多微服务的框架（比如 Spring Cloud、Dubbo）已经很好的解决了上面的问题。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Spring Cloud&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Spring Cloud为开发者提供了快速构建分布式系统的通用模型的工具（包括配置管理、服务发现、熔断器、智能路由、微代理、控制总线、一次性令牌、全局锁、领导选举、分布式会话、集群状态等）&lt;/p&gt;

&lt;p&gt;其实主要解决了部署的问题，他就是一个很好的部署工具，目前也开始拥抱docker了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Dubbo&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Dubbo是一个阿里巴巴开源出来的一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案，其实主要解决的服务间通信的问题。&lt;/p&gt;

&lt;p&gt;在这个时候组织架构就发生了很大的变化，基本都是要加上一层网关或者企业总线，也就是&lt;strong&gt;前台-网关（ESB）-后台&lt;/strong&gt;的结构。&lt;/p&gt;

&lt;h3 id=&#34;soa&#34;&gt;SOA&lt;/h3&gt;

&lt;p&gt;SOA是什么？SOA全英文是Service-Oriented Architecture，中文意思是面向服务架构，是一种思想，一种方法论，一种分布式的服务架构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/20180305.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实就是上面实现的架构思想，抽象共享解耦。&lt;/p&gt;

&lt;h2 id=&#34;下一代微服务-去中心化&#34;&gt;下一代微服务&amp;ndash;去中心化&lt;/h2&gt;

&lt;h3 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h3&gt;

&lt;p&gt;随着注册中心的出现，任何调用都走网关，网关的瓶颈有到来，于是出现了下一代的微服务架构service mesh，主要落地的项目就是istio，还有一些其他的项目，主要是去中心化的设计。&lt;/p&gt;

&lt;p&gt;在云原生模型里，一个应用可以由数百个服务组成，每个服务可能有数千个实例，而每个实例可能会持续地发生变化。这种情况下，服务间通信不仅异常复杂，而且也是运行时行为的基础。管理好服务间通信对于保证端到端的性能和可靠性来说是无疑是非常重要的。种种复杂局面便催生了服务间通信层的出现，这个层既不会与应用程序的代码耦合，又能捕捉到底层环境高度动态的特点，让业务开发者只关注自己的业务代码，并将应用云化后带来的诸多问题以不侵入业务代码的方式提供给开发者。&lt;/p&gt;

&lt;p&gt;这个服务间通信层就是 Service Mesh，它可以提供安全、快速、可靠的服务间通讯（service-to-service）。&lt;/p&gt;

&lt;p&gt;Service Mesh，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给 Service Mesh 就可以了。&lt;/p&gt;

&lt;p&gt;微服务的各种框架也有一些。但这些框架大多是编程语言层面来解决的，需要用户的业务代码中集成框架的类库，语言的选择也受限。这种方案很难作为单独的产品或者服务给用户使用，升级更新也受限于应用本身的更新与迭代。直到 Service Mesh 的概念的提出。Service Mesh 貌似也没有比较契合的翻译（有的译做服务齿合层，有的翻译做服务网格），这个概念就是试图在网络层抽象出一层，来统一接管一些微服务治理的功能。这样就可以做到跨语言，无侵入，独立升级。其中前一段时间 Google，IBM，Lyft 联合开源的&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-istio/&#34;&gt;istio&lt;/a&gt;就是这样一个工具，先看下它的功能简介：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;智能路由以及负载均衡&lt;/li&gt;
&lt;li&gt;跨语言以及平台&lt;/li&gt;
&lt;li&gt;全范围（Fleet-wide）策略执行&lt;/li&gt;
&lt;li&gt;深度监控和报告&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;主要做了什么&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可视化 其实本质上微服务治理的许多技术点都包含可视化要求，比如监控和链路追踪，比如服务依赖&lt;/li&gt;
&lt;li&gt;弹性（Resiliency 或者应该叫柔性，因为弹性很容易想到 scale） 就是网络层可以不那么生硬，比如超时控制，重试策略，错误注入，熔断，延迟注入都属于这个范围。&lt;/li&gt;
&lt;li&gt;效率（Efficiency） 网络层可以帮应用层多做一些事情，提升效率。比如卸载 TLS，协议转换兼容&lt;/li&gt;
&lt;li&gt;流量控制 比如根据一定规则分发流量到不同的 Service 后端，但对调用方来说是透明的。&lt;/li&gt;
&lt;li&gt;安全保护 在网络层对流量加密/解密，增加安全认证机制，而对应用透明。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前流行的 Service Mesh 开源软件还有 Linkerd、Envoy，而最近 Buoyant（开源 Linkerd 的公司）又发布了基于 Kubernetes 的 Service Mesh 开源项目 Conduit。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linkerd（&lt;a href=&#34;https://github.com/linkerd/linkerd）：第一代&#34;&gt;https://github.com/linkerd/linkerd）：第一代&lt;/a&gt; Service Mesh，2016 年 1 月 15 日首发布，业界第一个 Service Mesh 项目，由 Buoyant 创业小公司开发（前 Twitter 工程师），2017 年 7 月 11 日，宣布和 Istio 集成，成为 Istio 的数据面板。&lt;/li&gt;
&lt;li&gt;Envoy（&lt;a href=&#34;https://github.com/envoyproxy/envoy）：第一代&#34;&gt;https://github.com/envoyproxy/envoy）：第一代&lt;/a&gt; Service Mesh，2016 年 9 月 13 日首发布，由 Matt Klein 个人开发（Lyft 工程师），之后默默发展，版本较稳定。&lt;/li&gt;
&lt;li&gt;Conduit（&lt;a href=&#34;https://github.com/runconduit/conduit）：第二代&#34;&gt;https://github.com/runconduit/conduit）：第二代&lt;/a&gt; Service Mesh，2017 年 12 月 5 日首发布，由 Buoyant 公司开发（借鉴 Istio 整体架构，部分进行了优化），对抗 Istio 压力山大，也期待 Buoyant 公司的毅力。&lt;/li&gt;
&lt;li&gt;nginMesh（&lt;a href=&#34;https://github.com/nginmesh/nginmesh）：2017&#34;&gt;https://github.com/nginmesh/nginmesh）：2017&lt;/a&gt; 年 9 月首发布，由 Nginx 开发，定位是作为 Istio 的服务代理，也就是替代 Envoy，思路跟 Linkerd 之前和 Istio 集成很相似，极度低调，GitHub 上的 star 也只有不到 100。&lt;/li&gt;
&lt;li&gt;Kong（&lt;a href=&#34;https://github.com/Kong/kong）：比&#34;&gt;https://github.com/Kong/kong）：比&lt;/a&gt; nginMesh 更加低调，默默发展中。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/architecture/microservices/go-micro/&#34;&gt;go-micro&lt;/a&gt;是基于Go语言实现的插件化RPC微服务框架，与go-kit，kite等微服务框架相比，它具有易上手、部署简单、工具插件化等优点。go-micro框架提供了服务发现、负载均衡、同步传输、异步通信以及事件驱动等机制，它尝试去简化分布式系统间的通信，让我们可以专注于自身业务逻辑的开发。所以对于新手而言，go-micro是个不错的微服务实践的开始。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前，市面上有许多 Service Mesh 的实现。我们这里挑选 4 种当前最主流的 Service Mesh，对其诸多方面( 包括功能特性、支持平台、是否付费等 )进行横向对比，用以说明 Istio 所存在的优势和不足。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/servicemesh&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关于微服务和服务网格的区别，我的一些理解：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;service mesh主要是解决了微服务框架中三大难题之一的服务间通信的问题，类似于上一代服务架构dubbo的作用，是在容器docker和k8s的原生基础上实现的服务间通信的方案，所以比java系的dubbo方案更加友好。&lt;/li&gt;
&lt;li&gt;service mesh在上一代的SOA架构上做了升级，设计的是去中性化的通信架构，解决的网关或者总线的瓶颈问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;k8s和微服务&#34;&gt;k8s和微服务&lt;/h3&gt;

&lt;p&gt;k8s在docker的基础上实现了管理，促使了下一代的微服务得以落地，主要解决了微服务三大难题的部署问题。类似于springcloud，springcloud目前也开始拥抱k8s了，可见k8s是一种趋势。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/k8s-ecology&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;随着docker、容器的日渐成熟，容器编排的问题就凸显出来，大量的容器怎么去管理，怎么调度，怎么启停都成了迫切需要解决的问题。有需求就有人去解决，ApacheMesos、kubernetes、docker swarm陆续登场，大有三足鼎立之势，而随着各自的发展，到了2017年下半年，google的亲儿子kubernetes的呼声越来越高，社区也更加活跃、成熟。2017年底，docker swarm和ApacheMesos陆续宣称支持kubernetes，预示着容器编排大战的结束，kubernetes已然成为容器编排领域的事实标准。&lt;/p&gt;

&lt;p&gt;服务编排框架的成熟，使得容器的管理越来越方便、高效，容器带来的好处也随之凸显：提升资源利用率节省成本、更高效的持续集成，持续交付、解放运维、快速扩缩容，应对突发流量&amp;hellip;&lt;/p&gt;

&lt;p&gt;服务编排框架的成熟也让微服务的概念得以落地，同时也催生了java界微服务化的方案，像SpringBoot，SpringCloud。然而服务编排一定是对微服务的编排吗？也就是我们容器里运行的一定是微服务吗？不是的，我们可以运行任何服务，我们现有的业务可以不做任何改造就运行到容器中，让kubernetes去管理、调度。至于微服务呢，只是有了kubernetes，让微服务变得容易管理了。让我们有条件把服务拆分的足够小，足够简单。再也不用担心运维管理的复杂了。了解了docker，服务编排，微服务的关系，我们在看看他们在企业的落地情况。&lt;/p&gt;

&lt;p&gt;k8s实现了容器编排工具，能够满足微服务架构拆分落地的需求，包括了管理和运维。k8s算是一个工具，而微服务更多的是个一个概念，架构，可以使用k8s实现，Istio就是微服务使用k8s实现的一个实现。&lt;/p&gt;

&lt;h4 id=&#34;serverless&#34;&gt;Serverless&lt;/h4&gt;

&lt;p&gt;Serverless被翻译为“无服务器架构”，重应用，轻服务器，k8s其实就是这么一个概念。&lt;/p&gt;

&lt;h1 id=&#34;互联网架构的服务化&#34;&gt;互联网架构的服务化&lt;/h1&gt;

&lt;h2 id=&#34;为什么服务化&#34;&gt;为什么服务化&lt;/h2&gt;

&lt;p&gt;在服务化之前，互联网的高可用架构大致是这样一个架构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/server.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户端是浏览器browser，APP客户端&lt;/li&gt;
&lt;li&gt;后端入口是高可用的nginx集群，用于做反向代理&lt;/li&gt;
&lt;li&gt;中间核心是高可用的web-server集群，研发工程师主要编码工作就是在这一层&lt;/li&gt;
&lt;li&gt;后端存储是高可用的db集群，数据存储在这一层&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;痛点：代码冗余&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;比如各个业务线都是自己通过DAO写SQL访问user库来存取用户数据，这无形中就导致了代码的拷贝。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/server1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;痛点：复杂性扩散&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;随着并发量的越来越高，用户数据的访问数据库成了瓶颈，需要加入缓存来降低数据库的读压力，于是架构中引入了缓存，由于没有统一的服务层，各个业务线都需要关注缓存的引入导致的复杂性,这个复杂性是典型的“业务无关”的复杂性，业务方需要被迫升级。&lt;/p&gt;

&lt;p&gt;随着数据量的越来越大，数据库需要进行水平拆分，于是架构中又引入了分库分表，由于没有统一的服务层，各个业务线都需要关注分库分表的引入导致的复杂性,这个复杂性也是典型的“业务无关”的复杂性，业务方需要被迫升级。&lt;/p&gt;

&lt;p&gt;包括bug的修改，发现一个bug，多个地方都需要修改。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;痛点：库的复用与耦合&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;服务化并不是唯一的解决上述两痛点的方法，抽象出统一的“库”是最先容易想到的解决,的方法。抽象出一个user.so，负责整个用户数据的存取，从而避免代码的拷贝。至于复杂性，也只有user.so这一个地方需要关注了。&lt;/p&gt;

&lt;p&gt;但是解决了旧的问题，会引入新的问题，库的版本维护与业务线之间代码的耦合，服务化在这个时候各用各自的实例可以实现解耦。&lt;/p&gt;

&lt;p&gt;SQL质量得不到保障，业务相互影响，本质上SQL语句还是各个业务线拼装的，资深的工程师写出高质量的SQL没啥问题，经验没有这么丰富的工程师可能会写出一些低效的SQL，假如业务线A写了一个全表扫描的SQL，导致数据库的CPU100%，影响的不只是一个业务线，而是所有的业务线都会受影响。&lt;/p&gt;

&lt;h2 id=&#34;服务化&#34;&gt;服务化&lt;/h2&gt;

&lt;p&gt;为了解决上面的诸多问题，互联网高可用分层架构演进的过程中，引入了“服务层”，其实就是对db的服务封装。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/microservices/server2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;好处：调用直接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有服务层之前：业务方访问用户数据，需要通过DAO拼装SQL访问&lt;/p&gt;

&lt;p&gt;有服务层之后：业务方通过RPC访问用户数据，就像调用一个本地函数一样，非常之爽&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;好处：复用性，防止代码拷贝，屏蔽底层，统一优化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;升级一处升级，bug修改一处修改。&lt;/p&gt;

&lt;p&gt;在有了服务层之后，只有服务层需要专注关注底层的复杂性了，向上游屏蔽了细节。&lt;/p&gt;

&lt;p&gt;有了服务层之后，所有的SQL都是服务层提供的，业务线不能再为所欲为了。底层服务对于稳定性的要求更好的话，可以由更资深的工程师维护，而不是像原来SQL难以收口，难以控制。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;这里只是一个简单的数据库的封装，给数据库新增的一个服务层，通过本地调用函数的方式来获取数据，具体的数据库优化交给数据库专业的人来处理。其实实际业务中还可以服务化的组件有很多，比如会员订单等子业务和序列化、反序列化、网络框架、连接池、收发线程、超时处理、状态机等非业务，最后其实就是我们常说的SOA的架构。&lt;/p&gt;

&lt;h1 id=&#34;微服务的粒度&#34;&gt;微服务的粒度&lt;/h1&gt;

&lt;p&gt;大家也都认可，随着数据量、流量、业务复杂度的提升，服务化架构是架构演进中的必由之路，但是微服务架构多“微”才合适？&lt;/p&gt;

&lt;p&gt;不同粒度的服务化各有优缺点总的来说，细粒度拆分的优点有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务都能够独立部署&lt;/li&gt;
&lt;li&gt;扩容和缩容方便，有利于提高资源利用率&lt;/li&gt;
&lt;li&gt;拆得越细，耦合相对会减小&lt;/li&gt;
&lt;li&gt;拆得越细，容错相对会更好，一个服务出问题不影响其他服务&lt;/li&gt;
&lt;li&gt;扩展性更好&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;细粒度拆分的不足也很明显：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;拆得越细，系统越复杂&lt;/li&gt;
&lt;li&gt;系统之间的依赖关系也更复杂&lt;/li&gt;
&lt;li&gt;运维复杂度提升&lt;/li&gt;
&lt;li&gt;监控更加复杂&lt;/li&gt;
&lt;li&gt;出问题时定位问题更难&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;个人觉得，以“子业务系统”粒度作为微服务的单位是比较合适的。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Ioutil</title>
          <link>https://kingjcy.github.io/post/golang/go-ioutil/</link>
          <pubDate>Sat, 13 Jan 2018 11:04:07 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-ioutil/</guid>
          <description>&lt;p&gt;ioutil主要是提供了一些常用、方便的IO操作函数。&lt;/p&gt;

&lt;h1 id=&#34;ioutil&#34;&gt;ioutil&lt;/h1&gt;

&lt;p&gt;ioutil针对reader和writer这两个接口封装的基础操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Discard 是一个 io.Writer 接口，调用它的 Write 方法将不做任何事情
// 并且始终成功返回。
var Discard io.Writer = devNull(0)

// ReadAll 读取 r 中的所有数据，返回读取的数据和遇到的错误。
// 如果读取成功，则 err 返回 nil，而不是 EOF，因为 ReadAll 定义为读取
// 所有数据，所以不会把 EOF 当做错误处理。
func ReadAll(r io.Reader) ([]byte, error)

// ReadFile 读取文件中的所有数据，返回读取的数据和遇到的错误。
// 如果读取成功，则 err 返回 nil，而不是 EOF
func ReadFile(filename string) ([]byte, error)

// WriteFile 向文件中写入数据，写入前会清空文件。
// 如果文件不存在，则会以指定的权限创建该文件。
// 返回遇到的错误。
func WriteFile(filename string, data []byte, perm os.FileMode) error

// ReadDir 读取指定目录中的所有目录和文件（不包括子目录）。
// 返回读取到的文件信息列表和遇到的错误，列表是经过排序的。
func ReadDir(dirname string) ([]os.FileInfo, error)

// NopCloser 将 r 包装为一个 ReadCloser 类型，但 Close 方法不做任何事情。
func NopCloser(r io.Reader) io.ReadCloser

// TempFile 在 dir 目录中创建一个以 prefix 为前缀的临时文件，并将其以读
// 写模式打开。返回创建的文件对象和遇到的错误。
// 如果 dir 为空，则在默认的临时目录中创建文件（参见 os.TempDir），多次
// 调用会创建不同的临时文件，调用者可以通过 f.Name() 获取文件的完整路径。
// 调用本函数所创建的临时文件，应该由调用者自己删除。
func TempFile(dir, prefix string) (f *os.File, err error)

// TempDir 功能同 TempFile，只不过创建的是目录，返回目录的完整路径。
func TempDir(dir, prefix string) (name string, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;示例&#34;&gt;示例&lt;/h1&gt;

&lt;h2 id=&#34;读取目录&#34;&gt;读取目录&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    rd, err := ioutil.ReadDir(&amp;quot;/&amp;quot;)
    fmt.Println(err)
    for _, fi := range rd {
        if fi.IsDir() {
            fmt.Printf(&amp;quot;[%s]\n&amp;quot;, fi.Name())

        } else {
            fmt.Println(fi.Name())
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;临时目录-临时文件&#34;&gt;临时目录、临时文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    // 创建临时目录
    dir, err := ioutil.TempDir(&amp;quot;&amp;quot;, &amp;quot;Test&amp;quot;)
    if err != nil {
        fmt.Println(err)
    }
    defer os.Remove(dir) // 用完删除
    fmt.Printf(&amp;quot;%s\n&amp;quot;, dir)

    // 创建临时文件
    f, err := ioutil.TempFile(dir, &amp;quot;Test&amp;quot;)
    if err != nil {
        fmt.Println(err)
    }
    defer os.Remove(f.Name()) // 用完删除
    fmt.Printf(&amp;quot;%s\n&amp;quot;, f.Name())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;读取文件&#34;&gt;读取文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;io/ioutil&amp;quot;
)

func main() {
    b, err := ioutil.ReadFile(&amp;quot;test.log&amp;quot;)
    if err != nil {
        fmt.Print(err)
    }
    fmt.Println(b)
    str := string(b)
    fmt.Println(str)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;写文件&#34;&gt;写文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
   &amp;quot;io/ioutil&amp;quot;
)

func check(e error) {
   if e != nil {
       panic(e)
   }
}

func main() {

   d1 := []byte(&amp;quot;hello\ngo\n&amp;quot;)
   err := ioutil.WriteFile(&amp;quot;test.txt&amp;quot;, d1, 0644)
   check(err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读取文件和写文件内容还可以使用&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/#文件io&#34;&gt;os包&lt;/a&gt;来处理。一般也是使用os标准库来处理。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Bytes</title>
          <link>https://kingjcy.github.io/post/golang/go-bytes/</link>
          <pubDate>Mon, 25 Dec 2017 14:28:17 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-bytes/</guid>
          <description>&lt;p&gt;该包定义了一些操作 byte slice 的便利操作。因为字符串可以表示为 []byte，因此，bytes 包定义的函数、方法等和 strings 包很类似，所以讲解时会和 strings 包类似甚至可以直接参考。&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;h2 id=&#34;是否存在某个子-slice&#34;&gt;是否存在某个子 slice&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 子 slice subslice 在 b 中，返回 true
func Contains(b, subslice []byte) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数的内部调用了 bytes.Index 函数（在后面会讲解）:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Contains(b, subslice []byte) bool {
    return Index(b, subslice) != -1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;题外：对比 strings.Contains 你会发现，一个判断 &amp;gt;=0，一个判断 != -1，可见库不是一个人写的，没有做到一致性。&lt;/p&gt;

&lt;h2 id=&#34;byte-出现次数&#34;&gt;[]byte 出现次数&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// slice sep 在 s 中出现的次数（无重叠）
func Count(s, sep []byte) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和 strings 实现不同，此包中的 Count 核心代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;count := 0
c := sep[0]
i := 0
t := s[:len(s)-n+1]
for i &amp;lt; len(t) {
    // 判断 sep 第一个字节是否在 t[i:] 中
    // 如果在，则比较之后相应的字节
    if t[i] != c {
        o := IndexByte(t[i:], c)
        if o &amp;lt; 0 {
            break
        }
        i += o
    }
    // 执行到这里表示 sep[0] == t[i]
    if n == 1 || Equal(s[i:i+n], sep) {
        count++
        i += n
        continue
    }
    i++
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;runes-类型转换&#34;&gt;Runes 类型转换&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 将 []byte 转换为 []rune
func Runes(s []byte) []rune
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数将 []byte 转换为 []rune ，适用于汉字等多字节字符，示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b:=[]byte(&amp;quot;你好，世界&amp;quot;)
for k,v:=range b{
    fmt.Printf(&amp;quot;%d:%s |&amp;quot;,k,string(v))
}
r:=bytes.Runes(b)
for k,v:=range r{
    fmt.Printf(&amp;quot;%d:%s|&amp;quot;,k,string(v))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0:ä |1:½ |2:  |3:å |4:¥ |5:½ |6:ï |7:¼ |8:  |9:ä |10:¸ |11:  |12:ç |13:  |14: |
0:你|1:好|2:，|3:世|4:界|
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;其它函数&#34;&gt;其它函数&lt;/h2&gt;

&lt;p&gt;其它大部分函数、方法与 strings 包下的函数、方法类似，只是数据源从 string 变为了 []byte ，请参考 strings 包的用法。&lt;/p&gt;

&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;

&lt;h2 id=&#34;reader-类型&#34;&gt;Reader 类型&lt;/h2&gt;

&lt;p&gt;看到名字就能猜到，这是实现了 io 包中的接口。其实这就是缓存io，对缓存中的[]byte进行读写操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader struct {
    s        []byte
    i        int64 // 当前读取下标
    prevRune int   // 前一个字符的下标，也可能 &amp;lt; 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bytes 包下的 Reader 类型实现了 io 包下的 Reader, ReaderAt, RuneReader, RuneScanner, ByteReader, ByteScanner, ReadSeeker, Seeker, WriterTo 等多个接口。主要用于 Read 数据。&lt;/p&gt;

&lt;p&gt;我们需要在通过 bytes.NewReader 方法来初始化 bytes.Reader 类型的对象。初始化时传入 []byte 类型的数据。NewReader 函数签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(b []byte) *Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果直接声明该对象了，可以通过 Reset 方法重新写入数据，示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x:=[]byte(&amp;quot;你好，世界&amp;quot;)

r1:=bytes.NewReader(x)
d1:=make([]byte,len(x))
n,_:=r1.Read(d1)
fmt.Println(n,string(d1))

r2:=bytes.Reader{}
r2.Reset(x)
d2:=make([]byte,len(x))
n,_=r2.Read(d2)
fmt.Println(n,string(d2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;15 你好，世界
15 你好，世界
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reader 包含了 8 个读取相关的方法，实现了前面提到的 io 包下的 9 个接口（ReadSeeker 接口内嵌 Reader 和 Seeker 两个接口）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 读取数据至 b
func (r *Reader) Read(b []byte) (n int, err error)
// 读取一个字节
func (r *Reader) ReadByte() (byte, error)
// 读取一个字符
func (r *Reader) ReadRune() (ch rune, size int, err error)
// 读取数据至 w
func (r *Reader) WriteTo(w io.Writer) (n int64, err error)
// 进度下标指向前一个字节，如果 r.i &amp;lt;= 0 返回错误。
func (r *Reader) UnreadByte()
// 进度下标指向前一个字符，如果 r.i &amp;lt;= 0 返回错误，且只能在每次 ReadRune 方法后使用一次，否则返回错误。
func (r *Reader) UnreadRune()
// 读取 r.s[off:] 的数据至b，该方法忽略进度下标 i，不使用也不修改。
func (r *Reader) ReadAt(b []byte, off int64) (n int, err error)
// 根据 whence 的值，修改并返回进度下标 i ，当 whence == 0 ，进度下标修改为 off，当 whence == 1 ，进度下标修改为 i+off，当 whence == 2 ，进度下标修改为 len[s]+off.
// off 可以为负数，whence 的只能为 0，1，2，当 whence 为其他值或计算后的进度下标越界，则返回错误。
func (r *Reader) Seek(offset int64, whence int) (int64, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x := []byte(&amp;quot;你好，世界&amp;quot;)
r1 := bytes.NewReader(x)

ch, size, _ := r1.ReadRune()
fmt.Println(size, string(ch))
_ = r1.UnreadRune()
ch, size, _ = r1.ReadRune()
fmt.Println(size, string(ch))
_ = r1.UnreadRune()

by, _ := r1.ReadByte()
fmt.Println(by)
_ = r1.UnreadByte()
by, _ = r1.ReadByte()
fmt.Println(by)
_ = r1.UnreadByte()

d1 := make([]byte, 6)
n, _ := r1.Read(d1)
fmt.Println(n, string(d1))

d2 := make([]byte, 6)
n, _ = r1.ReadAt(d2, 0)
fmt.Println(n, string(d2))

w1 := &amp;amp;bytes.Buffer{}
_, _ = r1.Seek(0, 0)
_, _ = r1.WriteTo(w1)
fmt.Println(w1.String())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3 你
3 你
228
228
6 你好
6 你好
你好，世界
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;buffer-类型&#34;&gt;Buffer 类型&lt;/h2&gt;

&lt;p&gt;buffer类型也实现了缓存io，也是对[]byte进行读写操作，提供了多种实现化函数对象，个人感觉是对string，byte的reader的综合使用实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Buffer struct {
    buf      []byte
    off      int
    lastRead readOp
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上一个示例的最后，我们使用了 bytes.Buffer 类型，该类型实现了 io 包下的 ByteScanner, ByteWriter, ReadWriter, Reader, ReaderFrom, RuneReader, RuneScanner, StringWriter, Writer, WriterTo 等接口，可以方便的进行读写操作。&lt;/p&gt;

&lt;p&gt;对象可读取数据为 buf[off : len(buf)], off 表示进度下标，lastRead 表示最后读取的一个字符所占字节数，方便 Unread* 相关操作。&lt;/p&gt;

&lt;p&gt;Buffer 可以通过 3 中方法初始化对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := bytes.NewBufferString(&amp;quot;Hello World&amp;quot;)
b := bytes.NewBuffer([]byte(&amp;quot;Hello World&amp;quot;))
c := bytes.Buffer{}

fmt.Println(a)
fmt.Println(b)
fmt.Println(c)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Hello World
Hello World
{[] 0 0}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Buffer 包含了 21 个读写相关的方法，大部分同名方法的用法与前面讲的类似，这里只讲演示其中的 3 个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 读取到字节 delim 后，以字节数组的形式返回该字节及前面读取到的字节。如果遍历 b.buf 也找不到匹配的字节，则返回错误(一般是 EOF)
func (b *Buffer) ReadBytes(delim byte) (line []byte, err error)
// 读取到字节 delim 后，以字符串的形式返回该字节及前面读取到的字节。如果遍历 b.buf 也找不到匹配的字节，则返回错误(一般是 EOF)
func (b *Buffer) ReadString(delim byte) (line string, err error)
// 截断 b.buf , 舍弃 b.off+n 之后的数据。n == 0 时，调用 Reset 方法重置该对象，当 n 越界时（n &amp;lt; 0 || n &amp;gt; b.Len() ）方法会触发 panic.
func (b *Buffer) Truncate(n int)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := bytes.NewBufferString(&amp;quot;Good Night&amp;quot;)

x, err := a.ReadBytes(&#39;t&#39;)
if err != nil {
    fmt.Println(&amp;quot;delim:t err:&amp;quot;, err)
} else {
    fmt.Println(string(x))
}

a.Truncate(0)
a.WriteString(&amp;quot;Good Night&amp;quot;)
fmt.Println(a.Len())
a.Truncate(5)
fmt.Println(a.Len())
y, err := a.ReadString(&#39;N&#39;)
if err != nil {
    fmt.Println(&amp;quot;delim:N err:&amp;quot;, err)
} else {
    fmt.Println(y)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Good Night
10
5
delim:N err: EOF
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Archive</title>
          <link>https://kingjcy.github.io/post/golang/go-archive/</link>
          <pubDate>Mon, 25 Dec 2017 14:26:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-archive/</guid>
          <description>&lt;p&gt;archive就是使用tar和zip两种方式对文档进行归档，压缩看compress包。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;tar和zip有什么不同&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;二者使用平台不同，对于 Windows 平台而言，最常用的格式是 zip 和 rar，国内大多数是用 rar，国外大多数是用 zip。而对于类 Unix 平台而言，常用的格式是 tar 和 tar.gz，zip 比较少一些，rar 则几乎没有。&lt;/p&gt;

&lt;p&gt;zip 格式是开放且免费的，所以广泛使用在 Windows、Linux、MacOS 平台，要说 zip 有什么缺点的话，就是它的压缩率并不是很高，不如 rar及 tar.gz 等格式。&lt;/p&gt;

&lt;p&gt;严格的说，tar 只是一种打包格式，并不对文件进行压缩，主要是为了便于文件的管理，所以打包后的文档大小一般远远大于 zip 和 tar.gz，但这种格式也有很明显的优点，例如打包速度非常快，打包时 CPU 占用率也很低，因为不需要压缩嘛。&lt;/p&gt;

&lt;h1 id=&#34;archive-tar&#34;&gt;archive/tar&lt;/h1&gt;

&lt;h2 id=&#34;单个文件操作&#34;&gt;单个文件操作&lt;/h2&gt;

&lt;p&gt;这个非常简单，就是读取一个文件，进行打包及解包操作即可。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;单个文件打包&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从 /etc/passwd 下复制了一个 passwd 文件到当前目录下，用来做压缩测试。什么文件都是可以的，自己随意写一个也行。这里的示例主要为了说明 tar ，没有处理路径，所以过程全部假设是在当前目录下执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp /etc/passwd .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于文件的打包直接查看示例代码，已经在示例代码中做了详细的注释。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;os&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;archive/tar&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
)

func main() {
    // 准备打包的源文件
    var srcFile = &amp;quot;passwd&amp;quot;
    // 打包后的文件
    var desFile = fmt.Sprintf(&amp;quot;%s.tar&amp;quot;,srcFile)

    // 需要注意文件的打开即关闭的顺序，因为 defer 是后入先出，所以关闭顺序很重要
    // 第一次写这个示例的时候就没注意，导致写完的 tar 包不完整

    // ###### 第 1 步，先准备好一个 tar.Writer 结构，然后再向里面写入内容。 ######
    // 创建一个文件，用来保存打包后的 passwd.tar 文件
    fw, err := os.Create(desFile)
    ErrPrintln(err)
    defer fw.Close()

    // 通过 fw 创建一个 tar.Writer
    tw := tar.NewWriter(fw)
    // 这里不要忘记关闭，如果不能成功关闭会造成 tar 包不完整
    // 所以这里在关闭的同时进行判断，可以清楚的知道是否成功关闭
    defer func() {
        if err := tw.Close(); err != nil {
            ErrPrintln(err)
        }
    }()

    // ###### 第 2 步，处理文件信息，也就是 tar.Header 相关的 ######
    // tar 包共有两部分内容：文件信息和文件数据
    // 通过 Stat 获取 FileInfo，然后通过 FileInfoHeader 得到 hdr tar.*Header
    fi, err := os.Stat(srcFile)
    ErrPrintln(err)
    hdr, err := tar.FileInfoHeader(fi, &amp;quot;&amp;quot;)
    // 将 tar 的文件信息 hdr 写入到 tw
    err = tw.WriteHeader(hdr)
    ErrPrintln(err)

    // 将文件数据写入
    // 打开准备写入的文件
    fr, err := os.Open(srcFile)
    ErrPrintln(err)
    defer fr.Close()

    written, err := io.Copy(tw, fr)
    ErrPrintln(err)

    log.Printf(&amp;quot;共写入了 %d 个字符的数据\n&amp;quot;,written)
}

// 定义一个用来打印的函数，少写点代码，因为要处理很多次的 err
// 后面其他示例还会继续使用这个函数，就不单独再写，望看到此函数了解
func ErrPrintln(err error)  {
    if err != nil {
        log.Println(err)
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;单个文件解包&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个也很简单，基本上将上面过程反过来，只需要处理 tar.Reader 即可，详细的描述见示例。&lt;/p&gt;

&lt;p&gt;这里就用刚刚打包的 passwd.tar 文件做示例，如果怕结果看不出效果，可以将之前用的 passwd 源文件删除。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;os&amp;quot;
    &amp;quot;archive/tar&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
)

func main() {

    var srcFile = &amp;quot;passwd.tar&amp;quot;

    // 将 tar 包打开
    fr, err := os.Open(srcFile)
    ErrPrintln(err)
    defer fr.Close()

    // 通过 fr 创建一个 tar.*Reader 结构，然后将 tr 遍历，并将数据保存到磁盘中
    tr := tar.NewReader(fr)

    for hdr, err := tr.Next(); err != io.EOF; hdr, err = tr.Next(){
        // 处理 err ！= nil 的情况
        ErrPrintln(err)
        // 获取文件信息
        fi := hdr.FileInfo()

        // 创建一个空文件，用来写入解包后的数据
        fw, err := os.Create(fi.Name())
        ErrPrintln(err)

        // 将 tr 写入到 fw
        n, err := io.Copy(fw, tr)
        ErrPrintln(err)
        log.Printf(&amp;quot;解包： %s 到 %s ，共处理了 %d 个字符的数据。&amp;quot;, srcFile,fi.Name(),n)

        // 设置文件权限，这样可以保证和原始文件权限相同，如果不设置，会根据当前系统的 umask 来设置。
        os.Chmod(fi.Name(),fi.Mode().Perm())

        // 注意，因为是在循环中，所以就没有使用 defer 关闭文件
        // 如果想使用 defer 的话，可以将文件写入的步骤单独封装在一个函数中即可
        fw.Close()
    }
}

func ErrPrintln(err error){
    if err != nil {
        log.Fatalln(err)
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;操作整个目录&#34;&gt;操作整个目录&lt;/h2&gt;

&lt;p&gt;我们实际中 tar 很少会去打包单个文件，一般都是打包整个目录，并且打包的时候通过 gzip 或者 bzip2 压缩。&lt;/p&gt;

&lt;p&gt;如果要打包整个目录，可以通过递归的方式来实现。这里只演示了 gzip 方式压缩，这个实现非常简单，只需要在 fw 和 tw 之前加上一层压缩即可，详情见示例代码。&lt;/p&gt;

&lt;p&gt;为了测试打包整个目录，复制了一个 log 目录到当前路径下。什么目录和文件都可以，只是因为这个里面内容比较多，就拿这个来做测试了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;打包压缩&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;archive/tar&amp;quot;
    &amp;quot;compress/gzip&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;path/filepath&amp;quot;
    &amp;quot;strings&amp;quot;
)

func main() {
    // 修改日志格式，显示出错代码的所在行，方便调试，实际项目中一般不记录这个。

    var src = &amp;quot;apt&amp;quot;
    var dst = fmt.Sprintf(&amp;quot;%s.tar.gz&amp;quot;, src)

    // 将步骤写入了一个函数中，这样处理错误方便一些
    if err := Tar(src, dst); err != nil {
        log.Fatalln(err)
    }
}

func Tar(src, dst string) (err error) {
    // 创建文件
    fw, err := os.Create(dst)
    if err != nil {
        return
    }
    defer fw.Close()

    // 将 tar 包使用 gzip 压缩，其实添加压缩功能很简单，
    // 只需要在 fw 和 tw 之前加上一层压缩就行了，和 Linux 的管道的感觉类似
    gw := gzip.NewWriter(fw)
    defer gw.Close()

    // 创建 Tar.Writer 结构
    tw := tar.NewWriter(gw)
    // 如果需要启用 gzip 将上面代码注释，换成下面的

    defer tw.Close()

    // 下面就该开始处理数据了，这里的思路就是递归处理目录及目录下的所有文件和目录
    // 这里可以自己写个递归来处理，不过 Golang 提供了 filepath.Walk 函数，可以很方便的做这个事情
    // 直接将这个函数的处理结果返回就行，需要传给它一个源文件或目录，它就可以自己去处理
    // 我们就只需要去实现我们自己的 打包逻辑即可，不需要再去路径相关的事情
    return filepath.Walk(src, func(fileName string, fi os.FileInfo, err error) error {
        // 因为这个闭包会返回个 error ，所以先要处理一下这个
        if err != nil {
            return err
        }

        // 这里就不需要我们自己再 os.Stat 了，它已经做好了，我们直接使用 fi 即可
        hdr, err := tar.FileInfoHeader(fi, &amp;quot;&amp;quot;)
        if err != nil {
            return err
        }
        // 这里需要处理下 hdr 中的 Name，因为默认文件的名字是不带路径的，
        // 打包之后所有文件就会堆在一起，这样就破坏了原本的目录结果
        // 例如： 将原本 hdr.Name 的 syslog 替换程 log/syslog
        // 这个其实也很简单，回调函数的 fileName 字段给我们返回来的就是完整路径的 log/syslog
        // strings.TrimPrefix 将 fileName 的最左侧的 / 去掉，
        // 熟悉 Linux 的都知道为什么要去掉这个
        hdr.Name = strings.TrimPrefix(fileName, string(filepath.Separator))

        // 写入文件信息
        if err := tw.WriteHeader(hdr); err != nil {
            return err
        }

        // 判断下文件是否是标准文件，如果不是就不处理了，
        // 如： 目录，这里就只记录了文件信息，不会执行下面的 copy
        if !fi.Mode().IsRegular() {
            return nil
        }

        // 打开文件
        fr, err := os.Open(fileName)
        defer fr.Close()
        if err != nil {
            return err
        }

        // copy 文件数据到 tw
        n, err := io.Copy(tw, fr)
        if err != nil {
            return err
        }

        // 记录下过程，这个可以不记录，这个看需要，这样可以看到打包的过程
        log.Printf(&amp;quot;成功打包 %s ，共写入了 %d 字节的数据\n&amp;quot;, fileName, n)

        return nil
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打包及压缩就搞定了，不过这个代码现在我还发现有个问题，就是不能处理软链接&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;解包解压&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个过程基本就是把压缩的过程返回来，多了些创建目录的操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;archive/tar&amp;quot;
    &amp;quot;compress/gzip&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;path/filepath&amp;quot;
)

func main() {
    var dst = &amp;quot;&amp;quot; // 不写就是解压到当前目录
    var src = &amp;quot;log.tar.gz&amp;quot;

    UnTar(dst, src)
}

func UnTar(dst, src string) (err error) {
    // 打开准备解压的 tar 包
    fr, err := os.Open(src)
    if err != nil {
        return
    }
    defer fr.Close()

    // 将打开的文件先解压
    gr, err := gzip.NewReader(fr)
    if err != nil {
        return
    }
    defer gr.Close()

    // 通过 gr 创建 tar.Reader
    tr := tar.NewReader(gr)

    // 现在已经获得了 tar.Reader 结构了，只需要循环里面的数据写入文件就可以了
    for {
        hdr, err := tr.Next()

        switch {
        case err == io.EOF:
            return nil
        case err != nil:
            return err
        case hdr == nil:
            continue
        }

        // 处理下保存路径，将要保存的目录加上 header 中的 Name
        // 这个变量保存的有可能是目录，有可能是文件，所以就叫 FileDir 了……
        dstFileDir := filepath.Join(dst, hdr.Name)

        // 根据 header 的 Typeflag 字段，判断文件的类型
        switch hdr.Typeflag {
        case tar.TypeDir: // 如果是目录时候，创建目录
            // 判断下目录是否存在，不存在就创建
            if b := ExistDir(dstFileDir); !b {
                // 使用 MkdirAll 不使用 Mkdir ，就类似 Linux 终端下的 mkdir -p，
                // 可以递归创建每一级目录
                if err := os.MkdirAll(dstFileDir, 0775); err != nil {
                    return err
                }
            }
        case tar.TypeReg: // 如果是文件就写入到磁盘
            // 创建一个可以读写的文件，权限就使用 header 中记录的权限
            // 因为操作系统的 FileMode 是 int32 类型的，hdr 中的是 int64，所以转换下
            file, err := os.OpenFile(dstFileDir, os.O_CREATE|os.O_RDWR, os.FileMode(hdr.Mode))
            if err != nil {
                return err
            }
            n, err := io.Copy(file, tr)
            if err != nil {
                return err
            }
            // 将解压结果输出显示
            fmt.Printf(&amp;quot;成功解压： %s , 共处理了 %d 个字符\n&amp;quot;, dstFileDir, n)

            // 不要忘记关闭打开的文件，因为它是在 for 循环中，不能使用 defer
            // 如果想使用 defer 就放在一个单独的函数中
            file.Close()
        }
    }

    return nil
}

// 判断目录是否存在
func ExistDir(dirname string) bool {
    fi, err := os.Stat(dirname)
    return (err == nil || os.IsExist(err)) &amp;amp;&amp;amp; fi.IsDir()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;archive-zip&#34;&gt;archive/zip&lt;/h1&gt;

&lt;h2 id=&#34;压缩&#34;&gt;压缩&lt;/h2&gt;

&lt;p&gt;和 tar 的过程很像，只有些小的差别，详情见示例代码。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;archive/zip&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;path/filepath&amp;quot;
    &amp;quot;strings&amp;quot;
)

func main() {
    // 源档案（准备压缩的文件或目录）
    var src = &amp;quot;log&amp;quot;
    // 目标文件，压缩后的文件
    var dst = &amp;quot;log.zip&amp;quot;

    if err := Zip(dst, src); err != nil {
        log.Fatalln(err)
    }
}

func Zip(dst, src string) (err error) {
    // 创建准备写入的文件
    fw, err := os.Create(dst)
    defer fw.Close()
    if err != nil {
        return err
    }

    // 通过 fw 来创建 zip.Write
    zw := zip.NewWriter(fw)
    defer func() {
        // 检测一下是否成功关闭
        if err := zw.Close(); err != nil {
            log.Fatalln(err)
        }
    }()

    // 下面来将文件写入 zw ，因为有可能会有很多个目录及文件，所以递归处理
    return filepath.Walk(src, func(path string, fi os.FileInfo, errBack error) (err error) {
        if errBack != nil {
            return errBack
        }

        // 通过文件信息，创建 zip 的文件信息
        fh, err := zip.FileInfoHeader(fi)
        if err != nil {
            return
        }

        // 替换文件信息中的文件名
        fh.Name = strings.TrimPrefix(path, string(filepath.Separator))

        // 这步开始没有加，会发现解压的时候说它不是个目录
        if fi.IsDir() {
            fh.Name += &amp;quot;/&amp;quot;
        }

        // 写入文件信息，并返回一个 Write 结构
        w, err := zw.CreateHeader(fh)
        if err != nil {
            return
        }

        // 检测，如果不是标准文件就只写入头信息，不写入文件数据到 w
        // 如目录，也没有数据需要写
        if !fh.Mode().IsRegular() {
            return nil
        }

        // 打开要压缩的文件
        fr, err := os.Open(path)
        defer fr.Close()
        if err != nil {
            return
        }

        // 将打开的文件 Copy 到 w
        n, err := io.Copy(w, fr)
        if err != nil {
            return
        }
        // 输出压缩的内容
        fmt.Printf(&amp;quot;成功压缩文件： %s, 共写入了 %d 个字符的数据\n&amp;quot;, path, n)

        return nil
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;解压缩&#34;&gt;解压缩&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;archive/zip&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;path/filepath&amp;quot;
)

func main() {
    // 压缩包
    var src = &amp;quot;log.zip&amp;quot;
    // 解压后保存的位置，为空表示当前目录
    var dst = &amp;quot;&amp;quot;

    if err := UnZip(dst, src); err != nil {
        log.Fatalln(err)
    }
}

func UnZip(dst, src string) (err error) {
    // 打开压缩文件，这个 zip 包有个方便的 ReadCloser 类型
    // 这个里面有个方便的 OpenReader 函数，可以比 tar 的时候省去一个打开文件的步骤
    zr, err := zip.OpenReader(src)
    defer zr.Close()
    if err != nil {
        return
    }

    // 如果解压后不是放在当前目录就按照保存目录去创建目录
    if dst != &amp;quot;&amp;quot; {
        if err := os.MkdirAll(dst, 0755); err != nil {
            return err
        }
    }

    // 遍历 zr ，将文件写入到磁盘
    for _, file := range zr.File {
        path := filepath.Join(dst, file.Name)

        // 如果是目录，就创建目录
        if file.FileInfo().IsDir() {
            if err := os.MkdirAll(path, file.Mode()); err != nil {
                return err
            }
            // 因为是目录，跳过当前循环，因为后面都是文件的处理
            continue
        }

        // 获取到 Reader
        fr, err := file.Open()
        if err != nil {
            return err
        }

        // 创建要写出的文件对应的 Write
        fw, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR|os.O_TRUNC, file.Mode())
        if err != nil {
            return err
        }

        n, err := io.Copy(fw, fr)
        if err != nil {
            return err
        }

        // 将解压的结果输出
        fmt.Printf(&amp;quot;成功解压 %s ，共写入了 %d 个字符的数据\n&amp;quot;, path, n)

        // 因为是在循环中，无法使用 defer ，直接放在最后
        // 不过这样也有问题，当出现 err 的时候就不会执行这个了，
        // 可以把它单独放在一个函数中，这里是个实验，就这样了
        fw.Close()
        fr.Close()
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- zabbix源码阅读</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbixcode/</link>
          <pubDate>Sat, 25 Nov 2017 09:52:47 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbixcode/</guid>
          <description>&lt;p&gt;阅读源码，解析基本原理。&lt;/p&gt;

&lt;h1 id=&#34;流程&#34;&gt;流程&lt;/h1&gt;

&lt;p&gt;一个监控系统运行的大概的流程是这样的：&lt;/p&gt;

&lt;p&gt;agentd需要安装到被监控的主机上，它负责定期收集各项数据，并发送到zabbix server端，zabbix server将数据存储到数据库中，zabbix web根据数据在前端进行展现和绘图。这里agentd收集数据分为主动和被动两种模式：&lt;/p&gt;

&lt;p&gt;主动：agent请求server获取主动的监控项列表，并主动将监控项内需要检测的数据提交给server/proxy&lt;/p&gt;

&lt;p&gt;被动：server向agent请求获取监控项的数据，agent返回数据。&lt;/p&gt;

&lt;h1 id=&#34;主动监测&#34;&gt;主动监测&lt;/h1&gt;

&lt;p&gt;通信过程如下：&lt;/p&gt;

&lt;p&gt;zabbix首先向ServerActive配置的IP请求获取active items，获取并提交active tiems数据值server或者proxy。很多人会提出疑问：zabbix多久获取一次active items？它会根据配置文件中的RefreshActiveChecks的频率进行，如果获取失败，那么将会在60秒之后重试。分两个部分：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/zabbix/z1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1.获取ACTIVE ITEMS列表&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Agent打开TCP连接（主动检测变成Agent打开）&lt;/li&gt;
&lt;li&gt;Agent请求items检测列表&lt;/li&gt;
&lt;li&gt;Server返回items列表&lt;/li&gt;
&lt;li&gt;Agent 处理响应&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;li&gt;Agent开始收集数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2.主动检测提交数据过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Agent建立TCP连接&lt;/li&gt;
&lt;li&gt;Agent提交items列表收集的数据&lt;/li&gt;
&lt;li&gt;Server处理数据，并返回响应状态&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;被动监测&#34;&gt;被动监测&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/zabbix/z2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通信过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server打开一个TCP连接&lt;/li&gt;
&lt;li&gt;Server发送请求agent.ping\n&lt;/li&gt;
&lt;li&gt;Agent接收到请求并且响应&lt;code&gt;&amp;lt;HEADER&amp;gt;&amp;lt;DATALEN&amp;gt;1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Server处理接收到的数据1&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里，有人可以看出来，被动模式每次都需要打开一个tcp连接，这样当监控项越来越多时，就会出现server端性能问题了。&lt;/p&gt;

&lt;p&gt;比如not supported items通信过程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server打开一个TCP连接&lt;/li&gt;
&lt;li&gt;Server发送请求&lt;code&gt;vfs.fs.size[ no]\n&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Agent接收请求并且返回响应数据 &lt;code&gt;&amp;lt;HEADER&amp;gt;&amp;lt;DATALEN&amp;gt;ZBX_NOTSUPPORTED\0Cannot obtain filesystem information: [2] No such file or directory&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Server接收并处理数据, 将item的状态改为“ not supported ”&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有人会问，那实际监控中是用主动的还是被动的呢？这里主要涉及两个地方：&lt;/p&gt;

&lt;p&gt;1、新建监控项目时，选择的是zabbix代理还是zabbix端点代理程式（主动式），前者是被动模式，后者是主动模式。&lt;/p&gt;

&lt;p&gt;2、agentd配置文件中StartAgents参数的设置，如果为0，表示禁止被动模式，否则开启。一般建议不要设置为0，因为监控项目很多时，可以部分使用主动，部分使用被动模式。&lt;/p&gt;

&lt;h1 id=&#34;常用的监控架构平台&#34;&gt;常用的监控架构平台&lt;/h1&gt;

&lt;p&gt;1、server-agentd模式：&lt;/p&gt;

&lt;p&gt;这个是最简单的架构了，常用于监控主机比较少的情况下。&lt;/p&gt;

&lt;p&gt;2、server-proxy-agentd模式：&lt;/p&gt;

&lt;p&gt;这个常用于比较多的机器，使用proxy进行分布式监控，有效的减轻server端的压力。&lt;/p&gt;

&lt;h1 id=&#34;组件解析&#34;&gt;组件解析&lt;/h1&gt;

&lt;h2 id=&#34;agent&#34;&gt;Agent&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;入口函数：zabbix_agentd.c:MAIN_ZABBIX_ENTRY&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;采集线程：stats.c: ZBX_THREAD_ENTRY(collector_thread, args)，采集数据&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监听线程：listener.c: ZBX_THREAD_ENTRY(listener_thread, args)，监听端口（根据加密格式）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;发送线程：active.c:ZBX_THREAD_ENTRY(active_checks_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.发送报文函数: active.c:send_buffer，消息体为消息头+json格式的消息体，根据加密配置，分为不加密，cert加密和psk加密。Json的编码可以在这个函数里看。
2.加密可以使用openssl的库，主要实现在tls.c:zbx_tls_connect函数中。
3.消息头的编码：comms.c:zbx_tcp_send_ext，包括” ZBXD”+1字节flag+32位json消息长度+32位0x00，在发送json体的时候，使用了zlib的compress函数进行压缩，对端接收的时候使用uncompress进行了解压缩。
4.1字节flag有以下取值：

        {
        ZBX_TCP_PROTOCOL(0x01)
        ZBX_TCP_PROTOCOL |ZBX_TCP_COMPRESS (0x03)
        0x00
        }
        当flag&amp;amp; ZBX_TCP_COMPRESS!=0时，发送报文需要对消息体进行compress压缩，接收报文需要对消息体进行uncompress解压缩
        #define ZBX_TCP_PROTOCOL        0x01
        #define ZBX_TCP_COMPRESS        0x02
        当flag==0时，报文没有消息头，只有json消息体

5.消息长度

        发送报文时，如果加密，消息体最长16K
        #define ZBX_TLS_MAX_REC_LEN 16384
        如果不加密，没有限制，写json串时动态申请内存
        接收报文时，最大长度128M，根据接收的消息长度循环动态申请内存
        #define ZBX_MAX_RECV_DATA_SIZE  (128 * ZBX_MEBIBYTE)

6.json编码中request的类型

        #define ZBX_PROTO_VALUE_GET_ACTIVE_CHECKS   &amp;quot;active checks&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_CONFIG        &amp;quot;proxy config&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_HEARTBEAT     &amp;quot;proxy heartbeat&amp;quot;
        #define ZBX_PROTO_VALUE_SENDER_DATA     &amp;quot;sender data&amp;quot;
        #define ZBX_PROTO_VALUE_AGENT_DATA      &amp;quot;agent data&amp;quot;
        #define ZBX_PROTO_VALUE_COMMAND         &amp;quot;command&amp;quot;
        #define ZBX_PROTO_VALUE_JAVA_GATEWAY_INTERNAL   &amp;quot;java gateway internal&amp;quot;
        #define ZBX_PROTO_VALUE_JAVA_GATEWAY_JMX    &amp;quot;java gateway jmx&amp;quot;
        #define ZBX_PROTO_VALUE_GET_QUEUE       &amp;quot;queue.get&amp;quot;
        #define ZBX_PROTO_VALUE_GET_STATUS      &amp;quot;status.get&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_DATA      &amp;quot;proxy data&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_TASKS     &amp;quot;proxy tasks&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;proxy&#34;&gt;Proxy&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;入口函数：zabbix_proxy.c:MAIN_ZABBIX_ENTRY&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置同步线程：proxyconfig.c: ZBX_THREAD_ENTRY(proxyconfig_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.主要处理函数：proxyconfig.c: process_configuration_sync
2.发送request为#define ZBX_PROTO_VALUE_PROXY_CONFIG        &amp;quot;proxy config&amp;quot;的配置同步请求消息，消息头和消息体的格式和agent消息格式一样，见agent段落的第3节
3.接收对端的配置同步响应消息，并解析消息体中的json段
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;心跳线程：heartbeat.c:ZBX_THREAD_ENTRY(heart_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;发送request为#define ZBX_PROTO_VALUE_PROXY_HEARTBEAT       &amp;quot;proxy heartbeat&amp;quot;的心跳消息
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;发送线程：datasender.c: ZBX_THREAD_ENTRY(datasender_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.主要处理函数：datasender.c: proxy_data_sender
2.发送request为#define ZBX_PROTO_VALUE_PROXY_DATA      &amp;quot;proxy data&amp;quot;的消息，消息头和消息体的格式和agent消息格式一样，见agent段落的第3节
3.发送的消息体包括下面4个类型的数据，数据源主要从db中获取
    #define ZBX_DATASENDER_AVAILABILITY     0x0001
    #define ZBX_DATASENDER_HISTORY          0x0002
    #define ZBX_DATASENDER_DISCOVERY        0x0004
    #define ZBX_DATASENDER_AUTOREGISTRATION     0x0008
4.从数据库中获取remotetasks，zbx_tm_get_remote_tasks，根据获取的task组织json消息体zbx_tm_json_serialize_tasks
5.接收对端的响应消息，解析消息体中的json段，并更新db中的task数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;poller线程：poller.c: ZBX_THREAD_ENTRY(poller_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.poller.c: get_values，从队列中获取数据项串并解析substitute_simple_macros，根据接口类型(snmp,java等)获取数值get_values_snmp，get_values_java
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;trapper线程：trapper.c:ZBX_THREAD_ENTRY(trapper_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.解析各类响应消息并对应处理：trapper.c:process_trap
2.消息体格式分为json格式，ZBX_GET_ACTIVE_CHECKS开头格式，xml格式，host:key:value格式
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pinger线程：pinger.c:ZBX_THREAD_ENTRY(pinger_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.从snmp或者java接口中获取数据
2.Icmp.c:process_ping，写数据到zbx_get_thread_id()i.pinger文件中
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;housekeeper_thread线程：housekeeper.c:ZBX_THREAD_ENTRY(pinger_thread, args)
    1.连接数据库删除历史数据&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;discoverer线程：httppoller.c:ZBX_THREAD_ENTRY(httppoller_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.数据库操作，获取新主机
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dbsyncer线程：dbsyncer.c:ZBX_THREAD_ENTRY(dbsyncer_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.同步数据库和内存
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;snmptrapper线程：snmptrapper.c: ZBX_THREAD_ENTRY(snmptrapper_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.读取snmptrapper文件中的数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;selfmon线程：selfmon.c: ZBX_THREAD_ENTRY(selfmon_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.收集selfmon统计数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vmware线程：selfmon.c: ZBX_THREAD_ENTRY(vmware_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.收集vmware统计数据，使用soap协议
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;sever-proxy的交互&#34;&gt;Sever: proxy的交互&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;入口函数Proxypoll.c:ZBX_THREAD_ENTRY(proxypoller_thread, args)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;主要处理函数process_proxy，发送报文send_data_to_proxy，接收报文recv_data_from_proxy，回proxy响应zbx_send_proxy_data_response，报文格式仍然为json格式，同agent的第3部分&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>UML</title>
          <link>https://kingjcy.github.io/post/architecture/map/uml/</link>
          <pubDate>Wed, 08 Nov 2017 11:40:49 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/map/uml/</guid>
          <description>&lt;p&gt;UML（Unified Modeling Language）是一种统一建模语言，为面向对象开发系统的产品进行说明、可视化、和编制文档的一种标准语言。下面将对UML的九种图的基本概念进行介绍以及各个图的使用场景。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念　　&lt;/h1&gt;

&lt;p&gt;如下图所示，UML图分为用例视图、设计视图、进程视图、实现视图和拓扑视图，又可以静动分为静态视图和动态视图。&lt;/p&gt;

&lt;p&gt;静态图分为：用例图，类图，对象图，包图，构件图，部署图。&lt;/p&gt;

&lt;p&gt;动态图分为：状态图，活动图，协作图，序列图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;用例图-usecase-diagrams&#34;&gt;用例图（UseCase Diagrams）&lt;/h2&gt;

&lt;p&gt;用例图主要回答了两个问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、是谁用软件。
2、软件的功能。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从用户的角度描述了系统的功能，并指出各个功能的执行者，强调用户的使用者，系统为执行者完成哪些功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;类图-class-diagrams&#34;&gt;类图（Class Diagrams）&lt;/h2&gt;

&lt;p&gt;用户根据用例图抽象成类，描述类的内部结构和类与类之间的关系，是一种静态结构图。 在UML类图中，常见的有以下几种关系: 泛化（Generalization）, 实现（Realization），关联（Association)，聚合（Aggregation），组合(Composition)，依赖(Dependency)。&lt;/p&gt;

&lt;p&gt;各种关系的强弱顺序： 泛化 = 实现 &amp;gt; 组合 &amp;gt; 聚合 &amp;gt; 关联 &amp;gt; 依赖&lt;/p&gt;

&lt;p&gt;1.泛化&lt;/p&gt;

&lt;p&gt;泛化关系：是一种继承关系，表示一般与特殊的关系，它指定了子类如何继承父类的所有特征和行为。例如：老虎是动物的一种，即有老虎的特性也有动物的共性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.实现&lt;/p&gt;

&lt;p&gt;实现关系：是一种类与接口的关系，表示类是接口所有特征和行为的实现。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3.关联&lt;/p&gt;

&lt;p&gt;关联关系：是一种拥有的关系，它使一个类知道另一个类的属性和方法；如：老师与学生，丈夫与妻子关联可以是双向的，也可以是单向的。双向的关联可以有两个箭头或者没有箭头，单向的关联有一个箭头。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.共享聚合　&lt;/p&gt;

&lt;p&gt;聚合关系：是整体与部分的关系，且部分可以离开整体而单独存在。如车和轮胎是整体和部分的关系，轮胎离开车仍然可以存在。&lt;/p&gt;

&lt;p&gt;聚合关系是关联关系的一种，是强的关联关系；关联和聚合在语法上无法区分，必须考察具体的逻辑关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;5.组合集合&lt;/p&gt;

&lt;p&gt;组合关系：是整体与部分的关系，但部分不能离开整体而单独存在。如公司和部门是整体和部分的关系，没有公司就不存在部门。&lt;/p&gt;

&lt;p&gt;组合关系是关联关系的一种，是比聚合关系还要强的关系，它要求普通的聚合关系中代表整体的对象负责代表部分的对象的生命周期。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml6.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;6.依赖　　&lt;/p&gt;

&lt;p&gt;依赖关系：是一种使用的关系，即一个类的实现需要另一个类的协助，所以要尽量不使用双向的互相依赖.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;7 各种类图关系&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml8.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;对象图-object-diagrams&#34;&gt;对象图（Object Diagrams）&lt;/h2&gt;

&lt;p&gt;描述的是参与交互的各个对象在交互过程中某一时刻的状态。对象图可以被看作是类图在某一时刻的实例。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml9.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;状态图-statechart-diagrams&#34;&gt;状态图（Statechart Diagrams）&lt;/h2&gt;

&lt;p&gt;一种由状态、变迁、事件和活动组成的状态机，用来描述类的对象所有可能的状态以及时间发生时状态的转移条件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml10.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;活动图-activity-diagrams&#34;&gt;活动图（Activity Diagrams）：&lt;/h2&gt;

&lt;p&gt;状态图的一种特殊情况，这些状态大都处于活动状态。本质是一种流程图，它描述了活动到活动的控制流。　　　　&lt;/p&gt;

&lt;p&gt;交互图强调的是对象到对象的控制流，而活动图则强调的是从活动到活动的控制流。&lt;/p&gt;

&lt;p&gt;活动图是一种表述过程基理、业务过程以及工作流的技术。它可以用来对业务过程、工作流建模，也可以对用例实现甚至是程序实现来建模。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml11.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1.带泳道的活动图&lt;/p&gt;

&lt;p&gt;泳道表明每个活动是由哪些人或哪些部门负责完成。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml12.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.带对象流的活动图&lt;/p&gt;

&lt;p&gt;用活动图描述某个对象时，可以把涉及到的对象放置在活动图中，并用一个依赖将其连接到进行创建、修改和撤销的动作状态或者活动状态上，对象的这种使用方法就构成了对象流。对象流用带有箭头的虚线表示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml13.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;序列图-时序图-sequence-diagrams&#34;&gt;序列图-时序图（Sequence Diagrams）&lt;/h2&gt;

&lt;p&gt;交互图的一种，描述了对象之间消息发送的先后顺序，强调时间顺序。&lt;/p&gt;

&lt;p&gt;序列图的主要用途是把用例表达的需求，转化为进一步、更加正式层次的精细表达。用例常常被细化为一个或者更多的序列图。同时序列图更有效地描述如何分配各个类的职责以及各类具有相应职责的原因。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml14.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;消息用从一个对象的生命线到另一个对象生命线的箭头表示。箭头以时间顺序在图中从上到下排列。&lt;/p&gt;

&lt;p&gt;序列图中涉及的元素：&lt;/p&gt;

&lt;p&gt;1.角色&lt;/p&gt;

&lt;p&gt;系统角色，可以是人、及其甚至其他的系统或者子系统&lt;/p&gt;

&lt;p&gt;2.对象&lt;/p&gt;

&lt;p&gt;对象包括三种命名方式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  第一种方式包括对象名和类名；

  第二中方式只显示类名不显示对象名，即表示他是一个匿名对象；

  第三种方式只显示对象名不显示类。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.生命线&lt;/p&gt;

&lt;p&gt;生命线在顺序图中表示为从对象图标向下延伸的一条虚线，表示对象存在的时间。&lt;/p&gt;

&lt;p&gt;生命线名称可带下划线。当使用下划线时，意味着序列图中的生命线代表一个类的特定实例。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml15.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.控制焦点&lt;/p&gt;

&lt;p&gt;控制焦点是顺序图中表示时间段的符号，在这个时间段内对象将执行相应的操作。用小矩形表示&lt;/p&gt;

&lt;p&gt;5.同步消息&lt;/p&gt;

&lt;p&gt;同步等待消息&lt;/p&gt;

&lt;p&gt;消息的发送者把控制传递给消息的接收者，然后停止活动，等待消息的接收者放弃或者返回控制。用来表示同步的意义。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml16.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;6.异步消息&lt;/p&gt;

&lt;p&gt;异步发送消息，不需等待&lt;/p&gt;

&lt;p&gt;消息发送者通过消息把信号传递给消息的接收者，然后继续自己的活动，不等待接受者返回消息或者控制。异步消息的接收者和发送者是并发工作的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml17.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;7.注释&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml18.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;8.约束&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml19.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;9.组合　　&lt;/p&gt;

&lt;p&gt;组合片段用来解决交互执行的条件及方式。它允许在序列图中直接表示逻辑组件，用于通过指定条件或子进程的应用区域，为任何生命线的任何部分定义特殊条件和子进程。常用的组合片段有：抉择、选项、循环、并行。&lt;/p&gt;

&lt;h2 id=&#34;协作图-collaboration-diagrams&#34;&gt;协作图（Collaboration Diagrams）&lt;/h2&gt;

&lt;p&gt;交互图的一种，描述了收发消息的对象的组织关系，强调对象之间的合作关系。时序图按照时间顺序布图，而写作图按照空间结构布图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml20.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;构件图-component-diagrams&#34;&gt;构件图（Component Diagrams）：&lt;/h2&gt;

&lt;p&gt;构件图是用来表示系统中构件与构件之间，类或接口与构件之间的关系图。其中，构建图之间的关系表现为依赖关系，定义的类或接口与类之间的关系表现为依赖关系或实现关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml21.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署图-deployment-diagrams&#34;&gt;部署图（Deployment Diagrams）：&lt;/h2&gt;

&lt;p&gt;描述了系统运行时进行处理的结点以及在结点上活动的构件的配置。强调了物理设备以及之间的连接关系。&lt;/p&gt;

&lt;p&gt;部署模型的目的：&lt;/p&gt;

&lt;p&gt;描述一个具体应用的主要部署结构，通过对各种硬件，在硬件中的软件以及各种连接协议的显示，可以很好的描述系统是如何部署的；平衡系统运行时的计算资源分布；可以通过连接描述组织的硬件网络结构或者是嵌入式系统等具有多种硬件和软件相关的系统运行模型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml22.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Net/Http 应用层</title>
          <link>https://kingjcy.github.io/post/golang/go-net-http/</link>
          <pubDate>Tue, 26 Sep 2017 17:05:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-net-http/</guid>
          <description>&lt;p&gt;http包提供了HTTP协议的客户端和服务端的实现。&lt;/p&gt;

&lt;h1 id=&#34;http客户端&#34;&gt;HTTP客户端&lt;/h1&gt;

&lt;h2 id=&#34;直接使用http方法&#34;&gt;直接使用http方法&lt;/h2&gt;

&lt;p&gt;直接使用http方法，其实就是使用标准库默认的结构体client，transport等来实现请求。&lt;/p&gt;

&lt;p&gt;http包中封装了Get、Head、Post和PostForm函数可以直接发出HTTP/ HTTPS请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resp, err := http.Get(&amp;quot;http://example.com/&amp;quot;)
...
resp, err := http.Post(&amp;quot;http://example.com/upload&amp;quot;, &amp;quot;image/jpeg&amp;quot;, &amp;amp;buf)
...
resp, err := http.PostForm(&amp;quot;http://example.com/form&amp;quot;,
    url.Values{&amp;quot;key&amp;quot;: {&amp;quot;Value&amp;quot;}, &amp;quot;id&amp;quot;: {&amp;quot;123&amp;quot;}})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序在使用完回复后必须关闭回复的主体。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resp, err := http.Get(&amp;quot;http://example.com/&amp;quot;)
if err != nil {
    // handle error
}
defer resp.Body.Close()
body, err := ioutil.ReadAll(resp.Body)
// ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;原理解析&#34;&gt;原理解析&lt;/h3&gt;

&lt;p&gt;http直接提供的Post等方法实现在client.go文件中，以Post为例，其他都是一样&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Post(url, contentType string, body io.Reader) (resp *Response, err error) {
    return DefaultClient.Post(url, contentType, body)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际是调用了默认结构体client的Post方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var DefaultClient = &amp;amp;Client{}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看Post方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) Post(url, contentType string, body io.Reader) (resp *Response, err error) {
    req, err := NewRequest(&amp;quot;POST&amp;quot;, url, body)
    if err != nil {
        return nil, err
    }
    req.Header.Set(&amp;quot;Content-Type&amp;quot;, contentType)
    return c.Do(req)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据url和请求体body新建一个reqest，然后调用DefaultClient的Do方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) Do(req *Request) (*Response, error) {
    return c.do(req)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用内部的do方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) do(req *Request) (retres *Response, reterr error) {
    if testHookClientDoResult != nil {
        defer func() { testHookClientDoResult(retres, reterr) }()
    }
    if req.URL == nil {
        req.closeBody()
        return nil, &amp;amp;url.Error{
            Op:  urlErrorOp(req.Method),
            Err: errors.New(&amp;quot;http: nil Request.URL&amp;quot;),
        }
    }

    var (
        deadline      = c.deadline()
        reqs          []*Request
        resp          *Response
        copyHeaders   = c.makeHeadersCopier(req)
        reqBodyClosed = false // have we closed the current req.Body?

        // Redirect behavior:
        redirectMethod string
        includeBody    bool
    )
    uerr := func(err error) error {
        // the body may have been closed already by c.send()
        if !reqBodyClosed {
            req.closeBody()
        }
        var urlStr string
        if resp != nil &amp;amp;&amp;amp; resp.Request != nil {
            urlStr = stripPassword(resp.Request.URL)
        } else {
            urlStr = stripPassword(req.URL)
        }
        return &amp;amp;url.Error{
            Op:  urlErrorOp(reqs[0].Method),
            URL: urlStr,
            Err: err,
        }
    }
    for {
        // For all but the first request, create the next
        // request hop and replace req.
        if len(reqs) &amp;gt; 0 {
            loc := resp.Header.Get(&amp;quot;Location&amp;quot;)
            if loc == &amp;quot;&amp;quot; {
                resp.closeBody()
                return nil, uerr(fmt.Errorf(&amp;quot;%d response missing Location header&amp;quot;, resp.StatusCode))
            }
            u, err := req.URL.Parse(loc)
            if err != nil {
                resp.closeBody()
                return nil, uerr(fmt.Errorf(&amp;quot;failed to parse Location header %q: %v&amp;quot;, loc, err))
            }
            host := &amp;quot;&amp;quot;
            if req.Host != &amp;quot;&amp;quot; &amp;amp;&amp;amp; req.Host != req.URL.Host {
                // If the caller specified a custom Host header and the
                // redirect location is relative, preserve the Host header
                // through the redirect. See issue #22233.
                if u, _ := url.Parse(loc); u != nil &amp;amp;&amp;amp; !u.IsAbs() {
                    host = req.Host
                }
            }
            ireq := reqs[0]
            req = &amp;amp;Request{
                Method:   redirectMethod,
                Response: resp,
                URL:      u,
                Header:   make(Header),
                Host:     host,
                Cancel:   ireq.Cancel,
                ctx:      ireq.ctx,
            }
            if includeBody &amp;amp;&amp;amp; ireq.GetBody != nil {
                req.Body, err = ireq.GetBody()
                if err != nil {
                    resp.closeBody()
                    return nil, uerr(err)
                }
                req.ContentLength = ireq.ContentLength
            }

            // Copy original headers before setting the Referer,
            // in case the user set Referer on their first request.
            // If they really want to override, they can do it in
            // their CheckRedirect func.
            copyHeaders(req)

            // Add the Referer header from the most recent
            // request URL to the new one, if it&#39;s not https-&amp;gt;http:
            if ref := refererForURL(reqs[len(reqs)-1].URL, req.URL); ref != &amp;quot;&amp;quot; {
                req.Header.Set(&amp;quot;Referer&amp;quot;, ref)
            }
            err = c.checkRedirect(req, reqs)

            // Sentinel error to let users select the
            // previous response, without closing its
            // body. See Issue 10069.
            if err == ErrUseLastResponse {
                return resp, nil
            }

            // Close the previous response&#39;s body. But
            // read at least some of the body so if it&#39;s
            // small the underlying TCP connection will be
            // re-used. No need to check for errors: if it
            // fails, the Transport won&#39;t reuse it anyway.
            const maxBodySlurpSize = 2 &amp;lt;&amp;lt; 10
            if resp.ContentLength == -1 || resp.ContentLength &amp;lt;= maxBodySlurpSize {
                io.CopyN(ioutil.Discard, resp.Body, maxBodySlurpSize)
            }
            resp.Body.Close()

            if err != nil {
                // Special case for Go 1 compatibility: return both the response
                // and an error if the CheckRedirect function failed.
                // See https://golang.org/issue/3795
                // The resp.Body has already been closed.
                ue := uerr(err)
                ue.(*url.Error).URL = loc
                return resp, ue
            }
        }

        reqs = append(reqs, req)
        var err error
        var didTimeout func() bool
        //调用 send
        if resp, didTimeout, err = c.send(req, deadline); err != nil {
            // c.send() always closes req.Body
            reqBodyClosed = true
            if !deadline.IsZero() &amp;amp;&amp;amp; didTimeout() {
                err = &amp;amp;httpError{
                    // TODO: early in cycle: s/Client.Timeout exceeded/timeout or context cancelation/
                    err:     err.Error() + &amp;quot; (Client.Timeout exceeded while awaiting headers)&amp;quot;,
                    timeout: true,
                }
            }
            return nil, uerr(err)
        }

        var shouldRedirect bool
        redirectMethod, shouldRedirect, includeBody = redirectBehavior(req.Method, resp, reqs[0])
        if !shouldRedirect {
            return resp, nil
        }

        req.closeBody()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用send方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) send(req *Request, deadline time.Time) (resp *Response, didTimeout func() bool, err error) {
    if c.Jar != nil {
        for _, cookie := range c.Jar.Cookies(req.URL) {
            req.AddCookie(cookie)
        }
    }
    resp, didTimeout, err = send(req, c.transport(), deadline)
    if err != nil {
        return nil, didTimeout, err
    }
    if c.Jar != nil {
        if rc := resp.Cookies(); len(rc) &amp;gt; 0 {
            c.Jar.SetCookies(req.URL, rc)
        }
    }
    return resp, nil, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边需要确定实现transport的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) transport() RoundTripper {
    if c.Transport != nil {
        return c.Transport
    }
    return DefaultTransport
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用默认的DefaultTransport（如果transport自定义了，就使用自定义的，否则使用默认的），这边这个接口调用就是DefaultTransport，也就是Transport.go中的Transport结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var DefaultTransport RoundTripper = &amp;amp;Transport{
    Proxy: ProxyFromEnvironment,
    DialContext: (&amp;amp;net.Dialer{
        Timeout:   30 * time.Second,
        KeepAlive: 30 * time.Second,
        DualStack: true,
    }).DialContext,
    MaxIdleConns:          100,
    IdleConnTimeout:       90 * time.Second,
    TLSHandshakeTimeout:   10 * time.Second,
    ExpectContinueTimeout: 1 * time.Second,
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看Transport结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Transport struct {
    idleMu     sync.Mutex
    wantIdle   bool                                // user has requested to close all idle conns
    idleConn   map[connectMethodKey][]*persistConn // most recently used at end
    idleConnCh map[connectMethodKey]chan *persistConn
    idleLRU    connLRU

    reqMu       sync.Mutex
    reqCanceler map[*Request]func(error)

    altMu    sync.Mutex   // guards changing altProto only
    altProto atomic.Value // of nil or map[string]RoundTripper, key is URI scheme

    connCountMu          sync.Mutex
    connPerHostCount     map[connectMethodKey]int
    connPerHostAvailable map[connectMethodKey]chan struct{}

    // Proxy specifies a function to return a proxy for a given
    // Request. If the function returns a non-nil error, the
    // request is aborted with the provided error.
    //
    // The proxy type is determined by the URL scheme. &amp;quot;http&amp;quot;,
    // &amp;quot;https&amp;quot;, and &amp;quot;socks5&amp;quot; are supported. If the scheme is empty,
    // &amp;quot;http&amp;quot; is assumed.
    //
    // If Proxy is nil or returns a nil *URL, no proxy is used.
    Proxy func(*Request) (*url.URL, error)

    // DialContext specifies the dial function for creating unencrypted TCP connections.
    // If DialContext is nil (and the deprecated Dial below is also nil),
    // then the transport dials using package net.
    //
    // DialContext runs concurrently with calls to RoundTrip.
    // A RoundTrip call that initiates a dial may end up using
    // a connection dialed previously when the earlier connection
    // becomes idle before the later DialContext completes.
    DialContext func(ctx context.Context, network, addr string) (net.Conn, error)

    // Dial specifies the dial function for creating unencrypted TCP connections.
    //
    // Dial runs concurrently with calls to RoundTrip.
    // A RoundTrip call that initiates a dial may end up using
    // a connection dialed previously when the earlier connection
    // becomes idle before the later Dial completes.
    //
    // Deprecated: Use DialContext instead, which allows the transport
    // to cancel dials as soon as they are no longer needed.
    // If both are set, DialContext takes priority.
    Dial func(network, addr string) (net.Conn, error)

    // DialTLS specifies an optional dial function for creating
    // TLS connections for non-proxied HTTPS requests.
    //
    // If DialTLS is nil, Dial and TLSClientConfig are used.
    //
    // If DialTLS is set, the Dial hook is not used for HTTPS
    // requests and the TLSClientConfig and TLSHandshakeTimeout
    // are ignored. The returned net.Conn is assumed to already be
    // past the TLS handshake.
    DialTLS func(network, addr string) (net.Conn, error)

    // TLSClientConfig specifies the TLS configuration to use with
    // tls.Client.
    // If nil, the default configuration is used.
    // If non-nil, HTTP/2 support may not be enabled by default.
    TLSClientConfig *tls.Config

    // TLSHandshakeTimeout specifies the maximum amount of time waiting to
    // wait for a TLS handshake. Zero means no timeout.
    TLSHandshakeTimeout time.Duration

    // DisableKeepAlives, if true, disables HTTP keep-alives and
    // will only use the connection to the server for a single
    // HTTP request.
    //
    // This is unrelated to the similarly named TCP keep-alives.
    DisableKeepAlives bool

    // DisableCompression, if true, prevents the Transport from
    // requesting compression with an &amp;quot;Accept-Encoding: gzip&amp;quot;
    // request header when the Request contains no existing
    // Accept-Encoding value. If the Transport requests gzip on
    // its own and gets a gzipped response, it&#39;s transparently
    // decoded in the Response.Body. However, if the user
    // explicitly requested gzip it is not automatically
    // uncompressed.
    DisableCompression bool

    // MaxIdleConns controls the maximum number of idle (keep-alive)
    // connections across all hosts. Zero means no limit.
    MaxIdleConns int

    // MaxIdleConnsPerHost, if non-zero, controls the maximum idle
    // (keep-alive) connections to keep per-host. If zero,
    // DefaultMaxIdleConnsPerHost is used.
    MaxIdleConnsPerHost int

    // MaxConnsPerHost optionally limits the total number of
    // connections per host, including connections in the dialing,
    // active, and idle states. On limit violation, dials will block.
    //
    // Zero means no limit.
    //
    // For HTTP/2, this currently only controls the number of new
    // connections being created at a time, instead of the total
    // number. In practice, hosts using HTTP/2 only have about one
    // idle connection, though.
    MaxConnsPerHost int

    // IdleConnTimeout is the maximum amount of time an idle
    // (keep-alive) connection will remain idle before closing
    // itself.
    // Zero means no limit.
    IdleConnTimeout time.Duration

    // ResponseHeaderTimeout, if non-zero, specifies the amount of
    // time to wait for a server&#39;s response headers after fully
    // writing the request (including its body, if any). This
    // time does not include the time to read the response body.
    ResponseHeaderTimeout time.Duration

    // ExpectContinueTimeout, if non-zero, specifies the amount of
    // time to wait for a server&#39;s first response headers after fully
    // writing the request headers if the request has an
    // &amp;quot;Expect: 100-continue&amp;quot; header. Zero means no timeout and
    // causes the body to be sent immediately, without
    // waiting for the server to approve.
    // This time does not include the time to send the request header.
    ExpectContinueTimeout time.Duration

    // TLSNextProto specifies how the Transport switches to an
    // alternate protocol (such as HTTP/2) after a TLS NPN/ALPN
    // protocol negotiation. If Transport dials an TLS connection
    // with a non-empty protocol name and TLSNextProto contains a
    // map entry for that key (such as &amp;quot;h2&amp;quot;), then the func is
    // called with the request&#39;s authority (such as &amp;quot;example.com&amp;quot;
    // or &amp;quot;example.com:1234&amp;quot;) and the TLS connection. The function
    // must return a RoundTripper that then handles the request.
    // If TLSNextProto is not nil, HTTP/2 support is not enabled
    // automatically.
    TLSNextProto map[string]func(authority string, c *tls.Conn) RoundTripper

    // ProxyConnectHeader optionally specifies headers to send to
    // proxies during CONNECT requests.
    ProxyConnectHeader Header

    // MaxResponseHeaderBytes specifies a limit on how many
    // response bytes are allowed in the server&#39;s response
    // header.
    //
    // Zero means to use a default limit.
    MaxResponseHeaderBytes int64

    // nextProtoOnce guards initialization of TLSNextProto and
    // h2transport (via onceSetNextProtoDefaults)
    nextProtoOnce sync.Once
    h2transport   h2Transport // non-nil if http2 wired up
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;中文讲解&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Transport struct {
    // Proxy指定一个对给定请求返回代理的函数。
    // 如果该函数返回了非nil的错误值，请求的执行就会中断并返回该错误。
    // 如果Proxy为nil或返回nil的*URL置，将不使用代理。
    Proxy func(*Request) (*url.URL, error)

    // Dial指定创建TCP连接的拨号函数。如果Dial为nil，会使用net.Dial。
    //Dial获取一个tcp 连接，也就是net.Conn结构，你就记住可以往里面写request
    //然后从里面搞到response就行了
    Dial func(network, addr string) (net.Conn, error)

    // TLSClientConfig指定用于tls.Client的TLS配置信息。
    // 如果该字段为nil，会使用默认的配置信息。
    TLSClientConfig *tls.Config

    // TLSHandshakeTimeout指定等待TLS握手完成的最长时间。零值表示不设置超时。
    TLSHandshakeTimeout time.Duration

    // 如果DisableKeepAlives为真，会禁止不同HTTP请求之间TCP连接的重用。
    DisableKeepAlives bool

    // 如果DisableCompression为真，会禁止Transport在请求中没有Accept-Encoding头时，
    // 主动添加&amp;quot;Accept-Encoding: gzip&amp;quot;头，以获取压缩数据。
    // 如果Transport自己请求gzip并得到了压缩后的回复，它会主动解压缩回复的主体。
    // 但如果用户显式的请求gzip压缩数据，Transport是不会主动解压缩的。
    DisableCompression bool

    // 如果MaxIdleConnsPerHost!=0，会控制每个主机下的最大闲置连接。
    // 如果MaxIdleConnsPerHost==0，会使用DefaultMaxIdleConnsPerHost。
    MaxIdleConnsPerHost int

    // ResponseHeaderTimeout指定在发送完请求（包括其可能的主体）之后，
    // 等待接收服务端的回复的头域的最大时间。零值表示不设置超时。
    // 该时间不包括获取回复主体的时间。
    ResponseHeaderTimeout time.Duration

    // 内含隐藏或非导出字段



    //保存从 connectMethodKey （代表着不同的协议 不同的host，也就是不同的请求）到 persistConn 的映射
    idleConn   map[connectMethodKey][]*persistConn // most recently used at end
    //用来在并发http请求的时候在多个 goroutine 里面相互发送持久连接，也就是说， 这些持久连接是可以重复利用的， 你的http请求用某个persistConn用完了，通过这个channel发送给其他http请求使用这个persistConn，然后我们找到transport的RoundTrip方法
    idleConnCh map[connectMethodKey]chan *persistConn
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用内部send方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func send(ireq *Request, rt RoundTripper, deadline time.Time) (resp *Response, didTimeout func() bool, err error) {
    req := ireq // req is either the original request, or a modified fork

    if rt == nil {
        req.closeBody()
        return nil, alwaysFalse, errors.New(&amp;quot;http: no Client.Transport or DefaultTransport&amp;quot;)
    }

    if req.URL == nil {
        req.closeBody()
        return nil, alwaysFalse, errors.New(&amp;quot;http: nil Request.URL&amp;quot;)
    }

    if req.RequestURI != &amp;quot;&amp;quot; {
        req.closeBody()
        return nil, alwaysFalse, errors.New(&amp;quot;http: Request.RequestURI can&#39;t be set in client requests.&amp;quot;)
    }

    // forkReq forks req into a shallow clone of ireq the first
    // time it&#39;s called.
    forkReq := func() {
        if ireq == req {
            req = new(Request)
            *req = *ireq // shallow clone
        }
    }

    // Most the callers of send (Get, Post, et al) don&#39;t need
    // Headers, leaving it uninitialized. We guarantee to the
    // Transport that this has been initialized, though.
    if req.Header == nil {
        forkReq()
        req.Header = make(Header)
    }

    if u := req.URL.User; u != nil &amp;amp;&amp;amp; req.Header.Get(&amp;quot;Authorization&amp;quot;) == &amp;quot;&amp;quot; {
        username := u.Username()
        password, _ := u.Password()
        forkReq()
        req.Header = ireq.Header.clone()
        req.Header.Set(&amp;quot;Authorization&amp;quot;, &amp;quot;Basic &amp;quot;+basicAuth(username, password))
    }

    if !deadline.IsZero() {
        forkReq()
    }
    stopTimer, didTimeout := setRequestCancel(req, rt, deadline)

    resp, err = rt.RoundTrip(req)
    if err != nil {
        stopTimer()
        if resp != nil {
            log.Printf(&amp;quot;RoundTripper returned a response &amp;amp; error; ignoring response&amp;quot;)
        }
        if tlsErr, ok := err.(tls.RecordHeaderError); ok {
            // If we get a bad TLS record header, check to see if the
            // response looks like HTTP and give a more helpful error.
            // See golang.org/issue/11111.
            if string(tlsErr.RecordHeader[:]) == &amp;quot;HTTP/&amp;quot; {
                err = errors.New(&amp;quot;http: server gave HTTP response to HTTPS client&amp;quot;)
            }
        }
        return nil, didTimeout, err
    }
    if !deadline.IsZero() {
        resp.Body = &amp;amp;cancelTimerBody{
            stop:          stopTimer,
            rc:            resp.Body,
            reqDidTimeout: didTimeout,
        }
    }
    return resp, nil, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用DefaultTransport也就是Transport.go中的Transport结构体的RoundTrip方法（当出现自定义的时候，就调用对应的Transport的RoundTrip方法，这边直接使用这个借口就是DefaultTransport），可见使用golang net/http库发送http请求，最后都是调用 http transport的 RoundTrip方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// roundTrip implements a RoundTripper over HTTP.
func (t *Transport) roundTrip(req *Request) (*Response, error) {
    t.nextProtoOnce.Do(t.onceSetNextProtoDefaults)
    ctx := req.Context()
    trace := httptrace.ContextClientTrace(ctx)

    if req.URL == nil {
        req.closeBody()
        return nil, errors.New(&amp;quot;http: nil Request.URL&amp;quot;)
    }
    if req.Header == nil {
        req.closeBody()
        return nil, errors.New(&amp;quot;http: nil Request.Header&amp;quot;)
    }
    scheme := req.URL.Scheme
    isHTTP := scheme == &amp;quot;http&amp;quot; || scheme == &amp;quot;https&amp;quot;
    if isHTTP {
        for k, vv := range req.Header {
            if !httpguts.ValidHeaderFieldName(k) {
                return nil, fmt.Errorf(&amp;quot;net/http: invalid header field name %q&amp;quot;, k)
            }
            for _, v := range vv {
                if !httpguts.ValidHeaderFieldValue(v) {
                    return nil, fmt.Errorf(&amp;quot;net/http: invalid header field value %q for key %v&amp;quot;, v, k)
                }
            }
        }
    }

    if t.useRegisteredProtocol(req) {
        altProto, _ := t.altProto.Load().(map[string]RoundTripper)
        if altRT := altProto[scheme]; altRT != nil {
            if resp, err := altRT.RoundTrip(req); err != ErrSkipAltProtocol {
                return resp, err
            }
        }
    }
    if !isHTTP {
        req.closeBody()
        return nil, &amp;amp;badStringError{&amp;quot;unsupported protocol scheme&amp;quot;, scheme}
    }
    if req.Method != &amp;quot;&amp;quot; &amp;amp;&amp;amp; !validMethod(req.Method) {
        return nil, fmt.Errorf(&amp;quot;net/http: invalid method %q&amp;quot;, req.Method)
    }
    if req.URL.Host == &amp;quot;&amp;quot; {
        req.closeBody()
        return nil, errors.New(&amp;quot;http: no Host in request URL&amp;quot;)
    }

    for {
        select {
        case &amp;lt;-ctx.Done():
            req.closeBody()
            return nil, ctx.Err()
        default:
        }

        // treq gets modified by roundTrip, so we need to recreate for each retry.
        treq := &amp;amp;transportRequest{Request: req, trace: trace}
        cm, err := t.connectMethodForRequest(treq)
        if err != nil {
            req.closeBody()
            return nil, err
        }

        // Get the cached or newly-created connection to either the
        // host (for http or https), the http proxy, or the http proxy
        // pre-CONNECTed to https server. In any case, we&#39;ll be ready
        // to send it requests.
        pconn, err := t.getConn(treq, cm)
        if err != nil {
            t.setReqCanceler(req, nil)
            req.closeBody()
            return nil, err
        }

        var resp *Response
        if pconn.alt != nil {
            // HTTP/2 path.
            t.decHostConnCount(cm.key()) // don&#39;t count cached http2 conns toward conns per host
            t.setReqCanceler(req, nil)   // not cancelable with CancelRequest
            resp, err = pconn.alt.RoundTrip(req)
        } else {
            resp, err = pconn.roundTrip(treq)
        }
        if err == nil {
            return resp, nil
        }
        if !pconn.shouldRetryRequest(req, err) {
            // Issue 16465: return underlying net.Conn.Read error from peek,
            // as we&#39;ve historically done.
            if e, ok := err.(transportReadFromServerError); ok {
                err = e.err
            }
            return nil, err
        }
        testHookRoundTripRetried()

        // Rewind the body if we&#39;re able to.
        if req.GetBody != nil {
            newReq := *req
            var err error
            newReq.Body, err = req.GetBody()
            if err != nil {
                return nil, err
            }
            req = &amp;amp;newReq
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前面对输入的错误处理部分我们忽略， 其实就2步，先获取一个TCP长连接，所谓TCP长连接就是三次握手建立连接后不close而是一直保持重复使用（节约环保） 然后调用这个持久连接persistConn 这个struct的roundTrip方法。我们先看获取连接&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (t *Transport) getConn(req *Request, cm connectMethod) (*persistConn, error) {
    if pc := t.getIdleConn(cm); pc != nil {
        // set request canceler to some non-nil function so we
        // can detect whether it was cleared between now and when
        // we enter roundTrip
        t.setReqCanceler(req, func() {})
        return pc, nil
    }

    type dialRes struct {
        pc  *persistConn
        err error
    }
    dialc := make(chan dialRes)
    //定义了一个发送 persistConn的channel

    prePendingDial := prePendingDial
    postPendingDial := postPendingDial

    handlePendingDial := func() {
        if prePendingDial != nil {
            prePendingDial()
        }
        go func() {
            if v := &amp;lt;-dialc; v.err == nil {
                t.putIdleConn(v.pc)
            }
            if postPendingDial != nil {
                postPendingDial()
            }
        }()
    }

    cancelc := make(chan struct{})
    t.setReqCanceler(req, func() { close(cancelc) })

    // 启动了一个goroutine, 这个goroutine 获取里面调用dialConn搞到
    // persistConn, 然后发送到上面建立的channel  dialc里面，
    go func() {
        pc, err := t.dialConn(cm)
        dialc &amp;lt;- dialRes{pc, err}
    }()

    idleConnCh := t.getIdleConnCh(cm)
    select {
    case v := &amp;lt;-dialc:
        // dialc 我们的 dial 方法先搞到通过 dialc通道发过来了
        return v.pc, v.err
    case pc := &amp;lt;-idleConnCh:
        // 这里代表其他的http请求用完了归还的persistConn通过idleConnCh这个
        // channel发送来的
        handlePendingDial()
        return pc, nil
    case &amp;lt;-req.Cancel:
        handlePendingDial()
        return nil, errors.New(&amp;quot;net/http: request canceled while waiting for connection&amp;quot;)
    case &amp;lt;-cancelc:
        handlePendingDial()
        return nil, errors.New(&amp;quot;net/http: request canceled while waiting for connection&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里面的代码写的很有讲究 , 上面代码里面我也注释了， 定义了一个发送 persistConn的channel dialc， 启动了一个goroutine, 这个goroutine 获取里面调用dialConn搞到persistConn, 然后发送到dialc里面，主协程goroutine在 select里面监听多个channel,看看哪个通道里面先发过来 persistConn，就用哪个，然后return。&lt;/p&gt;

&lt;p&gt;这里要注意的是 idleConnCh 这个通道里面发送来的是其他的http请求用完了归还的persistConn， 如果从这个通道里面搞到了，dialc这个通道也等着发呢，不能浪费，就通过handlePendingDial这个方法把dialc通道里面的persistConn也发到idleConnCh，等待后续给其他http请求使用。&lt;/p&gt;

&lt;p&gt;每个新建的persistConn的时候都把tcp连接里地输入流，和输出流用br（br *bufio.Reader）,和bw(bw *bufio.Writer)包装了一下，往bw写就写到tcp输入流里面了，读输出流也是通过br读，并启动了读循环和写循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pconn.br = bufio.NewReader(noteEOFReader{pconn.conn, &amp;amp;pconn.sawEOF})
pconn.bw = bufio.NewWriter(pconn.conn)
go pconn.readLoop()
go pconn.writeLoop()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再看pconn.roundTrip 调用这个持久连接persistConn 这个struct的roundTrip方法。先瞄一下 persistConn 这个struct&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type persistConn struct {
    t        *Transport
    cacheKey connectMethodKey
    conn     net.Conn
    tlsState *tls.ConnectionState
    br       *bufio.Reader       // 从tcp输出流里面读
    sawEOF   bool                // whether we&#39;ve seen EOF from conn; owned by readLoop
    bw       *bufio.Writer       // 写到tcp输入流
     reqch    chan requestAndChan // 主goroutine 往channnel里面写，读循环从     
                                 // channnel里面接受
    writech  chan writeRequest   // 主goroutine 往channnel里面写                                      
                                 // 写循环从channel里面接受
    closech  chan struct{}       // 通知关闭tcp连接的channel 

    writeErrCh chan error

    lk                   sync.Mutex // guards following fields
    numExpectedResponses int
    closed               bool // whether conn has been closed
    broken               bool // an error has happened on this connection; marked broken so it&#39;s not reused.
    canceled             bool // whether this conn was broken due a CancelRequest
    // mutateHeaderFunc is an optional func to modify extra
    // headers on each outbound request before it&#39;s written. (the
    // original Request given to RoundTrip is not modified)
    mutateHeaderFunc func(Header)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;里面是各种channel, 用的是出神入化， 各位要好好理解一下，这里有三个goroutine，有两个channel writeRequest 和 requestAndChan&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type writeRequest struct {
    req *transportRequest
    ch  chan&amp;lt;- error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主goroutine 往writeRequest里面写，写循环从writeRequest里面接受&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type responseAndError struct {
    res *Response
    err error
}

type requestAndChan struct {
    req *Request
    ch  chan responseAndError
    addedGzip bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主goroutine 往requestAndChan里面写，读循环从requestAndChan里面接受。&lt;/p&gt;

&lt;p&gt;注意这里的channel都是双向channel，也就是channel 的struct里面有一个chan类型的字段， 比如 reqch chan requestAndChan 这里的 requestAndChan 里面的 ch chan responseAndError。&lt;/p&gt;

&lt;p&gt;这个是很牛叉，主 goroutine 通过 reqch 发送requestAndChan 给读循环，然后读循环搞到response后通过 requestAndChan 里面的通道responseAndError把response返给主goroutine，所以我画了一个双向箭头。&lt;/p&gt;

&lt;p&gt;我们研究一下代码，我理解下来其实就是三个goroutine通过channel互相协作的过程。&lt;/p&gt;

&lt;p&gt;主循环：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pc *persistConn) roundTrip(req *transportRequest) (resp *Response, err error) {
    ... 忽略
    // Write the request concurrently with waiting for a response,
    // in case the server decides to reply before reading our full
    // request body.
    writeErrCh := make(chan error, 1)
    pc.writech &amp;lt;- writeRequest{req, writeErrCh}
    //把request发送给写循环
    resc := make(chan responseAndError, 1)
    pc.reqch &amp;lt;- requestAndChan{req.Request, resc, requestedGzip}
    //发送给读循环
    var re responseAndError
    var respHeaderTimer &amp;lt;-chan time.Time
    cancelChan := req.Request.Cancel
WaitResponse:
    for {
        select {
        case err := &amp;lt;-writeErrCh:
            if isNetWriteError(err) {
                //写循环通过这个channel报告错误
                select {
                case re = &amp;lt;-resc:
                    pc.close()
                    break WaitResponse
                case &amp;lt;-time.After(50 * time.Millisecond):
                    // Fall through.
                }
            }
            if err != nil {
                re = responseAndError{nil, err}
                pc.close()
                break WaitResponse
            }
            if d := pc.t.ResponseHeaderTimeout; d &amp;gt; 0 {
                timer := time.NewTimer(d)
                defer timer.Stop() // prevent leaks
                respHeaderTimer = timer.C
            }
        case &amp;lt;-pc.closech:
            // 如果长连接挂了， 这里的channel有数据， 进入这个case, 进行处理

            select {
            case re = &amp;lt;-resc:
                if fn := testHookPersistConnClosedGotRes; fn != nil {
                    fn()
                }
            default:
                re = responseAndError{err: errClosed}
                if pc.isCanceled() {
                    re = responseAndError{err: errRequestCanceled}
                }
            }
            break WaitResponse
        case &amp;lt;-respHeaderTimer:
            pc.close()
            re = responseAndError{err: errTimeout}
            break WaitResponse
            // 如果timeout，这里的channel有数据， break掉for循环
        case re = &amp;lt;-resc:
            break WaitResponse
           // 获取到读循环的response, break掉 for循环
        case &amp;lt;-cancelChan:
            pc.t.CancelRequest(req.Request)
            cancelChan = nil
        }
    }

    if re.err != nil {
        pc.t.setReqCanceler(req.Request, nil)
    }
    return re.res, re.err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段代码主要就干了三件事&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;主goroutine -&amp;gt;requestAndChan -&amp;gt; 读循环goroutine

主goroutine -&amp;gt;writeRequest-&amp;gt; 写循环goroutine

主goroutine 通过select 监听各个channel上的数据， 比如请求取消， timeout，长连接挂了，写流出错，读流出错， 都是其他goroutine 发送过来的， 跟中断一样，然后相应处理，上面也提到了，有些channel是主goroutine通过channel发送给其他goroutine的struct里面包含的channel, 比如 case err := &amp;lt;-writeErrCh: case re = &amp;lt;-resc:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读循环代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pc *persistConn) readLoop() {

    ... 忽略
    alive := true
    for alive {

        ... 忽略
        rc := &amp;lt;-pc.reqch

        var resp *Response
        if err == nil {
            resp, err = ReadResponse(pc.br, rc.req)
            if err == nil &amp;amp;&amp;amp; resp.StatusCode == 100 {
                //100  Continue  初始的请求已经接受，客户应当继续发送请求的其 
                // 余部分
                resp, err = ReadResponse(pc.br, rc.req)
                // 读pc.br（tcp输出流）中的数据，这里的代码在response里面
                //解析statusCode，头字段， 转成标准的内存中的response 类型
                //  http在tcp数据流里面，head和body以 /r/n/r/n分开， 各个头
                // 字段 以/r/n分开
            }
        }

        if resp != nil {
            resp.TLS = pc.tlsState
        }

        ...忽略
        //上面处理一些http协议的一些逻辑行为，
        rc.ch &amp;lt;- responseAndError{resp, err} //把读到的response返回给    
                                             //主goroutine

        .. 忽略
        //忽略部分， 处理cancel req中断， 发送idleConnCh归还pc（持久连接）到持久连接池中（map）    
    pc.close()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;无关代码忽略，这段代码主要干了一件事情&lt;/p&gt;

&lt;p&gt;读循环goroutine 通过channel requestAndChan 接受主goroutine发送的request(rc := &amp;lt;-pc.reqch), 并从tcp输出流中读取response， 然后反序列化到结构体中， 最后通过channel 返给主goroutine (rc.ch &amp;lt;- responseAndError{resp, err} )&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pc *persistConn) writeLoop() {
    for {
        select {
        case wr := &amp;lt;-pc.writech:   //接受主goroutine的 request
            if pc.isBroken() {
                wr.ch &amp;lt;- errors.New(&amp;quot;http: can&#39;t write HTTP request on broken connection&amp;quot;)
                continue
            }
            err := wr.req.Request.write(pc.bw, pc.isProxy, wr.req.extra)   //写入tcp输入流
            if err == nil {
                err = pc.bw.Flush()
            }
            if err != nil {
                pc.markBroken()
                wr.req.Request.closeBody()
            }
            pc.writeErrCh &amp;lt;- err 
            wr.ch &amp;lt;- err         //  出错的时候返给主goroutineto 
        case &amp;lt;-pc.closech:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写循环就更简单了，select channel中主gouroutine的request，然后写入tcp输入流，如果出错了，channel 通知调用者。&lt;/p&gt;

&lt;p&gt;整体看下来，过程都很简单，但是代码中有很多值得我们学习的地方，比如高并发请求如何复用tcp连接，这里是连接池的做法，如果使用多个 goroutine相互协作完成一个http请求，出现错误的时候如何通知调用者中断错误，代码风格也有很多可以借鉴的地方。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;http.Client 表示一个http client端，用来处理HTTP相关的工作，例如cookies, redirect, timeout等工作，其内部包含一个Transport，tranport用来建立一个连接，其中维护了一个空闲连接池idleConn map[connectMethodKey][]*persistConn，其中的每个成员都是一个persistConn对象，persistConn是个具体的连接实例，包含了连接的上下文，会启动两个groutine分别执行readLoop和writeLoop, 每当transport调用roundTrip的时候，就会从连接池中选择一个空闲的persistConn，然后调用其roundTrip方法，将读写请求通过channel分别发送到readLoop和writeLoop中，然后会进行select各个channel的信息，包括连接关闭，请求超时，writeLoop出错， readLoop返回读取结果等。在writeLoop中发送请求，在readLoop中获取response并通过channe返回给roundTrip函数中，并再次将自己加入到idleConn中，等待下次请求到来。&lt;/p&gt;

&lt;h2 id=&#34;自定义client&#34;&gt;自定义client&lt;/h2&gt;

&lt;p&gt;在上面我们说到调用结构体的成员函数都是默认的结构体的成员函数，但是如果我们有一些特殊的需求，我们就需要重新定义这些结构体，然后实现自己的逻辑，整个http请求也就会按着我们的逻辑进行处理，这也是我们实现一些功能的必要手段。最基本的就是自定义client，也是我们编程常用的，深入一些就需要了解一些传输transport等。&lt;/p&gt;

&lt;p&gt;1、要管理HTTP客户端的头域、重定向策略和其他设置，创建一个Client：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;client := &amp;amp;http.Client{
    CheckRedirect: redirectPolicyFunc,
}
resp, err := client.Get(&amp;quot;http://example.com&amp;quot;)
// ...
req, err := http.NewRequest(&amp;quot;GET&amp;quot;, &amp;quot;http://example.com&amp;quot;, nil)
// ...
req.Header.Add(&amp;quot;If-None-Match&amp;quot;, `W/&amp;quot;wyzzy&amp;quot;`)
resp, err := client.Do(req)
// ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是在上面的基础上增加了对client结构体的设置，而不是使用DefaultClient，我们来看一下client的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Client struct {
    // Transport指定执行独立、单次HTTP请求的机制。
    // 如果Transport为nil，则使用DefaultTransport。
    Transport RoundTripper
    // CheckRedirect指定处理重定向的策略。
    // 如果CheckRedirect不为nil，客户端会在执行重定向之前调用本函数字段。
    // 参数req和via是将要执行的请求和已经执行的请求（切片，越新的请求越靠后）。
    // 如果CheckRedirect返回一个错误，本类型的Get方法不会发送请求req，
    // 而是返回之前得到的最后一个回复和该错误。（包装进url.Error类型里）
    //
    // 如果CheckRedirect为nil，会采用默认策略：连续10此请求后停止。
    CheckRedirect func(req *Request, via []*Request) error
    // Jar指定cookie管理器。
    // 如果Jar为nil，请求中不会发送cookie，回复中的cookie会被忽略。
    Jar CookieJar
    // Timeout指定本类型的值执行请求的时间限制。
    // 该超时限制包括连接时间、重定向和读取回复主体的时间。
    // 计时器会在Head、Get、Post或Do方法返回后继续运作并在超时后中断回复主体的读取。
    //
    // Timeout为零值表示不设置超时。
    //
    // Client实例的Transport字段必须支持CancelRequest方法，
    // 否则Client会在试图用Head、Get、Post或Do方法执行请求时返回错误。
    // 本类型的Transport字段默认值（DefaultTransport）支持CancelRequest方法。
    Timeout time.Duration
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要是对这些结构体中的成员的如何运用才是重点，然后就调用client的Get，Do等方法就是上面的执行逻辑，这边只是简单的client的处理，后面的逻辑依然使用的是默认的Transport。&lt;/p&gt;

&lt;p&gt;2、要管理代理、TLS配置、keep-alive、压缩和其他设置，创建一个Transport：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tr := &amp;amp;http.Transport{
    TLSClientConfig:    &amp;amp;tls.Config{RootCAs: pool},
    DisableCompression: true,
}
client := &amp;amp;http.Client{Transport: tr}
resp, err := client.Get(&amp;quot;https://example.com&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Client和Transport类型都可以安全的被多个go程同时使用。出于效率考虑，应该一次建立、尽量重用。&lt;/p&gt;

&lt;p&gt;这边在client的基础上对client的transport的管理代理、TLS配置、keep-alive、压缩和其他设置，然后后面的逻辑中主要是切换到自定义的transport的逻辑运行。&lt;/p&gt;

&lt;h1 id=&#34;http服务端&#34;&gt;http服务端&lt;/h1&gt;

&lt;h2 id=&#34;http-status&#34;&gt;http status&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;const (
    StatusContinue           = 100
    StatusSwitchingProtocols = 101
    StatusOK                   = 200
    StatusCreated              = 201
    StatusAccepted             = 202
    StatusNonAuthoritativeInfo = 203
    StatusNoContent            = 204
    StatusResetContent         = 205
    StatusPartialContent       = 206
    StatusMultipleChoices   = 300
    StatusMovedPermanently  = 301
    StatusFound             = 302
    StatusSeeOther          = 303
    StatusNotModified       = 304
    StatusUseProxy          = 305
    StatusTemporaryRedirect = 307
    StatusBadRequest                   = 400
    StatusUnauthorized                 = 401
    StatusPaymentRequired              = 402
    StatusForbidden                    = 403
    StatusNotFound                     = 404
    StatusMethodNotAllowed             = 405
    StatusNotAcceptable                = 406
    StatusProxyAuthRequired            = 407
    StatusRequestTimeout               = 408
    StatusConflict                     = 409
    StatusGone                         = 410
    StatusLengthRequired               = 411
    StatusPreconditionFailed           = 412
    StatusRequestEntityTooLarge        = 413
    StatusRequestURITooLong            = 414
    StatusUnsupportedMediaType         = 415
    StatusRequestedRangeNotSatisfiable = 416
    StatusExpectationFailed            = 417
    StatusTeapot                       = 418
    StatusInternalServerError     = 500
    StatusNotImplemented          = 501
    StatusBadGateway              = 502
    StatusServiceUnavailable      = 503
    StatusGatewayTimeout          = 504
    StatusHTTPVersionNotSupported = 505
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们比较常用的就是404（服务未发现），503（服务不可用）等。&lt;/p&gt;

&lt;h2 id=&#34;http-header&#34;&gt;http header&lt;/h2&gt;

&lt;p&gt;Header代表HTTP头域的键值对。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Header map[string][]string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基本操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Get(key string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get返回键对应的第一个值，如果键不存在会返回&amp;rdquo;&amp;ldquo;。如要获取该键对应的值切片，请直接用规范格式的键访问map。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Set(key, value string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set添加键值对到h，如键已存在则会用只有新值一个元素的切片取代旧值切片。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Add(key, value string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add添加键值对到h，如键已存在则会将新的值附加到旧值切片后面。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Del(key string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Del删除键值对。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Write(w io.Writer) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write以有线格式将头域写入w。&lt;/p&gt;

&lt;h2 id=&#34;用于http客户端和服务端的结构体&#34;&gt;用于http客户端和服务端的结构体&lt;/h2&gt;

&lt;p&gt;type Request&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Request struct {
    // Method指定HTTP方法（GET、POST、PUT等）。对客户端，&amp;quot;&amp;quot;代表GET。
    Method string
    // URL在服务端表示被请求的URI，在客户端表示要访问的URL。
    //
    // 在服务端，URL字段是解析请求行的URI（保存在RequestURI字段）得到的，
    // 对大多数请求来说，除了Path和RawQuery之外的字段都是空字符串。
    // （参见RFC 2616, Section 5.1.2）
    //
    // 在客户端，URL的Host字段指定了要连接的服务器，
    // 而Request的Host字段（可选地）指定要发送的HTTP请求的Host头的值。
    URL *url.URL
    // 接收到的请求的协议版本。本包生产的Request总是使用HTTP/1.1
    Proto      string // &amp;quot;HTTP/1.0&amp;quot;
    ProtoMajor int    // 1
    ProtoMinor int    // 0
    // Header字段用来表示HTTP请求的头域。如果头域（多行键值对格式）为：
    //  accept-encoding: gzip, deflate
    //  Accept-Language: en-us
    //  Connection: keep-alive
    // 则：
    //  Header = map[string][]string{
    //      &amp;quot;Accept-Encoding&amp;quot;: {&amp;quot;gzip, deflate&amp;quot;},
    //      &amp;quot;Accept-Language&amp;quot;: {&amp;quot;en-us&amp;quot;},
    //      &amp;quot;Connection&amp;quot;: {&amp;quot;keep-alive&amp;quot;},
    //  }
    // HTTP规定头域的键名（头名）是大小写敏感的，请求的解析器通过规范化头域的键名来实现这点。
    // 在客户端的请求，可能会被自动添加或重写Header中的特定的头，参见Request.Write方法。
    Header Header
    // Body是请求的主体。
    //
    // 在客户端，如果Body是nil表示该请求没有主体买入GET请求。
    // Client的Transport字段会负责调用Body的Close方法。
    //
    // 在服务端，Body字段总是非nil的；但在没有主体时，读取Body会立刻返回EOF。
    // Server会关闭请求的主体，ServeHTTP处理器不需要关闭Body字段。
    Body io.ReadCloser
    // ContentLength记录相关内容的长度。
    // 如果为-1，表示长度未知，如果&amp;gt;=0，表示可以从Body字段读取ContentLength字节数据。
    // 在客户端，如果Body非nil而该字段为0，表示不知道Body的长度。
    ContentLength int64
    // TransferEncoding按从最外到最里的顺序列出传输编码，空切片表示&amp;quot;identity&amp;quot;编码。
    // 本字段一般会被忽略。当发送或接受请求时，会自动添加或移除&amp;quot;chunked&amp;quot;传输编码。
    TransferEncoding []string
    // Close在服务端指定是否在回复请求后关闭连接，在客户端指定是否在发送请求后关闭连接。
    Close bool
    // 在服务端，Host指定URL会在其上寻找资源的主机。
    // 根据RFC 2616，该值可以是Host头的值，或者URL自身提供的主机名。
    // Host的格式可以是&amp;quot;host:port&amp;quot;。
    //
    // 在客户端，请求的Host字段（可选地）用来重写请求的Host头。
    // 如过该字段为&amp;quot;&amp;quot;，Request.Write方法会使用URL字段的Host。
    Host string
    // Form是解析好的表单数据，包括URL字段的query参数和POST或PUT的表单数据。
    // 本字段只有在调用ParseForm后才有效。在客户端，会忽略请求中的本字段而使用Body替代。
    Form url.Values
    // PostForm是解析好的POST或PUT的表单数据。
    // 本字段只有在调用ParseForm后才有效。在客户端，会忽略请求中的本字段而使用Body替代。
    PostForm url.Values
    // MultipartForm是解析好的多部件表单，包括上传的文件。
    // 本字段只有在调用ParseMultipartForm后才有效。
    // 在客户端，会忽略请求中的本字段而使用Body替代。
    MultipartForm *multipart.Form
    // Trailer指定了会在请求主体之后发送的额外的头域。
    //
    // 在服务端，Trailer字段必须初始化为只有trailer键，所有键都对应nil值。
    // （客户端会声明哪些trailer会发送）
    // 在处理器从Body读取时，不能使用本字段。
    // 在从Body的读取返回EOF后，Trailer字段会被更新完毕并包含非nil的值。
    // （如果客户端发送了这些键值对），此时才可以访问本字段。
    //
    // 在客户端，Trail必须初始化为一个包含将要发送的键值对的映射。（值可以是nil或其终值）
    // ContentLength字段必须是0或-1，以启用&amp;quot;chunked&amp;quot;传输编码发送请求。
    // 在开始发送请求后，Trailer可以在读取请求主体期间被修改，
    // 一旦请求主体返回EOF，调用者就不可再修改Trailer。
    //
    // 很少有HTTP客户端、服务端或代理支持HTTP trailer。
    Trailer Header
    // RemoteAddr允许HTTP服务器和其他软件记录该请求的来源地址，一般用于日志。
    // 本字段不是ReadRequest函数填写的，也没有定义格式。
    // 本包的HTTP服务器会在调用处理器之前设置RemoteAddr为&amp;quot;IP:port&amp;quot;格式的地址。
    // 客户端会忽略请求中的RemoteAddr字段。
    RemoteAddr string
    // RequestURI是被客户端发送到服务端的请求的请求行中未修改的请求URI
    // （参见RFC 2616, Section 5.1）
    // 一般应使用URI字段，在客户端设置请求的本字段会导致错误。
    RequestURI string
    // TLS字段允许HTTP服务器和其他软件记录接收到该请求的TLS连接的信息
    // 本字段不是ReadRequest函数填写的。
    // 对启用了TLS的连接，本包的HTTP服务器会在调用处理器之前设置TLS字段，否则将设TLS为nil。
    // 客户端会忽略请求中的TLS字段。
    TLS *tls.ConnectionState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Request类型代表一个服务端接受到的或者客户端发送出去的HTTP请求。&lt;/p&gt;

&lt;p&gt;Request各字段的意义和用途在服务端和客户端是不同的。除了字段本身上方文档，还可参见Request.Write方法和RoundTripper接口的文档。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Response struct {
    Status     string // 例如&amp;quot;200 OK&amp;quot;
    StatusCode int    // 例如200
    Proto      string // 例如&amp;quot;HTTP/1.0&amp;quot;
    ProtoMajor int    // 例如1
    ProtoMinor int    // 例如0
    // Header保管头域的键值对。
    // 如果回复中有多个头的键相同，Header中保存为该键对应用逗号分隔串联起来的这些头的值
    // （参见RFC 2616 Section 4.2）
    // 被本结构体中的其他字段复制保管的头（如ContentLength）会从Header中删掉。
    //
    // Header中的键都是规范化的，参见CanonicalHeaderKey函数
    Header Header
    // Body代表回复的主体。
    // Client类型和Transport类型会保证Body字段总是非nil的，即使回复没有主体或主体长度为0。
    // 关闭主体是调用者的责任。
    // 如果服务端采用&amp;quot;chunked&amp;quot;传输编码发送的回复，Body字段会自动进行解码。
    Body io.ReadCloser
    // ContentLength记录相关内容的长度。
    // 其值为-1表示长度未知（采用chunked传输编码）
    // 除非对应的Request.Method是&amp;quot;HEAD&amp;quot;，其值&amp;gt;=0表示可以从Body读取的字节数
    ContentLength int64
    // TransferEncoding按从最外到最里的顺序列出传输编码，空切片表示&amp;quot;identity&amp;quot;编码。
    TransferEncoding []string
    // Close记录头域是否指定应在读取完主体后关闭连接。（即Connection头）
    // 该值是给客户端的建议，Response.Write方法的ReadResponse函数都不会关闭连接。
    Close bool
    // Trailer字段保存和头域相同格式的trailer键值对，和Header字段相同类型
    Trailer Header
    // Request是用来获取此回复的请求
    // Request的Body字段是nil（因为已经被用掉了）
    // 这个字段是被Client类型发出请求并获得回复后填充的
    Request *Request
    // TLS包含接收到该回复的TLS连接的信息。 对未加密的回复，本字段为nil。
    // 返回的指针是被（同一TLS连接接收到的）回复共享的，不应被修改。
    TLS *tls.ConnectionState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Response代表一个HTTP请求的回复&lt;/p&gt;

&lt;p&gt;type ResponseWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ResponseWriter interface {
    // Header返回一个Header类型值，该值会被WriteHeader方法发送。
    // 在调用WriteHeader或Write方法后再改变该对象是没有意义的。
    Header() Header
    // WriteHeader该方法发送HTTP回复的头域和状态码。
    // 如果没有被显式调用，第一次调用Write时会触发隐式调用WriteHeader(http.StatusOK)
    // WriterHeader的显式调用主要用于发送错误码。
    WriteHeader(int)
    // Write向连接中写入作为HTTP的一部分回复的数据。
    // 如果被调用时还未调用WriteHeader，本方法会先调用WriteHeader(http.StatusOK)
    // 如果Header中没有&amp;quot;Content-Type&amp;quot;键，
    // 本方法会使用包函数DetectContentType检查数据的前512字节，将返回值作为该键的值。
    Write([]byte) (int, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResponseWriter接口被HTTP处理器用于构造HTTP回复。这个一般用于服务端处理请求&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;正常我们使用的返回方式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
 &amp;quot;net/http&amp;quot;
)

func main() {

 http.HandleFunc(&amp;quot;/&amp;quot;, func (w http.ResponseWriter, r *http.Request){


   w.Header().Set(&amp;quot;name&amp;quot;, &amp;quot;my name is smallsoup&amp;quot;)
   w.WriteHeader(500)
   w.Write([]byte(&amp;quot;hello world\n&amp;quot;))

 })

 http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;type CloseNotifier&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type CloseNotifier interface {
    // CloseNotify返回一个通道，该通道会在客户端连接丢失时接收到唯一的值
    CloseNotify() &amp;lt;-chan bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HTTP处理器ResponseWriter接口参数的下层如果实现了CloseNotifier接口，可以让用户检测下层的连接是否停止。如果客户端在回复准备好之前关闭了连接，该机制可以用于取消服务端耗时较长的操作。&lt;/p&gt;

&lt;h2 id=&#34;http-服务端使用和原理解析&#34;&gt;http 服务端使用和原理解析&lt;/h2&gt;

&lt;p&gt;ListenAndServe使用指定的监听地址和处理器启动一个HTTP服务端。处理器参数通常是nil，这表示采用包变量DefaultServeMux作为处理器。Handle和HandleFunc函数可以向DefaultServeMux添加处理器。如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.Handle(&amp;quot;/foo&amp;quot;, fooHandler)
http.HandleFunc(&amp;quot;/bar&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintf(w, &amp;quot;Hello, %q&amp;quot;, html.EscapeString(r.URL.Path))
})
log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenAndServe该方法用于在指定的TCP网络地址addr进行监听，然后调用服务端处理程序来处理传入的连接请求。该方法有两个参数：第一个参数addr 即监听地址；第二个参数表示服务端处理程序，通常为空，这意味着服务端调用 http.DefaultServeMux 进行处理，而服务端编写的业务逻辑处理程序 http.Handle() 或 http.HandleFunc() 默认注入 http.DefaultServeMux 中。&lt;/p&gt;

&lt;p&gt;理解HTTP相关的网络应用，主要关注两个地方-客户端(client)和服务端(server)，两者的交互主要是client的request以及server的response,主要就在于如何接受client的request并向client返回response。&lt;/p&gt;

&lt;p&gt;接收request的过程中，最重要的莫过于路由（router），即实现一个Multiplexer器。Go http中既可以使用内置的mutilplexer &amp;mdash; DefautServeMux，也可以自定义。Multiplexer路由的目的就是为了找到处理器函数（handler），后者将对request进行处理，同时构建response&lt;/p&gt;

&lt;p&gt;流程为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Clinet -&amp;gt; Requests -&amp;gt;  Multiplexer(router) -&amp;gt; handler  -&amp;gt; Response -&amp;gt; Clinet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于一个http服务，大致需要理解这两个封装的过程就可以理解上面的实现了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.首先需要注册路由，即提供url模式和handler函数的映射.
2.其次就是实例化一个server对象，并开启对客户端的监听。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看go http服务的代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/&amp;quot;, indexHandler) -----即是注册路由。
http.ListenAndServe(&amp;quot;127.0.0.1:8000&amp;quot;, nil)---启动server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server := &amp;amp;Server{Addr: addr, Handler: handler}
server.ListenAndServe()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;注册路由&#34;&gt;注册路由&lt;/h3&gt;

&lt;p&gt;net/http包暴露的注册路由的api很简单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/&amp;quot;, indexHandler) -----即是注册路由。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HandlerFunc是一个函数类型，如下定义，同时实现了Handler接口的ServeHTTP方法。使用HandlerFunc类型包装一下路由定义的indexHandler函数，其目的就是为了让这个函数也实现ServeHTTP方法，即转变成一个handler处理器(函数)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type HandlerFunc func(ResponseWriter, *Request)

// ServeHTTP calls f(w, r).
func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) {
    f(w, r)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们最开始写的例子中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/&amp;quot;,Indexhandler)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样 IndexHandler 函数也有了ServeHTTP方法。&lt;/p&gt;

&lt;p&gt;ServeMux和handler处理器函数的连接桥梁就是Handler接口。ServeMux的ServeHTTP方法实现了寻找注册路由的handler的函数（可以看下面的监控服务流程），并调用该handler的ServeHTTP方法。ServeHTTP方法就是真正处理请求和构造响应的地方。&lt;/p&gt;

&lt;p&gt;Go其实支持外部实现的路由器 ListenAndServe的第二个参数就是 用以配置外部路由器的，它是一个Handler接口，即外部路由器只要实现了Handler接口就可以,我们可以在自己实现 的路由器的ServHTTP里面实现自定义路由功能。&lt;/p&gt;

&lt;p&gt;如下代码所示，我们自己实现了一个简易的路由器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import ( 
    &amp;quot;fmt&amp;quot;
    &amp;quot;net/http&amp;quot; 
    )
type MyMux struct { }

func (p *MyMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path == &amp;quot;/&amp;quot; {
    sayhelloName(w, r)
    return 
}
    http.NotFound(w, r)
    return 
}

func sayhelloName(w http.ResponseWriter, r *http.Request) { f
    mt.Fprintf(w, &amp;quot;Hello myroute!&amp;quot;)
}

func main() {
    mux := &amp;amp;MyMux{}
    http.ListenAndServe(&amp;quot;:9090&amp;quot;, mux) 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;multiplexer&#34;&gt;multiplexer&lt;/h3&gt;

&lt;p&gt;http.HandleFunc选取了DefaultServeMux作为multiplexer：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) {
    DefaultServeMux.HandleFunc(pattern, handler)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DefaultServeMux是ServeMux的一个实例。当然http包也提供了NewServeMux方法创建一个ServeMux实例，默认则创建一个DefaultServeMux：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewServeMux allocates and returns a new ServeMux.
func NewServeMux() *ServeMux { return new(ServeMux) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DefaultServeMux的代码定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// DefaultServeMux is the default ServeMux used by Serve.
var DefaultServeMux = &amp;amp;defaultServeMux
var defaultServeMux ServeMux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然也可以是其他可以实现的实例 ，比如上面实现的mux。&lt;/p&gt;

&lt;p&gt;路由结构体ServeMux&lt;/p&gt;

&lt;p&gt;ServeMux的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ServeMux struct {
    mu    sync.RWMutex                      //锁，由于请求涉及到并发处理，因此这里需要一个锁机制
    m     map[string]muxEntry               // 路由规则，一个string对应一个mux实体，这里的string就是注册的路由
    hosts bool 
}

type muxEntry struct {
    explicit bool                // 是否精确匹配
    h        Handler              // 这个路由表达式对应哪个handler
    pattern  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ServeMux结构中最重要的字段为m，这是一个map，key是一些url模式，value是一个muxEntry结构，后者里定义存储了具体的url模式和handler。&lt;/p&gt;

&lt;p&gt;当然，所谓的ServeMux也实现了ServeHTTP接口，也算是一个handler，不过ServeMux的ServeHTTP方法不是用来处理request和respone，而是用来找到路由注册的handler，可以看服务监听时候的调用过程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// HandleFunc registers the handler function for the given pattern.
func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) {
    if handler == nil {
        panic(&amp;quot;http: nil handler&amp;quot;)
    }
    mux.Handle(pattern, HandlerFunc(handler))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ServeMux的Handle方法，将会对pattern和handler函数做一个map映射：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Handle registers the handler for the given pattern.
// If a handler already exists for pattern, Handle panics.
func (mux *ServeMux) Handle(pattern string, handler Handler) {
    mux.mu.Lock()
    defer mux.mu.Unlock()

    if pattern == &amp;quot;&amp;quot; {
        panic(&amp;quot;http: invalid pattern &amp;quot; + pattern)
    }
    if handler == nil {
        panic(&amp;quot;http: nil handler&amp;quot;)
    }
    if mux.m[pattern].explicit {
        panic(&amp;quot;http: multiple registrations for &amp;quot; + pattern)
    }

    if mux.m == nil {
        mux.m = make(map[string]muxEntry)
    }
    mux.m[pattern] = muxEntry{explicit: true, h: handler, pattern: pattern}

    if pattern[0] != &#39;/&#39; {
        mux.hosts = true
    }

    // Helpful behavior:
    // If pattern is /tree/, insert an implicit permanent redirect for /tree.
    // It can be overridden by an explicit registration.
    n := len(pattern)
    if n &amp;gt; 0 &amp;amp;&amp;amp; pattern[n-1] == &#39;/&#39; &amp;amp;&amp;amp; !mux.m[pattern[0:n-1]].explicit {
        // If pattern contains a host name, strip it and use remaining
        // path for redirect.
        path := pattern
        if pattern[0] != &#39;/&#39; {
            // In pattern, at least the last character is a &#39;/&#39;, so
            // strings.Index can&#39;t be -1.
            path = pattern[strings.Index(pattern, &amp;quot;/&amp;quot;):]
        }
        url := &amp;amp;url.URL{Path: path}
        mux.m[pattern[0:n-1]] = muxEntry{h: RedirectHandler(url.String(), StatusMovedPermanently), pattern: pattern}
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Handle函数的主要目的在于把handler和pattern模式绑定到map[string]muxEntry的map上，其中muxEntry保存了更多pattern和handler的信息，还记得前面讨论的Server结构吗？Server的m字段就是map[string]muxEntry这样一个map。&lt;/p&gt;

&lt;p&gt;此时，pattern和handler的路由注册完成。接下来就是如何开始server的监听，以接收客户端的请求。&lt;/p&gt;

&lt;h3 id=&#34;启动服务&#34;&gt;启动服务&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;http.ListenAndServe(&amp;quot;127.0.0.1:8000&amp;quot;, nil)---启动server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server := &amp;amp;Server{Addr: addr, Handler: handler}

server.ListenAndServe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注册好路由之后，启动web服务还需要开启服务器监听。http的ListenAndServer方法中可以看到创建了一个Server对象，并调用了Server对象的同名方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenAndServe(addr string, handler Handler) error {
    server := &amp;amp;Server{Addr: addr, Handler: handler}
    return server.ListenAndServe()
}
// ListenAndServe listens on the TCP network address srv.Addr and then
// calls Serve to handle requests on incoming connections.
// Accepted connections are configured to enable TCP keep-alives.
// If srv.Addr is blank, &amp;quot;:http&amp;quot; is used.
// ListenAndServe always returns a non-nil error.
func (srv *Server) ListenAndServe() error {
    addr := srv.Addr
    if addr == &amp;quot;&amp;quot; {
        addr = &amp;quot;:http&amp;quot;
    }
    ln, err := net.Listen(&amp;quot;tcp&amp;quot;, addr)
    if err != nil {
        return err
    }
    return srv.Serve(tcpKeepAliveListener{ln.(*net.TCPListener)})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Server的ListenAndServe方法中，会初始化监听地址Addr，同时调用Listen方法设置监听。最后将监听的TCP对象传入Serve方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Serve accepts incoming connections on the Listener l, creating a
// new service goroutine for each. The service goroutines read requests and
// then call srv.Handler to reply to them.
//
// For HTTP/2 support, srv.TLSConfig should be initialized to the
// provided listener&#39;s TLS Config before calling Serve. If
// srv.TLSConfig is non-nil and doesn&#39;t include the string &amp;quot;h2&amp;quot; in
// Config.NextProtos, HTTP/2 support is not enabled.
//
// Serve always returns a non-nil error. After Shutdown or Close, the
// returned error is ErrServerClosed.
func (srv *Server) Serve(l net.Listener) error {
    defer l.Close()
    if fn := testHookServerServe; fn != nil {
        fn(srv, l)
    }
    var tempDelay time.Duration // how long to sleep on accept failure

    if err := srv.setupHTTP2_Serve(); err != nil {
        return err
    }

    srv.trackListener(l, true)
    defer srv.trackListener(l, false)

    baseCtx := context.Background() // base is always background, per Issue 16220
    ctx := context.WithValue(baseCtx, ServerContextKey, srv)
    for {
        rw, e := l.Accept()
        if e != nil {
            select {
            case &amp;lt;-srv.getDoneChan():
                return ErrServerClosed
            default:
            }
            if ne, ok := e.(net.Error); ok &amp;amp;&amp;amp; ne.Temporary() {
                if tempDelay == 0 {
                    tempDelay = 5 * time.Millisecond
                } else {
                    tempDelay *= 2
                }
                if max := 1 * time.Second; tempDelay &amp;gt; max {
                    tempDelay = max
                }
                srv.logf(&amp;quot;http: Accept error: %v; retrying in %v&amp;quot;, e, tempDelay)
                time.Sleep(tempDelay)
                continue
            }
            return e
        }
        tempDelay = 0
        c := srv.newConn(rw)
        c.setState(c.rwc, StateNew) // before Serve can return
        go c.serve(ctx)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监听开启之后，一旦客户端请求到达，创建一个conn结构体，这个conn中保留了这次请求的信息，go就开启一个协程serve处理请求，主要逻辑都在serve方法之中。&lt;/p&gt;

&lt;p&gt;serve方法比较长，其主要职能就是，创建一个上下文对象，然后调用Listener的Accept方法用来　获取连接数据并使用newConn方法创建连接对象。最后使用goroutein协程的方式处理连接请求。因为每一个连接都开起了一个协程，请求的上下文都不同，同时又保证了go的高并发。serve也是一个长长的方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Serve a new connection.
func (c *conn) serve(ctx context.Context) {
    c.remoteAddr = c.rwc.RemoteAddr().String()
    ctx = context.WithValue(ctx, LocalAddrContextKey, c.rwc.LocalAddr())
    defer func() {
        if err := recover(); err != nil &amp;amp;&amp;amp; err != ErrAbortHandler {
            const size = 64 &amp;lt;&amp;lt; 10
            buf := make([]byte, size)
            buf = buf[:runtime.Stack(buf, false)]
            c.server.logf(&amp;quot;http: panic serving %v: %v\n%s&amp;quot;, c.remoteAddr, err, buf)
        }
        if !c.hijacked() {
            c.close()
            c.setState(c.rwc, StateClosed)
        }
    }()

    if tlsConn, ok := c.rwc.(*tls.Conn); ok {
        if d := c.server.ReadTimeout; d != 0 {
            c.rwc.SetReadDeadline(time.Now().Add(d))
        }
        if d := c.server.WriteTimeout; d != 0 {
            c.rwc.SetWriteDeadline(time.Now().Add(d))
        }
        if err := tlsConn.Handshake(); err != nil {
            c.server.logf(&amp;quot;http: TLS handshake error from %s: %v&amp;quot;, c.rwc.RemoteAddr(), err)
            return
        }
        c.tlsState = new(tls.ConnectionState)
        *c.tlsState = tlsConn.ConnectionState()
        if proto := c.tlsState.NegotiatedProtocol; validNPN(proto) {
            if fn := c.server.TLSNextProto[proto]; fn != nil {
                h := initNPNRequest{tlsConn, serverHandler{c.server}}
                fn(c.server, tlsConn, h)
            }
            return
        }
    }

    // HTTP/1.x from here on.

    ctx, cancelCtx := context.WithCancel(ctx)
    c.cancelCtx = cancelCtx
    defer cancelCtx()

    c.r = &amp;amp;connReader{conn: c}
    c.bufr = newBufioReader(c.r)
    c.bufw = newBufioWriterSize(checkConnErrorWriter{c}, 4&amp;lt;&amp;lt;10)

    for {
        w, err := c.readRequest(ctx)
        if c.r.remain != c.server.initialReadLimitSize() {
            // If we read any bytes off the wire, we&#39;re active.
            c.setState(c.rwc, StateActive)
        }
        if err != nil {
            const errorHeaders = &amp;quot;\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n&amp;quot;

            if err == errTooLarge {
                // Their HTTP client may or may not be
                // able to read this if we&#39;re
                // responding to them and hanging up
                // while they&#39;re still writing their
                // request. Undefined behavior.
                const publicErr = &amp;quot;431 Request Header Fields Too Large&amp;quot;
                fmt.Fprintf(c.rwc, &amp;quot;HTTP/1.1 &amp;quot;+publicErr+errorHeaders+publicErr)
                c.closeWriteAndWait()
                return
            }
            if isCommonNetReadError(err) {
                return // don&#39;t reply
            }

            publicErr := &amp;quot;400 Bad Request&amp;quot;
            if v, ok := err.(badRequestError); ok {
                publicErr = publicErr + &amp;quot;: &amp;quot; + string(v)
            }

            fmt.Fprintf(c.rwc, &amp;quot;HTTP/1.1 &amp;quot;+publicErr+errorHeaders+publicErr)
            return
        }

        // Expect 100 Continue support
        req := w.req
        if req.expectsContinue() {
            if req.ProtoAtLeast(1, 1) &amp;amp;&amp;amp; req.ContentLength != 0 {
                // Wrap the Body reader with one that replies on the connection
                req.Body = &amp;amp;expectContinueReader{readCloser: req.Body, resp: w}
            }
        } else if req.Header.get(&amp;quot;Expect&amp;quot;) != &amp;quot;&amp;quot; {
            w.sendExpectationFailed()
            return
        }

        c.curReq.Store(w)

        if requestBodyRemains(req.Body) {
            registerOnHitEOF(req.Body, w.conn.r.startBackgroundRead)
        } else {
            if w.conn.bufr.Buffered() &amp;gt; 0 {
                w.conn.r.closeNotifyFromPipelinedRequest()
            }
            w.conn.r.startBackgroundRead()
        }

        // HTTP cannot have multiple simultaneous active requests.[*]
        // Until the server replies to this request, it can&#39;t read another,
        // so we might as well run the handler in this goroutine.
        // [*] Not strictly true: HTTP pipelining. We could let them all process
        // in parallel even if their responses need to be serialized.
        // But we&#39;re not going to implement HTTP pipelining because it
        // was never deployed in the wild and the answer is HTTP/2.
        serverHandler{c.server}.ServeHTTP(w, w.req)
        w.cancelCtx()
        if c.hijacked() {
            return
        }
        w.finishRequest()
        if !w.shouldReuseConnection() {
            if w.requestBodyLimitHit || w.closedRequestBodyEarly() {
                c.closeWriteAndWait()
            }
            return
        }
        c.setState(c.rwc, StateIdle)
        c.curReq.Store((*response)(nil))

        if !w.conn.server.doKeepAlives() {
            // We&#39;re in shutdown mode. We might&#39;ve replied
            // to the user without &amp;quot;Connection: close&amp;quot; and
            // they might think they can send another
            // request, but such is life with HTTP/1.1.
            return
        }

        if d := c.server.idleTimeout(); d != 0 {
            c.rwc.SetReadDeadline(time.Now().Add(d))
            if _, err := c.bufr.Peek(4); err != nil {
                return
            }
        }
        c.rwc.SetReadDeadline(time.Time{})
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用defer定义了函数退出时，连接关闭相关的处理。然后就是读取连接的网络数据，并处理读取完毕时候的状态。接下来就是调用serverHandler{c.server}.ServeHTTP(w, w.req)方法处理请求了。最后就是请求处理完毕的逻辑。serverHandler是一个重要的结构，它近有一个字段，即Server结构，同时它也实现了Handler接口方法ServeHTTP，并在该接口方法中做了一个重要的事情，初始化multiplexer路由多路复用器。如果server对象没有指定Handler，则使用默认的DefaultServeMux作为路由Multiplexer。并调用初始化Handler的ServeHTTP方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// serverHandler delegates to either the server&#39;s Handler or
// DefaultServeMux and also handles &amp;quot;OPTIONS *&amp;quot; requests.
type serverHandler struct {
    srv *Server
}

func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) {
    handler := sh.srv.Handler
    if handler == nil {
        handler = DefaultServeMux
    }
    if req.RequestURI == &amp;quot;*&amp;quot; &amp;amp;&amp;amp; req.Method == &amp;quot;OPTIONS&amp;quot; {
        handler = globalOptionsHandler{}
    }
    handler.ServeHTTP(rw, req)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里DefaultServeMux的ServeHTTP方法其实也是定义在ServeMux结构中的，相关代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Find a handler on a handler map given a path string.
// Most-specific (longest) pattern wins.
func (mux *ServeMux) match(path string) (h Handler, pattern string) {
    // Check for exact match first.
    v, ok := mux.m[path]
    if ok {
        return v.h, v.pattern
    }

    // Check for longest valid match.
    var n = 0
    for k, v := range mux.m {
        if !pathMatch(k, path) {
            continue
        }
        if h == nil || len(k) &amp;gt; n {
            n = len(k)
            h = v.h
            pattern = v.pattern
        }
    }
    return
}
func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) {

    // CONNECT requests are not canonicalized.
    if r.Method == &amp;quot;CONNECT&amp;quot; {
        return mux.handler(r.Host, r.URL.Path)
    }

    // All other requests have any port stripped and path cleaned
    // before passing to mux.handler.
    host := stripHostPort(r.Host)
    path := cleanPath(r.URL.Path)
    if path != r.URL.Path {
        _, pattern = mux.handler(host, path)
        url := *r.URL
        url.Path = path
        return RedirectHandler(url.String(), StatusMovedPermanently), pattern
    }

    return mux.handler(host, r.URL.Path)
}

// handler is the main implementation of Handler.
// The path is known to be in canonical form, except for CONNECT methods.
func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) {
    mux.mu.RLock()
    defer mux.mu.RUnlock()

    // Host-specific pattern takes precedence over generic ones
    if mux.hosts {
        h, pattern = mux.match(host + path)
    }
    if h == nil {
        h, pattern = mux.match(path)
    }
    if h == nil {
        h, pattern = NotFoundHandler(), &amp;quot;&amp;quot;
    }
    return
}

// ServeHTTP dispatches the request to the handler whose
// pattern most closely matches the request URL.
func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) {
    if r.RequestURI == &amp;quot;*&amp;quot; {
        if r.ProtoAtLeast(1, 1) {
            w.Header().Set(&amp;quot;Connection&amp;quot;, &amp;quot;close&amp;quot;)
        }
        w.WriteHeader(StatusBadRequest)
        return
    }
    h, _ := mux.Handler(r)
    h.ServeHTTP(w, r)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mux的ServeHTTP方法通过调用其Handler方法寻找注册到路由上的handler函数，并调用该函数的ServeHTTP方法，本例则是IndexHandler函数。&lt;/p&gt;

&lt;p&gt;mux的Handler方法对URL简单的处理，然后调用handler方法，后者会创建一个锁，同时调用match方法返回一个handler和pattern。&lt;/p&gt;

&lt;p&gt;在match方法中，mux的m字段是map[string]muxEntry图，后者存储了pattern和handler处理器函数，因此通过迭代m寻找出注册路由的patten模式与实际url匹配的handler函数并返回。&lt;/p&gt;

&lt;p&gt;返回的结构一直传递到mux的ServeHTTP方法，接下来调用handler函数的ServeHTTP方法，即IndexHandler函数，然后把response写到http.RequestWirter对象返回给客户端。&lt;/p&gt;

&lt;p&gt;上述函数运行结束即serverHandler{c.server}.ServeHTTP(w, w.req)运行结束。接下来就是对请求处理完毕之后上希望和连接断开的相关逻辑。&lt;/p&gt;

&lt;p&gt;至此，Golang中一个完整的http服务介绍完毕，包括注册路由，开启监听，处理连接，路由处理函数。
多数的web应用基于HTTP协议，客户端和服务器通过request-response的方式交互。一个server并不可少的两部分莫过于路由注册和连接处理。Golang通过一个ServeMux实现了的multiplexer路由多路复用器来管理路由。同时提供一个Handler接口提供ServeHTTP用来实现handler处理其函数，后者可以处理实际request并构造response。&lt;/p&gt;

&lt;h3 id=&#34;总结-1&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;理解go中的http服务，最重要就是要理解Multiplexer和handler，Golang中的Multiplexer基于ServeMux结构，同时也实现了Handler接口。下面对几个重要概念说明，两个重要的结构体和一个接口&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Handler类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Golang没有继承，类多态的方式可以通过接口实现。所谓接口则是定义声明了函数签名，任何结构只要实现了与接口函数签名相同的方法，就等同于实现了接口。go的http服务都是基于handler进行处理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Handler interface {
    ServeHTTP(ResponseWriter, *Request)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;任何结构体，只要实现了ServeHTTP方法，这个结构就可以称之为handler对象。ServeMux会使用handler并调用其ServeHTTP方法处理请求并返回响应。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;handler处理器(函数)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;handler处理器(函数)-就是HandleFunc的第二个参数，是一个函数： 具有func(w http.ResponseWriter, r *http.Requests)签名的函数，经过HandlerFunc结构包装的handler函数，它实现了ServeHTTP接口方法的函数。调用handler处理器的ServeHTTP方法时，即调用handler函数本身。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;handler对象：实现了Handler接口ServeHTTP方法的结构。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ServeMux和handler处理器函数的连接桥梁就是Handler接口。ServeMux的ServeHTTP方法实现了寻找注册路由的handler的函数，并调用该handler的ServeHTTP方法。ServeHTTP方法就是真正处理请求和构造响应的地方。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Server结构体&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从http.ListenAndServe的源码可以看出，它还是创建了一个server对象，并调用server对象的ListenAndServe方法来实现监听路由：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenAndServe(addr string, handler Handler) error {
    server := &amp;amp;Server{Addr: addr, Handler: handler}
    return server.ListenAndServe()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看server的结构如下，其实上面已经解释过：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server struct {
    Addr         string        
    Handler      Handler       
    ReadTimeout  time.Duration 
    WriteTimeout time.Duration 
    TLSConfig    *tls.Config   

    MaxHeaderBytes int

    TLSNextProto map[string]func(*Server, *tls.Conn, Handler)

    ConnState func(net.Conn, ConnState)
    ErrorLog *log.Logger
    disableKeepAlives int32     nextProtoOnce     sync.Once 
    nextProtoErr      error     
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;server结构存储了服务器处理请求常见的字段。其中Handler字段也保留Handler接口。如果Server接口没有提供Handler结构对象，那么会使用DefautServeMux做multiplexer，后面再做分析。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;路由结构体ServeMux&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ServeMux的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ServeMux struct {
    mu    sync.RWMutex                      //锁，由于请求涉及到并发处理，因此这里需要一个锁机制
    m     map[string]muxEntry               // 路由规则，一个string对应一个mux实体，这里的string就是注册的路由
    hosts bool 
}

type muxEntry struct {
    explicit bool                // 是否精确匹配
    h        Handler              // 这个路由表达式对应哪个handler
    pattern  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ServeMux结构中最重要的字段为m，这是一个map，key是一些url模式，value是一个muxEntry结构，后者里定义存储了具体的url模式和handler。&lt;/p&gt;

&lt;p&gt;当然，所谓的ServeMux也实现了ServeHTTP接口，也算是一个handler，不过ServeMux的ServeHTTP方法不是用来处理request和respone，而是用来找到路由注册的handler&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Go代码的执行流程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;过对http包的分析之后，现在让我们来梳理一下整个的代码执行过程。&lt;/p&gt;

&lt;p&gt;1、首先调用Http.HandleFunc&lt;/p&gt;

&lt;p&gt;按顺序做了几件事:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1 调用了DefaultServerMux的HandleFunc
2 调用了DefaultServerMux的Handle
3 往DefaultServeMux的map[string]muxEntry中增加对应的handler和路由规则
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、其次调用http.ListenAndServe(&amp;rdquo;:9090&amp;rdquo;, nil)&lt;/p&gt;

&lt;p&gt;按顺序做了几件事情:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1 实例化Server
2 调用Server的ListenAndServe()
3 调用net.Listen(&amp;quot;tcp&amp;quot;, addr)监听端口
4 启动一个for循环，在循环体中Accept请求
5 对每个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve()
6 读取每个请求的内容w, err := c.readRequest()
7 判断handler是否为空，如果没有设置handler(这个例子就没有设置handler)，handler就设置为 DefaultServeMux
8 调用handler的ServeHttp
9 在这个例子中，下面就进入到DefaultServerMux.ServeHttp
10 根据request选择handler，并且进入到这个handler的ServeHTTP mux.handler(r).ServeHTTP(w, r)
11 选择handler:
    A 判断是否有路由能满足这个request(循环遍历ServerMux的muxEntry)
    B 如果有路由满足，调用这个路由handler的ServeHttp
    C 如果没有路由满足，调用NotFoundHandler的ServeHttp
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自定义server&#34;&gt;自定义server&lt;/h2&gt;

&lt;p&gt;要管理服务端的行为，可以创建一个自定义的Server：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s := &amp;amp;http.Server{
    Addr:           &amp;quot;:8080&amp;quot;,
    Handler:        myHandler,
    ReadTimeout:    10 * time.Second,
    WriteTimeout:   10 * time.Second,
    MaxHeaderBytes: 1 &amp;lt;&amp;lt; 20,
}
log.Fatal(s.ListenAndServe())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也是上面的流程，就是新增了一个server结构体的，做对应的操作，来看一下server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server struct {
    Addr           string        // TCP address to listen on, &amp;quot;:http&amp;quot; if empty
    Handler        Handler       // handler to invoke, http.DefaultServeMux if nil
    ReadTimeout    time.Duration // maximum duration before timing out read of the request
    WriteTimeout   time.Duration // maximum duration before timing out write of the response
    MaxHeaderBytes int           // maximum size of request headers, DefaultMaxHeaderBytes if 0
    TLSConfig      *tls.Config   // optional TLS config, used by ListenAndServeTLS

    // TLSNextProto optionally specifies a function to take over
    // ownership of the provided TLS connection when an NPN
    // protocol upgrade has occurred.  The map key is the protocol
    // name negotiated. The Handler argument should be used to
    // handle HTTP requests and will initialize the Request&#39;s TLS
    // and RemoteAddr if not already set.  The connection is
    // automatically closed when the function returns.
    TLSNextProto map[string]func(*Server, *tls.Conn, Handler)

    // ConnState specifies an optional callback function that is
    // called when a client connection changes state. See the
    // ConnState type and associated constants for details.
    ConnState func(net.Conn, ConnState)

    // ErrorLog specifies an optional logger for errors accepting
    // connections and unexpected behavior from handlers.
    // If nil, logging goes to os.Stderr via the log package&#39;s
    // standard logger.
    ErrorLog *log.Logger
    // contains filtered or unexported fields
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;都是什么作用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server struct {
    Addr           string        // 监听的TCP地址，如果为空字符串会使用&amp;quot;:http&amp;quot;
    Handler        Handler       // 调用的处理器，如为nil会调用http.DefaultServeMux
    ReadTimeout    time.Duration // 请求的读取操作在超时前的最大持续时间
    WriteTimeout   time.Duration // 回复的写入操作在超时前的最大持续时间
    MaxHeaderBytes int           // 请求的头域最大长度，如为0则用DefaultMaxHeaderBytes
    TLSConfig      *tls.Config   // 可选的TLS配置，用于ListenAndServeTLS方法
    // TLSNextProto（可选地）指定一个函数来在一个NPN型协议升级出现时接管TLS连接的所有权。
    // 映射的键为商谈的协议名；映射的值为函数，该函数的Handler参数应处理HTTP请求，
    // 并且初始化Handler.ServeHTTP的*Request参数的TLS和RemoteAddr字段（如果未设置）。
    // 连接在函数返回时会自动关闭。
    TLSNextProto map[string]func(*Server, *tls.Conn, Handler)
    // ConnState字段指定一个可选的回调函数，该函数会在一个与客户端的连接改变状态时被调用。
    // 参见ConnState类型和相关常数获取细节。
    ConnState func(net.Conn, ConnState)
    // ErrorLog指定一个可选的日志记录器，用于记录接收连接时的错误和处理器不正常的行为。
    // 如果本字段为nil，日志会通过log包的标准日志记录器写入os.Stderr。
    ErrorLog *log.Logger
    // 内含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server
func (s *Server) SetKeepAlivesEnabled(v bool)
func (srv *Server) Serve(l net.Listener) error
func (srv *Server) ListenAndServe() error
func (srv *Server) ListenAndServeTLS(certFile, keyFile string) error

func (*Server) SetKeepAlivesEnabled
func (s *Server) SetKeepAlivesEnabled(v bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SetKeepAlivesEnabled控制是否允许HTTP闲置连接重用（keep-alive）功能。默认该功能总是被启用的。只有资源非常紧张的环境或者服务端在关闭进程中时，才应该关闭该功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Server) Serve
func (srv *Server) Serve(l net.Listener) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Serve会接手监听器l收到的每一个连接，并为每一个连接创建一个新的服务go程。该go程会读取请求，然后调用srv.Handler回复请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Server) ListenAndServe
func (srv *Server) ListenAndServe() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenAndServe监听srv.Addr指定的TCP地址，并且会调用Serve方法接收到的连接。如果srv.Addr为空字符串，会使用&amp;rdquo;:http&amp;rdquo;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Server) ListenAndServeTLS
func (srv *Server) ListenAndServeTLS(certFile, keyFile string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenAndServeTLS监听srv.Addr确定的TCP地址，并且会调用Serve方法处理接收到的连接。必须提供证书文件和对应的私钥文件。如果证书是由权威机构签发的，certFile参数必须是顺序串联的服务端证书和CA证书。如果srv.Addr为空字符串，会使用&amp;rdquo;:https&amp;rdquo;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus入门</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus/</link>
          <pubDate>Thu, 29 Jun 2017 16:31:54 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus/</guid>
          <description>&lt;p&gt;Prometheus，它最早是借鉴了 Google 的 Borgmon 系统，完全是开源的，也是CNCF 下继 K8S 之后第二个项目。它们的开发人员都是原 Google 的 SRE，通过 HTTP 的方式来做数据收集，对其最深远的应该是其被设计成一个 self sustained 的系统，也就是说它是完全独立的系统，不需要外部依赖。&lt;/p&gt;

&lt;h1 id=&#34;时序数据库的发展&#34;&gt;时序数据库的发展&lt;/h1&gt;

&lt;h2 id=&#34;时序数据&#34;&gt;时序数据&lt;/h2&gt;

&lt;p&gt;时序数据的种类：常规和不规则。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;开发人员比较常见和熟悉的是常规时间序列，它只在规定的时间间隔内进行测量，如每10秒钟一次，通常会发生在传感器中，定期读取数据。常规时间序列代表了一些基本的原始事件流或分发。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不规则时间序列则对应离散事件，主要是针对API，例如股票交易。如果要以1分钟间隔计算API的平均响应时间，可以聚合各个请求以生成常规时间序列。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;关系型数据库和nosql&#34;&gt;关系型数据库和nosql&lt;/h2&gt;

&lt;p&gt;使用mysql或者分布式数据库cassandra等，数据频繁插入操作，数据量很大，查询困难，还需要不停的进行分区分表，在应用级获取的时候需要要大量的代码控制，所以需要一个时序数据库。&lt;/p&gt;

&lt;p&gt;nosql可以很好的处理的大规模数据的处理查询，但是缺乏规范的sql，现在虽然每种nosql都得到了广泛的应用，但是其实都是缓存数据库的思想，每中nosql都要有自己学习的成本，当然这个并不是使用时序数据库的理由，相反，缓存数据库在很多场景下都是得到的重用，但是针对一些特殊场景，比如以时间为主轴的数据，观察变化趋势的，优化后的时序数据库则拥有了更好的数据存储处理查询能力&lt;/p&gt;

&lt;p&gt;时间序列数据跟关系型数据库有太多不同，但是很多公司并不想放弃关系型数据库。于是就产生了一些特殊的用法，比如：用 MySQL 的 VividCortex, 用 Postgres 的 TimescaleDB；当然，还有人依赖K-V、NoSQL数据库或者列式数据库的，比如：OpenTSDB的HBase，而Druid则是一个不折不扣的列式存储系统；更多人觉得特殊的问题需要特殊的解决方法，于是很多时间序列数据库从头写起，不依赖任何现有的数据库, 比如： Graphite，InfluxDB。&lt;/p&gt;

&lt;p&gt;时序数据库基本上是基于缓存（nosql思想）的基础上处理大规模的数据，并且在一些场景，比如以时间为主轴的数据变化趋势：自动驾驶，交易，监控等行业，就需要时序数据库进行大规模的数据处理，用于跟踪历史数据。&lt;/p&gt;

&lt;p&gt;现在生活中时序的场景很多很多，所以时序数据库很受需要，已经成为发展最快的一种数据库。&lt;/p&gt;

&lt;p&gt;下面我们来全面对比一下关系数据库和时序数据库&lt;/p&gt;

&lt;p&gt;时序数据库&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;数据写入&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;时间是一个主坐标轴，数据通常按照时间顺序抵达&lt;/li&gt;
&lt;li&gt;大多数测量是在观察后的几秒或几分钟内写入的，抵达的数据几乎总是作为新条目被记录&lt;/li&gt;
&lt;li&gt;95％到99％的操作是写入，有时更高&lt;/li&gt;
&lt;li&gt;更新几乎没有&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据读取&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;随机位置的单个测量读取、删除操作几乎没有&lt;/li&gt;
&lt;li&gt;读取和删除是批量的，从某时间点开始的一段时间内&lt;/li&gt;
&lt;li&gt;时间段内读取的数据有可能非常巨大&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据存储&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据结构简单，价值随时间推移迅速降低&lt;/li&gt;
&lt;li&gt;通过压缩、移动、删除等手段降低存储成本&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而关系数据库主要应对的数据特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据写入：大多数操作都是DML操作，插入、更新、删除等；&lt;/li&gt;
&lt;li&gt;数据读取：读取逻辑一般都比较复杂；&lt;/li&gt;
&lt;li&gt;数据存储：很少压缩，一般也不设置数据生命周期管理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;针对这些特点，致使我们使用时序数据库，我们来看一下需要使用时序数据库的主要的特点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基本上都是插入，没有更新的需求。&lt;/li&gt;
&lt;li&gt;数据基本上都有时间属性，随着时间的推移不断产生新的数据。&lt;/li&gt;
&lt;li&gt;数据量大，每秒钟需要写入千万、上亿条数据&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为什么要使用时序数据库？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;因为数据量大，并且大部分都是写入的要求，并且要求性能特别高，并且有时间属性。这类数据使用时序数据库的特殊处理方式（以缓存为基础，以时间为主轴来存储数据），比较快捷高效&lt;/li&gt;
&lt;li&gt;数据重复性特别大，使用压缩来降低存储成本。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;时序数据库&#34;&gt;时序数据库&lt;/h2&gt;

&lt;h3 id=&#34;基本概念&#34;&gt;基本概念&lt;/h3&gt;

&lt;p&gt;一些基本概念(不同的时序数据库称呼略有不同)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Metric:  度量，相当于关系型数据库中的 table。&lt;/li&gt;
&lt;li&gt;Data point:  数据点，相当于关系型数据库中的 row。&lt;/li&gt;
&lt;li&gt;Timestamp：时间戳，代表数据点产生的时间。&lt;/li&gt;
&lt;li&gt;Field:  度量下的不同字段。比如位置这个度量具有经度和纬度两个 field。一般情况下存放的是随时间戳而变化的数据。&lt;/li&gt;
&lt;li&gt;Tag:  标签。一般存放的是不随时间戳变化的信息。timestamp 加上所有的 tags 可以视为 table 的 primary key。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如采集有关风的数据，度量为 Wind，每条数据都有时间戳timestamp，两个字段 field：direction(风向)、speed(风速)，两个tag：sensor(传感器编号)、city(城市)。&lt;/p&gt;

&lt;p&gt;业务方常见需求&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取最新状态，查询最近的数据(例如传感器最新的状态)&lt;/li&gt;
&lt;li&gt;展示区间统计，指定时间范围，查询统计信息，例如平均值，最大值，最小值，计数等。。。&lt;/li&gt;
&lt;li&gt;获取异常数据，根据指定条件，筛选异常数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;常见业务场景&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;监控软件系统： 虚拟机、容器、服务、应用&lt;/li&gt;
&lt;li&gt;监控物理系统： 水文监控、制造业工厂中的设备监控、国家安全相关的数据监控、通讯监控、传感器数据、血糖仪、血压变化、心率等&lt;/li&gt;
&lt;li&gt;资产跟踪应用： 汽车、卡车、物理容器、运货托盘&lt;/li&gt;
&lt;li&gt;金融交易系统： 传统证券、新兴的加密数字货币&lt;/li&gt;
&lt;li&gt;事件应用程序： 跟踪用户、客户的交互数据&lt;/li&gt;
&lt;li&gt;商业智能工具： 跟踪关键指标和业务的总体健康情况&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在互联网行业中，也有着非常多的时序数据，例如用户访问网站的行为轨迹，应用程序产生的日志数据等等。&lt;/p&gt;

&lt;h3 id=&#34;主流时序数据库&#34;&gt;主流时序数据库&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;influxdb，opentsdb，Graphite，prometheus，HiTSDB，LinDB
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;InfluxDB：很多公司都在用，包括饿了么有部分监控系统也是用的InfluxDB。其优点在于支持多维和多字段，存储也根据TSDB的特点做了优化，不过开源的部分并不支持。很多公司自己做集群化，但大多基于指标名来，这样就会有单指的热点问题。现在饿了么也是类似的做法，但热点问题很严重，大的指标已经用了最好的服务器，可查询性能还是不够理想，如果做成按Series Sharding，那成本还是有一点高；&lt;/li&gt;
&lt;li&gt;Graphite：根据指标写入及查询，计算函数很多，但很难支持多维，包括机房或多集群的查询。原来饿了么把业务层的监控指标存储在Graphite中，并工作的很好，不过多活之后基本已经很难满足一些需求了，由于其存储结构的特点，很占IO，根据目前线上的数据写放大差不多几十倍以上；&lt;/li&gt;
&lt;li&gt;OpenTSDB：基于HBase，优点在于存储层不用自己考虑，做好查询聚合就可以，也会存在HBase的热点问题等。在以前公司也用基于HBase实现的TSDB来解决OpenTSDB的一些问题， 如热点、部分查询聚合下放到HBase等，目的是优化其查询性能，但依赖HBase/HDFS还是很重；&lt;/li&gt;
&lt;li&gt;HiTSDB：阿里提供的TSDB，存储也是用HBase，在数据结构及Index上面做了很多优化，具体没有研究。&lt;/li&gt;
&lt;li&gt;LinDB：饿了么轻量级分布式时序数据库，基础组件如下

&lt;ul&gt;
&lt;li&gt;LinProxy主要做一些SQL的解析及一些中间结合的再聚合计算，如果不是跨集群，LinProxy可以不需要，对于单集群的每个节点都内嵌了一个LinProxy来提供查询服务；&lt;/li&gt;
&lt;li&gt;LinDB Client主要用于数据的写入，也有一些查询的API；&lt;/li&gt;
&lt;li&gt;LinStorage的每个节点组成一个集群，节点之间进行复制，并有副本的Leader节点提供读写服务，这点设计主要是参考Kafka的设计，可以把LinDB理解成类Kafka的数据写入复制+底层时间序列的存储层；&lt;/li&gt;
&lt;li&gt;LinMaster主要负责database、shard、replica的分配，所以LinStorage存储的调度及MetaData（目前存储Zookeeper中）的管理；由于LinStorage Node都是对等的，所以我们基于Zookeeper在集群的节点选一个成为Master，每个Node把自身的状态以心跳的方式上报到Master上，Master根据这些状态进行调度，如果Master挂了，自动再选一个Master出来，这个过程基本对整个服务是无损的，所以用户基本无感知。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;prometheus&#34;&gt;prometheus&lt;/h1&gt;

&lt;h2 id=&#34;安装编译&#34;&gt;安装编译&lt;/h2&gt;

&lt;p&gt;可以通过源码编译也可以通过下载二进制包，还可以通过docker启动，如果是源码编译很简单，clone下代码make build一下就行，会产生二进制文件prometheus，&lt;/p&gt;

&lt;p&gt;下载tar包二进制文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar xvfz prometheus-*.tar.gz
cd prometheus-*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./prometheus --config.file=prometheus.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用启动参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--storage.tsdb.path指定的路径存储文件，默认为./data
--web.listen-address=0.0.0.0:9090 指定监听的ip和端口
--config.file=/opt/prometheus-2.4.2.linux-amd64-k8s/prometheus.yml 指定启动的配置文件
--storage.tsdb.retention=10d 指定数据存储时间
--log.level=info 指定日志等级
--query.max-concurrency=2000 指定查询并发数量
--web.max-connections=4096 指定连接数
--web.read-timeout=40s 界面查询超时时间
--query.timeout=40s 指定查询超时时间
--query.lookback-delta=3600s 查询最长多少时间范围内的点
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;全部启动参数：2.4.2版本的详细说明，随着升级会有对应的变化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@promessitapp05 k8s-prometheus-2.4.2.linux-amd64-k8s]# ./prometheus -h
usage: prometheus [&amp;lt;flags&amp;gt;]

The Prometheus monitoring server

Flags:
  -h, --help                     Show context-sensitive help (also try --help-long and --help-man).
      --version                  Show application version.
      --config.file=&amp;quot;prometheus.yml&amp;quot;
                                 Prometheus configuration file path.
      --web.listen-address=&amp;quot;0.0.0.0:9090&amp;quot;
                                 Address to listen on for UI, API, and telemetry.
      --web.read-timeout=5m      Maximum duration before timing out read of the request, and closing idle connections.
      --web.max-connections=512  Maximum number of simultaneous connections.
      --web.external-url=&amp;lt;URL&amp;gt;   The URL under which Prometheus is externally reachable (for example, if Prometheus is served via a reverse proxy). Used for generating relative and
                                 absolute links back to Prometheus itself. If the URL has a path portion, it will be used to prefix all HTTP endpoints served by Prometheus. If
                                 omitted, relevant URL components will be derived automatically.
      --web.route-prefix=&amp;lt;path&amp;gt;  Prefix for the internal routes of web endpoints. Defaults to path of --web.external-url.
      --web.user-assets=&amp;lt;path&amp;gt;   Path to static asset directory, available at /user.
      --web.enable-lifecycle     Enable shutdown and reload via HTTP request.
      --web.enable-admin-api     Enable API endpoints for admin control actions.
      --web.console.templates=&amp;quot;consoles&amp;quot;
                                 Path to the console template directory, available at /consoles.
      --web.console.libraries=&amp;quot;console_libraries&amp;quot;
                                 Path to the console library directory.
      --storage.tsdb.path=&amp;quot;data/&amp;quot;
                                 Base path for metrics storage.
      --storage.tsdb.retention=15d
                                 How long to retain samples in storage.
      --storage.tsdb.no-lockfile
                                 Do not create lockfile in data directory.
      --storage.remote.flush-deadline=&amp;lt;duration&amp;gt;
                                 How long to wait flushing sample on shutdown or config reload.
      --storage.remote.read-sample-limit=5e7
                                 Maximum overall number of samples to return via the remote read interface, in a single query. 0 means no limit.
      --rules.alert.for-outage-tolerance=1h
                                 Max time to tolerate prometheus outage for restoring &#39;for&#39; state of alert.
      --rules.alert.for-grace-period=10m
                                 Minimum duration between alert and restored &#39;for&#39; state. This is maintained only for alerts with configured &#39;for&#39; time greater than grace period.
      --rules.alert.resend-delay=1m
                                 Minimum amount of time to wait before resending an alert to Alertmanager.
      --alertmanager.notification-queue-capacity=10000
                                 The capacity of the queue for pending Alertmanager notifications.
      --alertmanager.timeout=10s
                                 Timeout for sending alerts to Alertmanager.
      --query.lookback-delta=5m  The delta difference allowed for retrieving metrics during expression evaluations.就是查询当前时间前多长时间的数据中最新的一个数据，当配置较小的时候，可能采集间隔过大而获取不到数据。
      --query.timeout=2m         Maximum time a query may take before being aborted.
      --query.max-concurrency=20
                                 Maximum number of queries executed concurrently.
      --log.level=info           Only log messages with the given severity or above. One of: [debug, info, warn, error]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d--name=prometheus     --publish=9090:9090-v /etc/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml     -v /var/prometheus/storage:/prometheus     prom/prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;p&gt;1、就是上面的二进制或者docker直接启动&lt;/p&gt;

&lt;p&gt;2、k8s部署&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;直接使用这个项目中的yaml文件&lt;a href=&#34;https://github.com/giantswarm/prometheus&#34;&gt;https://github.com/giantswarm/prometheus&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Prometheus Operator部署&lt;/p&gt;

&lt;p&gt;具体可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/&#34;&gt;Prometheus Operator&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;配置文件&#34;&gt;配置文件&lt;/h2&gt;

&lt;p&gt;通常的配置文件如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# my global config全局配置
global:
  scrape_interval:     15s # By default, scrape targets every 15 seconds.采集频率
  evaluation_interval: 15s # By default, scrape targets every 15 seconds.规则计算的频率
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  # 给全局指标增加一个label
  external_labels:
      monitor: &#39;codelab-monitor&#39;

# Load and evaluate rules in this file every &#39;evaluation_interval&#39; seconds.
# 告警规则文件
rule_files:
  # - &amp;quot;first.rules&amp;quot;
  # - &amp;quot;second.rules&amp;quot;
  - &amp;quot;alert.rules&amp;quot;
  # - &amp;quot;record.rules&amp;quot;


#lertmanager configuration
# altermanager服务器的配置，所有的地址都要配置
alerting:
  alertmanagers:
  - static_configs:
    - targets: [&#39;10.242.182.161:9093&#39;,&#39;10.242.182.166:9093&#39;]


# A scrape configuration containing exactly one endpoint to scrape:
# Here it&#39;s Prometheus itself.
# 采集配置
scrape_configs:
  # The job name is added as a label `job=&amp;lt;job_name&amp;gt;` to any timeseries scraped from this config.
  # job的名字
  - job_name: &#39;windows-test&#39;

    # Override the global default and scrape targets from this job every 5 seconds.
    # 每个job可以单独设置采集频率，但是这个不能在label中设置，也就是说只能一个job一个采集频率，不能一个target一个采集频率
    scrape_interval: 1s

    # metrics_path defaults to &#39;/metrics&#39;，
    # 可以设置采集路经,默认是metrics，这个参数可以在label中设置
    metrics_path: /probe

    # Optional HTTP URL parameters.
    # params:
    #  [ &amp;lt;string&amp;gt;: [&amp;lt;string&amp;gt;, ...] ]
    # target的URL的请求参数，比如http://10.27.241.4:10260/metrics?all，就是k/v结构
    params:
        all: [&amp;quot;&amp;quot;]

    # 这边还有一个match的使用方法
    # 只采集job是node_exporter_1的数据。
    params:
      match[]:
        - &#39;{job=~&amp;quot;node_exporter_1&amp;quot;}&#39;

    # scheme defaults to &#39;http&#39;.
    # 可以设置http的方式，默认http，这个参数也可以在label中设置
    scheme： http

    # 静态target的配置，也可以使用其他的服务发现，但是都是job统一级别的
    static_configs:
      - targets: [&#39;192.168.3.1:9090&#39;,&#39;192.168.3.120:9090&#39;]
      # 可以直接设置采集数据的标签
        labels:
            appid : &#39;mycat&#39;



    # Sets the `Authorization` header on every scrape request with the
    # configured username and password.
    # password and password_file are mutually exclusive.
    # basic_auth:
    #  [ username: &amp;lt;string&amp;gt; ]
    #  [ password: &amp;lt;secret&amp;gt; ]
    #  [ password_file: &amp;lt;string&amp;gt; ]
    # 访问https的时候可以带上用户名和密码

    basic_auth:
      username: &amp;quot;admin&amp;quot;
      password: &amp;quot;Pwd123456&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面是默认的使用方式，使用的是static_configs直接静态配置ip，也可以使用一些服务发现来动态更新IP。&lt;/p&gt;

&lt;h3 id=&#34;服务发现&#34;&gt;服务发现&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;static_configs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;static_configs直接静态配置ip&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;文件服务发现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;file_sd_config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;node&#39;
file_sd_configs:
  - files:
    - /opt/promes/harbor-prometheus-2.4.2.linux-amd64/discoveries/node/discovery.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是使用了json文件的服务发现，可以把对应的target和label写入json文件，这边就可以使用一些模版生产工具（consul-template）来生成对应的json文件&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kubernetes_sd_configs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要参考&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s/#使用k8s的服务发现&#34;&gt;k8s监控方案中的prometheus in k8s的配置文件解析&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;这里我们主要关注在kubernetes下的采集目标发现的配置，Prometheus支持通过kubernetes的Rest API动态发现采集的目标Target信息，包括kubernetes下的node,service,pod,endpoints等信息。&lt;/p&gt;

&lt;p&gt;kubernets_sd_config下面role类型中的任何一个都能在发现目标上配置：&lt;/p&gt;

&lt;p&gt;节点node&lt;/p&gt;

&lt;p&gt;这个node角色发现带有地址的每一个集群节点一个目标，都指向Kublelet的HTTP端口。这个目标地址默认为Kubernetes节点对象的第一个现有地址，地址类型为NodeInernalIP, NodeExternalIP, NodeLegacyHostIP和NodeHostName。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_node_name: 节点对象的名称
__meta_kubernetes_node_label_&amp;lt;labelname&amp;gt;: 节点对象的每个标签
__meta_kubernetes_node_annotation_&amp;lt;annotationname&amp;gt;: 节点对象的每个注释
_meta_kubernetes_node_address&amp;lt;address_type&amp;gt;: 如果存在，每一个节点对象类型的第一个地址
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，对于节点的instance标签，将会被设置成从API服务中获取的节点名称。&lt;/p&gt;

&lt;p&gt;服务service&lt;/p&gt;

&lt;p&gt;对于每个服务每个服务端口，service角色发现一个目标。对于一个服务的黑盒监控是通常有用的。这个地址被设置成这个服务的Kubernetes DNS域名, 以及各自的服务端口。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_namespace: 服务对象的命名空间
__meta_kubernetes_service_name: 服务对象的名称
__meta_kubernetes_service_label_&amp;lt;labelname&amp;gt;: 服务对象的标签。
__meta_kubernetes_service_annotation_&amp;lt;annotationname&amp;gt;: 服务对象的注释
__meta_kubernetes_service_port_name: 目标服务端口的名称
__meta_kubernetes_service_port_number: 目标服务端口的数量
__meta_kubernetes_service_port_portocol: 目标服务端口的协议
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note：这里Service中同样标注了 prometheus.io/scrape: ‘true’从而确保prometheus会采集数据。&lt;/p&gt;

&lt;p&gt;pod&lt;/p&gt;

&lt;p&gt;pod角色发现所有的pods，并暴露它们的容器作为目标。对于每一个容器的声明端口，单个目标被生成。 如果一个容器没有指定端口，每个容器的无端口目标都是通过relabeling手动添加端口而创建的。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_namespace: pod对象的命名空间
__meta_kubernetes_pod_name: pod对象的名称
__meta_kubernetes_pod_ip: pod对象的IP地址
__meta_kubernetes_pod_label_&amp;lt;labelname&amp;gt;: pod对象的标签
__meta_kubernetes_pod_annotation_&amp;lt;annotationname&amp;gt;: pod对象的注释
__meta_kubernetes_pod_container_name: 目标地址的容器名称
__meta_kubernetes_pod_container_port_name: 容器端口名称
__meta_kubernetes_pod_container_port_number: 容器端口的数量
__meta_kubernetes_pod_container_port_protocol: 容器端口的协议
__meta_kubernetes_pod_ready: 设置pod ready状态为true或者false
__meta_kubernetes_pod_node_name: pod调度的node名称
__meta_kubernetes_pod_host_ip: 节点对象的主机IP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;endpoints端点&lt;/p&gt;

&lt;p&gt;endpoints角色发现来自于一个服务的列表端点目标。对于每一个终端地址，一个目标被一个port发现。如果这个终端被写入到pod中，这个节点的所有其他容器端口，未绑定到端点的端口，也会被目标发现。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_namespace: 端点对象的命名空间
__meta_kubernetes_endpoints_name: 端点对象的名称
对于直接从端点列表中获取的所有目标，下面的标签将会被附加上。
__meta_kubernetes_endpoint_ready: endpoint ready状态设置为true或者false。
__meta_kubernetes_endpoint_port_name: 端点的端口名称
__meta_kubernetes_endpoint_port_protocol: 端点的端口协议

如果端点属于一个服务，这个角色的所有标签：服务发现被附加上。
对于在pod中的所有目标，这个角色的所有表掐你：pod发现被附加上
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于Kuberntes发现，看看下面的配置选项：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# The information to access the Kubernetes API.

# The API server addresses. If left empty, Prometheus is assumed to run inside
# of the cluster and will discover API servers automatically and use the pod&#39;s
# CA certificate and bearer token file at /var/run/secrets/kubernetes.io/serviceaccount/.
[ api_server: &amp;lt;host&amp;gt; ]

# The Kubernetes role of entities that should be discovered.
role: &amp;lt;role&amp;gt;

# Optional authentication information used to authenticate to the API server.
# Note that `basic_auth`, `bearer_token` and `bearer_token_file` options are
# mutually exclusive.

# Optional HTTP basic authentication information.
basic_auth:
  [ username: &amp;lt;string&amp;gt; ]
  [ password: &amp;lt;string&amp;gt; ]

# Optional bearer token authentication information.
[ bearer_token: &amp;lt;string&amp;gt; ]

# Optional bearer token file authentication information.
[ bearer_token_file: &amp;lt;filename&amp;gt; ]

# TLS configuration.
tls_config:
  [ &amp;lt;tls_config&amp;gt; ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;role&amp;gt;&lt;/code&gt;必须是endpoints, service, pod或者node。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;consul服务发现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;consul_sd_configs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;TEST_NEW_1&#39;
    scrape_interval:     30s
    consul_sd_configs:
      - server: &#39;192.47.178.100:9996&#39;
        services: [&#39;node_exporter_1&#39;]
    relabel_configs:
    - source_labels: [&#39;__meta_consul_service&#39;]
      regex:         &#39;(.*)&#39;
      target_label:  &#39;job&#39;
      replacement:   &#39;PROMES_$1&#39;
    - source_labels: [&#39;__meta_consul_node&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;instance&#39;
      replacement:   &#39;$4&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;appId&#39;
      replacement:   &#39;$1&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;ldc&#39;
      replacement:   &#39;$2&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;env&#39;
      replacement:   &#39;$3&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;ip&#39;
      replacement:   &#39;$4&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;softType&#39;
      replacement:   &#39;$5&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;software&#39;
      replacement:   &#39;$6&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;exporter&#39;
      replacement:   &#39;$7&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;exporterVersion&#39;
      replacement:   &#39;$8&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由上面的配置可见，配置consul的服务器的地址和对应的services的名字就可以匹配到api注册需要采集对应的配置。&lt;/p&gt;

&lt;p&gt;consul服务发现中支持一下内部使用的metadata：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_consul_address: the address of the target
__meta_consul_dc: the datacenter name for the target
__meta_consul_tagged_address_&amp;lt;key&amp;gt;: each node tagged address key value of the target
__meta_consul_metadata_&amp;lt;key&amp;gt;: each node metadata key value of the target
__meta_consul_node: the node name defined for the target
__meta_consul_service_address: the service address of the target
__meta_consul_service_id: the service ID of the target
__meta_consul_service_metadata_&amp;lt;key&amp;gt;: each service metadata key value of the target
__meta_consul_service_port: the service port of the target
__meta_consul_service: the name of the service the target belongs to
__meta_consul_tags: the list of tags of the target joined by the tag separator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过注册tags的编号来替换对应内部专门使用的变量的值，来完成label的注册。&lt;/p&gt;

&lt;p&gt;这种注册和服务发现的模式，需要一直去请求consul的api，当数据量大的时候，会出现超时现象的性能瓶颈，影响采集的动态更新，小规模使用比较好，services还有自检的功能，但是大规模，直接使用k/v结构存储注册数据，作为数据来来使用，然后使用第三方模版工具(consul-template)生成json文件，来完成动态更新，etcd+confd也是类似的模式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他还有很多服务发现，没有用过，先不做说明。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;prometheus的relabeling机制&#34;&gt;Prometheus的Relabeling机制&lt;/h3&gt;

&lt;p&gt;在Prometheus所有的Target实例中，都包含一些默认的Metadata标签信息。可以通过Prometheus UI的Targets页面中查看这些实例的Metadata标签的内容：&lt;/p&gt;

&lt;p&gt;默认情况下，当Prometheus加载Target实例完成后，这些Target时候都会包含一些默认的标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__address__：当前Target实例的访问地址&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;
__scheme__：采集目标服务访问地址的HTTP Scheme，HTTP或者HTTPS
__metrics_path__：采集目标服务访问地址的访问路径
__param_&amp;lt;name&amp;gt;：采集任务目标服务的中包含的请求参数
__name__是特定的label标签，代表了metric name。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这些标签将会告诉Prometheus如何从该Target实例中获取监控数据。除了这些默认的标签以外，我们还可以为Target添加自定义的标签，也就是我们平常使用的label&lt;/p&gt;

&lt;p&gt;一般来说，Target以&lt;code&gt;__&lt;/code&gt;作为前置的标签是作为系统内部使用的，因此这些标签不会被写入到样本数据中。不过这里有一些例外，例如，我们会发现所有通过Prometheus采集的样本数据中都会包含一个名为instance的标签，该标签的内容对应到Target实例的&lt;code&gt;__address__&lt;/code&gt;。 这里实际上是发生了一次标签的重写处理。&lt;/p&gt;

&lt;p&gt;这种发生在采集样本数据之前，对Target实例的标签进行重写的机制在Prometheus被称为Relabeling。&lt;/p&gt;

&lt;p&gt;Promtheus允许用户在采集任务设置中通过relabel_configs来添加自定义的Relabeling过程。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;relabel_config&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;relabel_config的作用就是将时间序列中 label 的值做一个替换，具体的替换规则有配置决定，默认 job 的值是 job_name，&lt;code&gt;__address__&lt;/code&gt;的值为&lt;code&gt;&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;，instance的值默认就是 &lt;code&gt;__address__，__param_&amp;lt;name&amp;gt;&lt;/code&gt;的值就是请求url中&lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt;的值 &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;blackbox&#39;
metrics_path: /probe
params:
  module: [http_2xx]  # Look for a HTTP 200 response.
static_configs:
  - targets: [&amp;quot;https://test.com/api/projects&amp;quot;]
relabel_configs:
  - source_labels: [__address__]
    target_label: __param_target
  - source_labels: [__param_target]
    target_label: instance
  - target_label: __address__
    replacement: 10.243.129.101:9115  # The blackbox exporter&#39;s real hostname:port.
basic_auth:
  username: &amp;quot;admin&amp;quot;
  password: &amp;quot;Pwd123456&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这个配置的意思就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__param_target = __address__ ，&amp;lt;- https://test.com/api/projects
instance = __param_target &amp;lt;- https://test.com/api/projects
__address__ = 10.243.129.101:9115。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus最后是根据&lt;code&gt;__address__&lt;/code&gt;来作为采集的地址来拉去数据的。可以看出默认情况下，targets将地址给了&lt;code&gt;__address__&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;具体规则做一个简单的说明，其实就是relabel_action所决定的&lt;/p&gt;

&lt;p&gt;&lt;relabel_action&gt; determines the relabeling action to take:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;replace: Match regex against the concatenated source_labels. Then, set target_label to replacement, with match group references (${1}, ${2}, &amp;hellip;) in replacement substituted by their value. If regex does not match, no replacement takes place.&lt;/li&gt;
&lt;li&gt;keep: Drop targets for which regex does not match the concatenated source_labels.&lt;/li&gt;
&lt;li&gt;drop: Drop targets for which regex matches the concatenated source_labels.&lt;/li&gt;
&lt;li&gt;hashmod: Set target_label to the modulus of a hash of the concatenated source_labels.&lt;/li&gt;
&lt;li&gt;labelmap: Match regex against all label names. Then copy the values of the matching labels to label names given by replacement with match group references (${1}, ${2}, &amp;hellip;) in replacement substituted by their value.&lt;/li&gt;
&lt;li&gt;labeldrop: Match regex against all label names. Any label that matches will be removed from the set of labels.&lt;/li&gt;
&lt;li&gt;labelkeep: Match regex against all label names. Any label that does not match will be removed from the set of labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;重新贴标签的工作如下（对应每行数据）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;定义源标签列表。&lt;/li&gt;
&lt;li&gt;对于每个目标，这些标签的值与分隔符连接。&lt;/li&gt;
&lt;li&gt;正则表达式与结果字符串匹配。&lt;/li&gt;
&lt;li&gt;基于这些匹配的新值被分配给另一个标签。&lt;/li&gt;
&lt;li&gt;可以为每个刮擦配置定义多个重新标记规则。简单的将两个标签压成一个，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例看起来如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;relabel_configs:
- source_labels: [&#39;label_a&#39;, &#39;label_b&#39;]
  separator:     &#39;;&#39;
  regex:         &#39;(.*);(.*)&#39;
  replacement:   &#39;${1}-${2}&#39;
  target_label:  &#39;label_c&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这条规则用标签集转换目标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;job&amp;quot;: &amp;quot;job1&amp;quot;,
  &amp;quot;label_a&amp;quot;: &amp;quot;foo&amp;quot;,
  &amp;quot;label_b&amp;quot;: &amp;quot;bar&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;成为标签集的目标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;job&amp;quot;: &amp;quot;job1&amp;quot;,
  &amp;quot;label_a&amp;quot;: &amp;quot;foo&amp;quot;,
  &amp;quot;label_b&amp;quot;: &amp;quot;bar&amp;quot;,
  &amp;quot;label_c&amp;quot;: &amp;quot;foo-bar&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;separator&lt;/p&gt;

&lt;p&gt;意思是如果有多个&lt;code&gt;source_label([__address__,jod])&lt;/code&gt;的时候用separator去连接几个值&lt;/p&gt;

&lt;p&gt;regex&lt;/p&gt;

&lt;p&gt;意思是符合这个正则表达式的source_label会被赋值给replacement再赋值给target_label&lt;/p&gt;

&lt;p&gt;也可以在采集的时候drop掉某些label&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#如下是删除一个原来的标签
- action: labeldrop
  regex: job
- action: labeldrop
  regex: soft.*
- action: labeldrop
  regex: exporter.*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以在采集的时候不采集一类指标符合正则表达式，使用的是一个新的域标签metric_relabel_configs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metric_relabel_configs:
- source_labels: [ __name__ ]
  regex: &#39;go.*&#39;
  action: drop
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;hashmod&#34;&gt;hashmod&lt;/h3&gt;

&lt;p&gt;hashmod是基于服务发现的基础中的一种分布式集群的实现方式，多个prometheus实例来平均分配采集任务，完成prometheus的水平扩展。&lt;/p&gt;

&lt;p&gt;可以结合lb来负载均衡，也可以来指定ip去采集对应的数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: ibmmq
metrics_path: /metrics
params:
  module: [ibm-mq]
file_sd_configs:
  - files:
    - /opt/prometheus/discoveries/discovery.json
relabel_configs:
- source_labels: [__address__]
  modulus:       3    # 0 slaves
  target_label:  __tmp_hash
  action:        hashmod
- source_labels: [__tmp_hash]
  regex:         ^2$  # This is the 2nd slave
  action:        keep
- source_labels: [__address__]
  target_label: __param_target
- source_labels: [__param_target]
  target_label: instance
- target_label: __address__
  replacement: 10.47.247.214:9115
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当relabel_config设置为hashmod时，Promtheus会根据modulus的值作为系数，计算source_labels值的hash值。&lt;/p&gt;

&lt;p&gt;根据当前Target实例&lt;strong&gt;address&lt;/strong&gt;的值以4作为系数，这样每个Target实例都会包含一个新的标签tmp_hash，并且该值的范围在1~4之间。&lt;/p&gt;

&lt;p&gt;如果relabel的操作只是为了产生一个临时变量，以作为下一个relabel操作的输入，那么我们可以使用__tmp作为标签名的前缀，通过该前缀定义的标签就不会写入到Target或者采集到的样本的标签中。&lt;/p&gt;

&lt;p&gt;上面的可以理解为&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置的第一个 souce_labels 是对同一个任务抓取目标的 LabelSet 进行预处理，具体而言就是将抓取目标地址进行 hashmod, 并将 hashmod 的值存到一个自定义字段 __tmp_hash 中。&lt;/li&gt;
&lt;li&gt;配置的第二个 souce_labels 对预处理后的抓取目标进行筛选，只选取 __tmp_hash 值满足正则匹配的，例子中 hashmod != 2 将全部被忽略。&lt;/li&gt;
&lt;li&gt;通过以上两步，就非常容易对相同 job 的抓取目标进行散列，从而抓取命中的部分。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以采用 hashmod 配置，使用同样的配置列表，将抓取目标散列到不同的 Prometheus server 中去, 从而很好实现 Prometheus 数据收集的水平扩展。&lt;/p&gt;

&lt;h3 id=&#34;远程读写&#34;&gt;远程读写&lt;/h3&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#remote_read:
#  - url: &amp;quot;http://localhost:7201/api/v1/prom/remote/read&amp;quot;
    # To test reading even when local Prometheus has the data
#    read_recent: true
#remote_write:
#  - url: &amp;quot;http://localhost:7201/api/v1/prom/remote/write&amp;quot;
#  - url: &amp;quot;http://10.47.178.80:9268/write&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是m3db的远程读写的配置，prometheus采集的数据就会直接发生到prometheus的apadter中，然后通过调用m3db的接口，将数据存储在m3db中，查询也直接在m3db中查询数据。&lt;/p&gt;

&lt;p&gt;远程读写是prometheus的一个扩展功能，prometheus自身主要是做时序数据库，关于存储提供了一个&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/#remote-storage&#34;&gt;可扩展性的方案&lt;/a&gt;，可以自己实现，目前已经有很多项目支持prometheus远程存储，比如&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/&#34;&gt;m3db&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/&#34;&gt;cortex&lt;/a&gt;,&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM&lt;/a&gt;等，目前VM在这一块做的还是比较好的。&lt;/p&gt;

&lt;h3 id=&#34;支持密钥文件校验-也可以跳过密钥校验&#34;&gt;支持密钥文件校验，也可以跳过密钥校验&lt;/h3&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: k8s-etcd
file_sd_configs:
  - files:
    - /opt/prometheus/discoveries/discovery-etcd.json
scheme: https
tls_config:
  ca_file: /opt/prometheus/ssl/etcd-ca.pem
  cert_file: /opt/prometheus/ssl/etcd.pem
  key_file:  /opt/k8s-prometheus/ssl/etcd-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;带着etcd的密钥证书去验证采集。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: k8s-other
file_sd_configs:
  - files:
    - /opt/prometheus-2.4.2.linux-amd64/discoveries/discovery-k8s.json
tls_config:
  insecure_skip_verify: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以直接跳过验证，前提是跳过验证能拉到数据。&lt;/p&gt;

&lt;h3 id=&#34;prometheus支持yml文件的服务发现实现路径重新设置&#34;&gt;prometheus支持yml文件的服务发现实现路径重新设置&lt;/h3&gt;

&lt;p&gt;I achieved this by using file_sd_config option. All targets are described in separate file(s), which can be either in YML or JSON format.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus.yml:

scrape_configs:
  - job_name: &#39;dummy&#39;  # This will be overridden in targets.yml
    file_sd_configs:
      - files:
        - targets.yml



targets.yml:

- targets: [&#39;host1:9999&#39;]
  labels:
    job: my_job
    __metrics_path__: /path1

- targets: [&#39;host2:9999&#39;]
  labels:
    job: my_job  # can belong to the same job
    __metrics_path__: /path2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;reload&#34;&gt;reload&lt;/h3&gt;

&lt;p&gt;Prometheus can reload its configuration at runtime.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kill -HUP pid
curl -X POST http://IP/-/reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus可以在运行时重新加载它的配置。 如果新配置格式不正确，则更改将不会应用。 通过向Prometheus进程发送SIGHUP或向/-/reload端点发送HTTP POST请求（启用&amp;ndash;web.enable-lifecycle标志时）来触发配置reload。 这也将重新加载任何配置的规则文件。&lt;/p&gt;

&lt;p&gt;我个人更倾向于采用 curl -X POST 的方式，因为每次 reload 过后， pid 会改变，使用 kill 方式需要找到当前进程号。
从 2.0 开始，hot reload 功能是默认关闭的，如需开启，需要在启动 Prometheus 的时候，添加 &amp;ndash;web.enable-lifecycle 参数。&lt;/p&gt;

&lt;h2 id=&#34;高级特性&#34;&gt;高级特性&lt;/h2&gt;

&lt;h3 id=&#34;prometheus分布式&#34;&gt;prometheus分布式&lt;/h3&gt;

&lt;p&gt;1、目前prometheus处理百万级的数据是完全没有问题的，也就是一千个服务器，一千个指标，以10S的频率去采集完全没有问题的，如果量级上去了，可以分业务进行多个prometheus进行采集使用，如果需要聚合，就需要使用prometheus的联邦集群，如果已经分业务但是量级还是不够，就是需要分group采集，然后聚合，其实也是分布式的概念。&lt;/p&gt;

&lt;p&gt;2、&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos&lt;/a&gt;+hashmod实现分布式采集聚合查询。&lt;/p&gt;

&lt;p&gt;3、自己的想法，想开发一个类似于redis cluster的分片的集群，使用raft算法，目前并没有相关的实现方案。&lt;/p&gt;

&lt;p&gt;4、使用远程读写，比如目前性能比较优秀的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;federation-联合&#34;&gt;FEDERATION(联合)&lt;/h3&gt;

&lt;p&gt;Federation允许一个Prometheus从另一个Prometheus中拉取某些指定的时序数据，Federation是Prometheus提供的扩展机制，允许Prometheus从一个节点扩展到多个节点，实际使用中一般会扩展成树状的层级结构。下面是Prometheus官方文档中对federation的配置示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;federate&#39;
  scrape_interval: 15s

  honor_labels: true
  metrics_path: &#39;/federate&#39;

  params:
    &#39;match[]&#39;:
      - &#39;{job=&amp;quot;prometheus&amp;quot;}&#39;
      - &#39;{__name__=~&amp;quot;job:.*&amp;quot;}&#39;

  static_configs:
    - targets:
      - &#39;source-prometheus-1:9090&#39;
      - &#39;source-prometheus-2:9090&#39;
      - &#39;source-prometheus-3:9090&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段配置所属的Prometheus将从source-prometheus-1 ~ 3这3个Prometheus的/federate端点拉取监控数据。 match[]参数指定了只拉取带有job=”prometheus标签的指标或者名称以job开头的指标。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;federation的使用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、物理使用&lt;/p&gt;

&lt;p&gt;就是上面使用方式，将几个prometheus的数据聚合到一个prometheus中，往往就是使用几个性能差的机器来采集部分数据，然后使用性能好的来聚合，也缓解了探针连接和拉去的压力。&lt;/p&gt;

&lt;p&gt;2、k8s使用federation&lt;/p&gt;

&lt;p&gt;要实现对Kubernetes集群的监控，因为Kubernetes的rbac机制以及证书认证，当然是把Prometheus部署在Kubernetes集群上最方便。可是很多监控系统是以k8s集群外部的Prometheus为主的，grafana和告警都是使用这个外部的Prometheus，如果还需要在Kubernetes集群内部部署一个Prometheus的话一定要把它连通外部的Prometheus联合起来，好在Prometheus支持Federation。&lt;/p&gt;

&lt;p&gt;前面已经介绍了将使用Prometheus federation的形式，k8s集群外部的Prometheus从k8s集群中Prometheus拉取监控数据，外部的Prometheus才是监控数据的存储。 k8s集群中部署Prometheus的数据存储层可以简单的使用emptyDir,数据只保留24小时(或更短时间)即可，部署在k8s集群上的这个Prometheus实例即使发生故障也可以放心的让它在集群节点中漂移。&lt;/p&gt;

&lt;p&gt;federation也只能在数据量不是太大的情况下使用，如果数据量太大，聚合到prometheus中单实例还是有着各种瓶颈，并不适合后期的聚合查询使用。&lt;/p&gt;

&lt;h3 id=&#34;prometheus高可用&#34;&gt;prometheus高可用&lt;/h3&gt;

&lt;p&gt;目前prometheus解决单点故障还是使用的是多份一致数据，启动多个prometheus对同一个数据进行采集，保留多分数据，但是数据是一致的，时序数据对一致性要求不高，可以容忍数据的部分丢失，对外是一个service。&lt;/p&gt;

&lt;h3 id=&#34;adapter&#34;&gt;adapter&lt;/h3&gt;

&lt;p&gt;adapter就是一个适配器，通用的功能就是为了适配，在prometheus中有很多需要使用的地方，在&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/#remote-storage&#34;&gt;远程存储&lt;/a&gt;中是一种使用方式，可以将数据转化到其他数据库适配的格式发送到对应的数据库中，还可以转换适配其他一些应用，还有我们使用的&lt;a href=&#34;https://github.com/DirectXMan12/k8s-prometheus-adapter&#34;&gt;k8s-prometheus-adapter&lt;/a&gt;也是一种&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-autoscaler/#基于prometheus&#34;&gt;方式&lt;/a&gt;，用于k8s重prometheus拉去指标。&lt;/p&gt;

&lt;h2 id=&#34;监控方案选择&#34;&gt;监控方案选择&lt;/h2&gt;

&lt;p&gt;一直纠结于选择Prometheus还是Open-falcon。这两者都是非常棒的新一代监控解决方案，后者是小米公司开源的，目前包括小米、金山云、美团、京东金融、赶集网等都在使用Open-Falcon，最大区别在于前者采用的是pull的方式获取数据，后者使用push的方式，暂且不说这两种方式的优缺点。简单说下我喜欢Prometheus的原因：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;开箱即用，部署运维非常方便&lt;/li&gt;
&lt;li&gt;prometheus的社区非常活跃&lt;/li&gt;
&lt;li&gt;自带服务发现功能&lt;/li&gt;
&lt;li&gt;简单的文本存储格式，进行二次开发非常方便。&lt;/li&gt;
&lt;li&gt;最重要的一点，他的报警插件我非常喜欢，带有分组、报警抑制、静默提醒机制。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里并没有贬低open-falcon的意思，还是那句老话适合自己的才是最好的。&lt;/p&gt;

&lt;h2 id=&#34;prometheus二次开发项目&#34;&gt;prometheus二次开发项目&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/yunlzheng/prometheus-pusher&#34;&gt;prometheus改造&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;使用总结&#34;&gt;使用总结&lt;/h2&gt;

&lt;p&gt;1、 正确关闭Prometheus有助于降低启动延迟的风险。那你怎么做的？&lt;/p&gt;

&lt;p&gt;如果没有干净地关闭普罗米修斯理论上应该能够在启动时正常恢复，但是它可能需要更长的时间，或者你可能会在软件堆栈的某处遇到一个模糊的错误，这会导致问题。因此，最好让普罗米修斯自己一个个关闭对应程序，直接使用kill pid，不要加-9，然后等待停止所需的时间，这通常不会花费太多时间。&lt;/p&gt;

&lt;p&gt;2、 prometheus只支持数值，可以为正可以为负，字符串只能作为标签。&lt;/p&gt;

&lt;p&gt;在Prometheus的世界里面，所有的数值都是64bit的。每条时间序列里面记录的其实就是64bit timestamp(时间戳) + 64bit value(采样值)。&lt;/p&gt;

&lt;p&gt;3、Prometheus有着非常高效的时间序列数据存储方法，每个采样数据仅仅占用3.5byte左右空间，上百万条时间序列，30秒间隔，保留60天，大概花了200多G（引用官方PPT）&lt;/p&gt;

&lt;p&gt;我们实际环境中，Node Exporter 有 251 个测量点，Prometheus 服务本身有 775 个测量点。每一千个时间序列大约需要 1M 内存。每条数据占用了1K的空间，可见加了很多标签在里面，数据量还是很可观的。&lt;/p&gt;

&lt;p&gt;4、metrics&lt;/p&gt;

&lt;p&gt;指标名称只能由ASCII字符、数字、下划线以及冒号组成并必须符合正则表达式&lt;code&gt;[a-zA-Z:][a-zA-Z0-9_:]*&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;标签的名称只能由ASCII字符、数字以及下划线组成并满足正则表达式&lt;code&gt;[a-zA-Z_][a-zA-Z0-9_]*&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;其中以&lt;code&gt;__&lt;/code&gt;作为前缀的标签，是系统保留的关键字，只能在系统内部使用。标签的值则可以包含任何Unicode编码的字符。在Prometheus的底层实现中指标名称实际上是以&lt;code&gt;__name__=&amp;lt;metric name&amp;gt;&lt;/code&gt;的形式保存在数据库中的，因此以下两种方式均表示的同一条time-series：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;api_http_requests_total{method=&amp;quot;POST&amp;quot;, handler=&amp;quot;/messages&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等同于：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{__name__=&amp;quot;api_http_requests_total&amp;quot;，method=&amp;quot;POST&amp;quot;, handler=&amp;quot;/messages&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pro将所有数据保存为timeseries data，用metric name和label区分，label是在metric name上的更细维度的划分，其中的每一个实例是由一个float64和timestamp组成，只不过timestamp是隐式加上去的，有时候不会显示出来，如下面所示(数据来源于pro暴露的监控数据，访问&lt;a href=&#34;http://localhost:9090/metrics&#34;&gt;http://localhost:9090/metrics&lt;/a&gt; 可得），其中go_gc_duration_seconds是metrics name,quantile=&amp;ldquo;0.5&amp;rdquo;是key-value pair的label，而后面的值是float64 value。
pro为了方便client library的使用提供了四种数据类型： Counter, Gauge, Histogram, Summary, 简单理解就是Counter对数据只增不减，Gauage可增可减，Histogram,Summary提供跟多的统计信息。下面的实例中注释部分# TYPE go_gc_duration_seconds summary 标识出这是一个summary对象。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile=&amp;quot;0.5&amp;quot;} 0.000107458
go_gc_duration_seconds{quantile=&amp;quot;0.75&amp;quot;} 0.000200112
go_gc_duration_seconds{quantile=&amp;quot;1&amp;quot;} 0.000299278
go_gc_duration_seconds_sum 0.002341738
go_gc_duration_seconds_count 18
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 107
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在我们的使用场景中，大部分监控使用Counter来记录，例如接口请求次数、消息队列数量、重试操作次数等。比较推荐多使用Counter类型采集，因为Counter类型不会在两次采集间隔中间丢失信息。&lt;/p&gt;

&lt;p&gt;一小部分使用Gauge，如在线人数、协议流量、包大小等。Gauge模式比较适合记录无规律变化的数据，而且两次采集之间可能会丢失某些数值变化的情况。随着时间周期的粒度变大，丢失关键变化的情况也会增多。&lt;/p&gt;

&lt;p&gt;还有一小部分使用Histogram和Summary，用于统计平均延迟、请求延迟占比和分布率。另外针对Historgram，不论是打点还是查询对服务器的CPU消耗比较高，通过查询时查询结果的返回耗时会有十分直观的感受。&lt;/p&gt;

&lt;p&gt;5、PromQL&lt;/p&gt;

&lt;p&gt;直接通过类似于PromQL表达式httprequeststotal查询时间序列时，返回值中只会包含该时间序列中的最新的一个样本值，这样的返回结果我们称之为瞬时向量。而相应的这样的表达式称之为瞬时向量表达式。&lt;/p&gt;

&lt;p&gt;而如果我们想过去一段时间范围内的样本数据时，我们则需要使用区间向量表达式。区间向量表达式和瞬时向量表达式之间的差异在于在区间向量表达式中我们需要定义时间选择的范围，时间范围通过时间范围选择器[]进行定义。例如，通过以下表达式可以选择最近5分钟内的所有样本数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http_request_total{}[5m]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对比&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http_request_total{} # 瞬时向量表达式，选择当前最新的数据
http_request_total{}[5m] # 区间向量表达式，选择以当前时间为基准，5分钟内的数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、http api&lt;/p&gt;

&lt;p&gt;Prometheus API使用了JSON格式的响应内容。 当API调用成功后将会返回2xx的HTTP状态码。&lt;/p&gt;

&lt;p&gt;反之，当API调用失败时可能返回以下几种不同的HTTP状态码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;404 Bad Request：当参数错误或者缺失时。

422 Unprocessable Entity 当表达式无法执行时。

503 Service Unavailiable 当请求超时或者被中断时。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所有的API请求均使用以下的JSON格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;status&amp;quot;: &amp;quot;success&amp;quot; | &amp;quot;error&amp;quot;,
  &amp;quot;data&amp;quot;: &amp;lt;data&amp;gt;,
​
  // Only set if status is &amp;quot;error&amp;quot;. The data field may still hold
  // additional data.
  &amp;quot;errorType&amp;quot;: &amp;quot;&amp;lt;string&amp;gt;&amp;quot;,
  &amp;quot;error&amp;quot;: &amp;quot;&amp;lt;string&amp;gt;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;瞬时数据查询&lt;/p&gt;

&lt;p&gt;通过使用QUERY API我们可以查询PromQL在特定时间点下的计算结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /api/v1/query
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;URL请求参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query=：PromQL表达式。

time=：用于指定用于计算PromQL的时间戳。可选参数，默认情况下使用当前系统时间。

timeout=：超时设置。可选参数，默认情况下使用-query,timeout的全局设置。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如使用以下表达式查询表达式up在时间点2015-07-01T20:10:51.781Z的计算结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;http://localhost:9090/api/v1/query?query=up&amp;amp;time=2015-07-01T20:10:51.781Z&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;区间数据查询&lt;/p&gt;

&lt;p&gt;使用QUERY_RANGE API我们则可以直接查询PromQL表达式在一段时间返回内的计算结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /api/v1/query_range
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;URL请求参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query=: PromQL表达式。

start=: 起始时间。

end=: 结束时间。

step=: 查询步长。

timeout=: 超时设置。可选参数，默认情况下使用-query,timeout的全局设置。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当使用QUERY_RANGE API查询PromQL表达式时，返回结果一定是一个区间向量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;resultType&amp;quot;: &amp;quot;matrix&amp;quot;,
  &amp;quot;result&amp;quot;: &amp;lt;value&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要注意的是，在QUERY_RANGE API中PromQL只能使用瞬时向量选择器类型的表达式。&lt;/p&gt;

&lt;p&gt;7、sum&lt;/p&gt;

&lt;p&gt;sum_over_time(range-vector): 范围向量内每个度量指标的求和值。&lt;/p&gt;

&lt;p&gt;sum不能用于时间范围的求和，只能用于不同维度之间的求和&lt;/p&gt;

&lt;p&gt;8、编码方式和压缩比&lt;/p&gt;

&lt;p&gt;prometheus目前提供了三种算法(主要是为了压缩数据)用于块的编码,可以通过-storage.local.chunk-encoding-version进行配置.参数的有效值为0,1,2.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk-encoding为0时,采用的是一种叫做delta encoding的算法.早期的prometheus存储层用的就是该实现.&lt;/li&gt;
&lt;li&gt;chunk-encoding为1时,是一种改进型的double-delta encoding算法,目前的额prometheus默认使用该编码方式.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这两种编码方式对每个块使用固定的字节长度,这样有利于随机读取.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk-encoding为2时,使用的则是可变长的编码方式.这种编码比起上面两种方式,特点在于牺牲压缩速度换取了压缩率.facebook的时间序列数据库Beringei采用的编码方式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面展示了压缩同样大小的数据对比(文档说样本很大,但没说具体多少):&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;编码类型&lt;/th&gt;
&lt;th&gt;压缩后样本大小&lt;/th&gt;
&lt;th&gt;所用时间&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3.3bytes&lt;/td&gt;
&lt;td&gt;2.9s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1.3bytes&lt;/td&gt;
&lt;td&gt;4.9s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;测试:&lt;/p&gt;

&lt;p&gt;官方给出在生产环境中,每个样本加上索引信息后的大小一般为3-4bytes,我们可以做下测试看看实际的样本有多大,因为数据文件是经过处理后写入磁盘的,所以没办法查看单个样本的大小,只能采集一段时间的数据后计算.&lt;/p&gt;

&lt;p&gt;测试的监控目标的有两个,一个是prometheus本身的信息,一个是node-exporter输出的硬件数据,我们的分别访问host:port/metrics获取采集到的数据内容.在这个例子中,每进行一次采集,prometheus server就会取回145756 bytes的数据.(即访问两个/metrics接口返回的数据相加)&lt;/p&gt;

&lt;p&gt;五次测试得出的结果为:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;用时&lt;/th&gt;
&lt;th&gt;抓取频率&lt;/th&gt;
&lt;th&gt;数据变化量(bytes)&lt;/th&gt;
&lt;th&gt;原始大小(bytes)&lt;/th&gt;
&lt;th&gt;压缩率&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;第一次&lt;/td&gt;
&lt;td&gt;10min&lt;/td&gt;
&lt;td&gt;5s +1003520&lt;/td&gt;
&lt;td&gt;17490720&lt;/td&gt;
&lt;td&gt;94%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第二次&lt;/td&gt;
&lt;td&gt;20min&lt;/td&gt;
&lt;td&gt;5s +1597440&lt;/td&gt;
&lt;td&gt;34981440&lt;/td&gt;
&lt;td&gt;95%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第三次&lt;/td&gt;
&lt;td&gt;155min&lt;/td&gt;
&lt;td&gt;5s +4243456&lt;/td&gt;
&lt;td&gt;271106160&lt;/td&gt;
&lt;td&gt;98%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第四次&lt;/td&gt;
&lt;td&gt;10min&lt;/td&gt;
&lt;td&gt;1s +1658880&lt;/td&gt;
&lt;td&gt;17490720&lt;/td&gt;
&lt;td&gt;90%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第五次&lt;/td&gt;
&lt;td&gt;20min&lt;/td&gt;
&lt;td&gt;1s +3481600&lt;/td&gt;
&lt;td&gt;34981440&lt;/td&gt;
&lt;td&gt;90%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;按照抓取频率5s,压缩率90%进行粗略估算.&lt;/p&gt;

&lt;p&gt;假设检测的数据为系统的硬件指标,即node-exporter的输出(145756个字节),且集群中有10台机器,那么24个小时的数据量将不超过200m.假设监控数据保留1个月,那么大概需要6-7G左右的空间&lt;/p&gt;

&lt;p&gt;9、内存使用&lt;/p&gt;

&lt;p&gt;prometheus在内存里保存了最近使用的chunks，具体chunks的最大个数可以通过storage.local.memory-chunks来设定，默认值为1048576，即1048576个chunk，大小为1G。 除了采用的数据，prometheus还需要对数据进行各种运算，因此整体内存开销肯定会比配置的local.memory-chunks大小要来的大，因此官方建议要预留3倍的local.memory-chunks的内存大小。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;As a rule of thumb, you should have at least three times more RAM available than needed by the memory chunks alone
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过server的metrics去查看prometheus_local_storage_memory_chunks以及process_resident_memory_byte两个指标值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.prometheus_local_storage_memory_chunks

    The current number of chunks in memory, excluding cloned chunks 目前内存中暴露的chunks的个数

2.process_resident_memory_byte

    Resident memory size in bytes 驻存在内存的数据大小

3.prometheus_local_storage_persistence_urgency_score 介于0-1之间，当该值小于等于0.7时，prometheus离开rushed模式。 当大于0.8的时候，进入rushed模式

4.prometheus_local_storage_rushed_mode 1表示进入了rushed mode，0表示没有。进入了rushed模式的话，prometheus会利用storage.local.series-sync-strategy以及storage.local.checkpoint-interval的配置加速chunks的持久化。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监测当前使用的内存量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_chunks
process_resident_memory_bytes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监测当前使用的存储指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_series: 时间序列持有的内存当前块数量
prometheus_local_storage_memory_chunks: 在内存中持久块的当前数量


prometheus_local_storage_chunks_to_persist: 当前仍然需要持久化到磁盘的的内存块数量
prometheus_local_storage_persistence_urgency_score: 紧急程度分数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10、prometheus的target采用的是长连接的方式，会和target的机器端口一直保持连接。&lt;/p&gt;

&lt;p&gt;11、一般我们可以使用prometheus_egine_query_duration_seconds来评估prometheus整体的响应时间，如果响应过慢，可能是promql使用不当造成的，比如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大量使用join来组合指标或者增加label&lt;/li&gt;
&lt;li&gt;大范围时间查询，step很小，导致数据量很大&lt;/li&gt;
&lt;li&gt;rate时，range duration要大于step，否则会丢失数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;12、wal中文件太多，句柄不够用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level=error ts=2019-07-05T02:29:56.706Z caller=main.go:717 err=&amp;quot;opening storage failed: read WAL: open WAL segments: open segment:00020174 in dir:/data/wal: open /data/wal/00020174: too many open files&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;wal中文件太多，句柄不够用，需要打开句柄，句柄不够用可能导致压缩block出错，报错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level=error ts=2019-07-05T01:58:01.826Z caller=main.go:717 err=&amp;quot;opening storage failed: block dir: \&amp;quot;/data/01DEN382CDGHQR91QKNDHT77M8\&amp;quot;: open /data/01DEN382CDGHQR91QKNDHT77M8/meta.json: no such file or directory&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以需要在机器使用之前设置一下参数&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;锁定内存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;logMessage &amp;quot;lock mem&amp;quot;
echo &amp;quot;esadmin hard memlock unlimited&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf
echo &amp;quot;esadmin soft memlock unlimited&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;修改最大文件描述数&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;logMessage &amp;quot;file description &amp;quot;
echo &amp;quot;esadmin soft nofile 65536&amp;quot;  &amp;gt;&amp;gt;/etc/security/limits.conf
echo &amp;quot;esadmin hard nofile 131072&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf

#修改最大线程数
logMessage &amp;quot;max thread size &amp;quot;
echo &amp;quot;esadmin soft nproc 2048 &amp;quot;&amp;gt;&amp;gt; /etc/security/limits.conf
echo &amp;quot;esadmin hard nproc 4096 &amp;quot;&amp;gt;&amp;gt; /etc/security/limits.conf
echo &amp;quot;esadmin soft nproc 2048 &amp;quot;&amp;gt;&amp;gt; /etc/security/limits.d/90-nproc.conf

#修改内存映射区域最大数
logMessage &amp;quot;max mem count &amp;quot;
echo &amp;quot;vm.max_map_count=655360&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf
sysctl -p
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;函数与常用表达式&#34;&gt;函数与常用表达式&lt;/h3&gt;

&lt;h4 id=&#34;操作符&#34;&gt;操作符&lt;/h4&gt;

&lt;p&gt;或&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;up{exporterName=~&amp;quot;etcd|etcd-event&amp;quot; ,cluster_name=~&amp;quot;k8s_xingang_02&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正则匹配,全量配置.*&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;up{exporterName=~&amp;quot;etcd.*&amp;quot; ,cluster_name=~&amp;quot;k8s_xingang_02&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;函数&#34;&gt;函数&lt;/h4&gt;

&lt;p&gt;PromQL 有三个很简单的原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;任意 PromQL 返回的结果都不是原始数据，即使查询一个具体的 Metric（如 go_goroutines），结果也不是原始数据&lt;/li&gt;
&lt;li&gt;任意 Metrics 经过 Function 计算后会丢失 &lt;code&gt;__name__ Label&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;子序列间具备完全相同的 Label/Value 键值对（可以有不同的 &lt;code&gt;__name__&lt;/code&gt;）才能进行代数运算&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;rate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;(last值-first值)/时间差s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;irate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;(last值-last前一个值)/时间戳差值
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以cpu的使用率常用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;irate(node_cpu_seconds_total{mode=&amp;quot;idle&amp;quot;,ip=~&amp;quot;$ip&amp;quot;}[2m]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;avg&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;avg 同一时间的多条数据的平均值&lt;/li&gt;
&lt;li&gt;avg_over_time(range-vector): 范围向量内同一个度量指标不同时间的多条数据的平均值。&lt;/li&gt;
&lt;li&gt;同理的还有max，min等&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;相减&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边有一个两个指标相减的问题，必须是统一维度的才能相互计算，不能直接用指标value计算，可以对指标进行sum，max，rate等计算后进行加减乘除&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;increase()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;increase(v range-vector)函数，  度量指标：last值-first值,increase的返回值类型只能是counters，主要作用是增加图表和数据的可读性，使用rate记录规则的使用率，以便持续跟踪数据样本值的变化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;idelta()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;idelta(v range-vector)函数，输入一个范围向量，返回key: value = 度量指标： 每最后两个样本值差值。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;label_replace&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;label_replace给指标的label新生成一个指标名的指标&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;label_replace(v instant-vector, dst_label string, replacement string, src_label string, regex string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将正则表达式与标签值src_label匹配。如果匹配，则返回时间序列，标签值dst_label被替换的扩展替换。$1替换为第一个匹配子组，$2替换为第二个等。如果正则表达式不匹配，则时间序列不会更改。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;label_replace(redis_remote_replication_dest_repl_offset{},&amp;quot;destldcId&amp;quot;,&amp;quot;$1&amp;quot;, &amp;quot;ldcId&amp;quot;, &amp;quot;(.*)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;by&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当指标中的label发生变化的时候，哪怕是同一个指标名，在promethes也是两个数据，如果将变化的两条数据衔接起来，这个时候就使用by，by就是按着制订的维度来获取指标，可以摒弃不一样的label，这样就能是一条数据了，这样就可以使得时序图连接起来，例如ntp的client变更&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(ntp_offset{ip=~&amp;quot;$ip&amp;quot;})by(ip)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;by还可以用于表格的聚合，对于相同label的数据可以聚合在一个表格中的一条数据，所以用by获取到不通指标数据中的相同的label，就可以实现不同value的展示，但是label一样，就是一条数据。&lt;/p&gt;

&lt;p&gt;也可以sum不加by的数据可用和任何数据聚合，其实也就是聚合后少的标签可用和多的标签进行聚合。&lt;/p&gt;

&lt;p&gt;还可以使用or，当两个数据是对立的时候，一个出现另一个就不会出来。这样也能使得数据出来&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;topk(5, rate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, rate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval]))
topk(5, rate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval]))
topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m])) or topk(5, irate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m]))
topk(5, irate(redis_command_call_duration_seconds_count{ softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m])) or topk(5, irate(redis_commands_total{softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m]))
sum by (cmd)( rate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or sum by (cmd) (irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or sum by (cmd) (irate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval]))
redis_memory_fragmentation_ratio{ip=&amp;quot;$ip&amp;quot;}  or redis_memory_used_rss_bytes{ip=&amp;quot;$ip&amp;quot;} / redis_memory_used_bytes{ip=&amp;quot;$ip&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;原理解析&#34;&gt;原理解析&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle&#34;&gt;prometheus原理解析&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>消息队列系列---- Nsq</title>
          <link>https://kingjcy.github.io/post/middleware/mq/nsq/</link>
          <pubDate>Mon, 19 Jun 2017 20:21:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/mq/nsq/</guid>
          <description>&lt;p&gt;NSQ是一个基于Go语言的分布式实时消息平台，它基于MIT开源协议发布，由bitly公司开源出来的一款简单易用的消息中间件。可用于大规模系统中的实时消息服务，并且每天能够处理数亿(十亿)级别的消息，其设计目标是为在分布式环境下运行的去中心化服务提供一个强大的基础架构。&lt;/p&gt;

&lt;p&gt;NSQ具有分布式、去中心化的拓扑结构，该结构具有无单点故障、故障容错、高可用性以及能够保证消息的可靠传递的特征。NSQ非常容易配置和部署，且具有最大的灵活性，支持众多消息协议。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;NSQ 由 3 个守护进程组成:&lt;/p&gt;

&lt;p&gt;1、nsqd 是接收、队列和传送消息到客户端的守护进程。&lt;/p&gt;

&lt;p&gt;nsqd守护进程是NSQ的核心部分，它是一个单独的监听某个端口进来的消息的二进制程序。每个nsqd节点都独立运行，不共享任何状态。当一个节点启动时，它会同时开启tcp和http服务，两个服务都可以提供给生产者和消费者，向一组nsqlookupd节点进行注册操作，http服务还提供给nsqadmin获取该nsqd本地topic和channel信息；。&lt;/p&gt;

&lt;p&gt;客户端可以发布消息到nsqd守护进程上，或者从nsqd守护进程上读取消息。通常，消息发布者会向一个单一的local nsqd发布消息，消费者从连接了的一组nsqd节点的topic上远程读取消息。如果你不关心动态添加节点功能，你可以直接运行standalone模式。&lt;/p&gt;

&lt;p&gt;2、nsqlookupd 是管理的拓扑信息，并提供了最终一致发现服务的守护进程。&lt;/p&gt;

&lt;p&gt;nsqlookupd服务器像consul或etcd那样工作，只是它被设计得没有协调和强一致性能力。每个nsqlookupd都作为nsqd节点注册信息的短暂数据存储区。消费者连接这些节点去检测需要从哪个nsqd节点上读取消息。&lt;/p&gt;

&lt;p&gt;nsqlookupd服务同时开启tcp和http两个监听服务，nsqd会作为客户端，连上nsqlookupd的tcp服务，并上报自己的topic和channel信息，以及通过心跳机制判断nsqd状态；还有个http服务提供给nsqadmin获取集群信息；&lt;/p&gt;

&lt;p&gt;3、nsqadmin 是一个 Web UI 来实时监控集群和执行各种管理任务。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;术语&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;topic消息&lt;/p&gt;

&lt;p&gt;topic 是 NSQ 消息发布的 逻辑关键词 ，可以理解为人为定义的一种消息类型。当程序初次发布带 topic 的消息时,如果 topic 不存在,则会在 nsqd中创建。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;producer消息的生产者/发布者&lt;/p&gt;

&lt;p&gt;producer 通过 HTTP API 将消息发布到 nsqd 的指定 topic ，一般有 pub/mpub 两种方式， pub 发布一个消息， mpub 一个往返发布多个消息。&lt;/p&gt;

&lt;p&gt;producer 也可以通过 nsqd客户端 的 TCP接口 将消息发布给 nsqd 的指定 topic 。&lt;/p&gt;

&lt;p&gt;当生产者 producer 初次发布带 topic 的消息给 nsqd 时,如果 topic 不存在，则会在 nsqd 中创建 topic 。&lt;/p&gt;

&lt;p&gt;生产者会同时连上NSQ集群中所有nsqd节点，当然这些节点的地址是在Writer初始化时，通过外界传递进去；当发布消息时，writer会随机选择一个nsqd节点发布某个topic的消息；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;channel消息传递的通道&lt;/p&gt;

&lt;p&gt;当生产者每次发布消息的时候,消息会采用多播的方式被拷贝到各个 channel 中, channel 起到队列的作用。&lt;/p&gt;

&lt;p&gt;channel 与 consumer(消费者) 相关，是消费者之间的负载均衡,消费者通过这个特殊的channel读取消息。&lt;/p&gt;

&lt;p&gt;在 consumer 想单独获取某个 topic 的消息时，可以 subscribe(订阅)一个自己单独命名的 nsqd中还不存在的 channel, nsqd会为这个 consumer创建其命名的 channel&lt;/p&gt;

&lt;p&gt;Channel 会将消息进行排列，如果没有 consumer读取消息，消息首先会在内存中排队，当量太大时就会被保存到磁盘中。可以在配置中配置具体参数。&lt;/p&gt;

&lt;p&gt;一个 channel 一般会有多个 consumer 连接。假设所有已连接的 consumer 处于准备接收消息的状态，每个消息将被传递到一个随机的 consumer。&lt;/p&gt;

&lt;p&gt;Go语言中的channel是表达队列的一种自然方式，因此一个NSQ的topic/channel，其核心就是一个存放消息指针的Go-channel缓冲区。缓冲区的大小由 &amp;ndash;mem-queue-size 配置参数确定。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;consumer消息的消费者&lt;/p&gt;

&lt;p&gt;consumer 通过 TCPsubscribe 自己需要的 channel&lt;/p&gt;

&lt;p&gt;topic 和 channel 都没有预先配置。 topic 由第一次发布消息到命名 topic 的 producer 创建 或 第一次通过 subscribe 订阅一个命名 topic 的 consumer 来创建。 channel 被 consumer 第一次 subscribe 订阅到指定的 channel 创建。&lt;/p&gt;

&lt;p&gt;多个 consumersubscribe一个 channel，假设所有已连接的客户端处于准备接收消息的状态，每个消息将被传递到一个 随机 的 consumer。&lt;/p&gt;

&lt;p&gt;NSQ 支持延时消息， consumer 在配置的延时时间后才能接受相关消息。&lt;/p&gt;

&lt;p&gt;Channel在 consumer 退出后并不会删除，这点需要特别注意。&lt;/p&gt;

&lt;p&gt;消费者也会同时连上NSQ集群中所有nsqd节点，reader首先会连上nsqlookupd，获取集群中topic的所有producer，然后通过tcp连上所有producer节点，并在本地用tornado轮询每个连接，当某个连接有可读事件时，即有消息达到，处理即可；&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;h2 id=&#34;架构&#34;&gt;架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/mq/nsq/20170619.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;NSQ推荐通过 nsqd 实例使用协同定位 producer，这意味着即使面对网络分区，消息也会被保存在本地，直到它们被一个 consumer读取。更重要的是， producer不必去发现其他的 nsqd节点，他们总是可以向本地 nsqd实例发布消息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一个 producer向它的本地 nsqd发送消息，要做到这点，首先要先打开一个连接( NSQ 提供 HTTP API 和 TCP 客户端 等2种方式连接到 nsqd)，然后发送一个包含 topic和消息主体的发布命令(pub/mpub/publish)，在这种情况下，我们将消息发布到 topic上，消息会采用多播的方式被拷贝到各个 channel中, 然后通过多个 channel以分散到我们不同需求的 consumer中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;channel起到队列的作用。 多个 producer产生的 topic消息在每一个连接 topic的 channel上进行排队。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个 channel的消息都会进行排队，直到一个 consumer把他们消费，如果此队列超出了内存限制，消息将会被写入到磁盘中。 nsqd节点首先会向 nsqlookup 广播他们的位置信息，一旦它们注册成功， consumer将会从 nsqlookup 服务器节点上发现所有包含事件 topic的 nsqd节点。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个 consumer向每个 nsqd主机进行订阅操作，用于表明 consumer已经准备好接受消息了。这里我们不需要一个完整的连通图，但我们必须要保证每个单独的 nsqd实例拥有足够的消费者去消费它们的消息，否则 channel会被队列堆着。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;设计&#34;&gt;设计&lt;/h2&gt;

&lt;p&gt;1、分布式方案&lt;/p&gt;

&lt;p&gt;nsqd随意起， nsqlookup使用备份的方式，nsqlookupd的高可用是通过同时运行多个实例， 多个实例之间保持互备实现的。
一个client只会同时对一个nsqd建立连接， 所以一旦一个nsqd连接， 那么就不会对其他的topic建立连接
只有再一个nsqd坏掉的时候，才会重新选择nsqd。&lt;/p&gt;

&lt;p&gt;2、高可用&lt;/p&gt;

&lt;p&gt;高可用(无单点问题) writer和reader是直接连上各个nsqd节点，因此即使nsqlookupd挂了，也不影响线上正常使用；即使某个nsqd节点挂了，writer发布消息时，发现节点挂了，可以选择其他节点(当然，这是客户端负责的)，单个节点挂了对reader无影响；&lt;/p&gt;

&lt;p&gt;3、高性能&lt;/p&gt;

&lt;p&gt;writer在发布消息时，是随机发布到集群中nsqd节点，因此在一定程序上达到负载均衡；reader同时监听着集群中所有nsqd节点，无论哪个节点有消息，都会投递到reader上；&lt;/p&gt;

&lt;p&gt;4、高可扩展&lt;/p&gt;

&lt;p&gt;当向集群中添加节点时，首先reader会通过nsqlookupd发现新的节点加入，并自动连接；因为writer连接的nsqd节点的地址是初始化时设置的，因此增加节点时，只需要在初始化writer时，添加新节点的地址即可；&lt;/p&gt;

&lt;p&gt;5、client选择nsqd的原则&lt;/p&gt;

&lt;p&gt;nsq保证消息能够正常至少传输一次的方式是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;client表明已经可以接受消息&lt;/li&gt;
&lt;li&gt;nsqd将消息发送出去， 同时将这个消息进行本地存储&lt;/li&gt;
&lt;li&gt;client如果回复FIN 表示成功接受， 如果回复REQ， 表明需要重发， 如果没有回复， 则认为超时了， 进行重发&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以， 当nsqd异常关闭的时候， 没有来得及保存到本地的消息可能会丢失, 解决办法是讲同样的消息发送到两个nsqd中&lt;/p&gt;

&lt;p&gt;由于消息至少会被发送一次， 则意味着消息可能会被发送多次， 客户端需要能够确定收到消息所执行的操作是幂等的，即收到一次与收到多次的影响一致&lt;/p&gt;

&lt;p&gt;6、保证消息不丢失&lt;/p&gt;

&lt;p&gt;nsdlookup 如何路由请求&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;channels&amp;quot;: [ &amp;quot;nsq_to_file&amp;quot;, &amp;quot;c&amp;quot; ], &amp;quot;producers&amp;quot;: [ { &amp;quot;remote_address&amp;quot;: &amp;quot;127.0.0.1:58148&amp;quot;, &amp;quot;hostname&amp;quot;: &amp;quot;safedev01v.add.corp.qihoo.net&amp;quot;, &amp;quot;broadcast_address&amp;quot;: &amp;quot;safedev01v.add.corp.qihoo.net&amp;quot;, &amp;quot;tcp_port&amp;quot;: 4150, &amp;quot;http_port&amp;quot;: 4151, &amp;quot;version&amp;quot;: &amp;quot;0.3.6&amp;quot; }, { &amp;quot;remote_address&amp;quot;: &amp;quot;10.16.59.85:39652&amp;quot;, &amp;quot;hostname&amp;quot;: &amp;quot;safedev02v.add.corp.qihoo.net&amp;quot;, &amp;quot;broadcast_address&amp;quot;: &amp;quot;safedev02v.add.corp.qihoo.net&amp;quot;, &amp;quot;tcp_port&amp;quot;: 4150, &amp;quot;http_port&amp;quot;: 4151, &amp;quot;version&amp;quot;: &amp;quot;0.3.7&amp;quot; } ] }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7、细节&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;可以设置内存的使用大小， 但是并不建议将内存设置太小， 毕竟持久化是为了保证unclean关闭nsqd时，消息不会丢失&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;nsq-chan 的信息就是保存在go-chan中的，&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;8、每一个topic包含三个协程：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;router： 从go-chan中读取新发布的消息，并讲消息保存在一个队列（ram or rom）中，&lt;/li&gt;
&lt;li&gt;messagePump&lt;/li&gt;
&lt;li&gt;DiskQueue 讲内存中的消息写入到磁盘，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果一个topic没有订阅者（客户端），则该topic的内容就不会被diskqueue写入到磁盘中， 而是由DummyBackendQueue直接将消息丢弃掉&lt;/p&gt;

&lt;p&gt;9、nsqd中减小GC的优化方案&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;避免[]byte转换string&lt;/li&gt;
&lt;li&gt;重用缓存或者对象&lt;/li&gt;
&lt;li&gt;预先分配slice的内存， 并且知道每个item的大小&lt;/li&gt;
&lt;li&gt;避免使用interface{} 和封装的类型， &amp;gt;like a struct for a “multiple value” go-chan).&lt;/li&gt;
&lt;li&gt;避免使用defer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体的源码解析在&lt;a href=&#34;https://kingjcy.github.io/post/middleware/mq/nsq-principle/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;部署使用&#34;&gt;部署使用&lt;/h1&gt;

&lt;p&gt;1、下载有现成的二进制文件。&lt;/p&gt;

&lt;p&gt;2、首先启动 nsdlookupd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nsqlookupd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端通过查询 nsdlookupd 来发现指定topic的生产者，并且 nsqd 节点广播 topic 和通道 channel 信息&lt;/p&gt;

&lt;p&gt;该服务运行后有两个端口：TCP 接口，nsqd 用它来广播；HTTP 接口，客户端用它来发现和管理。&lt;/p&gt;

&lt;p&gt;在生产环境中，为了高可用，最好部署三个nsqlookupd服务。&lt;/p&gt;

&lt;p&gt;3、部署nsqd&lt;/p&gt;

&lt;p&gt;先创建 nsqd 的数据路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir /tmp/nsqdata1 /tmp/nsqdata2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行两个测试的 nsqd 实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nsqd --lookupd-tcp-address=127.0.0.1:4160 -broadcast-address=127.0.0.1 -tcp-address=127.0.0.1:4150 -http-address=0.0.0.0:4151 -data-path=/tmp/nsqdata1
nsqd --lookupd-tcp-address=127.0.0.1:4160 -broadcast-address=127.0.0.1 -tcp-address=127.0.0.1:4152 -http-address=0.0.0.0:4153 -data-path=/tmp/nsqdata2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;nsqd 可以独立运行，不过通常它是由 nsdlookupd 实例所在集群配置的(它在这能声明 topics 和 channels ，以便大家能找到)&lt;/p&gt;

&lt;p&gt;服务启动后有两个端口：一个给客户端(TCP)，另一个是 HTTP API。还能够开启HTTPS。&lt;/p&gt;

&lt;p&gt;同一台服务器启动多个 nsqd ，要注意端口和数据路径必须不同，包括： –lookupd-tcp-address 、 -tcp-address 、 –data-path&lt;/p&gt;

&lt;p&gt;删除 topic 、channel 需要 HTTP API 调用。&lt;/p&gt;

&lt;p&gt;4、启动 nsqadmin 前端Web监控&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nsqadmin --lookupd-http-address=localhost:4161
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;nsqadmin 是一套 WEB UI ，用来汇集集群的实时统计，并执行不同的管理任务。&lt;/p&gt;

&lt;p&gt;运行后，能够通过4171端口查看并管理 topic 和 channel 。&lt;/p&gt;

&lt;p&gt;nsqadmin 通常只需要运行一个。&lt;/p&gt;

&lt;h2 id=&#34;使用实例&#34;&gt;使用实例&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;2个Producer 1个Consumer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;produce1() 发布publish &amp;quot;x&amp;quot;,&amp;quot;y&amp;quot; 到 topic &amp;quot;test&amp;quot;
produce2() 发布publish &amp;quot;z&amp;quot; 到 topic &amp;quot;test&amp;quot;
consumer1() 订阅subscribe channel &amp;quot;sensor01&amp;quot; of topic &amp;quot;test&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package test

import (
        &amp;quot;log&amp;quot;
        &amp;quot;time&amp;quot;
        &amp;quot;testing&amp;quot;
        &amp;quot;strconv&amp;quot;

        &amp;quot;github.com/nsqio/go-nsq&amp;quot;
)

func TestNSQ1(t *testing.T) {
       NSQDsAddrs := []string{&amp;quot;127.0.0.1:4150&amp;quot;, &amp;quot;127.0.0.1:4152&amp;quot;}
       go consumer1(NSQDsAddrs)
       go produce1()
       go produce2()
       time.Sleep(30 * time.Second)
}

func produce1() {
        cfg := nsq.NewConfig()
        nsqdAddr := &amp;quot;127.0.0.1:4150&amp;quot;
        producer, err := nsq.NewProducer(nsqdAddr, cfg)
        if err != nil {
                log.Fatal(err)
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;x&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;y&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
}

func produce2() {
        cfg := nsq.NewConfig()
        nsqdAddr := &amp;quot;127.0.0.1:4152&amp;quot;
        producer, err := nsq.NewProducer(nsqdAddr, cfg)
        if err != nil {
                log.Fatal(err)
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;z&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
}

func consumer1(NSQDsAddrs []string) {
        cfg := nsq.NewConfig()
        consumer, err := nsq.NewConsumer(&amp;quot;test&amp;quot;, &amp;quot;sensor01&amp;quot;, cfg)
        if err != nil {
                log.Fatal(err)
        }
        consumer.AddHandler(nsq.HandlerFunc(
                func(message *nsq.Message) error {
                        log.Println(string(message.Body) + &amp;quot; C1&amp;quot;)
                        return nil
                }))
        if err := consumer.ConnectToNSQDs(NSQDsAddrs); err != nil {
                log.Fatal(err, &amp;quot; C1&amp;quot;)
        }
        &amp;lt;-consumer.StopChan
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试结果&lt;/p&gt;

&lt;p&gt;x,y,z 都被 consumer1 接收了。注意到接收时间， x,y 几乎同时被接收，它们都由 producer1 发布，而 z 由 producer2 发布，中间间隔10秒。测试了很多次都是10秒,偶尔是15秒或20秒。查看了ConnectToNSQDs()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// ConnectToNSQDs takes multiple nsqd addresses to connect directly to.
//
// It is recommended to use ConnectToNSQLookupd so that topics are discovered
// automatically.  This method is useful when you want to connect to local instance.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Consumer 每隔 x 秒，向 nsqlookud 进行http轮询，用来更新自己的 nsqd 地址目录,当一个 producer 的 channel 一直没有数据时，则会轮询到下一个 producer&lt;/p&gt;

&lt;p&gt;可见go的客户端代码库就是github.com/nsqio/go-nsq。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;1个Producer 3个Consumer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;produce3() 发布publish &amp;quot;x&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;z&amp;quot; 到 topic &amp;quot;test&amp;quot;
consumer1() 订阅subscribe channel &amp;quot;sensor01&amp;quot; of topic &amp;quot;test&amp;quot;
consumer2() 订阅subscribe channel &amp;quot;sensor01&amp;quot; of topic &amp;quot;test&amp;quot;
consumer3() 订阅subscribe channel &amp;quot;sensor02&amp;quot; of topic &amp;quot;test&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package test

import (
        &amp;quot;log&amp;quot;
        &amp;quot;time&amp;quot;
        &amp;quot;testing&amp;quot;
        &amp;quot;strconv&amp;quot;

        &amp;quot;github.com/nsqio/go-nsq&amp;quot;
)

func TestNSQ2(t *testing.T) {
        NSQDsAddrs := []string{&amp;quot;127.0.0.1:4150&amp;quot;}
        go consumer1(NSQDsAddrs)
        go consumer2(NSQDsAddrs)
        go consumer3(NSQDsAddrs)
        go produce3()
        time.Sleep(5 * time.Second)
}

func produce3() {
        cfg := nsq.NewConfig()
        nsqdAddr := &amp;quot;127.0.0.1:4150&amp;quot;
        producer, err := nsq.NewProducer(nsqdAddr, cfg)
        if err != nil {
                log.Fatal(err)
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;x&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;y&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
        if err := producer.Publish(&amp;quot;test&amp;quot;, []byte(&amp;quot;z&amp;quot;)); err != nil {
                log.Fatal(&amp;quot;publish error: &amp;quot; + err.Error())
        }
}

func consumer1(NSQDsAddrs []string) {
        cfg := nsq.NewConfig()
        consumer, err := nsq.NewConsumer(&amp;quot;test&amp;quot;, &amp;quot;sensor01&amp;quot;, cfg)
        if err != nil {
                log.Fatal(err)
        }
        consumer.AddHandler(nsq.HandlerFunc(
                func(message *nsq.Message) error {
                        log.Println(string(message.Body) + &amp;quot; C1&amp;quot;)
                        return nil
                }))
        if err := consumer.ConnectToNSQDs(NSQDsAddrs); err != nil {
                log.Fatal(err, &amp;quot; C1&amp;quot;)
        }
        &amp;lt;-consumer.StopChan
}

func consumer2(NSQDsAddrs []string) {
        cfg := nsq.NewConfig()
        consumer, err := nsq.NewConsumer(&amp;quot;test&amp;quot;, &amp;quot;sensor01&amp;quot;, cfg)
        if err != nil {
                log.Fatal(err)
        }
        consumer.AddHandler(nsq.HandlerFunc(
                func(message *nsq.Message) error {
                        log.Println(string(message.Body) + &amp;quot; C2&amp;quot;)
                        return nil
                }))
        if err := consumer.ConnectToNSQDs(NSQDsAddrs); err != nil {
                log.Fatal(err, &amp;quot; C2&amp;quot;)
        }
        &amp;lt;-consumer.StopChan
}

func consumer3(NSQDsAddrs []string) {
        cfg := nsq.NewConfig()
        consumer, err := nsq.NewConsumer(&amp;quot;test&amp;quot;, &amp;quot;sensor02&amp;quot;, cfg)
        if err != nil {
                log.Fatal(err)
        }
        consumer.AddHandler(nsq.HandlerFunc(
                func(message *nsq.Message) error {
                        log.Println(string(message.Body) + &amp;quot; C3&amp;quot;)
                        return nil
                }))
        if err := consumer.ConnectToNSQDs(NSQDsAddrs); err != nil {
               log.Fatal(err, &amp;quot; C3&amp;quot;)
        }
        &amp;lt;-consumer.StopChan
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;consumer1 接收到了 y
consumer2 接收到了 x,z
consumer3 接收到了 x,y,z
channelsensor01 中的消息被随机的分到了 consumer1 和 consumer2
consumer3 单独占有 channelsensor02，接收了其中的所有消息
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用细节&#34;&gt;使用细节&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;nsqd启动时，端口和数据存放要不同&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;消息发送必须指定具体的某个nsqd；而消费则可以通过lookupd获取再重定向&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;消费者接受数据时，要设置 config.MaxInFlight&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;channel在消费者退出后并不会删除，需要特别注意。如果紧紧是想利用nsq作为消息广播，不考虑离线数据保存，不妨考虑nats。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;channel的名字，有很多限制，基本ASSCI字符+数字，以及点号”.”,下划线”_”。中文（其他非英语文字应该也不行）、以及空格、冒号”:”、横线”-“等都不得出现&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 并发安全</title>
          <link>https://kingjcy.github.io/post/architecture/concurrencesafe/</link>
          <pubDate>Sun, 09 Apr 2017 19:25:10 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/concurrencesafe/</guid>
          <description>&lt;p&gt;并发安全，就是多个并发体在同一段时间内访问同一个共享数据，共享数据能被正确处理。&lt;/p&gt;

&lt;h1 id=&#34;并发不安全&#34;&gt;并发不安全&lt;/h1&gt;

&lt;p&gt;最典型的案例:卖票超售&lt;/p&gt;

&lt;p&gt;设想有一家电影院，有两个售票窗口，售票员售票时候先看一下当前剩余票数是否大于0，如果大于0则售出票。&lt;/p&gt;

&lt;p&gt;此时票数剩下一张票，两个售票窗口同时来了顾客，两个售票人都看了一下剩余票数还有一张，不约而同地收下顾客的钱，余票还剩一张，但是却售出了两张票，就会出现致命的问题。&lt;/p&gt;

&lt;h1 id=&#34;如何做到并发安全&#34;&gt;如何做到并发安全&lt;/h1&gt;

&lt;p&gt;目前最最主流的办法就是加锁就行操作，其实售票的整个操作同时间内只能一个人进行，在我看来归根到底加锁其实就是让查询和售票两个步骤原子化，只能一块执行，不能被其他程序中断，让这步操作变成串行化。下面就介绍一下使查询和售票原子化的常见程序操作：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;锁的做法就是每次进入这段变量共享的程序片段，都要先获取一下锁，如果获取成功则可以继续执行，如果获取失败则阻塞，直到其他并发体把锁给释放，程序得到执行调度才可以执行下去。&lt;/p&gt;

&lt;p&gt;锁本质上就是让并发体创建一个程序临界区，临界区一次只能进去一个并发体，伪代码示意如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lock()
totalNum = getTotalNum()
if totalNum &amp;gt; 0
    # 则售出一张票
    totalNum = totalNum - 1
else
    failedToSold()
unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读锁与写锁&lt;/p&gt;

&lt;p&gt;读锁也叫共享锁，写锁也叫排它锁，锁的概念被发明了之后，人们就想着如果我很多个并发体大部分时间都是读，如果就把变量读取的时候也要建立临界区，那就有点太大题小做了。于是人们发明了读锁，一个临界区如果加上了读锁，其他并发体执行到相同的临界区都可以加上读锁，执行下去，但不能加上写锁。这样就保证了可以多个并发体并发读取而又不会互相干扰。&lt;/p&gt;

&lt;p&gt;在golang中也是提供了mutex的锁机制。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;队列&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;队列也是解决并发不安全的做法。多个并发体去获取队列里的元素，然后进行处理，这种做法和上锁其实大同小异，本质都是把并发的操作串行化，同一个数据同一个时刻只能交给一个并发体去处理,伪代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 第一个获取到队列的元素就可以进行下去
isCanSold = canSoldList.pop()
totalNum = getTotalNum()
if totalNum &amp;gt; 0
    # 则售出一张票
    totalNum = totalNum - 1
else
    failedToSold()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在golang中也提供了队列机制，也就是Goroutine 通过 channel 进行安全读写共享变量。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;CAS&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CAS（compare and swap），先比对，然后再进行交换，和数据库里的乐观锁的做法很相似。&lt;/p&gt;

&lt;p&gt;乐观锁&lt;/p&gt;

&lt;p&gt;数据库里的乐观锁并不是真的使用了锁的机制，而是一种程序的实现思路。
乐观锁的想法是，每次拿取数据再去修改的时候很乐观，认为其他人不会去修改这个数据，表另外维护一个额外版本号的字段。
查数据的时候记录下该数据的版本号，如果成功修改的话，会修改该数据的版本号，如果修改的时候版本号和查询的时候版本号不一致，则认为数据已经被修改过，会重新尝试查询再次操作。&lt;/p&gt;

&lt;p&gt;设我们表有一个user表，除了必要的字段，还有一个字段version，表如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;id  username    money   version
1   a   10  100
2   b   20  100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候我们需要修改a的余额-10元，执行事务语句如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while
    select @money = money, @version = version from user where username = a;
    if @money &amp;lt; 10
        print(&#39;余额成功&#39;)
        break
    # 扣费前的预操作
    paied()
    # 实行扣费
    update user set money = money - 10, version = version + 1 where username = a and version = @version
    # 影响条数等于1，证明执行成功
    if @@ROWCOUNT == 1
        print(&#39;扣费成功&#39;)
        break
    else
        rollback
        print(&#39;扣费失败，重新进行尝试&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;乐观锁的做法就是使用版本的形式，每次写数据的时候会比对一下最开始的版本号，如果不同则证明有问题。&lt;/p&gt;

&lt;p&gt;CAS的做法也是一样的，在代码里面的实现稍有一点不同，由于SQL每条语句都是原子性，查询对应版本号的数据再更新的这个条件是原子性的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;update user set money = money - 10, version = version + 1 where username = a and version = @version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是在代码里面两条查询和赋值两个语句不是原子性的，需要有特定的函数让cpu底层把两个操作变成一个原子操作，在go里面有atomic包支持实现，是这样实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    user := getUserByName(A)
    version := user.version
    paied()
    if atomic.CompareAndSwapInt32(&amp;amp;user.version, version, version + 1) {
        user.money -= 10
    } else {
        rollback()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;atomic.CompareAndSwapInt32需要依次传入要比较变量的地址，旧变量的值，修改后变量的值，函数会判断旧变量的值是否与现在变量的地址是否相同，相同则把新变量的值写入到该变量。
CAS的好处是不需要程序去创建临界区，而是让CPU去把两个指令变成原子性操作，性能更好，但是如果变量会被频繁更改的话，重试的次数变多反而会使得效率不如加锁高。&lt;/p&gt;

&lt;p&gt;在golang中也提供了CAS机制，也就是Goroutine 通过 atomic进行安全读写共享变量。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Zabbix基本使用</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix/</link>
          <pubDate>Sat, 04 Mar 2017 17:54:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix/</guid>
          <description>&lt;p&gt;zabbix是目前各大互联网公司使用最广泛的开源监控之一,其历史最早可追溯到1998年,在业内拥有各种成熟的解决方案.&lt;/p&gt;

&lt;h1 id=&#34;网站可用性&#34;&gt;网站可用性&lt;/h1&gt;

&lt;p&gt;在软件系统的高可靠性（也称为可用性，英文描述为HA，High Available）里有个衡量其可靠性的标准——X个9，这个X是代表数字3~5。X个9表示在软件系统1年时间的使用过程中，系统可以正常使用时间与总时间（1年）之比，我们通过下面的计算来感受下X个9在不同级别的可靠性差异。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1个9：(1-90%)*365=36.5天，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是36.5天
2个9：(1-99%)*365=3.65天 ， 表示该软件系统在连续运行1年时间里最多可能的业务中断时间是3.65天
3个9：(1-99.9%)*365*24=8.76小时，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是8.76小时。
4个9：(1-99.99%)*365*24=0.876小时=52.6分钟，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是52.6分钟。
5个9：(1-99.999%)*365*24*60=5.26分钟，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是5.26分钟。
6个9：(1-99.9999%)*365*24*60*60=31秒， 示该软件系统在连续运行1年时间里最多可能的业务中断时间是31秒
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前能达到4个9就很好了。&lt;/p&gt;

&lt;h1 id=&#34;组件&#34;&gt;组件&lt;/h1&gt;

&lt;p&gt;zabbix属于CS架构,Server端基于C语言编写,相比其他语言具有一定的性能优势(在数据量不大的情况下!).Web管理端则使用了PHP. 而其client端有各种流行语言的库实现,方便使用其API&lt;/p&gt;

&lt;p&gt;在数据的存储方面,zabbix使用了关系性数据库,包括SQLite,MySQL,PostgreSQL,Oracle,DB2&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;yum安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;zabbix的安装比较繁琐,但也不算困难(主要是因为网上提供的资料足够多)&lt;/p&gt;

&lt;p&gt;我们需要一种关系型关系型数据库,目前提供的选择有MySQL,SQLite, PostgreSQL,Oracle,DB2&lt;/p&gt;

&lt;p&gt;接下来需要安装PHP的运行环境,Web服务器可是使用Apache或者Nginx都可以.&lt;/p&gt;

&lt;p&gt;最后一步是安装zabbix服务.&lt;/p&gt;

&lt;p&gt;完整的安装教程可以参考:&lt;a href=&#34;http://support.supermap.com.cn/DataWarehouse/WebDocHelp/icm/Appdix/Zabbix_server/Zabbix_Installation.htm&#34;&gt;zabbix安装指南&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;主要步骤&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;配置zabbix官方yum源，还有base和epel源&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install zabbix-server-mysql zabbix-get
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;初始化database&lt;/p&gt;

&lt;p&gt;导入zabbix-server-mysql包中的create.sql来初始化数据库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm -ql zabbix-server-mysql
mysql -uroot -p -Dzabbix &amp;lt; create.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以查看表了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置服务端配置文件并启动&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装web&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install httpd php php-mysql php-mbstring php-gd php-bamath php-ladp php-xml
yum install zabbix-web-mysql zabbix-web
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后访问　　&lt;a href=&#34;http://zabbix-web-ip/zabbix/setup.php进行zabbix初始化&#34;&gt;http://zabbix-web-ip/zabbix/setup.php进行zabbix初始化&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装zabbix-agent&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install zabbix-agent zabbix-sender
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置客户端端配置文件并启动&lt;/p&gt;

&lt;p&gt;服务端快速安装脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#clsn

#设置解析 注意：网络条件较好时，可以不用自建yum源
# echo &#39;10.0.0.1 mirrors.aliyuncs.com mirrors.aliyun.com repo.zabbix.com&#39; &amp;gt;&amp;gt; /etc/hosts

#安装zabbix源、aliyun YUM源
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
rpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm

#安装zabbix
yum install -y zabbix-server-mysql zabbix-web-mysql

#安装启动 mariadb数据库
yum install -y  mariadb-server
systemctl start mariadb.service

#创建数据库
mysql -e &#39;create database zabbix character set utf8 collate utf8_bin;&#39;
mysql -e &#39;grant all privileges on zabbix.* to zabbix@localhost identified by &amp;quot;zabbix&amp;quot;;&#39;

#导入数据
zcat /usr/share/doc/zabbix-server-mysql-3.0.13/create.sql.gz|mysql -uzabbix -pzabbix zabbix

#配置zabbixserver连接mysql
sed -i.ori &#39;115a DBPassword=zabbix&#39; /etc/zabbix/zabbix_server.conf

#添加时区
sed -i.ori &#39;18a php_value date.timezone  Asia/Shanghai&#39; /etc/httpd/conf.d/zabbix.conf

#解决中文乱码
yum -y install wqy-microhei-fonts
\cp /usr/share/fonts/wqy-microhei/wqy-microhei.ttc /usr/share/fonts/dejavu/DejaVuSans.ttf

#启动服务
systemctl start zabbix-server
systemctl start httpd

#写入开机自启动
chmod +x /etc/rc.d/rc.local
cat &amp;gt;&amp;gt;/etc/rc.d/rc.local&amp;lt;&amp;lt;EOF
systemctl start mariadb.service
systemctl start httpd
systemctl start zabbix-server
EOF

#输出信息
echo &amp;quot;浏览器访问 http://`hostname -I|awk &#39;{print $1}&#39;`/zabbix&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端快速部署脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#clsn

#设置解析
echo &#39;10.0.0.1 mirrors.aliyuncs.com mirrors.aliyun.com repo.zabbix.com&#39; &amp;gt;&amp;gt; /etc/hosts

#安装zabbix源、aliyu nYUM源
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
rpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm

#安装zabbix客户端
yum install zabbix-agent -y
sed -i.ori &#39;s#Server=127.0.0.1#Server=172.16.1.61#&#39; /etc/zabbix/zabbix_agentd.conf
systemctl start  zabbix-agent.service

#写入开机自启动
chmod +x /etc/rc.d/rc.local
cat &amp;gt;&amp;gt;/etc/rc.d/rc.local&amp;lt;&amp;lt;EOF
systemctl start  zabbix-agent.service
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;编译安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;系统环境&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    OS:         centos7.5
    software：  zabbix 4.0 LTS
    DBSever:    MariaDB-10.2.15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一、需要先把数据库装上，这里用到的是mariadb 二进制包安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、下载二进制包，
     官网的下载路径：
  wget http://mirrors.neusoft.edu.cn/mariadb//mariadb-10.2.15/bintar-linux-x86_64/mariadb-10.2.15-linux-x86_64.tar.gz

2、添加组和用户
  [root@node2 ~]# groupadd -r -g 306 mysql
  [root@node2 ~]# useradd -g mysql -u 306 -r mysql

3、解压mariadb二进制包到/usr/local下去
   [root@node2 ~]# tar xf mariadb-10.2.15-linux-x86_64.tar.gz -C /usr/local/

4、进入到/usr/local下面创建mysql的软连接
   [root@node2 ~]# cd /usr/local/
   [root@node2 /usr/local]# ln -s mariadb-10.2.15-linux-x86_64/ mysql

5、修改mysql的相对应的属主和属组权限
    [root@node2 /usr/local]# chown -R root.mysql mysql/

6、创建数据文件的存放路径，并修改所属组的权限为mysql
     [root@node2 ~]#   cd /app/
     [root@node2 /app]# mkdir mydata
     [root@node2 ]#  chown -R mysql.mysql  /app

7、初始化数据库，指定好数据文件的存放路径和用户
       [root@node2 ]# cd /usr/local/mysql/
       [root@node2 /usr/local/mysql/]# scripts/mysql_install_db --datadir=/app/mydata --user=mysql

8、拷贝mariadb的启动脚本到/etc/rc.d/init.d下命名为mysqld
       [root@node2 /usr/local/mysql/]# cp support-files/mysql.server /etc/rc.d/init.d/mysqld

9、把mysqld设置为开机启动
       [root@node2 /usr/local/mysql/]# chkconfig --add mysqld

10、创建mariadb的配置文件存放路径，并拷贝模版文件到这个目录下命名为my.cnf
      [root@node2 /usr/local/mysql/]# mkdir /etc/mysql
      [root@node2 /usr/local/mysql/]#cp support-files/my-large.cnf /etc/mysql/my.cnf

11、配置系统环境变量，重读配置文件让它生效
      [root@node2 /usr/local/mysql/]# vim /etc/profile.d/mysql.sh
      [root@node2 /usr/local/mysql/]#export PATH=/usr/local/mysql/bin:$PATH
      [root@node2 /usr/local/mysql/]# . /etc/profile.d/mysql.sh

12、修改mariadb的配置文件需要增加几条内容
      [root@node2 /usr/local/mysql/]# vim /etc/mysql/my.cnf
          lower_case_table_names = 1
          character-set-server = utf8
          datadir = /app/mydata
          innodb_file_per_table = on
          skip_name_resolve = o

13、启动数据库服务
      [root@node2 /usr/local/mysql/]#  service mysqld start

14、查看mariadb的服务端口是否正常监听
    [root@node2 /app]#ss -tnl
    State      Recv-Q Send-Q       Local Address:Port                      Peer Address:Port
    LISTEN     0      128                      *:52874                                *:*
    LISTEN     0      128                      *:11211                                *:*
    LISTEN     0      128                      *:111                                  *:*
    LISTEN     0      128                      *:22                                   *:*
    LISTEN     0      128              127.0.0.1:631                                  *:*
    LISTEN     0      100              127.0.0.1:25                                   *:*
    LISTEN     0      80                      :::3306                                :::*

15、数据库的安全初始操作，设置完之后就可以先创建zabbix相关的库和用户
    [root@node2 /app]#mysql_secure_installation
    [root@node2 /app]#mysql -uroot -p
16、创建zabbix库
    MariaDB [(none)]&amp;gt; create database zabbix character set utf8 collate utf8_bin;
17、给zabbix库授权并指定用户
    MariaDB [(none)]&amp;gt; grant all privileges on zabbix.* to zabbix@&#39;192.168.137.%&#39; identified by &#39;123456&#39;;

18、在另一台主机上测试用zabbix用是否能正常登陆数据库
    [root@node7 ~]#mysql -uzabbix -p123456 -h192.168.137.54
    Welcome to the MariaDB monitor.  Commands end with ; or \g.
    Your MariaDB connection id is 12
    Server version: 10.2.15-MariaDB-log MariaDB Server

    Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

    Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.

    MariaDB [(none)]&amp;gt; show databases;
    +--------------------+
    | Database           |
    +--------------------+
    | information_schema |
    | zabbix             |
    +--------------------+
    2 rows in set (0.00 sec)
    MariaDB [(none)]&amp;gt;
19、在zabbix server主机上导入zabbix自带的三个表，路径在/root/zabbix-4.0.1/database/mysql下后缀为.sql的三个文件
    [root@node6 ~/zabbix-4.0.1]#ls -l database/mysql/
    total 5816
    -rw-r--r-- 1 1001 1001 3795433 Oct 30 01:36 data.sql
    -rw-r--r-- 1 1001 1001 1978341 Oct 30 01:36 images.sql
    -rw-r--r-- 1 root root   15323 Nov 26 22:44 Makefile
    -rw-r--r-- 1 1001 1001     392 Oct 30 01:36 Makefile.am
    -rw-r--r-- 1 1001 1001   15711 Oct 30 01:36 Makefile.in
    -rw-r--r-- 1 1001 1001  140265 Oct 30 01:36 schema.sql

20、导入sql文件是有先后顺序的，先导schema.sql、images.sql、data.sql.
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456 zabbix &amp;lt; schema.sql
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456 zabbix &amp;lt; images.sql
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456 zabbix &amp;lt; data.sql

21、进到数据库里面查看zabbix库是否导入成功
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456
    MariaDB [(none)]&amp;gt; use zabbix
    MariaDB [zabbix]&amp;gt; show tables;
    +----------------------------+
    | Tables_in_zabbix           |
    +----------------------------+
    | acknowledges               |
    | actions                    |
    | alerts                     |
    | application_discovery      |
    | application_prototype      |
    | application_template       |
    | applications               |
    | auditlog                   |
    | auditlog_details           |
    | autoreg_host               |
    | conditions                 |
    | config                     |
    | corr_condition             |
    | corr_condition_group       |
    .......
    | users                      |
    | users_groups               |
    | usrgrp                     |
    | valuemaps                  |
    | widget                     |
    | widget_field               |
    +----------------------------+
    144 rows in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;二、编译zabbix&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、安装编译环境所需要的依赖包组
    [root@node6 ~]#yum install gcc  libxml2-devel libevent-devel net-snmp net-snmp-devel  curl  curl-devel php  php-bcmath  php-mbstring mariadb mariadb-devel –y

    还需要安装一些php的依赖包后续在网页端安装zabbix时需要用到所以先提前安装好
    [root@node6 ~]#yum install php-gettext php-session php-ctype php-xmlreader php-xmlwrer php-xml php-net-socket php-gd php-mysql -y

2、安装jdk环境，装的是jdk-8u191-linux-x64.rpm的包，要不后面编译时会报Java找不到。
    [root@node6 ~]#yum -y install jdk-8u191-linux-x64.rpm

3、创建zabbix用户
    [root@node6 ~]#useradd zabbix -s /sbin/nologin

4、下载zabbix的源码包
    [root@node6 ~]#wget http://192.168.137.53/yum/zabbix/zabbix-4.0.1.tar.gz

5、解压源码包，并进入到解压后的目录里去
    [root@node6 ~]#tar xf zabbix-4.0.1.tar.gz
    [root@node6 ~]#cd zabbix-4.0.1/
    [root@node6 ~/zabbix-4.0.1]#

6、开始编译安装zabbix
    [root@node6 ~/zabbix-4.0.1./configure  \
    --prefix=/usr/local/zabbix  \
    --enable-server  \
    --enable-agent  \
    --with-mysql   \
    --with-net-snmp  \
    --with-libcurl  \
    --with-libxml2  \
    --enable-java

7、执行make install
    [root@node6 ~/zabbix-4.0.1]#make -j 2 &amp;amp;&amp;amp; make install

8、拷贝启动脚本文件到/etc/init.d目录下
    [root@node6 ~/zabbix-4.0.1]#cp misc/init.d/fedora/core/* /etc/init.d/

9、拷贝过去的脚本需要修改下目录路径，server和agent都需要改
    [root@node6 ~/zabbix-4.0.1]#vim /etc/init.d/zabbix_server
        22         BASEDIR=/usr/local
    改成：
        22         BASEDIR=/usr/local/zabbix

    agent启动脚本修改也是一样
        [root@node6 ~/zabbix-4.0.1vim /etc/init.d/zabbix_agentd
        22         BASEDIR=/usr/local
    改成：
        22         BASEDIR=/usr/local/zabbix

10、创建zabbix的日志存放路径和修改/usr/local/zabbix的所属主为zabbix
    [root@node6 ~/zabbix-4.0.1]#mkdir /var/log/zabbix
    [root@node6 ~/zabbix-4.0.1]#chown -R zabbix.zabbix /var/log/zabbix
    [root@node6 ~/zabbix-4.0.1]#ll /var/log/zabbix/ -d
    drwxr-xr-x 2 zabbix zabbix 6 Nov 27 09:17 /var/log/zabbix/
    [root@node6 ~]#chown -R zabbix.zabbix /usr/local/zabbix/
    [root@node6 ~]#ll -d /usr/local/zabbix/
    drwxr-xr-x 7 zabbix zabbix 64 Nov 26 22:45 /usr/local/zabbix/

11、修改配置文件
    [root@node6 ~/zabbix-4.0.1]#vim /usr/local/zabbix/etc/zabbix_server.conf
    ListenPort=10051   启用监听端口，不过默认也是启用的。

    LogFile=/var/log/zabbix/zabbix_server.log    修改日志存放路径，默认是在/tmp下

    LogFileSize=5   开启日志滚动，单位为MB、达到指定值之后就生成新的日志文件。
    DebugLevel=4   日志级别等级，4为debug，利于排除错误，排错之后可以改成3级别的。
    PidFile=/usr/local/zabbix/zabbix_server.pid   zabbix pid文件路径默认为tmp下需要改成安装目录，并且安装目录的所属组要改成zabbix用户
    # SocketDir=/tmp
    User=zabbix                    启动的用户默认也是zabbix,如果要改成root的话 还需要修改一项
    # AllowRoot=0                  需要改成1才能使用root来启动，默认0的话是被禁止用root启动，不过最好别用root
    SocketDir=/usr/local/zabbix   socket 文件存放路径默认在/tmp下
    DBHost=192.168.137.54          数据库地址必须要填
    DBName=zabbix                  数据库名称
    DBUser=zabbix                  数据库连接用户
    DBPassword=123456              数据库连接密码，建议在生产中密码不要太简单了。
    DBPort=3306                    数据库端口，其实也不用开默认就是3306

12、启动zabbix、并查看端口是否正常监听
    [root@node6 ~/zabbix-4.0.1]#service zabbix_server start
    Reloading systemd:                                         [  OK  ]
    Starting zabbix_server (via systemctl):                    [  OK  ]
    [root@node6 ~/zabbix-4.0.1]#ss -tnl
    State       Recv-Q Send-Q          Local Address:Port                Peer Address:Port
    LISTEN      0      128                         *:10051                    *:*
    LISTEN      0      128                         *:111                      *:*
    LISTEN      0      128                         *:22                       *:*
    LISTEN      0      100                 127.0.0.1:25                       *:*

13、装前端展示端
    [root@node6 ~/zabbix-4.0.1]#yum -y install httpd

14、在httpd的默认工作目录下创建一个zabbix目录
    [root@node6 ~/zabbix-4.0.1]#mkdir /var/www/html/zabbix

15、从zabbix解压包里面把php的所有文件拷贝到/var/www/html/zabbix目录下
    [root@node6 ~/zabbix-4.0.1]#cp -a frontends/php/* /var/www/html/zabbix/

16、启动httpd、查看端口是否正常监听
    [root@node6 ~]#systemctl start httpd
    [root@node6 ~]#ss -tnl
    State       Recv-Q Send-Q          Local Address:Port                         Peer Address:Port
    LISTEN      0      128                         *:10051                                   *:*
    LISTEN      0      128                         *:111                                     *:*
    LISTEN      0      128                         *:22                                      *:*
    LISTEN      0      100                 127.0.0.1:25                                      *:*
    LISTEN      0      128                        :::111                                    :::*
    LISTEN      0      128                        :::80                                     :::*

17、通过网页来安装zabbix
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;zabbix使用&#34;&gt;zabbix使用&lt;/h1&gt;

&lt;p&gt;zabbix的使用基本上都是在界面完成操作的，比较简单，基本使用流程&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;添加需要监控的主机&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;为监控的主机添加监控项，也就是key，zabbix自身带有很多设定好的监控项，直接选择就好，比如cpu，内存，都是界面操作&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监控项中可以直接输入参数，来获取指定的数据&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监控项可以自定义key，主要设定key，和执行的脚本命令command，可见zabbix都是通过执行命令来获取监控数据的&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix有对应的告警机制，也就是触发器，设置触发器也就是表达式，达到阈值，就会产生事件，然后可以通过各种通信方式发送，都是支持界面操作。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;zabbix对比promethes&#34;&gt;zabbix对比promethes&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;zabbix采集数据只能通过脚本命令，比较局限，基本都是物理机上的一些命令，所以zabbix比较适合物理机的监控，prometheus不但能够监控物理机，更适合云环境（频繁变动），比如k8s，&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据存储在mysql等关系型数据库中，存储有限，而且很难扩展监控维度，prometheus则是一个时序数据库，还可以远程存储，更适合&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix监控界面不够实时，相比于grafana也是一点都不美观，而且定制化特别难，而grafana则是得到公认的可编辑可扩展美观软件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix集群规模有限，上线为10000个节点，但是promtheus监控节点可以有更大的规模，速度也快。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix已经发展比较成熟，确实在管理界面上比较完善。但是prometheus比较灵活。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;zabbix的报文协议&#34;&gt;zabbix的报文协议&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;cmppingloss[&lt;target&gt;,&lt;packets&gt;,&lt;interval&gt;,&lt;size&gt;,&lt;timeout&gt;] 目标服务器，包数量，包发送间隔，包大小，超时&lt;/li&gt;
&lt;li&gt;value是string，一般是出错信息&lt;/li&gt;
&lt;li&gt;redis.cpunu.discovery这个是一个做发现的配置，最后生成了如下的配置可以舍去  ￼&lt;/li&gt;
&lt;li&gt;state表示在key不支持，或者是监控数据的过程中出错时候会出来&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;zabbix配置文件&#34;&gt;zabbix配置文件&lt;/h1&gt;

&lt;p&gt;zabbix的配置文件一般有三种：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zabbixserver的配置文件zabbix_server.conf

zabbixproxy的配置文件zabbix_proxy.conf

zabbix_agentd的配置文件zabbix_agentd.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.zabbixserver的配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NodeID=0 #分布式节点id号，0代表是独立服务器，默认是被注释掉的，不强制配置
ListenPort=10051 #zabbix server的端口，默认是10051，可以自行修改，
范围是1024-32767 ，一般默认即可
SourceIP=  #连接的源ip地址，默认为空，默认即可
LogFile=/tmp/zabbix_server.log #日志文件的存放位置
LogFileSize=1 #日志文件的大小，单位为MB，当设置为0时，表示不仅行日志轮询，
默认设置为1，默认即可
DebugLevel=3 #指定调试级别，默认即可
PidFile=/tmp/zabbix_server.pid #pid文件的存放位置
DBHost=localhost #数据库主机名，当设置为localhost时，连接mysql通过sock
DBName=zabbix #指定存放zabbix数据数据库的名字
DBUser=zabbix #指定连接数据库的用户名
DBPassword=123456 #用户连接数据库需要的密码
DBSocket=/var/lib/mysql/mysql.sock #前文主机设置为localhost，用户
连接数据库所用的sock位置，
DBPort=3306 #数据库的端口号，当用sock连接时，无关紧要，当通过网络连接时需设置
StartPollers=5 #默认即可
StartIPMIPollers=0 #使用IPMI协议时，用到的参数
StartTrappers=5 #打开的进程数，
StartPingers=1 同上
StartDiscoverers=1
StartHTTPPollers=1
JavaGateway=127.0.0.1 #JavaGateway的ip地址或主机名
JavaGatewayPort=10052 #JavaGateway的端口号
StartJavaPollers=5 #开启连接javagatey的进程数
SNMPTrapperFile=/tmp/zabbix_traps.tmp
StartSNMPTrapper=0 #如果设置为1，snmp trapper进程就会开启
ListenIP=0.0.0.0 #监听来自trapper的ip地址
ListenIP=127.0.0.1
HousekeepingFrequency=1 #zabbix执行Housekeeping的频率，单位为hours
MaxHousekeeperDelete=500 #每次最多删除历史数据的行
SenderFrequency=30 #zabbix试图发送未发送的警报的时间，单位为秒
CacheSize=8M #缓存的大小
CacheUpdateFrequency=60#执行更新缓存配置的时间，单位为秒数
StartDBSyncers=4
HistoryCacheSize=8M
TrendCacheSize=4M
HistoryTextCacheSize=16M
NodeNoEvents=0
NodeNoHistory=0
Timeout=3
TrapperTimeout=300
UnreachablePeriod=45
UnavailableDelay=60
UnreachableDelay=15
AlertScriptsPath=/usr/local/zabbix/shell #脚本的存放路径
FpingLocation=/usr/local/sbin/fping #fping指令的绝对路径
SSHKeyLocation=
LogSlowQueries=0
TmpDir=/tmp
Include=/usr/local/etc/zabbix_server.general.conf
Include=/usr/local/etc/zabbix_server.conf.d/ #子配置文件路径
StartProxyPollers=1 #在zabbix proxy被动模式下用此参数
ProxyConfigFrequency=3600#同上
ProxyDataFrequency=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ListenPort=10051 #监听端口



LogFile=/opt/zabbix/logs/zabbix_server.log

LogFileSize=1024

DebugLevel=3

PidFile=/var/run/zabbix/zabbix_server.pid

#mysql 数据库配置
DBHost=10.243.51.107
DBName=zabbix
DBUser=zabbix
DBPassword=zabbix@suning
DBPort=3306


StartPollers=500

StartIPMIPollers=1

StartPollersUnreachable=100

StartTrappers=100

StartPingers=50

StartDiscoverers=10

StartHTTPPollers=10

StartTimers=10










SNMPTrapperFile=/opt/zabbix/zabbix_traps.tmp

StartSNMPTrapper=1

# 监听地址
ListenIP=0.0.0.0

CacheSize=8G

CacheUpdateFrequency=3600

StartDBSyncers=50

HistoryCacheSize=2G

TrendCacheSize=2G


ValueCacheSize=10G


Timeout=25
TrapperTimeout=120
UnreachablePeriod=300
UnavailableDelay=60
UnreachableDelay=60
AlertScriptsPath=/opt/zabbix/alertscripts
ExternalScripts=/opt/zabbix/externalscripts
FpingLocation=/usr/sbin/fping


LogSlowQueries=10

StartProxyPollers=100
ProxyConfigFrequency=3600
ProxyDataFrequency=30
AllowRoot=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.zabbixagentd的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PidFile=/tmp/zabbix_agentd.pid #pid文件的存放位置
LogFile=/tmp/zabbix_agentd.log #日志文件的位置
LogFileSize=1 #当日志文件达到多大时进行轮询操作
DebugLevel=3 #日志信息级别
SourceIP= #连接的源ip地址，默认为空，即可
EnableRemoteCommands=0 #是否允许zabbix server端的远程指令，
0表示不允许，
1表示允许
LogRemoteCommands=0 #是否开启日志记录shell命令作为警告 0表示不允许，1表示允许
Server=127.0.0.1 #zabbix server的ip地址或主机名，可同时列出多个，需要用逗号隔开
ListenPort=10050 #zabbix agent监听的端口
ListenIP=0.0.0.0 #zabbix agent监听的ip地址
StartAgents=3 #zabbix agent开启进程数
ServerActive=127.0.0.1 #开启主动检查
Hostname=Zabbix server#在zabbix server前端配置时指定的主机名要相同，最重要的配置
RefreshActiveChecks=120 #主动检查刷新的时间，单位为秒数
BufferSend=5 #数据缓冲的时间
BufferSize=100 #zabbix agent数据缓冲区的大小，当达到该值便会发送所有的数据到zabbix server
MaxLinesPerSecond=100 #zabbix agent发送给zabbix server最大的数据行
AllowRoot=0 #是否允许zabbix agent 以root用户运行
Timeout=3 #设定处理超时的时间
Include=/usr/local/etc/zabbix_agentd.userparams.conf
Include=/usr/local/etc/zabbix_agentd.conf.d/ #包含子配置文件的路径
UnsafeUserParameters=0 #是否允许所有字符参数的传递
UserParameter= #指定用户自定义参数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PidFile=/var/run/zabbix/zabbix_agentd.pid

LogFile=/var/log/zabbix/zabbix_agentd.log

LogFileSize=0
Server=10.243.51.50

# 推送指标连接的服务器，格式如下addr:port
ServerActive=10.243.51.48

Hostname=10.243.51.50

HostMetadataItem=system.uname
Timeout=15

AllowRoot=1

Include=/etc/zabbix/zabbix_agentd.d/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.zabbixproxy的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Server=192.168.70.133 #指定zabbix server的ip地址或主机名
Hostname=zabbix-proxy-1.35 #定义监控代理的主机名，需和zabbix server前端配置时指定的节点名相同
LogFile=/tmp/zabbix_proxy.log #指定日志文件的位置
PidFile=/tmp/zabbix_proxy.pid #pid文件的位置
DBName=zabbix_proxy #数据库名
DBUser=zabbix #连接数据库的用户
DBPassword=123456#连接数据库用户的密码
ConfigFrequency=60 #zabbix proxy从zabbix server取得配置数据的频率
DataSenderFrequency=60 #zabbix proxy发送监控到的数据给zabbix server的频率
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#连接server的地址
Server=10.243.51.48

ServerPort=10052

Hostname=10.243.51.48

#启动监听的地址和端口
ListenPort=10051
ListenIP=0.0.0.0

LogFile=/opt/zabbix/logs/zabbix_proxy.log

LogFileSize=1024

DebugLevel=3

PidFile=/var/run/zabbix/zabbix_proxy.pid

#自带数据库
DBHost=localhost
DBName=zabbix_proxy
DBUser=zabbix
DBPassword=zabbix@suning
DBSocket=/opt/mysql/run/mysqld.sock
DBPort=3306



ProxyOfflineBuffer=1


ConfigFrequency=3600

DataSenderFrequency=20


StartPollers=300

StartIPMIPollers=10

StartPollersUnreachable=100

StartTrappers=100

StartPingers=20

StartDiscoverers=50

StartHTTPPollers=100




StartVMwareCollectors=10

VMwareFrequency=60

VMwareCacheSize=256M

CacheSize=8G

StartDBSyncers=10

HistoryCacheSize=2G

HistoryTextCacheSize=2G

Timeout=30

TrapperTimeout=300

UnreachablePeriod=300

UnavailableDelay=60

UnreachableDelay=15

ExternalScripts=/opt/zabbix/externalscripts

FpingLocation=/usr/sbin/fping

LogSlowQueries=0


AllowRoot=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;官网配置文件：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_proxy&#34;&gt;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_proxy&lt;/a&gt;
&lt;a href=&#34;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_server&#34;&gt;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_server&lt;/a&gt;
&lt;a href=&#34;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_agentd&#34;&gt;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_agentd&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>服务发现系列---- Etcd</title>
          <link>https://kingjcy.github.io/post/middleware/serverdiscovery/etcd/</link>
          <pubDate>Tue, 14 Feb 2017 15:32:07 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/serverdiscovery/etcd/</guid>
          <description>&lt;p&gt;ETCD是coreOS开源的用于共享配置和服务发现的分布式，一致性的KV存储系统。是一款类似于zk有望取代复杂的zk的用go语言开发的存储系统。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;p&gt;etcd有着几方面的优势：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一致性协议： ETCD使用&lt;a href=&#34;https://kingjcy.github.io/post/algorithm/raft/&#34;&gt;Raft&lt;/a&gt;协议， ZK使用ZAB（类PAXOS协议），前者容易理解，方便工程实现；&lt;/li&gt;
&lt;li&gt;运维方面：ETCD方便运维，ZK难以运维；&lt;/li&gt;
&lt;li&gt;项目活跃度：ETCD社区与开发活跃，ZK已经很臃肿维护困难；&lt;/li&gt;
&lt;li&gt;API：ETCD提供HTTP+JSON, gRPC接口，跨平台跨语言，ZK需要使用其客户端；&lt;/li&gt;
&lt;li&gt;访问安全方面：ETCD支持HTTPS访问，ZK在这方面缺失；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;etcd的使用场景和zk相似&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置管理&lt;/li&gt;
&lt;li&gt;服务注册与发现&lt;/li&gt;
&lt;li&gt;分布式队列&lt;/li&gt;
&lt;li&gt;分布式锁&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;h2 id=&#34;单点部署&#34;&gt;单点部署&lt;/h2&gt;

&lt;p&gt;安装比较简单，直接去开源的github上去下在压缩包，然后解压就有对应的可执行文件，可以将可执行文件etcd，etcdctl复制到/usr/bin下面使用&lt;/p&gt;

&lt;h2 id=&#34;集群部署&#34;&gt;集群部署&lt;/h2&gt;

&lt;p&gt;静态部署用命令直接启动&lt;/p&gt;

&lt;p&gt;node1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcd -name niub1 -debug \
-initial-advertise-peer-urls http://node1-ip:2380 \
-listen-peer-urls http://node1-ip:2380 \
-listen-client-urls http://node1-ip:2379,http://127.0.0.1:2379 \
-advertise-client-urls http://node1-ip:2379 \
-initial-cluster-token etcd-cluster-1 \
-initial-cluster niub1=http://node1-ip:2380,niub2=http://node2-ip:2380,niub3=http://node3-ip:2380 \
-initial-cluster-state new  &amp;gt;&amp;gt; ./etcd.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcd -name niub2 -debug \
-initial-advertise-peer-urls http://node2-ip:2380 \
-listen-peer-urls http://node2-ip:2380 \
-listen-client-urls http://node2-ip:2379,http://127.0.0.1:2379 \
-advertise-client-urls http://node2-ip:2379 \
-initial-cluster-token etcd-cluster-1 \
-initial-cluster niub1=http://node1-ip:2380,niub2=http://node2-ip:2380,niub3=http://node3-ip:2380 \
-initial-cluster-state new  &amp;gt;&amp;gt; ./etcd.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcd -name niub3 -debug \
-initial-advertise-peer-urls http://node3-ip:2380 \
-listen-peer-urls http://node3-ip:2380 \
-listen-client-urls http://node3-ip:2379,http://127.0.0.1:2379 \
-advertise-client-urls http://node3-ip:2379 \
-initial-cluster-token etcd-cluster-1 \
-initial-cluster niub1=http://node1-ip:2380,niub2=http://node2-ip:2380,niub3=http://node3-ip:2380 \
-initial-cluster-state new  &amp;gt;&amp;gt; ./etcd.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下基本参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ETCD_NAME –节点名称
ETCD_DATA_DIR -指定节点的数据存储目录
ETCD_LISTEN_PEER_URLS -监听URL，用于与其他节点通讯
ETCD_LISTEN_CLIENT_URLS –暴露自己的同时最好新增一个127.0.0.1的监听地址，便于etcdctl调用，当然用0.0.0.0也是可以的

ETCD_INITIAL_ADVERTISE_PEER_URLS -  告知集群其他节点url
ETCD_INITIAL_CLUSTER -  告知集群其他节点url.
ETCD_INITIAL_CLUSTER_STATE -  静态模式部署 new
ETCD_INITIAL_CLUSTER_TOKEN -集群的识别码
ETCD_ADVERTISE_CLIENT_URLS -告知客户端url, 也就是服务的url,不能包含127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以检查一下对集群情况了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcdctl member list
curl http://10.10.0.14:2379/v2/members
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;两种方式都能返回三个节点的相关情况，也可以使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;etcdctl cluster-health
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样etcd的集群就搭建成功了。&lt;/p&gt;

&lt;p&gt;正常会将其加入到系统服务中，首先创建设置配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /etc/etcd/etcd.conf

# [member]
ETCD_NAME=&amp;quot;etcd-2&amp;quot;
ETCD_DATA_DIR=&amp;quot;/data/etcd/&amp;quot;
#ETCD_WAL_DIR=&amp;quot;&amp;quot;
#ETCD_SNAPSHOT_COUNT=&amp;quot;10000&amp;quot;
#ETCD_HEARTBEAT_INTERVAL=&amp;quot;100&amp;quot;
#ETCD_ELECTION_TIMEOUT=&amp;quot;1000&amp;quot;
#ETCD_LISTEN_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
#ETCD_LISTEN_CLIENT_URLS=&amp;quot;http://localhost:2379&amp;quot;
ETCD_LISTEN_PEER_URLS=&amp;quot;http://0.0.0.0:7001&amp;quot;
ETCD_LISTEN_CLIENT_URLS=&amp;quot;http://0.0.0.0:4001&amp;quot;
#ETCD_MAX_SNAPSHOTS=&amp;quot;5&amp;quot;
#ETCD_MAX_WALS=&amp;quot;5&amp;quot;
#ETCD_CORS=&amp;quot;&amp;quot;
#
#[cluster]
#ETCD_INITIAL_ADVERTISE_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
ETCD_INITIAL_ADVERTISE_PEER_URLS=&amp;quot;http://172.32.148.128:7001&amp;quot;
# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &amp;quot;test=http://...&amp;quot;
#ETCD_INITIAL_CLUSTER=&amp;quot;default=http://localhost:2380&amp;quot;
ETCD_INITIAL_CLUSTER=&amp;quot;etcd-1=http://172.32.148.127:7001,etcd-2=http://172.32.148.128:7001,etcd-3=http://172.32.148.129:7001,etcd-4=http://172.32.148.130:7001&amp;quot;
ETCD_INITIAL_CLUSTER_STATE=&amp;quot;new&amp;quot;
#ETCD_INITIAL_CLUSTER_TOKEN=&amp;quot;etcd-cluster&amp;quot;
#ETCD_ADVERTISE_CLIENT_URLS=&amp;quot;http://localhost:2379&amp;quot;
ETCD_ADVERTISE_CLIENT_URLS=&amp;quot;http://172.32.148.128:4001&amp;quot;
#ETCD_DISCOVERY=&amp;quot;&amp;quot;
#ETCD_DISCOVERY_SRV=&amp;quot;&amp;quot;
#ETCD_DISCOVERY_FALLBACK=&amp;quot;proxy&amp;quot;
#ETCD_DISCOVERY_PROXY=&amp;quot;&amp;quot;
#ETCD_STRICT_RECONFIG_CHECK=&amp;quot;false&amp;quot;
#ETCD_AUTO_COMPACTION_RETENTION=&amp;quot;0&amp;quot;
.......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后增加开机启动配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /uusr/lib/systemd/system/etcd.service

[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=root
# set GOMAXPROCS to number of processors
#ExecStart=/bin/bash -c &amp;quot;GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\&amp;quot;${
ETCD_NAME}\&amp;quot; --data-dir=\&amp;quot;${
ETCD_DATA_DIR}\&amp;quot; --listen-client-urls=\&amp;quot;${
ETCD_LISTEN_CLIENT_URLS}\&amp;quot;&amp;quot;


ExecStart=/bin/bash -c &amp;quot;GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\&amp;quot;${
ETCD_NAME}\&amp;quot; --data-dir=\&amp;quot;${
ETCD_DATA_DIR}\&amp;quot; --listen-client-urls=\&amp;quot;${
ETCD_LISTEN_CLIENT_URLS}\&amp;quot; --listen-peer-urls=\&amp;quot;${
ETCD_LISTEN_PEER_URLS}\&amp;quot; --advertise-client-urls=\&amp;quot;${
ETCD_ADVERTISE_CLIENT_URLS}\&amp;quot; --initial-advertise-peer-urls=\&amp;quot;${
ETCD_INITIAL_ADVERTISE_PEER_URLS}\&amp;quot; --initial-cluster=\&amp;quot;${
ETCD_INITIAL_CLUSTER}\&amp;quot; --initial-cluster-state=\&amp;quot;${
ETCD_INITIAL_CLUSTER_STATE}\&amp;quot;&amp;quot;


Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然我们可以使用别人的rpm包来安装，就有现成的配置文件，我们在上面修改就行了。&lt;/p&gt;

&lt;p&gt;下面我们来来系统启动etcd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable etcd.service
systemctl start etcd.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以检查集群了，有一些需要主要的地方，一个就是服务的用户要有对应目录的权限。&lt;/p&gt;

&lt;h2 id=&#34;k8s部署&#34;&gt;k8s部署&lt;/h2&gt;

&lt;p&gt;一般部署在每个master节点上，组成一个集群。&lt;/p&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;p&gt;直接参考官方的md文件。就是正常的key/value类型的数据库的使用方法，类似于redis的使用。比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;写数据
#etcdctl put foo bar
#etcdctl put fool bar1 --lease=1234abcd
读数据
#etcdctl get foo
#etcdctl get foo --rev=3
删除数据
#etcdctl del foo
Watch机制
#etcdctl watch foo
租约TTL
#etcdctl lease grant 10
#etcdctl put –lease=3269xxx foo bar
#etcdctl lease revoke 3269xxx
#etcdctl lease keep-alive 3269xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;etcd碎片整理（defragmentation)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ etcdctl defrag Finished defragmenting etcd member[127.0.0.1:2379]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对每个节点的defrag时间需要错开，不能同时进行。&lt;/p&gt;

&lt;h2 id=&#34;client&#34;&gt;client&lt;/h2&gt;

&lt;p&gt;etcd/clientv3 is the official Go etcd client for v3.&lt;/p&gt;

&lt;p&gt;基本实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;time&amp;quot;
    &amp;quot;github.com/coreos/etcd/clientv3&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;context&amp;quot;
    &amp;quot;github.com/coreos/etcd/mvcc/mvccpb&amp;quot;
    &amp;quot;ketang/netWork/0604_Socket/Tool&amp;quot;
)

var (
    dialTimeout = 5 * time.Second
    requestTimeout = 2 * time.Second
    endPoints = []string{&amp;quot;127.0.0.1:2379&amp;quot;} //etcd 默认接受数据的端口2379
)
//添加 删除 查找 前缀 延时

var etcd *clientv3.Client
func main()  {
    fmt.Println(Tool.GetLocalIp())
    var err error
    etcd, err =clientv3.New(clientv3.Config{
        Endpoints:endPoints,
        DialTimeout:dialTimeout,


    })
    if err != nil {
        fmt.Println(err)
    }

    //添加
    err = putValue(&amp;quot;a&amp;quot;, &amp;quot;abc&amp;quot;)
    fmt.Println(err)

    //查找
    result := getValue(&amp;quot;a&amp;quot;)
    fmt.Println(result)

    //删除
    cnt := delValue(&amp;quot;a&amp;quot;)
    fmt.Println(&amp;quot;delete:&amp;quot;, cnt)


    err = putValue(&amp;quot;b1&amp;quot;, &amp;quot;abc1&amp;quot;)
    err = putValue(&amp;quot;b2&amp;quot;, &amp;quot;abc2&amp;quot;)
    err = putValue(&amp;quot;b3&amp;quot;, &amp;quot;abc3&amp;quot;)

    //按前缀查找
    result = getValueWIthPrefix(&amp;quot;b&amp;quot;)
    fmt.Println(result)
    for _,item := range result {
        fmt.Println(string(item.Key),string(item.Value))
    }

    //按前缀删除
    cnt2 := delValueWithPrefix(&amp;quot;b&amp;quot;)
    fmt.Println(&amp;quot;批量删除：&amp;quot;, cnt2)



    //事务处理
    putValue(&amp;quot;user1&amp;quot;, &amp;quot;zhangsan&amp;quot;)
    _,err = etcd.Txn(context.TODO()).
        If(clientv3.Compare(clientv3.Value(&amp;quot;user1&amp;quot;),&amp;quot;=&amp;quot;, &amp;quot;zhangsan&amp;quot;)).
        Then(clientv3.OpPut(&amp;quot;user1&amp;quot;, &amp;quot;zhangsan&amp;quot;)).
        Else(clientv3.OpPut(&amp;quot;user1&amp;quot;, &amp;quot;lisi&amp;quot;)).Commit()

    fmt.Println(err)
    result = getValue(&amp;quot;user1&amp;quot;)
    fmt.Println(&amp;quot;user1:&amp;quot;, string(result[0].Value))


    //lease 设置有效时间
    resp, err:= etcd.Grant(context.TODO(), 1)
    _,err = etcd.Put(context.TODO(), &amp;quot;username&amp;quot;,&amp;quot;wangwu&amp;quot;,clientv3.WithLease(resp.ID))

    time.Sleep(3 * time.Second)

    v := getValue(&amp;quot;username&amp;quot;)
    fmt.Println(&amp;quot;lease:&amp;quot;,v)


    //watch监听的使用
    putValue(&amp;quot;w&amp;quot;, &amp;quot;hello&amp;quot;)
    go func() {
        rch := etcd.Watch(context.Background(),&amp;quot;w&amp;quot;)
        for wresp := range  rch {
            for _,ev := range wresp.Events {
                fmt.Printf(&amp;quot;watch&amp;gt;&amp;gt;w  %s %q %q\n&amp;quot;, ev.Type,ev.Kv, ev.Kv)
            }
        }
    }()

    putValue(&amp;quot;w&amp;quot;, &amp;quot;hello world!&amp;quot;)



    //监听某个key在一定范围内 value的变化
    //putValue(&amp;quot;fo0&amp;quot;, &amp;quot;a&amp;quot;)
    go func() {
        //监听范围 [fo0-fo3)
        rch := etcd.Watch(context.Background(), &amp;quot;fo0&amp;quot;, clientv3.WithRange(&amp;quot;fo3&amp;quot;))

        for wresp := range  rch {
            for _,ev := range wresp.Events {
                fmt.Printf(&amp;quot;watch range  --   %s %q %q\n&amp;quot;, ev.Type,ev.Kv, ev.Kv)
            }
        }
    }()

    putValue(&amp;quot;fo0&amp;quot;, &amp;quot;b&amp;quot;)
    putValue(&amp;quot;fo1&amp;quot;, &amp;quot;b&amp;quot;)
    putValue(&amp;quot;fo2&amp;quot;, &amp;quot;c&amp;quot;)
    putValue(&amp;quot;fo2.5&amp;quot;, &amp;quot;c&amp;quot;)
    putValue(&amp;quot;fo3&amp;quot;, &amp;quot;c&amp;quot;)


    time.Sleep(10 * time.Second)

}


//添加键值对
func putValue(key, value string)  error  {
    _, err := etcd.Put(context.TODO(),key, value)
    return err
}

//查询
func getValue(key string) []*mvccpb.KeyValue  {
    resp, err := etcd.Get(context.TODO(), key)
    if err != nil {
        return  nil
    } else {
        return resp.Kvs
    }
}

// 返回删除了几条数据
func delValue(key string) int64  {
    res,err := etcd.Delete(context.TODO(),key)
    if err != nil {
        return 0
    } else {
        return res.Deleted
    }

}


//按照前缀删除
func delValueWithPrefix(prefix string) int64  {
    res,err := etcd.Delete(context.TODO(),prefix,clientv3.WithPrefix())
    if err != nil {
        fmt.Println(err)
        return 0
    } else {
        return res.Deleted
    }
}

func getValueWIthPrefix(prefix string) []*mvccpb.KeyValue {
    resp, err := etcd.Get(context.TODO(), prefix, clientv3.WithPrefix())
    if err != nil {
        return  nil
    } else {
        return resp.Kvs
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用总结&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conn.(clientv3.Client).Close()

cannot call pointer method on conn.(clientv3.Client)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要使用指针&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conn.(*clientv3.Client).Close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;主要使用的raft协议实现，可以查看&lt;a href=&#34;https://kingjcy.github.io/post/algorithm/raft/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>服务发现系列---- Consul</title>
          <link>https://kingjcy.github.io/post/middleware/serverdiscovery/consul/</link>
          <pubDate>Sun, 12 Feb 2017 16:04:21 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/serverdiscovery/consul/</guid>
          <description>&lt;p&gt;Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对。&lt;/p&gt;

&lt;h1 id=&#34;consul&#34;&gt;consul&lt;/h1&gt;

&lt;p&gt;Consul是一个服务发现和注册的工具，其具有分布式、高扩展性能特点,主要包含如下功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务发现： 支持 http 和 dns 两种协议的服务注册和发现方式。&lt;/li&gt;
&lt;li&gt;监控检查： 支持多种方式的健康检查。&lt;/li&gt;
&lt;li&gt;Key/Value存储： 支持通过HTTP API实现分布式KV数据存储。&lt;/li&gt;
&lt;li&gt;多数据中心支持：支持任意数量数据中心。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Consul 的使用场景&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实例的注册与配置共享&lt;/li&gt;
&lt;li&gt;与 confd 服务集成，动态生成 nginx 和 haproxy 配置文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;优势&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用 Raft 算法来保证一致性, 比复杂的 Paxos 算法更直接. zookeeper 采用的是 Paxos, consul,etcd 使用的则是 Raft.&lt;/li&gt;
&lt;li&gt;支持多数据中心，内外网的服务采用不同的端口进行监听。 多数据中心集群可以避免单数据中心的单点故障,而其部署则需要考虑网络延迟, 分片等情况等. zookeeper 和 etcd 均不提供多数据中心功能的支持.&lt;/li&gt;
&lt;li&gt;支持健康检查. etcd 不提供此功能.&lt;/li&gt;
&lt;li&gt;支持 http 和 dns 协议接口. zookeeper 的集成较为复杂, etcd 只支持 http 协议.&lt;/li&gt;
&lt;li&gt;官方提供web管理界面, etcd 无此功能.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;角色&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;client: 客户端, 无状态, 将 HTTP 和 DNS 接口请求转发给局域网内的服务端集群.&lt;/li&gt;
&lt;li&gt;server: 服务端, 保存配置信息, 高可用集群, 在局域网内与本地客户端通讯, 通过广域网与其他数据中心通讯. 每个数据中心的 server 数量推荐为 3 个或是 5 个.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;consul安装&#34;&gt;Consul安装&lt;/h1&gt;

&lt;p&gt;下载并解压&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## 下载
wget  https://releases.hashicorp.com/consul/1.0.0/consul_1.0.0_linux_amd64.zip?_ga=2.31706621.2141899075.1510636997-716462484.1510636997
## 解压
unzip consul_1.0.0_linux_amd64.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以UI形式后台启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./consul agent -server -ui -bootstrap-expect 1 -data-dir /tmp/consul &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用脚本，正常使用的时候是需要完善参数的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup /opt/consul_1.0.2/consul agent -server -bootstrap-expect 1 -data-dir /opt/consul_1.0.2/data -ui  -http-port=9996 -bind=10.47.178.81 -client 0.0.0.0 -config-dir /opt/consul_1.0.2/consul.d -enable-script-checks  &amp;gt;/opt/consul_1.0.2/logs/start.log  2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看启动状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@iZ2ze74 home]# ./consul members
Node     Address   Status  Type    Build  Protocol  DC   Segment
iZ2ze74  172.17.120.102:8301  alive   server  1.0.0  2         dc1  &amp;lt;all&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Address：节点地址&lt;/li&gt;
&lt;li&gt;Status：alive表示节点健康&lt;/li&gt;
&lt;li&gt;Type：server运行状态是server状态&lt;/li&gt;
&lt;li&gt;DC：dc1表示该节点属于DataCenter1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;查看节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl 127.0.0.1:8500/v1/catalog/nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;h2 id=&#34;注册&#34;&gt;注册&lt;/h2&gt;

&lt;p&gt;consul注册注册service 的方式有多种&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;静态注册&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;创建文件夹consul.d&lt;/p&gt;

&lt;p&gt;添加如下test.json：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;service&amp;quot;:{
    &amp;quot;id&amp;quot;: &amp;quot;node&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;prometheus-node&amp;quot;,
    &amp;quot;address&amp;quot;: &amp;quot;127.0.0.1&amp;quot;,
    &amp;quot;port&amp;quot;: 9100,
    &amp;quot;tags&amp;quot;: [&amp;quot;prometheus-target&amp;quot;],
    &amp;quot;checks&amp;quot;: [
        {
            &amp;quot;http&amp;quot;: &amp;quot;http://127.0.0.1:9100/metrics&amp;quot;,
            &amp;quot;interval&amp;quot;: &amp;quot;15s&amp;quot;
        }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在consul启动命令中，指定配置路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-config-dir=consul.d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动后查看Prometheus 和consul 界面，可以看到target是否引入。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;使用http Api 的方式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;curl -X PUT -d &#39;{&amp;quot;service&amp;quot;:{&amp;quot;id&amp;quot;:&amp;quot;node&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;prometheus-node&amp;quot;,&amp;quot;address&amp;quot;:&amp;quot;127.0.0.1&amp;quot;,&amp;quot;port&amp;quot;:9100,&amp;quot;tags&amp;quot;:[&amp;quot;prometheus-target&amp;quot;],&amp;quot;checks&amp;quot;:[{&amp;quot;http&amp;quot;:&amp;quot;http://127.0.0.1:9100/metrics&amp;quot;,&amp;quot;interval&amp;quot;:&amp;quot;15s&amp;quot;}]}}&#39; http://127.0.0.1:8500/v1/agent/service/register
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;还可以使用各语言版本的sdk:&lt;a href=&#34;https://www.consul.io/api/libraries-and-sdks.html&#34;&gt;https://www.consul.io/api/libraries-and-sdks.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我这里使用JAVA 版本的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;com.orbitz.consul&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;consul-client&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;1.0.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
使用如下：

public class ConsulTest {
  Consul client;

  /**
   * 初始化.
   */
  @Before
  public void init() {

    client = Consul.builder().withHostAndPort(HostAndPort.fromParts(&amp;quot;xx.xx.xx.xx&amp;quot;, 8500)).build();
    //  catalogClient = client.catalogClient();
  }

  @Test
  public void queryAll() {
    Map&amp;lt;String, Service&amp;gt; services = client.agentClient().getServices();
    for (Map.Entry&amp;lt;String, Service&amp;gt; entry : services.entrySet()) {
      System.out.println(&amp;quot;key:&amp;quot; + entry.getKey());
      System.out.println(&amp;quot;value:&amp;quot; + entry.getValue().toString());
    }

  }


  @Test
  public void testDelete() {
    client.agentClient().deregister(&amp;quot;etcd&amp;quot;);
  }


  @Test
  public void testAdd1() {
    String serviceName = &amp;quot;prometheus-etcd&amp;quot;;
    String serviceId = &amp;quot;etcd&amp;quot;;
    Registration.RegCheck single = Registration.RegCheck.http(&amp;quot;http://127.0.0.1:2379/metrics&amp;quot;, 20);
    Registration reg = ImmutableRegistration.builder()
        .check(single)
        .addTags(&amp;quot;prometheus-target&amp;quot;)
        .address(&amp;quot;127.0.0.1&amp;quot;)
        .port(2379)
        .name(serviceName)
        .id(serviceId)
        .build();
    client.agentClient().register(reg);
  }



}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;发现&#34;&gt;发现&lt;/h2&gt;

&lt;p&gt;查询注册的服务&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;通过DNS api&lt;/p&gt;

&lt;p&gt;dig @127.0.0.1 -p 8600 web.service.consul(dns域名)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;通过http api&lt;/p&gt;

&lt;p&gt;curl &lt;a href=&#34;http://localhost:8500/v1/catalog/service/web&#34;&gt;http://localhost:8500/v1/catalog/service/web&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有一个connect模块，直接连接，docker和k8s中使用的比较多。&lt;/p&gt;

&lt;h2 id=&#34;健康检查&#34;&gt;健康检查&lt;/h2&gt;

&lt;p&gt;通过调用http api来获取监控状况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl &#39;http://localhost:8500/v1/health/service/web?passing&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;更新consul配置文件&#34;&gt;更新consul配置文件&lt;/h2&gt;

&lt;p&gt;1、可以通过更改配置文件并将SIGHUP代理文件发送到代理来更新服务定义。这使您可以在不停机或不可用于服务查询的情况下更新服务。现在还支持consul reload&lt;/p&gt;

&lt;p&gt;2、HTTP API可用于动态添加，删除和修改服务。&lt;/p&gt;

&lt;h2 id=&#34;常用命令&#34;&gt;常用命令&lt;/h2&gt;

&lt;p&gt;consul&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;agent：运行一个consul agent&lt;/li&gt;
&lt;li&gt;join：将agent加入到consul cluster&lt;/li&gt;
&lt;li&gt;members：列出consul cluster集群中的members&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;常用选项option：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-data-dir

作用：指定agent储存状态的数据目录
这是所有agent都必须的
对于server尤其重要，因为他们必须持久化集群的状态
-config-dir

作用：指定service的配置文件和检查定义所在的位置
通常会指定为&amp;quot;某一个路径/consul.d&amp;quot;（通常情况下，.d表示一系列配置文件存放的目录）
-config-file

作用：指定一个要装载的配置文件
该选项可以配置多次，进而配置多个配置文件（后边的会合并前边的，相同的值覆盖）
-dev

作用：创建一个开发环境下的server节点
该参数配置下，不会有任何持久化操作，即不会有任何数据写入到磁盘
这种模式不能用于生产环境（因为第二条）
-bootstrap-expect

作用：该命令通知consul server我们现在准备加入的server节点个数，该参数是为了延迟日志复制的启动直到我们指定数量的server节点成功的加入后启动。
-node

作用：指定节点在集群中的名称
该名称在集群中必须是唯一的（默认采用机器的host）
推荐：直接采用机器的IP
-bind

作用：指明节点的IP地址
-server

作用：指定节点为server
每个数据中心（DC）的server数推荐为3或5（理想的是，最多不要超过5）
所有的server都采用raft一致性算法来确保事务的一致性和线性化，事务修改了集群的状态，且集群的状态保存在每一台server上保证可用性
server也是与其他DC交互的门面（gateway）
-client

作用：指定节点为client
若不指定为-server，其实就是-client
-join

作用：将节点加入到集群
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;prometheus原生就支持consul的服务发现，可以直接获取consul上的配置，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrape_configs:
  # The job name is added as a label `job=&amp;lt;job_name&amp;gt;` to any timeseries scraped from this config.
  - job_name: &#39;prometheus&#39;

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    # metrics_path defaults to &#39;/metrics&#39;
    # scheme defaults to &#39;http&#39;.

    static_configs:
      - targets: [&#39;localhost:9090&#39;]

  - job_name: &#39;security&#39;
    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    metrics_path: &#39;/prometheus&#39;
    # scheme defaults to &#39;http&#39;.

    static_configs:
      - targets: [&#39;10.94.20.33:80&#39;]

  - job_name: &#39;overwritten-default&#39;
    consul_sd_configs:
    - server:   &#39;10.110.200.29:8500&#39;
      services: [&#39;lookup&#39;, &#39;security&#39;, &#39;workflow&#39;]

    relabel_configs:
    - source_labels: [&#39;__metrics_path__&#39;]
      regex:         &#39;/metrics&#39;
      target_label:  __metrics_path__
      replacement:   &#39;/prometheus&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里简单说明下上面的配置意义，在scrape_configs下，定义了3个job_name&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;job_name: &amp;lsquo;prometheus&amp;rsquo;是监听prometheus服务本身；&lt;/li&gt;
&lt;li&gt;job_name: &amp;lsquo;security&amp;rsquo;是按固定IP:PORT的方式监听微服务 ；&lt;/li&gt;
&lt;li&gt;job_name: &amp;lsquo;overwritten-default&amp;rsquo;就是一个监听consul的任务，在consul_sd_configs下，server是consul服务器的访问地址，services是微服务名的数组，如果什么都不填，则默认取consul上注册的所有微服务。relabel_configs是修改默认配置的规则，这里由于使用了springboot和promethues整合，暴露的metrics是通过/promethues路径访问的，而promethues默认的metrics访问路径（即metrics_path配置项）是/metrics，需要修改。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;consul-template&#34;&gt;consul-template&lt;/h2&gt;

&lt;p&gt;consul-template可以启动多个程序用于生成不同的模版json文件.&lt;/p&gt;

&lt;p&gt;consul-template的两种使用方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;直接启动作为进程使用，配置文件中间有一个时间设定，多长时间更新一次&lt;/li&gt;
&lt;li&gt;使用crontab来定时拉去一次，使用启动参数-once&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用consul+consul-template可以构建一套基于文件服务发现的动态注册和配置生成的服务发现功能。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过consul的api向consul上注册prometheus的采集信息，使用k/v的模式&lt;/li&gt;
&lt;li&gt;consul-template设置定时功能，命令行拉去consul上的配置，按着模版的形式生成json文件&lt;/li&gt;
&lt;li&gt;prometheus使用fd的服务发现模式读取json文件拉去采集信息&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;什么时候使用k/v模式？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用consul的services注册job的服务信息，然后使用consul-template动态生成prometheus的配置文件。然后prometheus通过查询consul中注册的信息正则匹配来完成prometheus的采集操作，在规模请求很小的时候，service完全没有问题
但是这样当job量很大的时候，比如有20组job，一组job130的target的的时候，就会出现consul请求瓶颈。&lt;/p&gt;

&lt;p&gt;所以在规模扩大的时候使用consul的k/v格式进行注册，直接通过IP：port作为key，对应的label作为vaule，然后使用consul-template动态生成discovery的json文件，然后prometheus使用file sd来发现这个json文件，相当于将对应的json的内容写到了prometheus的配置文件中去，这个时候五分钟consul-template动态生成一次，不会每次都去请求，这样consul的压力就几乎没有了，经过测试可以达到5000个target，prometheus的shard极限，对consul依旧没有什么压力，现在主要瓶颈在于json文件大小，filesd的压力，可以继续优化成多个文件。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;raft算法&#34;&gt;raft算法&lt;/h2&gt;

&lt;p&gt;Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选人（Candidate）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Leader：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。&lt;/li&gt;
&lt;li&gt;Follower：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。&lt;/li&gt;
&lt;li&gt;Candidate：Leader选举过程中的临时角色。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Raft实现了和Paxos相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等,核心就是Leader选举，可以&lt;a href=&#34;https://kingjcy.github.io/post/algorithm/raft/&#34;&gt;详细了解raft&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Raft 使用心跳（heartbeat）触发Leader选举。当服务器启动时，初始化为Follower。Leader向所有Followers周期性发送heartbeat。如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举。&lt;/p&gt;

&lt;p&gt;Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC 。结果有以下三种情况：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;赢得了多数的选票，成功选举为Leader；&lt;/li&gt;
&lt;li&gt;收到了Leader的消息，表示有其它服务器已经抢先当选了Leader；&lt;/li&gt;
&lt;li&gt;没有服务器赢得多数的选票，Leader选举失败，等待选举时间超时后发起下一次选举。&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
      
    
      
        <item>
          <title>服务发现系列---- Sd</title>
          <link>https://kingjcy.github.io/post/middleware/serverdiscovery/sd/</link>
          <pubDate>Sun, 12 Feb 2017 16:04:18 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/middleware/serverdiscovery/sd/</guid>
          <description>&lt;p&gt;服务发现就是程序如何通过一个标志来获取服务列表，并且这个服务列表是能够随着服务的状态而动态变更，最终得以调用到相应的服务。&lt;/p&gt;

&lt;p&gt;服务发现是在分布式系统规模越来越大的情况下，服务治理的必然产物，不然服务的配置调用将难以维护。&lt;/p&gt;

&lt;h1 id=&#34;服务发现&#34;&gt;服务发现&lt;/h1&gt;

&lt;p&gt;服务发现可以分为注册和解析两个部分。&lt;/p&gt;

&lt;h2 id=&#34;服务注册&#34;&gt;服务注册&lt;/h2&gt;

&lt;p&gt;存储的信息是域名和ip的对应映射关系，存储的解析信息至少包括正在运行的服务的主机ip和端口信息。&lt;/p&gt;

&lt;p&gt;注册的方式有两种&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;上报，每个服务启动后主动将自己的域名和ip信息上报存储下来&lt;/li&gt;
&lt;li&gt;监听，服务监听集群中服务的创建，并且获取相关信息存储下来，比如coreDNS&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;解析发现&#34;&gt;解析发现&lt;/h2&gt;

&lt;p&gt;服务发现主要存在有两种模式，客户端模式与服务端模式，两者的本质区别在于，客户端是否保存服务列表信息。&lt;/p&gt;

&lt;h3 id=&#34;客户端模式&#34;&gt;客户端模式&lt;/h3&gt;

&lt;p&gt;在客户端模式下，如果要进行微服务调用，首先要进行的是到服务注册中心获取服务列表，然后再根据调用端本地的负载均衡策略，进行服务调用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;客户端(自身有服务注册中心--可以获取服务列表)------&amp;gt;调用服务
   |
   |
   |
服务注册中心--服务列表
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们用图可以看出来&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/sd/sd&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;只需要周期性获取列表，在调用服务时可以直接调用少了一个RT。但需要在每个客户端维护获取列表的逻辑&lt;/li&gt;
&lt;li&gt;可用性高，即使注册中心出现故障也能正常工作&lt;/li&gt;
&lt;li&gt;服务上下线对调用方有影响（会出现短暂调用失败）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;大部分服务发现的实现都采取了客户端模式&lt;/p&gt;

&lt;h3 id=&#34;服务端模式&#34;&gt;服务端模式&lt;/h3&gt;

&lt;p&gt;在服务端模式下，调用方直接向服务注册中心进行请求，服务注册中心再通过自身负载均衡策略，对微服务进行调用。这个模式下，调用方不需要在自身节点维护服务发现逻辑以及服务注册信息，这个模式相对来说比较类似DNS模式。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;客户端(自身没有服务注册中心)------&amp;gt;服务注册中心(在服务端)------&amp;gt;调用服务
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/middleware/sd/sd1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;优点&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;简单，不需要在客户端维护获取服务列表的逻辑&lt;/li&gt;
&lt;li&gt;可用性由路由器中间件决定，路由中间件故障则所有服务不可用，同时，由于所有调度以及存储都由中间件服务器完成，中间件服务器可能会面临过高的负载&lt;/li&gt;
&lt;li&gt;服务上下线调用方无感知&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;框架&#34;&gt;框架&lt;/h2&gt;

&lt;p&gt;目前服务发现框架：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;consul---------go语言编写，常用，简单，无依赖，集成了http/DNS library
etcd-----------go语言编写，常用，简单，无依赖，集成了Client Binging／http
Zookeeper------java语言编写，常用，简单，依赖jvm，集成了Client Binging
Eureka---------java
smartstsck-----ruby
nsq------------go
serf-----------go
spotify--------DNS
skydns---------go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前三个都是比较常用和通用的，其他都是自己造轮子，适合特定场景。&lt;/p&gt;

&lt;h3 id=&#34;eureka&#34;&gt;Eureka&lt;/h3&gt;

&lt;p&gt;eureka是netflix用于服务注册和发现的框架。在这个框架中，分为server和client两种角色。server负责保存服务的注册信息，同时server之间也可以彼此相互注册，client则需要向特定的server进行注册。&lt;/p&gt;

&lt;p&gt;client/server通过RESTful Api向server进行服务注册，并且定期调用renew接口来更新服务的注册状态，若server在60s内没有收到服务的renew信息，则该服务就会被标志为下线。而如果服务需要主动下线的话，向server调用cancel就可以了。&lt;/p&gt;

&lt;h3 id=&#34;consul&#34;&gt;Consul&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/middleware/serverdiscovery/consul/&#34;&gt;Consul&lt;/a&gt;是强一致性的数据存储，使用Raft形成动态集群。它提供分级键/值存储方式，不仅可以存储数据，而且可以用于注册器件事各种任务，从发送数据改变通知到运行健康检查和自定义命令，具体如何取决于它们的输出。&lt;/p&gt;

&lt;p&gt;与Zookeeper和etcd不一样，Consul内嵌实现了服务发现系统，所以这样就不需要构建自己的系统或使用第三方系统。这一发现系统除了上述提到的特性之外，还包括节点健康检查和运行在其上的服务。&lt;/p&gt;

&lt;p&gt;Zookeeper和etcd只提供原始的键/值队存储，要求应用程序开发人员构建他们自己的系统提供服务发现功能。而Consul提供了一个内置的服务发现的框架。客户只需要注册服务并通过DNS或HTTP接口执行服务发现。其他两个工具需要一个亲手制作的解决方案或借助于第三方工具。&lt;/p&gt;

&lt;p&gt;Consul为多种数据中心提供了开箱即用的原生支持，其中的gossip系统不仅可以工作在同一集群内部的各个节点，而且还可以跨数据中心工作。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;模版Consul-template&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;confd可以像和etcd搭配一样用于Consul，不过Consul有自己的模板服务，其更适配Consul。&lt;/p&gt;

&lt;p&gt;通过从Consul获得的信息，Consul-template是一个非常方便的创建文件的途径，还有一个额外的好处就是在文件更新后可以运行任意命令，正如confd，Consul-template也可以使用Go模板格式。&lt;/p&gt;

&lt;h3 id=&#34;zookeeper&#34;&gt;Zookeeper&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/middleware/serverdiscovery/zookeeper/&#34;&gt;Zookeeper&lt;/a&gt;是这种类型的项目中历史最悠久的之一，它起源于Hadoop，帮助在Hadoop集群中维护各种组件。它非常成熟、可靠，被许多大公司（YouTube、eBay、雅虎等）使用。其数据存储的格式类似于文件系统，如果运行在一个服务器集群中，Zookeper将跨所有节点共享配置状态，每个集群选举一个领袖，客户端可以连接到任何一台服务器获取数据。&lt;/p&gt;

&lt;p&gt;Zookeeper的主要优势是其成熟、健壮以及丰富的特性，然而，它也有自己的缺点，其中采用Java开发以及复杂性是罪魁祸首。尽管Java在许多方面非常伟大，然后对于这种类型的工作还是太沉重了，Zookeeper使用Java以及相当数量的依赖使其对于资源竞争非常饥渴。因为上述的这些问题，Zookeeper变得非常复杂，维护它需要比我们期望从这种类型的应用程序中获得的收益更多的知识。这部分地是由于丰富的特性反而将其从优势转变为累赘。应用程序的特性功能越多，就会有越大的可能性不需要这些特性，因此，我们最终将会为这些不需要的特性付出复杂度方面的代价。&lt;/p&gt;

&lt;p&gt;Zookeeper为其他项目相当大的改进铺平了道路，“大数据玩家“在使用它，因为没有更好的选择。今天，Zookeeper已经老态龙钟了，我们有了更好的选择。&lt;/p&gt;

&lt;h3 id=&#34;etcd&#34;&gt;etcd&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/middleware/serverdiscovery/etcd/&#34;&gt;etcd&lt;/a&gt;是一个采用HTTP协议的健/值对存储系统，它是一个分布式和功能层次配置系统，可用于构建服务发现系统。其很容易部署、安装和使用，提供了可靠的数据持久化特性。它是安全的并且文档也十分齐全。&lt;/p&gt;

&lt;p&gt;etcd比Zookeeper是比更好的选择，因为它很简单，然而，它需要搭配一些第三方工具才可以提供服务发现功能。比如Confd&lt;/p&gt;

&lt;p&gt;Confd是一个轻量级的配置管理工具，常见的用法是通过使用存储在etcd、consul和其他一些数据登记处的数据保持配置文件的最新状态，它也可以用来在配置文件改变时重新加载应用程序。换句话说，我们可以用存储在etcd（或者其他注册中心）的信息来重新配置所有服务。&lt;/p&gt;

&lt;h3 id=&#34;对比&#34;&gt;对比&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;功能\组件&lt;/th&gt;
&lt;th&gt;Zookeeper&lt;/th&gt;
&lt;th&gt;etcd&lt;/th&gt;
&lt;th&gt;Consul&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;产生时间&lt;/td&gt;
&lt;td&gt;长&lt;/td&gt;
&lt;td&gt;短&lt;/td&gt;
&lt;td&gt;短&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;原生语言&lt;/td&gt;
&lt;td&gt;JAVA&lt;/td&gt;
&lt;td&gt;Go&lt;/td&gt;
&lt;td&gt;Go&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;算法&lt;/td&gt;
&lt;td&gt;Paxos&lt;/td&gt;
&lt;td&gt;Raft&lt;/td&gt;
&lt;td&gt;Raft&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;多数据中心&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;健康检查&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;web管理界面&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;http协议&lt;/td&gt;
&lt;td&gt;较为复杂&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DNS协议&lt;/td&gt;
&lt;td&gt;较为复杂&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;支持&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;对比的情况下，我们在k8s的集群中可以使用etcd做服务发现，但是在常规情况下，consul更加的全面直接的做服务发现，比如自带服务发现功能，支持多数据中心，直接界面友好等。&lt;/p&gt;

&lt;h2 id=&#34;高可用&#34;&gt;高可用&lt;/h2&gt;

&lt;p&gt;在多注册中心（server）的情况下，单个server在接收到服务的注册/更新信息的时候，它还会将这些信息同步给同样为server的peer(replicate to peer)，为了避免广播风暴，这些信息只会传递一次，也就是说，接收到的server，不会再同步给自身的peer。&lt;/p&gt;

&lt;p&gt;服务注册完成之后，当client需要进行服务调用的时候，就可以向server获取当前的服务列表，再根据服务列表中的ip地址以及端口号进行调用了。&lt;/p&gt;

&lt;p&gt;Consul是目前较为流行的一个服务发现以及配置工具，Consul能够承担包括服务注册与发现、健康检查（health check)以及键值对存储等，同时，Consul还支持多个数据中心。&lt;/p&gt;

&lt;p&gt;我们可以通过Consul的Restful Api（&lt;code&gt;curl -request PUT http://consul/v1//agent/service/register&lt;/code&gt;） 向Consul Agent注册服务信息，提交服务的端口号，ip地址，以及健康检查的方式。随后，这个Client就按照配置中的周期以及方式执行健康检查，当健康检查失败的时候，就会像Server Agent发送服务不可用的消息，这个服务就会被Consul标记为不可用了。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- DesignPatterns</title>
          <link>https://kingjcy.github.io/post/golang/designpatterns/</link>
          <pubDate>Sat, 28 Jan 2017 16:33:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/designpatterns/</guid>
          <description>&lt;p&gt;设计模式其实和语言关系不大，但是在项目工程的设计中有着很大的作用，这边使用golang实现相关的设计模式，也算是对过去看过用过的设计模式的回顾和总结。&lt;/p&gt;

&lt;h1 id=&#34;设计模式的六大原则&#34;&gt;设计模式的六大原则&lt;/h1&gt;

&lt;p&gt;1、开闭原则（Open Close Principle）&lt;/p&gt;

&lt;p&gt;开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。 所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。&lt;/p&gt;

&lt;p&gt;2、里氏代换原则（Liskov Substitution Principle）&lt;/p&gt;

&lt;p&gt;里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何 基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受 到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。 实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽 象化的具体步骤的规范。&lt;/p&gt;

&lt;p&gt;3、依赖倒转原则（Dependence Inversion Principle）&lt;/p&gt;

&lt;p&gt;这个是开闭原则的基础，具体内容：针对接口编程，依赖于抽象而不依赖于具体。&lt;/p&gt;

&lt;p&gt;4、接口隔离原则（Interface Segregation Principle）&lt;/p&gt;

&lt;p&gt;这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。还是一个降低类之间的耦合度的意思，从这儿我们看出， 其实设计模式就是一个软件的设计思想，从大型软件架构出发，为了升级和维护方便。所以上文中多次出现：降低依赖，降低耦合。&lt;/p&gt;

&lt;p&gt;5、迪米特法则（最少知道原则）（Demeter Principle）&lt;/p&gt;

&lt;p&gt;为什么叫最少知道原则，就是说：一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。&lt;/p&gt;

&lt;p&gt;类之间耦合越松，越有利于复用，一个处于弱耦合的类被修改，不会对有关系的类造成波及。&lt;/p&gt;

&lt;p&gt;6、合成复用原则（Composite Reuse Principle）&lt;/p&gt;

&lt;p&gt;原则是尽量使用合成/聚合的方式，而不是使用继承。&lt;/p&gt;

&lt;p&gt;合成是强拥有关系，体现了部分和整体的关系。&lt;/p&gt;

&lt;p&gt;聚合是弱拥有关系，体现了个体和群体的关系。&lt;/p&gt;

&lt;p&gt;优先使用合成／聚合原则有利于后面的类封装，使得类和继承保持较小规模。&lt;/p&gt;

&lt;p&gt;上面的六大规则在我们编程设计过程中整体上体现：解耦，抽象，封装，可复用可扩展。&lt;/p&gt;

&lt;h1 id=&#34;设计模式&#34;&gt;设计模式&lt;/h1&gt;

&lt;p&gt;正常23种设计模式，下面分的更加详细。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;创建型模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;抽象工厂模式：提供一个接口用于创建相关对象的家族；&lt;/li&gt;
&lt;li&gt;Builder模式：使用简单的对象来构建复杂的对象；&lt;/li&gt;
&lt;li&gt;工厂方法模式：一个创建产品对象的工厂接口，将实际创建工作推迟到子类当中；&lt;/li&gt;
&lt;li&gt;对象池模式：实例化并维护一组相同类型的对象实例；&lt;/li&gt;
&lt;li&gt;单例模式：限制类的实例，保证一个类只有一个实例。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;结构模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;适配器模式：适配另一个不兼容的接口来一起工作；&lt;/li&gt;
&lt;li&gt;桥接模式：将抽象化(Abstraction)与实现化(Implementation)脱耦，使得二者可以独立地变化；&lt;/li&gt;
&lt;li&gt;合成模式：将对象组织到树中，用来描述树的关系；&lt;/li&gt;
&lt;li&gt;装饰模式：给一个静态或动态对象添加行为；&lt;/li&gt;
&lt;li&gt;门面（Facade）模式：为子系统中的各类（或结构与方法）提供一个简明一致的界面，隐藏子系统的复杂性，使子系统更加容易使用；&lt;/li&gt;
&lt;li&gt;Flyweight模式：运用共享技术有效地支持大量细粒度的对象；&lt;/li&gt;
&lt;li&gt;MVC模式：是模型(model)－视图(view)－控制器(controller)的缩写，将一个应用程序划分成三个相互关联的部分，用一种业务逻辑、数据、界面显示分离的方法组织代码，将业务逻辑聚集到一个部件里，在改进和个性化定制界面及用户交互的同时，不需要重新编写业务逻辑。&lt;/li&gt;
&lt;li&gt;代理模式：为其他对象提供一种代理以控制对这个对象的访问。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;行为模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;责任链模式：使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系；&lt;/li&gt;
&lt;li&gt;命令模式：就是客户端发布一个命令（也就是“请求”），而这个命令已经被封装成一个对象。即这个命令对象的内部可能已经指定了该命令具体由谁负责执行；&lt;/li&gt;
&lt;li&gt;中介（Mediator）模式：用一个中介对象来封装一系列关于对象交互行为；&lt;/li&gt;
&lt;li&gt;观察者模式：对象间的一种一对多的依赖关系，以便一个对象的状态发生变化时，所有依赖于它的对象都得到通知并自动刷新；&lt;/li&gt;
&lt;li&gt;注册（Registry）模式：跟踪给定类的所有子类；&lt;/li&gt;
&lt;li&gt;状态模式：基于一个对象的内部状态，给相同对象提供多种行为；&lt;/li&gt;
&lt;li&gt;策略模式：定义一系列算法，并将每一个算法封装起来，而且使它们可以相互替换；&lt;/li&gt;
&lt;li&gt;模板（Template）模式：定义一个操作中算法的框架，而将一些步骤延迟到子类中。模板方法模式使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤；&lt;/li&gt;
&lt;li&gt;访问者模式：表示一个作用于某对象结构中的各元素的操作，它使开发者可以在不改变各元素类的前提下定义作用于这些元素的新操作。&lt;/li&gt;
&lt;li&gt;同步模式&lt;/li&gt;
&lt;li&gt;条件变量：利用线程间共享的全局变量进行同步的一种机制，主要包括两个动作：一个线程等待”条件变量的条件成立”而挂起；另一个线程使”条件成立”（给出条件成立信号）；&lt;/li&gt;
&lt;li&gt;Lock/Mutex：执行互斥限制资源获得独占访问；&lt;/li&gt;
&lt;li&gt;监视器模式：互斥锁和条件变量的组合模式；&lt;/li&gt;
&lt;li&gt;读写锁定模式：它把对共享资源的访问者划分成读者和写者，读者只对共享资源进行读访问，写者则需要对共享资源进行写操作；&lt;/li&gt;
&lt;li&gt;Semaphore：负责协调各个线程，以保证它们能够正确、合理地使用公共资源。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;并行模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Bounded Parallelism：完成大量资源限制的独立任务；&lt;/li&gt;
&lt;li&gt;广播（Broadcast）：把一个消息同时传输到所有接收端；&lt;/li&gt;
&lt;li&gt;协同（Coroutines）：允许在特定地方暂停和继续执行的子程序；&lt;/li&gt;
&lt;li&gt;生成器：一次性生成一系列值；&lt;/li&gt;
&lt;li&gt;Reactor模式：在事件驱动的应用中，将一个或多个客户的服务请求分离（demultiplex）和调度（dispatch）给应用程序。同步、有序地处理同时接收的多个服务请求。&lt;/li&gt;
&lt;li&gt;并行（Parallelism）：完成大量的独立任务；&lt;/li&gt;
&lt;li&gt;生产者消费者：从任务执行中分离任务；&lt;/li&gt;
&lt;li&gt;调度器（Scheduler）：协调任务步骤。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;消息传递模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;扇入（Fan-In）：该模块直接调用上级模块的个数，像漏斗型一样去工作；&lt;/li&gt;
&lt;li&gt;扇出（Fan-Out）：该模块直接调用的下级模块的个数；&lt;/li&gt;
&lt;li&gt;Futures &amp;amp; Promises：扮演一个占位角色，对未知的结果用于同步；&lt;/li&gt;
&lt;li&gt;Publish/Subscribe：将信息传递给订阅者；&lt;/li&gt;
&lt;li&gt;Push &amp;amp; Pull：把一个管道上的消息分发给多人。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;稳定模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Bulkheads：实施故障遏制原则，例如防止级联故障；&lt;/li&gt;
&lt;li&gt;断路器（Circuit-Breaker）模式：当请求有可能失败时，停止流动的请求；&lt;/li&gt;
&lt;li&gt;截止日期（Deadline）：一旦响应变缓，允许客户端停止一个正在等待的响应；&lt;/li&gt;
&lt;li&gt;Fail-Fast机制：集合的一种错误检测机制。当多个线程对集合进行结构上的改变操作时，有可能会产生fail-fast机制；&lt;/li&gt;
&lt;li&gt;Handshaking：如果一个组件的不能访问请求被拒绝，询问是否还能承担更多负载；&lt;/li&gt;
&lt;li&gt;稳定状态（Steady-State）：为每一个服务积累一个资源，其它服务必须回收这些资源；&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;剖析模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Timing Functions：包装和执行日志的函数；&lt;/li&gt;
&lt;li&gt;Functional Options：允许给默认值创建clean API和惯用重载；&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;反模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;级联故障：一个系统的某部分出现错误，与之有关的上下级也随之出现故障，导致多米诺效应。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;具体讲解&#34;&gt;具体讲解&lt;/h1&gt;

&lt;p&gt;具体讲解一下重要的设计模式&lt;/p&gt;

&lt;h2 id=&#34;策略模式-strategy&#34;&gt;策略模式(Strategy)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;策略模式： 将算法或操作抽象成实现共同接口、可以被替换的类，实现逻辑和具体算法的解耦。

将各种行为抽象成算法，封装算法为对象；
算法实现共同接口，调用者调用时不考虑算法具体实现，调用接口方法即可；
调用者可随时替换此算法对象；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;多个方法择一使用，且他们会被随时替换；
方法没有共性，使用继承会有大量重写，使用接口会有大量重复使用；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;两个算法： 冒泡排序和快速排序；

抽象冒泡排序和快速排序为算法对象，实现算法接口，拥有 used() 被使用方法；
计算器计算时不用理会是什么算法，调用 used() 即可；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;观察者模式-observer&#34;&gt;观察者模式(Observer)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;观察者模式：主题主动向观察者推送变化，解决观察者对主题对象的依赖。

观察者实现被通知接口，并在主题上注册，主题只保存观察者的引用，不关心观察者的实现；
在主题有变化时调用观察者的通知接口来通知已注册的观察者；
通知方式有推（主题变化时将变化数据推送给观察者）和拉（主题只告知变化，观察者主动来拉取数据）；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;一个主题，多个观察者，主题的任何变动，观察者都要第一时刻得到；
观察者获取主题变化困难，定时不及时，轮询消耗大；
观察者可以随时停止关注某主题；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;张三和李四是记者，他们需要及时了解城市发生的新闻；
张三和李四在电视台注册了自己的信息；
城市发生了新闻，电视台遍历注册信息，通知了张三和李四；
李四退休了，在电视台注销了自己的信息；
城市又发生了新闻，电视台只通知了张三；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;装饰者模式-decorator&#34;&gt;装饰者模式(Decorator)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;装饰者模式：包装一个对象，在被装饰对象的基础上添加功能；

装饰者与被装饰对象拥有同一个超类，装饰者拥有被装饰对象的所有外部接口，可被调用，外界无法感知调用的是装饰者还是被装饰者；
装饰者需要被装饰者作为参数传入，并在装饰者内部，在被装饰者实现的基础上添加或修改某些功能后，提供同被装饰者一样的接口；
装饰者也可被另一个装饰者装饰，即嵌套装饰；
装饰者是一群包装类，由于装饰的复杂性，会多出很多个装饰者小类；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;对象需要动态地添加和修改功能；
功能改变后不影响原对象的使用；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;在商店内，花作为被装饰者对象、红丝带和盒子作为花的装饰者；
花、红丝带、盒子有共同的超类“商品”，他们都能被卖掉；
我们可以在红丝带装饰过花后，再用盒子再包装一次；
包装后的花，顾客买时也不会受到任何影响；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;工厂模式-factory&#34;&gt;工厂模式(Factory)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;工厂模式： 顾名思义，工厂模式是对象的生产器，解耦用户对具体对象的依赖。

实现依赖倒置，让用户通过一个产品工厂依赖产品的抽象，而不是一个具体的产品；
简单工厂模式：接收参数并根据参数创建对应类，将对象的实例化和具体使用解耦；
抽象工厂模式：将工厂抽象出多个生产接口，不同类型的工厂调用生产接口时，生产不同类型的对象；
简单工厂常配合抽象工厂一起使用；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;根据不同条件需求不同的对象；
对象实例化的代码经常需要修改；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;简单工厂：向鞋厂内传入不同的类型（布制），鞋厂会生产出不同类型的鞋子（布鞋）；
抽象工厂：有两座鞋厂：李宁鞋厂、Adidas鞋厂，他们能生产对应各自品牌的鞋子；
搭配使用：向不同的抽象工厂（李宁）传入不同的类型（运动类型），会生产出对应品牌对应类型的鞋子（李宁运动鞋）；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;单例模式-singleton&#34;&gt;单例模式(Singleton)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;单例模式：保证同一个类全局只有一个实例对象;

在第一次实例化后会使用静态变量保存实例，后续全局使用此静态变量；
一般将构造方法私有化，构造方法添加 final 关键字无法被重写，添加一个类静态方法用于返回此实例；
在多线程时应该考虑并发问题，防止两次调用都被判定为实例未初始化而重复初始化对象；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;全局共享同一个实例对象（数据库连接等）；
某一处对此对象的更新全局可见；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;利用 Go 中包的可见性规则来隐藏对象的实例化权限；
使用包变量保存实例对象，获取实例时判断是否已实例化，如为nil，实例化对象并返回，如有值，直接返回值；
待用锁实现 Go routine 并发时的问题；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;命令模式-command&#34;&gt;命令模式(Command)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;命令模式:将一个命令封装成对象，解耦命令的发起者和执行者。

命令对象实现命令接口（excute[、undo]），命令发起者实例化命令对象，并传递此对象，并不关心此对象由谁执行；
命令执行者只负责调用命令对象的执行方法即可，不关心对象是由谁生成的；
与策略模式不同之处：策略模式是通过不同的算法做同一件事情（例如排序），而命令模式则是通过不同的命令做不同的事情；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;命令发起者与执行者无法直接接触；
命令需要撤销功能，却不易保存命令执行状态信息时；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;指挥官创建了一个“从树下跑到草地上”的命令；
命令被分配给张三执行，张三作为军人，接到命令后不管命令的具体内容，而是直接调用命令的执行接口执行；
指挥官发布了撤销指令，张三又从草地上跑到了树下；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;适配器模式-adapter&#34;&gt;适配器模式(Adapter)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;适配器模式：包装对象提供一个接口，以适配调用者。

适配器通过一个中间对象，封装目标接口以适应调用者调用；
调用者调用此适配器，以达到调用目标接口的目的；
适配器模式与装饰者模式的不同之处：适配器模式不改变接口的功能，而装饰者会添加或修改原接口功能；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;提供的接口与调用者调用的其他的接口都不一致；
为一个特殊接口修改调用者的调用方式得不偿失；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;张三是个正常人，他能通过说话直接地表达自己；
李四是个聋哑人，他没法直接表达自己，但他会写字；
笔记本作为一个适配器，用笔记本“包装”了李四之后，当李四需要表达自己想法时，调用笔记本的“表达”功能，笔记本再调用李四“写字”的方法；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;外观模式-facade&#34;&gt;外观模式(Facade)&lt;/h2&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;外观模式：通过封装多个复杂的接口来提供一个简化接口来实现一个复杂功能。

外观模式是通过封装多个接口来将接口简单化；
外观模式不会改变原有的多个复杂的单一接口，这些接口依然能被单独调用，只是提供了一个额外的接口；
外观模式与适配器模式的不同之处：外观模式是整合多个接口并添加一个简化接口，适配器是适配一个接口；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;场景&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;实现某一功能需要调用多个复杂接口；
经常需要实现此功能；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;正常的冲咖啡步骤是：磨咖啡豆、烧开水、倒开水搅拌咖啡。
我们经常需要直接冲咖啡，而不是使用单一步骤，每次喝咖啡时调用三个方法很麻烦；
封装三个接口，额外提供一个 “冲咖啡” 的方法，需要喝咖啡时只需要调用一次冲咖啡方法即可；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;小结&#34;&gt;小结&lt;/h2&gt;

&lt;p&gt;《Head First 设计模式》这书真心不错，例子很轻松，给人很多时间和空间来思考，同时介绍模式时使用结合故事，层层深入的方法，让人印象很深刻，推荐。&lt;/p&gt;

&lt;p&gt;书中详细介绍了 14 个基础设计模式，还有 9 个简化版，就自己查资料结合自己的理解来总结了。&lt;/p&gt;

&lt;h1 id=&#34;实践&#34;&gt;实践&lt;/h1&gt;

&lt;h2 id=&#34;工厂模式-解耦和面向对象&#34;&gt;工厂模式&amp;ndash;解耦和面向对象&lt;/h2&gt;

&lt;p&gt;WIKI:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In class-based programming, the factory method pattern is a creational pattern that uses factory methods to deal with the problem of creating objects without having to specify the exact class of the object that will be created.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;百度百科：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;工厂模式是我们最常用的实例化对象模式了，是用工厂方法代替new操作的一种模式。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单工厂模式是通过传递不同的参数生成不同的实例，缺点就是扩展不同的类别时需要修改代码。&lt;/p&gt;

&lt;p&gt;工厂方法模式为每一个product提供一个工程类，通过不同工厂创建不同实例。&lt;/p&gt;

&lt;p&gt;实现实例&lt;/p&gt;

&lt;p&gt;1.首先，我们定义一个计算的接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package calc

type CalcSuper interface {
    SetData(data ...interface{})
    CalcOperate() float64
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.接下来，我们实现这个类的两个子类，分别是加法和减法&lt;/p&gt;

&lt;p&gt;加法，就是用两个数来相加&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package calc

import &amp;quot;fmt&amp;quot;

type Add struct {
    Num1 float64
    Num2 float64
}

func NewAdd() *Add {
    instance := new(Add)
    return instance
}

func (a *Add) SetData(data ...interface{}) {
    if len(data) != 2 {
        fmt.Println(&amp;quot;error,Need two parameters &amp;quot;)
        return
    }
    if _, ok := data[0].(float64); !ok {
        fmt.Println(&amp;quot;error,Need float64 parameters &amp;quot;)
        return
    }
    if _, ok := data[1].(float64); !ok {
        fmt.Println(&amp;quot;error,Need float64 parameters &amp;quot;)
        return
    }
    a.Num1 = data[0].(float64)
    a.Num2 = data[1].(float64)
}

func (a Add) CalcOperate() float64 {
    return a.Num1 + a.Num2
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;减法，就是把两个数相减，我感觉我好冷。。。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package calc

import &amp;quot;fmt&amp;quot;

type Subtraction struct {
    Num1 float64
    Num2 float64
}

func NewSubtraction() *Subtraction {
    instance := new(Subtraction)
    return instance
}

func (a *Subtraction) SetData(data ...interface{}) {
    if len(data) != 2 {
        fmt.Println(&amp;quot;error,Need two parameters &amp;quot;)
        return
    }
    if _, ok := data[0].(float64); !ok {
        fmt.Println(&amp;quot;error,Need float64 parameters &amp;quot;)
        return
    }
    if _, ok := data[1].(float64); !ok {
        fmt.Println(&amp;quot;error,Need float64 parameters &amp;quot;)
        return
    }
    a.Num1 = data[0].(float64)
    a.Num2 = data[1].(float64)
}

func (a Subtraction) CalcOperate() float64 {
    return a.Num1 - a.Num2
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.下面到了大功告成的时候了，定义简易工厂，来实例化这两个类&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package calc

type CalcFactory struct {
}

func NewCalcFactory() *CalcFactory {
    instance := new(CalcFactory)
    return instance
}

func (f CalcFactory) CreateOperate(opType string) CalcSuper {
    var op CalcSuper
    switch opType {
    case &amp;quot;+&amp;quot;:
        op = NewAdd()
    case &amp;quot;-&amp;quot;:
        op = NewSubtraction()
    default:
        panic(&amp;quot;error ! dont has this operate&amp;quot;)
    }
    return op
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个简易工厂，我们只传入相应的运算方式，如“＋”，“－”，用来创建相关的运算策略。它会返回一个运算接口的实例，当我们得到这个实例，就能调用里面的方法进行运算了。&lt;/p&gt;

&lt;p&gt;4.测试&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 简易工厂模式 project main.go
package main

import (
    . &amp;quot;calc&amp;quot;
    &amp;quot;fmt&amp;quot;
)

func main() {
    factory := NewCalcFactory()

    op := factory.CreateOperate(&amp;quot;+&amp;quot;)
    op.SetData(1.0, 2.0)
    fmt.Println(op.CalcOperate())

    op = factory.CreateOperate(&amp;quot;-&amp;quot;)
    op.SetData(1.0, 2.0)
    fmt.Println(op.CalcOperate())
    /*
        输出：3
        -1
    */
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;策略模式strategy-定义一系列算法-将每一个算法封装起来-并让它们可以相互替换-策略模式让算法独立于使用它的客户而变化&#34;&gt;策略模式Strategy：定义一系列算法，将每一个算法封装起来，并让它们可以相互替换。策略模式让算法独立于使用它的客户而变化。&lt;/h2&gt;

&lt;p&gt;将多种算法进行封装，只暴露给外界固定数目的通用算法接口，这样就可以在后续的工作中在算法内部进行维护和扩展，避免更改用户端的代码实现，从而减小代码维护的成本，降低模块间的耦合程度。&lt;/p&gt;

&lt;p&gt;代码实现&lt;/p&gt;

&lt;p&gt;下面我们就开始以代码的形式来展示一下策略模式吧，代码很简单，我们用一个加减乘除法来模拟。&lt;/p&gt;

&lt;p&gt;首先，我们看到的将会是策略接口和一系列的策略，这些策略不要依赖高层模块的实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package strategy
/**
 * 策略接口
 */
type Strategier interface {
    Compute(num1, num2 int) int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很简单的一个接口，定义了一个方法Compute，接受两个参数，返回一个int类型的值，很容易理解，我们要实现的策略将会将两个参数的计算值返回。
接下来，我们来看一个我们实现的策略，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package strategy
import &amp;quot;fmt&amp;quot;

type Division struct {}

func (p Division) Compute(num1, num2 int) int {
    defer func() {
        if f := recover(); f != nil {
            fmt.Println(f)
            return
        }
    }()

    if num2 == 0 {
        panic(&amp;quot;num2 must not be 0!&amp;quot;)
    }

    return num1 / num2
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为什么要拿除法作为代表呢？因为除法特殊嘛，被除数不能为0，其他的加减乘基本都是一行代码搞定，除法我们需要判断被除数是否为0，如果是0则直接抛出异常。
ok，基本的策略定义好了，我们还需要一个工厂方法，根据不用的type来返回不同的策略，这个type我们准备从命令好输入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewStrategy(t string) (res Strategier) {
    switch t {
        case &amp;quot;s&amp;quot;: // 减法
            res = Subtraction{}
        case &amp;quot;m&amp;quot;: // 乘法
            res = Multiplication{}
        case &amp;quot;d&amp;quot;: // 除法
            res = Division{}
        case &amp;quot;a&amp;quot;: // 加法
            fallthrough
        default:
            res = Addition{}
    }

    return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个工厂方法会根据不用的类型来返回不同的策略实现，当然，哪天我们需要新增新的策略，我们只需要在这个函数中增加对应的类型判断就ok。&lt;/p&gt;

&lt;p&gt;现在策略貌似已经完成了，接下来我们来看看主流程代码，一个Computer，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package compute

import (
    &amp;quot;fmt&amp;quot;
    s &amp;quot;../strategy&amp;quot;
)

type Computer struct {
    Num1, Num2 int
    strate s.Strategier
}

func (p *Computer) SetStrategy(strate s.Strategier) {
    p.strate = strate
}

func (p Computer) Do() int {
    defer func() {
        if f := recover(); f != nil {
            fmt.Println(f)
        }
    }()

    if p.strate == nil {
        panic(&amp;quot;Strategier is null&amp;quot;)
    }

    return p.strate.Compute(p.Num1, p.Num2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个Computer中有三个参数，Num1和Num2当然是我们要操作的数了，strate是我们要设置的策略，可能是上面介绍的Division，也有可能是其他的，在main函数中我们会调用SetStrategy方法来设置要使用的策略，Do方法会执行运算，最后返回运算的结果，可以看到在Do中我们将计算的功能委托给了Strategier。&lt;/p&gt;

&lt;p&gt;貌似一切准备就绪，我们就来编写main的代码吧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;flag&amp;quot;
    c &amp;quot;./computer&amp;quot;
    s &amp;quot;./strategy&amp;quot;
)

var stra *string = flag.String(&amp;quot;type&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;input the strategy&amp;quot;)
var num1 *int = flag.Int(&amp;quot;num1&amp;quot;, 1, &amp;quot;input num1&amp;quot;)
var num2 *int = flag.Int(&amp;quot;num2&amp;quot;, 1, &amp;quot;input num2&amp;quot;)

func init() {
    flag.Parse()
}

func main() {
    com := c.Computer{Num1: *num1, Num2: *num2}
    strate := s.NewStrategy(*stra)

    com.SetStrategy(strate)
    fmt.Println(com.Do())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先我们要从命令行读取要使用的策略类型和两个操作数，在main函数中，我们初始化Computer这个结构体，并将输入的操作数赋值给Computer的Num1和Num2，接下来我们根据策略类型通过调用NewStrategy函数来获取一个策略，并调用Computer的SetStrategy方法给Computer设置上面获取到的策略，最后执行Do方法计算结果，最后打印。&lt;/p&gt;

&lt;p&gt;感觉策略就是对工厂的上一层封装，只保留对外的一个基本接口和数据结构&lt;/p&gt;

&lt;h2 id=&#34;单一职责&#34;&gt;单一职责&lt;/h2&gt;

&lt;p&gt;单例模式：保证一个类仅有一个实例，并提供一个访问它的全局访问点&lt;/p&gt;

&lt;p&gt;这个解释足够简单。说白了就是假如我们希望我们在我们的系统中该类仅仅存在1个或0个该类的实例。虽然单例模式很简单，但是熟悉java的同学可能了解，单例模式有很多写法,懒汉式、饿汉式、双重锁。。。 这么多形式，难道有什么目的？确实，不过他们的目的很明确，就是保证在一种特殊情况下的单例-并发。&lt;/p&gt;

&lt;p&gt;ok，既然了解了单例模式，那下面我们就开始用代码描述一下单例模式。首先是最简单的单例，这里我们并不去考虑并发的情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package manager
import (
    &amp;quot;fmt&amp;quot;
)

var m *Manager

func GetInstance() *Manager {
    if m == nil {
        m = &amp;amp;Manager {}
    }
    return m
}

type Manager struct {}

func (p Manager) Manage() {
    fmt.Println(&amp;quot;manage...&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是一个最简单的单例了，对于Manager结构体，我们提供了一个GetInstance函数去获取它的实例，这个函数中首先去判断m变量是否为空，如果为空才去赋值一个Manager的指针类型的值，一个小小的判断，就保证了我们在第第二次调用GetInstance的时候直接返回m，而不是重新获取Manager的实例，进而保证了唯一实例。&lt;/p&gt;

&lt;p&gt;上面的代码确实简单，也实现了最简单的单例模式，不过大家有没有考虑到并发这一点，在并发的情况下，这里是不是还可以正常工作呢？ 来，先跟着下面的思路走一走，来看看问题出现在哪。&lt;/p&gt;

&lt;p&gt;现在我们是在并发的情况下去调用的 GetInstance函数，现在恰好第一个goroutine执行到m = &amp;amp;Manager {}这句话之前，第二个goroutine也来获取实例了，第二个goroutine去判断m是不是nil,因为m = &amp;amp;Manager{}还没有来得及执行，所以m肯定是nil，现在出现的问题就是if中的语句可能会执行两遍！&lt;/p&gt;

&lt;p&gt;在上面介绍的这种情形中，因为m = &amp;amp;Manager{}可能会执行多次，所以我们写的单例失效了，这个时候我们就该考虑为我们的单例加锁啦。&lt;/p&gt;

&lt;p&gt;这个时候我们就需要引入go的锁机制-sync.Mutex了,修改我们的代码，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package manager
import (
    &amp;quot;sync&amp;quot;
    &amp;quot;fmt&amp;quot;
)

var m *Manager
var lock *sync.Mutex = &amp;amp;sync.Mutex {}

func GetInstance() *Manager {
    lock.Lock()
    defer lock.Unlock()
    if m == nil {
        m = &amp;amp;Manager {}
    }
    return m
}

type Manager struct {}

func (p Manager) Manage() {
    fmt.Println(&amp;quot;manage...&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码做了简单的修改了，引入了锁的机制，在GetInstance函数中，每次调用我们都会上一把锁，保证只有一个goroutine执行它，这个时候并发的问题就解决了。不过现在不管什么情况下都会上一把锁，而且加锁的代价是很大的，有没有办法继续对我们的代码进行进一步的优化呢？ 熟悉java的同学可能早就想到了双重的概念，没错，在go中我们也可以使用双重锁机制来提高效率。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package manager
import (
    &amp;quot;sync&amp;quot;
    &amp;quot;fmt&amp;quot;
)

var m *Manager
var lock *sync.Mutex = &amp;amp;sync.Mutex {}

func GetInstance() *Manager {
    if m == nil {
        lock.Lock()
        defer lock.Unlock()
        if m == nil {
            m = &amp;amp;Manager {}
        }
    }

    return m
}

type Manager struct {}

func (p Manager) Manage() {
    fmt.Println(&amp;quot;manage...&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码只是稍作修改而已，不过我们用了两个判断，而且我们将同步锁放在了条件判断之后，这样做就避免了每次调用都加锁，提高了代码的执行效率。&lt;/p&gt;

&lt;p&gt;这获取就是很完美的单例代码了，不过还没完，在go中我们还有更优雅的方式去实现。单例的目的是啥？保证实例化的代码只执行一次，在go中就中这么一种机制来保证代码只执行一次，而且不需要我们手工去加锁解锁。对，就是我们的sync.Once，它有一个Do方法，在它中的函数go会只保证仅仅调用一次！再次修改我们的代码，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package manager
import (
    &amp;quot;sync&amp;quot;
    &amp;quot;fmt&amp;quot;
)

var m *Manager
var once sync.Once

func GetInstance() *Manager {
    once.Do(func() {
        m = &amp;amp;Manager {}
    })
    return m
}

type Manager struct {}

func (p Manager) Manage() {
    fmt.Println(&amp;quot;manage...&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码更简单了，而且有没有发现-漂亮了！Once.Do方法的参数是一个函数，这里我们给的是一个匿名函数，在这个函数中我们做的工作很简单，就是去赋值m变量，而且go能保证这个函数中的代码仅仅执行一次！&lt;/p&gt;

&lt;p&gt;ok，到现在单例模式我们就介绍完了，内容并不多，因为单例模式太简单而且太常见了。我们用单例的目的是为了保证在整个系统中存在唯一的实例，我们加锁的目的是为了在并发的环境中单例依旧好用。不过虽然单例简单，我们还是不能任性的用，因为这样做实例会一直存在内存中，一些我们用的不是那么频繁的东西使用了单例是不是就造成了内存的浪费？大家在用单例的时候还是要多思考思考，这个模块适不适合用单例！&lt;/p&gt;

&lt;h2 id=&#34;开放封闭原则-对于扩展开放-对于修改封闭-尽量不修改-新增-在设计发生变化的时候-就要考虑抽象来应对未来的变化&#34;&gt;开放封闭原则：对于扩展开放，对于修改封闭，尽量不修改，新增，在设计发生变化的时候，就要考虑抽象来应对未来的变化&lt;/h2&gt;

&lt;p&gt;实现开放封闭的核心思想就是对抽象编程，而不对具体编程，因为抽象相对稳定。让类依赖于固定的抽象，所以对修改就是封闭的；而通过面向对象的继承和多态机制，可以实现对抽象体的继承，通过覆写其方法来改变固有行为，实现新的扩展方法，所以对于扩展就是开放的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  对于违反这一原则的类，必须通过重构来进行改善。常用于实现的设计模式主要有Template Method模式和Strategy 模式。而封装变化，是实现这一原则的重要手段，将经常变化的状态封装为一个类。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以银行业务员为例&lt;/p&gt;

&lt;p&gt;没有实现OCP的设计：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class BankProcess

    {  //存款 

       public void Deposite(){}

        //取款

        public void Withdraw(){ }

        //转账

        public void Transfer(){}

    }

    public class BankStaff

    {

        private BankProcess bankpro = new BankProcess();

        public void BankHandle(Client client)

        {

            switch (client.Type)

            {  //存款

                case &amp;quot;deposite&amp;quot;:

                    bankpro.Deposite();

                    break;

                    //取款

                case &amp;quot;withdraw&amp;quot;:

                    bankpro.Withdraw();

                    break;

                    //转账

                case &amp;quot;transfer&amp;quot;:

                    bankpro.Transfer();

                    break;

            }

        }

    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种设计显然是存在问题的，目前设计中就只有存款，取款和转账三个功能，将来如果业务增加了，比如增加申购基金功能，理财功能等，就必须要修改BankProcess业务类。我们分析上述设计就不能发现把不能业务封装在一个类里面，违反单一职责原则，而有新的需求发生，必须修改现有代码则违反了开放封闭原则。&lt;/p&gt;

&lt;p&gt;从开放封闭的角度来分析，在银行系统中最可能扩展的就是业务功能的增加或变更。对业务流程应该作为扩展的部分来实现。当有新的功能时，不需要再对现有业务进行重新梳理，然后再对系统做大的修改。&lt;/p&gt;

&lt;p&gt;如何才能实现耦合度和灵活性兼得呢？&lt;/p&gt;

&lt;p&gt;那就是抽象，将业务功能抽象为接口，当业务员依赖于固定的抽象时，对修改就是封闭的，而通过继承和多态继承，从抽象体中扩展出新的实现，就是对扩展的开放。&lt;/p&gt;

&lt;p&gt;以下是符合OCP的设计：&lt;/p&gt;

&lt;p&gt;首先声明一个业务处理接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public  interface IBankProcess{  void Process();}

public class DepositProcess : IBankProcess

    {

        public void Process()

        { //办理存款业务

            Console.WriteLine(&amp;quot;Process Deposit&amp;quot;);

        }

}

public class WithDrawProcess : IBankProcess

    {

        public void Process()

        { //办理取款业务

            Console.WriteLine(&amp;quot;Process WithDraw&amp;quot;);

        }

}

public class TransferProcess : IBankProcess

    {

        public void Process()

        { //办理转账业务

            Console.WriteLine(&amp;quot;Process Transfer&amp;quot;);

        }

    }

public class BankStaff

    {

        private IBankProcess bankpro = null;

        public void BankHandle(Client client)

        {

            switch (client.Type)

            {   //存款

                case &amp;quot;Deposit&amp;quot;:

                    userProc = new DepositUser();

                    break;

                    //转账

                case &amp;quot;Transfer&amp;quot;:

                    userProc = new TransferUser();

                    break;

                    //取款

                case &amp;quot;WithDraw&amp;quot;:

                    userProc = new WithDrawUser();

                    break;

            }

            userProc.Process();

        }

    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样当业务变更时，只需要修改对应的业务实现类就可以，其他不相干的业务就不必修改。当业务增加，只需要增加业务的实现就可以了。&lt;/p&gt;

&lt;p&gt;设计建议：&lt;/p&gt;

&lt;p&gt;开放封闭原则，是最为重要的设计原则，Liskov替换原则和合成/聚合复用原则为开放封闭原则提供保证。&lt;/p&gt;

&lt;p&gt;可以通过Template Method模式和Strategy模式进行重构，实现对修改封闭，对扩展开放的设计思路。&lt;/p&gt;

&lt;p&gt;封装变化，是实现开放封闭原则的重要手段，对于经常发生变化的状态，一般将其封装为一个抽象，例如银行业务中IBankProcess接口。&lt;/p&gt;

&lt;p&gt;拒绝滥用抽象，只将经常变化的部分进行抽象。&lt;/p&gt;

&lt;h2 id=&#34;依赖倒转原则&#34;&gt;依赖倒转原则&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;抽象不应该依赖于细节，细节应该依赖于抽象    
针对接口编程，不要对实现编程
高层不应该依赖于底层模块，两个都应该依赖于抽象
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;核心其实就是针对接口编程&lt;/p&gt;

&lt;h2 id=&#34;装饰模式&#34;&gt;装饰模式&lt;/h2&gt;

&lt;p&gt;装饰模式 （decorator）就是动态的给一个对象添加一些额外的职责，就增加功能来说。装饰模式比生成子类更加的灵活。&lt;/p&gt;

&lt;p&gt;装饰模式使用对象组合的方式动态改变或增加对象行为。&lt;/p&gt;

&lt;p&gt;Go语言借助于匿名组合和非入侵式接口可以很方便实现装饰模式。&lt;/p&gt;

&lt;p&gt;使用匿名组合，在装饰器中不必显式定义转调原对象方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package decorator

import (
    &amp;quot;fmt&amp;quot;
)

type person struct {
    Name string
}

func (p *person) show() {
    if p == nil {
        return
    }
    fmt.Println(&amp;quot;姓名：&amp;quot;, p.Name)
}

type AbsstractPerson interface {
    show()
}
type Decorator struct {
    AbsstractPerson
}

func (d *Decorator) SetDecorator(component AbsstractPerson) {
    if d == nil {
        return
    }
    d.AbsstractPerson = component
}

func (d *Decorator) show() {
    if d == nil {
        return
    }
    if d.AbsstractPerson != nil {
        d.AbsstractPerson.show()
    }
}

type TShirts struct {
    Decorator
}

func (t *TShirts) show() {
    if t == nil {
        return
    }
    t.Decorator.show()
    fmt.Println(&amp;quot;T恤&amp;quot;)
}

type BigTrouser struct {
    Decorator
}

func (b *BigTrouser) show() {
    if b == nil {
        return
    }
    b.Decorator.show()
    fmt.Println(&amp;quot;大裤衩&amp;quot;)
}

type Sneakers struct {
    Decorator
}

func (b *Sneakers) show() {
    if b == nil {
        return
    }
    b.Decorator.show()
    fmt.Println(&amp;quot;破球鞋&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;代理模式&#34;&gt;代理模式&lt;/h2&gt;

&lt;p&gt;Proxy 代理模式：为其他对象提供一种代理，以控制对这个对象的访问。&lt;/p&gt;

&lt;p&gt;代理模式用于延迟处理操作或者在进行实际操作前后进行其它处理。一般都是在结构体外新增一个结构体和这个结构体进行组合，来达到操作或者控制访问。&lt;/p&gt;

&lt;p&gt;代理模式的常见用法有&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;虚代理
COW代理
远程代理
保护代理
Cache 代理
防火墙代理
同步代理
智能指引
等。。。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package proxy

import (
    &amp;quot;fmt&amp;quot;
)

type GiveGift interface {
    giveDolls()
    giveFlowers()
    giveChocolate()
}

type Girl struct {
    name string
}

func (g *Girl) Name() string {
    if g == nil {
        return &amp;quot;&amp;quot;
    }
    return g.name
}

func (g *Girl) SetName(name string) {
    if g == nil {
        return
    }
    g.name = name
}

type Pursuit struct {
    girl Girl
}

func (p *Pursuit) giveDolls() {
    if p == nil {
        return
    }
    fmt.Println(p.girl.name, &amp;quot;送你洋娃娃&amp;quot;)
}

func (p *Pursuit) giveFlowers() {
    if p == nil {
        return
    }
    fmt.Println(p.girl.name, &amp;quot;送你玫瑰花&amp;quot;)
}

func (p *Pursuit) giveChocolate() {
    if p == nil {
        return
    }
    fmt.Println(p.girl.name, &amp;quot;送你巧克力&amp;quot;)
}

type Proxy struct {
    p Pursuit
}

func (p *Proxy) giveDolls() {
    if p == nil {
        return
    }
    p.p.giveDolls()
}

func (p *Proxy) giveFlowers() {
    if p == nil {
        return
    }
    p.p.giveFlowers()
}

func (p *Proxy) giveChocolate() {
    if p == nil {
        return
    }
    p.p.giveChocolate()
}

func NewProxy(mm Girl) *Proxy {
    gg := Pursuit{mm}
    return &amp;amp;Proxy{gg}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;工厂办法模式&#34;&gt;工厂办法模式&lt;/h2&gt;

&lt;p&gt;工厂方法模式使用子类的方式延迟生成对象到子类中实现。&lt;/p&gt;

&lt;p&gt;Go中不存在继承 所以使用匿名组合来实现&lt;/p&gt;

&lt;p&gt;简单工厂模式是通过传递不同的参数生成不同的实例，缺点就是扩展不同的类别时需要修改代码。&lt;/p&gt;

&lt;p&gt;工厂方法模式为每一个product提供一个工程类，通过不同工厂创建不同实例。&lt;/p&gt;

&lt;p&gt;简单工厂和工厂模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;简单工厂定义的是静态函数，
一个函数处理所有的产品创建，工厂模式将创建对象过程抽象为一个类组，
有抽象类，有对应产品的创建类，创建的过程有创建类来完成，
工厂模式主要使用的是依赖反转原则
（高层模块不依赖底层模块，统一依赖抽象层，抽象层不依赖细节层，细节层依赖抽象层），
解决简单工厂的缺少开放-封闭原则
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package factorymethod

//Operator 是被封装的实际类接口
type Operator interface {
    SetA(int)
    SetB(int)
    Result() int
}

//OperatorFactory 是工厂接口
type OperatorFactory interface {
    Create() Operator
}

//OperatorBase 是Operator 接口实现的基类，封装公用方法
type OperatorBase struct {
    a, b int
}

//SetA 设置 A
func (o *OperatorBase) SetA(a int) {
    o.a = a
}

//SetB 设置 B
func (o *OperatorBase) SetB(b int) {
    o.b = b
}

//PlusOperatorFactory 是 PlusOperator 的工厂类
type PlusOperatorFactory struct{}

func (PlusOperatorFactory) Create() Operator {
    return &amp;amp;PlusOperator{
        OperatorBase: &amp;amp;OperatorBase{},
    }
}

//PlusOperator Operator 的实际加法实现
type PlusOperator struct {
    *OperatorBase
}

//Result 获取结果
func (o PlusOperator) Result() int {
    return o.a + o.b
}

//MinusOperatorFactory 是 MinusOperator 的工厂类
type MinusOperatorFactory struct{}

func (MinusOperatorFactory) Create() Operator {
    return &amp;amp;MinusOperator{
        OperatorBase: &amp;amp;OperatorBase{},
    }
}

//MinusOperator Operator 的实际减法实现
type MinusOperator struct {
    *OperatorBase
}

//Result 获取结果
func (o MinusOperator) Result() int {
    return o.a - o.b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;原型模式&#34;&gt;原型模式&lt;/h2&gt;

&lt;p&gt;原型模式使对象能复制自身，并且暴露到接口中，使客户端面向接口编程时，不知道接口实际对象的情况下生成新的对象。&lt;/p&gt;

&lt;p&gt;原型模式配合原型管理器使用，使得客户端在不知道具体类的情况下，通过接口管理器得到新的实例，并且包含部分预设定配置。&lt;/p&gt;

&lt;p&gt;注意浅复制、深复制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package prototype

import (
    &amp;quot;fmt&amp;quot;
)

type Resume struct {
    name     string
    sex      string
    age      string
    timeArea string
    company  string
}

func (r *Resume) setPersonalInfo(name, sex, age string) {
    if r == nil {
        return
    }
    r.name = name
    r.age = age
    r.sex = sex
}

func (r *Resume) setWorkExperience(timeArea, company string) {
    if r == nil {
        return
    }
    r.company = company
    r.timeArea = timeArea
}

func (r *Resume) display() {
    if r == nil {
        return
    }
    fmt.Println(&amp;quot;个人信息：&amp;quot;, r.name, r.sex, r.age)
    fmt.Println(&amp;quot;工作经历：&amp;quot;, r.timeArea, r.company)
}

func (r *Resume) clone() *Resume {
    if r == nil {
        return nil
    }
    new_obj := (*r)
    return &amp;amp;new_obj
}

func NewResume() *Resume {
    return &amp;amp;Resume{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;模版方法模式&#34;&gt;模版方法模式&lt;/h2&gt;

&lt;p&gt;模版方法模式使用继承机制，把通用步骤和通用方法放到父类中，把具体实现延迟到子类中实现。使得实现符合开闭原则。&lt;/p&gt;

&lt;p&gt;如实例代码中通用步骤在父类中实现（准备、下载、保存、收尾）下载和保存的具体实现留到子类中，并且提供 保存方法的默认实现。&lt;/p&gt;

&lt;p&gt;因为Golang不提供继承机制，需要使用匿名组合模拟实现继承。&lt;/p&gt;

&lt;p&gt;此处需要注意：因为父类需要调用子类方法，所以子类需要匿名组合父类的同时，父类需要持有子类的引用。&lt;/p&gt;

&lt;p&gt;Template Methed模板方法：&lt;/p&gt;

&lt;p&gt;定义一个操作中的算法的骨架，而将一些具体步骤延迟到子类中。&lt;/p&gt;

&lt;p&gt;模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。&lt;/p&gt;

&lt;p&gt;与建造者：一个是行为型模式，一个是创建型模式&lt;/p&gt;

&lt;p&gt;模版方法其实就是将不变的行为抽象为一个方法，具体实现在子类中。然后直接子类结构体直接调用这个方法，就可以实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package template

import (
    &amp;quot;fmt&amp;quot;
)

type getfood interface {
    first()
    secend()
    three()
}

type template struct {
    g getfood
}

func (b *template) getsomefood() {
    if b == nil {
        return
    }
    b.g.first()
    b.g.secend()
    b.g.three()
}

type bingA struct {
    template
}

func NewBingA() *bingA {
    b := bingA{}
    return &amp;amp;b
}

func (b *bingA) first() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;打开冰箱&amp;quot;)
}

func (b *bingA) secend() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;拿出东西&amp;quot;)
}

func (b *bingA) three() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;关闭冰箱&amp;quot;)
}

type Guo struct {
    template
}

func NewGuo() *Guo {
    b := Guo{}
    return &amp;amp;b
}

func (b *Guo) first() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;打开锅&amp;quot;)
}

func (b *Guo) secend() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;拿出东西锅&amp;quot;)
}

func (b *Guo) three() {
    if b == nil {
        return
    }
    fmt.Println(&amp;quot;关闭锅&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;迪米特法则&#34;&gt;迪米特法则&lt;/h2&gt;

&lt;p&gt;最少知道原则，就是说：一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。&lt;/p&gt;

&lt;p&gt;类之间耦合越松，越有利于复用，一个处于弱耦合的类被修改，不会对有关系的类造成波及。&lt;/p&gt;

&lt;h2 id=&#34;外观模式&#34;&gt;外观模式&lt;/h2&gt;

&lt;p&gt;API 为facade 模块的外观接口，大部分代码使用此接口简化对facade类的访问。&lt;/p&gt;

&lt;p&gt;facade模块同时暴露了a和b 两个Module 的NewXXX和interface，其它代码如果需要使用细节功能时可以直接调用。&lt;/p&gt;

&lt;p&gt;为子系统中的一组接口提供一个一致的界面，此模式定义了一个高层接口，
        这个接口使得这一子系统更加容易使用（投资：基金，股票，房产）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package facade

import &amp;quot;fmt&amp;quot;

func NewAPI() API {
    return &amp;amp;apiImpl{
        a: NewAModuleAPI(),
        b: NewBModuleAPI(),
    }
}

//API is facade interface of facade package
type API interface {
    Test() string
}

//facade implement
type apiImpl struct {
    a AModuleAPI
    b BModuleAPI
}

func (a *apiImpl) Test() string {
    aRet := a.a.TestA()
    bRet := a.b.TestB()
    return fmt.Sprintf(&amp;quot;%s\n%s&amp;quot;, aRet, bRet)
}

//NewAModuleAPI return new AModuleAPI
func NewAModuleAPI() AModuleAPI {
    return &amp;amp;aModuleImpl{}
}

//AModuleAPI ...
type AModuleAPI interface {
    TestA() string
}

type aModuleImpl struct{}

func (*aModuleImpl) TestA() string {
    return &amp;quot;A module running&amp;quot;
}

//NewBModuleAPI return new BModuleAPI
func NewBModuleAPI() BModuleAPI {
    return &amp;amp;bModuleImpl{}
}

//BModuleAPI ...
type BModuleAPI interface {
    TestB() string
}

type bModuleImpl struct{}

func (*bModuleImpl) TestB() string {
    return &amp;quot;B module running&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;建造者模式&#34;&gt;建造者模式&lt;/h2&gt;

&lt;p&gt;Builder 生成器模式：（建造者模式）将一个复杂对象的构建与它表示分离，使得同样的构建过程可以创建不同的表示&lt;/p&gt;

&lt;p&gt;个人想法：建造者的建造流程是在指挥者中，指挥者在用户通知他现在具体的建造者是谁后，建造出对应的产品，建造者中实现了产品的建造细节&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package builder

import (
    &amp;quot;fmt&amp;quot;
)

type IBuilder interface {
    head()
    body()
    foot()
    hand()
}
type Thin struct {
}

func (t *Thin) head() {
    fmt.Println(&amp;quot;我的头很瘦&amp;quot;)
}

func (t *Thin) body() {
    fmt.Println(&amp;quot;我的身体很瘦&amp;quot;)
}
func (t *Thin) foot() {
    fmt.Println(&amp;quot;我的脚很瘦&amp;quot;)
}
func (t *Thin) hand() {
    fmt.Println(&amp;quot;我的身体手很瘦&amp;quot;)
}

type Fat struct {
}

func (t *Fat) head() {
    fmt.Println(&amp;quot;我的头很胖&amp;quot;)
}

func (t *Fat) body() {
    fmt.Println(&amp;quot;我的身体很胖&amp;quot;)
}
func (t *Fat) foot() {
    fmt.Println(&amp;quot;我的脚很胖&amp;quot;)
}
func (t *Fat) hand() {
    fmt.Println(&amp;quot;我的身体手很胖&amp;quot;)
}

type Director struct {
    person IBuilder
}

func (d *Director) CreatePerson() {
    if d == nil {
        return
    }
    d.person.head()
    d.person.body()
    d.person.foot()
    d.person.hand()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;观察者模式&#34;&gt;观察者模式&lt;/h2&gt;

&lt;p&gt;观察者模式用于触发联动。&lt;/p&gt;

&lt;p&gt;一个对象的改变会触发其它观察者的相关动作，而此对象无需关心连动对象的具体实现。&lt;/p&gt;

&lt;p&gt;Observer 观察者模式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    定义了一种一对多的依赖关系，让多个观察者对象同时监听某一个主题对象。
    这个主题对象在状态发生改变时，会通知所有观察者对象，使它们能够自动更新自己。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个系统分成多个系统互相合作，需要维护多个系统的一致性的时候，，不能为了一致性而使其紧密耦合，这样不利于扩展，重用，维护。而观察者有主题subject和观察者observer，一个subject可以有依赖于
他的任意数目的observers，一旦subject发送改变，所有的observer都可以得到通知。subject不需要关心observers的实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package observer

import (
    &amp;quot;fmt&amp;quot;
)

type Subject interface {
    Notify()
    State() int
    SetState(int)
    AddCallFunc(*update)
    RemoveCallFunc(*update)
}

type update func(int)

type SubjectA struct {
    state int
    call  []*update
}

func (s *SubjectA) Notify() {
    if s == nil {
        return
    }
    for _, c := range s.call {
        (*c)(s.state)
    }
}

func (s *SubjectA) State() int {
    if s == nil {
        return 0
    }
    return s.state
}

func (s *SubjectA) SetState(i int) {
    if s == nil {
        return
    }
    s.state = i
}
func (s *SubjectA) AddCallFunc(f *update) {
    if s == nil {
        return
    }
    for _, c := range s.call {
        if c == f {
            return
        }
    }

    s.call = append(s.call, f)
}

func (s *SubjectA) RemoveCallFunc(f *update) {
    if s == nil {
        return
    }
    for i, c := range s.call {
        if c == f {
            s.call = append(s.call[:i], s.call[i+1:]...)
        }
    }
}

func NewSubjectA(s int) *SubjectA {
    return &amp;amp;SubjectA{s, []*update{}}
}

type Observer interface {
    Update(int)
}

type ObserverA struct {
    s     Subject
    state int
}

func (o *ObserverA) Update(s int) {
    if o == nil {
        return
    }
    fmt.Println(&amp;quot;ObserverA&amp;quot;)
    fmt.Println(s)
    fmt.Println(o)
}
func NewObserverA(sa Subject, s int) *ObserverA {
    return &amp;amp;ObserverA{sa, s}
}

type ObserverB struct {
    s     Subject
    state int
}

func (o *ObserverB) Update(s int) {
    if o == nil {
        return
    }
    fmt.Println(&amp;quot;ObserverB&amp;quot;)
    fmt.Println(s)
    fmt.Println(o)
}
func NewObserverB(sa Subject, s int) *ObserverB {
    return &amp;amp;ObserverB{sa, s}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;抽象工厂模式&#34;&gt;抽象工厂模式&lt;/h2&gt;

&lt;p&gt;Abstract Factory 抽象工厂模式：提供一个创建一系列相关或者相互依赖对象的接口，而无需指定他们具体的类。&lt;/p&gt;

&lt;p&gt;工厂模式和抽象工厂模式：感觉抽象工厂可以叫集团模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    工厂模式下，是一个工厂下，对产品的每一个具体生成分配不同的流水线；
    集团模式：在集团下，有不同的工厂，可以生成不同的产品，每个工厂生产出来的同一个型号产品具体细节是不一样
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数据库（interface）&amp;mdash;-选择哪个数据库，创建对数据库的表进行操作实例&lt;/p&gt;

&lt;p&gt;iuser（interface）&amp;mdash;-不同实例对表的操作&lt;/p&gt;

&lt;p&gt;idepartment（interface）&amp;mdash;&amp;ndash;不同实例对表的操作&lt;/p&gt;

&lt;p&gt;其实就是返回对应接口的实现结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package abstractfactory

import (
    &amp;quot;fmt&amp;quot;
)

type User struct {
    id   int
    name string
}

func (u *User) Id() int {
    if u == nil {
        return -1
    }
    return u.id
}

func (u *User) SetId(id int) {
    if u == nil {
        return
    }
    u.id = id
}

func (u *User) Name() string {
    if u == nil {
        return &amp;quot;&amp;quot;
    }
    return u.name
}

func (u *User) SetName(name string) {
    if u == nil {
        return
    }
    u.name = name
}

type Department struct {
    id   int
    name string
}

func (d *Department) Id() int {
    if d == nil {
        return -1
    }
    return d.id
}
func (d *Department) SetId(id int) {
    if d == nil {
        return
    }
    d.id = id
}
func (d *Department) Name() string {
    if d == nil {
        return &amp;quot;&amp;quot;
    }
    return d.name
}
func (d *Department) SetName(name string) {
    if d == nil {
        return
    }
    d.name = name
}

type IUser interface {
    insert(*User)
    getUser(int) *User
}

type SqlServerUser struct {
}

func (s *SqlServerUser) insert(u *User) {
    if s == nil {
        return
    }
    fmt.Println(&amp;quot;往SqlServer的User表中插入一条User&amp;quot;, u)
}

func (s *SqlServerUser) getUser(id int) (u *User) {
    if s == nil {
        return nil
    }
    u = &amp;amp;User{id, &amp;quot;hclacS&amp;quot;}
    fmt.Println(&amp;quot;从SqlServer的User表中获取一条User&amp;quot;, *u)
    return
}

type AccessUser struct {
}

func (s *AccessUser) insert(u *User) {
    if s == nil {
        return
    }
    fmt.Println(&amp;quot;往AccessUser的User表中插入一条User&amp;quot;, *u)
}

func (s *AccessUser) getUser(id int) (u *User) {
    if s == nil {
        return nil
    }
    u = &amp;amp;User{id, &amp;quot;hclacA&amp;quot;}
    fmt.Println(&amp;quot;从AccessUser的User表中获取一条User&amp;quot;, *u)
    return
}

type IDepartment interface {
    insert(*Department)
    getDepartment(int) *Department
}

type SqlServerDepartment struct {
}

func (s *SqlServerDepartment) insert(d *Department) {
    if s == nil {
        return
    }
    fmt.Println(&amp;quot;往SqlServer的Department表中插入一条Department&amp;quot;, *d)
}

func (s *SqlServerDepartment) getDepartment(id int) (u *Department) {
    if s == nil {
        return nil
    }
    u = &amp;amp;Department{id, &amp;quot;hclacDS&amp;quot;}
    fmt.Println(&amp;quot;从SqlServer的Department表中获取一条Department&amp;quot;, *u)
    return
}

type AccessDepartment struct {
}

func (s *AccessDepartment) insert(u *Department) {
    if s == nil {
        return
    }
    fmt.Println(&amp;quot;往AccessDepartment的Department表中插入一条Department&amp;quot;, *u)
}

func (s *AccessDepartment) getDepartment(id int) (u *Department) {
    if s == nil {
        return nil
    }
    u = &amp;amp;Department{id, &amp;quot;hclacDA&amp;quot;}
    fmt.Println(&amp;quot;从AccessDepartment的Department表中获取一条Department&amp;quot;, *u)
    return
}

type Ifactory interface {
    createUser() IUser
    createDepartment() IDepartment
}

type SqlServerFactory struct {
}

func (s *SqlServerFactory) createUser() IUser {
    if s == nil {
        return nil
    }
    u := &amp;amp;SqlServerUser{}
    return u
}

func (s *SqlServerFactory) createDepartment() IDepartment {
    if s == nil {
        return nil
    }
    u := &amp;amp;SqlServerDepartment{}
    return u
}

type AccessFactory struct {
}

func (s *AccessFactory) createUser() IUser {
    if s == nil {
        return nil
    }
    u := &amp;amp;AccessUser{}
    return u
}

func (s *AccessFactory) createDepartment() IDepartment {
    if s == nil {
        return nil
    }
    u := &amp;amp;AccessDepartment{}
    return u
}

type DataAccess struct {
    db string
}

func (d *DataAccess) createUser(db string) IUser {
    if d == nil {
        return nil
    }

    var u IUser

    if db == &amp;quot;sqlserver&amp;quot; {
        u = new(SqlServerUser)
    } else if db == &amp;quot;access&amp;quot; {
        u = new(AccessUser)
    }
    return u
}

func (d *DataAccess) createDepartment(db string) IDepartment {
    if d == nil {
        return nil
    }

    var u IDepartment

    if db == &amp;quot;sqlserver&amp;quot; {
        u = new(SqlServerDepartment)
    } else if db == &amp;quot;access&amp;quot; {
        u = new(AccessDepartment)
    }
    return u
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;状态模式&#34;&gt;状态模式&lt;/h2&gt;

&lt;p&gt;方法实现过长是坏味道。&lt;/p&gt;

&lt;p&gt;State 状态模式：当一个对象的内在状态改变时，允许改变其行为，这个对象看起来像是改变了其类&lt;/p&gt;

&lt;p&gt;策略模式是用在对多个做同样事情（统一接口）的类对象的选择上，而状态模式是：将对某个事情的处理过程抽象成接口和实现类的形式，由context保存一份state，在state实现类处理事情时，修改状态传递给context，由context继续传递到下一个状态处理中。&lt;/p&gt;

&lt;p&gt;状态模式用于分离状态和行为。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package state

import (
    &amp;quot;fmt&amp;quot;
)

// 工作类 --context
type Work struct {
    hour  int
    state State
}

func (w *Work) Hour() int {
    if w == nil {
        return -1
    }
    return w.hour
}

func (w *Work) State() State {
    if w == nil {
        return nil
    }
    return w.state
}

func (w *Work) SetHour(h int) {
    if w == nil {
        return
    }
    w.hour = h
}

func (w *Work) SetState(s State) {
    if w == nil {
        return
    }
    w.state = s
}

func (w *Work) writeProgram() {
    if w == nil {
        return
    }
    w.state.writeProgram(w)
}

func NewWork() *Work {
    state := new(moringState)
    return &amp;amp;Work{state: state}
}

type State interface {
    writeProgram(w *Work)
}

// 上午时分状态类
type moringState struct {
}

func (m *moringState) writeProgram(w *Work) {
    if w.Hour() &amp;lt; 12 {
        fmt.Println(&amp;quot;现在是上午时分&amp;quot;, w.Hour())
    } else {
        w.SetState(new(NoonState))
        w.writeProgram()
    }
}

// 中午时分状态类
type NoonState struct {
}

func (m *NoonState) writeProgram(w *Work) {
    if w.Hour() &amp;lt; 13 {
        fmt.Println(&amp;quot;现在是中午时分&amp;quot;, w.Hour())
    } else {
        w.SetState(new(AfternoonState))
        w.writeProgram()
    }
}

// 下午时分状态类
type AfternoonState struct {
}

func (m *AfternoonState) writeProgram(w *Work) {
    if w.Hour() &amp;lt; 17 {
        fmt.Println(&amp;quot;现在是下午时分&amp;quot;, w.Hour())
    } else {
        w.SetState(new(EveningState))
        w.writeProgram()
    }
}

// 晚上时分状态类
type EveningState struct {
}

func (m *EveningState) writeProgram(w *Work) {
    if w.Hour() &amp;lt; 21 {
        fmt.Println(&amp;quot;现在是晚上时分&amp;quot;, w.Hour())
    } else {
        fmt.Println(&amp;quot;现在开始睡觉&amp;quot;, w.Hour())
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;适配器模式&#34;&gt;适配器模式&lt;/h2&gt;

&lt;p&gt;Adapter 适配器模式：将一个类的接口转换成客户端希望的另一个接口。
        适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作&lt;/p&gt;

&lt;p&gt;代理和适配器：代理和代理的对象接口一致，客户端不知道代理对象，而适配器是客户端想要适配器的接口，适配器对象的接口和客户端想要的不一样，适配器将适配器对象的接口封装一下，改成客户端想要的接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package adapter

import (
    &amp;quot;fmt&amp;quot;
)

type Player interface {
    attack()
    defense()
}

type Forwards struct {
    name string
}

func (f *Forwards) attack() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在进攻&amp;quot;)
}
func (f *Forwards) defense() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在防守&amp;quot;)
}

func NewForwards(name string) Player {
    return &amp;amp;Forwards{name}
}

type Centers struct {
    name string
}

func (f *Centers) attack() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在进攻&amp;quot;)
}
func (f *Centers) defense() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在防守&amp;quot;)
}

func NewCenter(name string) Player {
    return &amp;amp;Centers{name}
}

type ForeignCenter struct {
    name string
}

func (f *ForeignCenter) attack(what string) {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在进攻&amp;quot;)
}
func (f *ForeignCenter) defense() {
    if f == nil {
        return
    }
    fmt.Println(f.name, &amp;quot;在防守&amp;quot;)
}

type Translator struct {
    f ForeignCenter
}

// 这是用户想要的接口
func (t *Translator) attack() {
    if t == nil {
        return
    }
    t.f.attack(&amp;quot;进攻&amp;quot;)
}
func (t *Translator) defense() {
    if t == nil {
        return
    }
    t.f.defense()
}

func NewTranslator(name string) Player {
    return &amp;amp;Translator{ForeignCenter{name}}
}









package adapter

//Target 是适配的目标接口
type Target interface {
    Request() string
}

//Adaptee 是被适配的目标接口
type Adaptee interface {
    SpecificRequest() string
}

//NewAdaptee 是被适配接口的工厂函数
func NewAdaptee() Adaptee {
    return &amp;amp;adapteeImpl{}
}

//AdapteeImpl 是被适配的目标类
type adapteeImpl struct{}

//SpecificRequest 是目标类的一个方法
func (*adapteeImpl) SpecificRequest() string {
    return &amp;quot;adaptee method&amp;quot;
}

//NewAdapter 是Adapter的工厂函数
func NewAdapter(adaptee Adaptee) Target {
    return &amp;amp;adapter{
        Adaptee: adaptee,
    }
}

//Adapter 是转换Adaptee为Target接口的适配器
type adapter struct {
    Adaptee
}

//Request 实现Target接口
func (a *adapter) Request() string {
    return a.SpecificRequest()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;备忘录模式&#34;&gt;备忘录模式&lt;/h2&gt;

&lt;p&gt;Memento 备忘录模式：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可以将该对象恢复到原先保存的状态&lt;/p&gt;

&lt;p&gt;将某个类的状态（某些状态，具体有该类决定）保存在另外一个类中（代码级别：提供一个函数能够将状态保存起来，返回出去），保存好状态的类对象是管理类的成员，原来的类需要恢复时，再从管理类中获取原来的状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package memento

import (
    &amp;quot;fmt&amp;quot;
)

type GameRole struct {
    vit int
    atk int
    def int
}

func (g *GameRole) StateDisplay() {
    if g == nil {
        return
    }
    fmt.Println(&amp;quot;角色当前状态：&amp;quot;)
    fmt.Println(&amp;quot;体力：&amp;quot;, g.vit)
    fmt.Println(&amp;quot;攻击：&amp;quot;, g.atk)
    fmt.Println(&amp;quot;防御：&amp;quot;, g.def)
    fmt.Println(&amp;quot;============&amp;quot;)
}

func (g *GameRole) GetInitState() {
    if g == nil {
        return
    }
    g.vit = 100
    g.atk = 100
    g.def = 100
}

func (g *GameRole) Fight() {
    if g == nil {
        return
    }
    g.vit = 0
    g.atk = 0
    g.def = 0
}
func (g *GameRole) SaveState() RoleStateMemento {
    if g == nil {
        return RoleStateMemento{}
    }
    return RoleStateMemento{*g}
}

func (g *GameRole) RecoveryState(r RoleStateMemento) {
    if g == nil {
        return
    }
    g.vit = r.vit
    g.atk = r.atk
    g.def = r.def
}

type RoleStateMemento struct {
    GameRole
}

type RoleStateCaretaker struct {
    memento RoleStateMemento
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;备忘录模式用于保存程序内部状态到外部，又不希望暴露内部状态的情形。&lt;/p&gt;

&lt;p&gt;程序内部状态使用窄接口船体给外部进行存储，从而不暴露程序实现细节。&lt;/p&gt;

&lt;p&gt;备忘录模式同时可以离线保存内部状态，如保存到数据库，文件等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package memento

import &amp;quot;fmt&amp;quot;

type Memento interface{}

type Game struct {
    hp, mp int
}

type gameMemento struct {
    hp, mp int
}

func (g *Game) Play(mpDelta, hpDelta int) {
    g.mp += mpDelta
    g.hp += hpDelta
}

func (g *Game) Save() Memento {
    return &amp;amp;gameMemento{
        hp: g.hp,
        mp: g.mp,
    }
}

func (g *Game) Load(m Memento) {
    gm := m.(*gameMemento)
    g.mp = gm.mp
    g.hp = gm.hp
}

func (g *Game) Status() {
    fmt.Printf(&amp;quot;Current HP:%d, MP:%d\n&amp;quot;, g.hp, g.mp)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;组合模式&#34;&gt;组合模式&lt;/h2&gt;

&lt;p&gt;Composite 组合模式：将对象组合成树形结构，以表示“部分-整体”的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性&lt;/p&gt;

&lt;p&gt;组合模式统一对象和对象集，使得使用相同接口使用对象和对象集。&lt;/p&gt;

&lt;p&gt;组合模式常用于树状结构，用于统一叶子节点和树节点的访问，并且可以用于应用某一操作到所有子节点。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package composite

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;strings&amp;quot;
)

// 公司管理接口
type Company interface {
    add(Company)
    remove(Company)
    display(int)
    lineOfDuty()
}

type RealCompany struct {
    name string
}

// 具体公司
type ConcreateCompany struct {
    RealCompany
    list []Company
}

func NewConcreateCompany(name string) *ConcreateCompany {
    return &amp;amp;ConcreateCompany{RealCompany{name}, []Company{}}
}

func (c *ConcreateCompany) add(newc Company) {
    if c == nil {
        return
    }
    c.list = append(c.list, newc)
}

func (c *ConcreateCompany) remove(delc Company) {
    if c == nil {
        return
    }
    for i, val := range c.list {
        if val == delc {
            c.list = append(c.list[:i], c.list[i+1:]...)
            return
        }
    }
    return
}

func (c *ConcreateCompany) display(depth int) {
    if c == nil {
        return
    }
    fmt.Println(strings.Repeat(&amp;quot;-&amp;quot;, depth), &amp;quot; &amp;quot;, c.name)
    for _, val := range c.list {
        val.display(depth + 2)
    }
}

func (c *ConcreateCompany) lineOfDuty() {
    if c == nil {
        return
    }

    for _, val := range c.list {
        val.lineOfDuty()
    }
}

// 人力资源部门
type HRDepartment struct {
    RealCompany
}

func NewHRDepartment(name string) *HRDepartment {
    return &amp;amp;HRDepartment{RealCompany{name}}
}

func (h *HRDepartment) add(c Company)    {}
func (h *HRDepartment) remove(c Company) {}

func (h *HRDepartment) display(depth int) {
    if h == nil {
        return
    }
    fmt.Println(strings.Repeat(&amp;quot;-&amp;quot;, depth), &amp;quot; &amp;quot;, h.name)
}

func (h *HRDepartment) lineOfDuty() {
    if h == nil {
        return
    }
    fmt.Println(h.name, &amp;quot;员工招聘培训管理&amp;quot;)
}

// 财务部门
type FinanceDepartment struct {
    RealCompany
}

func NewFinanceDepartment(name string) *FinanceDepartment {
    return &amp;amp;FinanceDepartment{RealCompany{name}}
}

func (h *FinanceDepartment) add(c Company)    {}
func (h *FinanceDepartment) remove(c Company) {}

func (h *FinanceDepartment) display(depth int) {
    if h == nil {
        return
    }
    fmt.Println(strings.Repeat(&amp;quot;-&amp;quot;, depth), &amp;quot; &amp;quot;, h.name)
}

func (h *FinanceDepartment) lineOfDuty() {
    if h == nil {
        return
    }
    fmt.Println(h.name, &amp;quot;公司财务收支管理&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;迭代器模式&#34;&gt;迭代器模式&lt;/h2&gt;

&lt;p&gt;Iterator 迭代器模式：提供一种方法顺序访问一个聚合对象中的各个元素，而又不暴露该对象的内部表示。比如下面只是使用next，而不会暴露具体的东西，就像range。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package iterator

//&amp;quot;fmt&amp;quot;

type Book struct {
    name string
}

type Iterator interface {
    first() interface{}
    next() interface{}
}

type BookGroup struct {
    books []Book
}

func (b *BookGroup) add(newb Book) {
    if b == nil {
        return
    }
    b.books = append(b.books, newb)
}

func (b *BookGroup) createIterator() *BookIterator {
    if b == nil {
        return nil
    }
    return &amp;amp;BookIterator{b, 0}
}

type BookIterator struct {
    g     *BookGroup
    index int
}

func (b *BookIterator) first() interface{} {
    if b == nil {
        return nil
    }
    if len(b.g.books) &amp;gt; 0 {
        b.index = 0
        return b.g.books[b.index]
    }
    return nil
}

func (b *BookIterator) next() interface{} {
    if b == nil {
        return nil
    }
    if len(b.g.books) &amp;gt; b.index+1 {
        b.index++
        return b.g.books[b.index]
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;单例模式&#34;&gt;单例模式&lt;/h2&gt;

&lt;p&gt;Singleton 单例：保证一个类仅有一个实例，并提供一个访问它的全局访问点&lt;/p&gt;

&lt;p&gt;用Go实现时，巧妙使用包级别的变量声明规则：小写字母的包级别变量是不对外开放的，创建实例时，用同步库sync.Once来保证全局只有一个对象实例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package singleton

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;sync&amp;quot;
)

// 全局实例者
type singleton struct {
    data int
}

// 定义一个包级别的private实例变量
var sin *singleton

// 同步Once,保证每次调用时，只有第一次生效
var once sync.Once

// 获取实例对象函数
func GetSingleton() *singleton {
    once.Do(func() {
        sin = &amp;amp;singleton{12}
    })
    fmt.Println(&amp;quot;实例对象的信息和地址&amp;quot;, sin, &amp;amp;sin)
    return sin
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用懒惰模式的单例模式，使用双重检查加锁保证线程安全&lt;/p&gt;

&lt;h2 id=&#34;桥接模式&#34;&gt;桥接模式&lt;/h2&gt;

&lt;p&gt;Bridge 桥接模式：将抽象部分与它的实现部分分离，使它们都可以独立地变化&lt;/p&gt;

&lt;p&gt;桥接模式是聚合／合成规则的一种使用。&lt;/p&gt;

&lt;p&gt;桥接模式分离抽象部分和实现部分。使得两部分独立扩展。&lt;/p&gt;

&lt;p&gt;桥接模式类似于策略模式，区别在于策略模式封装一系列算法使得算法可以互相替换。&lt;/p&gt;

&lt;p&gt;策略模式使抽象部分和实现部分分离，可以独立变化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package bridge

import (
    &amp;quot;fmt&amp;quot;
)

type Phone struct {
    soft ISoftware
    name string
}

func (p *Phone) setSoft(soft ISoftware) {
    if p == nil {
        return
    }
    p.soft = soft
}

func (p *Phone) Run() {
    if p == nil {
        return
    }
    fmt.Println(p.name)
    p.soft.Run()
}

type PhoneA struct {
    Phone
}

func NewPhoneA(name string) *PhoneA {
    return &amp;amp;PhoneA{Phone{name: name}}
}

type PhoneB struct {
    Phone
}

func NewPhoneB(name string) *PhoneB {
    return &amp;amp;PhoneB{Phone{name: name}}
}

type ISoftware interface {
    Run()
}

type TSoftware struct {
    ISoftware
}

type Software struct {
    name string
}

type SoftwareA struct {
    Software
}

func (s *SoftwareA) Run() {
    if s == nil {
        return
    }
    fmt.Println(s.name)
}

type SoftwareB struct {
    Software
}

/*func (s *SoftwareB) Run() {
    if s == nil {
        return
    }
    fmt.Println(s.name)
}*/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;命令模式&#34;&gt;命令模式&lt;/h2&gt;

&lt;p&gt;命令模式本质是把某个对象的方法调用封装到对象中，方便传递、存储、调用。&lt;/p&gt;

&lt;p&gt;示例中把主板单中的启动(start)方法和重启(reboot)方法封装为命令对象，再传递到主机(box)对象中。于两个按钮进行绑定：&lt;/p&gt;

&lt;p&gt;第一个机箱(box1)设置按钮1(buttion1) 为开机按钮2(buttion2)为重启。
第二个机箱(box1)设置按钮2(buttion2) 为开机按钮1(buttion1)为重启。&lt;/p&gt;

&lt;p&gt;从而得到配置灵活性。&lt;/p&gt;

&lt;p&gt;除了配置灵活外，使用命令模式还可以用作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;批处理
任务队列
undo, redo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等把具体命令封装到对象中使用的场合&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package command

import &amp;quot;fmt&amp;quot;

type Command interface {
    Execute()
}

type StartCommand struct {
    mb *MotherBoard
}

func NewStartCommand(mb *MotherBoard) *StartCommand {
    return &amp;amp;StartCommand{
        mb: mb,
    }
}

func (c *StartCommand) Execute() {
    c.mb.Start()
}

type RebootCommand struct {
    mb *MotherBoard
}

func NewRebootCommand(mb *MotherBoard) *RebootCommand {
    return &amp;amp;RebootCommand{
        mb: mb,
    }
}

func (c *RebootCommand) Execute() {
    c.mb.Reboot()
}

type MotherBoard struct{}

func (*MotherBoard) Start() {
    fmt.Print(&amp;quot;system starting\n&amp;quot;)
}

func (*MotherBoard) Reboot() {
    fmt.Print(&amp;quot;system rebooting\n&amp;quot;)
}

type Box struct {
    buttion1 Command
    buttion2 Command
}

func NewBox(buttion1, buttion2 Command) *Box {
    return &amp;amp;Box{
        buttion1: buttion1,
        buttion2: buttion2,
    }
}

func (b *Box) PressButtion1() {
    b.buttion1.Execute()
}

func (b *Box) PressButtion2() {
    b.buttion2.Execute()
}







package command

import (
    &amp;quot;fmt&amp;quot;
)

// 命令接口 -- 可以保存在请求队形中，方便请求队形处理命令，具体对命令的执行体在实现这个接口的类型结构体中保存着
type Command interface {
    Run()
}

// 请求队形，保存命令列表，在ExecuteCommand函数中遍历执行命令
type Invoker struct {
    comlist []Command
}

// 添加命令
func (i *Invoker) AddCommand(c Command) {
    if i == nil {
        return
    }
    i.comlist = append(i.comlist, c)
}

// 执行命令
func (i *Invoker) ExecuteCommand() {
    if i == nil {
        return
    }
    for _, val := range i.comlist {
        val.Run()
    }
}

func NewInvoker() *Invoker {
    return &amp;amp;Invoker{[]Command{}}
}

// 具体命令,实现Command接口，保存一个对该命令如何处理的执行体
type ConcreteCommandA struct {
    receiver ReceiverA
}

func (c *ConcreteCommandA) SetReceiver(r ReceiverA) {
    if c == nil {
        return
    }
    c.receiver = r
}

// 具体命令的执行体
func (c *ConcreteCommandA) Run() {
    if c == nil {
        return
    }
    c.receiver.Execute()
}

func NewConcreteCommandA() *ConcreteCommandA {
    return &amp;amp;ConcreteCommandA{}
}

// 针对ConcreteCommand，如何处理该命令
type ReceiverA struct {
}

func (r *ReceiverA) Execute() {
    if r == nil {
        return
    }
    fmt.Println(&amp;quot;针对ConcreteCommandA，如何处理该命令&amp;quot;)
}

func NewReceiverA() *ReceiverA {
    return &amp;amp;ReceiverA{}
}

//////////////////////////////////////////////////////////

// 具体命令,实现Command接口，保存一个对该命令如何处理的执行体
type ConcreteCommandB struct {
    receiver ReceiverB
}

func (c *ConcreteCommandB) SetReceiver(r ReceiverB) {
    if c == nil {
        return
    }
    c.receiver = r
}

// 具体命令的执行体
func (c *ConcreteCommandB) Run() {
    if c == nil {
        return
    }
    c.receiver.Execute()
}

func NewConcreteCommandB() *ConcreteCommandB {
    return &amp;amp;ConcreteCommandB{}
}

// 针对ConcreteCommandB，如何处理该命令
type ReceiverB struct {
}

func (r *ReceiverB) Execute() {
    if r == nil {
        return
    }
    fmt.Println(&amp;quot;针对ConcreteCommandB，如何处理该命令&amp;quot;)
}

func NewReceiverB() *ReceiverB {
    return &amp;amp;ReceiverB{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;职责链模式&#34;&gt;职责链模式&lt;/h2&gt;

&lt;p&gt;Chain Of Responsibility 职责链模式：使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package chainofresponsibility

import (
    &amp;quot;fmt&amp;quot;
)

const (
    constHandler = iota
    constHandlerA
    constHandlerB
)

// 处理请求接口
type IHandler interface {
    SetSuccessor(IHandler)
    HandleRequest(int) int
}

// 实现处理请求的接口的基本结构体类型
type Handler struct {
    successor IHandler // 继承者
}

func (h *Handler) SetSuccessor(i IHandler) {
    if h == nil {
        return
    }
    h.successor = i
}

// 具体处理结构体，这里简单处理int类型的请求，判断是否在[1-10]之间，是：处理，否：交给successor处理
type ConcreteHandlerA struct {
    Handler
}

func (c *ConcreteHandlerA) HandleRequest(req int) int {
    if c == nil {
        return constHandler
    }
    if req &amp;gt; 0 &amp;amp;&amp;amp; req &amp;lt; 11 {
        fmt.Println(&amp;quot;ConcreteHandlerA可以处理这个请求&amp;quot;)
        return constHandlerA
    } else if c.successor != nil {
        return c.successor.HandleRequest(req)
    }
    return constHandler
}

func NewConcreteHandlerA() *ConcreteHandlerA {
    return &amp;amp;ConcreteHandlerA{}
}

// 具体处理结构体，这里简单处理int类型的请求，判断是否在[11-20]之间，是：处理，否：交给successor处理
type ConcreteHandlerB struct {
    Handler
}

func (c *ConcreteHandlerB) HandleRequest(req int) int {
    if c == nil {
        return constHandler
    }
    if req &amp;gt; 10 &amp;amp;&amp;amp; req &amp;lt; 21 {
        fmt.Println(&amp;quot;ConcreteHandlerB可以处理这个请求&amp;quot;)
        return constHandlerB
    } else if c.successor != nil {
        return c.successor.HandleRequest(req)
    }
    return constHandler
}

func NewConcreteHandlerB() *ConcreteHandlerB {
    return &amp;amp;ConcreteHandlerB{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;中介者模式&#34;&gt;中介者模式&lt;/h2&gt;

&lt;p&gt;Mediator 中介者模式：用一个中介对象来封装一系列的对象交互。中介这使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。&lt;/p&gt;

&lt;p&gt;每个对象都有一个中介者对象，发生变化时，通知中介者，由中介者判断通知其他的对象。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package Mediator

import (
    &amp;quot;fmt&amp;quot;
)

// 中介者接口
type IMediator interface {
    Send(string, IColleague)
}

// 实现中介者接口的基本类型
type Mediator struct {
}

// 具体的中介者
type ConcreteMediator struct {
    Mediator
    colleagues []IColleague
}

func (m *ConcreteMediator) AddColleague(c IColleague) {
    if m == nil {
        return
    }
    m.colleagues = append(m.colleagues, c)
}

func (m *ConcreteMediator) Send(message string, c IColleague) {
    if m == nil {
        return
    }
    for _, val := range m.colleagues {
        if c == val {
            continue
        }
        val.Notify(message)
    }
}

func NewConcreteMediator() *ConcreteMediator {
    return &amp;amp;ConcreteMediator{}
}

// 合作者接口
type IColleague interface {
    Send(string)
    Notify(string)
}

// 实现合作者接口的基本类型
type Colleague struct {
    mediator IMediator
}

// 具体合作者对象A
type ConcreteColleageA struct {
    Colleague
}

func (c *ConcreteColleageA) Notify(message string) {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;ConcreteColleageA get message:&amp;quot;, message)
}

func (c *ConcreteColleageA) Send(message string) {
    if c == nil {
        return
    }
    c.mediator.Send(message, c)

}
func NewConcreteColleageA(mediator IMediator) *ConcreteColleageA {
    return &amp;amp;ConcreteColleageA{Colleague{mediator}}
}

// 具体合作者对象B
type ConcreteColleageB struct {
    Colleague
}

func (c *ConcreteColleageB) Notify(message string) {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;ConcreteColleageB get message:&amp;quot;, message)
}
func (c *ConcreteColleageB) Send(message string) {
    if c == nil {
        return
    }
    c.mediator.Send(message, c)

}
func NewConcreteColleageB(mediator IMediator) *ConcreteColleageB {
    return &amp;amp;ConcreteColleageB{Colleague{mediator}}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;享元模式&#34;&gt;享元模式&lt;/h2&gt;

&lt;p&gt;Flyweight 享元模式：运用共享技术有效地支持大量细粒度的对象&lt;/p&gt;

&lt;p&gt;主要思想是共享，将可以共享的部分放在对象内部，不可以共享的部分放在外边，享元工厂创建几个享元对象就可以了，这样不同的外部状态，可以针对同一个对象，给人感觉是操作多个对象，通过参数的形式对同一个对象的操作，像是对多个对象的操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package flyweight

import (
    &amp;quot;fmt&amp;quot;
)

// 享元对象接口
type IFlyweight interface {
    Operation(int) //来自外部的状态
}

// 共享对象
type ConcreteFlyweight struct {
    name string
}

func (c *ConcreteFlyweight) Operation(outState int) {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;共享对象响应外部状态&amp;quot;, outState)
}

// 不共享对象
type UnsharedConcreteFlyweight struct {
    name string
}

func (c *UnsharedConcreteFlyweight) Operation(outState int) {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;不共享对象响应外部状态&amp;quot;, outState)
}

// 享元工厂对象
type FlyweightFactory struct {
    flyweights map[string]IFlyweight
}

func (f *FlyweightFactory) Flyweight(name string) IFlyweight {
    if f == nil {
        return nil
    }
    if name == &amp;quot;u&amp;quot; {
        return &amp;amp;UnsharedConcreteFlyweight{&amp;quot;u&amp;quot;}
    } else if _, ok := f.flyweights[name]; !ok {
        f.flyweights[name] = &amp;amp;ConcreteFlyweight{name}
    }
    return f.flyweights[name]
}

func NewFlyweightFactory() *FlyweightFactory {
    ff := FlyweightFactory{make(map[string]IFlyweight)}
    ff.flyweights[&amp;quot;a&amp;quot;] = &amp;amp;ConcreteFlyweight{&amp;quot;a&amp;quot;}
    ff.flyweights[&amp;quot;b&amp;quot;] = &amp;amp;ConcreteFlyweight{&amp;quot;b&amp;quot;}
    return &amp;amp;ff
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;享元模式从对象中剥离出不发生改变且多个实例需要的重复数据，独立出一个享元，使多个对象共享，从而节省内存以及减少对象数量。&lt;/p&gt;

&lt;h2 id=&#34;解释器模式&#34;&gt;解释器模式&lt;/h2&gt;

&lt;p&gt;Interpreter 解释器模式：给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package interpreter

import (
    &amp;quot;fmt&amp;quot;
)

type Context struct {
    text string
}

// 抽象表达式
type IAbstractExpression interface {
    Interpret(*Context)
}

// 终结符表达式
type TerminalExpression struct {
}

func (t *TerminalExpression) Interpret(context *Context) {
    if t == nil {
        return
    }
    context.text = context.text[:len(context.text)-1]
    fmt.Println(context)
}

// 非终结符表达式
type NonterminalExpression struct {
}

func (t *NonterminalExpression) Interpret(context *Context) {
    if t == nil {
        return
    }
    context.text = context.text[:len(context.text)-1]
    fmt.Println(context)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解释器模式定义一套语言文法，并设计该语言解释器，使用户能使用特定文法控制解释器行为。&lt;/p&gt;

&lt;p&gt;解释器模式的意义在于，它分离多种复杂功能的实现，每个功能只需关注自身的解释。&lt;/p&gt;

&lt;p&gt;对于调用者不用关心内部的解释器的工作，只需要用简单的方式组合命令就可以。&lt;/p&gt;

&lt;h2 id=&#34;访问者模式&#34;&gt;访问者模式&lt;/h2&gt;

&lt;p&gt;Visitor 访问者模式：表示一个作用于某对象结构中的各元素的操作，它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package visitor

import (
    &amp;quot;fmt&amp;quot;
)

// 访问接口
type IVisitor interface {
    VisitConcreteElementA(ConcreteElementA)
    VisitConcreteElementB(ConcreteElementB)
}

// 具体访问者A
type ConcreteVisitorA struct {
    name string
}

func (c *ConcreteVisitorA) VisitConcreteElementA(ce ConcreteElementA) {
    if c == nil {
        return
    }
    fmt.Println(ce.name, c.name)
    ce.OperatorA()
}

func (c *ConcreteVisitorA) VisitConcreteElementB(ce ConcreteElementB) {
    if c == nil {
        return
    }
    fmt.Println(ce.name, c.name)
    ce.OperatorB()
}

// 具体访问者B
type ConcreteVisitorB struct {
    name string
}

func (c *ConcreteVisitorB) VisitConcreteElementA(ce ConcreteElementA) {
    if c == nil {
        return
    }
    fmt.Println(ce.name, c.name)
    ce.OperatorA()
}

func (c *ConcreteVisitorB) VisitConcreteElementB(ce ConcreteElementB) {
    if c == nil {
        return
    }
    fmt.Println(ce.name, c.name)
    ce.OperatorB()
}

// 元素接口
type IElement interface {
    Accept(IVisitor)
}

// 具体元素A
type ConcreteElementA struct {
    name string
}

func (c *ConcreteElementA) Accept(visitor IVisitor) {
    if c == nil {
        return
    }
    visitor.VisitConcreteElementA(*c)
}
func (c *ConcreteElementA) OperatorA() {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;OperatorA&amp;quot;)
}

// 具体元素B
type ConcreteElementB struct {
    name string
}

func (c *ConcreteElementB) Accept(visitor IVisitor) {
    if c == nil {
        return
    }
    visitor.VisitConcreteElementB(*c)
}
func (c *ConcreteElementB) OperatorB() {
    if c == nil {
        return
    }
    fmt.Println(&amp;quot;OperatorB&amp;quot;)
}

// 维护元素集合
type ObjectStructure struct {
    list []IElement
}

func (o *ObjectStructure) Attach(e IElement) {
    if o == nil || e == nil {
        return
    }
    o.list = append(o.list, e)
}

func (o *ObjectStructure) Detach(e IElement) {
    if o == nil || e == nil {
        return
    }
    for i, val := range o.list {
        if val == e {
            o.list = append(o.list[:i], o.list[i+1:]...)
            break
        }
    }
}

func (o *ObjectStructure) Accept(v IVisitor) {
    if o == nil {
        return
    }
    for _, val := range o.list {
        val.Accept(v)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问者模式可以给一系列对象透明的添加功能，并且把相关代码封装到一个类中。&lt;/p&gt;

&lt;p&gt;对象只要预留访问者接口Accept则后期为对象添加功能的时候就不需要改动对象。&lt;/p&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;其实在golang语言编程设计中分为这么多模式是没有必要的，在以上的实践中，大体可以归纳我们设计时候需要使用的思想模式，其他都是异曲同工的。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;抽象，设定接口，根据不同的参数获取不同的实例，还可以在上面再封装一层，就如我们实现的简单工厂，策略，抽象工厂等模式,这种设计多数也是为了解耦，抽象解耦是设计中的最重要的思想。&lt;/li&gt;
&lt;li&gt;继承，开放封闭原则，可扩展不可修改，使用组合来完成具体实现在子类，父类就是一个接口，比如模版等模式&lt;/li&gt;
&lt;li&gt;封装，符合迪米特法则，就是在实现的基础上新建结构体暴露接口，比如外观，建造者等模式&lt;/li&gt;
&lt;li&gt;特殊场景，观察者模式，这类就是用于注册观察通知的使用方式.
        单例,全局一份。
        适配器，将两个不一样的结构进行适配。。。还有好几个，可以具体去看，这边就不一一列举了。&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>数据库mysql系列---- mysql前置缓存redis</title>
          <link>https://kingjcy.github.io/post/database/mysql/redis-mysql/</link>
          <pubDate>Sun, 22 Jan 2017 14:41:38 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/database/mysql/redis-mysql/</guid>
          <description>&lt;p&gt;mysql前置缓存redis是我们经常使用的提供性能的方案。&lt;/p&gt;

&lt;h1 id=&#34;常规使用&#34;&gt;常规使用&lt;/h1&gt;

&lt;p&gt;1、基于binlog使用mysql_udf_redis，将数据库中的数据同步到Redis。&lt;/p&gt;

&lt;p&gt;      无论MySQL还是Redis，自身都带有数据同步的机制，像比较常用的MySQL的Master/Slave模式，就是由Slave端分析Master的binlog来实现的，这样的数据其实还是一个异步过程，只不过当服务器都在同一内网时，异步的延迟几乎可以忽略。&lt;/p&gt;

&lt;p&gt;      那么理论上我们也可以用同样方式，分析MySQL的binlog文件并将数据插入Redis。但是这需要对binlog文件以及MySQL有非常深入的理解，同时由于binlog存在Statement/Row/Mixedlevel多种形式，分析binlog实现同步的工作量是非常大的。&lt;/p&gt;

&lt;p&gt;2、通过MySQL自动同步刷新Redis&lt;/p&gt;

&lt;p&gt;     当我们在业务层有数据查询需求时，先到Redis缓存中查询，如果查不到，再到MySQL数据库中查询，同时将查到的数据更新到Redis里；当我们在业务层有修改插入数据需求时，直接向MySQL发起请求，同时更新Redis缓存。 就是MySQL的CRUD发生后自动地更新到Redis里，这需要通过MySQL UDF来实现。具体来说，我们把更新Redis的逻辑放到MySQL中去做，即定义一个触发器Trigger，监听CRUD这些操作，当操作发生后，调用对应的UDF函数，远程写回Redis，所以业务逻辑只需要负责更新MySQL就行了，剩下的交给MySQL UDF去完成。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在我们的实际开发当中往往采用如下方式实现实现Mysql和Redis数据同步：当我们在MySQL数据库中进行增删改的时候，我们在增删改的service层将缓存中的数据清除，这个时候用户在此请求的时候我们缓存中没有数据了，直接去数据库中查询，查询回来之后将缓存中的数据放缓存当中，这个时候缓存中的数据就是最新的数据。&lt;/p&gt;

&lt;h1 id=&#34;实战&#34;&gt;实战&lt;/h1&gt;

&lt;p&gt;后台服务器信息查询&lt;/p&gt;

&lt;p&gt;1、后台重kafka订阅信息，如果有服务器信息，后台系统进行消费，放入到mysql数据库中&lt;/p&gt;

&lt;p&gt;2、很多场景都需要查询服务器的信息，比如我们接受到的zbabix的数据，需要根据ip的去查询服务器的信息&lt;/p&gt;

&lt;p&gt;3、这边在查询之间就加了一层redis缓存，先去redis缓存查数据，如果查到数据，就返回，如果没有查到数据就到mysql数据库进行查询，将查到的数据返回的同时更新到redis中，key是IP，json信息是value，同时设置过期事件，用来保存经常查询的数据。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Strings</title>
          <link>https://kingjcy.github.io/post/golang/go-strings/</link>
          <pubDate>Wed, 12 Oct 2016 19:37:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-strings/</guid>
          <description>&lt;p&gt;平时在开发过程中， 和字符串打交道还是比较多的，比如分割， 去除， 替换等等常用的方法， 这些都是由strings包来提供的。&lt;/p&gt;

&lt;h1 id=&#34;基本应用&#34;&gt;基本应用&lt;/h1&gt;

&lt;h2 id=&#34;字符串比较&#34;&gt;字符串比较&lt;/h2&gt;

&lt;p&gt;Compare 函数，用于比较两个字符串的大小，如果两个字符串相等，返回为 0。如果 a 小于 b ，返回 -1 ，反之返回 1 。不推荐使用这个函数，直接使用 == != &amp;gt; &amp;lt; &amp;gt;= &amp;lt;= 等一系列运算符更加直观。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Compare(a, b string) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EqualFold 函数，计算 s 与 t 忽略字母大小写后是否相等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func EqualFold(s, t string) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := &amp;quot;gopher&amp;quot;
b := &amp;quot;hello world&amp;quot;
fmt.Println(strings.Compare(a, b))
fmt.Println(strings.Compare(a, a))
fmt.Println(strings.Compare(b, a))

fmt.Println(strings.EqualFold(&amp;quot;GO&amp;quot;, &amp;quot;go&amp;quot;))
fmt.Println(strings.EqualFold(&amp;quot;壹&amp;quot;, &amp;quot;一&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-1
0
1
true
false
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;是否存在某个字符或子串&#34;&gt;是否存在某个字符或子串&lt;/h2&gt;

&lt;p&gt;有三个函数做这件事：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 子串 substr 在 s 中，返回 true
func Contains(s, substr string) bool
// chars 中任何一个 Unicode 代码点在 s 中，返回 true
func ContainsAny(s, chars string) bool
// Unicode 代码点 r 在 s 中，返回 true
func ContainsRune(s string, r rune) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里对 ContainsAny 函数进行一下说明，看如下例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.ContainsAny(&amp;quot;team&amp;quot;, &amp;quot;i&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;failure&amp;quot;, &amp;quot;u &amp;amp; i&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;in failure&amp;quot;, &amp;quot;s g&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;foo&amp;quot;, &amp;quot;&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;&amp;quot;, &amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;false
true
true
false
false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说，第二个参数 chars 中任意一个字符（Unicode Code Point）如果在第一个参数 s 中存在，则返回 true。&lt;/p&gt;

&lt;p&gt;查看这三个函数的源码，发现它们只是调用了相应的 Index 函数（子串出现的位置），然后和 0 作比较返回 true 或 fale。如，Contains：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Contains(s, substr string) bool {
  return Index(s, substr) &amp;gt;= 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;index则使用了我们常用的字符串匹配算法的rk算法。&lt;/p&gt;

&lt;h2 id=&#34;子串出现次数-字符串匹配&#34;&gt;子串出现次数 ( 字符串匹配 )&lt;/h2&gt;

&lt;p&gt;在数据结构与算法中，可能会讲解以下字符串匹配算法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;朴素匹配算法&lt;/li&gt;
&lt;li&gt;KMP 算法&lt;/li&gt;
&lt;li&gt;Rabin-Karp 算法&lt;/li&gt;
&lt;li&gt;Boyer-Moore 算法&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有其他的算法，这里不一一列举，感兴趣的可以网上搜一下。&lt;/p&gt;

&lt;p&gt;在 Go 中，查找子串出现次数即字符串模式匹配，根据长度，分别实现的是BF和 Rabin-Karp 算法。Count 函数的签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Count(s, sep string) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 Count 的实现中，处理了几种特殊情况，属于字符匹配预处理的一部分。这里要特别说明一下的是当 sep 为空时，Count 的返回值是：utf8.RuneCountInString(s) + 1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Count(&amp;quot;cheese&amp;quot;, &amp;quot;e&amp;quot;))
fmt.Println(len(&amp;quot;谷歌中国&amp;quot;))
fmt.Println(strings.Count(&amp;quot;谷歌中国&amp;quot;, &amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3
12
5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 Rabin-Karp 算法的实现，有兴趣的可以看看 Count 的源码。&lt;/p&gt;

&lt;p&gt;另外，Count 是计算子串在字符串中出现的无重叠的次数，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Count(&amp;quot;fivevev&amp;quot;, &amp;quot;vev&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串分割为-string&#34;&gt;字符串分割为[]string&lt;/h2&gt;

&lt;p&gt;这个需求很常见，倒不一定是为了得到[]string。&lt;/p&gt;

&lt;p&gt;该包提供了六个三组分割函数：Fields和FieldsFunc、Split和SplitAfter、SplitN和SplitAfterN。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Fields和FieldsFunc&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Fields(s string) []string
func FieldsFunc(s string, f func(rune) bool) []string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fields 用一个或多个连续的空格分隔字符串 s，返回子字符串的数组（slice）。如果字符串 s 只包含空格，则返回空列表 ([]string 的长度为 0）。其中，空格的定义是 unicode.IsSpace，之前已经介绍过。&lt;/p&gt;

&lt;p&gt;常见间隔符包括：&amp;rsquo;\t&amp;rsquo;, &amp;lsquo;\n&amp;rsquo;, &amp;lsquo;\v&amp;rsquo;, &amp;lsquo;\f&amp;rsquo;, &amp;lsquo;\r&amp;rsquo;, &amp;lsquo; &amp;lsquo;, U+0085 (NEL), U+00A0 (NBSP)&lt;/p&gt;

&lt;p&gt;由于是用空格分隔，因此结果中不会含有空格或空子字符串，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;Fields are: %q&amp;quot;, strings.Fields(&amp;quot;  foo bar  baz   &amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Fields are: [&amp;quot;foo&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;baz&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FieldsFunc通过实现一个回调函数来指定分隔字符串 s 的字符。比如上面的例子，我们通过 FieldsFunc 来实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.FieldsFunc(&amp;quot;  foo bar  baz   &amp;quot;, unicode.IsSpace))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上，Fields 函数就是调用 FieldsFunc 实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Fields(s string) []string {
  return FieldsFunc(s, unicode.IsSpace)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Split和SplitAfter、SplitN和SplitAfterN&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之所以将这四个函数放在一起讲，是因为它们都是通过一个同一个内部函数来实现的。它们的函数签名及其实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Split(s, sep string) []string { return genSplit(s, sep, 0, -1) }
func SplitAfter(s, sep string) []string { return genSplit(s, sep, len(sep), -1) }
func SplitN(s, sep string, n int) []string { return genSplit(s, sep, 0, n) }
func SplitAfterN(s, sep string, n int) []string { return genSplit(s, sep, len(sep), n) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它们都调用了 genSplit 函数。&lt;/p&gt;

&lt;p&gt;这四个函数都是通过 sep 进行分割，返回[]string。如果 sep 为空，相当于分成一个个的 UTF-8 字符，如 Split(&amp;ldquo;abc&amp;rdquo;,&amp;ldquo;&amp;rdquo;)，得到的是[a b c]。&lt;/p&gt;

&lt;p&gt;Split(s, sep) 和 SplitN(s, sep, -1) 等价；SplitAfter(s, sep) 和 SplitAfterN(s, sep, -1) 等价。&lt;/p&gt;

&lt;p&gt;那么，Split 和 SplitAfter 有啥区别呢？通过这两句代码的结果就知道它们的区别了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.SplitAfter(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;quot;foo&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;baz&amp;quot;]
[&amp;quot;foo,&amp;quot; &amp;quot;bar,&amp;quot; &amp;quot;baz&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说，Split 会将 s 中的 sep 去掉，而 SplitAfter 会保留 sep。&lt;/p&gt;

&lt;p&gt;带 N 的方法可以通过最后一个参数 n 控制返回的结果中的 slice 中的元素个数，当 n &amp;lt; 0 时，返回所有的子字符串；当 n == 0 时，返回的结果是 nil；当 n &amp;gt; 0 时，表示返回的 slice 中最多只有 n 个元素，其中，最后一个元素不会分割，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.SplitN(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;, 2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;quot;foo&amp;quot; &amp;quot;bar,baz&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外看一下官方文档提供的例子，注意一下输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;a,b,c&amp;quot;, &amp;quot;,&amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;a man a plan a canal panama&amp;quot;, &amp;quot;a &amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot; xyz &amp;quot;, &amp;quot;&amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;&amp;quot;, &amp;quot;Bernardo O&#39;Higgins&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;]
[&amp;quot;&amp;quot; &amp;quot;man &amp;quot; &amp;quot;plan &amp;quot; &amp;quot;canal panama&amp;quot;]
[&amp;quot; &amp;quot; &amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot; &amp;quot; &amp;quot;]
[&amp;quot;&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串是否有某个前缀或后缀&#34;&gt;字符串是否有某个前缀或后缀&lt;/h2&gt;

&lt;p&gt;这两个函数比较简单，源码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// s 中是否以 prefix 开始
func HasPrefix(s, prefix string) bool {
  return len(s) &amp;gt;= len(prefix) &amp;amp;&amp;amp; s[0:len(prefix)] == prefix
}
// s 中是否以 suffix 结尾
func HasSuffix(s, suffix string) bool {
  return len(s) &amp;gt;= len(suffix) &amp;amp;&amp;amp; s[len(s)-len(suffix):] == suffix
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 prefix 或 suffix 为 &amp;ldquo;&amp;rdquo; , 返回值总是 true。&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.HasPrefix(&amp;quot;Gopher&amp;quot;, &amp;quot;Go&amp;quot;))
fmt.Println(strings.HasPrefix(&amp;quot;Gopher&amp;quot;, &amp;quot;C&amp;quot;))
fmt.Println(strings.HasPrefix(&amp;quot;Gopher&amp;quot;, &amp;quot;&amp;quot;))
fmt.Println(strings.HasSuffix(&amp;quot;Amigo&amp;quot;, &amp;quot;go&amp;quot;))
fmt.Println(strings.HasSuffix(&amp;quot;Amigo&amp;quot;, &amp;quot;Ami&amp;quot;))
fmt.Println(strings.HasSuffix(&amp;quot;Amigo&amp;quot;, &amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;true
false
true
true
false
true
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符或子串在字符串中出现的位置&#34;&gt;字符或子串在字符串中出现的位置&lt;/h2&gt;

&lt;p&gt;有一序列函数与该功能有关：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 在 s 中查找 sep 的第一次出现，返回第一次出现的索引
func Index(s, sep string) int
// 在 s 中查找字节 c 的第一次出现，返回第一次出现的索引
func IndexByte(s string, c byte) int
// chars 中任何一个 Unicode 代码点在 s 中首次出现的位置
func IndexAny(s, chars string) int
// 查找字符 c 在 s 中第一次出现的位置，其中 c 满足 f(c) 返回 true
func IndexFunc(s string, f func(rune) bool) int
// Unicode 代码点 r 在 s 中第一次出现的位置
func IndexRune(s string, r rune) int

// 有三个对应的查找最后一次出现的位置
func LastIndex(s, sep string) int
func LastIndexByte(s string, c byte) int
func LastIndexAny(s, chars string) int
func LastIndexFunc(s string, f func(rune) bool) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一序列函数，只举 IndexFunc 的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;han := func(c rune) bool {
    return unicode.Is(unicode.Han, c) // 汉字
}
fmt.Println(strings.IndexFunc(&amp;quot;Hello, world&amp;quot;, han))
fmt.Println(strings.IndexFunc(&amp;quot;Hello, 世界&amp;quot;, han))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-1
7
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串-join-操作&#34;&gt;字符串 JOIN 操作&lt;/h2&gt;

&lt;p&gt;将字符串数组（或 slice）连接起来可以通过 Join 实现，函数签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Join(a []string, sep string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;假如没有这个库函数，我们自己实现一个，我们会这么实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Join(str []string, sep string) string {
  // 特殊情况应该做处理
  if len(str) == 0 {
      return &amp;quot;&amp;quot;
  }
  if len(str) == 1 {
      return str[0]
  }
  buffer := bytes.NewBufferString(str[0])
  for _, s := range str[1:] {
      buffer.WriteString(sep)
      buffer.WriteString(s)
  }
  return buffer.String()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里，我们使用了 bytes 包的 Buffer 类型，避免大量的字符串连接操作（因为 Go 中字符串是不可变的）。我们再看一下标准库的实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Join(a []string, sep string) string {
  if len(a) == 0 {
      return &amp;quot;&amp;quot;
  }
  if len(a) == 1 {
      return a[0]
  }
  n := len(sep) * (len(a) - 1)
  for i := 0; i &amp;lt; len(a); i++ {
      n += len(a[i])
  }

  b := make([]byte, n)
  bp := copy(b, a[0])
  for _, s := range a[1:] {
      bp += copy(b[bp:], sep)
      bp += copy(b[bp:], s)
  }
  return string(b)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;标准库的实现没有用 bytes 包，当然也不会简单的通过 + 号连接字符串。Go 中是不允许循环依赖的，标准库中很多时候会出现代码拷贝，而不是引入某个包。这里 Join 的实现方式挺好，我个人观点认为，不直接使用 bytes 包，也是不想依赖 bytes 包（其实 bytes 中的实现也是 copy 方式）。&lt;/p&gt;

&lt;p&gt;简单使用示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(Join([]string{&amp;quot;name=xxx&amp;quot;, &amp;quot;age=xx&amp;quot;}, &amp;quot;&amp;amp;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name=xxx&amp;amp;age=xx
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串重复几次&#34;&gt;字符串重复几次&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func Repeat(s string, count int) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 s 重复 count 次，如果 count 为负数或返回值长度 len(s)*count 超出 string 上限会导致 panic，这个函数使用很简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(&amp;quot;ba&amp;quot; + strings.Repeat(&amp;quot;na&amp;quot;, 2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;banana
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符替换&#34;&gt;字符替换&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;map&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Map(mapping func(rune) rune, s string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Map 函数，将 s 的每一个字符按照 mapping 的规则做映射替换，如果 mapping 返回值 &amp;lt;0 ，则舍弃该字符。该方法只能对每一个字符做处理，但处理方式很灵活，可以方便的过滤，筛选汉字等。&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mapping := func(r rune) rune {
    switch {
    case r &amp;gt;= &#39;A&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;Z&#39;: // 大写字母转小写
        return r + 32
    case r &amp;gt;= &#39;a&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;z&#39;: // 小写字母不处理
        return r
    case unicode.Is(unicode.Han, r): // 汉字换行
        return &#39;\n&#39;
    }
    return -1 // 过滤所有非字母、汉字的字符
}
fmt.Println(strings.Map(mapping, &amp;quot;Hello你#￥%……\n（&#39;World\n,好Hello^(&amp;amp;(*界gopher...&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hello
world
hello
gopher
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;replace&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;进行字符串替换时，考虑到性能问题，能不用正则尽量别用，应该用这里的函数。&lt;/p&gt;

&lt;p&gt;字符串替换的函数签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 用 new 替换 s 中的 old，一共替换 n 个。
// 如果 n &amp;lt; 0，则不限制替换次数，即全部替换
func Replace(s, old, new string, n int) string
// 该函数内部直接调用了函数 Replace(s, old, new , -1)
func ReplaceAll(s, old, new string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Replace(&amp;quot;oink oink oink&amp;quot;, &amp;quot;k&amp;quot;, &amp;quot;ky&amp;quot;, 2))
fmt.Println(strings.Replace(&amp;quot;oink oink oink&amp;quot;, &amp;quot;oink&amp;quot;, &amp;quot;moo&amp;quot;, -1))
fmt.Println(strings.ReplaceAll(&amp;quot;oink oink oink&amp;quot;, &amp;quot;oink&amp;quot;, &amp;quot;moo&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;oinky oinky oink
moo moo moo
moo moo moo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我们希望一次替换多个，比如我们希望替换 This is &lt;b&gt;HTML&lt;/b&gt; 中的 &amp;lt; 和 &amp;gt; 为 &amp;lt; 和 &amp;gt;，可以调用上面的函数两次。但标准库提供了另外的方法进行这种替换。&lt;/p&gt;

&lt;h2 id=&#34;大小写转换&#34;&gt;大小写转换&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func ToLower(s string) string
func ToLowerSpecial(c unicode.SpecialCase, s string) string
func ToUpper(s string) string
func ToUpperSpecial(c unicode.SpecialCase, s string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;大小写转换包含了 4 个相关函数，ToLower,ToUpper 用于大小写转换。ToLowerSpecial,ToUpperSpecial 可以转换特殊字符的大小写。 举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.ToLower(&amp;quot;HELLO WORLD&amp;quot;))
fmt.Println(strings.ToLower(&amp;quot;Ā Á Ǎ À&amp;quot;))
fmt.Println(strings.ToLowerSpecial(unicode.TurkishCase, &amp;quot;壹&amp;quot;))
fmt.Println(strings.ToLowerSpecial(unicode.TurkishCase, &amp;quot;HELLO WORLD&amp;quot;))
fmt.Println(strings.ToLower(&amp;quot;Önnek İş&amp;quot;))
fmt.Println(strings.ToLowerSpecial(unicode.TurkishCase, &amp;quot;Önnek İş&amp;quot;))

fmt.Println(strings.ToUpper(&amp;quot;hello world&amp;quot;))
fmt.Println(strings.ToUpper(&amp;quot;ā á ǎ à&amp;quot;))
fmt.Println(strings.ToUpperSpecial(unicode.TurkishCase, &amp;quot;一&amp;quot;))
fmt.Println(strings.ToUpperSpecial(unicode.TurkishCase, &amp;quot;hello world&amp;quot;))
fmt.Println(strings.ToUpper(&amp;quot;örnek iş&amp;quot;))
fmt.Println(strings.ToUpperSpecial(unicode.TurkishCase, &amp;quot;örnek iş&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hello world
ā á ǎ à
壹
hello world
önnek iş
önnek iş
HELLO WORLD
Ā Á Ǎ À       // 汉字拼音有效
一           //  汉字无效
HELLO WORLD
ÖRNEK IŞ
ÖRNEK İŞ    // 有细微差别
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;标题处理&#34;&gt;标题处理&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func Title(s string) string
func ToTitle(s string) string
func ToTitleSpecial(c unicode.SpecialCase, s string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;标题处理包含 3 个相关函数，其中 Title 会将 s 每个单词的首字母大写，不处理该单词的后续字符。ToTitle 将 s 的每个字母大写。ToTitleSpecial 将 s 的每个字母大写，并且会将一些特殊字母转换为其对应的特殊大写字母。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Title(&amp;quot;hElLo wOrLd&amp;quot;))
fmt.Println(strings.ToTitle(&amp;quot;hElLo wOrLd&amp;quot;))
fmt.Println(strings.ToTitleSpecial(unicode.TurkishCase, &amp;quot;hElLo wOrLd&amp;quot;))
fmt.Println(strings.Title(&amp;quot;āáǎà ōóǒò êēéěè&amp;quot;))
fmt.Println(strings.ToTitle(&amp;quot;āáǎà ōóǒò êēéěè&amp;quot;))
fmt.Println(strings.ToTitleSpecial(unicode.TurkishCase, &amp;quot;āáǎà ōóǒò êēéěè&amp;quot;))
fmt.Println(strings.Title(&amp;quot;dünyanın ilk borsa yapısı Aizonai kabul edilir&amp;quot;))
fmt.Println(strings.ToTitle(&amp;quot;dünyanın ilk borsa yapısı Aizonai kabul edilir&amp;quot;))
fmt.Println(strings.ToTitleSpecial(unicode.TurkishCase, &amp;quot;dünyanın ilk borsa yapısı Aizonai kabul edilir&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HElLo WOrLd
HELLO WORLD
HELLO WORLD
Āáǎà Ōóǒò Êēéěè
ĀÁǍÀ ŌÓǑÒ ÊĒÉĚÈ
ĀÁǍÀ ŌÓǑÒ ÊĒÉĚÈ
Dünyanın Ilk Borsa Yapısı Aizonai Kabul Edilir
DÜNYANIN ILK BORSA YAPISI AIZONAI KABUL EDILIR
DÜNYANIN İLK BORSA YAPISI AİZONAİ KABUL EDİLİR
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;修剪&#34;&gt;修剪&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 将 s 左侧和右侧中匹配 cutset 中的任一字符的字符去掉
func Trim(s string, cutset string) string
// 将 s 左侧的匹配 cutset 中的任一字符的字符去掉
func TrimLeft(s string, cutset string) string
// 将 s 右侧的匹配 cutset 中的任一字符的字符去掉
func TrimRight(s string, cutset string) string
// 如果 s 的前缀为 prefix 则返回去掉前缀后的 string , 否则 s 没有变化。
func TrimPrefix(s, prefix string) string
// 如果 s 的后缀为 suffix 则返回去掉后缀后的 string , 否则 s 没有变化。
func TrimSuffix(s, suffix string) string
// 将 s 左侧和右侧的间隔符去掉。常见间隔符包括：&#39;\t&#39;, &#39;\n&#39;, &#39;\v&#39;, &#39;\f&#39;, &#39;\r&#39;, &#39; &#39;, U+0085 (NEL)
func TrimSpace(s string) string
// 将 s 左侧和右侧的匹配 f 的字符去掉
func TrimFunc(s string, f func(rune) bool) string
// 将 s 左侧的匹配 f 的字符去掉
func TrimLeftFunc(s string, f func(rune) bool) string
// 将 s 右侧的匹配 f 的字符去掉
func TrimRightFunc(s string, f func(rune) bool) string
包含了 9 个相关函数用于修剪字符串。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x := &amp;quot;!!!@@@你好,!@#$ Gophers###$$$&amp;quot;
fmt.Println(strings.Trim(x, &amp;quot;@#$!%^&amp;amp;*()_+=-&amp;quot;))
fmt.Println(strings.TrimLeft(x, &amp;quot;@#$!%^&amp;amp;*()_+=-&amp;quot;))
fmt.Println(strings.TrimRight(x, &amp;quot;@#$!%^&amp;amp;*()_+=-&amp;quot;))
fmt.Println(strings.TrimSpace(&amp;quot; \t\n Hello, Gophers \n\t\r\n&amp;quot;))
fmt.Println(strings.TrimPrefix(x, &amp;quot;!&amp;quot;))
fmt.Println(strings.TrimSuffix(x, &amp;quot;$&amp;quot;))

f := func(r rune) bool {
    return !unicode.Is(unicode.Han, r) // 非汉字返回 true
}
fmt.Println(strings.TrimFunc(x, f))
fmt.Println(strings.TrimLeftFunc(x, f))
fmt.Println(strings.TrimRightFunc(x, f))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;你好,!@#$ Gophers
你好,!@#$ Gophers###$$$
!!!@@@你好,!@#$ Gophers
Hello, Gophers
!!@@@你好,!@#$ Gophers###$$$
!!!@@@你好,!@#$ Gophers###$$
你好
你好,!@#$ Gophers###$$$
!!!@@@你好
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;

&lt;h2 id=&#34;replacer-类型&#34;&gt;Replacer 类型&lt;/h2&gt;

&lt;p&gt;这是一个结构，没有导出任何字段，实例化通过 func NewReplacer(oldnew &amp;hellip;string) *Replacer 函数进行，其中不定参数 oldnew 是 old-new 对，即进行多个替换。如果 oldnew 长度与奇数，会导致 panic.&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r := strings.NewReplacer(&amp;quot;&amp;lt;&amp;quot;, &amp;quot;&amp;amp;lt;&amp;quot;, &amp;quot;&amp;gt;&amp;quot;, &amp;quot;&amp;amp;gt;&amp;quot;)
fmt.Println(r.Replace(&amp;quot;This is &amp;lt;b&amp;gt;HTML&amp;lt;/b&amp;gt;!&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This is &amp;amp;lt;b&amp;amp;gt;HTML&amp;amp;lt;/b&amp;amp;gt;!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，Replacer 还提供了另外一个方法，它在替换之后将结果写入 io.Writer 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Replacer) WriteString(w io.Writer, s string) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reader-类型&#34;&gt;Reader 类型&lt;/h2&gt;

&lt;p&gt;看到名字就能猜到，这是实现了 io 包中的接口。其实这就是缓存io，对缓存中的string进行读写操作。&lt;/p&gt;

&lt;p&gt;它实现了 io.Reader（Read 方法），io.ReaderAt（ReadAt 方法），io.Seeker（Seek 方法），io.WriterTo（WriteTo 方法），io.ByteReader（ReadByte 方法），io.ByteScanner（ReadByte 和 UnreadByte 方法），io.RuneReader（ReadRune 方法） 和 io.RuneScanner（ReadRune 和 UnreadRune 方法）。&lt;/p&gt;

&lt;p&gt;Reader 结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader struct {
    s        string    // Reader 读取的数据来源
    i        int // current reading index（当前读的索引位置）
    prevRune int // index of previous rune; or &amp;lt; 0（前一个读取的 rune 索引位置）
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见 Reader 结构没有导出任何字段，而是提供一个实例化方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(s string) *Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方法接收一个字符串，返回的 Reader 实例就是从该参数字符串读数据。&lt;/p&gt;

&lt;p&gt;在后面学习了 &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/#reader-类型&#34;&gt;bytes&lt;/a&gt; 包之后，可以知道 bytes.NewBufferString 有类似的功能，不过，如果只是为了读取，NewReader 会更高效。&lt;/p&gt;

&lt;h2 id=&#34;builder-类型&#34;&gt;Builder 类型&lt;/h2&gt;

&lt;p&gt;这个类型也是缓存io的一种实现方式。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Builder struct {
    addr *Builder // of receiver, to detect copies by value
    buf  []byte
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该类型实现了 io 包下的 Writer, ByteWriter, StringWriter 等接口，可以向该对象内写入数据，Builder 没有实现 Reader 等接口，所以该类型不可读，但提供了 String 方法可以获取对象内的数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 该方法向 b 写入一个字节
func (b *Builder) WriteByte(c byte) error
// WriteRune 方法向 b 写入一个字符
func (b *Builder) WriteRune(r rune) (int, error)
// WriteRune 方法向 b 写入字节数组 p
func (b *Builder) Write(p []byte) (int, error)
// WriteRune 方法向 b 写入字符串 s
func (b *Builder) WriteString(s string) (int, error)
// Len 方法返回 b 的数据长度。
func (b *Builder) Len() int
// Cap 方法返回 b 的 cap。
func (b *Builder) Cap() int
// Grow 方法将 b 的 cap 至少增加 n (可能会更多)。如果 n 为负数，会导致 panic。
func (b *Builder) Grow(n int)
// Reset 方法将 b 清空 b 的所有内容。
func (b *Builder) Reset()
// String 方法将 b 的数据以 string 类型返回。
func (b *Builder) String() string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Builder 有 4 个与写入相关的方法，这 4 个方法的 error 都总是为 nil.&lt;/p&gt;

&lt;p&gt;Builder 的 cap 会自动增长，一般不需要手动调用 Grow 方法。&lt;/p&gt;

&lt;p&gt;String 方法可以方便的获取 Builder 的内容。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b := strings.Builder{}
_ = b.WriteByte(&#39;7&#39;)
n, _ := b.WriteRune(&#39;夕&#39;)
fmt.Println(n)
n, _ = b.Write([]byte(&amp;quot;Hello, World&amp;quot;))
fmt.Println(n)
n, _ = b.WriteString(&amp;quot;你好，世界&amp;quot;)
fmt.Println(n)
fmt.Println(b.Len())
fmt.Println(b.Cap())
b.Grow(100)
fmt.Println(b.Len())
fmt.Println(b.Cap())
fmt.Println(b.String())
b.Reset()
fmt.Println(b.String())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3
12
15
31
32
31
164
7夕Hello, World你好，世界
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这边主要是要注意，使用返回值作为新值，原来值是不变的。&lt;/strong&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Strconv</title>
          <link>https://kingjcy.github.io/post/golang/go-strconv/</link>
          <pubDate>Wed, 12 Oct 2016 19:33:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-strconv/</guid>
          <description>&lt;p&gt;strconv包实现了基本数据类型和其字符串表示的相互转换。&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;p&gt;strconv主要就是字符之间的转化，我们直接看我们经常的使用就好。&lt;/p&gt;

&lt;h2 id=&#34;parseint&#34;&gt;ParseInt&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func ParseInt(s string, base int, bitSize int) (i int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回字符串表示的整数值，接受正负号。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;base指定进制（2到36），如果base为0，则会从字符串前置判断，&amp;rdquo;0x&amp;rdquo;是16进制，&amp;rdquo;0&amp;rdquo;是8进制，否则是10进制；&lt;/li&gt;
&lt;li&gt;bitSize指定结果必须能无溢出赋值的整数类型，0、8、16、32、64 分别代表 int、int8、int16、int32、int64；返回的err是*NumErr类型的，如果语法有误，err.Error = ErrSyntax；如果结果超出类型范围err.Error = ErrRange。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;int和string的转化&#34;&gt;int和string的转化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;int转string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;s := strconv.Itoa(i)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等价于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s := strconv.FormatInt(int64(i), 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;int64转string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;i := int64(123)
s := strconv.FormatInt(i, 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二个参数为基数，可选2~36&lt;/p&gt;

&lt;p&gt;注：对于无符号整形，可以使用FormatUint(i uint64, base int)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;string转int&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;i, err := strconv.Atoi(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;string转int64&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;i, err := strconv.ParseInt(s, 10, 64)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二个参数为基数（2~36），第三个参数位大小表示期望转换的结果类型，其值可以为0, 8, 16, 32和64，分别对应 int, int8, int16, int32和int64&lt;/p&gt;

&lt;h2 id=&#34;float相关&#34;&gt;float相关&lt;/h2&gt;

&lt;p&gt;float转string：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;v := 3.1415926535
s1 := strconv.FormatFloat(v, &#39;E&#39;, -1, 32)//float32s2 := strconv.FormatFloat(v, &#39;E&#39;, -1, 64)//float64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数原型及参数含义具体可查看：&lt;a href=&#34;https://golang.org/pkg/strconv/#FormatFloat&#34;&gt;https://golang.org/pkg/strconv/#FormatFloat&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;string转float：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s := &amp;quot;3.1415926535&amp;quot;
v1, err := strconv.ParseFloat(v, 32)
v2, err := strconv.ParseFloat(v, 64)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;error相关&#34;&gt;error相关&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;error转string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;err.Error()
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Io</title>
          <link>https://kingjcy.github.io/post/golang/go-io/</link>
          <pubDate>Sat, 30 Jul 2016 20:39:03 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-io/</guid>
          <description>&lt;p&gt;io包提供了所有需要交互的输入输出模式的基础。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;stream&#34;&gt;stream&lt;/h2&gt;

&lt;p&gt;我们先介绍一下stream的概念。stream就是数据流，数据流的概念其实非常基础，最早是在通讯领域使用的概念，这个概念最初在 1998 年由 Henzinger 在文献中提出，他将数据流定义为 “只能以事先规定好的顺序被读取一次的数据的一个序列”&lt;/p&gt;

&lt;p&gt;数据流就是由数据形成的流，就像由水形成的水流，非常形象，现代语言中，基本上都会有流的支持，比如 C++ 的 iostream，Node.js 的 stream 模块，以及 golang 的 io 包。&lt;/p&gt;

&lt;p&gt;Stream in Golang与流密切相关的就是 bufio io io/ioutil 这几个包：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、io 为 IO 原语（I/O primitives）提供基本的接口
2、io/ioutil 封装一些实用的 I/O 函数
3、fmt 实现格式化 I/O，类似 C 语言中的 printf 和 scanf
4、bufio 实现带缓冲I/O
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;io&#34;&gt;io&lt;/h2&gt;

&lt;p&gt;io 包为 I/O 原语提供了基本的接口。在 io 包中最重要的是两个接口：Reader 和 Writer 接口。本章所提到的各种 IO 包，都跟这两个接口有关，也就是说，只要满足这两个接口，它就可以使用 IO 包的功能。&lt;/p&gt;

&lt;h1 id=&#34;接口&#34;&gt;接口&lt;/h1&gt;

&lt;h2 id=&#34;读取器&#34;&gt;读取器&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;type Reader interface {
    //Read() 方法有两个返回值，一个是读取到的字节数，一个是发生错误时的错误。如果资源内容已全部读取完毕，应该返回 io.EOF 错误。
    Read(p []byte) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;io.Reader 表示一个读取器，它将数据从某个资源读取到传输缓冲区p。在缓冲区中，数据可以被流式传输和使用。&lt;/p&gt;

&lt;p&gt;实现这个接口需要实现如下功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Read 将 len(p) 个字节读取到 p 中。它返回读取的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)） 以及任何遇到的错误。

&lt;ul&gt;
&lt;li&gt;即使 Read 返回的 n &amp;lt; len(p)，它也会在调用过程中占用 len(p) 个字节作为暂存空间。&lt;/li&gt;
&lt;li&gt;若可读取的数据不到 len(p) 个字节，Read 会返回可用数据，而不是等待更多数据。&lt;/li&gt;
&lt;li&gt;当读取的时候没有数据也没有EOF的时候，会阻塞在这边等待。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;当 Read 在成功读取 n &amp;gt; 0 个字节后遇到一个错误或 EOF (end-of-file)，它会返回读取的字节数。

&lt;ul&gt;
&lt;li&gt;它可能会同时在本次的调用中返回一个non-nil错误,或在下一次的调用中返回这个错误（且 n 为 0）。&lt;/li&gt;
&lt;li&gt;一般情况下, Reader会返回一个非0字节数n, 若 n = len(p) 个字节从输入源的结尾处由 Read 返回，Read可能返回 err == EOF 或者 err == nil。并且之后的 Read() 都应该返回 (n:0, err:EOF)。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;调用者在考虑错误之前应当首先处理返回的数据。这样做可以正确地处理在读取一些字节后产生的 I/O 错误，同时允许EOF的出现。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于要用作读取器的类型，它必须实现 io.Reader 接口的唯一一个方法 Read(p []byte)。换句话说，只要实现了 Read(p []byte) ，那它就是一个读取器，使用标准库中已经实现的读写器，来举例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    reader := strings.NewReader(&amp;quot;Clear is better than clever&amp;quot;)
    p := make([]byte, 4)

    for {
        n, err := reader.Read(p)
        if err != nil{
            if err == io.EOF {
                fmt.Println(&amp;quot;EOF:&amp;quot;, n)
                break
            }
            fmt.Println(err)
            os.Exit(1)
        }
        fmt.Println(n, string(p[:n]))
    }
}

输出打印的内容：

4 Clea
4 r is
4  bet
4 ter 
4 than
4  cle
3 ver
EOF: 0 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;自定义reader&#34;&gt;自定义Reader&lt;/h3&gt;

&lt;p&gt;现在，让我们看看如何自己实现一个。它的功能是从流中过滤掉非字母字符。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type alphaReader struct {
    // 资源
    src string
    // 当前读取到的位置 
    cur int
}

// 创建一个实例
func newAlphaReader(src string) *alphaReader {
    return &amp;amp;alphaReader{src: src}
}

// 过滤函数
func alpha(r byte) byte {
    if (r &amp;gt;= &#39;A&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;Z&#39;) || (r &amp;gt;= &#39;a&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;z&#39;) {
        return r
    }
    return 0
}

// Read 方法，read函数是阻塞的
func (a *alphaReader) Read(p []byte) (int, error) {
    // 当前位置 &amp;gt;= 字符串长度 说明已经读取到结尾 返回 EOF
    if a.cur &amp;gt;= len(a.src) {
        return 0, io.EOF
    }

    // x 是剩余未读取的长度
    x := len(a.src) - a.cur
    n, bound := 0, 0
    if x &amp;gt;= len(p) {
        // 剩余长度超过缓冲区大小，说明本次可完全填满缓冲区
        bound = len(p)
    } else if x &amp;lt; len(p) {
        // 剩余长度小于缓冲区大小，使用剩余长度输出，缓冲区不补满
        bound = x
    }

    buf := make([]byte, bound)
    for n &amp;lt; bound {
        // 每次读取一个字节，执行过滤函数
        if char := alpha(a.src[a.cur]); char != 0 {
            buf[n] = char
        }
        n++
        a.cur++
    }
    // 将处理后得到的 buf 内容复制到 p 中
    copy(p, buf)
    return n, nil
}

func main() {
    reader := newAlphaReader(&amp;quot;Hello! It&#39;s 9am, where is the sun?&amp;quot;)
    p := make([]byte, 4)
    for {
        n, err := reader.Read(p)
        if err == io.EOF {
            break
        }
        fmt.Print(string(p[:n]))
    }
    fmt.Println()
}
输出打印的内容：
HelloItsamwhereisthesun
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;tcp粘包拆包&#34;&gt;TCP粘包拆包&lt;/h3&gt;

&lt;p&gt;这边讲解一下TCP粘包拆包问题，先看下面这个实例&lt;/p&gt;

&lt;p&gt;服务端代码 server/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    l, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:4044&amp;quot;)
    if err != nil {
        panic(err)
    }
    fmt.Println(&amp;quot;listen to 4044&amp;quot;)
    for {
        // 监听到新的连接，创建新的 goroutine 交给 handleConn函数 处理
        conn, err := l.Accept()
        if err != nil {
            fmt.Println(&amp;quot;conn err:&amp;quot;, err)
        } else {
            go handleConn(conn)
        }
    }
}

func handleConn(conn net.Conn) {
    defer conn.Close()
    defer fmt.Println(&amp;quot;关闭&amp;quot;)
    fmt.Println(&amp;quot;新连接：&amp;quot;, conn.RemoteAddr())

    result := bytes.NewBuffer(nil)
    var buf [1024]byte
    for {
        n, err := conn.Read(buf[0:])
        result.Write(buf[0:n])
        if err != nil {
            if err == io.EOF {
                continue
            } else {
                fmt.Println(&amp;quot;read err:&amp;quot;, err)
                break
            }
        } else {
            fmt.Println(&amp;quot;recv:&amp;quot;, result.String())
        }
        result.Reset()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端代码 client/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    data := []byte(&amp;quot;[这里才是一个完整的数据包]&amp;quot;)
    conn, err := net.DialTimeout(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:4044&amp;quot;, time.Second*30)
    if err != nil {
        fmt.Printf(&amp;quot;connect failed, err : %v\n&amp;quot;, err.Error())
        return
    }
    for i := 0; i &amp;lt;1000; i++ {
        _, err = conn.Write(data)
        if err != nil {
            fmt.Printf(&amp;quot;write failed , err : %v\n&amp;quot;, err)
            break
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;listen to 4044
新连接： [::1]:53079
recv: [这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据�
recv: �][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
...省略其它的...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从服务端的控制台输出可以看出，存在三种类型的输出：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一种是正常的一个数据包输出。&lt;/li&gt;
&lt;li&gt;一种是多个数据包“粘”在了一起，我们定义这种读到的包为粘包。&lt;/li&gt;
&lt;li&gt;一种是一个数据包被“拆”开，形成一个破碎的包，我们定义这种包为半包。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为什么会出现半包和粘包？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;客户端一段时间内发送包的速度太多，服务端没有全部处理完。于是数据就会积压起来，产生粘包。&lt;/li&gt;
&lt;li&gt;定义的读的buffer不够大，而数据包太大或者由于粘包产生，服务端不能一次全部读完，产生半包。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;什么时候需要考虑处理半包和粘包？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TCP连接是长连接，即一次连接多次发送数据。&lt;/li&gt;
&lt;li&gt;每次发送的数据是结构的，比如 JSON格式的数据 或者 数据包的协议是由我们自己定义的（包头部包含实际数据长度、协议魔数等）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解决思路&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;定长分隔(每个数据包最大为该长度，不足时使用特殊字符填充) ，但是数据不足时会浪费传输资源&lt;/li&gt;
&lt;li&gt;使用特定字符来分割数据包，但是若数据中含有分割字符则会出现Bug&lt;/li&gt;
&lt;li&gt;在数据包中添加长度字段，弥补了以上两种思路的不足，推荐使用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上述分析，我们最好通过第三种思路来解决拆包粘包问题。&lt;/p&gt;

&lt;p&gt;Golang的bufio库中有为我们提供了Scanner，来解决这类分割数据的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Scanner
Scanner provides a convenient interface for reading data such as a file of newline-delimited lines of text. Successive calls to the Scan method will step through the &#39;tokens&#39; of a file, skipping the bytes between the tokens. The specification of a token is defined by a split function of type SplitFunc; the default split function breaks the input into lines with line termination stripped. Split functions are defined in this package for scanning a file into lines, bytes, UTF-8-encoded runes, and space-delimited words. The client may instead provide a custom split function.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单来讲即是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Scanner为 读取数据 提供了方便的 接口。连续调用Scan方法会逐个得到文件的“tokens”，跳过 tokens 之间的字节。token 的规范由 SplitFunc 类型的函数定义。我们可以改为提供自定义拆分功能。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来看看 SplitFunc 类型的函数是什么样子的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SplitFunc func(data []byte, atEOF bool) (advance int, token []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    // An artificial input source.
    const input = &amp;quot;1234 5678 1234567901234567890&amp;quot;
    scanner := bufio.NewScanner(strings.NewReader(input))
    // Create a custom split function by wrapping the existing ScanWords function.
    split := func(data []byte, atEOF bool) (advance int, token []byte, err error) {
        advance, token, err = bufio.ScanWords(data, atEOF)
        if err == nil &amp;amp;&amp;amp; token != nil {
            _, err = strconv.ParseInt(string(token), 10, 32)
        }
        return
    }
    // Set the split function for the scanning operation.
    scanner.Split(split)
    // Validate the input
    for scanner.Scan() {
        fmt.Printf(&amp;quot;%s\n&amp;quot;, scanner.Text())
    }

    if err := scanner.Err(); err != nil {
        fmt.Printf(&amp;quot;Invalid input: %s&amp;quot;, err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;于是，我们可以这样改写我们的程序：&lt;/p&gt;

&lt;p&gt;服务端代码 server/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    l, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:4044&amp;quot;)
    if err != nil {
        panic(err)
    }
    fmt.Println(&amp;quot;listen to 4044&amp;quot;)
    for {
        conn, err := l.Accept()
        if err != nil {
            fmt.Println(&amp;quot;conn err:&amp;quot;, err)
        } else {
            go handleConn2(conn)
        }
    }
}

func packetSlitFunc(data []byte, atEOF bool) (advance int, token []byte, err error) {
        // 检查 atEOF 参数 和 数据包头部的四个字节是否 为 0x123456(我们定义的协议的魔数)
    if !atEOF &amp;amp;&amp;amp; len(data) &amp;gt; 6 &amp;amp;&amp;amp; binary.BigEndian.Uint32(data[:4]) == 0x123456 {
        var l int16
                // 读出 数据包中 实际数据 的长度(大小为 0 ~ 2^16)
        binary.Read(bytes.NewReader(data[4:6]), binary.BigEndian, &amp;amp;l)
        pl := int(l) + 6
        if pl &amp;lt;= len(data) {
            return pl, data[:pl], nil
        }
    }
    return
}

func handleConn2(conn net.Conn) {
    defer conn.Close()
    defer fmt.Println(&amp;quot;关闭&amp;quot;)
    fmt.Println(&amp;quot;新连接：&amp;quot;, conn.RemoteAddr())
    result := bytes.NewBuffer(nil)
        var buf [65542]byte // 由于 标识数据包长度 的只有两个字节 故数据包最大为 2^16+4(魔数)+2(长度标识)
    for {
        n, err := conn.Read(buf[0:])
        result.Write(buf[0:n])
        if err != nil {
            if err == io.EOF {
                continue
            } else {
                fmt.Println(&amp;quot;read err:&amp;quot;, err)
                break
            }
        } else {
            scanner := bufio.NewScanner(result)
            scanner.Split(packetSlitFunc)
            for scanner.Scan() {
                fmt.Println(&amp;quot;recv:&amp;quot;, string(scanner.Bytes()[6:]))
            }
        }
        result.Reset()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;复制代码客户端代码 client/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    data := []byte(&amp;quot;[这里才是一个完整的数据包]&amp;quot;)
    l := len(data)
    fmt.Println(l)
    magicNum := make([]byte, 4)
    binary.BigEndian.PutUint32(magicNum, 0x123456)
    lenNum := make([]byte, 2)
    binary.BigEndian.PutUint16(lenNum, uint16(l))
    packetBuf := bytes.NewBuffer(magicNum)
    packetBuf.Write(lenNum)
    packetBuf.Write(data)
    conn, err := net.DialTimeout(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:4044&amp;quot;, time.Second*30)
    if err != nil {
        fmt.Printf(&amp;quot;connect failed, err : %v\n&amp;quot;, err.Error())
                return
    }
    for i := 0; i &amp;lt;1000; i++ {
        _, err = conn.Write(packetBuf.Bytes())
        if err != nil {
            fmt.Printf(&amp;quot;write failed , err : %v\n&amp;quot;, err)
            break
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;复制代码运行结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;listen to 4044
新连接： [::1]:55738
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
...省略其它的...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;编写器&#34;&gt;编写器&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;type Writer
type Writer interface {
    //Write() 方法有两个返回值，一个是写入到目标资源的字节数，一个是发生错误时的错误。
    Write(p []byte) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;io.Writer 表示一个编写器，它从缓冲区读取数据，并将数据写入目标资源。&lt;/p&gt;

&lt;p&gt;实现这个接口就需要实现如下的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Write 将 len(p) 个字节从 p 中写入到基本数据流中。它返回从 p 中被写入的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)）以及任何遇到的引起写入提前停止的错误。若 Write 返回的 n &amp;lt; len(p)，它就必须返回一个 非nil 的错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于要用作编写器的类型，必须实现 io.Writer 接口的唯一一个方法 Write(p []byte),同样，只要实现了 Write(p []byte) ，那它就是一个编写器。举例，标准库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    proverbs := []string{
        &amp;quot;Channels orchestrate mutexes serialize&amp;quot;,
        &amp;quot;Cgo is not Go&amp;quot;,
        &amp;quot;Errors are values&amp;quot;,
        &amp;quot;Don&#39;t panic&amp;quot;,
    }
    var writer bytes.Buffer

    for _, p := range proverbs {
        n, err := writer.Write([]byte(p))
        if err != nil {
            fmt.Println(err)
            os.Exit(1)
        }
        if n != len(p) {
            fmt.Println(&amp;quot;failed to write data&amp;quot;)
            os.Exit(1)
        }
    }

    fmt.Println(writer.String())
}
输出打印的内容：
Channels orchestrate mutexes serializeCgo is not GoErrors are valuesDon&#39;t panic
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;自定义writer&#34;&gt;自定义Writer&lt;/h3&gt;

&lt;p&gt;下面我们来实现一个名为 chanWriter 的自定义 io.Writer ，它将其内容作为字节序列写入 channel 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type chanWriter struct {
    // ch 实际上就是目标资源
    ch chan byte
}

func newChanWriter() *chanWriter {
    return &amp;amp;chanWriter{make(chan byte, 1024)}
}

func (w *chanWriter) Chan() &amp;lt;-chan byte {
    return w.ch
}

func (w *chanWriter) Write(p []byte) (int, error) {
    n := 0
    // 遍历输入数据，按字节写入目标资源
    for _, b := range p {
        w.ch &amp;lt;- b
        n++
    }
    return n, nil
}

func (w *chanWriter) Close() error {
    close(w.ch)
    return nil
}

func main() {
    writer := newChanWriter()
    go func() {
        defer writer.Close()
        writer.Write([]byte(&amp;quot;Stream &amp;quot;))
        writer.Write([]byte(&amp;quot;me!&amp;quot;))
    }()
    for c := range writer.Chan() {
        fmt.Printf(&amp;quot;%c&amp;quot;, c)
    }
    fmt.Println()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要使用这个 Writer，只需在函数 main() 中调用 writer.Write()（在单独的goroutine中）。&lt;/p&gt;

&lt;p&gt;因为 chanWriter 还实现了接口 io.Closer ，所以调用方法 writer.Close() 来正确地关闭channel，以避免发生泄漏和死锁。&lt;/p&gt;

&lt;h2 id=&#34;closer&#34;&gt;closer&lt;/h2&gt;

&lt;p&gt;Closer 接口包装了基本的 Close 方法，用于关闭数据读写。Close 一般用于关闭文件，关闭通道，关闭连接，关闭数据库等，在不同的标准库实现中实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Closer interface {
    Close() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;seeker&#34;&gt;seeker&lt;/h2&gt;

&lt;p&gt;Seeker 接口包装了基本的 Seek 方法，用于移动数据的读写指针。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Seeker interface {
    Seek(offset int64, whence int) (ret int64, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seek 设置下一次读写操作的指针位置，每次的读写操作都是从指针位置开始的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;whence 的含义：

&lt;ul&gt;
&lt;li&gt;如果 whence 为 0：表示从数据的开头开始移动指针。&lt;/li&gt;
&lt;li&gt;如果 whence 为 1：表示从数据的当前指针位置开始移动指针。&lt;/li&gt;
&lt;li&gt;如果 whence 为 2：表示从数据的尾部开始移动指针。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;offset 是指针移动的偏移量。&lt;/li&gt;
&lt;li&gt;返回新指针位置和遇到的错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;whence 的值，在 io 包中定义了相应的常量，应该使用这些常量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
  SeekStart   = 0 // seek relative to the origin of the file
  SeekCurrent = 1 // seek relative to the current offset
  SeekEnd     = 2 // seek relative to the end
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而原先 os 包中的常量已经被标注为Deprecated&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Deprecated: Use io.SeekStart, io.SeekCurrent, and io.SeekEnd.
const (
  SEEK_SET int = 0 // seek relative to the origin of the file
  SEEK_CUR int = 1 // seek relative to the current offset
  SEEK_END int = 2 // seek relative to the end
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;组合接口&#34;&gt;组合接口&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;type ReadWriter interface {
    Reader
    Writer
}

type ReadSeeker interface {
    Reader
    Seeker
}

type WriteSeeker interface {
    Writer
    Seeker
}

type ReadWriteSeeker interface {
    Reader
    Writer
    Seeker
}

type ReadCloser interface {
    Reader
    Closer
}

type WriteCloser interface {
    Writer
    Closer
}

type ReadWriteCloser interface {
    Reader
    Writer
    Closer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些接口的作用是：有些时候同时需要某两个接口的所有功能，即必须同时实现了某两个接口的类型才能够被传入使用。可见，io 包中有大量的“小接口”，这样方便组合为“大接口”。&lt;/p&gt;

&lt;h2 id=&#34;其他接口&#34;&gt;其他接口&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;ReaderFrom&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReaderFrom 接口包装了基本的 ReadFrom 方法，用于从 r 中读取数据存入自身。直到遇到 EOF 或读取出错为止，返回读取的字节数和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ReaderFrom interface {
    ReadFrom(r Reader) (n int64, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ReadFrom 从 r 中读取数据，直到 EOF 或发生错误。其返回值 n 为读取的字节数。除 io.EOF 之外，在读取过程中遇到的任何错误也将被返回。&lt;/li&gt;
&lt;li&gt;如果 ReaderFrom 可用，Copy 函数就会使用它。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例：将文件中的数据全部读取（显示在标准输出）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file, err := os.Open(&amp;quot;writeAt.txt&amp;quot;)
if err != nil {
    panic(err)
}
defer file.Close()
writer := bufio.NewWriter(os.Stdout)
writer.ReadFrom(file)
writer.Flush()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，我们可以通过 ioutil 包的 ReadFile 函数获取文件全部内容。其实，跟踪一下 ioutil.ReadFile 的源码，会发现其实也是通过 ReadFrom 方法实现（用的是 bytes.Buffer，它实现了 ReaderFrom 接口）。&lt;/p&gt;

&lt;p&gt;如果不通过 ReadFrom 接口来做这件事，而是使用 io.Reader 接口，我们有两种思路：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;先获取文件的大小（File 的 Stat 方法），之后定义一个该大小的 []byte，通过 Read 一次性读取&lt;/li&gt;
&lt;li&gt;定义一个小的 []byte，不断的调用 Read 方法直到遇到 EOF，将所有读取到的 []byte 连接到一起&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;WriterTo&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;WriterTo 接口包装了基本的 WriteTo 方法，用于将自身的数据写入 w 中。直到数据全部写入完毕或遇到错误为止，返回写入的字节数和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type WriterTo interface {
    WriteTo(w Writer) (n int64, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;WriteTo 将数据写入 w 中，直到没有数据可写或发生错误。其返回值 n 为写入的字节数。 在写入过程中遇到的任何错误也将被返回。&lt;/li&gt;
&lt;li&gt;如果 WriterTo 可用，Copy 函数就会使用它。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;读者是否发现，其实 ReaderFrom 和 WriterTo 接口的方法接收的参数是 io.Reader 和 io.Writer 类型。根据 io.Reader 和 io.Writer 接口的讲解，对该接口的使用应该可以很好的掌握。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ReaderAt&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReaderAt 接口包装了基本的 ReadAt 方法，用于将自身的数据写入 p 中。ReadAt 忽略之前的读写位置，从起始位置的 off 偏移处开始读取。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ReaderAt interface {
    ReadAt(p []byte, off int64) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ReadAt 从基本输入源的偏移量 off 处开始，将 len(p) 个字节读取到 p 中。它返回读取的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)）以及任何遇到的错误。&lt;/li&gt;
&lt;li&gt;当 ReadAt 返回的 n &amp;lt; len(p) 时，它就会返回一个 非nil 的错误来解释 为什么没有返回更多的字节。在这一点上，ReadAt 比 Read 更严格。&lt;/li&gt;
&lt;li&gt;即使 ReadAt 返回的 n &amp;lt; len(p)，它也会在调用过程中使用 p 的全部作为暂存空间。若可读取的数据不到 len(p) 字节，ReadAt 就会阻塞,直到所有数据都可用或一个错误发生。 在这一点上 ReadAt 不同于 Read。&lt;/li&gt;
&lt;li&gt;若 n = len(p) 个字节从输入源的结尾处由 ReadAt 返回，Read可能返回 err == EOF 或者 err == nil&lt;/li&gt;
&lt;li&gt;若 ReadAt 携带一个偏移量从输入源读取，ReadAt 应当既不影响偏移量也不被它所影响。&lt;/li&gt;
&lt;li&gt;可对相同的输入源并行执行 ReadAt 调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;标准库上面说的很多都是实现了这个接口，简单示例代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := strings.NewReader(&amp;quot;Go语言中文网&amp;quot;)
p := make([]byte, 6)
n, err := reader.ReadAt(p, 2)
if err != nil {
    panic(err)
}
fmt.Printf(&amp;quot;%s, %d\n&amp;quot;, p, n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;语言, 6
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;WriterAt&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;WriterAt 接口包装了基本的 WriteAt 方法，用于将 p 中的数据写入自身。ReadAt 忽略之前的读写位置，从起始位置的 off 偏移处开始写入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type WriterAt interface {
    WriteAt(p []byte, off int64) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;WriteAt 从 p 中将 len(p) 个字节写入到偏移量 off 处的基本数据流中。它返回从 p 中被写入的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)）以及任何遇到的引起写入提前停止的错误。若 WriteAt 返回的 n &amp;lt; len(p)，它就必须返回一个 非nil 的错误。&lt;/li&gt;
&lt;li&gt;若 WriteAt 携带一个偏移量写入到目标中，WriteAt 应当既不影响偏移量也不被它所影响。&lt;/li&gt;
&lt;li&gt;若被写区域没有重叠，可对相同的目标并行执行 WriteAt 调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;os.File 实现了 WriterAt 接口，实例如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file, err := os.Create(&amp;quot;writeAt.txt&amp;quot;)
if err != nil {
    panic(err)
}
defer file.Close()
file.WriteString(&amp;quot;Golang中文社区——这里是多余&amp;quot;)
n, err := file.WriteAt([]byte(&amp;quot;Go语言中文网&amp;quot;), 24)
if err != nil {
    panic(err)
}
fmt.Println(n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Golang中文社区——Go语言中文网。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分析：file.WriteString(&amp;ldquo;Golang中文社区——这里是多余&amp;rdquo;) 往文件中写入 Golang中文社区——这里是多余，之后 file.WriteAt([]byte(&amp;ldquo;Go语言中文网&amp;rdquo;), 24) 在文件流的 offset=24 处写入 Go语言中文网（会覆盖该位置的内容）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ByteReader和ByteWriter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ByteReader 接口包装了基本的 ReadByte 方法，用于从自身读出一个字节。返回读出的字节和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ByteReader interface {
    ReadByte() (c byte, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ByteWriter 接口包装了基本的 WriteByte 方法，用于将一个字节写入自身返回遇到的错误&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ByteWriter interface {
    WriteByte(c byte) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这组接口在标准库中也有实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bufio.Reader/Writer 分别实现了io.ByteReader 和 io.ByteWriter
bytes.Buffer 同时实现了 io.ByteReader 和 io.ByteWriter
bytes.Reader 实现了 io.ByteReader
strings.Reader 实现了 io.ByteReader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ch byte
fmt.Scanf(&amp;quot;%c\n&amp;quot;, &amp;amp;ch)

buffer := new(bytes.Buffer)
err := buffer.WriteByte(ch)
if err == nil {
    fmt.Println(&amp;quot;写入一个字节成功！准备读取该字节……&amp;quot;)
    newCh, _ := buffer.ReadByte()
    fmt.Printf(&amp;quot;读取的字节：%c\n&amp;quot;, newCh)
} else {
    fmt.Println(&amp;quot;写入错误&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;ByteScanner&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ByteScanner 在 ByteReader 的基础上增加了一个 UnreadByte 方法，用于撤消最后一次的 ReadByte 操作，以便下次的 ReadByte 操作可以读出与前一次一样的数据。UnreadByte 之前必须是 ReadByte 才能撤消成功，否则可能会返回一个错误信息（根据不同的需求，UnreadByte 也可能返回 nil，允许随意调用 UnreadByte，但只有最后一次的 ReadByte 可以被撤销，其它 UnreadByte 不执行任何操作）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ByteScanner interface {
    ByteReader
    UnreadByte() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;RuneReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;RuneReader 接口包装了基本的 ReadRune 方法，用于从自身读取一个 UTF-8 编码的字符到 r 中。返回读取的字符、字符的编码长度和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type RuneReader interface {
    ReadRune() (r rune, size int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;RuneScanner&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;RuneScanner 在 RuneReader 的基础上增加了一个 UnreadRune 方法，用于撤消最后一次的 ReadRune 操作，以便下次的 ReadRune 操作可以读出与前一次一样的数据。UnreadRune 之前必须是 ReadRune 才能撤消成功，否则可能会返回一个错误信息（根据不同的需求，UnreadRune 也可能返回 nil，允许随意调用 UnreadRune，但只有最后一次的 ReadRune 可以被撤销，其它 UnreadRune 不执行任何操作）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type RuneScanner interface {
    RuneReader
    UnreadRune() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;bytes.NewBuffer 实现了很多基本的接口，可以通过 bytes 包学习接口的实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    buf := bytes.NewBuffer([]byte(&amp;quot;Hello World!&amp;quot;))
    b := make([]byte, buf.Len())

    n, err := buf.Read(b)
    fmt.Printf(&amp;quot;%s   %v\n&amp;quot;, b[:n], err)
    // Hello World!   &amp;lt;nil&amp;gt;

    buf.WriteString(&amp;quot;ABCDEFG\n&amp;quot;)
    buf.WriteTo(os.Stdout)
    // ABCDEFG

    n, err = buf.Write(b)
    fmt.Printf(&amp;quot;%d   %s   %v\n&amp;quot;, n, buf.String(), err)
    // 12   Hello World!   &amp;lt;nil&amp;gt;

    c, err := buf.ReadByte()
    fmt.Printf(&amp;quot;%c   %s   %v\n&amp;quot;, c, buf.String(), err)
    // H   ello World!   &amp;lt;nil&amp;gt;

    c, err = buf.ReadByte()
    fmt.Printf(&amp;quot;%c   %s   %v\n&amp;quot;, c, buf.String(), err)
    // e   llo World!   &amp;lt;nil&amp;gt;

    err = buf.UnreadByte()
    fmt.Printf(&amp;quot;%s   %v\n&amp;quot;, buf.String(), err)
    // ello World!   &amp;lt;nil&amp;gt;

    err = buf.UnreadByte()
    fmt.Printf(&amp;quot;%s   %v\n&amp;quot;, buf.String(), err)
    // ello World!   bytes.Buffer: UnreadByte: previous operation was not a read
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;

&lt;p&gt;io包中定义了很多原生的类型。都是实现了上面的接口，可以直接创建使用的类型。&lt;/p&gt;

&lt;h2 id=&#34;sectionreader-类型&#34;&gt;SectionReader 类型&lt;/h2&gt;

&lt;p&gt;SectionReader 是一个 struct（没有任何导出的字段），实现了 Read, Seek 和 ReadAt，同时，内嵌了 ReaderAt 接口。结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SectionReader struct {
    r     ReaderAt    // 该类型最终的 Read/ReadAt 最终都是通过 r 的 ReadAt 实现
    base  int64        // NewSectionReader 会将 base 设置为 off
    off   int64        // 从 r 中的 off 偏移处开始读取数据
    limit int64        // limit - off = SectionReader 流的长度
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从名称我们可以猜到，该类型读取数据流中部分数据。看一下常见的创建函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewSectionReader(r ReaderAt, off int64, n int64) *SectionReader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewSectionReader 返回一个 SectionReader，它从 r 中的偏移量 off 处读取 n 个字节后以 EOF 停止。也就是说，SectionReader 只是内部（内嵌）ReaderAt 表示的数据流的一部分：从 off 开始后的 n 个字节。这个类型的作用是：方便重复操作某一段 (section) 数据流；或者同时需要 ReadAt 和 Seek 的功能。&lt;/p&gt;

&lt;h2 id=&#34;limitedreader-类型&#34;&gt;LimitedReader 类型&lt;/h2&gt;

&lt;p&gt;LimitedReader 结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type LimitedReader struct {
    R Reader // underlying reader，最终的读取操作通过 R.Read 完成
    N int64  // max bytes remaining
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从 R 读取但将返回的数据量限制为 N 字节。每调用一次 Read 都将更新 N 来反应新的剩余数量。也就是说，最多只能返回 N 字节数据。LimitedReader 只实现了 Read 方法（Reader 接口）。&lt;/p&gt;

&lt;p&gt;使用示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;content := &amp;quot;This Is LimitReader Example&amp;quot;
reader := strings.NewReader(content)
limitReader := &amp;amp;io.LimitedReader{R: reader, N: 8}
for limitReader.N &amp;gt; 0 {
    tmp := make([]byte, 2)
    limitReader.Read(tmp)
    fmt.Printf(&amp;quot;%s&amp;quot;, tmp)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This Is
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，通过该类型可以达到 只允许读取一定长度数据 的目的。&lt;/p&gt;

&lt;p&gt;在 io 包中，LimitReader 函数的实现其实就是调用 LimitedReader：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LimitReader(r Reader, n int64) Reader { return &amp;amp;LimitedReader{r, n} }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pipereader-和-pipewriter-类型&#34;&gt;PipeReader 和 PipeWriter 类型&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;PipeReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PipeReader（一个没有任何导出字段的 struct）是管道的读取端。它实现了 io.Reader 和 io.Closer 接口。结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PipeReader struct {
    p *pipe
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 PipeReader.Read 方法的说明：从管道中读取数据。该方法会堵塞，直到管道写入端开始写入数据或写入端被关闭。如果写入端关闭时带有 error（即调用 CloseWithError 关闭），该Read返回的 err 就是写入端传递的error；否则 err 为 EOF。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;PipeWriter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PipeWriter（一个没有任何导出字段的 struct）是管道的写入端。它实现了 io.Writer 和 io.Closer 接口。结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PipeWriter struct {
    p *pipe
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 PipeWriter.Write 方法的说明：写数据到管道中。该方法会堵塞，直到管道读取端读完所有数据或读取端被关闭。如果读取端关闭时带有 error（即调用 CloseWithError 关闭），该Write返回的 err 就是读取端传递的error；否则 err 为 ErrClosedPipe。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Pipe&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;io.Pipe() 用于创建一个同步的内存管道 (synchronous in-memory pipe)，函数签名：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Pipe() (*PipeReader, *PipeWriter)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它将 io.Reader 连接到 io.Writer。一端的读取匹配另一端的写入，直接在这两端之间复制数据；它没有内部缓存。它对于并行调用 Read 和 Write 以及其它函数或 Close 来说都是安全的。一旦等待的 I/O 结束，Close 就会完成。并行调用 Read 或并行调用 Write 也同样安全：同种类的调用将按顺序进行控制。&lt;/p&gt;

&lt;p&gt;正因为是同步的，因此不能在一个 goroutine 中进行读和写。&lt;/p&gt;

&lt;p&gt;读关闭管道&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *PipeReader) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读关闭管道并传入错误信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *PipeReader) CloseWithError(err error) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从管道中读取数据，如果管道被关闭，则会返会一个错误信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、如果写入端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。&lt;/li&gt;
&lt;li&gt;2、如果写入端通过 Close 方法关闭了管道，则返回 io.EOF。&lt;/li&gt;
&lt;li&gt;3、如果是读取端关闭了管道，则返回 io.ErrClosedPipe。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例：管道（读取端关闭）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r, w := io.Pipe()
    // 启用一个例程进行读取
    go func() {
        buf := make([]byte, 5)
        for n, err := 0, error(nil); err == nil; {
            n, err = r.Read(buf)
            r.CloseWithError(errors.New(&amp;quot;管道被读取端关闭&amp;quot;))
            fmt.Printf(&amp;quot;读取：%d, %v, %s\n&amp;quot;, n, err, buf[:n])
        }
    }()
    // 主例程进行写入
    n, err := w.Write([]byte(&amp;quot;Hello World !&amp;quot;))
    fmt.Printf(&amp;quot;写入：%d, %v\n&amp;quot;, n, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写关闭管道&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (w *PipeWriter) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写关闭管道并传入错误信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (w *PipeWriter) CloseWithError(err error) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;向管道中写入数据，如果管道被关闭，则会返会一个错误信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、如果读取端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。&lt;/li&gt;
&lt;li&gt;2、如果读取端通过 Close 方法关闭了管道，则返回 io.ErrClosedPipe。&lt;/li&gt;
&lt;li&gt;3、如果是写入端关闭了管道，则返回 io.ErrClosedPipe。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例：管道（写入端关闭）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r, w := io.Pipe()
    // 启用一个例程进行读取
    go func() {
        buf := make([]byte, 5)
        for n, err := 0, error(nil); err == nil; {
            n, err = r.Read(buf)
            fmt.Printf(&amp;quot;读取：%d, %v, %s\n&amp;quot;, n, err, buf[:n])
        }
    }()
    // 主例程进行写入
    n, err := w.Write([]byte(&amp;quot;Hello World !&amp;quot;))
    fmt.Printf(&amp;quot;写入：%d, %v\n&amp;quot;, n, err)

    w.CloseWithError(errors.New(&amp;quot;管道被写入端关闭&amp;quot;))
    n, err = w.Write([]byte(&amp;quot;Hello World !&amp;quot;))
    fmt.Printf(&amp;quot;写入：%d, %v\n&amp;quot;, n, err)
    time.Sleep(time.Second * 1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;综合使用实例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    pipeReader, pipeWriter := io.Pipe()
    go PipeWrite(pipeWriter)
    go PipeRead(pipeReader)
    time.Sleep(30 * time.Second)
}

func PipeWrite(writer *io.PipeWriter){
    data := []byte(&amp;quot;Go语言中文网&amp;quot;)
    for i := 0; i &amp;lt; 3; i++{
        n, err := writer.Write(data)
        if err != nil{
            fmt.Println(err)
            return
        }
        fmt.Printf(&amp;quot;写入字节 %d\n&amp;quot;,n)
    }
    writer.CloseWithError(errors.New(&amp;quot;写入段已关闭&amp;quot;))
}

func PipeRead(reader *io.PipeReader){
    buf := make([]byte, 128)
    for{
        fmt.Println(&amp;quot;接口端开始阻塞5秒钟...&amp;quot;)
        time.Sleep(5 * time.Second)
        fmt.Println(&amp;quot;接收端开始接受&amp;quot;)
        n, err := reader.Read(buf)
        if err != nil{
            fmt.Println(err)
            return
        }
        fmt.Printf(&amp;quot;收到字节: %d\n buf内容: %s\n&amp;quot;,n,buf)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;函数&#34;&gt;函数&lt;/h1&gt;

&lt;p&gt;io包中也有一下原生实现可以使用的函数。其实都是直接操作结构体的函数。&lt;/p&gt;

&lt;h2 id=&#34;writestring&#34;&gt;WriteString&lt;/h2&gt;

&lt;p&gt;WriteString 将字符串 s 写入到 w 中，返回写入的字节数和遇到的错误。如果 w 实现了 WriteString 方法，则优先使用该方法将 s 写入 w 中。否则，将 s 转换为 []byte，然后调用 w.Write 方法将数据写入 w 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func WriteString(w Writer, s string) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;readatleast&#34;&gt;ReadAtLeast&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadAtLeast&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReadAtLeast 从 r 中读取数据到 buf 中，要求至少读取 min 个字节。返回读取的字节数和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ReadAtLeast(r Reader, buf []byte, min int) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 min 超出了 buf 的容量，则 err 返回 io.ErrShortBuffer，否则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、读出的数据长度 == 0  ，则 err 返回 EOF。&lt;/li&gt;
&lt;li&gt;2、读出的数据长度 &amp;lt;  min，则 err 返回 io.ErrUnexpectedEOF。&lt;/li&gt;
&lt;li&gt;3、读出的数据长度 &amp;gt;= min，则 err 返回 nil。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadFull&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReadFull 的功能和 ReadAtLeast 一样，只不过 min = len(buf)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ReadFull(r Reader, buf []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：WriteString、ReadAtLeast、ReadFull&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    io.WriteString(os.Stdout, &amp;quot;Hello World!\n&amp;quot;)
    // Hello World!

    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    b := make([]byte, 15)

    n, err := io.ReadAtLeast(r, b, 20)
    fmt.Printf(&amp;quot;%q   %d   %v\n&amp;quot;, b[:n], n, err)
    // &amp;quot;&amp;quot;   0   short buffer

    r.Seek(0, 0)
    b = make([]byte, 15)

    n, err = io.ReadFull(r, b)
    fmt.Printf(&amp;quot;%q   %d   %v\n&amp;quot;, b[:n], n, err)
    // &amp;quot;Hello World!&amp;quot;   12   unexpected EOF
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;LimitReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;LimitReader 对 r 进行封装，使其最多只能读取 n 个字节的数据。相当于对 r 做了一个切片 r[:n] 返回。底层实现是一个 *LimitedReader（只有一个 Read 方法）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LimitReader(r Reader, n int64) Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;MultiReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;MultiReader 将多个 Reader 封装成一个单独的 Reader，多个 Reader 会按顺序读取，当多个 Reader 都返回 EOF 之后，单独的 Reader 才返回 EOF，否则返回读取过程中遇到的任何错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MultiReader(readers ...Reader) Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;MultiWriter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;MultiWriter 将向自身写入的数据同步写入到所有 writers 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MultiWriter(writers ...Writer) Writer
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;TeeReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;TeeReader 对 r 进行封装，使 r 在读取数据的同时，自动向 w 中写入数据。它是一个无缓冲的 Reader，所以对 w 的写入操作必须在 r 的 Read 操作结束之前完成。所有写入时遇到的错误都会被作为 Read 方法的 err 返回。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func TeeReader(r Reader, w Writer) Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 LimitReader&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    lr := io.LimitReader(r, 5)

    n, err := io.Copy(os.Stdout, lr)  // Hello
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err) // 5   &amp;lt;nil&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 MultiReader&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r1 := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    r2 := strings.NewReader(&amp;quot;ABCDEFG&amp;quot;)
    r3 := strings.NewReader(&amp;quot;abcdefg&amp;quot;)
    b := make([]byte, 15)
    mr := io.MultiReader(r1, r2, r3)

    for n, err := 0, error(nil); err == nil; {
        n, err = mr.Read(b)
        fmt.Printf(&amp;quot;%q\n&amp;quot;, b[:n])
    }
    // &amp;quot;Hello World!&amp;quot;
    // &amp;quot;ABCDEFG&amp;quot;
    // &amp;quot;abcdefg&amp;quot;
    // &amp;quot;&amp;quot;

    r1.Seek(0, 0)
    r2.Seek(0, 0)
    r3.Seek(0, 0)
    mr = io.MultiReader(r1, r2, r3)
    io.Copy(os.Stdout, mr)
    // Hello World!ABCDEFGabcdefg
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 MultiWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!\n&amp;quot;)
    mw := io.MultiWriter(os.Stdout, os.Stdout, os.Stdout)

    r.WriteTo(mw)
    // Hello World!
    // Hello World!
    // Hello World!
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 TeeReader&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    b := make([]byte, 15)
    tr := io.TeeReader(r, os.Stdout)

    n, err := tr.Read(b)                  // Hello World!
    fmt.Printf(&amp;quot;\n%s   %v\n&amp;quot;, b[:n], err) // Hello World!   &amp;lt;nil&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;copy&#34;&gt;Copy&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;CopyN&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CopyN 从 src 中复制 n 个字节的数据到 dst 中，返回复制的字节数和遇到的错误。只有当 written = n 时，err 才返回 nil。如果 dst 实现了 ReadFrom 方法，则优先调用该方法执行复制操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func CopyN(dst Writer, src Reader, n int64) (written int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Copy&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Copy 从 src 中复制数据到 dst 中，直到所有数据都复制完毕，返回复制的字节数和遇到的错误。如果复制过程成功结束，则 err 返回 nil，而不是 EOF，因为 Copy 的定义为“直到所有数据都复制完毕”，所以不会将 EOF 视为错误返回。如果 src 实现了 WriteTo 方法，则调用 src.WriteTo(dst) 复制数据，否则如果 dst 实现了 ReadeFrom 方法，则调用 dst.ReadeFrom(src) 复制数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Copy(dst Writer, src Reader) (written int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;CopyBuffer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CopyBuffer 相当于 Copy，只不 Copy 在执行的过程中会创建一个临时的缓冲区来中转数据，而 CopyBuffer 则可以单独提供一个缓冲区，让多个复制操作共用同一个缓冲区，避免每次复制操作都创建新的缓冲区。如果 buf == nil，则 CopyBuffer 会自动创建缓冲区。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func CopyBuffer(dst Writer, src Reader, buf []byte) (written int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：CopyN、Copy、CopyBuffer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    buf := make([]byte, 32)

    n, err := io.CopyN(os.Stdout, r, 5) // Hello
    fmt.Printf(&amp;quot;\n%d   %v\n\n&amp;quot;, n, err) // 5   &amp;lt;nil&amp;gt;

    r.Seek(0, 0)
    n, err = io.Copy(os.Stdout, r)      // Hello World!
    fmt.Printf(&amp;quot;\n%d   %v\n\n&amp;quot;, n, err) // 12   &amp;lt;nil&amp;gt;

    r.Seek(0, 0)
    r2 := strings.NewReader(&amp;quot;ABCDEFG&amp;quot;)
    r3 := strings.NewReader(&amp;quot;abcdefg&amp;quot;)

    n, err = io.CopyBuffer(os.Stdout, r, buf) // Hello World!
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err)         // 12   &amp;lt;nil&amp;gt;

    n, err = io.CopyBuffer(os.Stdout, r2, buf) // ABCDEFG
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err)          // 7   &amp;lt;nil&amp;gt;

    n, err = io.CopyBuffer(os.Stdout, r3, buf) // abcdefg
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err)          // 7   &amp;lt;nil&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数还是我们在网络消息流量转发的时候还是经常使用的。&lt;/p&gt;

&lt;h1 id=&#34;场景举例&#34;&gt;场景举例&lt;/h1&gt;

&lt;h2 id=&#34;base64编码成字符串&#34;&gt;base64编码成字符串&lt;/h2&gt;

&lt;p&gt;encoding/base64包中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewEncoder(enc *Encoding, w io.Writer) io.WriteCloser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个用来做base64编码，但是仔细观察发现，它需要一个io.Writer作为输出目标，并用返回的WriteCloser的Write方法将结果写入目标，下面是Go官方文档的例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input := []byte(&amp;quot;foo\x00bar&amp;quot;)
encoder := base64.NewEncoder(base64.StdEncoding, os.Stdout)
encoder.Write(input)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个例子是将结果写入到Stdout，如果我们希望得到一个字符串呢？可以用bytes.Buffer作为目标io.Writer：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input := []byte(&amp;quot;foo\x00bar&amp;quot;)
buffer := new(bytes.Buffer)
encoder := base64.NewEncoder(base64.StdEncoding, buffer)
encoder.Write(input)
fmt.Println(string(buffer.Bytes())
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;byte和struct之间正反序列化&#34;&gt;[]byte和struct之间正反序列化&lt;/h2&gt;

&lt;p&gt;这种场景经常用在基于字节的协议上，比如有一个具有固定长度的结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Protocol struct {
    Version     uint8
    BodyLen     uint16
    Reserved    [2]byte
    Unit        uint8
    Value       uint32
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过一个[]byte来反序列化得到这个Protocol，一种思路是遍历这个[]byte，然后逐一赋值。其实在encoding/binary包中有个方便的方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Read(r io.Reader, order ByteOrder, data interface{}) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个方法从一个io.Reader中读取字节，并已order指定的端模式，来给填充data（data需要是fixed-sized的结构或者类型）。要用到这个方法首先要有一个io.Reader，从上面的图中不难发现，我们可以这么写：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var p Protocol
var bin []byte
//...
binary.Read(bytes.NewReader(bin), binary.LittleEndian, &amp;amp;p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;换句话说，我们将一个[]byte转成了一个io.Reader。&lt;/p&gt;

&lt;p&gt;反过来，我们需要将Protocol序列化得到[]byte，使用encoding/binary包中有个对应的Write方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Write(w io.Writer, order ByteOrder, data interface{}) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过将[]byte转成一个io.Writer即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var p Protocol
buffer := new(bytes.Buffer)
//...
binary.Writer(buffer, binary.LittleEndian, p)
bin := buffer.Bytes()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;从流中按行读取&#34;&gt;从流中按行读取&lt;/h2&gt;

&lt;p&gt;比如对于常见的基于文本行的HTTP协议的读取，我们需要将一个流按照行来读取。本质上，我们需要一个基于缓冲的读写机制（读一些到缓冲，然后遍历缓冲中我们关心的字节或字符）。在Go中有一个bufio的包可以实现带缓冲的读写：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(rd io.Reader) *Reader
func (b *Reader) ReadString(delim byte) (string, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个ReadString方法从io.Reader中读取字符串，直到delim，就返回delim和之前的字符串。如果将delim设置为\n，相当于按行来读取了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var conn net.Conn
//...
reader := NewReader(conn)
for {
    line, err := reader.ReadString([]byte(&#39;\n&#39;))
    //...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;string-to-byte&#34;&gt;string to byte&lt;/h2&gt;

&lt;p&gt;花式技（zuo）巧（si）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;string转[]byte
a := &amp;quot;Hello, playground&amp;quot;
fmt.Println([]byte(a))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等价于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := &amp;quot;Hello, playground&amp;quot;
buf := new(bytes.Buffer)
buf.ReadFrom(strings.NewReader(a))
fmt.Println(buf.Bytes())
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;标准库中实现的读取器和编写器的实例&#34;&gt;标准库中实现的读取器和编写器的实例&lt;/h1&gt;

&lt;p&gt;目前，Go 文档中还没有直接列出实现了某个接口的所有类型。不过，我们可以通过查看标准库文档，列出实现了 io.Reader 或 io.Writer 接口的类型（导出的类型）：（注：godoc 命令支持额外参数 -analysis ，能列出都有哪些类型实现了某个接口，相关参考 godoc -h 或 Static analysis features of godoc。另外，还有一个地址：&lt;a href=&#34;http://docs.studygolang.com。&#34;&gt;http://docs.studygolang.com。&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/&#34;&gt;os.File&lt;/a&gt; 同时实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/#reader-类型&#34;&gt;strings.Reader&lt;/a&gt; 实现了 io.Reader&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bufio/&#34;&gt;bufio.Reader/Writer&lt;/a&gt; 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/&#34;&gt;bytes.Buffer&lt;/a&gt; 同时实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/&#34;&gt;bytes.Reader&lt;/a&gt; 实现了 io.Reader&lt;/li&gt;
&lt;li&gt;compress/gzip.Reader/Writer 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;crypto/cipher.StreamReader/StreamWriter 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;crypto/tls.Conn 同时实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;encoding/csv.Reader/Writer 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;mime/multipart.Part 实现了 io.Reader&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net/&#34;&gt;net.Conn&lt;/a&gt; 分别实现了 io.Reader 和 io.Writer(Conn接口定义了Read/Write)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此之外，io 包本身也有这两个接口的实现类型。如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实现了 Reader 的类型：&lt;a href=&#34;#limitedreader-类型&#34;&gt;LimitedReader&lt;/a&gt;、&lt;a href=&#34;#pipereader-和-pipewriter-类型&#34;&gt;PipeReader&lt;/a&gt;、&lt;a href=&#34;#sectionreader-类型&#34;&gt;SectionReader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实现了 Writer 的类型：&lt;a href=&#34;#pipereader-和-pipewriter-类型&#34;&gt;PipeWriter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上类型中，常用的类型有，文件IO，缓冲IO，网络IO，在标准库中都有实现&lt;/p&gt;

&lt;p&gt;网络io/文件io/标准io&amp;ndash;其实就是操作网络数据和文件中的数据
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net/&#34;&gt;net.Conn&lt;/a&gt;, &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/&#34;&gt;os.Stdin&lt;/a&gt;, &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/&#34;&gt;os.File&lt;/a&gt;: 网络、标准输入输出、文件的流读取，对应&amp;mdash;frp就是基于这个基础上实现的&lt;/p&gt;

&lt;p&gt;缓存io&amp;ndash;其实就是操作缓存中的string，[]byte
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/#reader-类型&#34;&gt;strings.Reader&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/#builder-类型&#34;&gt;strings.Builder&lt;/a&gt;: 把字符串抽象成Reader
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/#reader-类型&#34;&gt;bytes.Reader&lt;/a&gt;: 把[]byte抽象成Reader
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/#buffer-类型&#34;&gt;bytes.Buffer&lt;/a&gt;: 把[]byte抽象成Reader和Writer&lt;/p&gt;

&lt;p&gt;缓存io&amp;ndash;还是使用缓存，但是主要是对io.reader实例进行读写
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bufio/&#34;&gt;bufio.Reader/Writer&lt;/a&gt;: 抽象成带缓冲的流读取（比如按行读写）&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Go Net 协议层</title>
          <link>https://kingjcy.github.io/post/golang/go-net/</link>
          <pubDate>Mon, 11 Jul 2016 17:34:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-net/</guid>
          <description>&lt;p&gt;网络编程是go语言使用的一个核心模块。golang的网络封装使用对于底层socket或者上层的http，甚至是web服务都很友好。&lt;/p&gt;

&lt;h1 id=&#34;net&#34;&gt;net&lt;/h1&gt;

&lt;p&gt;net包提供了可移植的网络I/O接口，包括TCP/IP、UDP、域名解析和Unix域socket等方式的通信。其中每一种通信方式都使用 xxConn 结构体来表示，诸如IPConn、TCPConn等，这些结构体都实现了Conn接口，Conn接口实现了基本的读、写、关闭、获取远程和本地地址、设置timeout等功能。&lt;/p&gt;

&lt;p&gt;conn的接口定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Conn interface {
    // Read从连接中读取数据
    // Read方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    Read(b []byte) (n int, err error)
    // Write从连接中写入数据
    // Write方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    Write(b []byte) (n int, err error)
    // Close方法关闭该连接
    // 并会导致任何阻塞中的Read或Write方法不再阻塞并返回错误
    Close() error
    // 返回本地网络地址
    LocalAddr() Addr
    // 返回远端网络地址
    RemoteAddr() Addr
    // 设定该连接的读写deadline，等价于同时调用SetReadDeadline和SetWriteDeadline
    // deadline是一个绝对时间，超过该时间后I/O操作就会直接因超时失败返回而不会阻塞
    // deadline对之后的所有I/O操作都起效，而不仅仅是下一次的读或写操作
    // 参数t为零值表示不设置期限
    SetDeadline(t time.Time) error
    // 设定该连接的读操作deadline，参数t为零值表示不设置期限
    SetReadDeadline(t time.Time) error
    // 设定该连接的写操作deadline，参数t为零值表示不设置期限
    // 即使写入超时，返回值n也可能&amp;gt;0，说明成功写入了部分数据
    SetWriteDeadline(t time.Time) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后每种类型都是对应的结构体实现这些接口。&lt;/p&gt;

&lt;p&gt;还有一个常用的接口定义PacketConn&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PacketConn interface {
    // ReadFrom方法从连接读取一个数据包，并将有效信息写入b
    // ReadFrom方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    // 返回写入的字节数和该数据包的来源地址
    ReadFrom(b []byte) (n int, addr Addr, err error)
    // WriteTo方法将有效数据b写入一个数据包发送给addr
    // WriteTo方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    // 在面向数据包的连接中，写入超时非常罕见
    WriteTo(b []byte, addr Addr) (n int, err error)
    // Close方法关闭该连接
    // 会导致任何阻塞中的ReadFrom或WriteTo方法不再阻塞并返回错误
    Close() error
    // 返回本地网络地址
    LocalAddr() Addr
    // 设定该连接的读写deadline
    SetDeadline(t time.Time) error
    // 设定该连接的读操作deadline，参数t为零值表示不设置期限
    // 如果时间到达deadline，读操作就会直接因超时失败返回而不会阻塞
    SetReadDeadline(t time.Time) error
    // 设定该连接的写操作deadline，参数t为零值表示不设置期限
    // 如果时间到达deadline，写操作就会直接因超时失败返回而不会阻塞
    // 即使写入超时，返回值n也可能&amp;gt;0，说明成功写入了部分数据
    SetWriteDeadline(t time.Time) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ip&#34;&gt;ip&lt;/h2&gt;

&lt;p&gt;使用IPConn结构体来表示，它实现了Conn、PacketConn两种接口。使用如下两个函数进行Dial（连接）和Listen（监听）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialIP(netProto string, laddr, raddr *IPAddr) (*IPConn, error)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DialIP在网络协议netProto上连接本地地址laddr和远端地址raddr，netProto必须是&amp;rdquo;ip&amp;rdquo;、&amp;rdquo;ip4&amp;rdquo;或&amp;rdquo;ip6&amp;rdquo;后跟冒号和协议名或协议号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenIP(netProto string, laddr *IPAddr) (*IPConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenIP创建一个接收目的地是本地地址laddr的IP数据包的网络连接，返回的*IPConn的ReadFrom和WriteTo方法可以用来发送和接收IP数据包。（每个包都可获取来源址或者设置目标地址）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、IPAddr类型&lt;/p&gt;

&lt;p&gt;位于iprawsock.go中在net包的许多函数和方法会返回一个指向IPAddr的指针。这不过只是一个包含IP类型的结构体。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type IPAddr struct {
    IP   IP
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个类型的另一个主要用途是通过IP主机名执行DNS查找。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ResolveIPAddr
ResolveIPAddr有两个参数第一个参数.必须为&amp;quot;ip&amp;quot;,&amp;quot;ip4&amp;quot;,&amp;quot;ip6&amp;quot;,第二个参数多为要解析的域名.返回一个IPAddr的指针类型

addr, _ := net.ResolveIPAddr(&amp;quot;ip&amp;quot;, &amp;quot;www.baidu.com&amp;quot;)
fmt.Println(addr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ip.go 中还定义了三个类型.分别是IP,IPMask,IPNet&lt;/p&gt;

&lt;p&gt;2、IP类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type IP []byte
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IP类型被定义为一个字节数组。 ParseIP(String) 可以将字符窜转换为一个IP类型.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name := &amp;quot;127.0.0.1&amp;quot;
addr := net.ParseIP(name)
fmt.Println(addr.IsLoopback())// IsLoopback reports whether ip is a loopback address.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、IPMask类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// An IP mask is an IP address.
type IPMask []byte
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个掩码的字符串形式是一个十六进制数，如掩码255.255.0.0为ffff0000。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func IPv4Mask(a, b, c, d byte) IPMask :用一个4字节的IPv4地址来创建一个掩码.
func CIDRMask(ones, bits int) IPMask : 用ones和bits来创建一个掩码
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、IPNet类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// An IPNet represents an IP network.
type IPNet struct {
    IP   IP     // network number
    Mask IPMask // network mask
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由IP类型和IPMask组成一个网段,其字符串形式是CIDR地址,如:“192.168.100.1/24”或“2001:DB8::/ 48”&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    mask := net.IPv4Mask(byte(255), byte(255), byte(255), byte(0))
    ip := net.ParseIP(&amp;quot;192.168.1.125&amp;quot;).Mask(mask)
    in := &amp;amp;net.IPNet{ip, mask}
    fmt.Println(in)         //  192.168.1.0/24
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边插播一个经常使用的实例：获取本地IP&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net&amp;quot;
    &amp;quot;os&amp;quot;
)
func main() {
    addrs, err := net.InterfaceAddrs()
    if err != nil {
        fmt.Println(err)
        os.Exit(1)
    }
    for _, address := range addrs {
        // 检查ip地址判断是否回环地址
        if ipnet, ok := address.(*net.IPNet); ok &amp;amp;&amp;amp; !ipnet.IP.IsLoopback() {
            if ipnet.IP.To4() != nil {
                fmt.Println(ipnet.IP.String())
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tcp&#34;&gt;tcp&lt;/h2&gt;

&lt;p&gt;使用TCPConn结构体来表示，它实现了Conn接口。&lt;/p&gt;

&lt;p&gt;使用DialTCP进行Dial操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialTCP(net string, laddr, raddr *TCPAddr) (*TCPConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DialTCP在网络协议net上连接本地地址laddr和远端地址raddr。net必须是&amp;rdquo;tcp&amp;rdquo;、&amp;rdquo;tcp4&amp;rdquo;、&amp;rdquo;tcp6&amp;rdquo;；如果laddr不是nil，将使用它作为本地地址，否则自动选择一个本地地址。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenTCP(net string, laddr *TCPAddr) (*TCPListener, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 ListenTCP函数进行Listen，产生一个TCPListener结构体，使用TCPListener的AcceptTCP方法建立通信链路，得到TCPConn。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;TCPAddr类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;位于tcpsock.go中TCPAddr类型包含一个IP和一个port的结构:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type TCPAddr struct {
    IP   IP
    Port int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResolveTCPAddr&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ResolveTCPAddr(net, addr string) (*TCPAddr, os.Error) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数用来创建一个TCPAddr,第一个参数为,tcp,tcp4或者tcp6,addr是一个字符串，由主机名或IP地址，以及&amp;rdquo;:&amp;ldquo;后跟随着端口号组成，例如： &amp;ldquo;www.google.com:80&amp;rdquo; 或 &amp;lsquo;127.0.0.1:22&amp;rdquo;。如果地址是一个IPv6地址，由于已经有冒号，主机部分，必须放在方括号内, 例如：&amp;rdquo;[::1]:23&amp;rdquo;. 另一种特殊情况是经常用于服务器, 主机地址为0, 因此，TCP地址实际上就是端口名称, 例如：&amp;rdquo;:80&amp;rdquo; 用来表示HTTP服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;addr, _ := net.ResolveTCPAddr(&amp;quot;tcp&amp;quot;, &amp;quot;www.baidu.com:80&amp;quot;)
fmt.Println(addr)   //220.181.111.147:80
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;udp&#34;&gt;udp&lt;/h2&gt;

&lt;p&gt;使用UDPConn接口体来表示，它实现了Conn、PacketConn两种接口。使用如下两个函数进行Dial和Listen。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialUDP(net string, laddr, raddr *UDPAddr) (*UDPConn, error)    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DialTCP在网络协议net上连接本地地址laddr和远端地址raddr。net必须是&amp;rdquo;udp&amp;rdquo;、&amp;rdquo;udp4&amp;rdquo;、&amp;rdquo;udp6&amp;rdquo;；如果laddr不是nil，将使用它作为本地地址，否则自动选择一个本地地址。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenUDP(net string, laddr *UDPAddr) (*UDPConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenUDP创建一个接收目的地是本地地址laddr的UDP数据包的网络连接。net必须是&amp;rdquo;udp&amp;rdquo;、&amp;rdquo;udp4&amp;rdquo;、&amp;rdquo;udp6&amp;rdquo;；如果laddr端口为0，函数将选择一个当前可用的端口，可以用Listener的Addr方法获得该端口。返回的*UDPConn的ReadFrom和WriteTo方法可以用来发送和接收UDP数据包（每个包都可获得来源地址或设置目标地址）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、UDPAddr类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type UDPAddr struct {
    IP   IP
    Port int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResolveUDPAddr同样的功能&lt;/p&gt;

&lt;p&gt;2、UnixAddr类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type UnixAddr struct {
    Name string
    Net  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResolveUnixAddr同样的功能&lt;/p&gt;

&lt;h2 id=&#34;unix&#34;&gt;unix&lt;/h2&gt;

&lt;p&gt;UnixConn实现了Conn、PacketConn两种接口，其中unix又分为SOCK_DGRAM、SOCK_STREAM。&lt;/p&gt;

&lt;p&gt;1.对于unix（SOCK_DGRAM），使用如下两个函数进行Dial和Listen。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialUnix(net string, laddr, raddr *UnixAddr) (*UnixConn, error)    

func ListenUnixgram(net string, laddr *UnixAddr) (*UnixConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.对于unix（SOCK_STREAM）&lt;/p&gt;

&lt;p&gt;客户端使用DialUnix进行Dial操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialUnix(net string, laddr, raddr *UnixAddr) (*UnixConn, error)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务端使用ListenUnix函数进行Listen操作，然后使用UnixListener进行AcceptUnix&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenUnix(net string, laddr *UnixAddr) (*UnixListener, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;函数整合&#34;&gt;函数整合&lt;/h1&gt;

&lt;p&gt;为了使用方便，golang将上面一些重复的操作集中到一个函数中。在参数中制定上面不同协议类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenPacket(net, laddr string) (PacketConn, error)　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数用于侦听ip、udp、unix（DGRAM）等协议，返回一个PacketConn接口，同样根据侦听的协议不同，这个接口可以包含IPCon、UDPConn、UnixConn等，它们都实现了PacketConn。可以发现与ip、unix（stream）协议不同，直接返回的是xxConn，不是间接的通过Listener进行Accept操作后，才得到一个Conn。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Listen(net, laddr string) (Listener, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数用于侦听tcp、unix（stream）等协议，返回一个Listener接口、根据侦听的协议不同，这个接口可以包含TCPListener、UnixListener等，它们都实现了Listener接口，然后通过调用其Accept方法可以得到Conn接口，进行通信。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Dial(network, address string) (Conn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数对于所有的协议都是相同的操作，返回一个Conn接口，根据协议的不同实际上包含IPConn、UDPConn、UnixConn、IPConn，它们都实现了Conn接口&lt;/p&gt;

&lt;h1 id=&#34;基本c-s功能&#34;&gt;基本c/s功能&lt;/h1&gt;

&lt;p&gt;在 Unix/Linux 中的 Socket 编程主要通过调用 listen, accept, write read 等函数来实现的. 具体如下图所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/unix_socket.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;服务端&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;服务端listen, accept&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func connHandler(c net.Conn) {
    for {
        cnt, err := c.Read(buf)
        c.Write(buf)
    }
}
func main() {
    server, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:1208&amp;quot;)
    for {
        conn, err := server.Accept()
        go connHandler(conn)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接使用net的listen返回的就是对应协议已经定义好的结构体，比如tcp&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type TCPListener struct {
    fd *netFD
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个结构体实现了listener接口的所有接口，所以可以作为返回值返回。其他协议类型也是一样。&lt;/p&gt;

&lt;p&gt;accept后返回的conn是一个存储着连接信息的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Network file descriptor.
type netFD struct {
    pfd poll.FD

    // immutable until Close
    family      int
    sotype      int
    isConnected bool // handshake completed or use of association with peer
    net         string
    laddr       Addr
    raddr       Addr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;客户端&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;客户端dial&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func connHandler(c net.Conn) {
    for {
        c.Write(...)
        c.Read(...)
    }
}
func main() {
    conn, err := net.Dial(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:1208&amp;quot;)
    connHandler(conn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Dial(net, addr string) (Conn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中net参数是网络协议的名字， addr参数是IP地址或域名，而端口号以“:”的形式跟随在地址
或域名的后面，端口号可选。如果连接成功，返回连接对象，否则返回error。&lt;/p&gt;

&lt;p&gt;Dial() 函数支持如下几种网络协议：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;tcp&amp;quot; 、 &amp;quot;tcp4&amp;quot; （仅限IPv4）、 &amp;quot;tcp6&amp;quot; （仅限IPv6）、 &amp;quot;udp&amp;quot; 、 &amp;quot;udp4&amp;quot;（仅限IPv4）、 &amp;quot;udp6&amp;quot;（仅限IPv6）、 &amp;quot;ip&amp;quot; 、 &amp;quot;ip4&amp;quot;（仅限IPv4）和&amp;quot;ip6&amp;quot;（仅限IPv6）。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以直接用相关协议的函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialTCP(net string, laddr, raddr *TCPAddr) (c *TCPConn, err error)
func DialUDP(net string, laddr, raddr *UDPAddr) (c *UDPConn, err error)
func DialIP(netProto string, laddr, raddr *IPAddr) (*IPConn, error)
func DialUnix(net string, laddr, raddr *UnixAddr) (c *UnixConn, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;特性功能&#34;&gt;特性功能&lt;/h2&gt;

&lt;p&gt;1、控制TCP连接&lt;/p&gt;

&lt;p&gt;TCP连接有很多控制函数，我们平常用到比较多的有如下几个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *TCPConn) SetTimeout(nsec int64) os.Error
func (c *TCPConn) SetKeepAlive(keepalive bool) os.Error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个函数用来设置超时时间，客户端和服务器端都适用，当超过设置的时间时那么该链接就失效。&lt;/p&gt;

&lt;p&gt;第二个函数用来设置客户端是否和服务器端一直保持着连接，即使没有任何的数据发送&lt;/p&gt;

&lt;h1 id=&#34;实例&#34;&gt;实例&lt;/h1&gt;

&lt;p&gt;从零开始写Socket Server： Socket-Client框架&lt;/p&gt;

&lt;p&gt;在golang中，网络协议已经被封装的非常完好了，想要写一个Socket的Server，我们并不用像其他语言那样需要为socket、bind、listen、receive等一系列操作头疼，只要使用Golang中自带的net包即可很方便的完成连接等操作~&lt;/p&gt;

&lt;p&gt;在这里，给出一个最最基础的基于Socket的Server的写法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
)


func main() {

//建立socket，监听端口
    netListen, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:1024&amp;quot;)
    CheckError(err)
    defer netListen.Close()

    Log(&amp;quot;Waiting for clients&amp;quot;)
    for {
        conn, err := netListen.Accept()
        if err != nil {
            continue
        }

        Log(conn.RemoteAddr().String(), &amp;quot; tcp connect success&amp;quot;)
        handleConnection(conn)
    }
}
//处理连接
func handleConnection(conn net.Conn) {

    buffer := make([]byte, 2048)

    for {

        n, err := conn.Read(buffer)

        if err != nil {
            Log(conn.RemoteAddr().String(), &amp;quot; connection error: &amp;quot;, err)
            return
        }


        Log(conn.RemoteAddr().String(), &amp;quot;receive data string:\n&amp;quot;, string(buffer[:n]))

    }

}
func Log(v ...interface{}) {
    log.Println(v...)
}

func CheckError(err error) {
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Fatal error: %s&amp;quot;, err.Error())
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;唔，抛除Go语言里面10行代码有5行error的蛋疼之处,你可以看到，Server想要建立并接受一个Socket，其核心流程就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netListen, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:1024&amp;quot;)
conn, err := netListen.Accept()
n, err := conn.Read(buffer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这三步，通过Listen、Accept 和Read，我们就成功的绑定了一个端口，并能够读取从该端口传来的内容~&lt;/p&gt;

&lt;p&gt;这边插播一个内容，关于read是阻塞的，如果读取不到内容，代码会阻塞在这边，直到有内容可以读取，包括connection断掉返回的io.EOF,一般对这个都有特殊处理。一般重conn读取数据也是在for循环中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;net&amp;quot;
)

func main(){
    ln, err := net.Listen(&amp;quot;tcp&amp;quot;,&amp;quot;127.0.0.1:10051&amp;quot;)

    if err != nil {
        panic(err)
    }

    for {
        conn, _ := ln.Accept() //The loop will be held here
        fmt.Println(&amp;quot;get connect&amp;quot;)
        go handleread(conn)


    }
}

func handleread(conn net.Conn){
    defer conn.Close()

    var tatalBuffer  []byte
    var all int
    for {
        buffer := make([]byte, 2)
        n,err := conn.Read(buffer)
        if err == io.EOF{
            fmt.Println(err,n)
            break
        }

        tatalBuffer = append(tatalBuffer,buffer...)
        all += n

        fmt.Println(string(buffer),n,string(tatalBuffer[:all]),all)
    }



}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这个例子中，会重conn中两个字符循环读取内容，这边slice不会动态扩容，所以需要使用append来获取全部内容。&lt;/p&gt;

&lt;p&gt;还有一点，buffer := make([]byte, 2)这个代码，放在for循环中，浪费内存，可以放在gor循环外部，然后使用n来截取buf[:n]可以解决buf最后一部分重复的问题。&lt;/p&gt;

&lt;p&gt;插播结束，回到server。&lt;/p&gt;

&lt;p&gt;Server写好之后，接下来就是Client方面啦，我手写一个HelloWorld给大家：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net&amp;quot;
    &amp;quot;os&amp;quot;
)

func sender(conn net.Conn) {
        words := &amp;quot;hello world!&amp;quot;
        conn.Write([]byte(words))
    fmt.Println(&amp;quot;send over&amp;quot;)

}



func main() {
    server := &amp;quot;127.0.0.1:1024&amp;quot;
    tcpAddr, err := net.ResolveTCPAddr(&amp;quot;tcp4&amp;quot;, server)
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Fatal error: %s&amp;quot;, err.Error())
        os.Exit(1)
    }

    conn, err := net.DialTCP(&amp;quot;tcp&amp;quot;, nil, tcpAddr)
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Fatal error: %s&amp;quot;, err.Error())
        os.Exit(1)
    }


    fmt.Println(&amp;quot;connect success&amp;quot;)
    sender(conn)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，Client这里的关键在于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tcpAddr, err := net.ResolveTCPAddr(&amp;quot;tcp4&amp;quot;, server)
conn, err := net.DialTCP(&amp;quot;tcp&amp;quot;, nil, tcpAddr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这两步，主要是负责解析端口和连接。&lt;/p&gt;

&lt;p&gt;这边插播一个tcp协议的三次握手图，加强理解。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/tcp_open_close.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;其实我们最常用的还是&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net-http/&#34;&gt;http协议&lt;/a&gt;，也即是应用层的协议，其实http协议是在tcp协议的基础上进行封装，最终还是使用的这边基本的网络IO，所以在网络传输中，网络IO的基本协议的实现是基础。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列----  Builtin</title>
          <link>https://kingjcy.github.io/post/golang/go-builtin/</link>
          <pubDate>Tue, 28 Jun 2016 20:36:55 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-builtin/</guid>
          <description>&lt;p&gt;builtin包是go的预声明定义，包括go语言中常用的各种类型和方法声明，包括变量和常量两部分．&lt;/p&gt;

&lt;p&gt;builtin 包为Go的预声明标识符提供了文档。我们常用的一下常量和函数就是在这个包中定义的，以便于我们直接使用，下面对一些用过的进行整理和记录。&lt;/p&gt;

&lt;h1 id=&#34;常量&#34;&gt;常量&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;true和false&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;const (
        true  = 0 == 0 // Untyped bool.
        false = 0 != 0 // Untyped bool.
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;true和false是两个无类型的bool值&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;error&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type error interface {
    Error() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内建error接口类型是约定用于表示错误信息，nil值表示无错误。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;iota&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;const iota = 0 // Untyped int.无类型int
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;变量&#34;&gt;变量&lt;/h1&gt;

&lt;p&gt;变量就有很多了，比如我们常量的append函数就是在这边定义的，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func append(slice []Type, elems ...Type) []Type
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;

&lt;p&gt;还有很多使用的时候看一下，这边就不多说了，主要知道内置的这边常量和变量定义的位置。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Os</title>
          <link>https://kingjcy.github.io/post/golang/go-os/</link>
          <pubDate>Thu, 02 Jun 2016 09:52:35 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-os/</guid>
          <description>&lt;p&gt;os包中实现了不依赖平台的操作系统函数接口(平台无关的接口)，设计向Unix风格，但是错误处理是go风格，当os包使用时，如果失败之后返回错误类型而不是错误数量,返回错误值而非错误码,可以包含更多信息。&lt;/p&gt;

&lt;h1 id=&#34;os&#34;&gt;os&lt;/h1&gt;

&lt;p&gt;os 依赖于 syscall。在实际编程中，我们应该总是优先使用 os 中提供的功能，而不是 syscall。&lt;/p&gt;

&lt;p&gt;os包提供了操作系统函数的不依赖平台的接口。一般都是linux下的一些基本命令的操作，比如文件，目录操作之类。&lt;/p&gt;

&lt;p&gt;我们运行程序常用的命令行参数就是在这个包中可以获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var Args []string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Args保管了命令行参数，第一个是程序名。&lt;/p&gt;

&lt;h2 id=&#34;文件io&#34;&gt;文件io&lt;/h2&gt;

&lt;p&gt;文件IO就是对文件的读写操作，我们先了解一些os中的基本概念。&lt;/p&gt;

&lt;h3 id=&#34;基本概念&#34;&gt;基本概念&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;文件描述符&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有 I/O 操作以文件描述符 ( 一个非负整数 , 通常是小整数 ) 来指代打开的文件。文件描述符用以表示所有类型的已打开文件，包括管道（pipe）、FIFO、socket、终端、设备和普通文件。&lt;/p&gt;

&lt;p&gt;在 Go 中，文件描述符封装在 os.File 结构中，通过 File.Fd() 可以获得底层的文件描述符：fd。&lt;/p&gt;

&lt;p&gt;File结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type File struct {
    *file
}
// file is the real representation of *File.
// The extra level of indirection ensures that no clients of os
// can overwrite this data, which could cause the finalizer
// to close the wrong file descriptor.
type file struct {
    fd      int
    name    string
    dirinfo *dirInfo // nil unless directory being read
}

// Auxiliary information if the File describes a directory
type dirInfo struct {
    buf  []byte // buffer for directory I/O
    nbuf int    // length of buf; return value from Getdirentries
    bufp int    // location of next record in buf.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;标准定义&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;按照惯例，大多数程序都期望能够使用 3 种标准的文件描述符：0- 标准输入；1- 标准输出；2- 标准错误。os 包提供了 3 个 File 对象，分别代表这 3 种标准描述符：Stdin、Stdout 和 Stderr，它们对应的文件名分别是：/dev/stdin、/dev/stdout 和 /dev/stderr。&lt;/p&gt;

&lt;h3 id=&#34;基本操作&#34;&gt;基本操作&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewFile(fd uintptr, name string) *File
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewFile使用给出的Unix文件描述符和名称创建一个文件。&lt;/p&gt;

&lt;p&gt;正常使用create来创建一个文件，比如文件不存在，就创建一个&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file,er:=os.Open(&amp;quot;xxx&amp;quot;)
defer func(){file.Close()}()
if er!=nil &amp;amp;&amp;amp; os.IfNotExist(er
r){
  file = os.Create(&amp;quot;xx&amp;quot;)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;打开&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Open(name string) (file *File, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open打开一个文件用于读取。如果操作成功，返回的文件对象的方法可用于读取数据；对应的文件描述符具有O_RDONLY模式。如果出错，错误底层类型是*PathError。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func OpenFile(name string, flag int, perm FileMode) (file *File, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OpenFile是一个更一般性的文件打开函数，大多数调用者都应用Open或Create代替本函数。它会使用指定的选项（如O_RDONLY等）、指定的模式（如0666等）打开指定名称的文件。如果操作成功，返回的文件对象可用于I/O。如果出错，错误底层类型是*PathError。&lt;/p&gt;

&lt;p&gt;位掩码参数 flag 用于指定文件的访问模式，可用的值在 os 中定义为常量（以下值并非所有操作系统都可用）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    O_RDONLY int = syscall.O_RDONLY // 只读模式打开文件
    O_WRONLY int = syscall.O_WRONLY // 只写模式打开文件
    O_RDWR   int = syscall.O_RDWR   // 读写模式打开文件
    O_APPEND int = syscall.O_APPEND // 写操作时将数据附加到文件尾部
    O_CREATE int = syscall.O_CREAT  // 如果不存在将创建一个新文件
    O_EXCL   int = syscall.O_EXCL   // 和 O_CREATE 配合使用，文件必须不存在
    O_SYNC   int = syscall.O_SYNC   // 打开文件用于同步 I/O
    O_TRUNC  int = syscall.O_TRUNC  // 如果可能，打开时清空文件
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O_TRUNC这个参数可以用来清空文件，如果可以的话，还可以用这个函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;os.Truncate(name, size)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Truncate(size int64) error
size 填0 就把文件清空了。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面有详细的说明&lt;/p&gt;

&lt;p&gt;位掩码参数 perm 指定了文件的模式和权限位，类型是 os.FileMode，文件模式位常量定义在 os 中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    // 单字符是被 String 方法用于格式化的属性缩写。
    ModeDir        FileMode = 1 &amp;lt;&amp;lt; (32 - 1 - iota) // d: 目录
    ModeAppend                                     // a: 只能写入，且只能写入到末尾
    ModeExclusive                                  // l: 用于执行
    ModeTemporary                                  // T: 临时文件（非备份文件）
    ModeSymlink                                    // L: 符号链接（不是快捷方式文件）
    ModeDevice                                     // D: 设备
    ModeNamedPipe                                  // p: 命名管道（FIFO）
    ModeSocket                                     // S: Unix 域 socket
    ModeSetuid                                     // u: 表示文件具有其创建者用户 id 权限
    ModeSetgid                                     // g: 表示文件具有其创建者组 id 的权限
    ModeCharDevice                                 // c: 字符设备，需已设置 ModeDevice
    ModeSticky                                     // t: 只有 root/ 创建者能删除 / 移动文件

    // 覆盖所有类型位（用于通过 &amp;amp; 获取类型位），对普通文件，所有这些位都不应被设置
    ModeType = ModeDir | ModeSymlink | ModeNamedPipe | ModeSocket | ModeDevice
    ModePerm FileMode = 0777 // 覆盖所有 Unix 权限位（用于通过 &amp;amp; 获取类型位）
)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Read(b []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Read 方法从 f 中读取最多 len(b) 字节数据并写入 b。它返回读取的字节数和可能遇到的任何错误。文件终止标志是读取 0 个字节且返回值 err 为 io.EOF。&lt;/p&gt;

&lt;p&gt;从方法声明可以知道，File 实现了 io.Reader 接口。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) ReadAt(b []byte, off int64) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ReadAt 从指定的位置（相对于文件开始位置）读取长度为 len(b) 个字节数据并写入 b。它返回读取的字节数和可能遇到的任何错误。当 n&amp;lt;len(b) 时，本方法总是会返回错误；如果是因为到达文件结尾，返回值 err 会是 io.EOF。它对应的系统调用是 pread。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var chunks []byte
buf := make([]byte, 1024)
var count = 0
for {
    n, err := f.Read(buf)
    if err != nil &amp;amp;&amp;amp; err != io.EOF {
        panic(err)
    }
    if 0 == n {
        break
    }
    count = count + n
    chunks = append(chunks, buf[:n]...)
}
r.logger.Debugf(&amp;quot;read file content : %s&amp;quot;,string(chunks[:count]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边这个实例主要是要说明一下几个重点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、buf必须make，不然会panic
2、read必须for循环，直到io.EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;write&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Write(b []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write 向文件中写入 len(b) 字节数据。它返回写入的字节数和可能遇到的任何错误。如果返回值 n!=len(b)，本方法会返回一个非 nil 的错误。&lt;/p&gt;

&lt;p&gt;从方法声明可以知道，File 实现了 io.Writer 接口。&lt;/p&gt;

&lt;p&gt;Write 与 WriteAt 的区别同 Read 与 ReadAt 的区别一样。为了方便，还提供了 WriteString 方法，它实际是对 Write 的封装。&lt;/p&gt;

&lt;p&gt;注意：Write 调用成功并不能保证数据已经写入磁盘，因为内核会缓存磁盘的 I/O 操作。如果希望立刻将数据写入磁盘（一般场景不建议这么做，因为会影响性能），有两种办法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 打开文件时指定 `os.O_SYNC`；
2. 调用 `File.Sync()` 方法。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：File.Sync() 底层调用的是 fsync 系统调用，这会将数据和元数据都刷到磁盘；如果只想刷数据到磁盘（比如，文件大小没变，只是变了文件数据），需要自己封装，调用 fdatasync（syscall.Fdatasync） 系统调用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;close&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;close() 系统调用关闭一个打开的文件描述符，并将其释放回调用进程，供该进程继续使用。当进程终止时，将自动关闭其已打开的所有文件描述符。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;os.File.Close() 是对 close() 的封装。我们应该养成关闭不需要的文件的良好编程习惯。文件描述符是资源，Go 的 gc 是针对内存的，并不会自动回收资源，如果不关闭文件描述符，长期运行的服务可能会把文件描述符耗尽。&lt;/p&gt;

&lt;p&gt;以下两种情况会导致 Close 返回错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 关闭一个未打开的文件；
2. 两次关闭同一个文件；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常，我们不会去检查 Close 的错误&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;seek&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Seek(offset int64, whence int) (ret int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seek 设置下一次读 / 写的位置。offset 为相对偏移量，而 whence 决定相对位置：0 为相对文件开头，1 为相对当前位置，2 为相对文件结尾。它返回新的偏移量（相对开头）和可能的错误。使用中，whence 应该使用 os 包中的常量：SEEK_SET、SEEK_CUR 和 SEEK_END&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file.Seek(0, os.SEEK_SET)    // 文件开始处
file.Seek(0, SEEK_END)        // 文件结尾处的下一个字节
file.Seek(-1, SEEK_END)        // 文件最后一个字节
file.Seek(-10, SEEK_CUR)     // 当前位置前 10 个字节
file.Seek(1000, SEEK_END)    // 文件结尾处的下 1001 个字节
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;trucate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;trucate 和 ftruncate 系统调用将文件大小设置为 size 参数指定的值；Go 语言中相应的包装函数是 os.Truncate 和 os.File.Truncate。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Truncate(name string, size int64) error
func (f *File) Truncate(size int64) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果文件当前长度大于参数 size，调用将丢弃超出部分，若小于参数 size，调用将在文件尾部添加一系列空字节或是一个文件空洞。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;remove&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Remove(name string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove删除name指定的文件或目录。如果出错，会返回*PathError底层类型的错误。&lt;/p&gt;

&lt;h2 id=&#34;文件属性&#34;&gt;文件属性&lt;/h2&gt;

&lt;h3 id=&#34;文件信息&#34;&gt;文件信息&lt;/h3&gt;

&lt;p&gt;可以通过包里的函数 Stat、Lstat 和 File.Stat 可以得到os.FileInfo 接口的信息。这三个函数对应三个系统调用：stat、lstat 和 fstat。&lt;/p&gt;

&lt;p&gt;这三个函数的区别：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;stat 会返回所命名文件的相关信息。&lt;/li&gt;
&lt;li&gt;lstat 与 stat 类似，区别在于如果文件是符号链接，那么所返回的信息针对的是符号链接自身（而非符号链接所指向的文件）。&lt;/li&gt;
&lt;li&gt;fstat 则会返回由某个打开文件描述符（Go 中则是当前打开文件 File）所指代文件的相关信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stat 和 Lstat 无需对其所操作的文件本身拥有任何权限，但针对指定 name 的父目录要有执行（搜索）权限。而只要 File 对象 ok，File.Stat 总是成功。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Stat() (fi FileInfo, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stat返回描述文件f的FileInfo类型值。如果出错，错误底层类型是*PathError。这个方法也可以用于检查文件是否有问题，上面说到文件的信息是存储在FileInfo 接口中的，我们来看一下这个接口&lt;/p&gt;

&lt;p&gt;FileInfo是一个接口，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// A FileInfo describes a file and is returned by Stat and Lstat.
type FileInfo interface {
    Name() string       // base name of the file 文件的名字（不含扩展名）
    Size() int64        // length in bytes for regular files; system-dependent for others  普通文件返回值表示其大小；其他文件的返回值含义各系统不同
    Mode() FileMode     // file mode bits   文件的模式位
    ModTime() time.Time // modification time    文件的修改时间
    IsDir() bool        // abbreviation for Mode().IsDir()  等价于 Mode().IsDir()
    Sys() interface{}   // underlying data source (can return nil)  底层数据来源（可以返回 nil）
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该接口提供了一个sys函数，Sys() 底层数据的 C 语言 结构 statbuf 格式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct stat {
  dev_t    st_dev;    // 设备 ID
  ino_t    st_ino;    // 文件 i 节点号
  mode_t    st_mode;    // 位掩码，文件类型和文件权限
  nlink_t    st_nlink;    // 硬链接数
  uid_t    st_uid;    // 文件属主，用户 ID
  gid_t    st_gid;    // 文件属组，组 ID
  dev_t    st_rdev;    // 如果针对设备 i 节点，则此字段包含主、辅 ID
  off_t    st_size;    // 常规文件，则是文件字节数；符号链接，则是链接所指路径名的长度，字节为单位；对于共享内存对象，则是对象大小
  blksize_t    st_blsize;    // 分配给文件的总块数，块大小为 512 字节
  blkcnt_t    st_blocks;    // 实际分配给文件的磁盘块数量
  time_t    st_atime;        // 对文件上次访问时间
  time_t    st_mtime;        // 对文件上次修改时间
  time_t    st_ctime;        // 文件状态发生改变的上次时间
}
Go 中 syscal.Stat_t 与该结构对应。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我们要获取 FileInfo 接口没法直接返回的信息，比如想获取文件的上次访问时间，示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fileInfo, err := os.Stat(&amp;quot;test.log&amp;quot;)
if err != nil {
  log.Fatal(err)
}
sys := fileInfo.Sys()
stat := sys.(*syscall.Stat_t)
fmt.Println(time.Unix(stat.Atimespec.Unix()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常返回的是实现这个接口的结构体，也就是fileStat，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// A fileStat is the implementation of FileInfo returned by Stat and Lstat.
type fileStat struct {
    name    string
    size    int64
    mode    FileMode
    modTime time.Time
    sys     syscall.Stat_t
}

func (fs *fileStat) Size() int64        { return fs.size }
func (fs *fileStat) Mode() FileMode     { return fs.mode }
func (fs *fileStat) ModTime() time.Time { return fs.modTime }
func (fs *fileStat) Sys() interface{}   { return &amp;amp;fs.sys }

func sameFile(fs1, fs2 *fileStat) bool {
    return fs1.sys.Dev == fs2.sys.Dev &amp;amp;&amp;amp; fs1.sys.Ino == fs2.sys.Ino
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中有一个syscall.Stat_t，源于syscall的结构体，这个结构体是需要区分系统的，不同的系统调用不一样，不然编译不通过，报错如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;registry/delete.go:49:27: stat.Ctimespec undefined (type *syscall.Stat_t has no field or method Ctimespec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;是因为在linux下结构体成名名是Ctim，在drawin下是Ctimespec，导致跨平台编译报错。&lt;/p&gt;

&lt;h3 id=&#34;文件时间&#34;&gt;文件时间&lt;/h3&gt;

&lt;p&gt;通过包里的Chtimes函数可以显式改变文件的访问时间和修改时间。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Chtimes(name string, atime time.Time, mtime time.Time) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chtimes 修改 name 指定的文件对象的访问时间和修改时间，类似 Unix 的 utime() 或 utimes() 函数。底层的文件系统可能会截断 / 舍入时间单位到更低的精确度。如果出错，会返回 *PathError 类型的错误。在 Unix 中，底层实现会调用 utimenstat()，它提供纳秒级别的精度&lt;/p&gt;

&lt;h3 id=&#34;文件权限&#34;&gt;文件权限&lt;/h3&gt;

&lt;p&gt;系统调用 chown、lchown 和 fchown 可用来改变文件的属主和属组，Go 中os包中对应的函数或方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Chown(name string, uid, gid int) error
func Lchown(name string, uid, gid int) error
func (f *File) Chown(uid, gid int) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它们的区别和上文提到的 Stat 相关函数类似。&lt;/p&gt;

&lt;p&gt;在文件相关操作报错时，可以通过 os.IsPermission 检查是否是权限的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func IsPermission(err error) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个布尔值说明该错误是否表示因权限不足要求被拒绝。ErrPermission 和一些系统调用错误会使它返回真。&lt;/p&gt;

&lt;p&gt;另外，syscall.Access 可以获取文件的权限。这对应系统调用 access。&lt;/p&gt;

&lt;p&gt;os.Chmod 和 os.File.Chmod 可以修改文件权限（包括 sticky 位），分别对应系统调用 chmod 和 fchmod。&lt;/p&gt;

&lt;h2 id=&#34;目录&#34;&gt;目录&lt;/h2&gt;

&lt;p&gt;在 Unix 文件系统中，目录的存储方式类似于普通文件。目录和普通文件的区别有二：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在其 i-node 条目中，会将目录标记为一种不同的文件类型。&lt;/li&gt;
&lt;li&gt;目录是经特殊组织而成的文件。本质上说就是一个表格，包含文件名和 i-node 标号&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;目录操作&#34;&gt;目录操作&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Mkdir(name string, perm FileMode) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mkdir 使用指定的权限和名称创建一个目录。如果出错，会返回 *PathError 类型的错误。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;name 参数指定了新目录的路径名，可以是相对路径，也可以是绝对路径。如果已经存在，则调用失败并返回 os.ErrExist 错误。&lt;/li&gt;
&lt;li&gt;perm 参数指定了新目录的权限。对该位掩码值的指定方式和 os.OpenFile 相同，也可以直接赋予八进制数值。注意，perm 值还将于进程掩码相与（&amp;amp;）。如果 perm 中设置了 sticky 位，那么将对新目录设置该权限。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为 Mkdir 所创建的只是路径名中的最后一部分，如果父目录不存在，创建会失败。os.MkdirAll 用于递归创建所有不存在的目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MkdirAll(path string, perm FileMode) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MkdirAll使用指定的权限和名称创建一个目录，包括任何必要的上级目录，并返回nil，否则返回错误。权限位perm会应用在每一个被本函数创建的目录上。如果path指定了一个已经存在的目录，MkdirAll不做任何操作并返回nil。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;删除&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Remove(name string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove 删除 name 指定的文件或目录。如果出错，会返回 *PathError 类型的错误。如果目录不为空，Remove 会返回失败。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func RemoveAll(path string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RemoveAll 删除 path 指定的文件，或目录及它包含的任何下级对象。它会尝试删除所有东西，除非遇到错误并返回。如果 path 指定的对象不存在，RemoveAll 会返回 nil 而不返回错误。&lt;/p&gt;

&lt;p&gt;RemoveAll 的内部实现逻辑如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;调用 Remove 尝试进行删除，如果成功或返回 path 不存在，则直接返回 nil；&lt;/li&gt;
&lt;li&gt;调用 Lstat 获取 path 信息，以便判断是否是目录。注意，这里使用 Lstat，表示不对符号链接解引用；&lt;/li&gt;
&lt;li&gt;调用 Open 打开目录，递归读取目录中内容，执行删除操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;读&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Readdirnames(n int) (names []string, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readdirnames 读取目录 f 的内容，返回一个最多有 n 个成员的[]string，切片成员为目录中文件对象的名字，采用目录顺序。对本函数的下一次调用会返回上一次调用未读取的内容的信息。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果 n&amp;gt;0，Readdirnames 函数会返回一个最多 n 个成员的切片。这时，如果 Readdirnames 返回一个空切片，它会返回一个非 nil 的错误说明原因。如果到达了目录 f 的结尾，返回值 err 会是 io.EOF。&lt;/li&gt;
&lt;li&gt;如果 n&amp;lt;=0，Readdirnames 函数返回目录中剩余所有文件对象的名字构成的切片。此时，如果 Readdirnames 调用成功（读取所有内容直到结尾），它会返回该切片和 nil 的错误值。如果在到达结尾前遇到错误，会返回之前成功读取的名字构成的切片和该错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Readdir 内部会调用 Readdirnames，将得到的 names 构造路径，通过 Lstat 构造出 []FileInfo。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Readdir(n int) (fi []FileInfo, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;连接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Link(oldname, newname string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Link 创建一个名为 newname 指向 oldname 的硬链接。如果出错，会返回 *LinkError 类型的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Symlink(oldname, newname string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Symlink 创建一个名为 newname 指向 oldname 的符号链接。如果出错，会返回 *LinkError 类型的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Readlink(name string) (string, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readlink 获取 name 指定的符号链接指向的文件的路径。如果出错，会返回 *PathError 类型的错误。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;更改文件名&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Rename(oldpath, newpath string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Rename 修改一个文件的名字或移动一个文件。如果 newpath 已经存在，则替换它。注意，可能会有一些个操作系统特定的限制。&lt;/p&gt;

&lt;h1 id=&#34;os-singal&#34;&gt;os/singal&lt;/h1&gt;

&lt;h2 id=&#34;类型&#34;&gt;类型&lt;/h2&gt;

&lt;p&gt;Signal是一个接口，所有的信号都实现了这个接口，可以直接传递，我们传递信号的时候，需要定义这个类型的channel来传递信号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Signal interface {
    String() string
    Signal() // to distinguish from other Stringers
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;syscall 包中定义了所有的信号常量，比如syscall.SIGINT，其实就是一个int的数字信号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SIGINT    = Signal(0x2)
type Signal int

func (s Signal) Signal() {}

func (s Signal) String() string {
    if 0 &amp;lt;= s &amp;amp;&amp;amp; int(s) &amp;lt; len(signals) {
        str := signals[s]
        if str != &amp;quot;&amp;quot; {
            return str
        }
    }
    return &amp;quot;signal &amp;quot; + itoa(int(s))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;函数&#34;&gt;函数&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Notify&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;singnal主要是用于信号的传递，一般程序中需要使用信号的时候使用。主要使用下面两个方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Notify(c chan&amp;lt;- os.Signal, sig ...os.Signal)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notify函数让signal包将输入信号转发到c。如果没有列出要传递的信号，会将所有输入信号传递到c；否则只传递列出的输入信号。&lt;/p&gt;

&lt;p&gt;signal包不会为了向c发送信息而阻塞（就是说如果发送时c阻塞了，signal包会直接放弃）：调用者应该保证c有足够的缓存空间可以跟上期望的信号频率。对使用单一信号用于通知的通道，缓存为1就足够了。&lt;/p&gt;

&lt;p&gt;可以使用同一通道多次调用Notify：每一次都会扩展该通道接收的信号集。可以使用同一信号和不同通道多次调用Notify：每一个通道都会独立接收到该信号的一个拷贝。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import &amp;quot;fmt&amp;quot;
import &amp;quot;os&amp;quot;
import &amp;quot;os/signal&amp;quot;
import &amp;quot;syscall&amp;quot;
func main() {
    sigs := make(chan os.Signal, 1)
    done := make(chan bool, 1)
    signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)

    go func() {
        sig := &amp;lt;-sigs
        fmt.Println()
        fmt.Println(sig)
        done &amp;lt;- true
    }()

    fmt.Println(&amp;quot;awaiting signal&amp;quot;)
    &amp;lt;-done
    fmt.Println(&amp;quot;exiting&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;stop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;唯一从信号集去除信号的方法是调用Stop。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Stop(c chan&amp;lt;- os.Signal)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stop函数让signal包停止向c转发信号。它会取消之前使用c调用的所有Notify的效果。当Stop返回后，会保证c不再接收到任何信号。&lt;/p&gt;

&lt;h1 id=&#34;os-exec&#34;&gt;os/exec&lt;/h1&gt;

&lt;h2 id=&#34;进程io&#34;&gt;进程io&lt;/h2&gt;

&lt;p&gt;exec包用于执行外部命令。它包装了os.StartProcess函数以便更容易的修正输入和输出，使用管道连接I/O。主要用于创建一个子进程来执行相关的命令。创建子进程一定要wait，不能出现僵死进程。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;调用脚本命令&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang标准库中提供了两种方式可以用来启动进程调用脚本&lt;/p&gt;

&lt;p&gt;第一种是在os库中的Process类型，Process类型包含一系列方法用来启动进程并对进程进行操作（参考： &lt;a href=&#34;https://golang.org/pkg/os/#Process）&#34;&gt;https://golang.org/pkg/os/#Process）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;示例 使用Process执行脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os&amp;quot;
)

func main() {
    shellPath := &amp;quot;/home/xx/test.sh&amp;quot;
    argv := make([]string, 1) 
    attr := new(os.ProcAttr)
    newProcess, err := os.StartProcess(shellPath, argv, attr)  //运行脚本
    if err != nil {
        fmt.Println(err)
    }
    fmt.Println(&amp;quot;Process PID&amp;quot;, newProcess.Pid)
    processState, err := newProcess.Wait() //等待命令执行完
    if err != nil {
        fmt.Println(err)
    }
    fmt.Println(&amp;quot;processState PID:&amp;quot;, processState.Pid())//获取PID
    fmt.Println(&amp;quot;ProcessExit:&amp;quot;, processState.Exited())//获取进程是否退出
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二种是在os/exec库种通过Cmd类型的各个函数实现对脚本的调用，实际上Cmd是对Process中各种方法的高层次封装（参考： &lt;a href=&#34;https://golang.org/pkg/os/exec/）&#34;&gt;https://golang.org/pkg/os/exec/）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1、LookPath&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LookPath(file string) (string, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在环境变量PATH指定的目录中搜索可执行文件，如file中有斜杠，则只在当前目录搜索。返回完整路径或者相对于当前目录的一个相对路径。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    output, err := exec.LookPath(&amp;quot;ls&amp;quot;)
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf(output)
}

output:

[ `go run test.go` | done: 616.254982ms ]
  /bin/ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、Cmd&lt;/p&gt;

&lt;p&gt;Cmd代表一个正在准备或者在执行中的外部命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Cmd struct {
    // Path是将要执行的命令的路径。
    //
    // 该字段不能为空，如为相对路径会相对于Dir字段。
    Path string
    // Args保管命令的参数，包括命令名作为第一个参数；如果为空切片或者nil，相当于无参数命令。
    //
    // 典型用法下，Path和Args都应被Command函数设定。
    Args []string
    // Env指定进程的环境，如为nil，则是在当前进程的环境下执行。
    Env []string
    // Dir指定命令的工作目录。如为空字符串，会在调用者的进程当前目录下执行。
    Dir string
    // Stdin指定进程的标准输入，如为nil，进程会从空设备读取（os.DevNull）
    Stdin io.Reader
    // Stdout和Stderr指定进程的标准输出和标准错误输出。
    //
    // 如果任一个为nil，Run方法会将对应的文件描述符关联到空设备（os.DevNull）
    //
    // 如果两个字段相同，同一时间最多有一个线程可以写入。
    Stdout io.Writer
    Stderr io.Writer
    // ExtraFiles指定额外被新进程继承的已打开文件流，不包括标准输入、标准输出、标准错误输出。
    // 如果本字段非nil，entry i会变成文件描述符3+i。
    //
    // BUG: 在OS X 10.6系统中，子进程可能会继承不期望的文件描述符。
    // http://golang.org/issue/2603
    ExtraFiles []*os.File
    // SysProcAttr保管可选的、各操作系统特定的sys执行属性。
    // Run方法会将它作为os.ProcAttr的Sys字段传递给os.StartProcess函数。
    SysProcAttr *syscall.SysProcAttr
    // Process是底层的，只执行一次的进程。
    Process *os.Process
    // ProcessState包含一个已经存在的进程的信息，只有在调用Wait或Run后才可用。
    ProcessState *os.ProcessState
    // 内含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用Command来创建cmd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Command(name string, arg ...string) *Cmd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数返回一个*Cmd，用于使用给出的参数执行name指定的程序。返回值只设定了Path和Args两个参数。&lt;/p&gt;

&lt;p&gt;如果name不含路径分隔符，将使用LookPath获取完整路径；否则直接使用name。参数arg不应包含命令名&lt;/p&gt;

&lt;p&gt;使用Run运行cmd命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Run() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run执行c包含的命令，并阻塞直到完成。如果命令成功执行，stdin、stdout、stderr的转交没有问题，并且返回状态码为0，方法的返回值为nil；如果命令没有执行或者执行失败，会返回错误；&lt;/p&gt;

&lt;p&gt;使用Start和wait来运行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Start() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start开始执行c包含的命令，但并不会等待该命令完成即返回。可以配合使用Wait方法来达到和Run一样的效果。wait方法会返回命令的返回状态码并在命令返回后释放相关的资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Wait() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait会阻塞直到该命令执行完成，该命令必须是被Start方法开始执行的。&lt;/p&gt;

&lt;p&gt;通过Run的源码可以看出其实Run方法内部也是调用了Start和Wait方法。Run方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Run() error {
    if err := c.Start(); err != nil {
        return err
    }
    return c.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    cmd := exec.Command(&amp;quot;tr&amp;quot;, &amp;quot;a-z&amp;quot;, &amp;quot;A-Z&amp;quot;)
    cmd.Stdin = strings.NewReader(&amp;quot;abc def&amp;quot;)
    var out bytes.Buffer
    cmd.Stdout = &amp;amp;out
    err := cmd.Run()
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;GOGOGO: %q\n&amp;quot;, out.String())
}

output:

[ `go run test.go` | done: 286.798242ms ]
  GOGOGO: &amp;quot;ABC DEF&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用Output输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Output() ([]byte, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行命令并返回标准输出的切片。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) CombinedOutput() ([]byte, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行命令并返回标准输出和错误输出合并的切片.&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    out, err := exec.Command(&amp;quot;date&amp;quot;).Output()
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;The date is %s\n&amp;quot;, out)
}

output:

[ `go run test.go` | done: 585.495467ms ]
  The date is Tue Aug  1 19:24:11 CST 2017
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用pipe&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Cmd) StdinPipe
func (c *Cmd) StdinPipe() (io.WriteCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StdinPipe方法返回一个在命令Start后与命令标准输入关联的管道。Wait方法获知命令结束后会关闭这个管道。必要时调用者可以调用Close方法来强行关闭管道，例如命令在输入关闭后才会执行返回时需要显式关闭管道。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Cmd) StdoutPipe
func (c *Cmd) StdoutPipe() (io.ReadCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StdoutPipe方法返回一个在命令Start后与命令标准输出关联的管道。Wait方法获知命令结束后会关闭这个管道，一般不需要显式的关闭该管道。但是在从管道读取完全部数据之前调用Wait是错误的；同样使用StdoutPipe方法时调用Run函数也是错误的。例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Cmd) StderrPipe
func (c *Cmd) StderrPipe() (io.ReadCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StderrPipe方法返回一个在命令Start后与命令标准错误输出关联的管道。Wait方法获知命令结束后会关闭这个管道，一般不需要显式的关闭该管道。但是在从管道读取完全部数据之前调用Wait是错误的；同样使用StderrPipe方法时调用Run函数也是错误的。请参照StdoutPipe的例子。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    cmd := exec.Command(&amp;quot;echo&amp;quot;, &amp;quot;-n&amp;quot;, `{&amp;quot;Name&amp;quot;: &amp;quot;Bob&amp;quot;, &amp;quot;Age&amp;quot;: 32}`)
    stdout, err := cmd.StdoutPipe()
    if err != nil {
        log.Fatal(err)
    }
    if err := cmd.Start(); err != nil {
        log.Fatal(err)
    }
    var person struct {
        Name string
        Age  int
    }
    json.NewDecoder(r)
    if err := json.NewDecoder(stdout).Decode(&amp;amp;person); err != nil {
        log.Fatal(err)
    }
    if err := cmd.Wait(); err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;%s is %d years old\n&amp;quot;, person.Name, person.Age)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取命令返回值&lt;/p&gt;

&lt;p&gt;实际上脚本或命令执行完后，会将结果返回到ProcessState中的status去， 但是status不是export的，所以我们需要通过一些手段将脚本返回值从syscall.WaitStatus找出来&lt;/p&gt;

&lt;p&gt;ProcessState定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ProcessState struct {
    pid    int                // The process&#39;s id.
    status syscall.WaitStatus // System-dependent status info.
    rusage *syscall.Rusage
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于上面使用Cmd的例子，可以在进程退出后可以通过以下语句获取到返回值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(&amp;quot;Exit Code&amp;quot;, command.ProcessState.Sys().(syscall.WaitStatus).ExitStatus())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用Process方式的也可以通过对ProcessState通过相同的方式获取到返回结果。&lt;/p&gt;

&lt;h1 id=&#34;os-user&#34;&gt;os/user&lt;/h1&gt;

&lt;p&gt;os/user 模块的主要作用是通过用户名或者 id 从而获取系统用户的相关属性。&lt;/p&gt;

&lt;h2 id=&#34;类型-1&#34;&gt;类型&lt;/h2&gt;

&lt;p&gt;User 结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type User struct {
    Uid      string
    Gid      string
    Username string
    Name     string
    HomeDir  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;User 代表一个用户账户：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Uid ：用户的 ID&lt;/li&gt;
&lt;li&gt;Gid ：用户所属组的 ID，如果属于多个组，那么此 ID 为主组的 ID&lt;/li&gt;
&lt;li&gt;Username ：用户名&lt;/li&gt;
&lt;li&gt;Name ：属组名称，如果属于多个组，那么此名称为主组的名称&lt;/li&gt;
&lt;li&gt;HomeDir ：用户的宿主目录&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;方法&#34;&gt;方法&lt;/h2&gt;

&lt;p&gt;返回当前用户。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Current() (*User, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过用户名查找用户，如果没有找到这个用户那么将返回 UnknownUserError 错误类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Lookup(username string) (*User, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过用户 ID 查找用户，如果没有找到这个用户那么将返回 UnknownUserIdError 错误类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LookupId(uid string) (*User, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os/user&amp;quot;
    &amp;quot;reflect&amp;quot;
)

func main() {
    fmt.Println(&amp;quot;== 测试 Current 正常情况 ==&amp;quot;)
    if u, err := user.Current(); err == nil {
        fmt.Println(&amp;quot;用户ID: &amp;quot; + u.Uid)
        fmt.Println(&amp;quot;主组ID: &amp;quot; + u.Gid)
        fmt.Println(&amp;quot;用户名: &amp;quot; + u.Username)
        fmt.Println(&amp;quot;主组名: &amp;quot; + u.Name)
        fmt.Println(&amp;quot;家目录: &amp;quot; + u.HomeDir)
    }

    fmt.Println(&amp;quot;== 测试 Lookup 正常情况 ==&amp;quot;)
    if u, err := user.Lookup(&amp;quot;root&amp;quot;); err == nil {
        fmt.Println(&amp;quot;用户ID: &amp;quot; + u.Uid)
        fmt.Println(&amp;quot;主组ID: &amp;quot; + u.Gid)
        fmt.Println(&amp;quot;用户名: &amp;quot; + u.Username)
        fmt.Println(&amp;quot;主组名: &amp;quot; + u.Name)
        fmt.Println(&amp;quot;家目录: &amp;quot; + u.HomeDir)
    }
    fmt.Println(&amp;quot;== 测试 Lookup 异常情况 ==&amp;quot;)
    if _, err := user.Lookup(&amp;quot;roo&amp;quot;); err == nil {
    } else {
        fmt.Println(&amp;quot;错误信息: &amp;quot; + err.Error())
        fmt.Print(&amp;quot;错误类型: &amp;quot;)
        fmt.Println(reflect.TypeOf(err))
    }

    fmt.Println(&amp;quot;== 测试 LookupId 正常情况 ==&amp;quot;)
    if u, err := user.LookupId(&amp;quot;0&amp;quot;); err == nil {
        fmt.Println(&amp;quot;用户ID: &amp;quot; + u.Uid)
        fmt.Println(&amp;quot;主组ID: &amp;quot; + u.Gid)
        fmt.Println(&amp;quot;用户名: &amp;quot; + u.Username)
        fmt.Println(&amp;quot;主组名: &amp;quot; + u.Name)
        fmt.Println(&amp;quot;家目录: &amp;quot; + u.HomeDir)
    }
    fmt.Println(&amp;quot;== 测试 LookupId 异常情况 ==&amp;quot;)
    if _, err := user.LookupId(&amp;quot;10000&amp;quot;); err == nil {
    } else {
        fmt.Println(&amp;quot;错误信息: &amp;quot; + err.Error())
        fmt.Print(&amp;quot;错误类型: &amp;quot;)
        fmt.Println(reflect.TypeOf(err))
    }

}
输出结果如下：

== 测试 Current 正常情况 ==
用户ID: 0
主组ID: 0
用户名: root
主组名: root
家目录: /root
== 测试 Lookup 正常情况 ==
用户ID: 0
主组ID: 0
用户名: root
主组名: root
家目录: /root
== 测试 Lookup 异常情况 ==
错误信息: user: unknown user roo
错误类型: user.UnknownUserError
== 测试 LookupId 正常情况 ==
用户ID: 0
主组ID: 0
用户名: root
主组名: root
家目录: /root
== 测试 LookupId 异常情况 ==
错误信息: user: unknown userid 10000
错误类型: user.UnknownUserIdError
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Fmt</title>
          <link>https://kingjcy.github.io/post/golang/go-fmt/</link>
          <pubDate>Mon, 30 May 2016 11:57:05 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-fmt/</guid>
          <description>&lt;p&gt;fmt是实现了格式化的I/O函数，这点类似Ｃ语言中的printf和scanf，但是更加简单。&lt;/p&gt;

&lt;h1 id=&#34;print&#34;&gt;Print&lt;/h1&gt;

&lt;h2 id=&#34;基本函数&#34;&gt;基本函数&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Print&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;// Print 将参数列表 a 中的各个参数转换为字符串并写入到标准输出中。
// 非字符串参数之间会添加空格，返回写入的字节数。
func Print(a ...interface{}) (n int, err error)

// Println 功能类似 Print，只不过最后会添加一个换行符。
// 所有参数之间会添加空格，返回写入的字节数。
func Println(a ...interface{}) (n int, err error)

// Printf 将参数列表 a 填写到格式字符串 format 的占位符中。
// 填写后的结果写入到标准输出中，返回写入的字节数。
func Printf(format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Fprint&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过将转换结果写入到 w 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Fprint(w io.Writer, a ...interface{}) (n int, err error)
func Fprintln(w io.Writer, a ...interface{}) (n int, err error)
func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Sprint&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过将转换结果以字符串形式返回。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Sprint(a ...interface{}) string
func Sprintln(a ...interface{}) string
func Sprintf(format string, a ...interface{}) string
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Errorf&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同 Sprintf，只不过结果字符串被包装成了 error 类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Errorf(format string, a ...interface{}) error
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    fmt.Print(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, 1, 2, 3, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;\n&amp;quot;)
    fmt.Println(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, 1, 2, 3, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
    fmt.Printf(&amp;quot;ab %d %d %d cd\n&amp;quot;, 1, 2, 3)
    // ab1 2 3cd
    // a b 1 2 3 c d
    // ab 1 2 3 cd

    if err := percent(30, 70, 90, 160); err != nil {
        fmt.Println(err)
    }
    // 30%
    // 70%
    // 90%
    // 数值 160 超出范围（100）
}

func percent(i ...int) error {
    for _, n := range i {
        if n &amp;gt; 100 {
            return fmt.Errorf(&amp;quot;数值 %d 超出范围（100）&amp;quot;, n)
        }
        fmt.Print(n, &amp;quot;%\n&amp;quot;)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自定义format类型&#34;&gt;自定义format类型&lt;/h2&gt;

&lt;p&gt;Formatter 由自定义类型实现，用于实现该类型的自定义格式化过程。&lt;/p&gt;

&lt;p&gt;当格式化器需要格式化该类型的变量时，会调用其 Format 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Formatter interface {
    // f 用于获取占位符的旗标、宽度、精度等信息，也用于输出格式化的结果
    // c 是占位符中的动词
    Format(f State, c rune)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由格式化器（Print 之类的函数）实现，用于给自定义格式化过程提供信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type State interface {
    // Formatter 通过 Write 方法将格式化结果写入格式化器中，以便输出。
    Write(b []byte) (ret int, err error)
    // Formatter 通过 Width 方法获取占位符中的宽度信息及其是否被设置。
    Width() (wid int, ok bool)
    // Formatter 通过 Precision 方法获取占位符中的精度信息及其是否被设置。
    Precision() (prec int, ok bool)
    // Formatter 通过 Flag 方法获取占位符中的旗标[+- 0#]是否被设置。
    Flag(c int) bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stringer 由自定义类型实现，用于实现该类型的自定义格式化过程。&lt;/p&gt;

&lt;p&gt;当格式化器需要输出该类型的字符串格式时就会调用其 String 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Stringer interface {
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stringer 由自定义类型实现，用于实现该类型的自定义格式化过程。&lt;/p&gt;

&lt;p&gt;当格式化器需要输出该类型的 Go 语法字符串（%#v）时就会调用其 String 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type GoStringer interface {
    GoString() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Ustr string

func (us Ustr) String() string {
    return strings.ToUpper(string(us))
}

func (us Ustr) GoString() string {
    return `&amp;quot;` + strings.ToUpper(string(us)) + `&amp;quot;`
}

func (u Ustr) Format(f fmt.State, c rune) {
    write := func(s string) {
        f.Write([]byte(s))
    }
    switch c {
    case &#39;m&#39;, &#39;M&#39;:
        write(&amp;quot;旗标：[&amp;quot;)
        for s := &amp;quot;+- 0#&amp;quot;; len(s) &amp;gt; 0; s = s[1:] {
            if f.Flag(int(s[0])) {
                write(s[:1])
            }
        }
        write(&amp;quot;]&amp;quot;)
        if v, ok := f.Width(); ok {
            write(&amp;quot; | 宽度：&amp;quot; + strconv.FormatInt(int64(v), 10))
        }
        if v, ok := f.Precision(); ok {
            write(&amp;quot; | 精度：&amp;quot; + strconv.FormatInt(int64(v), 10))
        }
    case &#39;s&#39;, &#39;v&#39;: // 如果使用 Format 函数，则必须自己处理所有格式，包括 %#v
        if c == &#39;v&#39; &amp;amp;&amp;amp; f.Flag(&#39;#&#39;) {
            write(u.GoString())
        } else {
            write(u.String())
        }
    default: // 如果使用 Format 函数，则必须自己处理默认输出
        write(&amp;quot;无效格式：&amp;quot; + string(c))
    }
}

func main() {
    u := Ustr(&amp;quot;Hello World!&amp;quot;)
    // &amp;quot;-&amp;quot; 标记和 &amp;quot;0&amp;quot; 标记不能同时存在
    fmt.Printf(&amp;quot;%-+ 0#8.5m\n&amp;quot;, u) // 旗标：[+- #] | 宽度：8 | 精度：5
    fmt.Printf(&amp;quot;%+ 0#8.5M\n&amp;quot;, u)  // 旗标：[+ 0#] | 宽度：8 | 精度：5
    fmt.Println(u)                // HELLO WORLD!
    fmt.Printf(&amp;quot;%s\n&amp;quot;, u)         // HELLO WORLD!
    fmt.Printf(&amp;quot;%#v\n&amp;quot;, u)        // &amp;quot;HELLO WORLD!&amp;quot;
    fmt.Printf(&amp;quot;%d\n&amp;quot;, u)         // 无效格式：d
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;scan&#34;&gt;Scan&lt;/h1&gt;

&lt;h2 id=&#34;基本函数-1&#34;&gt;基本函数&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Scan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Scan 从标准输入中读取数据，并将数据用空白分割并解析后存入 a 提供的变量中（换行符会被当作空白处理），变量必须以指针传入。当读到 EOF 或所有变量都填写完毕则停止扫描。返回成功解析的参数数量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Scan(a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scanln 和 Scan 类似，只不过遇到换行符就停止扫描。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Scanln(a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scanf 从标准输入中读取数据，并根据格式字符串 format 对数据进行解析，将解析结果存入参数 a 所提供的变量中，变量必须以指针传入。输入端的换行符必须和 format 中的换行符相对应（如果格式字符串中有换行符，则输入端必须输入相应的换行符）。占位符 %c 总是匹配下一个字符，包括空白，比如空格符、制表符、换行符。返回成功解析的参数数量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Scanf(format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Fscan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过从 r 中读取数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Fscan(r io.Reader, a ...interface{}) (n int, err error)
func Fscanln(r io.Reader, a ...interface{}) (n int, err error)
func Fscanf(r io.Reader, format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Sscan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过从 str 中读取数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Sscan(str string, a ...interface{}) (n int, err error)
func Sscanln(str string, a ...interface{}) (n int, err error)
func Sscanf(str string, format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;// 对于 Scan 而言，回车视为空白
func main() {
    a, b, c := &amp;quot;&amp;quot;, 0, false
    fmt.Scan(&amp;amp;a, &amp;amp;b, &amp;amp;c)
    fmt.Println(a, b, c)
    // 在终端执行后，输入 abc 1 回车 true 回车
    // 结果 abc 1 true
}

// 对于 Scanln 而言，回车结束扫描
func main() {
    a, b, c := &amp;quot;&amp;quot;, 0, false
    fmt.Scanln(&amp;amp;a, &amp;amp;b, &amp;amp;c)
    fmt.Println(a, b, c)
    // 在终端执行后，输入 abc 1 true 回车
    // 结果 abc 1 true
}

// 格式字符串可以指定宽度
func main() {
    a, b, c := &amp;quot;&amp;quot;, 0, false
    fmt.Scanf(&amp;quot;%4s%d%t&amp;quot;, &amp;amp;a, &amp;amp;b, &amp;amp;c)
    fmt.Println(a, b, c)
    // 在终端执行后，输入 1234567true 回车
    // 结果 1234 567 true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从键盘和标准输入 os.Stdin 读取输入，最简单的办法是使用 fmt 包提供的 Scan 和 Sscan 开头的函数。&lt;/p&gt;

&lt;p&gt;从控制台读取输入:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import &amp;quot;fmt&amp;quot;

var (
   firstName, lastName, s string
   i int
   f float32
   input = &amp;quot;56.12 / 5212 / Go&amp;quot;
   format = &amp;quot;%f / %d / %s&amp;quot;
)

func main() {
   fmt.Println(&amp;quot;Please enter your full name: &amp;quot;)
   fmt.Scanln(&amp;amp;firstName, &amp;amp;lastName)
   // fmt.Scanf(&amp;quot;%s %s&amp;quot;, &amp;amp;firstName, &amp;amp;lastName)
   fmt.Printf(&amp;quot;Hi %s %s!\n&amp;quot;, firstName, lastName) // Hi Chris Naegels
   fmt.Sscanf(input, format, &amp;amp;f, &amp;amp;i, &amp;amp;s)
   fmt.Println(&amp;quot;From the string we read: &amp;quot;, f, i, s)
    // 输出结果: From the string we read: 56.12 5212 Go
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scanln 扫描来自标准输入的文本，将空格分隔的值依次存放到后续的参数内，直到碰到换行。&lt;/p&gt;

&lt;p&gt;Scanf 与其类似，除了 Scanf 的第一个参数用作格式字符串，用来决定如何读取。&lt;/p&gt;

&lt;p&gt;Sscan 和以 Sscan 开头的函数则是从字符串读取，除此之外，与 Scanf相同。如果这些函数读取到的结果与您预想的不同，您可以检查成功读入数据的个数和返回的错误。&lt;/p&gt;

&lt;p&gt;也可以使用 bufio 包提供的缓冲读取（buffered reader）来读取数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;bufio&amp;quot;
    &amp;quot;os&amp;quot;
)

var inputReader *bufio.Reader
var input string
var err error

func main() {
    inputReader = bufio.NewReader(os.Stdin)
    fmt.Println(&amp;quot;Please enter some input: &amp;quot;)
    input, err = inputReader.ReadString(&#39;\n&#39;)
    if err == nil {
        fmt.Printf(&amp;quot;The input was: %s\n&amp;quot;, input)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自定义format类型-1&#34;&gt;自定义format类型&lt;/h2&gt;

&lt;p&gt;Scanner 由自定义类型实现，用于实现该类型的自定义扫描过程。&lt;/p&gt;

&lt;p&gt;当扫描器需要解析该类型的数据时，会调用其 Scan 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Scanner interface {
    // state 用于获取占位符中的宽度信息，也用于从扫描器中读取数据进行解析。
    // verb 是占位符中的动词
    Scan(state ScanState, verb rune) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由扫描器（Scan 之类的函数）实现，用于给自定义扫描过程提供数据和信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ScanState interface {
    // ReadRune 从扫描器中读取一个字符，如果用在 Scanln 类的扫描器中，
    // 则该方法会在读到第一个换行符之后或读到指定宽度之后返回 EOF。
    // 返回“读取的字符”和“字符编码所占用的字节数”
    ReadRune() (r rune, size int, err error)
    // UnreadRune 撤消最后一次的 ReadRune 操作，
    // 使下次的 ReadRune 操作得到与前一次 ReadRune 相同的结果。
    UnreadRune() error
    // SkipSpace 为 Scan 方法提供跳过开头空白的能力。
    // 根据扫描器的不同（Scan 或 Scanln）决定是否跳过换行符。
    SkipSpace()
    // Token 用于从扫描器中读取符合要求的字符串，
    // Token 从扫描器中读取连续的符合 f(c) 的字符 c，准备解析。
    // 如果 f 为 nil，则使用 !unicode.IsSpace(c) 代替 f(c)。
    // skipSpace：是否跳过开头的连续空白。返回读取到的数据。
    // 注意：token 指向共享的数据，下次的 Token 操作可能会覆盖本次的结果。
    Token(skipSpace bool, f func(rune) bool) (token []byte, err error)
    // Width 返回占位符中的宽度值以及宽度值是否被设置
    Width() (wid int, ok bool)
    // 因为上面实现了 ReadRune 方法，所以 Read 方法永远不应该被调用。
    // 一个好的 ScanState 应该让 Read 直接返回相应的错误信息。
    Read(buf []byte) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Ustr string

func (u *Ustr) Scan(state fmt.ScanState, verb rune) (err error) {
    var s []byte
    switch verb {
    case &#39;S&#39;:
        s, err = state.Token(true, func(c rune) bool { return &#39;A&#39; &amp;lt;= c &amp;amp;&amp;amp; c &amp;lt;= &#39;Z&#39; })
        if err != nil {
            return
        }
    case &#39;s&#39;, &#39;v&#39;:
        s, err = state.Token(true, func(c rune) bool { return &#39;a&#39; &amp;lt;= c &amp;amp;&amp;amp; c &amp;lt;= &#39;z&#39; })
        if err != nil {
            return
        }
    default:
        return fmt.Errorf(&amp;quot;无效格式：%c&amp;quot;, verb)
    }
    *u = Ustr(s)
    return nil
}

func main() {
    var a, b, c, d, e Ustr
    n, err := fmt.Scanf(&amp;quot;%3S%S%3s%2v%x&amp;quot;, &amp;amp;a, &amp;amp;b, &amp;amp;c, &amp;amp;d, &amp;amp;e)
    fmt.Println(a, b, c, d, e)
    fmt.Println(n, err)
    // 在终端执行后，输入 ABCDEFGabcdefg 回车
    // 结果：
    // ABC DEFG abc de
    // 4 无效格式：x
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- flag</title>
          <link>https://kingjcy.github.io/post/golang/go-flag/</link>
          <pubDate>Sun, 29 May 2016 10:09:03 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-flag/</guid>
          <description>&lt;p&gt;golang自身带的命令行包flag，各种case，有代码洁癖的人看着就令人头大,我们一般使用其他的命令行解析包比如pflag，cobra等，cobra是个非常不错的命令行包(golang命令行解析库)，docker，hugo都在使用.&lt;/p&gt;

&lt;h1 id=&#34;cobra&#34;&gt;cobra&lt;/h1&gt;

&lt;h2 id=&#34;基命令&#34;&gt;基命令&lt;/h2&gt;

&lt;p&gt;首先创建一个基命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package cmd

import (
    &amp;quot;github.com/spf13/cobra&amp;quot;
)

var RootCmd = &amp;amp;cobra.Command{
    Use: &amp;quot;gonne&amp;quot;,
    Run: func(cmd *cobra.Command, args []string) {
        println(&amp;quot;gonne is my ai friend&amp;quot;)
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用命令&lt;/p&gt;

&lt;p&gt;在main方法中调用命令，恩，就这么简单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os&amp;quot;

    &amp;quot;lastsweetop.com/cmd&amp;quot;
)

func main() {
    if err := cmd.RootCmd.Execute(); err != nil {
        fmt.Println(err)
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在命令行输入 gonne，就会执行基命令中Run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appletekiMacBook-Pro:src apple$ gonne
gonne is my ai friend
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;子命令&#34;&gt;子命令&lt;/h2&gt;

&lt;p&gt;在基命令上增加子命令也相当简单，根本无需在基命令和main方法中写任何代码，只需新建一个go文件，多个子命令间也是相互独立的，多么优雅的代码，告别各种case&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package cmd

import &amp;quot;github.com/spf13/cobra&amp;quot;

func init() {
    RootCmd.AddCommand(versionCmd)
}

var versionCmd = &amp;amp;cobra.Command{
    Use:   &amp;quot;version&amp;quot;,
    Short: &amp;quot;Print the version number of Gonne&amp;quot;,
    Run: func(cmd *cobra.Command, args []string) {
        println(&amp;quot;gonne version is 0.0.1&amp;quot;)
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appletekiMacBook-Pro:src apple$ gonne version
gonne version is 0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;启动命令&#34;&gt;启动命令&lt;/h2&gt;

&lt;p&gt;我们先来个非后台运行的启动命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    startCmd := &amp;amp;cobra.Command{
        Use:   &amp;quot;start&amp;quot;,
        Short: &amp;quot;Start Gonne&amp;quot;,
        Run: func(cmd *cobra.Command, args []string) {
            startHttp()
        },
    }
    startCmd.Flags().BoolVarP(&amp;amp;daemon, &amp;quot;deamon&amp;quot;, &amp;quot;d&amp;quot;, false, &amp;quot;is daemon?&amp;quot;)
    RootCmd.AddCommand(startCmd)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;startHttp方法启动一个http的web服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func startHttp() {
    http.HandleFunc(&amp;quot;/&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
        fmt.Fprintf(w, &amp;quot;Hello cmd!&amp;quot;)
    })
    if err := http.ListenAndServe(&amp;quot;:9090&amp;quot;, nil); err != nil {
        log.Fatal(&amp;quot;ListenAndServe: &amp;quot;, err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在通过gonne start便可以启动一个web服务了，但是程序停留在命令行，如果ctrl+C程序也会终止了&lt;/p&gt;

&lt;h2 id=&#34;命令行参数&#34;&gt;命令行参数&lt;/h2&gt;

&lt;p&gt;如果想要后台启动，那么得让start命令知道是要后台运行的，参照docker命令行的方式就是加上-d，给一个命令添加参数的判断只需很少的代码&lt;/p&gt;

&lt;p&gt;改造一下代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    var daemon bool
    startCmd := &amp;amp;cobra.Command{
        Use:   &amp;quot;start&amp;quot;,
        Short: &amp;quot;Start Gonne&amp;quot;,
        Run: func(cmd *cobra.Command, args []string) {
            if daemon {
        fmt.Println(&amp;quot;gonne start&amp;quot;,daemon)        
            }
            startHttp()
        },
    }
    startCmd.Flags().BoolVarP(&amp;amp;daemon, &amp;quot;deamon&amp;quot;, &amp;quot;d&amp;quot;, false, &amp;quot;is daemon?&amp;quot;)
    RootCmd.AddCommand(startCmd)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;命令行输入&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gonne start -d
1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以接收到-d参数了，这里要说明一下，第一个参数取值，第二个参数代码&amp;ndash;deamon，第三个参数代表-d ,第四个参数代码不加-d时候的默认值，第五参数是描述&lt;/p&gt;

&lt;h2 id=&#34;后台运行&#34;&gt;后台运行&lt;/h2&gt;

&lt;p&gt;后台运行其实这里使用的是一个巧妙的方法，就是使用系统的command命令行启动自己的命令行输入，是不是有点绕，再看看看改造后的代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Run: func(cmd *cobra.Command, args []string) {
  if daemon {
    command := exec.Command(&amp;quot;gonne&amp;quot;, &amp;quot;start&amp;quot;)
    command.Start()
    fmt.Printf(&amp;quot;gonne start, [PID] %d running...\n&amp;quot;, command.Process.Pid)
    ioutil.WriteFile(&amp;quot;gonne.lock&amp;quot;, []byte(fmt.Sprintf(&amp;quot;%d&amp;quot;, command.Process.Pid)), 0666)
    daemon = false
    os.Exit(0)
  } else {
    fmt.Println(&amp;quot;gonne start&amp;quot;)
  }
  startHttp()
},
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用exec的Command启动刚输入的gonne start -d，就会拦截到这条请求然后通过gonne start,但是程序就不会停留在命令行了，然后发现http服务还在，还可以访问。&lt;/p&gt;

&lt;p&gt;还有一点就是把pid输出到gonne.lock文件，给停止的程序调用&lt;/p&gt;

&lt;h2 id=&#34;终止后台程序&#34;&gt;终止后台程序&lt;/h2&gt;

&lt;p&gt;有了之前的操作后，停止就简单多了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    RootCmd.AddCommand(stopCmd)
}

var stopCmd = &amp;amp;cobra.Command{
    Use:   &amp;quot;stop&amp;quot;,
    Short: &amp;quot;Stop Gonne&amp;quot;,
    Run: func(cmd *cobra.Command, args []string) {
        strb, _ := ioutil.ReadFile(&amp;quot;gonne.lock&amp;quot;)
        command := exec.Command(&amp;quot;kill&amp;quot;, string(strb))
        command.Start()
        println(&amp;quot;gonne stop&amp;quot;)
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行 gonne stop 即可终止之前启动的http服务&lt;/p&gt;

&lt;h2 id=&#34;help命令&#34;&gt;help命令&lt;/h2&gt;

&lt;p&gt;好了，关于命令的操作讲完了，再看看cobra给的福利，自动生成的help命令&lt;/p&gt;

&lt;p&gt;这个不需要你做什么操作，只需要输入gonne help,相关信息已经帮你生产好了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appletekiMacBook-Pro:andev apple$ gonne help
Usage:
  gonne [flags]
  gonne [command]

Available Commands:
  help        Help about any command
  start       Start Gonne
  stop        Stop Gonne
  version     Print the version number of Gonne

Flags:
  -h, --help   help for gonne

Use &amp;quot;gonne [command] --help&amp;quot; for more information about a command.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，子命令也有&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appletekiMacBook-Pro:andev apple$ gonne start -h
Start Gonne

Usage:
  gonne start [flags]

Flags:
  -d, --deamon   is daemon?
  -h, --help     help for start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;自此告别各种脚本&lt;/p&gt;

&lt;h1 id=&#34;flag&#34;&gt;flag&lt;/h1&gt;

&lt;p&gt;golang自带的一个解析命令行参数的方法或库，是经常用的。&lt;/p&gt;

&lt;h2 id=&#34;结构体和默认实例&#34;&gt;结构体和默认实例&lt;/h2&gt;

&lt;p&gt;首先&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type FlagSet struct {
    // Usage is the function called when an error occurs while parsing flags.
    // The field is a function (not a method) that may be changed to point to
    // a custom error handler. What happens after Usage is called depends
    // on the ErrorHandling setting; for the command line, this defaults
    // to ExitOnError, which exits the program after calling Usage.
    Usage func()

    name          string
    parsed        bool
    actual        map[string]*Flag
    formal        map[string]*Flag
    args          []string // arguments after flags
    errorHandling ErrorHandling
    output        io.Writer // nil means stderr; use out() accessor
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个flag的集合&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Flag struct {
    Name     string // flag在命令行中的名字
    Usage    string // 帮助信息
    Value    Value  // 要设置的值
    DefValue string // 默认值（文本格式），用于使用信息
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;flag库中使用了CommandLine对flagset进行了初始化，默认传入参数是启动文件名&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var CommandLine = NewFlagSet(os.Args[0], ExitOnError)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个flagset的指针。&lt;/p&gt;

&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;

&lt;p&gt;定义 flags 有三种方式&lt;/p&gt;

&lt;p&gt;1）flag.Xxx()，其中 Xxx 可以是 Int、String 等；返回一个相应类型的指针，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ip = flag.Int(&amp;quot;flagname&amp;quot;, 1234, &amp;quot;help message for flagname&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种模式其实是对第二种模式的一种封装，是一种最顶层的使用，其实是使用默认的CommandLine实例调用下面这种方式声明的XxxVar（）函数。对应的第一个变量直接new一个，其实这个是用于存储默认值的。&lt;/p&gt;

&lt;p&gt;2）flag.XxxVar()，将 flag 绑定到一个变量上，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var flagvar int
flag.IntVar(&amp;amp;flagvar, &amp;quot;flagname&amp;quot;, 1234, &amp;quot;help message for flagname&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种模式其实是对自定义的一种封装，其实就是调用了var（）函数。&lt;/p&gt;

&lt;p&gt;3）还可以创建自定义 flag，只要实现 flag.Value 接口即可（要求 receiver 是指针），这时候可以通过如下方式定义该 flag：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flag.Var(&amp;amp;flagVal, &amp;quot;name&amp;quot;, &amp;quot;help message for flagname&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边才是真正的实现，其实就是将启动参数按着flag的结构体存储到flagset种的formal这个map中去，最后给parse去解析。第一个参数是一个value的接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Value interface {
    String() string
    Set(string) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用于接收定义类型的结构体对象指针，最终找到对应方法的实现。&lt;/p&gt;

&lt;p&gt;自定义这个可以举个例子加强理解&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type sliceValue []string

func newSliceValue(vals []string, p *[]string) *sliceValue {
    *p = vals
    return (*sliceValue)(p)
}

func (s *sliceValue) Set(val string) error {
    *s = sliceValue(strings.Split(val, &amp;quot;,&amp;quot;))
    return nil
}

func (s *sliceValue) Get() interface{} { return []string(*s) }

func (s *sliceValue) String() string { return strings.Join([]string(*s), &amp;quot;,&amp;quot;) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后可以这么使用：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var languages []string
flag.Var(newSliceValue([]string{}, &amp;amp;languages), &amp;quot;slice&amp;quot;, &amp;quot;I like programming `languages`&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样通过 -slice &amp;ldquo;go,php&amp;rdquo; 这样的形式传递参数，languages 得到的就是 [go, php]。&lt;/p&gt;

&lt;p&gt;flag 中对 Duration 这种非基本类型的支持，使用的就是类似这样的方式。&lt;/p&gt;

&lt;h2 id=&#34;解析&#34;&gt;解析&lt;/h2&gt;

&lt;p&gt;在所有的 flag 定义完成之后，可以通过调用 flag.Parse() 进行解析。&lt;/p&gt;

&lt;p&gt;命令行 flag 的语法有如下三种形式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-flag // 只支持bool类型
-flag=x
-flag x // 只支持非bool类型
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;flag&amp;quot;
    &amp;quot;fmt&amp;quot;
)

var inputName = flag.String(&amp;quot;name&amp;quot;, &amp;quot;CHENJIAN&amp;quot;, &amp;quot;Input Your Name.&amp;quot;)
var inputAge = flag.Int(&amp;quot;age&amp;quot;, 27, &amp;quot;Input Your Age&amp;quot;)
var inputGender = flag.String(&amp;quot;gender&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;Input Your Gender&amp;quot;)
var inputFlagvar int

func Init() {
    flag.IntVar(&amp;amp;inputFlagvar, &amp;quot;flagname&amp;quot;, 1234, &amp;quot;Help&amp;quot;)
}
func main() {
    Init()
    flag.Parse()
    // func Args() []string
    // Args returns the non-flag command-line arguments.
    // func NArg() int
    // NArg is the number of arguments remaining after flags have been processed.
    fmt.Printf(&amp;quot;args=%s, num=%d\n&amp;quot;, flag.Args(), flag.NArg())
    for i := 0; i != flag.NArg(); i++ {
        fmt.Printf(&amp;quot;arg[%d]=%s\n&amp;quot;, i, flag.Arg(i))
    }
    fmt.Println(&amp;quot;name=&amp;quot;, *inputName)
    fmt.Println(&amp;quot;age=&amp;quot;, *inputAge)
    fmt.Println(&amp;quot;gender=&amp;quot;, *inputGender)
    fmt.Println(&amp;quot;flagname=&amp;quot;, inputFlagvar)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;操作:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go build example_flag.go

./example_flag -h

&amp;lt;&amp;lt;&#39;COMMENT&#39;
Usage of ./exampleFlag:
  -age int
        Input Your Age (default 27)
  -flagname int
        Help (default 1234)
  -gender string
        Input Your Gender (default &amp;quot;female&amp;quot;)
  -name string
        Input Your Name. (default &amp;quot;CHENJIAN&amp;quot;)
COMMENT

 ./example_flag chenjian

 &amp;lt;&amp;lt;&#39;COMMENT&#39;
args=[chenjian], num=1
arg[0]=chenjian
name= CHENJIAN
age= 27
gender= female
flagname= 1234
COMMENT

./example_flag -name balbalba -age 1111 -flagname=12333 dfdf xccccc eette

 &amp;lt;&amp;lt;&#39;COMMENT&#39;
args=[dfdf xccccc eette], num=3
arg[0]=dfdf
arg[1]=xccccc
arg[2]=eette
name= balbalba
age= 1111
gender= female
flagname= 12333
COMMENT
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;kingpin&#34;&gt;kingpin&lt;/h1&gt;

&lt;p&gt;功能比flag库强大，用法差不多，相比flag库，最重要的一点就是支持不加&amp;rdquo;-&amp;ldquo;的调用。&lt;/p&gt;

&lt;p&gt;下面实例就说明了大部分的用法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;os&amp;quot;
    &amp;quot;strings&amp;quot;
    &amp;quot;gopkg.in/alecthomas/kingpin.v2&amp;quot;
)
var (
    app          = kingpin.New(&amp;quot;chat&amp;quot;, &amp;quot;A command-line chat application.&amp;quot;)
    //bool类型参数，可以通过 --debug使该值为true
    debug        = app.Flag(&amp;quot;debug&amp;quot;, &amp;quot;Enable debug mode.&amp;quot;).Bool()
    //识别 ./cli register
    register     = app.Command(&amp;quot;register&amp;quot;, &amp;quot;Register a new user.&amp;quot;)
    // ./cli register之后的参数，可通过./cli register gggle 123456 传入name为gggle pwd为123456 参数类型为字符串
    registerName = register.Arg(&amp;quot;name&amp;quot;, &amp;quot;Name for user.&amp;quot;).Required().String()
    registerPwd  = register.Arg(&amp;quot;pwd&amp;quot;, &amp;quot;pwd of user.&amp;quot;).Required().String()
    //识别 ./cli post
    post         = app.Command(&amp;quot;post&amp;quot;, &amp;quot;Post a message to a channel.&amp;quot;)
    //可以通过 ./cli post --image file  或者 ./cli post -i file 传入文件
    postImage    = post.Flag(&amp;quot;image&amp;quot;, &amp;quot;Image to post.&amp;quot;).Short(&#39;i&#39;).String()
    //可以通过./cli post txt 传入字符串，有默认值&amp;quot;hello world&amp;quot;
    postText     = post.Arg(&amp;quot;text&amp;quot;, &amp;quot;Text to post.&amp;quot;).Default(&amp;quot;hello world&amp;quot;).Strings()
)
func main() {
    //从os接收参数传给kingpin处理
    switch kingpin.MustParse(app.Parse(os.Args[1:])) {
    case register.FullCommand():
        println(&amp;quot;name:&amp;quot; + *registerName)
        println(&amp;quot;pwd:&amp;quot; + *registerPwd)
    case post.FullCommand():
        println((*postImage))
        text := strings.Join(*postText, &amp;quot; &amp;quot;)
        println(&amp;quot;Post:&amp;quot;, text)
    }
    if *debug == true {
        println(&amp;quot;debug&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;pflag&#34;&gt;Pflag&lt;/h1&gt;

&lt;p&gt;Docker源码中使用了Pflag，安装spf13/pflag&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/spf13/pflag
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;p&gt;基本的使用和“flag包”基本相同&lt;/p&gt;

&lt;p&gt;新增:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;添加shorthand参数
// func IntP(name, shorthand string, value int, usage string) *int
// IntP is like Int, but accepts a shorthand letter that can be used after a single dash.
var ip= flag.IntP(&amp;quot;flagname&amp;quot;, &amp;quot;f&amp;quot;, 1234, &amp;quot;help message&amp;quot;)
设置非必须选项的默认值
var ip = flag.IntP(&amp;quot;flagname&amp;quot;, &amp;quot;f&amp;quot;, 1234, &amp;quot;help message&amp;quot;)
flag.Lookup(&amp;quot;flagname&amp;quot;).NoOptDefVal = &amp;quot;4321&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果如下图:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Parsed Arguments    Resulting Value
–flagname=1357  ip=1357
–flagname   ip=4321
[nothing]   ip=1234
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;命令行语法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--flag    // 布尔flags, 或者非必须选项默认值
--flag x  // 只对于没有默认值的flags
--flag=x
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;flag定制化&#34;&gt;flag定制化&lt;/h2&gt;

&lt;p&gt;例如希望使用“-”，“_”或者“.”，像&amp;ndash;my-flag == &amp;ndash;my_flag == &amp;ndash;my.flag:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func wordSepNormalizeFunc(f *pflag.FlagSet, name string) pflag.NormalizedName {
    from := []string{&amp;quot;-&amp;quot;, &amp;quot;_&amp;quot;}
    to := &amp;quot;.&amp;quot;
    for _, sep := range from {
        name = strings.Replace(name, sep, to, -1)
    }
    return pflag.NormalizedName(name)
}

myFlagSet.SetNormalizeFunc(wordSepNormalizeFunc)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如希望联合两个参数,像&amp;ndash;old-flag-name == &amp;ndash;new-flag-name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func aliasNormalizeFunc(f *pflag.FlagSet, name string) pflag.NormalizedName {
    switch name {
    case &amp;quot;old-flag-name&amp;quot;:
        name = &amp;quot;new-flag-name&amp;quot;
        break
    }
    return pflag.NormalizedName(name)
}

myFlagSet.SetNormalizeFunc(aliasNormalizeFunc)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;弃用flag或者它的shothand&lt;/p&gt;

&lt;p&gt;例如希望弃用名叫badflag参数，并告知开发者使用代替参数:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// deprecate a flag by specifying its name and a usage message
flags.MarkDeprecated(&amp;quot;badflag&amp;quot;, &amp;quot;please use --good-flag instead&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从而当使用badflag时，会提示Flag &amp;ndash;badflag has been deprecated, please use &amp;ndash;good-flag instead&lt;/p&gt;

&lt;p&gt;例如希望保持使用noshorthandflag，但想弃用简称n:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// deprecate a flag shorthand by specifying its flag name and a usage message
flags.MarkShorthandDeprecated(&amp;quot;noshorthandflag&amp;quot;, &amp;quot;please use --noshorthandflag only&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从而当使用n时，会提示Flag shorthand -n has been deprecated, please use &amp;ndash;noshorthandflag only&lt;/p&gt;

&lt;p&gt;隐藏flag&lt;/p&gt;

&lt;p&gt;例如希望保持使用secretFlag参数，但在help文档中隐藏这个参数的说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// hide a flag by specifying its name
flags.MarkHidden(&amp;quot;secretFlag&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关闭flags的排序&lt;/p&gt;

&lt;p&gt;例如希望关闭对help文档或使用说明的flag排序：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flags.BoolP(&amp;quot;verbose&amp;quot;, &amp;quot;v&amp;quot;, false, &amp;quot;verbose output&amp;quot;)
flags.String(&amp;quot;coolflag&amp;quot;, &amp;quot;yeaah&amp;quot;, &amp;quot;it&#39;s really cool flag&amp;quot;)
flags.Int(&amp;quot;usefulflag&amp;quot;, 777, &amp;quot;sometimes it&#39;s very useful&amp;quot;)
flags.SortFlags = false
flags.PrintDefaults()
输出：

-v, --verbose           verbose output
    --coolflag string   it&#39;s really cool flag (default &amp;quot;yeaah&amp;quot;)
    --usefulflag int    sometimes it&#39;s very useful (default 777)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;同时使用flag包和pflag包&#34;&gt;同时使用flag包和pflag包&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import (
    goflag &amp;quot;flag&amp;quot;
    flag &amp;quot;github.com/spf13/pflag&amp;quot;
)

var ip *int = flag.Int(&amp;quot;flagname&amp;quot;, 1234, &amp;quot;help message for flagname&amp;quot;)

func main() {
    flag.CommandLine.AddGoFlagSet(goflag.CommandLine)
    flag.Parse()
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>golang使用系列---- Time</title>
          <link>https://kingjcy.github.io/post/golang/go-time/</link>
          <pubDate>Tue, 12 Apr 2016 20:11:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-time/</guid>
          <description>&lt;p&gt;time包中包括两类时间：时间点（某一时刻）和时长（某一段时间）的基本操作。&lt;/p&gt;

&lt;h1 id=&#34;time&#34;&gt;time&lt;/h1&gt;

&lt;h2 id=&#34;基本结构&#34;&gt;基本结构&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Time&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Time struct {
    wall uint64
    ext  int64
    loc *Location
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;wall  秒&lt;/li&gt;
&lt;li&gt;ext   纳秒&lt;/li&gt;
&lt;li&gt;loc *Location

&lt;ul&gt;
&lt;li&gt;time.UTC utc时间&lt;/li&gt;
&lt;li&gt;time.Local 本地时间&lt;/li&gt;
&lt;li&gt;FixedZone(name string, offset int) *Location   设置时区名,以及与UTC0的时间偏差.返回Location&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Duration&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Duration int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Duration类型代表两个时间点之间经过的时间，以纳秒为单位。可表示的最长时间段大约290年。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;时间常量&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Duration的单位为 nanosecond，为了便于使用，time中定义了时间常量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    Nanosecond Duration = 1
    Microsecond = 1000 * Nanosecond
    Millisecond = 1000 * Microsecond
    Second = 1000 * Millisecond
    Minute = 60 * Second
    Hour = 60 * Minute
)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Ticker&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Ticker
type Ticker struct {
    C &amp;lt;-chan Time // 周期性传递时间信息的通道
    // 内含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ticker保管一个通道，并每隔一段时间向其传递&amp;rdquo;tick&amp;rdquo;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewTicker
func NewTicker(d Duration) *Ticker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewTicker返回一个新的Ticker，该Ticker包含一个通道字段，并会每隔时间段d就向该通道发送当时的时间。它会调整时间间隔或者丢弃tick信息以适应反应慢的接收者。如果d&amp;lt;=0会panic。关闭该Ticker可以释放相关资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Ticker) Stop
func (t *Ticker) Stop()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stop关闭一个Ticker。在关闭后，将不会发送更多的tick信息。Stop不会关闭通道t.C，以避免从该通道的读取不正确的成功。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;time.Duration（时长，耗时）&lt;/li&gt;
&lt;li&gt;time.Time（时间点）&lt;/li&gt;
&lt;li&gt;time.C（放时间点的管道）[ Time.C:=make(chan time.Time) ]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;基本函数&#34;&gt;基本函数&lt;/h2&gt;

&lt;p&gt;time包提供了时间的显示和测量用的函数。日历的计算采用的是公历。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Now
func Now() Time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now返回当前本地时间。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (Time) Before
func (t Time) Before(u Time) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果t代表的时间点在u之前，返回真；否则返回假。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (Time) Add
func (t Time) Add(d Duration) Time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add返回时间点t+d。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (Time) Second
func (t Time) Second() int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回t对应的那一分钟的第几秒，范围[0, 59]。&lt;/p&gt;

&lt;h1 id=&#34;常规使用&#34;&gt;常规使用&lt;/h1&gt;

&lt;h2 id=&#34;sleep&#34;&gt;sleep&lt;/h2&gt;

&lt;p&gt;golang的休眠可以使用time包中的sleep。&lt;/p&gt;

&lt;p&gt;函数原型为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Sleep(d Duration)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面实现休眠2秒功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {

    fmt.Println(&amp;quot;begin&amp;quot;)
    time.Sleep(time.Duration(2)*time.Second)
    fmt.Println(&amp;quot;end&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;time使用变量的时候需要强制转换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time.Duration(cfg.CTimeOut) * time.Second
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;定时器&#34;&gt;定时器&lt;/h2&gt;

&lt;p&gt;定时器只会传达一次到期事件，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Timer struct {
    C &amp;lt;-chan Time
    r runtimeTimer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每天定时0点执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;time&amp;quot;
    &amp;quot;fmt&amp;quot;
)

//定时结算Boottime表数据
func BoottimeTimingSettlement() {
    for {
        now := time.Now()
        // 计算下一个零点
        next := now.Add(time.Hour * 24)
        next = time.Date(next.Year(), next.Month(), next.Day(), 0, 0, 0, 0, next.Location())
        t := time.NewTimer(next.Sub(now))
        &amp;lt;-t.C
        Printf(&amp;quot;定时结算Boottime表数据，结算完成: %v\n&amp;quot;,time.Now())
        //以下为定时执行的操作
        BoottimeSettlement()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;断续器&#34;&gt;断续器&lt;/h2&gt;

&lt;p&gt;周期性的传达到期事件的装置，定时器只会传达一次到期事件，断续器会持续工作直到停止。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Ticker struct {
    C &amp;lt;-chan Time // The channel on which the ticks are delivered.
    r runtimeTimer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewTicker(d Duration) *Ticker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewTicker返回一个新的Ticker，该Ticker包含一个通道字段，并会每隔时间段d就向该通道发送当时的时间。它会调整时间间隔或者丢弃tick信息以适应反应慢的接收者。如果d&amp;lt;=0会panic。关闭该Ticker可以释放相关资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ticker := time.NewTicker(time.Millisecond * 500)
go func() {
    for t := range ticker.C {
        fmt.Println(&amp;quot;Tick at&amp;quot;, t)
    }
}()

time.Sleep(time.Millisecond * 1500)   //阻塞，则执行次数为sleep的休眠时间/ticker的时间
ticker.Stop()    
fmt.Println(&amp;quot;Ticker stopped&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取时间&#34;&gt;获取时间&lt;/h2&gt;

&lt;p&gt;各种现有时间的获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    fmt.Printf(&amp;quot;时间戳（秒）：%v;\n&amp;quot;, time.Now().Unix())
    fmt.Printf(&amp;quot;时间戳（纳秒）：%v;\n&amp;quot;,time.Now().UnixNano())
    fmt.Printf(&amp;quot;时间戳（毫秒）：%v;\n&amp;quot;,time.Now().UnixNano() / 1e6)
    fmt.Printf(&amp;quot;时间戳（纳秒转换为秒）：%v;\n&amp;quot;,time.Now().UnixNano() / 1e9)
}


时间戳（秒）：1530027865;
时间戳（纳秒）：1530027865231834600;
时间戳（毫秒）：1530027865231;
时间戳（纳秒转换为秒）：1530027865;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;时间转化&#34;&gt;时间转化&lt;/h2&gt;

&lt;p&gt;处理时间单位自动转化问题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ParseDuration(s string) (Duration, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;传入字符串，返回响应的时间，其中传入的字符串中的有效时间单位如下：h,m,s,ms,us,ns，其他单位均无效，如果传入无效时间单位，则会返回０&lt;/p&gt;

&lt;p&gt;获取前n天的时间&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//获取两天前的时间
currentTime := time.Now()
oldTime := currentTime.AddDate(0, 0, -2)        //若要获取3天前的时间，则应将-2改为-3
//oldTime 的结果为go的时间time类型，2018-09-25 13:24:58.287714118 +0000 UTC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比较时间，使用before&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time1 := &amp;quot;2015-03-20 08:50:29&amp;quot;
time2 := &amp;quot;2015-03-21 09:04:25&amp;quot;
//先把时间字符串格式化成相同的时间类型
t1, err := time.Parse(&amp;quot;2006-01-02 15:04:05&amp;quot;, time1)
t2, err := time.Parse(&amp;quot;2006-01-02 15:04:05&amp;quot;, time2)
if err == nil &amp;amp;&amp;amp; t1.Before(t2) {
    //处理逻辑
    fmt.Println(&amp;quot;true&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取文件的各种时间&#34;&gt;获取文件的各种时间&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    finfo, _ := os.Stat(filename)
    // Sys()返回的是interface{}，所以需要类型断言，不同平台需要的类型不一样，linux上为*syscall.Stat_t
    stat_t := finfo.Sys().(*syscall.Stat_t)
    fmt.Println(stat_t)
    // atime，ctime，mtime分别是访问时间，创建时间和修改时间，具体参见man 2 stat
    fmt.Println(timespecToTime(stat_t.Atim))
    fmt.Println(timespecToTime(stat_t.Ctim))
    fmt.Println(timespecToTime(stat_t.Mtim))
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Zabbix监控方案</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix-scheme/</link>
          <pubDate>Fri, 04 Mar 2016 17:54:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix-scheme/</guid>
          <description>&lt;p&gt;zabbix是目前各大互联网公司使用最广泛的开源监控之一,其历史最早可追溯到1998年,在业内拥有各种成熟的解决方案，但是对容器的监控还是比较薄弱，我们也不多说，主要用于基础设施VM的监控。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/zabbix/zabbix.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;详细说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;agent：负载采集数据，所有的采集都在这一个进程中，不像prometheus的exporter有很多。&lt;/li&gt;
&lt;li&gt;proxy：是一个汇聚层，将数据聚合后发送到server。&lt;/li&gt;
&lt;li&gt;server：服务端，用于存储数据，对外进行查询展示。&lt;/li&gt;
&lt;li&gt;DB：数据库，存储数据&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;zabbix的核心组件&#34;&gt;zabbix的核心组件&lt;/h1&gt;

&lt;p&gt;1、zabbix server 负责采集和收取agent采集的信息。&lt;/p&gt;

&lt;p&gt;2、zabbix database   用于存储zabbix的配置信息，监控数据&lt;/p&gt;

&lt;p&gt;3、zabbix web zabbix的管理界面，监控界面，可以独立部署，只要能连接到database就可以&lt;/p&gt;

&lt;p&gt;4、zabbix agent 不数据监控主机主机上，负责采集数据，把数据推送到server或者server来去数据（主动和被动模式，可以同时设置）&lt;/p&gt;

&lt;p&gt;5、zabbix proxy 用于分布式监控，作用就是用于聚合部分数据，最后统一发完server&lt;/p&gt;

&lt;p&gt;zabbix对分布式的数据采集非常好,支持两种分布式架构,一种是Proxy,一种是Node.Proxy作为zabbix server的代理去监控服务器,并发数据汇聚到Zabbix server.而Node本身就是一个完整的Zabbix server, 使用Node可以将多个Zabbix server组成一个具有基层关系的分布式架构.&lt;/p&gt;

&lt;p&gt;两者的区别如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                proxy   Node
轻量级         √       ×
GUI前端           ×       √
是否可以独立运行    √       ×
容易运维            √       ×
本地Admin管理   ×       √
中心化配置       √       ×
产生通知            ×       √
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、zabbix get 安装服务器上，来测试获取agent的数据的工具&lt;/p&gt;

&lt;p&gt;7、zabbix sender 安装在客户端机器上，用于测试推送数据到server的的工具&lt;/p&gt;

&lt;h1 id=&#34;zabbix监控方式&#34;&gt;Zabbix监控方式&lt;/h1&gt;

&lt;p&gt;1、被动模式&lt;/p&gt;

&lt;p&gt;被动检测：相对于agent而言；agent, server向agent请求获取配置的各监控项相关的数据，agent接收请求、获取数据并响应给server；&lt;/p&gt;

&lt;p&gt;2、主动模式&lt;/p&gt;

&lt;p&gt;主动检测：相对于agent而言；agent(active),agent向server请求与自己相关监控项配置，主动地将server配置的监控项相关的数据发送给server；&lt;/p&gt;

&lt;p&gt;主动监控能极大节约监控server 的资源。&lt;/p&gt;

&lt;h1 id=&#34;zabbix的使用&#34;&gt;zabbix的使用&lt;/h1&gt;

&lt;p&gt;基本安装使用可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix/&#34;&gt;这里&lt;/a&gt;,相关源码解析可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbixcode/&#34;&gt;这里&lt;/a&gt;,这些就不多说了。&lt;/p&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;随着系统监控规模的越来越大，zabbix出现越来越多的瓶颈，随着时序数据库的广泛使用，监控已经渐渐切换到了时序数据库，对于原始的zabbix监控项，监控数据如何处理？我们可以将zabbix数据存到时序数据库中，统一使用，相关&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix2tsdb/&#34;&gt;实现方案&lt;/a&gt;就需要自己实现了。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>分布式系列---- 文件存储系统nfs</title>
          <link>https://kingjcy.github.io/post/distributed/store/fs/nfs/</link>
          <pubDate>Sat, 16 Jan 2016 20:32:58 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/distributed/store/fs/nfs/</guid>
          <description>&lt;p&gt;NFS是Network File System的缩写，就是网络文件系统，主要功能是让应用程序在客户端通过网络访问位于服务器磁盘中的数据，是在类Unix系统间实现磁盘文件共享的一种方法。&lt;/p&gt;

&lt;p&gt;NFS系统和Windows网络共享、网络驱动器类似, 只不过windows用于局域网, NFS用于企业集群架构中, 如果是大型网站, 会用到更复杂的分布式文件系统&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/fastfs/&#34;&gt;fastdfs&lt;/a&gt;,&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/glusterfs/&#34;&gt;glusterfs&lt;/a&gt;,&lt;a href=&#34;https://kingjcy.github.io/post/distributed/store/fs/hfds/&#34;&gt;HDFS&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;安装使用&#34;&gt;安装使用&lt;/h1&gt;

&lt;h2 id=&#34;服务器&#34;&gt;服务器&lt;/h2&gt;

&lt;p&gt;系统的默认软件包安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nfs-utils-* :包括基本的NFS命令与监控程序
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note：这里要注意的是需要在集群每个节点都安装nfs-utils安装包，不然挂载会失败！&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master mnt]# yum install nfs-utils
已加载插件：fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.cn99.com
 * extras: mirrors.cn99.com
 * updates: mirrors.cn99.com
软件包 1:nfs-utils-1.3.0-0.61.el7.x86_64 已安装并且是最新版本
无须任何处理
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NFS服务的配置相对简单，只需要在对应文件中进行配置，然后启动NFS服务即可。&lt;/p&gt;

&lt;p&gt;NFS常用文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/etc/exports NFS服务的主要配置文件&lt;/li&gt;
&lt;li&gt;/usr/sbin/exportfs NFS服务的管理命令&lt;/li&gt;
&lt;li&gt;/usr/sbin/showmount 客户端的查看命令&lt;/li&gt;
&lt;li&gt;/var/lib/nfs/etab 记录NFS分享出来的目录的完整权限设定值&lt;/li&gt;
&lt;li&gt;/var/lib/nfs/xtab 记录曾经登录过的Clinent 信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;编辑/etc/exports文件添加以下内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost share]# vim /etc/exports
    /share  192.168.254.0/24(insecure,rw,no_root_squash)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置文件说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[输出目录 客户端 选项（访问权限,用户映射,其他）] [客户端2 选项（访问权限,用户映射,其他）]

1、输出目录：
输出目录是指NFS系统中需要共享给客户端使用的目录
2、客户端：
客户端是指网络中可以访问这个NFS Server的主机，客户端常用的指定方式如下：
    指定IP地址：172.16.7.57
    指定子网中的主机：172.16.7.0/24
    指定域名的主机：ssq-54-57.zerounix.com
    指定域中的所有主机：*.zerounix.com
    所有主机：*
3、选项
主要有3类选项：
访问权限选项：
    设置输出目录只读：ro
    设置输出目录读写：rw
用户映射选项：
    all_squash： 将远程访问的所有普通用户及属组都映射为匿名用户或用户组(nfsnobody)；
    no_all_squash： 与all_squash相反（default）；
    root_squash： 将root用户及属组都映射问匿名用户或用户组（default）；
    no_root_squash：
    anonuid=xxx： 将远程访问的所有用户都映射为匿名用户，并指定用户问本地用户（UID=xxx）；
    anongid=xxx： 将远程访问的所有用户都映射为匿名用户组，并指定用户问本地用户组（GID=xxx）；
其他选项：
    secure： 限制客户端只能从小于1024的tcp端口连接NFS Server（default）；
    insecure： 允许客户端从大于1024的tcp端口连接NFS Server；
    sync： 将数据同步下乳内存缓冲区与磁盘中，效率低，但是可以保证数据的一致性；
    async： 将数据先保存在内存缓冲区中，必要时才写入磁盘；
    wdelay： 检查是否有相关的写操作，如果有则见这些写操作一起执行，可以提高效率（default）；
    no_wdelay： 若有写操作立即执行，应与sync配合使用；
    subtree： 若输出目录是一个子目录，则NFS Server将检查其父目录权限（default）；
    no_subtree： 若输出目录是一个子目录，则NFS Server将不检查其父目录权限；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在启动之前需要关闭防火墙。&lt;/p&gt;

&lt;p&gt;启动NFS Server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service nfs start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看进程状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# systemctl status nfs-server
● nfs-server.service - NFS server and services
   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)
   Active: active (exited) since Thu 2019-10-31 09:53:29 CST; 20s ago
 Main PID: 20402 (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/nfs-server.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证nfs的服务是否正常&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo rpcinfo -p|grep nfs
100003    3   tcp   2049  nfs
100003    4   tcp   2049  nfs
100003    3   udp   2049  nfs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看具体目录的挂载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /var/lib/nfs/etab
/share  *(rw,sync,wdelay,hide,nocrossmnt,secure,no_root_squash,no_all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid=65534,anongid=65534,sec=sys,rw,secure,no_root_squash,no_all_squash)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在/share目录中写一个index.html文件并且写入内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost share]# echo &amp;quot;nfs server&amp;quot; &amp;gt; /share/index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;客户端&#34;&gt;客户端&lt;/h2&gt;

&lt;p&gt;同样是安装nfs服务，然后可以通过命令来查看server端的共享信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@web01 ~]# showmount -e 172.16.1.31

Export list for 172.16.1.31:

/share 172.16.1.0/24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们使用挂载命令将远程服务的共享目录挂载到本地&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@web01 ~]# mkdir /share

[root@web01 ~]# mount -t nfs 172.16.1.31:/share /share/

[root@web01 ~]# df -h

文件系统                   容量  已用   可用    已用% 挂载

172.16.1.31:/share         50G  2.6G   48G    6% /share
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以到/share目录下获取文件了。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/distribute/fs/nfs&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;NFS在文件传送或信息传送过程中依赖于RPC协议。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户进程访问NFS客户端，使用不同的函数对数据进行处理&lt;/li&gt;
&lt;li&gt;NFS客户端通过TCP/IP的方式传递给NFS服务端。&lt;/li&gt;
&lt;li&gt;NFS服务端接收到请求后，会先调用portmap进程进行端口映射。&lt;/li&gt;
&lt;li&gt;nfsd进程用于判断NFS客户端是否拥有权限连接NFS服务端。&lt;/li&gt;
&lt;li&gt;Rpc.mount进程判断客户端是否有对应的权限进行验证。&lt;/li&gt;
&lt;li&gt;idmap进程实现用户映射和压缩&lt;/li&gt;
&lt;li&gt;最后NFS服务端会将对应请求的函数转换为本地能识别的命令，传递至内核，由内核驱动硬件。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见，nfs其实就是一个rpc调用传输文件的系统，在这个过程中加了很多权限的控制。&lt;/p&gt;

&lt;h1 id=&#34;应用&#34;&gt;应用&lt;/h1&gt;

&lt;p&gt;NFS存储优点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NFS文件系统简单易用、方便部署，数据可靠，服务稳定，满足中小企业需求&lt;/li&gt;
&lt;li&gt;、NFS文件系统内存放的数据都在文件系统之上，所有数据都是可见的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NFS存储局限&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;存在单点故障，构建高可用维护麻烦web&amp;mdash;nfs&amp;mdash;backup&lt;/li&gt;
&lt;li&gt;NFS数据明文，没有校验&lt;/li&gt;
&lt;li&gt;客户端挂载没有密码，安全性一般（也就内网使用）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NFS应用建议&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;生产场景应用将静态数据尽可能向前推送，减少后端存储的压力&lt;/li&gt;
&lt;li&gt;必须将存储的静态资源通过CDN缓存jpg/png/mp4/avi/css/js&lt;/li&gt;
&lt;li&gt;如果没有缓存，存储再多对网站的速度也没有太大帮助&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;大小问题&lt;/p&gt;

&lt;p&gt;当我们在WEB上挂载了NFS共享的/data目录之后，查看一下，我们发现/data的大小是50G，为什么是50G，我们难道共享了一个目录就一下子共享了50G吗？其实就是这样的，你想呀，我们在/data的根下创建了目录，那这个目录是多大？我们在创建目录的时候也不能指定这个目录最大能到多少G，这就是和我们在windows上创建一个文件夹一样，这个文件夹最终能变大取决于该文件夹所在的分区有多大，在NFS上/data是在根所有的分区上创建的，理论上NFS的根最大能变多大，/data目录就能变多大。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@Web01 html]# df -h
Filesystem               Size  Used Avail Use% Mounted on
192.168.80.221:/data      50G  4.0G   47G   8% /var/www/html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在NFS上查看一下根的大小，发现根就是50G，这下你明白了吧！&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@Nfs ~]# df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-root   50G  4.0G   47G   8% /
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>用hugo&#43;github构建自己的blog</title>
          <link>https://kingjcy.github.io/post/tool/hugo-blog-build/</link>
          <pubDate>Fri, 29 Aug 2014 09:29:40 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/tool/hugo-blog-build/</guid>
          <description>&lt;p&gt;这个是我用hugo+github搭建起个人blog写的第一篇文章，有点小兴奋。。。首先把搭建测过程写起来和大家分享一下吧。&lt;/p&gt;

&lt;p&gt;首先，作为一个程序员，不拥有自己搭建的blog，而去用别人搭建好的去注册一下，我是无法接受的！！搭建个人blog需要两个东西：&lt;/p&gt;

&lt;p&gt;1、静态网页生成器，有jekyll，hexo，hugo等，由于最近在玩go语言，所以就选择了hugo，其他的也没有深入了解，后面搭建起来，发现hugo还是比较简单。&lt;/p&gt;

&lt;p&gt;2、github pages 这个是github提供的一个托管工作，相当好用。&lt;/p&gt;

&lt;h1 id=&#34;静态页面生成器hugo&#34;&gt;静态页面生成器hugo&lt;/h1&gt;

&lt;p&gt;这个比较方便的静态页面生成器，首先需要安装，我的系统是centos 64位的.&lt;/p&gt;

&lt;p&gt;现在换成了macos系统了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;install&#34;&gt;install&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、直接下载二进制文件，这也是我说的方便的地方。&lt;/p&gt;

&lt;p&gt;Hugo二进制下载地址：&lt;a href=&#34;https://github.com/spf13/hugo/releases&#34;&gt;https://github.com/spf13/hugo/releases&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2。使用macos系统后直接使用homebrew进行安装更新，这个就是一个类似于linux的yum的工具。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install hugo
brew upgrade hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;use&#34;&gt;use&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;下载下来后，首先要生成自己的站点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`hugo new site mysite`--这边hugo的二进制文件不一定是这个名字，可以起个别名alias来用
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时会在mysite目录下生成一些目录和文件，这边简单的介绍一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、config.toml是网站的配置文件，这是它的作者GitHub联合创始人Tom Preston-Werner 觉得YAML不够优雅，捣鼓出来的一个新格式。如果你不喜欢这种格式，你可以将config.toml替换为YAML格式的config.yaml，或者json格式的config.json。hugo都支持。
2、content目录里放的是你写的markdown文章。
3、layouts目录里放的是网站的模板文件。
4、static目录里放的是一些图片、css、js等资源。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后进入站点目录mysite，新建文档&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`cd mysite`

`hugo new about.md`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边新建一个md文件会出现在content目录下，一般这个about.md文件是一个关于本站的介绍或者blog个人介绍，在这边将一下md文件的编辑，其实就是MarkDown格式文件的编写，具体的格式可以参考本文的编辑，或者去网上去搜索一下就ok,这边我说几点，我经常记错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、就是&amp;quot;+++&amp;quot;内的赋值用&amp;quot;=&amp;quot;，&amp;quot;---&amp;quot;内的用&amp;quot;:&amp;quot;。

2、`###`后面必须有空格。

3、有空行才能换行。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般我们写博文，会放在content/post下，正如我这边编写的第一篇文&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`hugo new post/first.md`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后用vim编辑器进行编辑，编辑好后，就可以将你编辑的文字生成静态网页了，当然你肯定需要一个模板，这样可以使你的网页根据美观，这边在讲一下模板的使用&lt;/p&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;模版&#34;&gt;模版&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、模板放在站点的themes下，一般木有这个文件夹，我们需要新增一个&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`mkdir themes`

`cd themes`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、模板可以到hugo官网上去找,那边可以showcase预览一下自己喜欢的，具体的安装方式也有介绍，就是用&lt;code&gt;git clone&lt;/code&gt;把源码下到themes目录下就好&lt;/p&gt;

&lt;p&gt;官网：&lt;a href=&#34;https://gohugo.io/overview/introduction/&#34;&gt;https://gohugo.io/overview/introduction/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、编辑模板的配置文件，这个视具体模板，可以参考我的配置&lt;a href=&#34;https://github.com/kingjcy/&#34;&gt;https://github.com/kingjcy/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下面就是生成我们需要的静态网页了，也就是前端的html文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`hugo --theme=hyde --baseUrl=&amp;quot;http://kingjcy.github.io/&amp;quot;`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不出意外的话，应该在站点目录下生成一个public文件夹，这个就是我们需要的所有文件了，至此第一步已经完成了。可以看见直接编译是hugo，启动一个web服务是hugo server&lt;/p&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、huo new XXXX生成文件是可以直接生成自己想要的内容的，取决于模版，默认是archetypes/default.md，可以对其进行修改，变成自己的样子。&lt;/p&gt;

&lt;p&gt;2、使用图片，默认把图片放在media目录下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;![](/media/worklife/baby/XXX.JPG)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;github-pages托管&#34;&gt;github pages托管&lt;/h1&gt;

&lt;p&gt;这个就简单了，因为本身就是github提供现成的东西，首先新增一个repo，命名为：&lt;code&gt;kingjcy.github.io&lt;/code&gt; （kingjcy替换为你的github用户名）。&lt;/p&gt;

&lt;p&gt;然后将第一步的public加入git版本，上传到这个项目，就可以访问你的个人blog：&lt;code&gt;http://kingjcy.github.io/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;至于git版本控制和github直接的传输，这边就不多讲了，如果需要可以参考我的另外一篇博文《git和github的使用》。&lt;/p&gt;

&lt;p&gt;这边简单列举一些过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd public
$ git init
$ git remote add origin https://github.com/kingjcy/kingjcy.github.io.git
$ git add -A
$ git commit -m &amp;quot;first commit&amp;quot;
$ git push -u origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;终于搭建完了，欢迎指正,tks。&lt;/p&gt;</description>
        </item>
      
    
      
    

  </channel>
</rss>

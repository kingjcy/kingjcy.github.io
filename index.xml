<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kingjcy blog </title>
    <link>https://kingjcy.github.io/</link>
    <language>en-us</language>
    <author></author>
    <rights>(C) 2020</rights>
    <updated>2020-01-18 19:45:30 &#43;0800 CST</updated>

    
      
        <item>
          <title>监控日志系列---- loki</title>
          <link>https://kingjcy.github.io/post/monitor/log/loki/loki/</link>
          <pubDate>Sat, 18 Jan 2020 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/loki/loki/</guid>
          <description>&lt;p&gt;Loki是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签，为 Prometheus和 Kubernetes用户做了相关优化。项目受 Prometheus 启发，类似于 Prometheus 的日志系统。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;使用场景&#34;&gt;使用场景&lt;/h2&gt;

&lt;p&gt;当我们的容器云运行的应用或者某个节点出现问题了，解决思路应该如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一般在容器云中使用prometheus生态来做监控告警，在metrics触发告警的时候，我们就需要查看日志来处理问题，这个时候就需要日志系统来收集日志进行搜索查看。&lt;/p&gt;

&lt;p&gt;现有的很多日志采集的方案都是采用全文检索对日志进行索引（如ELK方案），优点是功能丰富，允许复杂的操作。但是，这些方案往往规模复杂，资源占用高，操作苦难。很多功能往往用不上，大多数查询只关注一定时间范围和一些简单的参数（如host、service等），这个时候就需要一个轻量级的日志系统，这个时候loki就比较合适了。&lt;/p&gt;

&lt;h2 id=&#34;基本组件&#34;&gt;基本组件&lt;/h2&gt;

&lt;p&gt;Loki 整个系统需要三个组件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、Loki: 相当于 EFK 中的 ElasticSearch，用于存储和查询日志
2、Promtail: 相当于 EFK 中的 Filebeat/Fluentd，用于采集和发送日志
3、Grafana: 相当于 EFK 中的 Kibana，用于 UI 展示
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些组件以以下的部署在我们的系统中&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们可以看出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、loki: 以 Statefulset 方式部署，可横向扩容
2、promtail: 以 Daemonset 方式部署，采集每个节点上容器日志并发送给 loki
3、grafana: 默认不开启，如果集群中已经有 grafana 就可以不用在部署 grafana，如果没有，部署时可以选择也同时部署 grafana
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不使用容器部署，也大体可以看出对应的部署方式，就是Promtail作为采集组件需要部署在每个一个机器上然后将数据推送到loki中，grafana在loki中拉去数据进行展示。&lt;/p&gt;

&lt;h1 id=&#34;部署使用&#34;&gt;部署使用&lt;/h1&gt;

&lt;h2 id=&#34;k8s部署&#34;&gt;k8s部署&lt;/h2&gt;

&lt;p&gt;新增helm源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm repo add loki https://grafana.github.io/loki/charts
$ helm repo update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用helm3部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install loki loki/loki-stack
# 安装到指定命名空间
# helm install loki loki/loki-stack -n monitoring
# 持久化 loki 的数据，避免 loki 重启后数据丢失
# helm install loki loki/loki-stack --set=&amp;quot;loki.persistence.enabled=ture,loki.persistence.size=100G&amp;quot;
# 部署 grafana
# helm install loki loki/loki-stack --set=&amp;quot;grafana.enabled=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们就可以看到对应启动了如下应用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get all -n monitoring | grep loki
pod/loki-0                                 1/1     Running   1          20h
pod/loki-promtail-8phlp                    1/1     Running   1          20h
service/loki                    NodePort    10.111.208.19    &amp;lt;none&amp;gt;        3100:31278/TCP               20h
service/loki-headless           ClusterIP   None             &amp;lt;none&amp;gt;        3100/TCP                     20h
daemonset.apps/loki-promtail   1         1         1       1            1           &amp;lt;none&amp;gt;                   20h
statefulset.apps/loki                1/1     20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如上使用了以 Daemonset 方式部署了promtail，使用Statefulset 方式部署loki，然后用service暴露给grafana。&lt;/p&gt;

&lt;h2 id=&#34;配置文件&#34;&gt;配置文件&lt;/h2&gt;

&lt;p&gt;上面启动了对应的应用，我们来看一下默认的启动情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti loki-0 -n monitoring -- sh
/ $ ps -ef
PID   USER     TIME  COMMAND
    1 loki      0:01 /usr/bin/loki -config.file=/etc/loki/loki.yaml
   23 loki      0:00 sh
   28 loki      0:00 ps -ef
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到就是使用二进制文件和配置文件进行启动，所以我们关键看一下配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/ $ cat /etc/loki/loki.yaml
auth_enabled: false
chunk_store_config:
  max_look_back_period: 0s
ingester:
  chunk_block_size: 262144
  chunk_idle_period: 3m
  chunk_retain_period: 1m
  lifecycler:
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  max_transfer_retries: 0
limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
schema_config:
  configs:
  - from: &amp;quot;2018-04-15&amp;quot;
    index:
      period: 168h
      prefix: index_
    object_store: filesystem
    schema: v9
    store: boltdb
server:
  http_listen_port: 3100
storage_config:
  boltdb:
    directory: /data/loki/index
  filesystem:
    directory: /data/loki/chunks
table_manager:
  retention_deletes_enabled: false
  retention_period: 0s/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们这边详细说明一下配置文件，配置文件主要有以下几块组成&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、target：[target: &amp;lt;string&amp;gt; | default = &amp;quot;all&amp;quot;]
2、auth_enabled：[auth_enabled: &amp;lt;boolean&amp;gt; | default = true] 启动验证，默认是启动的，如果需要关闭，需要设置为false
3、server：主要是配置loki的http模块，最常见的就是配置http的地址和端口
    # HTTP server listen host
    [http_listen_address: &amp;lt;string&amp;gt;]

    # HTTP server listen port
    [http_listen_port: &amp;lt;int&amp;gt; | default = 80]

    # gRPC server listen host
    [grpc_listen_address: &amp;lt;string&amp;gt;]

    # gRPC server listen port
    [grpc_listen_port: &amp;lt;int&amp;gt; | default = 9095]

    # Register instrumentation handlers (/metrics, etc.)
    [register_instrumentation: &amp;lt;boolean&amp;gt; | default = true]

    # Timeout for graceful shutdowns
    [graceful_shutdown_timeout: &amp;lt;duration&amp;gt; | default = 30s]

    # Read timeout for HTTP server
    [http_server_read_timeout: &amp;lt;duration&amp;gt; | default = 30s]

    # Write timeout for HTTP server
    [http_server_write_timeout: &amp;lt;duration&amp;gt; | default = 30s]

    # Idle timeout for HTTP server
    [http_server_idle_timeout: &amp;lt;duration&amp;gt; | default = 120s]

    # Max gRPC message size that can be received
    [grpc_server_max_recv_msg_size: &amp;lt;int&amp;gt; | default = 4194304]

    # Max gRPC message size that can be sent
    [grpc_server_max_send_msg_size: &amp;lt;int&amp;gt; | default = 4194304]

    # Limit on the number of concurrent streams for gRPC calls (0 = unlimited)
    [grpc_server_max_concurrent_streams: &amp;lt;int&amp;gt; | default = 100]

    # Log only messages with the given severity or above. Supported values [debug,
    # info, warn, error]
    [log_level: &amp;lt;string&amp;gt; | default = &amp;quot;info&amp;quot;]

    # Base path to server all API routes from (e.g., /v1/).
    [http_path_prefix: &amp;lt;string&amp;gt;]
4、distributor主要是配置loki的分发，目前只有ring轮询
    [ring: &amp;lt;ring_config&amp;gt;]
5、ring_config主要是用来发现和连接Ingesters
    kvstore:
      # The backend storage to use for the ring. Supported values are
      # consul, etcd, inmemory
      store: &amp;lt;string&amp;gt;

      # The prefix for the keys in the store. Should end with a /.
      [prefix: &amp;lt;string&amp;gt; | default = &amp;quot;collectors/&amp;quot;]

      # Configuration for a Consul client. Only applies if store
      # is &amp;quot;consul&amp;quot;
      consul:
        # The hostname and port of Consul.
        [host: &amp;lt;string&amp;gt; | duration = &amp;quot;localhost:8500&amp;quot;]

        # The ACL Token used to interact with Consul.
        [acl_token: &amp;lt;string&amp;gt;]

        # The HTTP timeout when communicating with Consul
        [http_client_timeout: &amp;lt;duration&amp;gt; | default = 20s]

        # Whether or not consistent reads to Consul are enabled.
        [consistent_reads: &amp;lt;boolean&amp;gt; | default = true]

      # Configuration for an ETCD v3 client. Only applies if
      # store is &amp;quot;etcd&amp;quot;
      etcd:
        # The ETCD endpoints to connect to.
        endpoints:
          - &amp;lt;string&amp;gt;

        # The Dial timeout for the ETCD connection.
        [dial_timeout: &amp;lt;duration&amp;gt; | default = 10s]

        # The maximum number of retries to do for failed ops to ETCD.
        [max_retries: &amp;lt;int&amp;gt; | default = 10]

    # The heartbeat timeout after which ingesters are skipped for
    # reading and writing.
    [heartbeat_timeout: &amp;lt;duration&amp;gt; | default = 1m]

    # The number of ingesters to write to and read from. Must be at least
    # 1.
    [replication_factor: &amp;lt;int&amp;gt; | default = 3]
6、querier主要是查询配置
    # Timeout when querying ingesters or storage during the execution of a
    # query request.
    [query_timeout: &amp;lt;duration&amp;gt; | default = 1m]

    # Limit of the duration for which live tailing requests should be
    # served.
    [tail_max_duration: &amp;lt;duration&amp;gt; | default = 1h]

    # Time to wait before sending more than the minimum successful query
    # requests.
    [extra_query_delay: &amp;lt;duration&amp;gt; | default = 0s]

    # Maximum lookback beyond which queries are not sent to ingester.
    # 0 means all queries are sent to ingester.
    [query_ingesters_within: &amp;lt;duration&amp;gt; | default = 0s]

    # Configuration options for the LogQL engine.
    engine:
      # Timeout for query execution
      [timeout: &amp;lt;duration&amp;gt; | default = 3m]

      # The maximum amount of time to look back for log lines. Only
      # applicable for instant log queries.
      [max_look_back_period: &amp;lt;duration&amp;gt; | default = 30s]
7、ingester_client配置ingester的客户端，其实就是distributor连接ingester的配置
    # Configures how connections are pooled
    pool_config:
      # Whether or not to do health checks.
      [health_check_ingesters: &amp;lt;boolean&amp;gt; | default = false]

      # How frequently to clean up clients for servers that have gone away after
      # a health check.
      [client_cleanup_period: &amp;lt;duration&amp;gt; | default = 15s]

      # How quickly a dead client will be removed after it has been detected
      # to disappear. Set this to a value to allow time for a secondary
      # health check to recover the missing client.
      [remotetimeout: &amp;lt;duration&amp;gt;]

    # The remote request timeout on the client side.
    [remote_timeout: &amp;lt;duration&amp;gt; | default = 5s]

    # Configures how the gRPC connection to ingesters work as a
    # client.
    [grpc_client_config: &amp;lt;grpc_client_config&amp;gt;]
8、grpc_client_config上面的client可以使用grpc，这个时候就要对grpc进行配置
    # The maximum size in bytes the client can receive
    [max_recv_msg_size: &amp;lt;int&amp;gt; | default = 104857600]

    # The maximum size in bytes the client can send
    [max_send_msg_size: &amp;lt;int&amp;gt; | default = 16777216]

    # Whether or not messages should be compressed
    [use_gzip_compression: &amp;lt;bool&amp;gt; | default = false]

    # Rate limit for gRPC client. 0 is disabled
    [rate_limit: &amp;lt;float&amp;gt; | default = 0]

    # Rate limit burst for gRPC client.
    [rate_limit_burst: &amp;lt;int&amp;gt; | default = 0]

    # Enable backoff and retry when a rate limit is hit.
    [backoff_on_ratelimits: &amp;lt;bool&amp;gt; | default = false]

    # Configures backoff when enabled.
    backoff_config:
      # Minimum delay when backing off.
      [min_period: &amp;lt;duration&amp;gt; | default = 100ms]

      # The maximum delay when backing off.
      [max_period: &amp;lt;duration&amp;gt; | default = 10s]

      # Number of times to backoff and retry before failing.
      [max_retries: &amp;lt;int&amp;gt; | default = 10]
9、ingester_config配置Ingesters，主要是配置Ingesters的范围
    # Configures how the lifecycle of the ingester will operate
    # and where it will register for discovery.
    [lifecycler: &amp;lt;lifecycler_config&amp;gt;]

    # Number of times to try and transfer chunks when leaving before
    # falling back to flushing to the store. Zero = no transfers are done.
    [max_transfer_retries: &amp;lt;int&amp;gt; | default = 10]

    # How many flushes can happen concurrently from each stream.
    [concurrent_flushes: &amp;lt;int&amp;gt; | default = 16]

    # How often should the ingester see if there are any blocks
    # to flush
    [flush_check_period: &amp;lt;duration&amp;gt; | default = 30s]

    # The timeout before a flush is cancelled
    [flush_op_timeout: &amp;lt;duration&amp;gt; | default = 10s]

    # How long chunks should be retained in-memory after they&#39;ve
    # been flushed.
    [chunk_retain_period: &amp;lt;duration&amp;gt; | default = 15m]

    # How long chunks should sit in-memory with no updates before
    # being flushed if they don&#39;t hit the max block size. This means
    # that half-empty chunks will still be flushed after a certain
    # period as long as they receive no further activity.
    [chunk_idle_period: &amp;lt;duration&amp;gt; | default = 30m]

    # The targeted _uncompressed_ size in bytes of a chunk block
    # When this threshold is exceeded the head block will be cut and compressed inside the chunk
    [chunk_block_size: &amp;lt;int&amp;gt; | default = 262144]

    # A target _compressed_ size in bytes for chunks.
    # This is a desired size not an exact size, chunks may be slightly bigger
    # or significantly smaller if they get flushed for other reasons (e.g. chunk_idle_period)
    # The default value of 0 for this will create chunks with a fixed 10 blocks,
    # A non zero value will create chunks with a variable number of blocks to meet the target size.
    [chunk_target_size: &amp;lt;int&amp;gt; | default = 0]

    # The compression algorithm to use for chunks. (supported: gzip, lz4, snappy)
    # You should choose your algorithm depending on your need:
    # - `gzip` highest compression ratio but also slowest decompression speed. (144 kB per chunk)
    # - `lz4` fastest compression speed (188 kB per chunk)
    # - `snappy` fast and popular compression algorithm (272 kB per chunk)
    [chunk_encoding: &amp;lt;string&amp;gt; | default = gzip]

    # Parameters used to synchronize ingesters to cut chunks at the same moment.
    # Sync period is used to roll over incoming entry to a new chunk. If chunk&#39;s utilization
    # isn&#39;t high enough (eg. less than 50% when sync_min_utilization is set to 0.5), then
    # this chunk rollover doesn&#39;t happen.
    [sync_period: &amp;lt;duration&amp;gt; | default = 0]
    [sync_min_utilization: &amp;lt;float&amp;gt; | Default = 0]

    # The maximum number of errors a stream will report to the user
    # when a push fails. 0 to make unlimited.
    [max_returned_stream_errors: &amp;lt;int&amp;gt; | default = 10]

    # The maximum duration of a timeseries chunk in memory. If a timeseries runs for longer than this the current chunk will be flushed to the store and a new chunk created.
    [max_chunk_age: &amp;lt;duration&amp;gt; | default = 1h]

    # How far in the past an ingester is allowed to query the store for data.
    # This is only useful for running multiple loki binaries with a shared ring with a `filesystem` store which is NOT shared between the binaries
    # When using any &amp;quot;shared&amp;quot; object store like S3 or GCS this value must always be left as 0
    # It is an error to configure this to a non-zero value when using any object store other than `filesystem`
    # Use a value of -1 to allow the ingester to query the store infinitely far back in time.
    [query_store_max_look_back_period: &amp;lt;duration&amp;gt; | default = 0]
10、lifecycler_config主要就是控制
    # Configures the ring the lifecycler connects to
    [ring: &amp;lt;ring_config&amp;gt;]

    # The number of tokens the lifecycler will generate and put into the ring if
    # it joined without transferring tokens from another lifecycler.
    [num_tokens: &amp;lt;int&amp;gt; | default = 128]

    # Period at which to heartbeat to the underlying ring.
    [heartbeat_period: &amp;lt;duration&amp;gt; | default = 5s]

    # How long to wait to claim tokens and chunks from another member when
    # that member is leaving. Will join automatically after the duration expires.
    [join_after: &amp;lt;duration&amp;gt; | default = 0s]

    # Minimum duration to wait before becoming ready. This is to work around race
    # conditions with ingesters exiting and updating the ring.
    [min_ready_duration: &amp;lt;duration&amp;gt; | default = 1m]

    # Name of network interfaces to read addresses from.
    interface_names:
      - [&amp;lt;string&amp;gt; ... | default = [&amp;quot;eth0&amp;quot;, &amp;quot;en0&amp;quot;]]

    # Duration to sleep before exiting to ensure metrics are scraped.
    [final_sleep: &amp;lt;duration&amp;gt; | default = 30s]
11、storage_config主要是存储的配置，可以是本地file，可以是s3等远程存储。这边有很多配置就不一一看了。
12、cache_config就是将数据放到缓存中，比如memche，redis等
13、chunk_store_config是对chunk存储的设置包括多长时间进行存储等
    # The cache configuration for storing chunks
    [chunk_cache_config: &amp;lt;cache_config&amp;gt;]

    # The cache configuration for deduplicating writes
    [write_dedupe_cache_config: &amp;lt;cache_config&amp;gt;]

    # The minimum time between a chunk update and being saved
    # to the store.
    [min_chunk_age: &amp;lt;duration&amp;gt;]

    # Cache index entries older than this period. Default is
    # disabled.
    [cache_lookups_older_than: &amp;lt;duration&amp;gt;]

    # Limit how long back data can be queried. Default is disabled.
    # This should always be set to a value less than or equal to
    # what is set in `table_manager.retention_period`.
    [max_look_back_period: &amp;lt;duration&amp;gt;]
14、schema_config主要是对时间进行设置，格式是period_config
    # The configuration for chunk index schemas.
    configs:
      - [&amp;lt;period_config&amp;gt;]
    # The date of the first day that index buckets should be created. Use
    # a date in the past if this is your only period_config, otherwise
    # use a date when you want the schema to switch over.
    [from: &amp;lt;daytime&amp;gt;]

    # store and object_store below affect which &amp;lt;storage_config&amp;gt; key is
    # used.

    # Which store to use for the index. Either aws, gcp, bigtable, bigtable-hashed,
    # cassandra, or boltdb.
    store: &amp;lt;string&amp;gt;

    # Which store to use for the chunks. Either aws, aws-dynamo, azure, gcp,
    # bigtable, gcs, cassandra, swift or filesystem. If omitted, defaults to the same
    # value as store.
    [object_store: &amp;lt;string&amp;gt;]

    # The schema version to use, current recommended schema is v11.
    schema: &amp;lt;string&amp;gt;

    # Configures how the index is updated and stored.
    index:
      # Table prefix for all period tables.
      prefix: &amp;lt;string&amp;gt;
      # Table period.
      [period: &amp;lt;duration&amp;gt; | default = 168h]
      # A map to be added to all managed tables.
      tags:
        [&amp;lt;string&amp;gt;: &amp;lt;string&amp;gt; ...]

    # Configured how the chunks are updated and stored.
    chunks:
      # Table prefix for all period tables.
      prefix: &amp;lt;string&amp;gt;
      # Table period.
      [period: &amp;lt;duration&amp;gt; | default = 168h]
      # A map to be added to all managed tables.
      tags:
        [&amp;lt;string&amp;gt;: &amp;lt;string&amp;gt; ...]

    # How many shards will be created. Only used if schema is v10 or greater.
    [row_shards: &amp;lt;int&amp;gt; | default = 16]
15、limits_config
    # Whether the ingestion rate limit should be applied individually to each
    # distributor instance (local), or evenly shared across the cluster (global).
    # The ingestion rate strategy cannot be overridden on a per-tenant basis.
    #
    # - local: enforces the limit on a per distributor basis. The actual effective
    #   rate limit will be N times higher, where N is the number of distributor
    #   replicas.
    # - global: enforces the limit globally, configuring a per-distributor local
    #   rate limiter as &amp;quot;ingestion_rate / N&amp;quot;, where N is the number of distributor
    #   replicas (it&#39;s automatically adjusted if the number of replicas change).
    #   The global strategy requires the distributors to form their own ring, which
    #   is used to keep track of the current number of healthy distributor replicas.
    [ingestion_rate_strategy: &amp;lt;string&amp;gt; | default = &amp;quot;local&amp;quot;]

    # Per-user ingestion rate limit in sample size per second. Units in MB.
    [ingestion_rate_mb: &amp;lt;float&amp;gt; | default = 4]

    # Per-user allowed ingestion burst size (in sample size). Units in MB.
    # The burst size refers to the per-distributor local rate limiter even in the
    # case of the &amp;quot;global&amp;quot; strategy, and should be set at least to the maximum logs
    # size expected in a single push request.
    [ingestion_burst_size_mb: &amp;lt;int&amp;gt; | default = 6]

    # Maximum length of a label name.
    [max_label_name_length: &amp;lt;int&amp;gt; | default = 1024]

    # Maximum length of a label value.
    [max_label_value_length: &amp;lt;int&amp;gt; | default = 2048]

    # Maximum number of label names per series.
    [max_label_names_per_series: &amp;lt;int&amp;gt; | default = 30]

    # Whether or not old samples will be rejected.
    [reject_old_samples: &amp;lt;bool&amp;gt; | default = false]

    # Maximum accepted sample age before rejecting.
    [reject_old_samples_max_age: &amp;lt;duration&amp;gt; | default = 336h]

    # Duration for a table to be created/deleted before/after it&#39;s
    # needed. Samples won&#39;t be accepted before this time.
    [creation_grace_period: &amp;lt;duration&amp;gt; | default = 10m]

    # Enforce every sample has a metric name.
    [enforce_metric_name: &amp;lt;boolean&amp;gt; | default = true]

    # Maximum number of active streams per user, per ingester. 0 to disable.
    [max_streams_per_user: &amp;lt;int&amp;gt; | default = 10000]

    # Maximum line size on ingestion path. Example: 256kb.
    # There is no limit when unset.
    [max_line_size: &amp;lt;string&amp;gt; | default = none ]

    # Maximum number of log entries that will be returned for a query. 0 to disable.
    [max_entries_limit: &amp;lt;int&amp;gt; | default = 5000 ]

    # Maximum number of active streams per user, across the cluster. 0 to disable.
    # When the global limit is enabled, each ingester is configured with a dynamic
    # local limit based on the replication factor and the current number of healthy
    # ingesters, and is kept updated whenever the number of ingesters change.
    [max_global_streams_per_user: &amp;lt;int&amp;gt; | default = 0]

    # Maximum number of chunks that can be fetched by a single query.
    [max_chunks_per_query: &amp;lt;int&amp;gt; | default = 2000000]

    # The limit to length of chunk store queries. 0 to disable.
    [max_query_length: &amp;lt;duration&amp;gt; | default = 0]

    # Maximum number of queries that will be scheduled in parallel by the
    # frontend.
    [max_query_parallelism: &amp;lt;int&amp;gt; | default = 14]

    # Cardinality limit for index queries
    [cardinality_limit: &amp;lt;int&amp;gt; | default = 100000]

    # Maximum number of stream matchers per query.
    [max_streams_matchers_per_query: &amp;lt;int&amp;gt; | default = 1000]

    # Feature renamed to &#39;runtime configuration&#39;, flag deprecated in favor of -runtime-config.file (runtime_config.file in YAML)
    [per_tenant_override_config: &amp;lt;string&amp;gt;]

    # Feature renamed to &#39;runtime configuration&#39;, flag deprecated in favor of -runtime-config.reload-period (runtime_config.period in YAML)
    [per_tenant_override_period: &amp;lt;duration&amp;gt; | default = 10s]
16、frontend_worker_config
    # Address of query frontend service, in host:port format.
    # CLI flag: -querier.frontend-address
    [frontend_address: &amp;lt;string&amp;gt; | default = &amp;quot;&amp;quot;]

    # Number of simultaneous queries to process.
    # CLI flag: -querier.worker-parallelism
    [parallelism: &amp;lt;int&amp;gt; | default = 10]

    # How often to query DNS.
    # CLI flag: -querier.dns-lookup-period
    [dns_lookup_duration: &amp;lt;duration&amp;gt; | default = 10s]

    grpc_client_config:
      # gRPC client max receive message size (bytes).
      # CLI flag: -querier.frontend-client.grpc-max-recv-msg-size
      [max_recv_msg_size: &amp;lt;int&amp;gt; | default = 104857600]

      # gRPC client max send message size (bytes).
      # CLI flag: -querier.frontend-client.grpc-max-send-msg-size
      [max_send_msg_size: &amp;lt;int&amp;gt; | default = 16777216]

      # Use compression when sending messages.
      # CLI flag: -querier.frontend-client.grpc-use-gzip-compression
      [use_gzip_compression: &amp;lt;boolean&amp;gt; | default = false]

      # Rate limit for gRPC client; 0 means disabled.
      # CLI flag: -querier.frontend-client.grpc-client-rate-limit
      [rate_limit: &amp;lt;float&amp;gt; | default = 0]

      # Rate limit burst for gRPC client.
      # CLI flag: -querier.frontend-client.grpc-client-rate-limit-burst
      [rate_limit_burst: &amp;lt;int&amp;gt; | default = 0]

      # Enable backoff and retry when we hit ratelimits.
      # CLI flag: -querier.frontend-client.backoff-on-ratelimits
      [backoff_on_ratelimits: &amp;lt;boolean&amp;gt; | default = false]

      backoff_config:
        # Minimum delay when backing off.
        # CLI flag: -querier.frontend-client.backoff-min-period
        [min_period: &amp;lt;duration&amp;gt; | default = 100ms]

        # Maximum delay when backing off.
        # CLI flag: -querier.frontend-client.backoff-max-period
        [max_period: &amp;lt;duration&amp;gt; | default = 10s]

        # Number of times to backoff and retry before failing.
        # CLI flag: -querier.frontend-client.backoff-retries
        [max_retries: &amp;lt;int&amp;gt; | default = 10]
17、table_manager_config，provision_config都是用于DynamoDB。
18、auto_scaling_config用于DynamoDB的自动伸缩
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体可以参考&lt;a href=&#34;https://github.com/grafana/loki/tree/v1.5.0/docs/configuration&#34;&gt;官网&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;loki的配置还是比较复杂的，下面我们再来看一下promtail的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat promtail.yaml
client:
  backoff_config:
    max_period: 5s
    max_retries: 20
    min_period: 100ms
  batchsize: 102400
  batchwait: 1s
  external_labels: {}
  timeout: 10s
positions:
  filename: /run/promtail/positions.yaml
server:
  http_listen_port: 3101
target_config:
  sync_period: 10s
scrape_configs:
- job_name: kubernetes-pods-name
  pipeline_stages:
    - docker: {}
  kubernetes_sd_configs:
  - role: pod
  relabel_configs:
  - source_labels:
    - __meta_kubernetes_pod_label_name
    target_label: __service__
  - source_labels:
    - __meta_kubernetes_pod_node_name
    target_label: __host__
  - action: drop
    regex: &#39;&#39;
    source_labels:
    - __service__
  - action: labelmap
    regex: __meta_kubernetes_pod_label_(.+)
  - action: replace
    replacement: $1
    separator: /
    source_labels:
    - __meta_kubernetes_namespace
    - __service__
    target_label: job
  - action: replace
    source_labels:
    - __meta_kubernetes_namespace
    target_label: namespace
  - action: replace
    source_labels:
    - __meta_kubernetes_pod_name
    target_label: pod
  - action: replace
    source_labels:
    - __meta_kubernetes_pod_container_name
    target_label: container
  - replacement: /var/log/pods/*$1/*.log
    separator: /
    source_labels:
    - __meta_kubernetes_pod_uid
    - __meta_kubernetes_pod_container_name
    target_label: __path__
- job_name: kubernetes-pods-app
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;promtail的配置和prometheus很像，我们也简单说明一下，promtail的复杂配置分为四个部分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server_config 配置promtail作为一个服务器。开启一个http端口
client_config 配置promtail怎么连接loki，它作为loki的客户端
position_config 指明promtail的配置文件在什么地方生成，重启的时候会读取一些信息
scrape_config 配置一些常用的抓取策略
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们主要配置的地方，就是scrape_config 。它又分为几种常见的抓取方式，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journal_config
syslog_config
relabel_config
static_config
file_sd_config
kubernetes_sd_config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于我们来说，最常使用的就是static_config，比如指定业务的某个日志文件。这部分的描述很长，具体可以参考github文档。&lt;/p&gt;

&lt;p&gt;一个配置文件中，是可以针对不同类型的日志文件同时进行监控的。比如下面的长长的配置文件，就加入了三个抓取策略。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://localhost:3100/loki/api/v1/push

scrape_configs:
  - job_name: journal
    journal:
      max_age: 12h
      labels:
        job: systemd-journal
    relabel_configs:
      - source_labels: [&#39;__journal__systemd_unit&#39;]
        target_label: &#39;unit&#39;
  - job_name: system
    pipeline_stages:
    static_configs:
    - labels:
       job: varlogs
       host: yourhost
       __path__: /var/log/*.log
  - job_name: biz001
    pipeline_stages:
    - match:
       selector: &#39;{app=&amp;quot;test&amp;quot;}&#39;
       stages:
       - regex:
          expression: &#39;.*level=(?P&amp;lt;level&amp;gt;[a-zA-Z]+).*ts=(?P&amp;lt;timestamp&amp;gt;[T\d-:.Z]*).*component=(?P&amp;lt;component&amp;gt;[a-zA-Z]+)&#39;
       - labels:
          level:
          component:
          ts:
          timestrap:
    static_configs:
    - labels:
       job: biz001
       app: test
       node: 001
       host: localhost
       __path__: /alertmgr/dingtalk/nohup.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们配置了三个job（概念见普罗米修斯），journal，system和biz001。尤其注意biz001的配置，这代表了我们对一些日志的通用配置方式。&lt;/p&gt;

&lt;p&gt;首先，看一下biz001的日志格式。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level=info ts=2020-04-30T01:20:38.631Z caller=entry.go:22 component=web http_scheme=http http_proto=HTTP/1.1 http_method=POST remote_addr=[::1]:57710 user_agent=Alertmanager/0.20.0 uri=http://localhost:8060/dingtalk/webhook1/send resp_status=200 resp_bytes_length=2 resp_elapsed_ms=5207.398549 msg=&amp;quot;request complete&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在将日志传送到Loki之前，promtail可以对其进行一系列的操作。比如过滤一些日志，提取一些label，替换一些日志的内容等。&lt;/p&gt;

&lt;p&gt;对于这部分的操作，现有的日志收集工具都搞了一套自己的，而且都很难用。&lt;/p&gt;

&lt;p&gt;比如我们用来解析我们固定格式的nginx日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ps -ef | grep promtail
root     14449 14356  0 21:06 pts/0    00:00:00 grep promtail
root     28509     1  0 Jul21 ?        00:23:12 /opt/promes/loki/promtail-linux-amd64 --config.file=/opt/promes/loki/nginx.yaml
[root@promessitweb19 ~]# cat /opt/promes/loki/nginx.yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /opt/promes/loki/positions.yaml

clients:
  - url: http://10.243.51.50:3100/loki/api/v1/push

scrape_configs:
- job_name: nginx
  static_configs:
  - targets:
      - localhost
    labels:
      job: nginxAccess
      __path__: /opt/rsync_log/access_http.log
      ip: &amp;quot;10.243.58.14&amp;quot;
      appId: PROMES
      softType: blackbox
  pipeline_stages:
  - match:
      selector: &#39;{app=&amp;quot;nginx&amp;quot;}&#39;
      stages:
      - regex:
          expression: &#39;^(?P&amp;lt;remote_addr&amp;gt;\\S+)   (?P&amp;lt;http_x_forwarded_for&amp;gt;\\S+)  (?P&amp;lt;http_x_forwarded_for2&amp;gt;\\S+) (?P&amp;lt;http_x_forwarded_for3&amp;gt;\\S+) (?P&amp;lt;time_iso8601&amp;gt;\\S+)  (?P&amp;lt;request_method&amp;gt;\\S+)    &amp;quot;(?P&amp;lt;document_uri&amp;gt;\\S+)&amp;quot;    &amp;quot;(?P&amp;lt;query_string&amp;gt;\\S+)&amp;quot;    (?P&amp;lt;request_http_protocol&amp;gt;\\S+) (?P&amp;lt;status&amp;gt;\\d{3}|-)    (?P&amp;lt;body_bytes_sent&amp;gt;\\d{3}|-)   (?P&amp;lt;request_time&amp;gt;\\S+)  &amp;quot;(?P&amp;lt;http_referer&amp;gt;\\S+)&amp;quot;    &amp;quot;(?P&amp;lt;user_agent&amp;gt;\\S+)&amp;quot;  traceId:(?P&amp;lt;traceId&amp;gt;\\S+),spanId:(?P&amp;lt;spanId&amp;gt;\\S+)   (?P&amp;lt;server_addr&amp;gt;\\S+)   (?P&amp;lt;hostname&amp;gt;\\S+)  (?P&amp;lt;host&amp;gt;\\S+)  (?P&amp;lt;remote_port&amp;gt;\\S+)   (?P&amp;lt;server_port&amp;gt;\\S+)   &amp;quot;(?P&amp;lt;upstream_addr&amp;gt;\\S+)&amp;quot;   &amp;quot;(?P&amp;lt;upstream_status&amp;gt;\\S+)&amp;quot; &amp;quot;(?P&amp;lt;upstream_response_time&amp;gt;\\S+)&amp;quot;  (?P&amp;lt;version&amp;gt;\\S+)?$&#39;
      - labels:
          remote_addr:
          http_x_forwarded_for:
          http_x_forwarded_for2:
          http_x_forwarded_for3:
          timestamp:
          request_method:
          document_uri:
          query_string:
          request_http_protocol:
          status:
          body_bytes_sent:
          request_time:
          http_referer:
          user_agent:
          traceId:
          spanId:
          server_addr:
          hostname:
          host:
          remote_port:
          server_port:
          upstream_addr:
          upstream_status:
          upstream_response_time:
          version:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;物理部署&#34;&gt;物理部署&lt;/h2&gt;

&lt;p&gt;物理部署很简单，可以直接下载二进制文件，官方还提供来repo，我们还可以编译&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/grafana/loki $GOPATH/src/github.com/grafana/loki
$ cd $GOPATH/src/github.com/grafana/loki
$ make loki
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后直接用二进制文件加配置文件进行启动就可以了，配置文件在/etc/loki/promtail.yaml and /etc/loki/loki.yaml。&lt;/p&gt;

&lt;h2 id=&#34;基本使用&#34;&gt;基本使用&lt;/h2&gt;

&lt;p&gt;下面我们就可以到grafana界面进行操作了，进入 grafana 界面，添加 loki 作为数据源，grafana原生就是支持loki的，所以直接添加loki 在集群中的地址，比如: &lt;a href=&#34;http://loki.monitoring.svc.cluster.local:3100&#34;&gt;http://loki.monitoring.svc.cluster.local:3100&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据源添加好了，我们就可以开始查询分析日志了，点击 Explore，下拉选择 loki 作为数据源，切到 Logs 模式(不用 Metrics 模式)，在 Log labels 按钮那里就能通过 label 筛选日志了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;选择器&#34;&gt;选择器&lt;/h2&gt;

&lt;p&gt;对于查询表达式的标签部分，将其包装在花括号中{}，然后使用键值对的语法来选择标签，多个标签表达式用逗号分隔，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{app=&amp;quot;mysql&amp;quot;,name=&amp;quot;mysql-backup&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前支持以下标签匹配运算符：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;=等于
!=不相等
=~正则表达式匹配
!~不匹配正则表达式
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{name=~&amp;quot;mysql.+&amp;quot;}
{name!~&amp;quot;mysql.+&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;适用于Prometheus标签选择器规则同样也适用于Loki日志流选择器,可以查看官网的&lt;a href=&#34;https://github.com/grafana/loki/blob/v1.5.0/docs/logql.md&#34;&gt;logQL&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;h2 id=&#34;promtail&#34;&gt;Promtail&lt;/h2&gt;

&lt;p&gt;promtail 可以理解为采集日志的 “Prometheus”. 它最巧妙的设计是完全复用了 Prometheus 的服务发现机制与 label 机制.&lt;/p&gt;

&lt;p&gt;以 Kubernetes 服务发现为例, Prometheus 可以通过 Pod 的 Annotations 与 Labels 等信息来确定 Pod 是否需要抓取指标, 假如要的话 Pod 的指标暴露在哪个端口上, 以及这个 Pod 本身有哪些 label, 即 target label.&lt;/p&gt;

&lt;p&gt;确定了这些信息之后, Prometheus 就可以去拉应用的指标了. 同时, 这些指标都会被打上 target label, 用于标注指标的来源. 等到在查询的时候, 我们就可以通过 target label, 比方说 pod_name=foo-123512 或 service=user-service 来获取特定的一个或一组 Pod 上的指标信息.&lt;/p&gt;

&lt;p&gt;promtail 是一样的道理. 它也是通过 Pod 的一些元信息来确定该 Pod 的日志文件位置, 同时为日志打上特定的 target label. 但要注意, 这个 label 不是标注在每一行日志事件上的, 而是被标注在”整个日志”上的. 这里”整个日志”在 loki 中抽象为 stream(日志流). 这就是 loki 文档中所说的”不索引日志, 只索引日志流”. 最终在查询端, 我们通过这些 label 就可以快速查询一个或一组特定的 stream.&lt;/p&gt;

&lt;p&gt;服务发现部分的代码非常直白, 可以去 pkg/promtail/targetmanager.go 中自己看一下, 提两个实现细节:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;promtail 要求所有 target 都跟自己属于同一个 node, 处于其它 node 上的 target 会被忽略;
promtail 使用 target 的 __path__ label 来确定日志路径;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过服务发现确定要收集的应用以及应用的日志路径后, promtail 就开始了真正的日志收集过程. 这里分三步:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、用 fsnotify 监听对应目录下的文件创建与删除(处理 log rolling)
2、对每个活跃的日志文件起一个 goroutine 进行类似 tail -f 的读取, 读取到的内容发送给 channel
3、一个单独的 goroutine 会解析 channel 中的日志行, 分批发送给 loki 的 backend
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;监听&#34;&gt;监听&lt;/h3&gt;

&lt;p&gt;fsnotify负责监听&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    select {
    case event := &amp;lt;-t.watcher.Events:
        switch event.Op {
        case fsnotify.Create:
            // protect against double Creates.
            if _, ok := t.tails[event.Name]; ok {
                level.Info(t.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;got &#39;create&#39; for existing file&amp;quot;, &amp;quot;filename&amp;quot;, event.Name)
                continue
            }

            // newTailer 中会启动一个 goroutine 来读目标文件
            tailer := newTailer(t.logger, t.handler, t.positions, t.path, event.Name)
            t.tails[event.Name] = tailer

        case fsnotify.Remove:
            tailer, ok := t.tails[event.Name]
            if ok {
                // 关闭 tailer
                helpers.LogError(&amp;quot;stopping tailer&amp;quot;, tailer.stop)
                delete(t.tails, event.Name)
            }
        }
    case err := &amp;lt;-t.watcher.Errors:
        level.Error(t.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error from fswatch&amp;quot;, &amp;quot;error&amp;quot;, err)
    case &amp;lt;-t.quit:
        return
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个for循环，一直来处理对应目录下的文件创建与删除的事件。&lt;/p&gt;

&lt;h3 id=&#34;tail日志&#34;&gt;tail日志&lt;/h3&gt;

&lt;p&gt;newTailer() 这个方法中启动的日志文件读取逻辑&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unc newTailer() {
    tail := tail.TailFile(path, tail.Config{
        Follow: true,
        Location: &amp;amp;tail.SeekInfo{
            Offset: positions.Get(path),
            Whence: 0,
        },
    })

    tailer := ...
    go tailer.run()
}

func (t *tailer) run() {
    for {
        select {
        case &amp;lt;-positionWait.C:
            // 定时同步当前读取位置
            pos := t.tail.Tell()
            t.positions.Put(t.path, pos)

        case line, ok := &amp;lt;-t.tail.Lines:
            // handler.Handle() 中是一些日志行的预处理逻辑, 最后将日志行转化为 `Entry` 对象扔进 channel
            if err := t.handler.Handle(model.LabelSet{}, line.Time, line.Text); err != nil {
                level.Error(t.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error handling line&amp;quot;, &amp;quot;error&amp;quot;, err)
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里直接调用了 hpcloud/tail 这个包来完成文件的 tail 操作. hpcloud/tail 的内部实现中, 在读到 EOF 之后, 同样调用了 fsnotify 来获取新内容写入的通知. fsnotify 这个包内部则是依赖了 inotify_init 和 inotify_add_watch 这两个系统调用。&lt;/p&gt;

&lt;h3 id=&#34;日志channel&#34;&gt;日志channel&lt;/h3&gt;

&lt;p&gt;这里有一个单独的 goroutine 会读取所有 tailer 通过 channel 传过来的日志(Entry对象), 然后按批发送给 loki&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    // 每次发送之后要重置计时器
    maxWait.Reset(c.cfg.BatchWait)
    select {
    case &amp;lt;-c.quit:
        return
    case e := &amp;lt;-c.entries:
        // Batch 足够大之后, 执行发送逻辑
        if batchSize+len(e.Line) &amp;gt; c.cfg.BatchSize {
            c.send(batch)
            // 重置 Batch
            batchSize = 0
            batch = map[model.Fingerprint]*logproto.Stream{}
        }

        // 收到 Entry, 先写进 Batch 当中
        batchSize += len(e.Line)

        // 每个 entry 要根据 label 放进对应的日志流(Stream)中
        fp := e.labels.FastFingerprint()
        stream, ok := batch[fp]
        if !ok {
            stream = &amp;amp;logproto.Stream{
                Labels: e.labels.String(),
            }
            batch[fp] = stream
        }
        stream.Entries = append(stream.Entries, e.Entry)

    case &amp;lt;-maxWait.C:
        // 到达每个批次的最大等待时间, 同样执行发送
        if len(batch) &amp;gt; 0 {
            c.send(batch);
            batchSize = 0
            batch = map[model.Fingerprint]*logproto.Stream{}
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用 channel + select 写 batch 逻辑真的挺优雅, 简单易读.&lt;/p&gt;

&lt;h2 id=&#34;loki&#34;&gt;loki&lt;/h2&gt;

&lt;p&gt;loki的基本架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki4&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;distributor&#34;&gt;Distributor&lt;/h3&gt;

&lt;p&gt;我们都知道promtail封装后label后的log数据发生到loki，Distributor就是第一个接收日志的组件。由于日志的写入量可能很大，所以不能在它们传入时将它们写入数据库。这会毁掉数据库。我们需要批处理和压缩数据。&lt;/p&gt;

&lt;p&gt;Loki通过构建压缩数据块来实现这一点，方法是在日志进入时对其进行gzip操作，组件ingester是一个有状态的组件，负责构建和刷新chunck，当chunk达到一定的数量或者时间后，刷新到存储中去。每个流的日志对应一个ingester,当日志到达Distributor后，根据元数据和hash算法计算出应该到哪个ingester上面。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki8&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们具体看一下promtail 的日志写入请求, 请求体由 protobuf 编码, 格式如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 一次写入请求, 包含多段日志流
type PushRequest struct {
    Streams []*Stream `protobuf:&amp;quot;bytes,1,rep,name=streams&amp;quot; json:&amp;quot;streams,omitempty&amp;quot;`
}
// 一段日志流, 包含它的 label, 以及这段日志流当中的每个日志事件: Entry
type Stream struct {
    Labels  string  `protobuf:&amp;quot;bytes,1,opt,name=labels,proto3&amp;quot; json:&amp;quot;labels,omitempty&amp;quot;`
    Entries []Entry `protobuf:&amp;quot;bytes,2,rep,name=entries&amp;quot; json:&amp;quot;entries&amp;quot;`
}
// 一个日志事件, 包含时间戳与内容
type Entry struct {
    Timestamp time.Time `protobuf:&amp;quot;bytes,1,opt,name=timestamp,stdtime&amp;quot; json:&amp;quot;timestamp&amp;quot;`
    Line      string    `protobuf:&amp;quot;bytes,2,opt,name=line,proto3&amp;quot; json:&amp;quot;line,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;distributor 收到请求后, 会将一个 PushRequest 中的 Stream 根据 labels 拆分成多个 PushRequest, 这个过程使用一致性哈希:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;streams := make([]streamTracker, len(req.Streams))
keys := make([]uint32, 0, len(req.Streams))
for i, stream := range req.Streams {
    // 获取每个 stream 的 label hash
    keys = append(keys, tokenFor(userID, stream.Labels))
    streams[i].stream = stream
}

// 根据 label hash 到 hash ring 上获取对应的 ingester 节点
// 这里的节点指 hash ring 上的节点, 一个节点可能有多个对等的 ingester 副本来做 HA
replicationSets := d.ring.BatchGet(keys, ring.Write)

// 将 Stream 按对应的 ingester 节点进行分组
samplesByIngester := map[string][]*streamTracker{}
ingesterDescs := map[string]ring.IngesterDesc{}
for i, replicationSet := range replicationSets {
    for _, ingester := range replicationSet.Ingesters {
        samplesByIngester[ingester.Addr] = append(samplesByIngester[ingester.Addr], &amp;amp;streams[i])
        ingesterDescs[ingester.Addr] = ingester
    }
}

for ingester, samples := range samplesByIngester {
    // 每组 Stream[] 又作为一个 PushRequest, 下发给对应的 ingester 节点
    d.sendSamples(localCtx, ingester, samples, &amp;amp;tracker)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 All in One 的运行模式中, hash ring 直接存储在内存中. 在生产环境, 由于要起多个 distributor 节点做高可用, 这个 hash ring 会存储到外部的 Consul 集群中.&lt;/p&gt;

&lt;h3 id=&#34;ingester&#34;&gt;Ingester&lt;/h3&gt;

&lt;p&gt;ingester接收到日志并开始构建chunk:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki5&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;基本上就是将日志进行压缩并附加到chunk上面。一旦chunk“填满”（数据达到一定数量或者过了一定期限），ingester将其刷新到数据库。我们对块和索引使用单独的数据库，因为它们存储的数据类型不同。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki6&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;刷新一个chunk之后，ingester然后创建一个新的空chunk并将新条目添加到该chunk中。&lt;/p&gt;

&lt;p&gt;我们再重代码层来分析一下，ingester 接收 distributor 下发的 PushRequest, 也就是多段日志流([]Entry). 在 ingester 内部会先将收到的 []Entry Append 到内存中的 Chunk 流([]Chunk). 同时会有一组 goroutine 异步将 Chunk 流存储到对象存储当中:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki9&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第一个 Append 过程很关键&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (i *instance) Push(ctx context.Context, req *logproto.PushRequest) error {
    for _, s := range req.Streams {
        // 将收到的日志流 Append 到内存中的日志流上, 同样地, 日志流按 label hash 索引
        fp := client.FastFingerprint(req.labels)
        stream, ok := i.streams[fp]
        if !ok {
            stream = newStream(fp, req.labels)
            // 这个过程中, 还会维护日志流的倒排索引(label -&amp;gt; stream)
            i.index.Add(labels, fp)
            i.streams[fp] = stream
        }
        stream.Push(ctx, s.Entries)
    }
    return nil
}

func (s *stream) Push(_ context.Context, entries []logproto.Entry) error {
    for i := range entries {
        // 假如当前 Chunk 已经关闭或者已经到达设定的最大 Chunk 大小, 则再创建一个新的 Chunk
        if s.chunks[0].closed || !s.chunks[0].chunk.SpaceFor(&amp;amp;entries[i]) {
            s.chunks = append(s.chunks, chunkDesc{
                chunk: chunkenc.NewMemChunk(chunkenc.EncGZIP),
            })
        }
        s.chunks[len(s.chunks)-1].chunk.Append(&amp;amp;entries[i])
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chunk 其实就是多条日志构成的压缩包. 将日志压成 Chunk 的意义是可以直接存入对象存储, 而对象存储是最便宜的(便宜是 loki 的核心目标之一). 在 一个 Chunk 到达指定大小之前它就是 open 的, 会不断 Append 新的日志(Entry) 到里面. 而在达到大小之后, Chunk 就会关闭等待持久化(强制持久化也会关闭 Chunk, 比如关闭 ingester 实例时就会关闭所有的 Chunk并持久化).&lt;/p&gt;

&lt;p&gt;对 Chunk 的大小控制是一个调优要点:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;假如 Chunk 容量过小: 首先是导致压缩效率不高. 同时也会增加整体的 Chunk 数量, 导致倒排索引过大. 最后, 对象存储的操作次数也会变多, 带来额外的性能开销;
假如 Chunk 过大: 一个 Chunk 的 open 时间会更长, 占用额外的内存空间, 同时, 也增加了丢数据的风险. 最后, Chunk 过大也会导致查询读放大, 比方说查一小时的数据却要下载整天的 Chunk;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;丢数据问题: 所有 Chunk 要在 close 之后才会进行存储. 因此假如 ingester 异常宕机, 处于 open 状态的 Chunk, 以及 close 了但还没有来得及持久化的 Chunk 数据都会丢失. 从这个角度来说, ingester 其实也是 stateful 的, 在生产中可以通过给 ingester 跑多个副本来解决这个问题. 另外, ingester 里似乎还没有写 WAL, 这感觉是一个 PR 机会, 可以练习一下写存储的基本功.&lt;/p&gt;

&lt;p&gt;异步存储过程就很简单了, 是一个一对多的生产者消费者模型:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 一个 goroutine 将所有的待存储的 chunks enqueue
func (i *Ingester) sweepStream(instance *instance, stream *stream, immediate bool) {

    // 有一组待存储的队列(默认16个), 取模找一个队列把要存储的 chunk 的引用塞进去
    flushQueueIndex := int(uint64(stream.fp) % uint64(i.cfg.ConcurrentFlushes))
    firstTime, _ := stream.chunks[0].chunk.Bounds()
    i.flushQueues[flushQueueIndex].Enqueue(&amp;amp;flushOp{
        model.TimeFromUnixNano(firstTime.UnixNano()), instance.instanceID,
        stream.fp, immediate,
    })
}

// 每个队列都有一个 goroutine 作为消费者在 dequeue
func (i *Ingester) flushLoop(j int) {
    for {
        op := i.flushQueues[j].Dequeue()
        // 实际的存储操作在这个方法中, 存储完成后, Chunk 会被清理掉
        i.flushUserSeries(op.userID, op.fp, op.immediate)

        // 存储失败的 chunk 会重新塞回队列中
        if op.immediate &amp;amp;&amp;amp; err != nil {
            op.from = op.from.Add(flushBackoff)
            i.flushQueues[j].Enqueue(op)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后是清理过程, 同样是一个单独的 goroutine 定时在跑. ingester 里的所有 Chunk 会在持久化之后隔一小段时间才被清理掉. 这个”一小段时间”由 chunk-retain-time 参数进行控制(默认 15 分钟). 这么做是为了加速热点数据的读取(真正被人看的日志中, 有99%都是生成后的一小段时间内被查看的).&lt;/p&gt;

&lt;h3 id=&#34;querier&#34;&gt;Querier&lt;/h3&gt;

&lt;p&gt;读取就非常简单了，由Querier负责给定一个时间范围和标签选择器，Querier查看索引以确定哪些块匹配，并通过greps将结果显示出来。它还从Ingester获取尚未刷新的最新数据，合并后返回。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/loki/loki7&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;合并返回日志的时候，loki 里用了堆, 时间正序就用最小堆, 时间逆序就用最大堆:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这部分代码实现了一个简单的二叉堆, MinHeap 和 MaxHeap 实现了相反的 `Less()` 方法
type iteratorHeap []EntryIterator
func (h iteratorHeap) Len() int            { return len(h) }
func (h iteratorHeap) Swap(i, j int)       { h[i], h[j] = h[j], h[i] }
func (h iteratorHeap) Peek() EntryIterator { return h[0] }
func (h *iteratorHeap) Push(x interface{}) {
    *h = append(*h, x.(EntryIterator))
}
func (h *iteratorHeap) Pop() interface{} {
    old := *h
    n := len(old)
    x := old[n-1]
    *h = old[0 : n-1]
    return x
}
type iteratorMinHeap struct {
    iteratorHeap
}
func (h iteratorMinHeap) Less(i, j int) bool {
    return h.iteratorHeap[i].Entry().Timestamp.Before(h.iteratorHeap[j].Entry().Timestamp)
}
type iteratorMaxHeap struct {
    iteratorHeap
}
func (h iteratorMaxHeap) Less(i, j int) bool {
    return h.iteratorHeap[i].Entry().Timestamp.After(h.iteratorHeap[j].Entry().Timestamp)
}

// 将一组 Stream 的 iterator 合并成一个 HeapIterator
func NewHeapIterator(is []EntryIterator, direction logproto.Direction) EntryIterator {
    result := &amp;amp;heapIterator{}
    switch direction {
    case logproto.BACKWARD:
        result.heap = &amp;amp;iteratorMaxHeap{}
    case logproto.FORWARD:
        result.heap = &amp;amp;iteratorMinHeap{}
    default:
        panic(&amp;quot;bad direction&amp;quot;)
    }
    // pre-next each iterator, drop empty.
    for _, i := range is {
        result.requeue(i)
    }
    return result
}

func (i *heapIterator) requeue(ei EntryIterator) {
    if ei.Next() {
        heap.Push(i.heap, ei)
        return
    }
    if err := ei.Error(); err != nil {
        i.errs = append(i.errs, err)
    }
    helpers.LogError(&amp;quot;closing iterator&amp;quot;, ei.Close)
}

func (i *heapIterator) Next() bool {
    if i.curr != nil {
        i.requeue(i.curr)
    }
    if i.heap.Len() == 0 {
        return false
    }
    i.curr = heap.Pop(i.heap).(EntryIterator)
    currEntry := i.curr.Entry()
    // keep popping entries off if they match, to dedupe
    for i.heap.Len() &amp;gt; 0 {
        next := i.heap.Peek()
        nextEntry := next.Entry()
        if !currEntry.Equal(nextEntry) {
            break
        }

        next = heap.Pop(i.heap).(EntryIterator)
        i.requeue(next)
    }
    return true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;1、Loki的索引存储可以是cassandra/bigtable/dynamodb来进行扩展，chuncks可以是各种对象存储，放入对象存储中进行扩展。&lt;/p&gt;

&lt;p&gt;2、Querier和Distributor都是无状态的组件，可以水平扩展，可以使用负载均衡。&lt;/p&gt;

&lt;p&gt;3、对于ingester他虽然是有状态的但是，当新的节点加入或者减少，整节点间的chunk会重新分配，已适应新的散列环。这些信息需要存储到etcd或者consul等第三方工具中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.5.20&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;loki今天发布了1.5.0版本！引入了名为boltdb-shipper的新索引选项，这个新索引允许您仅使用对象存储（S3，GCS，文件系统等）来运行Loki。您不再需要单独的专用索引存储（DynamoDB，Bigtable，Cassandra等）！&lt;/p&gt;

&lt;p&gt;该boltdb-shipper索引使用内存中的boltdb索引，但会定期将快照发送到对象存储。这允许通过对象存储共享索引信息。&lt;/p&gt;

&lt;p&gt;将来可扩展可以通过boltdb-shipper索引和memberlist的gossip来完成集群功能。&lt;/p&gt;

&lt;p&gt;在云存储上，ring的信息可以通过gossip协议来进行同步。可以看一下下面的这个配置，基于s3和memberlist的可扩展模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;auth_enabled: false

server:
  http_listen_port: 3100

distributor:
  ring:
    store: memberlist

ingester:
  lifecycler:
    ring:
      kvstore:
        store: memberlist
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 5m
  chunk_retain_period: 30s

memberlist:
  abort_if_cluster_join_fails: false

  # Expose this port on all distributor, ingester
  # and querier replicas.
  bind_port: 7946

  # You can use a headless k8s service for all distributor,
  # ingester and querier components.
  join_members:
  - loki-gossip-ring.loki.svc.cluster.local:7946

  max_join_backoff: 1m
  max_join_retries: 10
  min_join_backoff: 1s

schema_config:
  configs:
  - from: 2020-05-15
    store: boltdb-shipper
    object_store: s3
    schema: v11
    index:
      prefix: index_
      period: 168h

storage_config:
 boltdb_shipper:
   active_index_directory: /loki/index
   cache_location: /loki/index_cache
   resync_interval: 5s
   shared_store: s3

 aws:
   s3: s3://access_key:secret_access_key@custom_endpoint/bucket_name
   s3forcepathstyle: true

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列----VictoriaMetrics</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/</link>
          <pubDate>Thu, 13 Jun 2019 16:19:46 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/</guid>
          <description>&lt;p&gt;VictoriaMetrics是一个高性能的，长期存储的prometheus的远程解决方案，实现集群使用的federation的方式，只不过性能很优秀，包括write和query，聚合数据也解决了查询问题。&lt;/p&gt;

&lt;h1 id=&#34;优势&#34;&gt;优势&lt;/h1&gt;

&lt;p&gt;VictoriaMetrics不仅仅是时序数据库,它的优势主要体现在一下几点:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对外支持Prometheus相关的API，所以它可以直接用于Grafana作为Prometheus数据源使用, 同时扩展了PromQL, 详细使用可参考&lt;a href=&#34;https://github.com/VictoriaMetrics/VictoriaMetrics/wiki/ExtendedPromQL&#34;&gt;https://github.com/VictoriaMetrics/VictoriaMetrics/wiki/ExtendedPromQL&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;针对Prometheus的Metrics插入查询具备高性能和良好的扩展性。甚至性能比InfluxDB和TimescaleDB高出20x&lt;/li&gt;
&lt;li&gt;内存占用方面也做出了优化, 比InfluxDB少10x&lt;/li&gt;
&lt;li&gt;高性能的数据压缩方式,使存入存储的数据量比TimescaleDB多达70x&lt;/li&gt;
&lt;li&gt;优化了高延迟IO和低iops的存储&lt;/li&gt;
&lt;li&gt;操作简单&lt;/li&gt;
&lt;li&gt;支持从第三方时序数据库获取数据源&lt;/li&gt;
&lt;li&gt;异常关闭情况下可以保护存储数据损坏&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;部署&#34;&gt;部署&lt;/h1&gt;

&lt;h2 id=&#34;单点&#34;&gt;单点&lt;/h2&gt;

&lt;h3 id=&#34;编译&#34;&gt;编译&lt;/h3&gt;

&lt;p&gt;1、二进制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make victoria-metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make victoria-metrics-prod
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;启动&#34;&gt;启动&lt;/h3&gt;

&lt;p&gt;直接使用二进制文件进行启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup /opt/victoria-metrics/victoria-metrics-prod -storageDataPath=&amp;quot;/data/victoria&amp;quot; -retentionPeriod=2  &amp;gt;&amp;gt;/opt/promes/victoria-metrics/logs/start.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;-storageDataPath - path to data directory. VictoriaMetrics stores all the data in this directory.&lt;/li&gt;
&lt;li&gt;-retentionPeriod - retention period in months for the data. Older data is automatically deleted.&lt;/li&gt;
&lt;li&gt;-httpListenAddr - TCP address to listen to for http requests. By default it listens port 8428 on all the network interfaces.&lt;/li&gt;
&lt;li&gt;-graphiteListenAddr - TCP and UDP address to listen to for Graphite data. By default it is disabled.&lt;/li&gt;
&lt;li&gt;-opentsdbListenAddr - TCP and UDP address to listen to for OpenTSDB data. By default it is disabled.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见他也是一个时序数据库，支持将prometheus，influxdb，graphite，opentsdb的数据的写入，比如使用的是prometheus，只使用了http的端口，在我们对应的prometheus文件中配置远程写入，将数据写入到victoria-metrics中去，配置如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remote_write:
  - url: http://&amp;lt;victoriametrics-addr&amp;gt;:8428/api/v1/write
    queue_config:
      max_samples_per_send: 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数据存储到victoria-metrics，我们还是通过8428端口来读取，我们在grafana中配置datasource：&lt;a href=&#34;http://victoriametrics-addr-ip:8428&#34;&gt;http://victoriametrics-addr-ip:8428&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;停止&#34;&gt;停止&lt;/h3&gt;

&lt;p&gt;发送SIGINT给进程&lt;/p&gt;

&lt;h3 id=&#34;高可用&#34;&gt;高可用&lt;/h3&gt;

&lt;p&gt;启动多个实例，将prometheus的数据分别写入到这些节点中，加一层负载均衡，就可以实现高可用，解决单点问题，prometheus配置如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remote_write:
  - url: http://&amp;lt;victoriametrics-addr-1&amp;gt;:8428/api/v1/write
    queue_config:
      max_samples_per_send: 10000
  # ...
  - url: http://&amp;lt;victoriametrics-addr-N&amp;gt;:8428/api/v1/write
    queue_config:
      max_samples_per_send: 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边讲一下高可用和水平扩展&lt;/p&gt;

&lt;p&gt;高可用是指多活，解决单点故障，正常就是多个相同的服务同时提供服务，来确保一个节点挂了，就能转移到其他的节点上，不影响外部整体的使用，比如redis的主备切换，sentinel机制，还有上面的virtoria-metrics的方式&lt;/p&gt;

&lt;p&gt;水平扩展是一种分布式的能力，一个节点不能处理，就多个节点一起处理，这样分担一下，整体的量就上去了，比如redis的cluster集群，理论上只要加节点，就可以存储月来越多的数据，实际集群内部交互还是有瓶颈的&lt;/p&gt;

&lt;p&gt;正常的服务，可以说在集群同时解决高可用和水平扩展是很困难的，正常的一个集群的作用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;集群内主节点都获取全部数据，然后其他节点都重主节点复制数据，对外一直提供主节点查询，当主节点出现问题的时候，主备切换，这样实现了高可用，但是有单节点数量瓶颈，不能水平扩展。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群内每个节点获取一部分数据，然后集群内节点相互复制，实现最终一致性，每个节点都保存完整的数据，这个时候一个节点挂了，会出问题，单个节点也会有瓶颈，所以在这个基础上收取前加一层负载均衡，这样当一个节点挂了之后，负载均衡会分配到其他节点上，这样实现了高可用，也实现了水平扩展，但是这个很难实现，而且还是有单节点瓶颈，一般是适用这种数据量很小的需要一致性的服务发现。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群内每个节点获取一部分数据，并且只存储这一部分数据，然后集群内使用一些数据库或者自身实现关系映射，然后对外查询会路由到对应的节点上去查询数据。这种模式就是支持水平扩展的，但是有一个节点
出问题，查询就会出问题，没有实现高可用，所以在这个基础上实现高可用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个单节点的设置主从复制，相互切换&lt;/li&gt;
&lt;li&gt;完成集群间的复制&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后可以说是目前比较好的解决方式。&lt;/p&gt;

&lt;h3 id=&#34;其他操作&#34;&gt;其他操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;可以删除数据&lt;/li&gt;
&lt;li&gt;可以导出数据&lt;/li&gt;
&lt;li&gt;目前不支持Downsampling，但是victoria-metrics的压缩率和查询效率足以使用&lt;/li&gt;
&lt;li&gt;单节点不支持水平扩展，但是单节点足以媲美thanos和M3，timescaleDB的性能，如果还是觉得不够用，可以尝试集群版本&lt;/li&gt;
&lt;li&gt;virtoria-metrics的参数基本不用调整，都是优化后的合理设计，自身也支持prometheus监控&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;测试&#34;&gt;测试&lt;/h3&gt;

&lt;p&gt;启动脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test victoria-metrics]# cat start.sh
nohup /opt/victoria-metrics/victoria-metrics-prod -storageDataPath=&amp;quot;/data/victoria&amp;quot; -retentionPeriod=2  &amp;gt;&amp;gt;/opt/victoria-metrics/logs/start.log 2&amp;gt;&amp;amp;1 &amp;amp;


-storageDataPath=&amp;quot;/data/victoria&amp;quot;：数据存储目录

-retentionPeriod=2：数据存储时间，两个月
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;11天的数据量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@test victoria-metrics]# du -sh /data/victoria
20G /data/victoria
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cpu和内存消耗&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
21899 root      20   0 42.9g  26g 6280 S 188.5 20.8  13486:10 victoria-metric
 4560 root      20   0  159g  26g 365m S 1110.1 20.8   9846:56 prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;集群模式是采用的分布式部署，将数据分别存储在不同的节点上，实现了水平扩展，目前还没有relaese版本，需要自己编译，但是解决了数据量的问题，同时在性能方面并没有发生太大的影响。&lt;/p&gt;

&lt;h3 id=&#34;编译-1&#34;&gt;编译&lt;/h3&gt;

&lt;p&gt;直接make就会在bin目录下生成可执行文件vmstorage, vmselect and vminsert。&lt;/p&gt;

&lt;h3 id=&#34;架构原理图&#34;&gt;架构原理图&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/vm/vm.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;组件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;vmstorage - stores the data&lt;/p&gt;

&lt;p&gt;vmstore其实就是我们数据存在的地方，需要先启动，否则insert会找不到插入的节点，导致数据丢失。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vminsert - proxies the ingested data to vmstorage shards using consistent hashing&lt;/p&gt;

&lt;p&gt;vminsert对采集的提供的代理接口，同时选择将数据插入到我们指定的store节点，可以是单节点，也可以是集群上所有的机器都部署，通过nginx来负载均衡，可以减少节点压力，但是并不能解决单点问题。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vmselect - performs incoming queries using the data from vmstorage&lt;/p&gt;

&lt;p&gt;vmselect是给外部进行查询的接口，同时也负责查询数据的聚合功能。负载均衡和vmisert一样，使用nginx。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;http-api&#34;&gt;HTTP api&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;insert&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;vminsert-ip&amp;gt;:8480/insert/&amp;lt;accountID&amp;gt;/&amp;lt;suffix&amp;gt;:

&amp;lt;accountID&amp;gt; is an arbitrary number identifying namespace for data ingestion (aka tenant)
&amp;lt;suffix&amp;gt; may have the following values:

    1.prometheus - for inserting data with Prometheus remote write API
    2.influx/write or influx/api/v2/write - for inserting data with Influx line protocol
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;querying&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;vmselect-ip&amp;gt;:8481/select/&amp;lt;accountID&amp;gt;/prometheus/&amp;lt;suffix&amp;gt;:

&amp;lt;accountID&amp;gt; is an arbitrary number identifying data namespace for the query (aka tenant)
&amp;lt;suffix&amp;gt; may have the following values:

    1.api/v1/query - performs PromQL instant query
    2.api/v1/query_range - performs PromQL range query
    3.api/v1/series - performs series query
    4.api/v1/labels - returns a list of label names
    5.api/v1/label/&amp;lt;label_name&amp;gt;/values - returns values for the given &amp;lt;label_name&amp;gt; according to API
    6.federate - returns federated metrics
    7.api/v1/export - exports raw data. See this article for details
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;delete&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;http://&amp;lt;vmselect-ip&amp;gt;:8481/delete/&amp;lt;accountID&amp;gt;/prometheus/api/v1/admin/tsdb/delete_series?match[]=&amp;lt;timeseries_selector_for_delete&amp;gt;.
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;vmstorage&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;vmstore保留了8482端口，提供一下URL：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/snapshot/create - create instant snapshot, which can be used for backups in background. Snapshots are created in &amp;lt;storageDataPath&amp;gt;/snapshots folder, where &amp;lt;storageDataPath&amp;gt; is the corresponding command-line flag value.
/snapshot/list - list available snasphots.
/snapshot/delete?snapshot=&amp;lt;id&amp;gt; - delete the given snapshot.
/snapshot/delete_all - delete all the snapshots.
Snapshots may be created independently on each vmstorage node. There is no need in synchronizing snapshots&#39; creation across vmstorage nodes.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;扩展&#34;&gt;扩展&lt;/h3&gt;

&lt;p&gt;1、vminsert and vmselect是可扩展的，无状态的，可以随时扩展或者缩容，并不影响，只是需要在负载均衡中将相关节点处理一下&lt;/p&gt;

&lt;p&gt;2、vmstore是有状态的，因为是分布式存储数据的，所以新增节点需要如下步骤&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Start new vmstorage node with the same -retentionPeriod as existing nodes in the cluster.&lt;/li&gt;
&lt;li&gt;Gradually restart all the vmselect nodes with new -storageNode arg containing &lt;new_vmstorage_host&gt;:8401.&lt;/li&gt;
&lt;li&gt;Gradually restart all the vminsert nodes with new -storageNode arg containing &lt;new_vmstorage_host&gt;:8400.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;备份和恢复&#34;&gt;备份和恢复&lt;/h3&gt;

&lt;p&gt;1、主要使用vmstore的url来进行备份&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Create an instant snapshot by navigating to /snapshot/create HTTP handler. It will create snapshot and return its name.
Archive the created snapshot from &amp;lt;-storageDataPath&amp;gt;/snapshots/&amp;lt;snapshot_name&amp;gt; folder using any suitable tool that follows symlinks. For instance, cp -L, rsync -L or scp -r. The archival process doesn&#39;t interfere with vmstorage work, so it may be performed at any suitable time. Incremental backups are possible with rsync --delete, which should remove extraneous files from backup dir.
Delete unused snapshots via /snapshot/delete?snapshot=&amp;lt;snapshot_name&amp;gt; or /snapshot/delete_all in order to free up occupied storage space.
There is no need in synchronizing backups among all the vmstorage nodes.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、恢复&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stop vmstorage node with kill -INT.&lt;/li&gt;
&lt;li&gt;Delete all the contents of the directory pointed by -storageDataPath command-line flag.&lt;/li&gt;
&lt;li&gt;Copy all the contents of the backup directory to -storageDataPath directory.&lt;/li&gt;
&lt;li&gt;Start vmstorage node.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;实现原理&#34;&gt;实现原理&lt;/h1&gt;

&lt;h2 id=&#34;vminsert&#34;&gt;vminsert&lt;/h2&gt;

&lt;p&gt;插入数据就比较简单了，使用了prometheus差不多的数据结构体来存储数据，只要将数据转化为对应的结构体直接存入数据就可以。对于其他的时序数据库比如influxdb都是差不多的数据结构，只要稍微进行转换，就可以将数据存储到存储节点去。&lt;/p&gt;

&lt;h2 id=&#34;vmstore&#34;&gt;vmstore&lt;/h2&gt;

&lt;p&gt;存储数据可以比常规的节省10倍的内存&lt;/p&gt;

&lt;h2 id=&#34;vmselect&#34;&gt;vmselect&lt;/h2&gt;

&lt;p&gt;查询数据很快&lt;/p&gt;

&lt;h2 id=&#34;mergetree&#34;&gt;MergeTree&lt;/h2&gt;

&lt;p&gt;VictoriaMetrics将数据存储在相似于ClickHouse的 MergeTree表 数据结构中。它是用于剖析数据和其余事件流的最快的数据库。在典型的剖析查问上，它的性能要比PostgreSQL和MySQL等传统数据库高10到1000倍。&lt;/p&gt;

&lt;h1 id=&#34;特性&#34;&gt;特性&lt;/h1&gt;

&lt;h2 id=&#34;扩展了promeql&#34;&gt;扩展了promeql&lt;/h2&gt;

&lt;p&gt;1 、模版&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;((node_memory_MemTotal_bytes{instance=~&amp;quot;$node:$port&amp;quot;, job=~&amp;quot;$job&amp;quot;} - node_memory_MemFree_bytes{instance=~&amp;quot;$node:$port&amp;quot;, job=~&amp;quot;$job&amp;quot;}) /
node_memory_MemTotal_bytes{instance=~&amp;quot;$node:$port&amp;quot;, job=~&amp;quot;$job&amp;quot;}) * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用模版&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WITH (
    commonFilters = {instance=~&amp;quot;$node:$port&amp;quot;,job=~&amp;quot;$job&amp;quot;}
)
(node_memory_MemTotal_bytes{commonFilters} - node_memory_MemFree_bytes{commonFilters}) /
    node_memory_MemTotal_bytes{commonFilters} * 100
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;发展&#34;&gt;发展&lt;/h1&gt;

&lt;h2 id=&#34;vmagent&#34;&gt;vmagent&lt;/h2&gt;

&lt;p&gt;vmagent是一个很小巧但优秀的代理，它可以帮助您从各种来源收集指标并将其存储到VictoriaMetrics或任何其他支持remote_write协议的与Prometheus兼容的存储系统。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/vm/vm2&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以用作Prometheus的直接替代品，用于抓取目标（例如node_exporter）。&lt;/li&gt;
&lt;li&gt;可以像Prometheus那样，重新添加，删除和修改标签。可以在将数据发送到远程存储之前对其进行过滤。&lt;/li&gt;
&lt;li&gt;支持多种VictoriaMetrics支持的数据格式，比如Influx，OpenTSDB，Graphite，Prometheus等。&lt;/li&gt;
&lt;li&gt;可以将收集的指标同时复制到多个远程存储系统。在与远程存储连接不稳定的环境中工作。如果远程存储不可用，则将收集的指标缓存在-remoteWrite.tmpDataPath中。一旦恢复远程存储的连接，缓冲的metrcis即发送到远程存储。可以通过-remoteWrite.maxDiskUsagePerURL限制缓冲区的最大磁盘使用量。&lt;/li&gt;
&lt;li&gt;与Prometheus相比，使用较少的RAM，CPU，磁盘IO和网络带宽。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前来讲，Prometheus依旧不可或缺。vmagent 还处于开发阶段。但是vmagent有取代prometheus的想法是可以看出来的。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Cortex</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/</link>
          <pubDate>Thu, 13 Jun 2019 14:28:39 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/</guid>
          <description>&lt;p&gt;crotex是一个为了支持prometheus扩展的服务，支持水平扩展，高可用，多租户，长期存储。主要开发者也是promehteus的开发者&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/cortex/architecture.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;组件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Distributor&lt;/p&gt;

&lt;p&gt;Distributor就是负责接收promtheus发送过来的数据，然后将数据分发给lngester。&lt;/p&gt;

&lt;p&gt;Distributor只要和lngester进行交互，使用的是grpc&lt;/p&gt;

&lt;p&gt;Distributor使用一致性hash来将数据分发给哪个lngester实例，consistent hash ring is stored in Consul&lt;/p&gt;

&lt;p&gt;建议使用负载均衡来运行多个distributors实例。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;lngester&lt;/p&gt;

&lt;p&gt;lngester组件主要是接受Distributor发来的数据，然后发送到后段的数据库存储&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ruler&lt;/p&gt;

&lt;p&gt;ruler组件主要是负责处理alertmanager产生的告警&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Query frontend&lt;/p&gt;

&lt;p&gt;Query frontend组件主要是接受http请求，把他们按着tenant ID排列，并且重试一些返回错误的请求，比如large query&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Querier&lt;/p&gt;

&lt;p&gt;Querier组件主要是处理promql&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Chunk store&lt;/p&gt;

&lt;p&gt;Chunk store组件就是长期存储&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;p&gt;就是我们常用的集群架构：聚合，将所有数据都发送到一个节点，用于存储+查询&lt;/p&gt;

&lt;h1 id=&#34;安装使用&#34;&gt;安装使用&lt;/h1&gt;

&lt;p&gt;编译启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build ./cmd/cortex
$ ./cortex -config.file=./docs/single-process-config.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置prometheus的远程写，将prometheus数据写入到cortex中去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;remote_write:
- url: http://localhost:9009/api/prom/push
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup /opt/promes/cortex/cortex -config.file=/opt/promes/cortex/config/single-process-config.yaml -distributor.ingestion-rate-limit=100000 -ring.store=consul -consul.hostname=10.47.182.224:9996 -distributor.replication-factor=2 &amp;gt;&amp;gt;/opt/promes/cortex/logs/start.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;-distributor.ingestion-rate-limit=100000：限制数据量为100000，其实达不到这个量，prometheus默认remote_write的10000个并发，每个包含100个数据，这个时候会大量出错，所以在写入性能上达不到这个量，测试最大每个包含25个数据可以处理，同样的机器上victoria-metrics可以达到10000个数据而不出错。&lt;/li&gt;
&lt;li&gt;-ring.store=consul -consul.hostname=10.47.182.224:9996：一个令牌存储在consul上，用我们现有的consul&lt;/li&gt;
&lt;li&gt;-distributor.replication-factor=2：集群节点的数量，这边主要是高可用，两个节点互相复制，完成一致性哈希&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前没有看到cortex的可扩展的优秀的方面，可能是社区开发还没有完成，等release。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- M3db</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/</link>
          <pubDate>Wed, 13 Mar 2019 17:13:10 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/</guid>
          <description>&lt;p&gt;Uber开发了指标平台M3和分布式时间序列数据库M3DB。来解决Uber在发展过程当中遇到的问题：使用开源软件后，因为可靠性，成本等问题，在操做密集型方面没法大规模使用这些开源软件。因此Uber逐步构建了本身的指标平台。咱们利用经验来帮助咱们构建本地分布式时间序列数据库，高度动态和高性能的聚合服务，查询引擎以及其余支持基础架构。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;M3包括了以下的组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;M3DB &amp;ndash; M3db是一个使用TSDB（时间数据库），保存全部Prometheus指标，M3db是分布式，高可用性和复制的数据库，它使用Etcd做为共识算法。&lt;/li&gt;
&lt;li&gt;M3Coordinator &amp;ndash; 是Prometheus实例与M3db之间的适配器，它公开了Prometheus用来从数据库中推送和提取数据的读/写端点。&lt;/li&gt;
&lt;li&gt;M3Query &amp;ndash; 众所周知，Prometheus努力处理显示大量数据的查询，而不是从Prometheus提取数据，M3Query实现了相同的PromQL并能够响应此类请求。&lt;/li&gt;
&lt;li&gt;M3Aggregator &amp;ndash; 可选但很重要，此服务将下降指标的采样率，为长期存储作好准备。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总体架构图以下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/m3/m3&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关于M3，咱们目前积累了一些生产实践。目前的问题是，社区不够活跃，文档也不够丰富。不少时候遇到问题，只能去研究代码。M3query对PromSql支持的不够，因此M3query并不能生产环境使用。&lt;/p&gt;

&lt;h1 id=&#34;调研&#34;&gt;调研&lt;/h1&gt;

&lt;p&gt;首先，我们想把大量的数据存储到m3中，给prometheus进行查询告警。但是数据量很大，m3db数据插入性能是有必要进行保证的。&lt;/p&gt;

&lt;p&gt;之前写过一个adapter，将数据转化为json调用json api将数据已经插入了m3db中，当时对数据性能没有要求，这次使用json api进行压测的时候，发现性能很差，而且在数据并发达到100个goroutine，一个goroutine发送100条数据，m3db就崩溃了，不接受连接。然后简单的测试了一下得到以下的数据&lt;/p&gt;

&lt;p&gt;这个远远达不到要求啊，于是看看有没有批量操作的接口，官方文档说明，有两种方式插入数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Test RPC
To test out some of the functionality of M3DB there are some user friendly HTTP JSON APIs that you can use. These use the DB node cluster service endpoints.

Note: performance sensitive users are expected to use the more performant endpoints via either the Go src/dbnode/client/Session API, or the GRPC endpoints exposed via src/coordinator.
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;go api（src/dbnode/client/Session），session看代码使用的是apache的thrift rpc。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GRPC（src/coordinator）&lt;/p&gt;

&lt;p&gt;官方提供了benchmark（src/query/benchmark），于是去编译进行测试，但是m3开源的太差了，很多第三方库都是使用的老版本，兼容性很差，各种api对不上，也不把自己的vendor包一同开源，踩了许多坑：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;github.com/thrift &amp;mdash;0.10.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/uber-go/tally&amp;mdash;3.3.7&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;client_golang&amp;mdash;0.8.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/coreos/etcd&amp;mdash;&amp;ndash;3.2.0，还是缺少参数，坑&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;google.golang.org/grpc&amp;ndash;laster&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;golang.org/x/text&amp;mdash;&amp;mdash;laster&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/satori/go.uuid&amp;mdash;-1.2.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/couchbase/vellum&amp;mdash;&amp;ndash;master&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;github.com/pilosa/pilosa-最新班都缺少参数&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;还是没有编译完成，后续持续跟进，看看官方有没有继续开源和改进。&lt;/p&gt;

&lt;p&gt;目前得到以下结论&lt;/p&gt;

&lt;p&gt;m3db目前没有发现批量处理的方式&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Coordinator的方式最后还是一条一条的发送（通过查看代码，未能运行）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;session方式，其中一个包github.com/pilosa/pilosa/roaring目前最新版本都没有m3中使用的参数，最终无法编译使用。网上使用session运行成功的，我未能找到他使用了什么版本的github.com/pilosa/pilosa/roaring包。但是通过他运行的结果来看（结合代码api），也是一条一条发送的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;m3中很多都是老版本的库，兼容性很差，api很多不兼容，官方也未推出他使用了什么库，如果继续，应该需要大量的时间去校验和编译&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Map</title>
          <link>https://kingjcy.github.io/post/golang/go-map/</link>
          <pubDate>Thu, 28 Feb 2019 17:42:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-map/</guid>
          <description>&lt;p&gt;map是我们经常使用的一种数据结构，也是很重要的一种数据结构，我们来详细的了解一下map。&lt;/p&gt;

&lt;h1 id=&#34;map&#34;&gt;map&lt;/h1&gt;

&lt;p&gt;map就是k/v的映射，map持有对底层数据结构的引用。如果将map传递给函数，其对map的内容做了改变，则这些改变对于调用者是可见的。&lt;/p&gt;

&lt;h2 id=&#34;map的实现&#34;&gt;map的实现&lt;/h2&gt;

&lt;p&gt;所有的Map底层一般都是使用数组+链表的hashmap来实现，会借用哈希算法辅助。对于给定的 key，一般先进行 hash 操作，然后相对哈希表的长度取模，将 key 映射到指定的链表中。&lt;/p&gt;

&lt;p&gt;Golang的map正常是使用哈希表作为底层实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;hashtable是一种根据key，key在通过hash函数来对应的数组的数据。&lt;/li&gt;
&lt;li&gt;hashmap是hashtable使用拉链法实现的一种方式，数组+链表的实现方式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;正常情况下，golang使用数组+桶（buckets）来实现，默认每个桶的数量是8个，在超出8个的情况下，新增一个桶，使用链表连接起来，这是时候就变成了hash数组 + 桶 + 溢出的桶链表了。&lt;/p&gt;

&lt;p&gt;其实map不是并发安全的，也是因为hashmap不是并发安全的，实现并发安全的几种方式，可以参考&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/&#34;&gt;并发安全&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;数据结构&#34;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;map数据结构由runtime/map.go/hmap定义:&lt;/p&gt;

&lt;p&gt;hmap&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/map.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type hmap struct {
    count     int
    flags     uint8
    B         uint8
    noverflow uint16
    hash0     uint32

    buckets    unsafe.Pointer
    oldbuckets unsafe.Pointer
    nevacuate  uintptr

    extra *mapextra
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;count 表示当前哈希表中的元素数量；类似于&lt;code&gt;buckets[count]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;B表示当前哈希表持有的buckets数量，但是因为哈希表中桶的数量都2的倍数，所以该字段会存储对数，也就是 len(buckets) == 2^B；类似于&lt;code&gt;buckets[2^B]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;hash0 是哈希的种子，它能为哈希函数的结果引入随机性，这个值在创建哈希表时确定，并在调用哈希函数时作为参数传入；&lt;/li&gt;
&lt;li&gt;oldbuckets 是哈希在扩容时用于保存之前 buckets 的字段，它的大小是当前 buckets 的一半；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;bmap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type bmap struct {
    tophash [8]uint8 //存储哈希值的高8位
    //data和overflow并不是在结构体中显示定义的，而是直接通过指针运算进行访问的。
    data    byte[1]  //key value数据:key/key/key/.../value/value/value...
    overflow *bmap   //溢出bucket的地址
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;tophash是个长度为8的数组，哈希值相同的键（准确的说是哈希值低位相同的键）存入当前bucket时会将哈希值的高位存储在该数组中，以方便后续匹配。&lt;/li&gt;
&lt;li&gt;data区存放的是key-value数据，存放顺序是key/key/key/&amp;hellip;value/value/value，如此存放是为了节省字节对齐带来的空间浪费。&lt;/li&gt;
&lt;li&gt;overflow 指针指向的是下一个bucket，据此将所有冲突的键连接起来。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我们能根据编译期间的 cmd/compile/internal/gc.bmap 函数对它的结构重建：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type bmap struct {
    topbits  [8]uint8
    keys     [8]keytype
    values   [8]valuetype
    pad      uintptr
    overflow uintptr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hmap.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hmap2.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://kingjcy.github.io/media/golang/go/hmap3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;基本机制&#34;&gt;基本机制&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;负载因子和rehash&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;负载因子用于衡量一个哈希表冲突情况，公式为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;负载因子 = 键数量/bucket数量
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如，对于一个bucket数量为4，包含4个键值对的哈希表来说，这个哈希表的负载因子为1.&lt;/p&gt;

&lt;p&gt;哈希表需要将负载因子控制在合适的大小，超过其阀值需要进行rehash，也即键值对重新组织：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;哈希因子过小，说明空间利用率低&lt;/li&gt;
&lt;li&gt;哈希因子过大，说明冲突严重，存取效率低&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个哈希表的实现对负载因子容忍程度不同，比如Redis实现中负载因子大于1时就会触发rehash，而Go则在在负载因子达到6.5时才会触发rehash，因为Redis的每个bucket只能存1个键值对，而Go的bucket可能存8个键值对，所以Go可以容忍更高的负载因子。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map的删除机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;采用的是惰性删除的策略，打上一个empty的标记，实际上并没有删除，也不会释放内存，还可以在后面进行复用，这样做主要为了解决遍历过程的溢出问题，因为是用数组实现的。实现迭代安全&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map的扩容机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;增量扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当负载因子大于6.5也即平均每个bucket存储的键值对达到6.5个，就新建一个bucket，新的bucket长度是原来的2倍，然后旧bucket数据搬迁到新的bucket。&lt;/p&gt;

&lt;p&gt;考虑到如果map存储了数以亿计的key-value，一次性搬迁将会造成比较大的延时，Go采用逐步搬迁策略，即每次访问map时都会触发一次搬迁，每次搬迁2个键值对。&lt;/p&gt;

&lt;p&gt;扩容流程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/dilatation.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当前map存储了7个键值对，只有1个bucket。此地负载因子为7。再次插入数据时将会触发扩容操作，扩容之后再将新插入键写入新的bucket。&lt;/p&gt;

&lt;p&gt;当第8个键值对插入时，将会触发扩容，扩容后示意图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/dilatation2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;hmap数据结构中oldbuckets成员指身原bucket，而buckets指向了新申请的bucket。新的键值对被插入新的bucket中。 后续对map的访问操作会触发迁移，将oldbuckets中的键值对逐步的搬迁过来。当oldbuckets中的键值对全部搬迁完毕后，删除oldbuckets。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/dilatation3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;等量扩容&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实际上并不是扩大容量，buckets数量不变，重新做一遍类似增量扩容的搬迁动作，把松散的键值对重新排列一次，以使bucket的使用率更高，进而保证更快的存取。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map的缩容机制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;溢出的桶数量noverflow&amp;gt;=32768(1&amp;lt;&lt;15)或者&gt;=hash数组大小。&lt;/p&gt;

&lt;p&gt;但是缩容并不会释放已经占用的空间，真的要释放空间，就新建一个map进行迁移&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;什么时候转化为红黑树&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang中没有转化为红黑树，就是正常的扩容使用数组，在java中超过8个桶，就会转化为红黑树，和查询次数也即是时间复杂度有关，因为Map中桶的元素初始化是数组保存的，其查找性能是O(n)，而树结构能将查找性能提升到O(log(n))。当数组长度很小的时候，即使遍历，速度也非常快，但是当链表长度不断变长，肯定会对查询性能有一定的影响，所以才需要转成树。有利于减少查询的次数&lt;/p&gt;

&lt;p&gt;8个桶这个其实是概率统计出来的，8最合适。golang中的桶有8个kv应该是也是这个道理。&lt;/p&gt;

&lt;h1 id=&#34;map并发安全&#34;&gt;map并发安全&lt;/h1&gt;

&lt;p&gt;Go 原生的 map 数据类型是非并发安全的，在go1.9开始发布了sync.map是线程安全的。&lt;/p&gt;

&lt;p&gt;我们先看看基于原生map的基础上加mutex实现并发安全。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;map+mutex&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang1.9以前，我们都是使用读写锁（sync.RWMutex）来实现并发安全，比如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package beego

import (
    &amp;quot;sync&amp;quot;
)

type BeeMap struct {
    lock *sync.RWMutex
    bm   map[interface{}]interface{}

}

func NewBeeMap() *BeeMap {
    return &amp;amp;BeeMap{
        lock: new(sync.RWMutex),
        bm:   make(map[interface{}]interface{}),
    }
}

//Get from maps return the k&#39;s value
func (m *BeeMap) Get(k interface{}) interface{} {
    m.lock.RLock()
    defer m.lock.RUnlock()
    if val, ok := m.bm[k]; ok {
        return val
    }
    return nil
}

// Maps the given key and value. Returns false
// if the key is already in the map and changes nothing.
func (m *BeeMap) Set(k interface{}, v interface{}) bool {
    m.lock.Lock()
    defer m.lock.Unlock()
    if val, ok := m.bm[k]; !ok {
        m.bm[k] = v
    } else if val != v {
        m.bm[k] = v
    } else {
        return false
    }
    return true
}

// Returns true if k is exist in the map.
func (m *BeeMap) Check(k interface{}) bool {
    m.lock.RLock()
    defer m.lock.RUnlock()
    if _, ok := m.bm[k]; !ok {
        return false
    }
    return true
}

func (m *BeeMap) Delete(k interface{}) {
    m.lock.Lock()
    defer m.lock.Unlock()
    delete(m.bm, k)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在项目中经常使用的方式：通过数组、map、sync.RWMutex来实现原生map的并发读写（采用map数组，把key hash到相应的map，每个map单独加锁以降低锁的粒度）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;sync.map&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;那么golang sync.map是如何实现并发安全的呢？&lt;/p&gt;

&lt;p&gt;简单总结就是使用了用空间换时间（多存储一份map作为缓存，减少锁的使用）的思想来实现来一个高效的并发安全。主要下面的函数接口实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Load：读取指定 key 返回 value&lt;/li&gt;
&lt;li&gt;Store： 存储（增或改）key-value&lt;/li&gt;
&lt;li&gt;Delete： 删除指定 key&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4种操作：读key、增加key、更新key、删除key的基本流程，其实在代码中只有上面三个函数实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;读key：先到read中读取，如果有则直接返回结果，如果没有就加锁，再次检查read是否存在，因为刚刚是无锁读，如果没有，则到dirty加锁中读取，如果有返回结果并更新miss数（用于数据迁移），这边在read这边设置了一个amended，如果为true则表明当前read只读map的数据不完整，dirty map中包含部分数据，这边决定了刚刚上一步是否执行到dirty中查找&lt;/li&gt;
&lt;li&gt;更新key（增加key）：其实更新和增加使用的是同一个函数store，首先查找read中是否存在，如果存在直接更新，如果没有就加锁，再次检查read是否存在，因为刚刚是无锁读，如果不存在，同样有上面的判断是否是全量数据，不是就继续到dirty中查找，找到了就更新，找不到就新建一个存储，就是新增key&lt;/li&gt;
&lt;li&gt;删除key：先到read中看看有没有，如果有p标记为nil，如果没有则到dirty中直接删除（同样有数据全不全的判断）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;源码详解解读&#34;&gt;源码详解解读&lt;/h2&gt;

&lt;h3 id=&#34;数据结构分析&#34;&gt;数据结构分析&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Map&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Map struct {
    mu Mutex
    // read是一个readOnly的指针，里面包含了一个map结构，就是我们说的只读map对该map的元素的访问
    // 不需要加锁，只需要通过atomic加载最新的指针即可
    read atomic.Value // readOnly

    // dirty包含部分map的键值对，如果要访问需要进行mutex获取
    // 最终dirty中的元素会被全部提升到read里面的map中
    dirty map[interface{}]*entry

   // misses是一个计数器用于记录从read中没有加载到数据
    // 尝试从dirty中进行获取的次数，从而决定将数据从dirty迁移到read的时机
    misses int
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;readOnly&lt;/p&gt;

&lt;p&gt;只读map,对该map元素的访问不需要加锁，但是该map也不会进行元素的增加，元素会被先添加到dirty中然后后续再转移到read只读map中，通过atomic原子操作不需要进行锁操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type readOnly struct {
    // m包含所有只读数据，不会进行任何的数据增加和删除操作
    // 但是可以修改entry的指针因为这个不会导致map的元素移动
    m       map[interface{}]*entry
    // 标志位，如果为true则表明当前read只读map的数据不完整，dirty map中包含部分数据
    amended bool
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;entry&lt;/p&gt;

&lt;p&gt;entry是sync.Map中值得指针，如果当p指针指向expunged这个指针的时候，则表明该元素被删除，但不会立即从map中删除，如果在未删除之前又重新赋值则会重用该元素&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type entry struct {
    // 指向元素实际值得指针
    p unsafe.Pointer // *interface{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;p 有三种状态：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;p == nil: 键值已经被删除，且 m.dirty == nil&lt;/li&gt;
&lt;li&gt;p == expunged: 键值已经被删除，但 m.dirty!=nil 且 m.dirty 不存在该键值（expunged 实际是空接口指针）&lt;/li&gt;
&lt;li&gt;键值对存在，存在于 m.read.m 中，如果 m.dirty!=nil 则也存在于 m.dirty&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;数据存储&#34;&gt;数据存储&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/date-store.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;元素存在read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;元素如果存储在只读map中，则只需要获取entry元素，然后修改其p的指针指向新的元素就可以了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    read, _ := m.read.Load().(readOnly)
    if e, ok := read.m[key]; ok &amp;amp;&amp;amp; e.tryStore(&amp;amp;value) {
        return
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边其实有两点需要特别说明的&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;map不是并发安全的，这边为什么可以直接修改呢，因为这边使用atomic中的CAS的乐观思想来实现乐观锁，实现并发安全。&lt;/li&gt;
&lt;li&gt;cas就是实现了对比并交互，这个操作是用来实现乐观锁这种思路的，乐观锁并不是真正的锁，它用版本好来标记数据是否被修改，如果被修改则重试。&lt;/li&gt;
&lt;li&gt;怎么同步到dirty中去，entry是一个指向实际值的地址，所以read和dirty是共享地址的。所以就一起改了&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;元素存在dirty中&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果元素存在dirty中其实同read map逻辑一样，只需要修改对应元素的指针即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;} else if e, ok := m.dirty[key]; ok {
    // 如果已经在dirty中就会直接存储
    e.storeLocked(&amp;amp;value)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边有两点需要说明&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;read中是有一个标识amended来判断，如果为true则表明当前read只读map的数据不完整，dirty map中包含部分数据,才会进来查询。&lt;/li&gt;
&lt;li&gt;在加锁之后，还是需要重read中查询一次的，查到就直接修改，因为是一开始的查询是无锁读，存在并发安全问题，可能在这段未加锁的时间内数据发生了改变。&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;元素不存在&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果元素之前不存在当前Map中则需要先将其存储在dirty map中，同时将amended标识为true,即当前read中的数据不全，有一部分数据存储在dirty中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 如果当前不是在修正状态
if !read.amended {
    // 新加入的key会先被添加到dirty map中， 并进行read标记为不完整
    // 如果dirty为空则将read中的所有没有被删除的数据都迁移到dirty中
    m.dirtyLocked()
    m.read.Store(readOnly{m: read.m, amended: true})
}
m.dirty[key] = newEntry(value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) Store(key, value interface{}) {
    read, _ := m.read.Load().(readOnly)
    // 如果 read 里存在，则尝试存到 entry 里
    if e, ok := read.m[key]; ok &amp;amp;&amp;amp; e.tryStore(&amp;amp;value) {
        return
    }

    // 如果上一步没执行成功，则要分情况处理
    m.mu.Lock()
    read, _ = m.read.Load().(readOnly)
    // 和 Load 一样，重新从 read 获取一次
    if e, ok := read.m[key]; ok {
        // 情况 1：read 里存在
        if e.unexpungeLocked() {
            // 如果 p == expunged，则需要先将 entry 赋值给 dirty（因为 expunged 数据不会留在 dirty）
            m.dirty[key] = e
        }
        // 用值更新 entry
        e.storeLocked(&amp;amp;value)
    } else if e, ok := m.dirty[key]; ok {
        // 情况 2：read 里不存在，但 dirty 里存在，则用值更新 entry
        e.storeLocked(&amp;amp;value)
    } else {
        // 情况 3：read 和 dirty 里都不存在
        if !read.amended {
            // 如果 amended == false，则调用 dirtyLocked 将 read 拷贝到 dirty（除了被标记删除的数据）
            m.dirtyLocked()
            // 然后将 amended 改为 true
            m.read.Store(readOnly{m: read.m, amended: true})
        }
        // 将新的键值存入 dirty
        m.dirty[key] = newEntry(value)
    }
    m.mu.Unlock()
}

func (e *entry) tryStore(i *interface{}) bool {
    for {
        p := atomic.LoadPointer(&amp;amp;e.p)
        if p == expunged {
            return false
        }
        //原子操作cas
        if atomic.CompareAndSwapPointer(&amp;amp;e.p, p, unsafe.Pointer(i)) {
            return true
        }
    }
}

func (e *entry) unexpungeLocked() (wasExpunged bool) {
    return atomic.CompareAndSwapPointer(&amp;amp;e.p, expunged, nil)
}

func (e *entry) storeLocked(i *interface{}) {
    atomic.StorePointer(&amp;amp;e.p, unsafe.Pointer(i))
}

func (m *Map) dirtyLocked() {
    if m.dirty != nil {
        return
    }

    read, _ := m.read.Load().(readOnly)
    m.dirty = make(map[interface{}]*entry, len(read.m))
    for k, e := range read.m {
        // 判断 entry 是否被删除，否则就存到 dirty 中
        if !e.tryExpungeLocked() {
            m.dirty[k] = e
        }
    }
}

func (e *entry) tryExpungeLocked() (isExpunged bool) {
    p := atomic.LoadPointer(&amp;amp;e.p)
    for p == nil {
        // 如果有 p == nil（即键值对被 delete），则会在这个时机被置为 expunged
        if atomic.CompareAndSwapPointer(&amp;amp;e.p, nil, expunged) {
            return true
        }
        p = atomic.LoadPointer(&amp;amp;e.p)
    }
    return p == expunged
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在数据存储里其实还有数据迁移的逻辑&lt;/p&gt;

&lt;p&gt;当read多次都没有命中数据，达到阈值，表示这个cache命中率太低，这时直接将整个read用dirty替换掉，然后dirty又重新置为nil，下一次再添加一个新key的时候，会触发一次read到dirty的复制，这样二者又保持了一致。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-migration.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在刚初始化和将所有元素迁移到read中后，dirty默认都是nil元素，而此时如果有新的元素增加，则需要先将read map中的所有未删除数据先迁移到dirty中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) dirtyLocked() {
    if m.dirty != nil {
        return
    }

    read, _ := m.read.Load().(readOnly)
    m.dirty = make(map[interface{}]*entry, len(read.m))
    for k, e := range read.m {
        if !e.tryExpungeLocked() {
            m.dirty[k] = e
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-migration2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当持续的从read访问穿透到dirty中后，也就是上面的miss值大于dirty存储数据的长度，就会触发一次从dirty到read的迁移，这也意味着如果我们的元素读写比差比较小，其实就会导致频繁的迁移操作，性能其实可能并不如rwmutex等实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) missLocked() {
    m.misses++
    if m.misses &amp;lt; len(m.dirty) {
        return
    }
    m.read.Store(readOnly{m: m.dirty})
    m.dirty = nil
    m.misses = 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据查询&#34;&gt;数据查询&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-query.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关于amended标识的使用，和存储是一样的&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;数据在read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Load数据的时候回先从read中获取，如果此时发现元素，则直接返回即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;read, _ := m.read.Load().(readOnly)
e, ok := read.m[key]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;数据在dirty&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加锁后会尝试从read和dirty中读取，同时进行misses计数器的递增，如果满足迁移条件则会进行数据迁移&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;read, _ = m.read.Load().(readOnly)
e, ok = read.m[key]
if !ok &amp;amp;&amp;amp; read.amended {
    e, ok = m.dirty[key]
    // 这里将采取缓慢迁移的策略
    // 只有当misses计数==len(m.dirty)的时候，才会将dirty里面的数据全部晋升到read中
    m.missLocked()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) Load(key interface{}) (value interface{}, ok bool) {
    // 首先尝试从 read 中读取 readOnly 对象
    read, _ := m.read.Load().(readOnly)
    e, ok := read.m[key]

    // 如果不存在则尝试从 dirty 中获取
    if !ok &amp;amp;&amp;amp; read.amended {
        m.mu.Lock()
        // 由于上面 read 获取没有加锁，为了安全再检查一次
        read, _ = m.read.Load().(readOnly)
        e, ok = read.m[key]

        // 确实不存在则从 dirty 获取
        if !ok &amp;amp;&amp;amp; read.amended {
            e, ok = m.dirty[key]
            // 调用 miss 的逻辑
            m.missLocked()
        }
        m.mu.Unlock()
    }

    if !ok {
        return nil, false
    }
    // 从 entry.p 读取值
    return e.load()
}

func (m *Map) missLocked() {
    m.misses++
    if m.misses &amp;lt; len(m.dirty) {
        return
    }
    // 当 miss 积累过多，会将 dirty 存入 read，然后 将 amended = false，且 m.dirty = nil
    m.read.Store(readOnly{m: m.dirty})
    m.dirty = nil
    m.misses = 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据删除&#34;&gt;数据删除&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/map/data-query.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果数据在read中，则就直接修改entry的标志位指向删除的指针即可，如果当前read中数据不全，则需要进行dirty里面的元素删除尝试，&lt;/li&gt;
&lt;li&gt;如果存在就直接从dirty中删除即可&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;源码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Map) Delete(key interface{}) {
    m.LoadAndDelete(key)
}

// LoadAndDelete 作用等同于 Delete，并且会返回值与是否存在
func (m *Map) LoadAndDelete(key interface{}) (value interface{}, loaded bool) {
    // 获取逻辑和 Load 类似，read 不存在则查询 dirty
    read, _ := m.read.Load().(readOnly)
    e, ok := read.m[key]
    if !ok &amp;amp;&amp;amp; read.amended {
        m.mu.Lock()
        read, _ = m.read.Load().(readOnly)
        e, ok = read.m[key]
        if !ok &amp;amp;&amp;amp; read.amended {
            e, ok = m.dirty[key]
            m.missLocked()
        }
        m.mu.Unlock()
    }
    // 查询到 entry 后执行删除
    if ok {
        // 将 entry.p 标记为 nil，数据并没有实际删除
        // 真正删除数据并被被置为 expunged，是在 Store 的 tryExpungeLocked 中
        return e.delete()
    }
    return nil, false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;设计思想&#34;&gt;设计思想&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;自动扩缩容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在Mutex和RWMutex实现的并发安全的map中map随着时间和元素数量的增加、删除，容量会不断的递增，在某些情况下比如在某个时间点频繁的进行大量数据的增加，然后又大量的删除，其map的容量并不会随着元素的删除而缩小，而在sync.Map中，当进行元素从dirty进行提升到read map的时候会进行重建，可以实现自动扩缩容。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;读写分离&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;并发访问map读的主要问题其实是在扩容的时候，可能会导致元素被hash到其他的地址，那如果我的读的map不会进行扩容操作，就可以进行并发安全的访问了，而sync.map里面正是采用了这种方式，对增加元素通过dirty来进行保存&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;无锁读&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过read只读和dirty写map将操作分离，其实就只需要通过原子指令对read map来进行读操作而不需要加锁了，从而提高读的性能&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;写加锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面提到增加元素操作可能会先增加大dirty写map中，那针对多个goroutine同时写，其实就需要进行Mutex加锁了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;延迟提升&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面提到了read只读map和dirty写map, 那就会有个问题，默认增加元素都放在dirty中，那后续访问新的元素如果都通过 mutex加锁，那read只读map就失去意义，sync.Map中采用一直延迟提升的策略，进行批量将当前map中的所有元素都提升到read只读map中从而为后续的读访问提供无锁支持&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;惰性删除&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;惰性删除是并发设计中一中常见的设计，比如删除某个个链表元素，如果要删除则需要修改前后元素的指针，而采用惰性删除，则通常只需要给某个标志位设定为删除，然后在后续修改中再进行操作，sync.Map中也采用这种方式，通过给指针指向某个标识删除的指针，从而实现惰性删除&lt;/p&gt;

&lt;p&gt;我觉得最重要的就是实现了读写分离，加锁分离，从而实现了空间换取时间的快速处理。read相当于dirty的缓存，read是原子操作，不需要加锁，快速，dirty可以延迟提升，就和缓存数据库做缓存是一个道理，包括热点数据的提升。&lt;/p&gt;

&lt;h2 id=&#34;应用场景&#34;&gt;应用场景&lt;/h2&gt;

&lt;p&gt;由以上方法可得知，无论是读操作，还是更新操作，亦或者删除操作，都会先从read进行操作，因为read的读取更新不需要锁，是原子操作，这样既做到了并发安全，又做到了尽量减少锁的争用，虽然采用的是空间换时间的策略，通过两个冗余的map，实现了这一点，但是底层存的都是指针类型，所以对于空间占用，也是做到了最大程度的优化。
但是同时也可以得知，当存在大量写操作时，会导致read中读不到数据，依然会频繁加锁，同时dirty升级为read，整体性能就会很低，所以sync.Map更加适合大量读、少量写的场景。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Sync</title>
          <link>https://kingjcy.github.io/post/golang/go-sync/</link>
          <pubDate>Thu, 28 Feb 2019 17:42:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-sync/</guid>
          <description>&lt;p&gt;sync包提供了基本的同步基元，如锁，WaitGroup、Once 和 Cond等同步原语。除了Once和WaitGroup类型，大部分都是适用于普通程序线程，大型并发同步使用channel通信（csp）更好一些。&lt;/p&gt;

&lt;h1 id=&#34;sync&#34;&gt;sync&lt;/h1&gt;

&lt;p&gt;sync同步功能主要提供了once，mutex，cond，并发安全map，安全并发pool，waitgroup。&lt;/p&gt;

&lt;h2 id=&#34;sync-once&#34;&gt;sync.Once&lt;/h2&gt;

&lt;p&gt;sync.Once是一个简单而强大的原语，可确保一个函数仅执行一次。&lt;/p&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Once struct {
        // 非暴露字段
}

func (o *Once) Do(f func())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用前先定义 Once 类型变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var once Once
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用的时候向 Once 类型变量传入函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;once.Do(func() { init() })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;多次调用 once.Do(f) 只会触发一次 f 的执行，即第一次 f 的执行。&lt;/p&gt;

&lt;p&gt;用法实例&lt;/p&gt;

&lt;p&gt;某些操作只需要执行一次（比如一些初始化动作），这时就可使用 Once，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    var once sync.Once
    onceBody := func() {
        fmt.Println(&amp;quot;Only once&amp;quot;)
    }
    done := make(chan bool)

    // 创建 10 个 goroutine，但是 onceBody 只会执行 1 次
    for i := 0; i &amp;lt; 10; i++ {
        go func() {
            once.Do(onceBody)
            done &amp;lt;- true
        }()
    }

    // 等待 10 个 goroutine 结束
    for i := 0; i &amp;lt; 10; i++ {
        &amp;lt;-done
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实 Once 的实现非常简单，就是互斥锁+原子变量&lt;/p&gt;

&lt;h2 id=&#34;sync-waitgroup&#34;&gt;sync.WaitGroup&lt;/h2&gt;

&lt;p&gt;sync.WaitGroup拥有一个内部计数器。当计数器等于0时，则Wait()方法会立即返回。否则它将阻塞执行Wait()方法的goroutine直到计数器等于0时为止。&lt;/p&gt;

&lt;p&gt;WaitGroup 的使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type WaitGroup struct {
    // 包含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WaitGroup用于等待一组线程的结束。父线程调用Add方法来设定应等待的线程的数量。每个被等待的线程在结束时应调用Done方法。同时，主线程里可以调用Wait方法阻塞至所有线程结束。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*WaitGroup) Add
func (wg *WaitGroup) Add(delta int)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add方法向内部计数加上delta，delta可以是负数；如果内部计数器变为0，Wait方法阻塞等待的所有线程都会释放，如果计数器小于0，方法panic。注意Add加上正数的调用应在Wait之前，否则Wait可能只会等待很少的线程。一般来说本方法应在创建新的线程或者其他应等待的事件之前调用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*WaitGroup) Done
func (wg *WaitGroup) Done()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done方法减少WaitGroup计数器的值，应在线程的最后执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*WaitGroup) Wait
func (wg *WaitGroup) Wait()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait方法阻塞直到WaitGroup计数器减为0。&lt;/p&gt;

&lt;h2 id=&#34;sync-pool&#34;&gt;sync.Pool&lt;/h2&gt;

&lt;p&gt;sync.Pool是一个并发池，负责安全地保存一组对象。&lt;/p&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Pool struct {
        // 当 Get() 找不到一个对象时，会使用 New() 生成一个对象
        New func() interface{}

        // 剩下的是非暴露字段
}

// 任意从 pool 中挑选一个对象返回给客户端，如果找不到就使用 p.New 生成
func (p *pool) Get() interface{}

// 将对象 x 放回到 pool 中
func (p *pool) Put(x interface{})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 Pool 可以用以管理一些临时对象供多个 package 的客户端使用，客户端对 Pool 的逻辑是无感知的：需要的时候 Get，不需要的时候 Put，而且 Pool 可根据当前负载自动调整对象池的大小。&lt;/p&gt;

&lt;p&gt;一个典型的应用是日志，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var bufPool = sync.Pool{
    New: func() interface{} {
        return new(bytes.Buffer)
    },
}

func Log(w io.Writer, key, val string) {
    // 从对象池中获取 buffer
    b := bufPool.Get().(*bytes.Buffer)
    b.Reset()
    b.WriteString(time.Now().Format(time.RFC3339))
    b.WriteByte(&#39; &#39;)
    b.WriteString(key)
    b.WriteByte(&#39;=&#39;)
    b.WriteString(val)
    w.Write(b.Bytes())
    // 使用完毕，归还 buffer
    bufPool.Put(b)
}

func main() {
    Log(os.Stdout, &amp;quot;path&amp;quot;, &amp;quot;/search?q=flowers&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么什么时候使用sync.Pool？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一个是当我们必须重用共享的和长期存在的对象（例如，数据库连接）时。&lt;/li&gt;
&lt;li&gt;第二个是用于优化内存分配。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sync-mutex&#34;&gt;sync.Mutex&lt;/h2&gt;

&lt;p&gt;sync.Mutex可能是sync包中使用最广泛的原语。它允许在共享资源上互斥访问（不能同时访问），&lt;a href=&#34;https://kingjcy.github.io/post/architecture/concurrencesafe/#如何做到并发安全&#34;&gt;锁&lt;/a&gt;在并发安全中有着很重要的作用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mutex := &amp;amp;sync.Mutex{}

mutex.Lock()
// Update共享变量 (比如切片，结构体指针等)
mutex.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sync.RWMutex是一个读写互斥锁，它提供了我们上面的刚刚看到的sync.Mutex的Lock和UnLock方法（因为这两个结构都实现了sync.Locker接口）。但是，它还允许使用RLock和RUnlock方法进行并发读取：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mutex := &amp;amp;sync.RWMutex{}

mutex.Lock()
// Update 共享变量
mutex.Unlock()

mutex.RLock()
// Read 共享变量
mutex.RUnlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只有在频繁读取和不频繁写入的场景里，才应该使用sync.RWMutex。&lt;/p&gt;

&lt;h2 id=&#34;sync-cond&#34;&gt;sync.Cond&lt;/h2&gt;

&lt;p&gt;sync.Cond可能是sync包提供的同步原语中最不常用的一个，它用于发出信号（一对一）或广播信号（一对多）到goroutine。&lt;/p&gt;

&lt;p&gt;条件变量做的事情很简单：让多个 goroutine 等待在某个条件上，如果条件不满足，进入等待状态；如果条件满足，继续运行。&lt;/p&gt;

&lt;p&gt;Cond 内部维护着一个 notifyList，当条件不满足的时候，则将对应的 goroutine 添加到列表上然后进入等待状态。当条件满足时，一般会有其他执行者显式使用 Signal() 或者 Broadcast() 去唤醒 notifyList 上 goroutine。&lt;/p&gt;

&lt;p&gt;当进行条件的判断时，必须使用互斥锁来保证条件的安全，即在判断的时候条件没有被其他人修改。所以 Cond 一般会与一个符合 Lock 接口的 Mutex 一起使用。&lt;/p&gt;

&lt;p&gt;定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Cond struct {
    // 读写条件状态需要加锁
    L Locker
    // 剩下的是非暴露字段
}

func NewCond(l Locker) *Cond

// 广播所以等待的 goroutine，条件已经满足
func (c *Cond) Broadcast()

// 单播其中一个等待的 goroutine，条件已经满足
func (c *Cond) Signal()

// 如果条件不满足，调用 Wait() 进入等待状态
func (c *Cond) Wait()
此处要特别小心 Wait() 的使用。正如前文所说，条件的判断需要使用互斥锁来确保条件读取前后是一致的，即：

    c.L.Lock() // 进行条件判断，加锁
    if !condition() { // 如果不满足条件，进入 if 中
        c.Wait() // Wait() 内部会自动解锁
    }

    ... 这里可能会对 condition 作出改变 ...
    c.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述代码其实还有一个很严重的问题，为了说明这个问题，让我们来看看 Wait() 的实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cond) Wait() {
    c.checker.check()
    t := runtime_notifyListAdd(&amp;amp;c.notify) // 加入 notifyList
    c.L.Unlock() // 解锁
    runtime_notifyListWait(&amp;amp;c.notify, t) // 进入等待模式
    c.L.Lock() // 运行到此处说明条件已经满足，开始获取互斥锁，如果锁已经被别人用了，开始等待
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的例子可以看出，当 Wait() 返回时（即已经获取到了互斥锁），有可能条件已经被其他先获取互斥锁的 goroutine 改变了，所以此时必须再次判断一下条件，即：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c.L.Lock() // 进行条件判断，加锁
if !condition() { // 如果不满足条件，进入 if 中
    c.Wait() // Wait() 内部会自动解锁
}

if !condition() { // 如果不满足条件，进入 if 中
    c.Wait() // Wait() 内部会自动解锁
}
... 这里可能会对 condition 作出改变 ...
c.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果代码这么写，就太费劲了，上面代码可以简化为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c.L.Lock() // 进行条件判断，加锁
for !condition() { // 如果不满足条件，进入 if 中
    c.Wait() // Wait() 内部会自动解锁
}

... 这里可能会对 condition 作出改变 ...
c.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即将 if 替换为 for，从而当从 Wait() 返回时，再次判断条件是否满足。&lt;/p&gt;

&lt;p&gt;用法实例&lt;/p&gt;

&lt;p&gt;用一个简单的例子来介绍一下 Cond 如何使用，即：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
    wakeup    = false
    workerNum = 3
)

func worker(workerID int, c *sync.Cond) {
    fmt.Printf(&amp;quot;Worker [%d] is RUNNING\n&amp;quot;, workerID)
    c.L.Lock()
    for !wakeup {
        fmt.Printf(&amp;quot;Worker [%d] check conditon\n&amp;quot;, workerID)
        c.Wait()
    }
    fmt.Printf(&amp;quot;Worker [%d] wakeup, DO something\n&amp;quot;, workerID)
    // 将唤醒标志改为 false
    // 此时其他已经醒来并抢夺互斥锁的 goroutine 重新判断条件后
    // 将再次进入 wait 状态
    wakeup = false 
    c.L.Unlock()
}

func main() {
    cond := sync.NewCond(&amp;amp;sync.Mutex{})
    for i := 0; i &amp;lt; workerNum; i++ {
        go worker(i, cond)
    }

    time.Sleep(2 * time.Second)
    wakeup = true
    cond.Broadcast() // 向所有 goroutine 进行广播，条件已经满足，即 wakeup = true

    time.Sleep(2 * time.Second)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行后的输出为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Worker [0] is RUNNING
Worker [1] is RUNNING
Worker [0] check conditon
Worker [1] check conditon
Worker [2] is RUNNING
Worker [2] check conditon
Worker [0] wakeup, DO something
Worker [1] check conditon
Worker [2] check conditon
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当 worker0 醒来后，又重新把条件变量进行了修改，从而导致 worker1 和 worker2 获取到互斥锁后重新检查到条件不满足，再次进入 wait 状态。&lt;/p&gt;

&lt;h2 id=&#34;map&#34;&gt;map&lt;/h2&gt;

&lt;p&gt;这是一个重点的数据结构，实现十分经典，具体看&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-map/&#34;&gt;map&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;sync-atomic&#34;&gt;sync/atomic&lt;/h1&gt;

&lt;p&gt;atomic包提供了底层的原子级内存操作，对于同步算法的实现很有用。支持类型共有六种：int32, int64, uint32, uint64, uintptr, unsafe.Pinter，实现的操作共五种：增减，比较并交换，载入，存储，交换。&lt;/p&gt;

&lt;h2 id=&#34;增或减&#34;&gt;增或减&lt;/h2&gt;

&lt;p&gt;顾名思义，原子增或减即可实现对被操作值的增大或减少。因此该操作只能操作数值类型。
　　
被用于进行增或减的原子操作都是以“Add”为前缀，并后面跟针对具体类型的名称。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func AddUint32(addr *uint32, delta uint32) (new uint32)

func AddInt64(addr *int64, delta int64) (new int64)
AddInt64原子性的将val的值添加到*addr并返回新值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;增&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;栗子：（在原来的基础上加n）
atomic.AddUint32(&amp;amp;addr,n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;减&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;栗子：(在原来的基础上加n（n为负数))
atomic.AddUint32(*addr,uint32(int32(n)))
//或
atomic.AddUint32(&amp;amp;addr,^uint32(-n-1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;比较并交换&#34;&gt;比较并交换&lt;/h2&gt;

&lt;p&gt;比较并交换&amp;mdash;-Compare And Swap 简称CAS&lt;/p&gt;

&lt;p&gt;他是假设被操作的值未曾被改变（即与旧值相等），并一旦确定这个假设的真实性就立即进行值替换，这是典型的乐观实现思想。&lt;/p&gt;

&lt;p&gt;如果想安全的并发一些类型的值，我们总是应该优先使用CAS&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子：（如果addr和old相同,就用new代替addr）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ok:=atomic.CompareAndSwapInt32(&amp;amp;addr,old,new)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;载入&#34;&gt;载入&lt;/h2&gt;

&lt;p&gt;如果一个写操作未完成，有一个读操作就已经发生了，这样读操作使很糟糕的。&lt;/p&gt;

&lt;p&gt;为了原子的读取某个值sync/atomic代码包同样为我们提供了一系列的函数。这些函数都以&amp;rdquo;Load&amp;rdquo;为前缀，意为载入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func LoadInt32(addr *int32) (val int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fun addValue(delta int32){
    for{
        v:=atomic.LoadInt32(&amp;amp;addr)
        if atomic.CompareAndSwapInt32(&amp;amp;v,addr,(delta+v)){
            break;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;存储&#34;&gt;存储&lt;/h2&gt;

&lt;p&gt;　　
与读操作对应的是写入操作，sync/atomic也提供了与原子的值载入函数相对应的原子的值存储函数。这些函数的名称均以“Store”为前缀
　　&lt;/p&gt;

&lt;p&gt;在原子的存储某个值的过程中，任何cpu都不会进行针对进行同一个值的读或写操作。如果我们把所有针对此值的写操作都改为原子操作，那么就不会出现针对此值的读操作读操作因被并发的进行而读到修改了一半的情况。
　　
原子操作总会成功，因为他不必关心被操作值的旧值是什么。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func StoreInt32(addr *int32, val int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;atomic.StoreInt32(被操作值的指针,新值)
atomic.StoreInt32(&amp;amp;value,newaddr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;交换&#34;&gt;交换&lt;/h2&gt;

&lt;p&gt;　　
原子交换操作，这类函数的名称都以“Swap”为前缀，与CAS不同，交换操作直接赋予新值，不管旧值。
　　
会返回旧值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//方法源码
func SwapInt32(addr *int32, new int32) (old int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;栗子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;atomic.SwapInt32(被操作值的指针,新值)（返回旧值）
oldval：=atomic.StoreInt32(&amp;amp;value,newaddr)
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>数据库系列---- Elasticsearch</title>
          <link>https://kingjcy.github.io/post/database/elasticsearch/</link>
          <pubDate>Thu, 21 Feb 2019 19:28:32 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/database/elasticsearch/</guid>
          <description>&lt;p&gt;开源的 Elasticsearch （以下简称 Elastic）是目前全文搜索引擎的首选。它可以快速地储存、搜索和分析海量数据。并且支持分布式，解决Lucene（支持全文索引的数据库系统）单机问题，目前维基百科、Stack Overflow、Github 都采用它。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;存储结构&#34;&gt;存储结构&lt;/h2&gt;

&lt;p&gt;在ES中，存储结构主要有四种，与传统的关系型数据库对比如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;index（Indices）相当于一个database&lt;/li&gt;
&lt;li&gt;type相当于一个table&lt;/li&gt;
&lt;li&gt;document相当于一个row&lt;/li&gt;
&lt;li&gt;properties（Fields）相当于一个column&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以如下对比&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Relational DB -&amp;gt; Databases -&amp;gt; Tables -&amp;gt; Rows -&amp;gt; Columns
Elasticsearch -&amp;gt; Indices -&amp;gt; Types -&amp;gt; Documents -&amp;gt; Fields
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Node 与 Cluster&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Elastic 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。&lt;/p&gt;

&lt;p&gt;单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Index&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Elastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。&lt;/p&gt;

&lt;p&gt;所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。&lt;/p&gt;

&lt;p&gt;下面的命令可以查看当前节点的所有 Index。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X GET &#39;http://localhost:9200/_cat/indices?v&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Document&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。&lt;/p&gt;

&lt;p&gt;Document 使用 JSON 格式表示，下面是一个例子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;user&amp;quot;: &amp;quot;张三&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
  &amp;quot;desc&amp;quot;: &amp;quot;数据库管理&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Type&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Document 可以分组，比如weather这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。&lt;/p&gt;

&lt;p&gt;不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。&lt;/p&gt;

&lt;p&gt;下面的命令可以列出每个 Index 所包含的 Type。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/_mapping?pretty=true&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据规划，Elastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。&lt;/p&gt;

&lt;h1 id=&#34;基本操作&#34;&gt;基本操作&lt;/h1&gt;

&lt;h2 id=&#34;新建和删除-index&#34;&gt;新建和删除 Index&lt;/h2&gt;

&lt;p&gt;新建 Index，可以直接向 Elastic 服务器发出 PUT 请求。下面的例子是新建一个名叫weather的 Index。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/weather&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务器返回一个 JSON 对象，里面的acknowledged字段表示操作成功。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;acknowledged&amp;quot;:true,
  &amp;quot;shards_acknowledged&amp;quot;:true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，我们发出 DELETE 请求，删除这个 Index。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X DELETE &#39;localhost:9200/weather&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;中文分词设置&#34;&gt;中文分词设置&lt;/h2&gt;

&lt;p&gt;首先，安装中文分词插件。这里使用的是 ik，也可以考虑其他插件（比如 smartcn）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码安装的是5.5.1版的插件，与 Elastic 5.5.1 配合使用。&lt;/p&gt;

&lt;p&gt;接着，重新启动 Elastic，就会自动加载这个新安装的插件。&lt;/p&gt;

&lt;p&gt;然后，新建一个 Index，指定需要分词的字段。这一步根据数据结构而异，下面的命令只针对本文。基本上，凡是需要搜索的中文字段，都要单独设置一下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/accounts&#39; -d &#39;
{
  &amp;quot;mappings&amp;quot;: {
    &amp;quot;person&amp;quot;: {
      &amp;quot;properties&amp;quot;: {
        &amp;quot;user&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
          &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
          &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
        },
        &amp;quot;title&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
          &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
          &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
        },
        &amp;quot;desc&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
          &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
          &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
        }
      }
    }
  }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，首先新建一个名称为accounts的 Index，里面有一个名称为person的 Type。person有三个字段。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user
title
desc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这三个字段都是中文，而且类型都是文本（text），所以需要指定中文分词器，不能使用默认的英文分词器。&lt;/p&gt;

&lt;p&gt;Elastic 的分词器称为 analyzer。我们对每个字段指定分词器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;user&amp;quot;: {
  &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
  &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;,
  &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，analyzer是字段文本的分词器，search_analyzer是搜索词的分词器。ik_max_word分词器是插件ik提供的，可以对文本进行最大数量的分词。&lt;/p&gt;

&lt;h2 id=&#34;新增记录&#34;&gt;新增记录&lt;/h2&gt;

&lt;p&gt;向指定的 /Index/Type 发送 PUT 请求，就可以在 Index 里面新增一条记录。比如，向/accounts/person发送请求，就可以新增一条人员记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/accounts/person/1&#39; -d &#39;
{
  &amp;quot;user&amp;quot;: &amp;quot;张三&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
  &amp;quot;desc&amp;quot;: &amp;quot;数据库管理&amp;quot;
}&#39; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务器返回的 JSON 对象，会给出 Index、Type、Id、Version 等信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
  &amp;quot;_version&amp;quot;:1,
  &amp;quot;result&amp;quot;:&amp;quot;created&amp;quot;,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:2,&amp;quot;successful&amp;quot;:1,&amp;quot;failed&amp;quot;:0},
  &amp;quot;created&amp;quot;:true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你仔细看，会发现请求路径是/accounts/person/1，最后的1是该条记录的 Id。它不一定是数字，任意字符串（比如abc）都可以。&lt;/p&gt;

&lt;p&gt;新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X POST &#39;localhost:9200/accounts/person&#39; -d &#39;
{
  &amp;quot;user&amp;quot;: &amp;quot;李四&amp;quot;,
  &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
  &amp;quot;desc&amp;quot;: &amp;quot;系统管理&amp;quot;
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，向/accounts/person发出一个 POST 请求，添加一个记录。这时，服务器返回的 JSON 对象里面，_id字段就是一个随机字符串。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot;:&amp;quot;AV3qGfrC6jMbsbXb6k1p&amp;quot;,
  &amp;quot;_version&amp;quot;:1,
  &amp;quot;result&amp;quot;:&amp;quot;created&amp;quot;,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:2,&amp;quot;successful&amp;quot;:1,&amp;quot;failed&amp;quot;:0},
  &amp;quot;created&amp;quot;:true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，如果没有先创建 Index（这个例子是accounts），直接执行上面的命令，Elastic 也不会报错，而是直接生成指定的 Index。所以，打字的时候要小心，不要写错 Index 的名称。&lt;/p&gt;

&lt;h2 id=&#34;查看记录&#34;&gt;查看记录&lt;/h2&gt;

&lt;p&gt;向/Index/Type/Id发出 GET 请求，就可以查看这条记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/1?pretty=true&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码请求查看/accounts/person/1这条记录，URL 的参数pretty=true表示以易读的格式返回。&lt;/p&gt;

&lt;p&gt;返回的数据中，found字段表示查询成功，_source字段返回原始记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;_index&amp;quot; : &amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot; : &amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
  &amp;quot;_version&amp;quot; : 1,
  &amp;quot;found&amp;quot; : true,
  &amp;quot;_source&amp;quot; : {
    &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
    &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
    &amp;quot;desc&amp;quot; : &amp;quot;数据库管理&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 Id 不正确，就查不到数据，found字段就是false。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/weather/beijing/abc?pretty=true&#39;

{
  &amp;quot;_index&amp;quot; : &amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot; : &amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot; : &amp;quot;abc&amp;quot;,
  &amp;quot;found&amp;quot; : false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;删除记录&#34;&gt;删除记录&lt;/h2&gt;

&lt;p&gt;删除记录就是发出 DELETE 请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X DELETE &#39;localhost:9200/accounts/person/1&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里先不要删除这条记录，后面还要用到。&lt;/p&gt;

&lt;h2 id=&#34;更新记录&#34;&gt;更新记录&lt;/h2&gt;

&lt;p&gt;更新记录就是使用 PUT 请求，重新发送一次数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X PUT &#39;localhost:9200/accounts/person/1&#39; -d &#39;
{
    &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
    &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
    &amp;quot;desc&amp;quot; : &amp;quot;数据库管理，软件开发&amp;quot;
}&#39; 

{
  &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
  &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
  &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
  &amp;quot;_version&amp;quot;:2,
  &amp;quot;result&amp;quot;:&amp;quot;updated&amp;quot;,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:2,&amp;quot;successful&amp;quot;:1,&amp;quot;failed&amp;quot;:0},
  &amp;quot;created&amp;quot;:false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，我们将原始数据从&amp;rdquo;数据库管理&amp;rdquo;改成&amp;rdquo;数据库管理，软件开发&amp;rdquo;。 返回结果里面，有几个字段发生了变化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;_version&amp;quot; : 2,
&amp;quot;result&amp;quot; : &amp;quot;updated&amp;quot;,
&amp;quot;created&amp;quot; : false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，记录的 Id 没变，但是版本（version）从1变成2，操作类型（result）从created变成updated，created字段变成false，因为这次不是新建记录。&lt;/p&gt;

&lt;h2 id=&#34;数据查询&#34;&gt;数据查询&lt;/h2&gt;

&lt;p&gt;返回所有记录&lt;/p&gt;

&lt;p&gt;使用 GET 方法，直接请求/Index/Type/_search，就会返回所有记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;

{
  &amp;quot;took&amp;quot;:2,
  &amp;quot;timed_out&amp;quot;:false,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:5,&amp;quot;successful&amp;quot;:5,&amp;quot;failed&amp;quot;:0},
  &amp;quot;hits&amp;quot;:{
    &amp;quot;total&amp;quot;:2,
    &amp;quot;max_score&amp;quot;:1.0,
    &amp;quot;hits&amp;quot;:[
      {
        &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
        &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
        &amp;quot;_id&amp;quot;:&amp;quot;AV3qGfrC6jMbsbXb6k1p&amp;quot;,
        &amp;quot;_score&amp;quot;:1.0,
        &amp;quot;_source&amp;quot;: {
          &amp;quot;user&amp;quot;: &amp;quot;李四&amp;quot;,
          &amp;quot;title&amp;quot;: &amp;quot;工程师&amp;quot;,
          &amp;quot;desc&amp;quot;: &amp;quot;系统管理&amp;quot;
        }
      },
      {
        &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
        &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
        &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
        &amp;quot;_score&amp;quot;:1.0,
        &amp;quot;_source&amp;quot;: {
          &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
          &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
          &amp;quot;desc&amp;quot; : &amp;quot;数据库管理，软件开发&amp;quot;
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;total：返回记录数，本例是2条。
max_score：最高的匹配程度，本例是1.0。
hits：返回的记录组成的数组。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。&lt;/p&gt;

&lt;h2 id=&#34;全文搜索&#34;&gt;全文搜索&lt;/h2&gt;

&lt;p&gt;Elastic 的查询非常特别，使用自己的查询语法，要求 GET 请求带有数据体。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;软件&amp;quot; }}
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码使用 Match 查询，指定的匹配条件是desc字段里面包含&amp;rdquo;软件&amp;rdquo;这个词。返回结果如下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;took&amp;quot;:3,
  &amp;quot;timed_out&amp;quot;:false,
  &amp;quot;_shards&amp;quot;:{&amp;quot;total&amp;quot;:5,&amp;quot;successful&amp;quot;:5,&amp;quot;failed&amp;quot;:0},
  &amp;quot;hits&amp;quot;:{
    &amp;quot;total&amp;quot;:1,
    &amp;quot;max_score&amp;quot;:0.28582606,
    &amp;quot;hits&amp;quot;:[
      {
        &amp;quot;_index&amp;quot;:&amp;quot;accounts&amp;quot;,
        &amp;quot;_type&amp;quot;:&amp;quot;person&amp;quot;,
        &amp;quot;_id&amp;quot;:&amp;quot;1&amp;quot;,
        &amp;quot;_score&amp;quot;:0.28582606,
        &amp;quot;_source&amp;quot;: {
          &amp;quot;user&amp;quot; : &amp;quot;张三&amp;quot;,
          &amp;quot;title&amp;quot; : &amp;quot;工程师&amp;quot;,
          &amp;quot;desc&amp;quot; : &amp;quot;数据库管理，软件开发&amp;quot;
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Elastic 默认一次返回10条结果，可以通过size字段改变这个设置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;管理&amp;quot; }},
  &amp;quot;size&amp;quot;: 1
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码指定，每次只返回一条结果。&lt;/p&gt;

&lt;p&gt;还可以通过from字段，指定位移。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;管理&amp;quot; }},
  &amp;quot;from&amp;quot;: 1,
  &amp;quot;size&amp;quot;: 1
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码指定，从位置1开始（默认是从位置0开始），只返回一条结果。&lt;/p&gt;

&lt;h2 id=&#34;逻辑运算&#34;&gt;逻辑运算&lt;/h2&gt;

&lt;p&gt;如果有多个搜索关键字， Elastic 认为它们是or关系。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot; : { &amp;quot;match&amp;quot; : { &amp;quot;desc&amp;quot; : &amp;quot;软件 系统&amp;quot; }}
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面代码搜索的是软件 or 系统。&lt;/p&gt;

&lt;p&gt;如果要执行多个关键词的and搜索，必须使用布尔查询。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;localhost:9200/accounts/person/_search&#39;  -d &#39;
{
  &amp;quot;query&amp;quot;: {
    &amp;quot;bool&amp;quot;: {
      &amp;quot;must&amp;quot;: [
        { &amp;quot;match&amp;quot;: { &amp;quot;desc&amp;quot;: &amp;quot;软件&amp;quot; } },
        { &amp;quot;match&amp;quot;: { &amp;quot;desc&amp;quot;: &amp;quot;系统&amp;quot; } }
      ]
    }
  }
}&#39;
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Client_golang</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/</link>
          <pubDate>Tue, 12 Feb 2019 15:59:42 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/</guid>
          <description>&lt;p&gt;client_golang 是Prometheus client的使用，基于golang语言。提供了prometheus的数据规范。&lt;/p&gt;

&lt;h1 id=&#34;库结构&#34;&gt;库结构&lt;/h1&gt;

&lt;p&gt;地址： &lt;a href=&#34;https://github.com/prometheus/client_golang&#34;&gt;https://github.com/prometheus/client_golang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Directories（描述）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;api                     Package api provides clients for the HTTP APIs.
api/prometheus/v1       Package v1 provides bindings to the Prometheus HTTP API v1: http://prometheus.io/docs/querying/api/

examples/random         A simple example exposing fictional RPC latencies with different types of random distributions (uniform, normal, and exponential) as Prometheus metrics.

examples/simple         A minimal example of how to include Prometheus instrumentation.
prometheus              Package prometheus is the core instrumentation package.
prometheus/graphite     Package graphite provides a bridge to push Prometheus metrics to a Graphite server.
prometheus/promauto     Package promauto provides constructors for the usual Prometheus metrics that return them already registered with the global registry (prometheus.DefaultRegisterer).
prometheus/promhttp     Package promhttp provides tooling around HTTP servers and clients.
prometheus/push         Package push provides functions to push metrics to a Pushgateway.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面对client_golang库的结构和使用进行了总结。&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;原理解析&#34;&gt;原理解析&lt;/h2&gt;

&lt;p&gt;下面是客户端的UML图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/library/client_golang/client_golang.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、默认调用gocollect这个采集器，在引入package registry包的时候，就会调用init初始化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;registry会调用describe这个接口，实现就是gocollect这个对应的describe&lt;/li&gt;
&lt;li&gt;http.handle会调用registry的gather函数，然后函数调用collect接口，实现就是gocollect这个对应的collect&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、上面只是一种特殊类型，其实对应的四种类型分别都有对应的结构体继承vec的基本函数接口，也有对应的接口，会有对应的实现&lt;/p&gt;

&lt;p&gt;然后这个四种类型就是就是四种collecter，同样的流程&lt;/p&gt;

&lt;p&gt;3、可以新建一个struct作为一个collecter，实现describe，collect接口，就可以实现自己的逻辑，最后其实还是调用四种类型，结合使用&lt;/p&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;p&gt;1、四种类型有实现的函数赋值，常用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set（）
WithLabelValues().set()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、注册的几种方式&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;第一种&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;//statement
proxyTodayTrafficIn := prometheus.NewGaugeVec(prometheus.GaugeOpts{
    Name: &amp;quot;&amp;quot;,
    Help: &amp;quot;The today trafficin of proxy.&amp;quot;,
},[]string{&amp;quot;type&amp;quot;,&amp;quot;laststarttime&amp;quot;,&amp;quot;lastclosetime&amp;quot;})
//get value
proxyTodayTrafficIn.With(prometheus.Labels{&amp;quot;type&amp;quot;:v.Type,&amp;quot;laststarttime&amp;quot;:v.LastStartTime,&amp;quot;lastclosetime&amp;quot;:v.LastCloseTime}).Set(float64(v.TodayTrafficIn))

//registry
prometheus.MustRegister(proxyTodayTrafficIn)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;第二种&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;serverBindPort := prometheus.NewGauge(prometheus.GaugeOpts{
    Name: &amp;quot;frps_server_bind_port&amp;quot;,
    Help: &amp;quot;The port of server frps.&amp;quot;,
})

serverBindPort.Set(float64(cfg.BindPort))

//registry
prometheus.MustRegister(serverBindPort)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面两种可以归为一类，都是采用的默认的方式，下面就涉及到自定义结构体，根据上面的原理，我们需要重自定义的结构体中获取到两个结构体的值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *结构体) Describe(ch chan&amp;lt;- *prometheus.Desc) {}----可见这个接口的实现需要将prometheus.Desc放倒channel中去
func (s *结构体) Collect(ch chan&amp;lt;- prometheus.Metric) {}----可见这个接口的实现需要将prometheus.Metric放倒channel中去
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看这两个结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Desc struct {
    // fqName has been built from Namespace, Subsystem, and Name.
    fqName string
    // help provides some helpful information about this metric.
    help string
    // constLabelPairs contains precalculated DTO label pairs based on
    // the constant labels.
    constLabelPairs []*dto.LabelPair
    // VariableLabels contains names of labels for which the metric
    // maintains variable values.
    variableLabels []string
    // id is a hash of the values of the ConstLabels and fqName. This
    // must be unique among all registered descriptors and can therefore be
    // used as an identifier of the descriptor.
    id uint64
    // dimHash is a hash of the label names (preset and variable) and the
    // Help string. Each Desc with the same fqName must have the same
    // dimHash.
    dimHash uint64
    // err is an error that occured during construction. It is reported on
    // registration time.
    err error
}

type Metric interface {
    // Desc returns the descriptor for the Metric. This method idempotently
    // returns the same descriptor throughout the lifetime of the
    // Metric. The returned descriptor is immutable by contract. A Metric
    // unable to describe itself must return an invalid descriptor (created
    // with NewInvalidDesc).
    Desc() *Desc
    // Write encodes the Metric into a &amp;quot;Metric&amp;quot; Protocol Buffer data
    // transmission object.
    //
    // Metric implementations must observe concurrency safety as reads of
    // this metric may occur at any time, and any blocking occurs at the
    // expense of total performance of rendering all registered
    // metrics. Ideally, Metric implementations should support concurrent
    // readers.
    //
    // While populating dto.Metric, it is the responsibility of the
    // implementation to ensure validity of the Metric protobuf (like valid
    // UTF-8 strings or syntactically valid metric and label names). It is
    // recommended to sort labels lexicographically. (Implementers may find
    // LabelPairSorter useful for that.) Callers of Write should still make
    // sure of sorting if they depend on it.
    Write(*dto.Metric) error
    // TODO(beorn7): The original rationale of passing in a pre-allocated
    // dto.Metric protobuf to save allocations has disappeared. The
    // signature of this method should be changed to &amp;quot;Write() (*dto.Metric,
    // error)&amp;quot;.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我看看如何获取这两种值，首先desc，每种数据类型都有一个desc函数可以直接获取，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name := fmt.Sprintf(&amp;quot;%s_%s&amp;quot;, namespace, metricMaps.Name)

gaugeDescription := prometheus.NewGauge(
    prometheus.GaugeOpts{
        Name:      name,
        Help:      metricMaps.Description,
    },
)

ch &amp;lt;- gaugeDescription.Desc()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以直接新建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewDesc(fqName, help string, variableLabels []string, constLabels Labels) *Desc {}

//new desc
desc := prometheus.NewDesc(name, metricMaps.Description, constLabels, nil)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看metrics这个接口，找到其相应的结构体实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MustNewConstMetric(desc *Desc, valueType ValueType, value float64, labelValues ...string) Metric {}

//channel
ch &amp;lt;- prometheus.MustNewConstMetric(desc, vtype, value, labelValue...)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;第三种&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;新建结构体，完成上面方法的使用，就可以了，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    //set var
    vtype := prometheus.CounterValue
    name := fmt.Sprintf(&amp;quot;%s_%s&amp;quot;, namespace, metricMaps.Name)
    log.Debugf(&amp;quot;counter name: %s&amp;quot;, name)

    //new desc
    desc := prometheus.NewDesc(name, metricMaps.Description, constLabels, nil)

    //deal Value
    value, err := dealValue(res[i])
    if err != nil {
        log.Errorf(&amp;quot;parse value error: %s&amp;quot;,err)
        break
    }
    log.Debugf(&amp;quot;counter value: %s&amp;quot;, value)

    //channel
    ch &amp;lt;- prometheus.MustNewConstMetric(desc, vtype, value, labelValue...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、Newregistry调用和直接调用prometheus的对应的MustRegister其实是一样的，都是默认new一个registry的结构体&lt;/p&gt;

&lt;p&gt;4、duplicate metrics collector registration attempted&amp;mdash;重复注册&lt;/p&gt;

&lt;h2 id=&#34;collector&#34;&gt;Collector&lt;/h2&gt;

&lt;p&gt;Collector 中 Describe 和 Collect 方法都是无状态的函数，其中 Describe 暴露全部可能的 Metric 描述列表，在注册（Register）或注销（Unregister）Collector 时会调用 Describe 来获取完整的 Metric 列表，用以检测 Metric 定义的冲突，另外在 github.com/prometheus/client_golang/prometheus/promhttp 下的 Instrument Handler 中，也会通过 Describe 获取 Metric 列表，并检查 label 列表（InstrumentHandler 中只支持 code 和 method 两种自定义 label）；而通过 Collect 可以获取采样数据，然后通过 HTTP 接口暴露给 Prom Server。另外，一些临时性的进程，如批处理任务，可以把数据 push 到 Push Gateway，由 Push Gateway 暴露 pull 接口，此处不赘述。&lt;/p&gt;

&lt;p&gt;客户端对数据的收集大多是针对标准数据结构来进行的,如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Counter：收集事件次数等单调递增的数据&lt;/li&gt;
&lt;li&gt;Gauge：收集当前的状态，可增可减，比如数据库连接数&lt;/li&gt;
&lt;li&gt;Histogram：收集随机正态分布数据，比如响应延迟&lt;/li&gt;
&lt;li&gt;Summary：收集随机正态分布数据，和 Histogram 是类似的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每种标准数据结构还对应了 Vec 结构，通过 Vec 可以简洁的定义一组相同性质的 Metric，在采集数据的时候传入一组自定义的 Label/Value 获取具体的 Metric（Counter/Gauge/Histogram/Summary），最终都会落实到基本的数据结构上，这里不再赘述。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Counter 和 Gauge&lt;/p&gt;

&lt;p&gt;Gauge 和 Counter 基本实现上看是一个进程内共享的浮点数，基于 value 结构实现，而 Counter 和 Gauge 仅仅封装了对这个共享浮点数的各种操作和合法性检查逻辑。&lt;/p&gt;

&lt;p&gt;看 Counter 中 Inc 函数的实现&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/library/client_golang/add.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;value.Add 中修改共享数据时采用了“无锁”实现，相比“有锁 (Mutex)”实现可以更充分利用多核处理器的并行计算能力，性能相比加 Mutex 的实现会有很大提升。下图是 Go Benchmark 的测试结果，对比了“有锁”（用 defer 或不用 defer 来释放锁）和“无锁”实现在多核场景下对性能的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/library/client_golang/benchmark.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Histogram&lt;/p&gt;

&lt;p&gt;Histogram 实现了 Observer 接口，用来获取客户端状态初始化（重启）到某个时间点的采样点分布，监控数据常需要服从正态分布。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Summary&lt;/p&gt;

&lt;p&gt;Summary 是标准数据结构中最复杂的一个，用来收集服从正态分布的采样数据。在 Go 客户端 Summary 结构和 Histogram 一样，都实现了 Observer 接口&lt;/p&gt;

&lt;p&gt;这两个比较复杂，使用较少，可以先不研究，使用的时候研究&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;集成注意事项&#34;&gt;集成注意事项&lt;/h2&gt;

&lt;p&gt;Go 客户端为 HTTP 层的集成提供了方便的 API，但使用中需要注意不要使用 github.com/prometheus/client_golang/prometheus 下定义的已经 deprecated 的 Instrument 函数，除了会引入额外（通常不需要）的监控数据，不仅会对程序性能造成不利影响，而且可能存在危险的 race（如计算请求大小时存在 goroutine 并发地访问 Header 逻辑）。&lt;/p&gt;

&lt;h1 id=&#34;prometheus-exporter实例&#34;&gt;Prometheus Exporter实例&lt;/h1&gt;

&lt;p&gt;Exporter是基于Prometheus实施的监控系统中重要的组成部分，承担数据指标的采集工作，官方的exporter列表中已经包含了常见的绝大多数的系统指标监控，比如用于机器性能监控的node_exporter, 用于网络设备监控的snmp_exporter等等。这些已有的exporter对于监控来说，仅仅需要很少的配置工作就能提供完善的数据指标采集。&lt;/p&gt;

&lt;p&gt;有时我们需要自己去写一些与业务逻辑比较相关的指标监控，这些指标无法通过常见的exporter获取到。比如我们需要提供对于DNS解析情况的整体监控，了解如何编写exporter对于业务监控很重要，也是完善监控系统需要经历的一个阶段。接下来我们就介绍如何编写exporter, 本篇内容编写的语言为golang, 官方也提供了python, java等其他的语言实现的库，采集方式其实大同小异。编写exporter的方式也是大同小异，就是集成对应的prometheus库，我们使用golang语言，就是集成client_golang。&lt;/p&gt;

&lt;p&gt;下面我们就使用golang语言集成cleint_golang来开发一个exporter。&lt;/p&gt;

&lt;h2 id=&#34;搭建环境&#34;&gt;搭建环境&lt;/h2&gt;

&lt;p&gt;首先确保机器上安装了go语言(1.7版本以上)，并设置好了对应的GOPATH。接下来我们就可以开始编写代码了。以下是一个简单的exporter&lt;/p&gt;

&lt;p&gt;下载对应的prometheus包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/prometheus/client_golang/prometheus/promhttp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序主函数:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;log&amp;quot;
    &amp;quot;net/http&amp;quot;
    &amp;quot;github.com/prometheus/client_golang/prometheus/promhttp&amp;quot;
)
func main() {
    http.Handle(&amp;quot;/metrics&amp;quot;, promhttp.Handler())
    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个代码中我们仅仅通过http模块指定了一个路径，并将client_golang库中的promhttp.Handler()作为处理函数传递进去后，就可以获取指标信息了,两行代码实现了一个exporter。这里内部其实是使用了一个默认的收集器将通过NewGoCollector采集当前Go运行时的相关信息比如go堆栈使用,goroutine的数据等等。 通过访问&lt;a href=&#34;http://localhost:8080/metrics&#34;&gt;http://localhost:8080/metrics&lt;/a&gt; 即可查看详细的指标参数。&lt;/p&gt;

&lt;p&gt;上面的代码仅仅展示了一个默认的采集器，并且通过接口调用隐藏了太多实施细节，对于下一步开发并没什么作用，为了实现自定义的监控我们需要先了解一些基本概念。&lt;/p&gt;

&lt;h2 id=&#34;指标类别&#34;&gt;指标类别&lt;/h2&gt;

&lt;p&gt;Prometheus中主要使用的四类指标类型，如下所示&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Counter (累加指标)&lt;/li&gt;
&lt;li&gt;Gauge (测量指标)&lt;/li&gt;
&lt;li&gt;Summary (概略图)&lt;/li&gt;
&lt;li&gt;Histogram (直方图)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些我们在上面基本原理中已经介绍过了，这边详细的介绍，并在下面加以使用。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Counter 一个累加指标数据，这个值随着时间只会逐渐的增加，比如程序完成的总任务数量，运行错误发生的总次数。常见的还有交换机中snmp采集的数据流量也属于该类型，代表了持续增加的数据包或者传输字节累加值。&lt;/li&gt;
&lt;li&gt;Gauge代表了采集的一个单一数据，这个数据可以增加也可以减少，比如CPU使用情况，内存使用量，硬盘当前的空间容量等等&lt;/li&gt;
&lt;li&gt;Histogram和Summary使用的频率较少，两种都是基于采样的方式。另外有一些库对于这两个指标的使用和支持程度不同，有些仅仅实现了部分功能。这两个类型对于某一些业务需求可能比较常见，比如查询单位时间内：总的响应时间低于300ms的占比，或者查询95%用户查询的门限值对应的响应时间是多少。 使用Histogram和Summary指标的时候同时会产生多组数据，_count代表了采样的总数，_sum则代表采样值的和。 _bucket则代表了落入此范围的数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是使用historam来定义的一组指标，计算出了平均五分钟内的查询请求小于0.3s的请求占比总量的比例值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  sum(rate(http_request_duration_seconds_bucket{le=&amp;quot;0.3&amp;quot;}[5m])) by (job)
/
  sum(rate(http_request_duration_seconds_count[5m])) by (job)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果需要聚合数据，可以使用histogram. 并且如果对于分布范围有明确的值的情况下（比如300ms），也可以使用histogram。但是如果仅仅是一个百分比的值（比如上面的95%），则使用Summary&lt;/p&gt;

&lt;h2 id=&#34;定义指标&#34;&gt;定义指标&lt;/h2&gt;

&lt;p&gt;这里我们需要引入另一个依赖库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/prometheus/client_golang/prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面先来定义了两个指标数据，一个是Guage类型， 一个是Counter类型。分别代表了CPU温度和磁盘失败次数统计，使用上面的定义进行分类。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cpuTemp = prometheus.NewGauge(prometheus.GaugeOpts{
    Name: &amp;quot;cpu_temperature_celsius&amp;quot;,
    Help: &amp;quot;Current temperature of the CPU.&amp;quot;,
})
hdFailures = prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Name: &amp;quot;hd_errors_total&amp;quot;,
        Help: &amp;quot;Number of hard-disk errors.&amp;quot;,
    },
    []string{&amp;quot;device&amp;quot;},
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;加一个counter的用法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;totalScrapes: prometheus.NewCounter(prometheus.CounterOpts{
            Namespace: namespace,
            Name:      &amp;quot;exporter_scrapes_total&amp;quot;,
            Help:      &amp;quot;Current total redis scrapes.&amp;quot;,
        })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里还可以注册其他的参数，比如上面的磁盘失败次数统计上，我们可以同时传递一个device设备名称进去，这样我们采集的时候就可以获得多个不同的指标。每个指标对应了一个设备的磁盘失败次数统计。&lt;/p&gt;

&lt;h2 id=&#34;注册指标&#34;&gt;注册指标&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    // Metrics have to be registered to be exposed:
    prometheus.MustRegister(cpuTemp)
    prometheus.MustRegister(hdFailures)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用prometheus.MustRegister是将数据直接注册到Default Registry，就像上面的运行的例子一样，这个Default Registry不需要额外的任何代码就可以将指标传递出去。注册后既可以在程序层面上去使用该指标了，这里我们使用之前定义的指标提供的API（Set和With().Inc）去改变指标的数据内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    cpuTemp.Set(65.3)
    hdFailures.With(prometheus.Labels{&amp;quot;device&amp;quot;:&amp;quot;/dev/sda&amp;quot;}).Inc()

    // The Handler function provides a default handler to expose metrics
    // via an HTTP server. &amp;quot;/metrics&amp;quot; is the usual endpoint for that.
    http.Handle(&amp;quot;/metrics&amp;quot;, promhttp.Handler())
    log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中With函数是传递到之前定义的label=”device”上的值，也就是生成指标类似于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cpu_temperature_celsius 65.3
hd_errors_total{&amp;quot;device&amp;quot;=&amp;quot;/dev/sda&amp;quot;} 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然我们写在main函数中的方式是有问题的，这样这个指标仅仅改变了一次，不会随着我们下次采集数据的时候发生任何变化，我们希望的是每次执行采集的时候，程序都去自动的抓取指标并将数据通过http的方式传递给我们。&lt;/p&gt;

&lt;p&gt;到这里，一套基本的采集流程也就完成了，这是最基本的使用方式，当然其中也还是封装了很多过程，比如采集器等，如果需要自定义一些东西，就要了解这些封装的过程，完成重写，下面我们自定义exporter。&lt;/p&gt;

&lt;h2 id=&#34;自定义exporter&#34;&gt;自定义exporter&lt;/h2&gt;

&lt;p&gt;counter数据采集实例，重写collecter&lt;/p&gt;

&lt;p&gt;下面是一个采集Counter类型数据的实例，这个例子中实现了一个自定义的，满足采集器(Collector)接口的结构体，并手动注册该结构体后，使其每次查询的时候自动执行采集任务。&lt;/p&gt;

&lt;p&gt;我们先来看下采集器Collector接口的实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Collector interface {
    // 用于传递所有可能的指标的定义描述符
    // 可以在程序运行期间添加新的描述，收集新的指标信息
    // 重复的描述符将被忽略。两个不同的Collector不要设置相同的描述符
    Describe(chan&amp;lt;- *Desc)

    // Prometheus的注册器调用Collect执行实际的抓取参数的工作，
    // 并将收集的数据传递到Channel中返回
    // 收集的指标信息来自于Describe中传递，可以并发的执行抓取工作，但是必须要保证线程的安全。
    Collect(chan&amp;lt;- Metric)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;了解了接口的实现后，我们就可以写自己的实现了，先定义结构体，这是一个集群的指标采集器，每个集群都有自己的Zone,代表集群的名称。另外两个是保存的采集的指标。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ClusterManager struct {
    Zone         string
    OOMCountDesc *prometheus.Desc
    RAMUsageDesc *prometheus.Desc
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来实现一个采集工作,放到了ReallyExpensiveAssessmentOfTheSystemState函数中实现，每次执行的时候，返回一个按照主机名作为键采集到的数据，两个返回值分别代表了OOM错误计数，和RAM使用指标信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *ClusterManager) ReallyExpensiveAssessmentOfTheSystemState() (
    oomCountByHost map[string]int, ramUsageByHost map[string]float64,
) {
    oomCountByHost = map[string]int{
        &amp;quot;foo.example.org&amp;quot;: int(rand.Int31n(1000)),
        &amp;quot;bar.example.org&amp;quot;: int(rand.Int31n(1000)),
    }
    ramUsageByHost = map[string]float64{
        &amp;quot;foo.example.org&amp;quot;: rand.Float64() * 100,
        &amp;quot;bar.example.org&amp;quot;: rand.Float64() * 100,
    }
    return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实现Describe接口，传递指标描述符到channel&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Describe simply sends the two Descs in the struct to the channel.
func (c *ClusterManager) Describe(ch chan&amp;lt;- *prometheus.Desc) {
    ch &amp;lt;- c.OOMCountDesc
    ch &amp;lt;- c.RAMUsageDesc
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Collect函数将执行抓取函数并返回数据，返回的数据传递到channel中，并且传递的同时绑定原先的指标描述符。以及指标的类型（一个Counter和一个Guage）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *ClusterManager) Collect(ch chan&amp;lt;- prometheus.Metric) {
    oomCountByHost, ramUsageByHost := c.ReallyExpensiveAssessmentOfTheSystemState()
    for host, oomCount := range oomCountByHost {
        ch &amp;lt;- prometheus.MustNewConstMetric(
            c.OOMCountDesc,
            prometheus.CounterValue,
            float64(oomCount),
            host,
        )
    }
    for host, ramUsage := range ramUsageByHost {
        ch &amp;lt;- prometheus.MustNewConstMetric(
            c.RAMUsageDesc,
            prometheus.GaugeValue,
            ramUsage,
            host,
        )
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建结构体及对应的指标信息,NewDesc参数第一个为指标的名称，第二个为帮助信息，显示在指标的上面作为注释，第三个是定义的label名称数组，第四个是定义的Labels&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewClusterManager(zone string) *ClusterManager {
    return &amp;amp;ClusterManager{
        Zone: zone,
        OOMCountDesc: prometheus.NewDesc(
            &amp;quot;clustermanager_oom_crashes_total&amp;quot;,
            &amp;quot;Number of OOM crashes.&amp;quot;,
            []string{&amp;quot;host&amp;quot;},
            prometheus.Labels{&amp;quot;zone&amp;quot;: zone},
        ),
        RAMUsageDesc: prometheus.NewDesc(
            &amp;quot;clustermanager_ram_usage_bytes&amp;quot;,
            &amp;quot;RAM usage as reported to the cluster manager.&amp;quot;,
            []string{&amp;quot;host&amp;quot;},
            prometheus.Labels{&amp;quot;zone&amp;quot;: zone},
        ),
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行主程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    workerDB := NewClusterManager(&amp;quot;db&amp;quot;)
    workerCA := NewClusterManager(&amp;quot;ca&amp;quot;)

    // Since we are dealing with custom Collector implementations, it might
    // be a good idea to try it out with a pedantic registry.
    reg := prometheus.NewPedanticRegistry()
    reg.MustRegister(workerDB)
    reg.MustRegister(workerCA)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果直接执行上面的参数的话，不会获取任何的参数，因为程序将自动推出，我们并未定义http接口去暴露数据出来，因此数据在执行的时候还需要定义一个httphandler来处理http请求。&lt;/p&gt;

&lt;p&gt;添加下面的代码到main函数后面，即可实现数据传递到http接口上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gatherers := prometheus.Gatherers{
    prometheus.DefaultGatherer,
    reg,
}

h := promhttp.HandlerFor(gatherers,
    promhttp.HandlerOpts{
        ErrorLog:      log.NewErrorLogger(),
        ErrorHandling: promhttp.ContinueOnError,
    })
http.HandleFunc(&amp;quot;/metrics&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
    h.ServeHTTP(w, r)
})
log.Infoln(&amp;quot;Start server at :8080&amp;quot;)
if err := http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil); err != nil {
    log.Errorf(&amp;quot;Error occur when start server %v&amp;quot;, err)
    os.Exit(1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中prometheus.Gatherers用来定义一个采集数据的收集器集合，可以merge多个不同的采集数据到一个结果集合，这里我们传递了缺省的DefaultGatherer，所以他在输出中也会包含go运行时指标信息。同时包含reg是我们之前生成的一个注册对象，用来自定义采集数据。&lt;/p&gt;

&lt;p&gt;promhttp.HandlerFor()函数传递之前的Gatherers对象，并返回一个httpHandler对象，这个httpHandler对象可以调用其自身的ServHTTP函数来接手http请求，并返回响应。其中promhttp.HandlerOpts定义了采集过程中如果发生错误时，继续采集其他的数据。&lt;/p&gt;

&lt;p&gt;尝试刷新几次浏览器获取最新的指标信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clustermanager_oom_crashes_total{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 364
clustermanager_oom_crashes_total{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 90
clustermanager_oom_crashes_total{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 844
clustermanager_oom_crashes_total{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 801
# HELP clustermanager_ram_usage_bytes RAM usage as reported to the cluster manager.
# TYPE clustermanager_ram_usage_bytes gauge
clustermanager_ram_usage_bytes{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 10.738111282075208
clustermanager_ram_usage_bytes{host=&amp;quot;bar.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 19.003276633920805
clustermanager_ram_usage_bytes{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;ca&amp;quot;} 79.72085409108028
clustermanager_ram_usage_bytes{host=&amp;quot;foo.example.org&amp;quot;,zone=&amp;quot;db&amp;quot;} 13.041384617379178
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每次刷新的时候，我们都会获得不同的数据，类似于实现了一个数值不断改变的采集器。当然，具体的指标和采集函数还需要按照需求进行修改，满足实际的业务需求。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Bufio</title>
          <link>https://kingjcy.github.io/post/golang/go-bufio/</link>
          <pubDate>Tue, 25 Dec 2018 14:27:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-bufio/</guid>
          <description>&lt;p&gt;bufio 包实现了缓存IO。它包装了 io.Reader 和 io.Writer 对象，创建了另外的Reader和Writer对象，它们也实现了 io.Reader 和 io.Writer 接口，不过它们是有缓存的。该包同时为文本I/O提供了一些便利操作。&lt;/p&gt;

&lt;h1 id=&#34;类型和方法&#34;&gt;类型和方法&lt;/h1&gt;

&lt;h2 id=&#34;reader-类型和方法&#34;&gt;Reader 类型和方法&lt;/h2&gt;

&lt;h3 id=&#34;类型&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;bufio.Reader 结构包装了一个 io.Reader 对象，提供缓存功能，同时实现了 io.Reader 接口。&lt;/p&gt;

&lt;p&gt;Reader 结构没有任何导出的字段，结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader struct {
    buf          []byte        // 缓存
    rd           io.Reader    // 底层的io.Reader
    // r:从buf中读走的字节（偏移）；w:buf中填充内容的偏移；
    // w - r 是buf中可被读的长度（缓存数据的大小），也是Buffered()方法的返回值
    r, w         int
    err          error        // 读过程中遇到的错误
    lastByte     int        // 最后一次读到的字节（ReadByte/UnreadByte)
    lastRuneSize int        // 最后一次读到的Rune的大小 (ReadRune/UnreadRune)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;bufio 包提供了两个实例化 bufio.Reader 对象的函数：NewReader 和 NewReaderSize。其中，NewReader 函数是调用 NewReaderSize 函数实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(rd io.Reader) *Reader {
    // 默认缓存大小：defaultBufSize=4096
    return NewReaderSize(rd, defaultBufSize)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下NewReaderSize的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReaderSize(rd io.Reader, size int) *Reader {
    // 已经是bufio.Reader类型，且缓存大小不小于 size，则直接返回
    b, ok := rd.(*Reader)
    if ok &amp;amp;&amp;amp; len(b.buf) &amp;gt;= size {
        return b
    }
    // 缓存大小不会小于 minReadBufferSize （16字节）
    if size &amp;lt; minReadBufferSize {
        size = minReadBufferSize
    }
    // 构造一个bufio.Reader实例
    return &amp;amp;Reader{
        buf:          make([]byte, size),
        rd:           rd,
        lastByte:     -1,
        lastRuneSize: -1,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见需要一个id.reader的实例来进行初始化，一般我们都是使用string或者[]byte的reader类型来创建。&lt;/p&gt;

&lt;h3 id=&#34;方法&#34;&gt;方法&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadSlice、ReadBytes、ReadString 和 ReadLine 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之所以将这几个方法放在一起，是因为他们有着类似的行为。事实上，后三个方法最终都是调用ReadSlice来实现的。所以，我们先来看看ReadSlice方法(感觉这一段直接看源码较好)。&lt;/p&gt;

&lt;p&gt;1.ReadSlice方法签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadSlice(delim byte) (line []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ReadSlice 从输入中读取，直到遇到第一个界定符（delim）为止，返回一个指向缓存中字节的 slice，在下次调用读操作（read）时，这些字节会无效。举例说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := bufio.NewReader(strings.NewReader(&amp;quot;http://studygolang.com. \nIt is the home of gophers&amp;quot;))
line, _ := reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
// 这里可以换上任意的 bufio 的 Read/Write 操作
n, _ := reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
fmt.Println(string(n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;the line:http://studygolang.com.

the line:It is the home of gophers
It is the home of gophers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从结果可以看出，第一次ReadSlice的结果（line），在第二次调用读操作后，内容发生了变化。也就是说，ReadSlice 返回的 []byte 是指向 Reader 中的 buffer ，而不是 copy 一份返回。正因为ReadSlice 返回的数据会被下次的 I/O 操作重写，因此许多的客户端会选择使用 ReadBytes 或者 ReadString 来代替。读者可以将上面代码中的 ReadSlice 改为 ReadBytes 或 ReadString ，看看结果有什么不同。&lt;/p&gt;

&lt;p&gt;注意，这里的界定符可以是任意的字符，可以将上面代码中的&amp;rsquo;\n&amp;rsquo;改为&amp;rsquo;m&amp;rsquo;试试。同时，返回的结果是包含界定符本身的，上例中，输出结果有一空行就是&amp;rsquo;\n&amp;rsquo;本身(line携带一个&amp;rsquo;\n&amp;rsquo;,printf又追加了一个&amp;rsquo;\n&amp;rsquo;)。&lt;/p&gt;

&lt;p&gt;如果 ReadSlice 在找到界定符之前遇到了 error ，它就会返回缓存中所有的数据和错误本身（经常是 io.EOF）。如果在找到界定符之前缓存已经满了，ReadSlice 会返回 bufio.ErrBufferFull 错误。当且仅当返回的结果（line）没有以界定符结束的时候，ReadSlice 返回err != nil，也就是说，如果ReadSlice 返回的结果 line 不是以界定符 delim 结尾，那么返回的 er r也一定不等于 nil（可能是bufio.ErrBufferFull或io.EOF）。 例子代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := bufio.NewReaderSize(strings.NewReader(&amp;quot;http://studygolang.com&amp;quot;),16)
line, err := reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;line:%s\terror:%s\n&amp;quot;, line, err)
line, err = reader.ReadSlice(&#39;\n&#39;)
fmt.Printf(&amp;quot;line:%s\terror:%s\n&amp;quot;, line, err)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line:http://studygola    error:bufio: buffer full
line:ng.com    error:EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.ReadBytes方法签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadBytes(delim byte) (line []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方法的参数和返回值类型与 ReadSlice 都一样。 ReadBytes 从输入中读取直到遇到界定符（delim）为止，返回的 slice 包含了从当前到界定符的内容 （包括界定符）。如果 ReadBytes 在遇到界定符之前就捕获到一个错误，它会返回遇到错误之前已经读取的数据，和这个捕获到的错误（经常是 io.EOF）。跟 ReadSlice 一样，如果 ReadBytes 返回的结果 line 不是以界定符 delim 结尾，那么返回的 err 也一定不等于 nil（可能是bufio.ErrBufferFull 或 io.EOF）。&lt;/p&gt;

&lt;p&gt;从这个说明可以看出，ReadBytes和ReadSlice功能和用法都很像，那他们有什么不同呢？&lt;/p&gt;

&lt;p&gt;在讲解ReadSlice时说到，它返回的 []byte 是指向 Reader 中的 buffer，而不是 copy 一份返回，也正因为如此，通常我们会使用 ReadBytes 或 ReadString。很显然，ReadBytes 返回的 []byte 不会是指向 Reader 中的 buffer，通过查看源码可以证实这一点。&lt;/p&gt;

&lt;p&gt;还是上面的例子，我们将 ReadSlice 改为 ReadBytes：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := bufio.NewReader(strings.NewReader(&amp;quot;http://studygolang.com. \nIt is the home of gophers&amp;quot;))
line, _ := reader.ReadBytes(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
// 这里可以换上任意的 bufio 的 Read/Write 操作
n, _ := reader.ReadBytes(&#39;\n&#39;)
fmt.Printf(&amp;quot;the line:%s\n&amp;quot;, line)
fmt.Println(string(n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;the line:http://studygolang.com.

the line:http://studygolang.com.

It is the home of gophers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.ReadString方法&lt;/p&gt;

&lt;p&gt;看一下该方法的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadString(delim byte) (line string, err error) {
    bytes, err := b.ReadBytes(delim)
    return string(bytes), err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它调用了 ReadBytes 方法，并将结果的 []byte 转为 string 类型。&lt;/p&gt;

&lt;p&gt;4.ReadLine方法签名如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) ReadLine() (line []byte, isPrefix bool, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ReadLine 是一个底层的原始行读取命令。许多调用者或许会使用 ReadBytes(&amp;rsquo;\n&amp;rsquo;) 或者 ReadString(&amp;rsquo;\n&amp;rsquo;) 来代替这个方法。&lt;/p&gt;

&lt;p&gt;ReadLine 尝试返回单独的行，不包括行尾的换行符。如果一行大于缓存，isPrefix 会被设置为 true，同时返回该行的开始部分（等于缓存大小的部分）。该行剩余的部分就会在下次调用的时候返回。当下次调用返回该行剩余部分时，isPrefix 将会是 false 。跟 ReadSlice 一样，返回的 line 只是 buffer 的引用，在下次执行IO操作时，line 会无效。可以将 ReadSlice 中的例子该为 ReadLine 试试。&lt;/p&gt;

&lt;p&gt;注意，返回值中，要么 line 不是 nil，要么 err 非 nil，两者不会同时非 nil。ReadLine 返回的文本不会包含行结尾（&amp;rdquo;\r\n&amp;rdquo;或者&amp;rdquo;\n&amp;rdquo;）。如果输入中没有行尾标识符，不会返回任何指示或者错误。&lt;/p&gt;

&lt;p&gt;个人建议可以这么实现读取一行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line, err := reader.ReadBytes(&#39;\n&#39;)
line = bytes.TrimRight(line, &amp;quot;\r\n&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样既读取了一行，也去掉了行尾结束符（当然，如果你希望留下行尾结束符，只用ReadBytes即可）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Peek 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从方法的名称可以猜到，该方法只是“窥探”一下 Reader 中没有读取的 n 个字节。好比栈数据结构中的取栈顶元素，但不出栈。&lt;/p&gt;

&lt;p&gt;方法的签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) Peek(n int) ([]byte, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同上面介绍的 ReadSlice一样，返回的 []byte 只是 buffer 中的引用，在下次IO操作后会无效，可见该方法（以及ReadSlice这样的，返回buffer引用的方法）对多 goroutine 是不安全的，也就是在多并发环境下，不能依赖其结果。&lt;/p&gt;

&lt;p&gt;我们通过例子来证明一下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;bufio&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;strings&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {
    reader := bufio.NewReaderSize(strings.NewReader(&amp;quot;http://studygolang.com.\t It is the home of gophers&amp;quot;), 14)
    go Peek(reader)
    go reader.ReadBytes(&#39;\t&#39;)
    time.Sleep(1e8)
}

func Peek(reader *bufio.Reader) {
    line, _ := reader.Peek(14)
    fmt.Printf(&amp;quot;%s\n&amp;quot;, line)
    // time.Sleep(1)
    fmt.Printf(&amp;quot;%s\n&amp;quot;, line)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygo
http://studygo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果和预期的一致。然而，这是由于目前的 goroutine 调度方式导致的结果。如果我们将例子中注释掉的 time.Sleep(1) 取消注释（这样调度其他 goroutine 执行），再次运行，得到的结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygo
ng.com.     It is
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，Reader 的 Peek 方法如果返回的 []byte 长度小于 n，这时返回的 err != nil ，用于解释为啥会小于 n。如果 n 大于 reader 的 buffer 长度，err 会是 ErrBufferFull。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Reader 的其他方法都是实现了 io 包中的接口，它们的使用方法在io包中都有介绍，在此不赘述。&lt;/p&gt;

&lt;p&gt;这些方法包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Reader) Read(p []byte) (n int, err error)
func (b *Reader) ReadByte() (c byte, err error)
func (b *Reader) ReadRune() (r rune, size int, err error)
func (b *Reader) UnreadByte() error
func (b *Reader) UnreadRune() error
func (b *Reader) WriteTo(w io.Writer) (n int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你应该知道它们都是哪个接口的方法吧。&lt;/p&gt;

&lt;h2 id=&#34;scanner-类型和方法&#34;&gt;Scanner 类型和方法&lt;/h2&gt;

&lt;h3 id=&#34;类型-1&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;对于简单的读取一行，在 Reader 类型中，感觉没有让人特别满意的方法。于是，Go1.1增加了一个类型：Scanner。官方关于Go1.1增加该类型的说明如下：&lt;/p&gt;

&lt;p&gt;在 bufio 包中有多种方式获取文本输入，ReadBytes、ReadString 和独特的 ReadLine，对于简单的目的这些都有些过于复杂了。在 Go 1.1 中，添加了一个新类型，Scanner，以便更容易的处理如按行读取输入序列或空格分隔单词等，这类简单的任务。它终结了如输入一个很长的有问题的行这样的输入错误，并且提供了简单的默认行为：基于行的输入，每行都剔除分隔标识。这里的代码展示一次输入一行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scanner := bufio.NewScanner(os.Stdin)
for scanner.Scan() {
    fmt.Println(scanner.Text()) // Println will add back the final &#39;\n&#39;
}
if err := scanner.Err(); err != nil {
    fmt.Fprintln(os.Stderr, &amp;quot;reading standard input:&amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输入的行为可以通过一个函数控制，来控制输入的每个部分（参阅 SplitFunc 的文档），但是对于复杂的问题或持续传递错误的，可能还是需要原有接口。&lt;/p&gt;

&lt;p&gt;Scanner 类型和 Reader 类型一样，没有任何导出的字段，同时它也包装了一个 io.Reader 对象，但它没有实现 io.Reader 接口。&lt;/p&gt;

&lt;p&gt;Scanner 的结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Scanner struct {
    r            io.Reader // The reader provided by the client.
    split        SplitFunc // The function to split the tokens.
    maxTokenSize int       // Maximum size of a token; modified by tests.
    token        []byte    // Last token returned by split.
    buf          []byte    // Buffer used as argument to split.
    start        int       // First non-processed byte in buf.
    end          int       // End of data in buf.
    err          error     // Sticky error.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里 split、maxTokenSize 和 token 需要讲解一下。&lt;/p&gt;

&lt;h4 id=&#34;split&#34;&gt;split&lt;/h4&gt;

&lt;p&gt;split对应的类型是SplitFunc ，我们需要了解一下SplitFunc类型，定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SplitFunc func(data []byte, atEOF bool) (advance int, token []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SplitFunc 定义了 用于对输入进行分词的 split 函数的签名。参数 data 是还未处理的数据，atEOF 标识 Reader 是否还有更多数据（是否到了EOF）。返回值 advance 表示从输入中读取的字节数，token 表示下一个结果数据，err 则代表可能的错误。&lt;/p&gt;

&lt;p&gt;举例说明一下这里的 token 代表的意思：&lt;/p&gt;

&lt;p&gt;有数据 &amp;ldquo;studygolang\tpolaris\tgolangchina&amp;rdquo;，通过&amp;rdquo;\t&amp;rdquo;进行分词，那么会得到三个token，它们的内容分别是：studygolang、polaris 和 golangchina。而 SplitFunc 的功能是：进行分词，并返回未处理的数据中第一个 token。对于这个数据，就是返回 studygolang。
如果 data 中没有一个完整的 token，例如，在扫描行（scanning lines）时没有换行符，SplitFunc 会返回(0,nil,nil)通知 Scanner 读取更多数据到 slice 中。&lt;/p&gt;

&lt;p&gt;如果 err != nil，扫描停止，同时该错误会返回。&lt;/p&gt;

&lt;p&gt;如果参数 data 为空的 slice，除非 atEOF 为 true，否则该函数永远不会被调用。如果 atEOF 为 true，这时 data 可以非空，这时的数据是没有处理的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;SplitFunc 的实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在 bufio 包中预定义了一些 split 函数，也就是说，在 Scanner 结构中的 split 字段，可以通过这些预定义的 split 赋值，同时 Scanner 类型的 Split 方法也可以接收这些预定义函数作为参数。所以，我们可以说，这些预定义 split 函数都是 SplitFunc 类型的实例。这些函数包括：ScanBytes、ScanRunes、ScanWords 和 ScanLines。（由于都是 SplitFunc 的实例，自然这些函数的签名都和 SplitFunc 一样）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ScanBytes 返回单个字节作为一个 token。&lt;/li&gt;
&lt;li&gt;ScanRunes 返回单个 UTF-8 编码的 rune 作为一个 token。返回的 rune 序列（token）和 range string类型 返回的序列是等价的，也就是说，对于无效的 UTF-8 编码会解释为 U+FFFD = &amp;ldquo;\xef\xbf\xbd&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;ScanWords 返回通过“空格”分词的单词。如：study golang，调用会返回study。注意，这里的“空格”是 unicode.IsSpace()，即包括：&amp;rsquo;\t&amp;rsquo;, &amp;lsquo;\n&amp;rsquo;, &amp;lsquo;\v&amp;rsquo;, &amp;lsquo;\f&amp;rsquo;, &amp;lsquo;\r&amp;rsquo;, &amp;lsquo; &amp;lsquo;, U+0085 (NEL), U+00A0 (NBSP)。&lt;/li&gt;
&lt;li&gt;ScanLines 返回一行文本，不包括行尾的换行符。这里的换行包括了Windows下的&amp;rdquo;\r\n&amp;rdquo;和Unix下的&amp;rdquo;\n&amp;rdquo;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般地，我们不会单独使用这些函数，而是提供给 Scanner 实例使用。&lt;/p&gt;

&lt;h4 id=&#34;maxtokensize&#34;&gt;maxTokenSize&lt;/h4&gt;

&lt;p&gt;maxTokenSize 字段 表示通过 split 分词后的一个 token 允许的最大长度。在该包中定义了一个常量 MaxScanTokenSize = 64 * 1024，这是允许的最大 token 长度（64k）。&lt;/p&gt;

&lt;h3 id=&#34;方法-1&#34;&gt;方法&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Scanner 没有导出任何字段，而它需要有外部的 io.Reader 对象，因此，我们不能直接实例化 Scanner 对象，必须通过 bufio 包提供的实例化函数来实例化。实例化函数签名以及内部实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewScanner(r io.Reader) *Scanner {
    return &amp;amp;Scanner{
        r:            r,
        split:        ScanLines,
        maxTokenSize: MaxScanTokenSize,
        buf:          make([]byte, 4096), // Plausible starting size; needn&#39;t be large.
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，返回的 Scanner 实例默认的 split 函数是 ScanLines。这边实例化也是需要一个id.reader的实例，所以我们一般也是使用string或者[]byte的实例来做创建参数使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Split 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;前面我们提到过可以通过 Split 方法为 Scanner 实例设置分词行为。由于 Scanner 实例的默认 split 总是 ScanLines，如果我们想要用其他的 split，可以通过 Split 方法做到。&lt;/p&gt;

&lt;p&gt;比如，我们想要统计一段英文有多少个单词（不排除重复），我们可以这么做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const input = &amp;quot;This is The Golang Standard Library.\nWelcome you!&amp;quot;
scanner := bufio.NewScanner(strings.NewReader(input))
scanner.Split(bufio.ScanWords)
count := 0
for scanner.Scan() {
    count++
}
if err := scanner.Err(); err != nil {
    fmt.Fprintln(os.Stderr, &amp;quot;reading input:&amp;quot;, err)
}
fmt.Println(count)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们实例化 Scanner 后，通过调用 scanner.Split(bufio.ScanWords) 来更改 split 函数。注意，我们应该在调用 Scan 方法之前调用 Split 方法。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Scan 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;该方法好比 iterator 中的 Next 方法，它用于将 Scanner 获取下一个 token，以便 Bytes 和 Text 方法可用。当扫描停止时，它返回false，这时候，要么是到了输入的末尾要么是遇到了一个错误。注意，当 Scan 返回 false 时，通过 Err 方法可以获取第一个遇到的错误（但如果错误是 io.EOF，Err 方法会返回 nil）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Bytes 和 Text 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这两个方法的行为一致，都是返回最近的 token，无非 Bytes 返回的是 []byte，Text 返回的是 string。该方法应该在 Scan 调用后调用，而且，下次调用 Scan 会覆盖这次的 token。比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scanner := bufio.NewScanner(strings.NewReader(&amp;quot;http://studygolang.com. \nIt is the home of gophers&amp;quot;))
if scanner.Scan() {
    scanner.Scan()
    fmt.Printf(&amp;quot;%s&amp;quot;, scanner.Text())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回的是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;It is the home of gophers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而不是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygolang.com.
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Err 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;前面已经提到，通过 Err 方法可以获取第一个遇到的错误（但如果错误是 io.EOF，Err 方法会返回 nil）。&lt;/p&gt;

&lt;h3 id=&#34;完整实例&#34;&gt;完整实例&lt;/h3&gt;

&lt;p&gt;我们经常会有这样的需求：读取文件中的数据，一次读取一行。在学习了 Reader 类型，我们可以使用它的 ReadBytes 或 ReadString来实现，甚至使用 ReadLine 来实现。然而，在 Go1.1 中，我们可以使用 Scanner 来做这件事，而且更简单好用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file, err := os.Create(&amp;quot;scanner.txt&amp;quot;)
if err != nil {
    panic(err)
}
defer file.Close()
file.WriteString(&amp;quot;http://studygolang.com.\nIt is the home of gophers.\nIf you are studying golang, welcome you!&amp;quot;)
// 将文件 offset 设置到文件开头
file.Seek(0, os.SEEK_SET)
scanner := bufio.NewScanner(file)
for scanner.Scan() {
    fmt.Println(scanner.Text())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://studygolang.com.
It is the home of gophers.
If you are studying golang, welcome you!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;writer-类型和方法&#34;&gt;Writer 类型和方法&lt;/h2&gt;

&lt;h3 id=&#34;类型-2&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;bufio.Writer 结构包装了一个 io.Writer 对象，提供缓存功能，同时实现了 io.Writer 接口。&lt;/p&gt;

&lt;p&gt;Writer 结构没有任何导出的字段，结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Writer struct {
    err error        // 写过程中遇到的错误
    buf []byte        // 缓存
    n   int            // 当前缓存中的字节数
    wr  io.Writer    // 底层的 io.Writer 对象
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;相比 bufio.Reader, bufio.Writer 结构定义简单很多。&lt;/p&gt;

&lt;p&gt;注意：如果在写数据到 Writer 的时候出现了一个错误，不会再允许有数据被写进来了，并且所有随后的写操作都会返回该错误。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;和 Reader 类型一样，bufio 包提供了两个实例化 bufio.Writer 对象的函数：NewWriter 和 NewWriterSize。其中，NewWriter 函数是调用 NewWriterSize 函数实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewWriter(wr io.Writer) *Writer {
    // 默认缓存大小：defaultBufSize=4096
    return NewWriterSize(wr, defaultBufSize)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下 NewWriterSize 的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewWriterSize(wr io.Writer, size int) *Writer {
    // 已经是 bufio.Writer 类型，且缓存大小不小于 size，则直接返回
    b, ok := wr.(*Writer)
    if ok &amp;amp;&amp;amp; len(b.buf) &amp;gt;= size {
        return b
    }
    if size &amp;lt;= 0 {
        size = defaultBufSize
    }
    return &amp;amp;Writer{
        buf: make([]byte, size),
        wr:  w,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;方法-2&#34;&gt;方法&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Available 和 Buffered 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Available 方法获取缓存中还未使用的字节数（缓存大小 - 字段 n 的值）；&lt;/p&gt;

&lt;p&gt;Buffered 方法获取写入当前缓存中的字节数（字段 n 的值）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Flush 方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;该方法将缓存中的所有数据写入底层的 io.Writer 对象中。使用 bufio.Writer 时，在所有的 Write 操作完成之后，应该调用 Flush 方法使得缓存都写入 io.Writer 对象中。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Writer 类型其他方法是一些实际的写方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 实现了 io.ReaderFrom 接口
func (b *Writer) ReadFrom(r io.Reader) (n int64, err error)

// 实现了 io.Writer 接口
func (b *Writer) Write(p []byte) (nn int, err error)

// 实现了 io.ByteWriter 接口
func (b *Writer) WriteByte(c byte) error

// io 中没有该方法的接口，它用于写入单个 Unicode 码点，返回写入的字节数（码点占用的字节），内部实现会根据当前 rune 的范围调用 WriteByte 或 WriteString
func (b *Writer) WriteRune(r rune) (size int, err error)

// 写入字符串，如果返回写入的字节数比 len(s) 小，返回的error会解释原因
func (b *Writer) WriteString(s string) (int, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些写方法在缓存满了时会调用 Flush 方法。另外，这些写方法源码开始处，有这样的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if b.err != nil {
    return b.err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说，只要写的过程中遇到了错误，再次调用写操作会直接返回该错误。&lt;/p&gt;

&lt;h2 id=&#34;readwriter-类型和实例化&#34;&gt;ReadWriter 类型和实例化&lt;/h2&gt;

&lt;h3 id=&#34;类型-3&#34;&gt;类型&lt;/h3&gt;

&lt;p&gt;ReadWriter 结构存储了 bufio.Reader 和 bufio.Writer 类型的指针（内嵌），它实现了 io.ReadWriter 结构。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ReadWriter struct {
    *Reader
    *Writer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;实例&#34;&gt;实例&lt;/h3&gt;

&lt;p&gt;ReadWriter 的实例化可以跟普通结构类型一样，也可以通过调用 bufio.NewReadWriter 函数来实现：只是简单的实例化 ReadWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReadWriter(r *Reader, w *Writer) *ReadWriter {
    return &amp;amp;ReadWriter{r, w}
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控系列---- log</title>
          <link>https://kingjcy.github.io/post/monitor/log/log-scheme/</link>
          <pubDate>Mon, 13 Aug 2018 11:16:45 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/log-scheme/</guid>
          <description>&lt;p&gt;日志是设备或者程序对自身状态和运作行为的记录，日志监控平台是包括日志采集，存储，分析，索引查询，告警以及各种流程管理的一站式日志服务，日志监控是监控体系中核心的建设，而且可以说是量最大的一项监控。&lt;/p&gt;

&lt;h1 id=&#34;日志&#34;&gt;日志&lt;/h1&gt;

&lt;p&gt;日志是设备或者程序对自身状态和运作行为的记录。日志记录了事件，通过日志就可以看到设备和程序运行的历史信息，通过这些信息，可以了解设备和程序运行情况的变化，以更好的对于设备和程序进行维护。主要是在系统出现问题的时候，通过对于运行过程中发生的历史事件，可以查找问题出现的原因。&lt;/p&gt;

&lt;p&gt;我们可以通过下图来对日志有一个直观的概念&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/log/log&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;日志平台&#34;&gt;日志平台&lt;/h1&gt;

&lt;p&gt;业内最常见的日志采集方案就是 ELK，在 ELK 出来之前，日志管理基本上都是通过登陆日志所在机器然后使用 Linux 命令或人为查看和统计 ，这样是非常没有效率的。&lt;/p&gt;

&lt;h2 id=&#34;架构&#34;&gt;架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/log/log1&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这是一个最简化版的日志收集架构，很多基于ELK的日志架构是从它演化而来，比如中加上kafka等队列缓存，核心的问题就是日志数据都保存到ElasticSearch中。其实核心的是四大模块&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据采集模块：负责从各节点上实时采集数据，建议选用filebeat来实现。&lt;/li&gt;
&lt;li&gt;数据接入模块：由于采集数据的速度和数据处理的速度不一定同步，因此添加一个消息中间件来作为缓冲，建议选用Kafka来实现。&lt;/li&gt;
&lt;li&gt;存储计算模块：对采集到的数据进行实时存储分析，建议选用ES来实现。&lt;/li&gt;
&lt;li&gt;数据输出模块：对分析后的结果展示，一般使用kibana。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;采集&#34;&gt;采集&lt;/h2&gt;

&lt;p&gt;在日志采集方面，可以说是有很多项目的支持，从以一开始的logstash，Rsyslog到后来Flume，Fluentd，Filebeat等。采集越来越倾向于轻量级，性能越来越高。容器日志采集和寻常的采集也不一样，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/collect-scheme/&#34;&gt;不同的方案&lt;/a&gt;有不同的适用场景。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.02.20&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;今天调研了grafana推出的loki也是处理日志,这边采集是promtail，具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/loki/loki/&#34;&gt;loki调研&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;存储&#34;&gt;存储&lt;/h2&gt;

&lt;p&gt;在日志存储索引查询方面，目前只有&lt;a href=&#34;https://kingjcy.github.io/post/database/elasticsearch/&#34;&gt;ES&lt;/a&gt;一个核心技术站，并没有过多的选择。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2020.02.20&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;今天调研了grafana推出的loki也是处理日志，借鉴了prometheus的label和metrics理念，通过label完成检索，具体可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/loki/loki/&#34;&gt;loki调研&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;展示&#34;&gt;展示&lt;/h2&gt;

&lt;p&gt;在数据展示报表方面，目前对日志也没有什么选择，只有kibana。Kibana主要负责读取ElasticSearch中的数据，并进行可视化展示。并且，它还自带Tool，可以方便调用ElasticSearch的Rest API。在日志平台中，我们通过Kibana查看日志。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系统---- Thanos</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/</link>
          <pubDate>Fri, 13 Jul 2018 17:14:15 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/</guid>
          <description>&lt;p&gt;Thanos，一组通过跨集群联合、跨集群无限存储和全局查询为Prometheus 增加高可用性的组件。&lt;/p&gt;

&lt;h1 id=&#34;基本功能&#34;&gt;基本功能&lt;/h1&gt;

&lt;p&gt;prometheus单点能够支持百万的metrics，但是在规模越来越大的系统中，已经不能满足要求，需要集群的功能来处理更加庞大的数据，基于这个情况，thanos诞生了，thanos的主要功能：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;去重，单点问题，可以让prometheus高可用，实现多采集情况下的数据查询，query是无状态的，可以使用负载均衡&lt;/li&gt;
&lt;li&gt;聚合，实现不同prometheus的数据的聚合，匹配prometheus的hashmode功能，实现集群的方式&lt;/li&gt;
&lt;li&gt;数据备份，主要是基于s3的，相当于远程存储，我们没有使用，直接将数据写入到了kafka&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;基本组件&#34;&gt;基本组件&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Sidecar&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sidecar作为一个单独的进程和已有的Prometheus实例运行在一个server上，互不影响。Sidecar可以视为一个Proxy组件，所有对Prometheus的访问都通过Sidecar来代理进行。通过Sidecar还可以将采集到的数据直接备份到云端对象存储服务器。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Querier&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有的Sidecar与Querier直连，同时Querier实现了一套Prometheus官方的HTTP API从而保证对外提供与Prometheus一致的数据源接口，Grafana可以通过同一个查询接口请求不同集群的数据，Querier负责找到对应的集群并通过Sidecar获取数据。Querier本身无状态的也是水平可扩展的，因而可以实现高可部署，而且Querier可以实现对高可部署的Prometheus的数据进行合并从而保证多次查询结果的一致性，从而解决全局视图和高可用的问题。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Store&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Store实现了一套和Sidecar完全一致的API提供给Querier用于查询Sidecar备份到云端对象存储的数据。因为Sidecar在完成数据备份后，Prometheus会清理掉本地数据保证本地空间可用。所以当监控人员需要调取历史数据时只能去对象存储空间获取，而Store就提供了这样一个接口。Store Gateway只会缓存对象存储的基本信息，例如存储块的索引，从而保证实现快速查询的同时占用较少本地空间。&lt;/p&gt;

&lt;p&gt;store和sidecar都提供了相同gprc的api，给外部client进行查询，其实是一回事。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Comactor&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Compactor主要用于对采集到的数据进行压缩，实现将数据存储至对象存储时节省空间。单独使用，和集群没有什么关系。主要是将对象存储 Bucket 中的多个小 的相同的Block 合并成 大 Block&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;h2 id=&#34;sidecar&#34;&gt;sidecar&lt;/h2&gt;

&lt;p&gt;sidecar部署在prometheus机器上,直接使用二进制文件配置不同的启动参数来启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/promes/thanos-sidecar/thanos sidecar --log.level=debug --tsdb.path=/data --prometheus.url=http://localhost:9099
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;query&#34;&gt;query&lt;/h2&gt;

&lt;p&gt;query用于查询，单独部署，然后和prometheus一样使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/promes/thanos-query/thanos query --query.timeout=15s --store.response-timeout=15s --log.level=debug --store=10.243.53.96:19091 --store=10.243.53.100:19091 --store=10.243.53.101:19091 --store=10.243.53.186:19091
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sd&#34;&gt;sd&lt;/h2&gt;

&lt;p&gt;thanos有三种sd的方式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Static Flags&lt;/p&gt;

&lt;p&gt;最简单的就是在参数中配置列表，就是我们上面使用的方式&lt;/p&gt;

&lt;p&gt;&amp;ndash;store参数指定的是每个sidecar的grpc端口，query会根据&amp;ndash;store参数列表找到对应的prometheus进行查询，所有组件的端口都是有默认值的，如果需要修改则指定参数&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Interface&lt;/th&gt;
&lt;th&gt;Port&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sidecar&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10901&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sidecar&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10902&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Query&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10903&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Query&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10904&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Store&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10905&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Store&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10906&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Receive&lt;/td&gt;
&lt;td&gt;gRPC (store API)&lt;/td&gt;
&lt;td&gt;10907&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Receive&lt;/td&gt;
&lt;td&gt;HTTP (remote write API)&lt;/td&gt;
&lt;td&gt;10908&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Receive&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10909&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rule&lt;/td&gt;
&lt;td&gt;gRPC&lt;/td&gt;
&lt;td&gt;10910&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rule&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10911&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Compact&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;td&gt;10912&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;File SD&lt;/p&gt;

&lt;p&gt;&amp;ndash;store.sd-files=&lt;path&gt;和 &amp;ndash;store.sd-interval=&lt;5m&gt;来获取对应的prometheus列表&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;DNS SD&lt;/p&gt;

&lt;p&gt;&amp;ndash;store=dns+stores.thanos.mycompany.org:9090
&amp;ndash;store=dnssrv+_thanosstores._tcp.mycompany.org
&amp;ndash;store=dnssrvnoa+_thanosstores._tcp.mycompany.org&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基本原理&#34;&gt;基本原理&lt;/h1&gt;

&lt;p&gt;Thanos 在每一台 Prometheus 服务器上运行一个sidecar组件，并提供了一个用于处理 PromQL 查询的中央 Querier 组件，因而在所有服务器之间引入了一个中央查询层。这些组件构成了一个 Thanos 部署，并基于 memberlist gossip 协议实现组件间通信。Querier 可以水平扩展，因为它是无状态的，并且可充当智能逆向代理，将请求转发给sidecar，汇总它们的响应，并对 PromQL 查询进行评估。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;实现细节&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;Thanos 通过使用后端的对象存储来解决数据保留问题。Prometheus 在将数据写入磁盘时，sidecar的 StoreAPI 组件会检测到，并将数据上传到对象存储器中。Store 组件还可以作为一个基于 gossip 协议的检索代理，让 Querier 组件与它进行通信以获取数据。&lt;/li&gt;
&lt;li&gt;我们使用基本的过滤器（基于时间范围和外部标签）过滤掉不会提供所需数据的 StoreAPI（叶子），然后执行剩余的查询。然后将来自不同来源的数据按照时间顺序追加的方式合并在一起。&lt;/li&gt;
&lt;li&gt;Querier 组件可以基于用户规模自动调整密度（例如 5 分钟、1 小时或 24 小时）&lt;/li&gt;
&lt;li&gt;StoreAPI 组件了解 Prometheus 的数据格式，因此它可以优化查询执行计划，并缓存数据块的特定索引，以对用户查询做出足够快的响应，避免了缓存大量数据的必要。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;我们通过为所有 Prometheus+ sidecar实例提供唯一的外部标签来解决多个边车试图将相同的数据块上传到对象存储的问题，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;First:

&amp;quot;cluster&amp;quot;: &amp;quot;prod1&amp;quot;

&amp;quot;replica&amp;quot;: &amp;quot;0&amp;quot;

Second:

&amp;quot;cluster&amp;quot;:&amp;quot;prod1&amp;quot;

&amp;quot;replica&amp;quot;: &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于标签集是唯一的，所以不会有什么问题。不过，如果指定了副本，查询层可以在运行时通过“replica”标签进行除重操作。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thanos 还提供了时间序列数据的压缩和降采样（downsample）存储。Prometheus 提供了一个内置的压缩​​模型，现有较小的数据块被重写为较大的数据块，并进行结构重组以提高查询性能。Thanos 在Compactor 组件（作为批次作业运行）中使用了相同的机制，并压缩对象存储数据。Płotka 说，Compactor 也对数据进行降采样，“目前降采样时间间隔不可配置，不过我们选择了一些合理的时间间隔——5 分钟和1 小时”。压缩也是其他时间序列数据库（如 InfluxDB 和 OpenTSDB ）的常见功能。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;遇到的问题&#34;&gt;遇到的问题&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;查询速度较慢，在数据量基本特别大的时候，查询会超时。&lt;/li&gt;
&lt;li&gt;thanos 目前还不能支持默认查询lookback时间，promehteus可以设置默认查询时间，thanos默认是根据规模自动调整的，目前发现有10m，20m等，这边可以暂时表达式加时间处理这个问题。&lt;/li&gt;
&lt;li&gt;sidecar的启动参数&amp;ndash;cluster-peers是什么作用&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;在prometheus的聚合和集群发展中，出现了很多的相同的项目，大部分都是使用了远程存储的概念，比如&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/&#34;&gt;cortex&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/&#34;&gt;M3DB&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;victoriametrics&lt;/a&gt;，thanos在调研落地的过程中，各方面还是相对做到比较好的，适合做为prometheus的扩展和聚合方案。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/cluster/thanos/thanos&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2019.9.9&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;victoriametrics&lt;/a&gt;在存储和查询上更加的优秀，目前比较推荐victoriametrics。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- Filebeat</title>
          <link>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat/</link>
          <pubDate>Sun, 08 Jul 2018 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat/</guid>
          <description>&lt;p&gt;Filebeat 是使用 Golang 实现的轻量型日志采集器，是基于原先 logstash-forwarder 的源码改造出来的，没有任何依赖，可以单独存在的搞性能采集工具。&lt;/p&gt;

&lt;h1 id=&#34;认识beats&#34;&gt;认识beats&lt;/h1&gt;

&lt;p&gt;Beats是轻量级（资源高效，无依赖性，小型）采集程序的集合。这些可以是日志文件（Filebeat），网络数据（Packetbeat），服务器指标（Metricbeat）等，Beats建立在名为libbeat的Go框架之上，该框架主要用于数据转发，Elastic和社区开发的越来越多的Beats可以收集的任何其他类型的数据，收集后，数据将直接发送到Elasticsearch或Logstash中进行其他处理。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Filebeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;顾名思义，Filebeat用于收集和传送日志文件，它也是最常用的Beat。 Filebeat如此高效的事实之一就是它处理背压的方式，如果Logstash繁忙，Filebeat会减慢其读取速率，并在减速结束后加快节奏。
Filebeat几乎可以安装在任何操作系统上，包括作为Docker容器安装，还随附用于特定平台（例如Apache，MySQL，Docker等）的内部模块，其中包含这些平台的默认配置和Kibana对象。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Packetbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;网络数据包分析器Packetbeat是第一个引入的beat。 Packetbeat捕获服务器之间的网络流量，因此可用于应用程序和性能监视。
Packetbeat可以安装在受监视的服务器上，也可以安装在其专用服务器上。 Packetbeat跟踪网络流量，解码协议并记录每笔交易的数据。 Packetbeat支持的协议包括：DNS，HTTP，ICMP，Redis，MySQL，MongoDB，Cassandra等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Metricbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Metricbeat是一种非常受欢迎的beat，它收集并报告各种系统和平台的各种系统级度量。 Metricbeat还支持用于从特定平台收集统计信息的内部模块。您可以使用这些模块和称为指标集的metricsets来配置Metricbeat收集指标的频率以及要收集哪些特定指标。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Heartbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Heartbeat是用于“uptime monitoring”的。本质上，Heartbeat是探测服务以检查它们是否可访问的功能，例如，它可以用来验证服务的正常运行时间是否符合您的SLA。 您要做的就是为Heartbeat提供URL和正常运行时间指标的列表。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Auditbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Auditbeat可用于操作Linux服务器上的用户和进程活动。 与其他传统的系统工具（systemd，auditd）类似，Auditbeat可用于识别安全漏洞-文件更改，配置更改，恶意行为等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Winlogbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Winlogbeat仅会引起Windows系统管理员或工程师的兴趣，因为它是专门为收集Windows事件日志而设计的。 它可用于分析安全事件，已安装的更新等。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Functionbeat&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Functionbeat主要是为“serverless”而设计，可以将其部署为收集数据并将其发送到ELK堆栈的功能。 Functionbeat专为监视云环境而设计，目前已针对Amazon设置量身定制，可以部署为Amazon Lambda函数，以从Amazon CloudWatch，Kinesis和SQS收集数据。&lt;/p&gt;

&lt;h1 id=&#34;filebeat&#34;&gt;filebeat&lt;/h1&gt;

&lt;p&gt;为什么选择filebeat&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;性能好&lt;/li&gt;
&lt;li&gt;基于golang的技术站，对于容器生态友好&lt;/li&gt;
&lt;li&gt;使用部署方便，功能齐全&lt;/li&gt;
&lt;li&gt;其他技术方案，主要是社区的fluentd，使用的ruby+c，使用起来很复杂，各种功能都需要写插件来完成。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;基本使用&#34;&gt;基本使用&lt;/h2&gt;

&lt;h3 id=&#34;下载&#34;&gt;下载&lt;/h3&gt;

&lt;p&gt;直接去github上可以下载二进制文件，可以直接运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.10.2-linux-x86_64.tar.gz
tar xzvf filebeat-7.10.2-linux-x86_64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以使用源码编译&lt;/p&gt;

&lt;p&gt;After installing Go, set the GOPATH environment variable to point to your workspace location, and make sure $GOPATH/bin is in your PATH.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p ${GOPATH}/src/github.com/elastic
git clone https://github.com/elastic/beats ${GOPATH}/src/github.com/elastic/beats
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have multiple go paths, use ${GOPATH%%:*} instead of ${GOPATH}.&lt;/p&gt;

&lt;p&gt;Then you can compile a particular Beat by using the Makefile. For example, for Packetbeat:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd beats/packetbeat
make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of the Beats might have extra development requirements, in which case you’ll find a CONTRIBUTING.md file in the Beat directory.&lt;/p&gt;

&lt;p&gt;We use an EditorConfig file in the beats repository to standardise how different editors handle whitespace, line endings, and other coding styles in our files. Most popular editors have a plugin for EditorConfig and we strongly recommend that you install it.&lt;/p&gt;

&lt;h3 id=&#34;配置文件&#34;&gt;配置文件&lt;/h3&gt;

&lt;p&gt;FileBeat 的配置文件定义了在读取文件的位置，输出流的位置以及相应的性能参数，本实例是以 Kafka 消息中间件作为缓冲，所有的日志收集器都向 Kafka 输送日志流，相应的最简单的配置项如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim fileat.yml

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /wls/applogs/rtlog/app.log
  fields:
    log_topic: appName
  multiline:
        # pattern for error log, if start with space or cause by
        pattern: &#39;^[[:space:]]+(at|\.{3})\b|^Caused by:&#39;
        negate:  false
        match:   after

output.kafka:
   enabled: true
   hosts: [&amp;quot;kafka-1:9092&amp;quot;,&amp;quot;kafka-2:9092&amp;quot;]
   topic: applog
   version: &amp;quot;0.10.2.0&amp;quot;
   compression: gzip

processors:
- drop_fields:
   fields: [&amp;quot;beat&amp;quot;, &amp;quot;input&amp;quot;, &amp;quot;source&amp;quot;, &amp;quot;offset&amp;quot;]

logging.level: error
name: app-server-ip
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;paths:定义了日志文件路径，可以采用模糊匹配模式，如*.log&lt;/li&gt;
&lt;li&gt;fields：topic 对应的消息字段或自定义增加的字段。&lt;/li&gt;
&lt;li&gt;output.kafka：filebeat 支持多种输出，支持向 kafka，logstash，elasticsearch 输出数据，此处设置数据输出到 kafka。&lt;/li&gt;
&lt;li&gt;enabled：这个启动这个模块。&lt;/li&gt;
&lt;li&gt;topic：指定要发送数据给 kafka 集群的哪个 topic，若指定的 topic 不存在，则会自动创建此 topic。&lt;/li&gt;
&lt;li&gt;version：指定 kafka 的版本。&lt;/li&gt;
&lt;li&gt;drop_fields：舍弃字段，filebeat 会 json 日志信息，适当舍弃无用字段节省空间资源。&lt;/li&gt;
&lt;li&gt;name：收集日志中对应主机的名字，建议 name 这里设置为 IP，便于区分多台主机的日志信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每一个不同的输出都用不同的配置项，还有很多功能性能的配置项，比如&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;合并规则&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;multiline:
        pattern: &#39;^[[:space:]]+(at|\.{3})\b|^Caused by:&#39;
        negate:  false
        match:   after
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用的合并规则&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[0-9]{4}-[0-9]{2}-[0-9]{2} 按照yyyy-mm-dd时间戳合并，日志不是以这个时间戳开头的都合并
[[0-9]{4}-[0-9]{2}-[0-9]{2} 按照[yyyy-mm-dd时间戳合并，日志不是以这个时间戳开头的都合并
* 不合并----不合并最后处理出来的结果就是没有这一段的配置
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;过滤规则&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;include_lines: [&amp;quot;FATAL&amp;quot;,&amp;quot;ERROR&amp;quot;,&amp;quot;WARN&amp;quot;,&amp;quot;INFO&amp;quot;,&amp;quot;fatal&amp;quot;,&amp;quot;error&amp;quot;,&amp;quot;warn&amp;quot;,&amp;quot;info&amp;quot;,&amp;quot;Fatal&amp;quot;,&amp;quot;Error&amp;quot;,&amp;quot;Warn&amp;quot;,&amp;quot;Info&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常用于日志级别的选择，比如上面只是采集包含这些字段的日志，其实也就是INFO级别的日志的采集。当然还是使用exclude_lines，不包含，和白名单黑名单一样的概念。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;资源限制&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;logging.level: debug    日志级别
max_procs: 2            cpu核数限制

queue:
      mem:
        events: 32768               pipeline队列长度
        flush.min_events: 1024
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;采集配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;close_eof: false
close_inactive: 5m（5分钟没有活跃，就会停止采集）
close_removed: false
close_renamed: false
ignore_older: 48h（即将开启的采集文件如果大于48H，就不要采集了）
# State options
clean_removed: true
clean_inactive: 72h（如果文件72h没有活跃就删除采集记录）
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;输出配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;output.kafka:
      topic: &amp;quot;%{[topic]}&amp;quot;
      version: &amp;quot;0.8.2.2&amp;quot;
      codec.format:
        ignoreNotFound: true
        string: &#39;%{[message]}&#39;
      metadata:
        retry.max: 2
        full: true
      worker: 10（The number of concurrent load-balanced Kafka output workers.）
      channel_buffer_size: 30000（每一个连接可以缓存消息的长度，默认是256）
      ##bulk_max_size: 20480（一次发送kafka请求最多的事件数量，默认是2048）
      #keep_alive: 0（是否保持连接，默认0，不保持）
      ##required_acks: 0（代表kafka是否需要等待回复，有1，0，-1，默认是1，需要等待主节点回复）
      compression: none（代表压缩级别，none代表不压缩，默认压缩是gzip）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多详细的配置可以去查看完整的配置文件说明。&lt;/p&gt;

&lt;h3 id=&#34;启动&#34;&gt;启动&lt;/h3&gt;

&lt;p&gt;调试模式下采用：终端启动（退出终端或 ctrl+c 会退出运行）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./filebeat -e -c filebeat.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;线上环境配合 error 级别使用：以后台守护进程启动启动 filebeats&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup ./filebeat -e -c filebeat.yml &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;零输出启动（不推荐）：将所有标准输出及标准错误输出到/dev/null空设备，即没有任何输出信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nohup ./filebeat -e -c filebeat.yml &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止运行 FileBeat 进程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ps -ef | grep filebeat
Kill -9 线程号
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;特性&#34;&gt;特性&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;采集路径可以使用正则表达式，来采集当前目录下所有的文件，包括子目录下，比如/k8s_log/*&lt;em&gt;/&lt;/em&gt;.log*&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;可以动态加载，可以在配置文件中配置reload的时间，filebeat本身自动加载，但是这个加载不能更新output&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filebeat本身日志支持备份切换，默认一个文件10M，保留8个文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filebaet支持句柄保持和checkpoint功能。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filebeat输出到kafka可以不支持多个kafka集群，可以改造多pipeline来发送到不同的Kafka集群。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;h3 id=&#34;filebeat创建一个beater&#34;&gt;filebeat创建一个beater&lt;/h3&gt;

&lt;p&gt;filebaet创建了一个beater实例启动&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.启动了一个Crawler，用于

&lt;ul&gt;
&lt;li&gt;1.启动静态的 input (写在主配置里的)&lt;/li&gt;
&lt;li&gt;2.启动 reloader，动态的 input 由 reloader 管理。&lt;/li&gt;
&lt;li&gt;3.启动Registrar&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2.每个input又启动了Harvester，Harvester就是负责采集日志&lt;/li&gt;
&lt;li&gt;3.Harvester 连接 pipeline 时，调用 outlet factory 创建一个 outleter，Outleter 封装了 pipeline 的 producer，调用 outleter OnEvent 方法发送数据到 pipeline&lt;/li&gt;
&lt;li&gt;4.Registrar 负责 checkpoint 文件的更新&lt;/li&gt;
&lt;li&gt;5.启动Pipeline 模块，Pipeline 是一个大的功能模块，包含 &lt;code&gt;queue&lt;/code&gt;, &lt;code&gt;outputController&lt;/code&gt;, &lt;code&gt;consumer&lt;/code&gt;, &lt;code&gt;output&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;1.Queue (memqueue)
真正的 queue 对象时 Broker。创建 broker 时启动事件循环，负责处理 publish 和 consume 等各种请求

&lt;ul&gt;
&lt;li&gt;创建 broker&lt;/li&gt;
&lt;li&gt;事件循环&lt;/li&gt;
&lt;li&gt;Consumer&lt;/li&gt;
&lt;li&gt;Producer 真实创建的是 ackProducer&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2.Output Controller
负责管理 consumer。Consumer 负责从 queue 中获取 batch，然后发送到 workQueue(chan publisher.Batch)。&lt;/li&gt;
&lt;li&gt;3.Consumer
Consumer 会启动多个 outputWorker。outputWorker 负责从 workQueue 获取 batch，然后调用 output client 的 Publish 方法发送出去。&lt;/li&gt;
&lt;li&gt;4.Retryer
Retry 负责重试发送失败的请求&lt;/li&gt;
&lt;li&gt;5.Output(kafka)
Connect 调用 sarama 库，创建一个 kafka AsyncProducer。连接时会启动两个循环，处理成功响应和失败响应。&lt;/li&gt;
&lt;li&gt;6.msgRef
msgRef 注入到发送给 AsyncProducer 的事件中。在处理响应的时候回调。如果成功就调用 batch.ACK()。失败就调用 batch.OnRetry 重试。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;核心代码模块&#34;&gt;核心代码模块&lt;/h3&gt;

&lt;h4 id=&#34;filebeat-1&#34;&gt;filebeat&lt;/h4&gt;

&lt;p&gt;beater 启动
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/beater/filebeat.go#L273&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/beater/filebeat.go#L273&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Crawler 启动时加载配置文件中的 inputs。启动 reloader，定期加载动态配置文件目录中的 inputs。启动Registrar，负责checkpoint
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/crawler/crawler.go#L33&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/crawler/crawler.go#L33&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动静态的 input (写在主配置里的)&lt;/li&gt;
&lt;li&gt;启动 reloader，动态的 input 由 reloader 管理&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;InputInput 是一个用来包装 &lt;code&gt;harvester&lt;/code&gt; 的数据结构，对外提供生命周期管理接口。Input 在建立起来时，会调用 pipeline 的 &lt;code&gt;ConnectWith&lt;/code&gt; 方法获取一个 client，用于发送 events。
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/input.go#L35&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/input.go#L35&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Harvester
负责采集日志
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/log/harvester.go#L88&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/input/log/harvester.go#L88&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Outleter：
Harvester 连接 pipeline 时，调用 outlet factory 创建一个 outleter
Outleter 封装了 pipeline 的 producer，调用 outleter OnEvent 方法发送数据到 pipeline&lt;/p&gt;

&lt;p&gt;Outleter 创建
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/channel/factory.go#L70&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/channel/factory.go#L70&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Registrar
负责 checkpoint 文件的更新
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/filebeat/registrar/registrar.go#L19&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/filebeat/registrar/registrar.go#L19&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;libbeat&#34;&gt;Libbeat&lt;/h4&gt;

&lt;p&gt;Pipeline 模块启动
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L30&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L30&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;加载 output &lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L85&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/module.go#L85&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Pipeline
Pipeline 包含 queue, output controller, consumer, output&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/pipeline.go#L135&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/pipeline.go#L135&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Output Controller

&lt;ul&gt;
&lt;li&gt;负责管理 consumer。Consumer 负责从 queue 中获取 batch，然后发送到 workQueue(chan publisher.Batch)。&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/controller.go#L13&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/controller.go#L13&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Consumer

&lt;ul&gt;
&lt;li&gt;Consumer 会启动多个 outputWorker。outputWorker 负责从 workQueue 获取 batch，然后调用 output client 的 Publish 方法发送出去。
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/consumer.go#L43&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/consumer.go#L43&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Retryer

&lt;ul&gt;
&lt;li&gt;Retry 负责重试发送失败的请求
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/retry.go#L14&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/pipeline/retry.go#L14&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Output(kafka)

&lt;ul&gt;
&lt;li&gt;创建 &lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/kafka.go#L114&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/kafka.go#L114&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Connect
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L68&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L68&lt;/a&gt;
调用 sarama 库，创建一个 kafka AsyncProducer。连接时会启动两个循环，处理成功响应和失败响应。&lt;/li&gt;
&lt;li&gt;msgRef
msgRef 注入到发送给 AsyncProducer 的事件中。在处理响应的时候回调。如果成功就调用 batch.ACK()。失败就调用 batch.OnRetry 重试。
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L222&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/outputs/kafka/client.go#L222&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Queue (memqueue)
真正的 queue 对象时 Broker。创建 broker 时启动事件循环，负责处理 publish 和 consume 等各种请求&lt;/li&gt;
&lt;li&gt;创建 broker
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/broker.go#L63&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/broker.go#L63&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;事件循环
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/eventloop.go#L30&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/eventloop.go#L30&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consumer
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/consume.go#L12&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/consume.go#L12&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Producer
真实创建的是 ackProducer
&lt;a href=&#34;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/produce.go#L39&#34;&gt;https://github.com/elastic/beats/blob/v6.3.2/libbeat/publisher/queue/memqueue/produce.go#L39&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;工作机制:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;队列容量为 &lt;code&gt;Events&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;当队列中的 events 数量大于 &lt;code&gt;FlushMinEvents&lt;/code&gt; 开始 flush&lt;/li&gt;
&lt;li&gt;当队列中有 events 并且离上一次 flush 过了 &lt;code&gt;FlushTimeout&lt;/code&gt; 时间，开始 flush&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这边简单梳理了filebeat的模块，核心的原理包括一些beats的原理可以看我写的另一篇&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/&#34;&gt;filebeat原理&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- Filebeat原理</title>
          <link>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/</link>
          <pubDate>Sun, 08 Jul 2018 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/</guid>
          <description>&lt;p&gt;Filebeat 是使用 Golang 实现的轻量型日志采集器，也是 Elasticsearch stack 里面的一员。本质上是一个 agent，可以安装在各个节点上，根据配置读取对应位置的日志，并上报到相应的地方去。&lt;/p&gt;

&lt;p&gt;filebeat源码归属于beats项目，而beats项目的设计初衷是为了采集各类的数据，所以beats抽象出了一个libbeat库，基于libbeat我们可以快速的开发实现一个采集的工具，除了filebeat，还有像metricbeat、packetbeat等官方的项目也是在beats工程中。libbeat已经实现了内存缓存队列memqueue、几种output日志发送客户端，数据的过滤处理processor,配置解析、日志打印、事件处理和发送等通用功能，而filebeat只需要实现日志文件的读取等和日志相关的逻辑即可。&lt;/p&gt;

&lt;h1 id=&#34;beats&#34;&gt;beats&lt;/h1&gt;

&lt;p&gt;对于任一种beats来说，主要逻辑都包含两个部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;收集数据并转换成事件&lt;/li&gt;
&lt;li&gt;发送事件到指定的输出&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中第二点已由libbeat实现，因此各个beats实际只需要关心如何收集数据并生成事件后发送给libbeat的Publisher。&lt;/p&gt;

&lt;h1 id=&#34;filebeat整体架构&#34;&gt;filebeat整体架构&lt;/h1&gt;

&lt;h2 id=&#34;架构图&#34;&gt;架构图&lt;/h2&gt;

&lt;p&gt;下图是 Filebeat 官方提供的架构图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/filebeat.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;下图是看代码的一些模块组合&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/filebeat1.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其实我个人觉得这一幅图是最形象的说明了filebeat的功能&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/filebeat2.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;模块&#34;&gt;模块&lt;/h2&gt;

&lt;p&gt;除了图中提到的各个模块，整个 filebeat 主要包含以下重要模块：&lt;/p&gt;

&lt;p&gt;1.filebeat主要模块&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Crawler: 负责管理和启动各个Input,管理所有Input收集数据并发送事件到libbeat的Publisher
Input: 负责管理和解析输入源的信息，以及为每个文件启动 Harvester。可由配置文件指定输入源信息。
    Harvester: 负责读取一个文件的数据,对应一个输入源，是收集数据的实际工作者。配置中，一个具体的Input可以包含多个输入源（Harvester）
module: 简化了一些常见程序日志（比如nginx日志）收集、解析、可视化（kibana dashboard）配置项
    fileset: module下具体的一种Input定义（比如nginx包括access和error log），包含：
        1）输入配置；
        2）es ingest node pipeline定义；
        3）事件字段定义；
        4）示例kibana dashboard
Registrar：接收libbeat反馈回来的ACK, 作相应的持久化，管理记录每个文件处理状态，包括偏移量、文件名等信息。当 Filebeat 启动时，会从 Registrar 恢复文件处理状态。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.libbeat主要模块&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Pipeline（publisher）: 负责管理缓存、Harvester 的信息写入以及 Output 的消费等，是 Filebeat 最核心的组件。
    client: 提供Publish接口让filebeat将事件发送到Publisher。在发送到队列之前，内部会先调用processors（包括input 内部的processors和全局processors）进行处理。
    processor: 事件处理器，可对事件按照配置中的条件进行各种处理（比如删除事件、保留指定字段，过滤添加字段，多行合并等）。配置项
    queue: 事件队列，有memqueue（基于内存）和spool（基于磁盘文件）两种实现。配置项
    outputs: 事件的输出端，比如ES、Logstash、kafka等。配置项
    acker: 事件确认回调，在事件发送成功后进行回调
autodiscover：用于自动发现容器并将其作为输入源
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;filebeat 的整个生命周期，几个组件共同协作，完成了日志从采集到上报的整个过程。&lt;/p&gt;

&lt;h1 id=&#34;基本原理-源码解析&#34;&gt;基本原理（源码解析）&lt;/h1&gt;

&lt;h2 id=&#34;文件目录组织&#34;&gt;文件目录组织&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;├── autodiscover        # 包含filebeat的autodiscover适配器（adapter），当autodiscover发现新容器时创建对应类型的输入
├── beater              # 包含与libbeat库交互相关的文件
├── channel             # 包含filebeat输出到pipeline相关的文件
├── config              # 包含filebeat配置结构和解析函数
├── crawler             # 包含Crawler结构和相关函数
├── fileset             # 包含module和fileset相关的结构
├── harvester           # 包含Harvester接口定义、Reader接口及实现等
├── input               # 包含所有输入类型的实现（比如: log, stdin, syslog）
├── inputsource         # 在syslog输入类型中用于读取tcp或udp syslog
├── module              # 包含各module和fileset配置
├── modules.d           # 包含各module对应的日志路径配置文件，用于修改默认路径
├── processor           # 用于从容器日志的事件字段source中提取容器id
├── prospector          # 包含旧版本的输入结构Prospector，现已被Input取代
├── registrar           # 包含Registrar结构和方法
└── util                # 包含beat事件和文件状态的通用结构Data
└── ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这些目录中还有一些重要的文件&lt;/p&gt;

&lt;p&gt;/beater：包含与libbeat库交互相关的文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acker.go: 包含在libbeat设置的ack回调函数，事件成功发送后被调用
channels.go: 包含在ack回调函数中被调用的记录者（logger），包括：
    registrarLogger: 将已确认事件写入registrar运行队列
    finishedLogger: 统计已确认事件数量
filebeat.go: 包含实现了beater接口的filebeat结构，接口函数包括：
    New：创建了filebeat实例
    Run：运行filebeat
    Stop: 停止filebeat运行
signalwait.go：基于channel实现的等待函数，在filebeat中用于：
    等待fileebat结束
    等待确认事件被写入registry文件
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/channel：filebeat输出（到pipeline）相关的文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;factory.go: 包含OutletFactory，用于创建输出器Outleter对象
interface.go: 定义输出接口Outleter
outlet.go: 实现Outleter，封装了libbeat的pipeline client，其在harvester中被调用用于将事件发送给pipeline
util.go: 定义ack回调的参数结构data，包含beat事件和文件状态
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/input：包含Input接口及各种输入类型的Input和Harvester实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input：对应配置中的一个Input项，同个Input下可包含多个输入源（比如文件）
Harvester：每个输入源对应一个Harvester，负责实际收集数据、并发送事件到pipeline
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/harvester：包含Harvester接口定义、Reader接口及实现等&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;forwarder.go: Forwarder结构（包含outlet）定义，用于转发事件
harvester.go: Harvester接口定义，具体实现则在/input目录下
registry.go: Registry结构，用于在Input中管理多个Harvester（输入源）的启动和停止
source.go: Source接口定义，表示输入源。目前仅有Pipe一种实现（包含os.File），用在log、stdin和docker输入类型中。btw，这三种输入类型都是用的log input的实现。
/reader目录: Reader接口定义和各种Reader实现
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;重要数据结构&#34;&gt;重要数据结构&lt;/h2&gt;

&lt;p&gt;beats通用事件结构(libbeat/beat/event.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Event struct {
    Timestamp time.Time     // 收集日志时记录的时间戳，对应es文档中的@timestamp字段
    Meta      common.MapStr // meta信息，outpus可选的将其作为事件字段输出。比如输出为es且指定了pipeline时，其pipeline id就被包含在此字段中
    Fields    common.MapStr // 默认输出字段定义在field.yml，其他字段可以在通过fields配置项指定
    Private   interface{} // for beats private use
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Crawler(filebeat/crawler/crawler.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Crawler 负责抓取日志并发送到libbeat pipeline
type Crawler struct {
    inputs          map[uint64]*input.Runner // 包含所有输入的runner
    inputConfigs    []*common.Config
    out             channel.Factory
    wg              sync.WaitGroup
    InputsFactory   cfgfile.RunnerFactory
    ModulesFactory  cfgfile.RunnerFactory
    modulesReloader *cfgfile.Reloader
    inputReloader   *cfgfile.Reloader
    once            bool
    beatVersion     string
    beatDone        chan struct{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;log类型Input(filebeat/input/log/input.go)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Input contains the input and its config
type Input struct {
    cfg           *common.Config
    config        config
    states        *file.States
    harvesters    *harvester.Registry   // 包含Input所有Harvester
    outlet        channel.Outleter      // Input共享的Publisher client
    stateOutlet   channel.Outleter
    done          chan struct{}
    numHarvesters atomic.Uint32
    meta          map[string]string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;log类型Harvester(filebeat/input/log/harvester.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Harvester struct {
    id     uuid.UUID
    config config
    source harvester.Source // the source being watched

    // shutdown handling
    done     chan struct{}
    stopOnce sync.Once
    stopWg   *sync.WaitGroup
    stopLock sync.Mutex

    // internal harvester state
    state  file.State
    states *file.States
    log    *Log

    // file reader pipeline
    reader          reader.Reader
    encodingFactory encoding.EncodingFactory
    encoding        encoding.Encoding

    // event/state publishing
    outletFactory OutletFactory
    publishState  func(*util.Data) bool

    onTerminate func()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Registrar(filebeat/registrar/registrar.go):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Registrar struct {
    Channel      chan []file.State
    out          successLogger
    done         chan struct{}
    registryFile string      // Path to the Registry File
    fileMode     os.FileMode // Permissions to apply on the Registry File
    wg           sync.WaitGroup

    states               *file.States // Map with all file paths inside and the corresponding state
    gcRequired           bool         // gcRequired is set if registry state needs to be gc&#39;ed before the next write
    gcEnabled            bool         // gcEnabled indictes the registry contains some state that can be gc&#39;ed in the future
    flushTimeout         time.Duration
    bufferedStateUpdates int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;libbeat Pipeline(libbeat/publisher/pipeline/pipeline.go)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Pipeline struct {
    beatInfo beat.Info

    logger *logp.Logger
    queue  queue.Queue
    output *outputController

    observer observer

    eventer pipelineEventer

    // wait close support
    waitCloseMode    WaitCloseMode
    waitCloseTimeout time.Duration
    waitCloser       *waitCloser

    // pipeline ack
    ackMode    pipelineACKMode
    ackActive  atomic.Bool
    ackDone    chan struct{}
    ackBuilder ackBuilder // pipelineEventsACK
    eventSema  *sema

    processors pipelineProcessors
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;启动&#34;&gt;启动&lt;/h2&gt;

&lt;p&gt;filebeat启动流程图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/f1.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每个 beat 的构建是独立的。从 filebeat 的入口文件filebeat/main.go可以看到，它向libbeat传递了名字、版本和构造函数来构造自身。跟着走到libbeat/beater/beater.go，我们可以看到程序的启动时的主要工作都是在这里完成的，包括命令行参数的处理、通用配置项的解析，以及最为重要的：调用象征一个beat的生命周期的若干方法&lt;/p&gt;

&lt;p&gt;我们来看filebeat的启动过程。&lt;/p&gt;

&lt;p&gt;1、执行root命令&lt;/p&gt;

&lt;p&gt;在filebeat/main.go文件中，main函数调用了cmd.RootCmd.Execute()，而RootCmd则是在cmd/root.go中被init函数初始化，其中就注册了filebeat.go:New函数以创建实现了beater接口的filebeat实例&lt;/p&gt;

&lt;p&gt;对于任意一个beats来说，都需要有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实现Beater接口的具体Beater（如Filebeat）;&lt;/li&gt;
&lt;li&gt;创建该具体Beater的(New)函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;beater接口定义（beat/beat.go）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Beater interface {
    // The main event loop. This method should block until signalled to stop by an
    // invocation of the Stop() method.
    Run(b *Beat) error

    // Stop is invoked to signal that the Run method should finish its execution.
    // It will be invoked at most once.
    Stop()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、初始化和运行Filebeat&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;创建libbeat/cmd/instance/beat.go:Beat结构&lt;/li&gt;
&lt;li&gt;执行(*Beat).launch方法

&lt;ul&gt;
&lt;li&gt;(*Beat).Init() 初始化Beat：加载beats公共config&lt;/li&gt;
&lt;li&gt;(*Beat).createBeater&lt;/li&gt;
&lt;li&gt;registerTemplateLoading: 当输出为es时，注册加载es模板的回调函数&lt;/li&gt;
&lt;li&gt;pipeline.Load: 创建Pipeline：包含队列、事件处理器、输出等&lt;/li&gt;
&lt;li&gt;setupMetrics: 安装监控&lt;/li&gt;
&lt;li&gt;filebeat.New: 解析配置(其中输入配置包括配置文件中的Input和module Input)等&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;loadDashboards 加载kibana dashboard&lt;/li&gt;
&lt;li&gt;(*Filebeat).Run: 运行filebeat&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、Filebeat运行&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;设置加载es pipeline的回调函数&lt;/li&gt;
&lt;li&gt;初始化registrar和crawler&lt;/li&gt;
&lt;li&gt;设置事件完成的回调函数&lt;/li&gt;
&lt;li&gt;启动Registrar、启动Crawler、启动Autodiscover&lt;/li&gt;
&lt;li&gt;等待filebeat运行结束&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们再重代码看一下这个启动过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;main.go

    package main
    import (
        &amp;quot;os&amp;quot;
        &amp;quot;github.com/elastic/beats/filebeat/cmd&amp;quot;
    )
    func main() {
        if err := cmd.RootCmd.Execute(); err != nil {
            os.Exit(1)
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入到filebeat/cmd执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package cmd

import (
    &amp;quot;flag&amp;quot;

    &amp;quot;github.com/spf13/pflag&amp;quot;

    &amp;quot;github.com/elastic/beats/filebeat/beater&amp;quot;

    cmd &amp;quot;github.com/elastic/beats/libbeat/cmd&amp;quot;
)

// Name of this beat
var Name = &amp;quot;filebeat&amp;quot;

// RootCmd to handle beats cli
var RootCmd *cmd.BeatsRootCmd

func init() {
    var runFlags = pflag.NewFlagSet(Name, pflag.ExitOnError)
    runFlags.AddGoFlag(flag.CommandLine.Lookup(&amp;quot;once&amp;quot;))
    runFlags.AddGoFlag(flag.CommandLine.Lookup(&amp;quot;modules&amp;quot;))

    RootCmd = cmd.GenRootCmdWithRunFlags(Name, &amp;quot;&amp;quot;, beater.New, runFlags)
    RootCmd.PersistentFlags().AddGoFlag(flag.CommandLine.Lookup(&amp;quot;M&amp;quot;))
    RootCmd.TestCmd.Flags().AddGoFlag(flag.CommandLine.Lookup(&amp;quot;modules&amp;quot;))
    RootCmd.SetupCmd.Flags().AddGoFlag(flag.CommandLine.Lookup(&amp;quot;modules&amp;quot;))
    RootCmd.AddCommand(cmd.GenModulesCmd(Name, &amp;quot;&amp;quot;, buildModulesManager))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RootCmd 在这一句初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RootCmd = cmd.GenRootCmdWithRunFlags(Name, &amp;quot;&amp;quot;, beater.New, runFlags)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;beater.New跟进去看到是filebeat.go，这个函数会在后面进行调用，来创建filebeat结构体，传递filebeat相关的配置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func New(b beat.Beat, rawConfig common.Config) (beat.Beater, error) {...}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在进入GenRootCmdWithRunFlags方法，一路跟进去到GenRootCmdWithSettings，真正的初始化是在这个方法里面。&lt;/p&gt;

&lt;p&gt;忽略前面的一段初始化值方法，看到RunCmd的初始化在：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rootCmd.RunCmd = genRunCmd(settings, beatCreator, runFlags)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入getRunCmd，看到执行代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err := instance.Run(settings, beatCreator)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;跟到\elastic\beats\libbeat\cmd\instance\beat.go的Run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b, err := NewBeat(name, idxPrefix, version)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里新建了beat结构体，同时将filebeat的New方法也传递了进来，就是参数beatCreator，我们可以看到在beat通过launch函数创建了filebeat结构体类型的beater&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return b.launch(settings, bt)---&amp;gt;beater, err := b.createBeater(bt)---&amp;gt;beater, err := bt(&amp;amp;b.Beat, sub)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入launch后，还做了很多的事情&lt;/p&gt;

&lt;p&gt;1、还初始化了配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err := b.InitWithSettings(settings)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、&lt;a href=&#34;#pipeline初始化&#34;&gt;pipeline的初始化&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pipeline, err := pipeline.Load(b.Info,
        pipeline.Monitors{
            Metrics:   reg,
            Telemetry: monitoring.GetNamespace(&amp;quot;state&amp;quot;).GetRegistry(),
            Logger:    logp.L().Named(&amp;quot;publisher&amp;quot;),
        },
        b.Config.Pipeline,
        b.processing,
        b.makeOutputFactory(b.Config.Output),
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在launch的末尾，还调用了beater启动方法，也就是filebeat的run函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return beater.Run(&amp;amp;b.Beat)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为启动的是filebeat，我们到filebeat.go的Run方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (fb *Filebeat) Run(b *beat.Beat) error {
       var err error
       config := fb.config

       if !fb.moduleRegistry.Empty() {
              err = fb.loadModulesPipelines(b)
              if err != nil {
                     return err
              }
       }

       waitFinished := newSignalWait()
       waitEvents := newSignalWait()

       // count active events for waiting on shutdown
       wgEvents := &amp;amp;eventCounter{
              count: monitoring.NewInt(nil, &amp;quot;filebeat.events.active&amp;quot;),
              added: monitoring.NewUint(nil, &amp;quot;filebeat.events.added&amp;quot;),
              done:  monitoring.NewUint(nil, &amp;quot;filebeat.events.done&amp;quot;),
       }
       finishedLogger := newFinishedLogger(wgEvents)

       // Setup registrar to persist state
       registrar, err := registrar.New(config.RegistryFile, config.RegistryFilePermissions, config.RegistryFlush, finishedLogger)
       if err != nil {
              logp.Err(&amp;quot;Could not init registrar: %v&amp;quot;, err)
              return err
       }

       // Make sure all events that were published in
       registrarChannel := newRegistrarLogger(registrar)

       err = b.Publisher.SetACKHandler(beat.PipelineACKHandler{
              ACKEvents: newEventACKer(finishedLogger, registrarChannel).ackEvents,
       })
       if err != nil {
              logp.Err(&amp;quot;Failed to install the registry with the publisher pipeline: %v&amp;quot;, err)
              return err
       }

       outDone := make(chan struct{}) // outDone closes down all active pipeline connections
       crawler, err := crawler.New(
              channel.NewOutletFactory(outDone, wgEvents).Create,
              config.Inputs,
              b.Info.Version,
              fb.done,
              *once)
       if err != nil {
              logp.Err(&amp;quot;Could not init crawler: %v&amp;quot;, err)
              return err
       }

       // The order of starting and stopping is important. Stopping is inverted to the starting order.
       // The current order is: registrar, publisher, spooler, crawler
       // That means, crawler is stopped first.

       // Start the registrar
       err = registrar.Start()
       if err != nil {
              return fmt.Errorf(&amp;quot;Could not start registrar: %v&amp;quot;, err)
       }

       // Stopping registrar will write last state
       defer registrar.Stop()

       // Stopping publisher (might potentially drop items)
       defer func() {
              // Closes first the registrar logger to make sure not more events arrive at the registrar
              // registrarChannel must be closed first to potentially unblock (pretty unlikely) the publisher
              registrarChannel.Close()
              close(outDone) // finally close all active connections to publisher pipeline
       }()

       // Wait for all events to be processed or timeout
       defer waitEvents.Wait()

       // Create a ES connection factory for dynamic modules pipeline loading
       var pipelineLoaderFactory fileset.PipelineLoaderFactory
       if b.Config.Output.Name() == &amp;quot;elasticsearch&amp;quot; {
              pipelineLoaderFactory = newPipelineLoaderFactory(b.Config.Output.Config())
       } else {
              logp.Warn(pipelinesWarning)
       }

       if config.OverwritePipelines {
              logp.Debug(&amp;quot;modules&amp;quot;, &amp;quot;Existing Ingest pipelines will be updated&amp;quot;)
       }

       err = crawler.Start(b.Publisher, registrar, config.ConfigInput, config.ConfigModules, pipelineLoaderFactory, config.OverwritePipelines)
       if err != nil {
              crawler.Stop()
              return err
       }

       // If run once, add crawler completion check as alternative to done signal
       if *once {
              runOnce := func() {
                     logp.Info(&amp;quot;Running filebeat once. Waiting for completion ...&amp;quot;)
                     crawler.WaitForCompletion()
                     logp.Info(&amp;quot;All data collection completed. Shutting down.&amp;quot;)
              }
              waitFinished.Add(runOnce)
       }

       // Register reloadable list of inputs and modules
       inputs := cfgfile.NewRunnerList(management.DebugK, crawler.InputsFactory, b.Publisher)
       reload.Register.MustRegisterList(&amp;quot;filebeat.inputs&amp;quot;, inputs)

       modules := cfgfile.NewRunnerList(management.DebugK, crawler.ModulesFactory, b.Publisher)
       reload.Register.MustRegisterList(&amp;quot;filebeat.modules&amp;quot;, modules)

       var adiscover *autodiscover.Autodiscover
       if fb.config.Autodiscover != nil {
              adapter := fbautodiscover.NewAutodiscoverAdapter(crawler.InputsFactory, crawler.ModulesFactory)
              adiscover, err = autodiscover.NewAutodiscover(&amp;quot;filebeat&amp;quot;, b.Publisher, adapter, config.Autodiscover)
              if err != nil {
                     return err
              }
       }
       adiscover.Start()

       // Add done channel to wait for shutdown signal
       waitFinished.AddChan(fb.done)
       waitFinished.Wait()

       // Stop reloadable lists, autodiscover -&amp;gt; Stop crawler -&amp;gt; stop inputs -&amp;gt; stop harvesters
       // Note: waiting for crawlers to stop here in order to install wgEvents.Wait
       //       after all events have been enqueued for publishing. Otherwise wgEvents.Wait
       //       or publisher might panic due to concurrent updates.
       inputs.Stop()
       modules.Stop()
       adiscover.Stop()
       crawler.Stop()

       timeout := fb.config.ShutdownTimeout
       // Checks if on shutdown it should wait for all events to be published
       waitPublished := fb.config.ShutdownTimeout &amp;gt; 0 || *once
       if waitPublished {
              // Wait for registrar to finish writing registry
              waitEvents.Add(withLog(wgEvents.Wait,
                     &amp;quot;Continue shutdown: All enqueued events being published.&amp;quot;))
              // Wait for either timeout or all events having been ACKed by outputs.
              if fb.config.ShutdownTimeout &amp;gt; 0 {
                     logp.Info(&amp;quot;Shutdown output timer started. Waiting for max %v.&amp;quot;, timeout)
                     waitEvents.Add(withLog(waitDuration(timeout),
                            &amp;quot;Continue shutdown: Time out waiting for events being published.&amp;quot;))
              } else {
                     waitEvents.AddChan(fb.done)
              }
       }

       return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构造了&lt;a href=&#34;#registry和ack-机制&#34;&gt;registrar&lt;/a&gt;和crawler，用于监控文件状态变更和数据采集。然后&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err = crawler.Start(b.Publisher, registrar, config.ConfigInput, config.ConfigModules, pipelineLoaderFactory, config.OverwritePipelines)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;crawler开始启动采集数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for _, inputConfig := range c.inputConfigs {
       err := c.startInput(pipeline, inputConfig, r.GetStates())
       if err != nil {
              return err
       }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;crawler的Start方法里面根据每个配置的输入调用一次startInput&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Crawler) startInput(
       pipeline beat.Pipeline,
       config *common.Config,
       states []file.State,
) error {
       if !config.Enabled() {
              return nil
       }

       connector := channel.ConnectTo(pipeline, c.out)
       p, err := input.New(config, connector, c.beatDone, states, nil)
       if err != nil {
              return fmt.Errorf(&amp;quot;Error in initing input: %s&amp;quot;, err)
       }
       p.Once = c.once

       if _, ok := c.inputs[p.ID]; ok {
              return fmt.Errorf(&amp;quot;Input with same ID already exists: %d&amp;quot;, p.ID)
       }

       c.inputs[p.ID] = p

       p.Start()

       return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据配置的input，构造log/input&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Input) Run() {
       logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Start next scan&amp;quot;)

       // TailFiles is like ignore_older = 1ns and only on startup
       if p.config.TailFiles {
              ignoreOlder := p.config.IgnoreOlder

              // Overwrite ignore_older for the first scan
              p.config.IgnoreOlder = 1
              defer func() {
                     // Reset ignore_older after first run
                     p.config.IgnoreOlder = ignoreOlder
                     // Disable tail_files after the first run
                     p.config.TailFiles = false
              }()
       }
       p.scan()

       // It is important that a first scan is run before cleanup to make sure all new states are read first
       if p.config.CleanInactive &amp;gt; 0 || p.config.CleanRemoved {
              beforeCount := p.states.Count()
              cleanedStates, pendingClean := p.states.Cleanup()
              logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;input states cleaned up. Before: %d, After: %d, Pending: %d&amp;quot;,
                     beforeCount, beforeCount-cleanedStates, pendingClean)
       }

       // Marking removed files to be cleaned up. Cleanup happens after next scan to make sure all states are updated first
       if p.config.CleanRemoved {
              for _, state := range p.states.GetStates() {
                     // os.Stat will return an error in case the file does not exist
                     stat, err := os.Stat(state.Source)
                     if err != nil {
                            if os.IsNotExist(err) {
                                   p.removeState(state)
                                   logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Remove state for file as file removed: %s&amp;quot;, state.Source)
                            } else {
                                   logp.Err(&amp;quot;input state for %s was not removed: %s&amp;quot;, state.Source, err)
                            }
                     } else {
                            // Check if existing source on disk and state are the same. Remove if not the case.
                            newState := file.NewState(stat, state.Source, p.config.Type, p.meta)
                            if !newState.FileStateOS.IsSame(state.FileStateOS) {
                                   p.removeState(state)
                                   logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Remove state for file as file removed or renamed: %s&amp;quot;, state.Source)
                            }
                     }
              }
       }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;input开始根据配置的输入路径扫描所有符合的文件，并启动harvester&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Input) scan() {
       var sortInfos []FileSortInfo
       var files []string

       paths := p.getFiles()

       var err error

       if p.config.ScanSort != &amp;quot;&amp;quot; {
              sortInfos, err = getSortedFiles(p.config.ScanOrder, p.config.ScanSort, getSortInfos(paths))
              if err != nil {
                     logp.Err(&amp;quot;Failed to sort files during scan due to error %s&amp;quot;, err)
              }
       }

       if sortInfos == nil {
              files = getKeys(paths)
       }

       for i := 0; i &amp;lt; len(paths); i++ {

              var path string
              var info os.FileInfo

              if sortInfos == nil {
                     path = files[i]
                     info = paths[path]
              } else {
                     path = sortInfos[i].path
                     info = sortInfos[i].info
              }

              select {
              case &amp;lt;-p.done:
                     logp.Info(&amp;quot;Scan aborted because input stopped.&amp;quot;)
                     return
              default:
              }

              newState, err := getFileState(path, info, p)
              if err != nil {
                     logp.Err(&amp;quot;Skipping file %s due to error %s&amp;quot;, path, err)
              }

              // Load last state
              lastState := p.states.FindPrevious(newState)

              // Ignores all files which fall under ignore_older
              if p.isIgnoreOlder(newState) {
                     err := p.handleIgnoreOlder(lastState, newState)
                     if err != nil {
                            logp.Err(&amp;quot;Updating ignore_older state error: %s&amp;quot;, err)
                     }
                     continue
              }

              // Decides if previous state exists
              if lastState.IsEmpty() {
                     logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Start harvester for new file: %s&amp;quot;, newState.Source)
                     err := p.startHarvester(newState, 0)
                     if err == errHarvesterLimit {
                            logp.Debug(&amp;quot;input&amp;quot;, harvesterErrMsg, newState.Source, err)
                            continue
                     }
                     if err != nil {
                            logp.Err(harvesterErrMsg, newState.Source, err)
                     }
              } else {
                     p.harvestExistingFile(newState, lastState)
              }
       }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在harvest的Run看到一个死循环读取message，预处理之后交由forwarder发送到目标输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;message, err := h.reader.Next()
h.sendEvent(data, forwarder)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，整个filebeat的启动到发送数据就理完了&lt;/p&gt;

&lt;h2 id=&#34;配置文件解析&#34;&gt;配置文件解析&lt;/h2&gt;

&lt;p&gt;在libbeat中实现了通用的配置文件解析，在启动的过程中，在每次createbeater时候就会进行config。&lt;/p&gt;

&lt;p&gt;调用 cfgfile.Load方法解析到cfg对象，进入load方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Load(path string, beatOverrides *common.Config) (*common.Config, error) {
       var config *common.Config
       var err error

       cfgpath := GetPathConfig()

       if path == &amp;quot;&amp;quot; {
              list := []string{}
              for _, cfg := range configfiles.List() {
                     if !filepath.IsAbs(cfg) {
                            list = append(list, filepath.Join(cfgpath, cfg))
                     } else {
                            list = append(list, cfg)
                     }
              }
              config, err = common.LoadFiles(list...)
       } else {
              if !filepath.IsAbs(path) {
                     path = filepath.Join(cfgpath, path)
              }
              config, err = common.LoadFile(path)
       }
       if err != nil {
              return nil, err
       }

       if beatOverrides != nil {
              config, err = common.MergeConfigs(
                     defaults,
                     beatOverrides,
                     config,
                     overwrites,
              )
              if err != nil {
                     return nil, err
              }
       } else {
              config, err = common.MergeConfigs(
                     defaults,
                     config,
                     overwrites,
              )
       }

       config.PrintDebugf(&amp;quot;Complete configuration loaded:&amp;quot;)
       return config, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果不输入配置文件，使用configfiles定义文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;configfiles = common.StringArrFlag(nil, &amp;quot;c&amp;quot;, &amp;quot;beat.yml&amp;quot;, &amp;quot;Configuration file, relative to path.config&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果输入配置文件进入else分支&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config, err = common.LoadFile(path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据配置文件构造config对象，使用的是yaml解析库。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c, err := yaml.NewConfigWithFile(path, configOpts...)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pipeline初始化&#34;&gt;pipeline初始化&lt;/h2&gt;

&lt;p&gt;pipeline的初始化是在libbeat的创建对于的filebeat 的结构体的时候进行的在func (b *Beat) createBeater(bt beat.Creator) (beat.Beater, error) {}函数中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pipeline, err := pipeline.Load(b.Info,
    pipeline.Monitors{
        Metrics:   reg,
        Telemetry: monitoring.GetNamespace(&amp;quot;state&amp;quot;).GetRegistry(),
        Logger:    logp.L().Named(&amp;quot;publisher&amp;quot;),
    },
    b.Config.Pipeline,
    b.processing,
    b.makeOutputFactory(b.Config.Output),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看load函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load uses a Config object to create a new complete Pipeline instance with
// configured queue and outputs.
func Load(
    beatInfo beat.Info,
    monitors Monitors,
    config Config,
    processors processing.Supporter,
    makeOutput func(outputs.Observer) (string, outputs.Group, error),
) (*Pipeline, error) {
    log := monitors.Logger
    if log == nil {
        log = logp.L()
    }

    if publishDisabled {
        log.Info(&amp;quot;Dry run mode. All output types except the file based one are disabled.&amp;quot;)
    }

    name := beatInfo.Name
    settings := Settings{
        WaitClose:     0,
        WaitCloseMode: NoWaitOnClose,
        Processors:    processors,
    }

    queueBuilder, err := createQueueBuilder(config.Queue, monitors)
    if err != nil {
        return nil, err
    }

    out, err := loadOutput(monitors, makeOutput)
    if err != nil {
        return nil, err
    }

    p, err := New(beatInfo, monitors, queueBuilder, out, settings)
    if err != nil {
        return nil, err
    }

    log.Infof(&amp;quot;Beat name: %s&amp;quot;, name)
    return p, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要是初始化queue，output，并创建对应的pipeline。&lt;/p&gt;

&lt;p&gt;1、queue&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;queueBuilder, err := createQueueBuilder(config.Queue, monitors)
    if err != nil {
        return nil, err
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入createQueueBuilder&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func createQueueBuilder(
    config common.ConfigNamespace,
    monitors Monitors,
) (func(queue.Eventer) (queue.Queue, error), error) {
    queueType := defaultQueueType
    if b := config.Name(); b != &amp;quot;&amp;quot; {
        queueType = b
    }

    queueFactory := queue.FindFactory(queueType)
    if queueFactory == nil {
        return nil, fmt.Errorf(&amp;quot;&#39;%v&#39; is no valid queue type&amp;quot;, queueType)
    }

    queueConfig := config.Config()
    if queueConfig == nil {
        queueConfig = common.NewConfig()
    }

    if monitors.Telemetry != nil {
        queueReg := monitors.Telemetry.NewRegistry(&amp;quot;queue&amp;quot;)
        monitoring.NewString(queueReg, &amp;quot;name&amp;quot;).Set(queueType)
    }

    return func(eventer queue.Eventer) (queue.Queue, error) {
        return queueFactory(eventer, monitors.Logger, queueConfig)
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据queueType（有默认类型mem）找到创建的方法，一般mem就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    queue.RegisterType(&amp;quot;mem&amp;quot;, create)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下create函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func create(eventer queue.Eventer, logger *logp.Logger, cfg *common.Config) (queue.Queue, error) {
    config := defaultConfig
    if err := cfg.Unpack(&amp;amp;config); err != nil {
        return nil, err
    }

    if logger == nil {
        logger = logp.L()
    }

    return NewBroker(logger, Settings{
        Eventer:        eventer,
        Events:         config.Events,
        FlushMinEvents: config.FlushMinEvents,
        FlushTimeout:   config.FlushTimeout,
    }), nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建了一个broker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewBroker creates a new broker based in-memory queue holding up to sz number of events.
// If waitOnClose is set to true, the broker will block on Close, until all internal
// workers handling incoming messages and ACKs have been shut down.
func NewBroker(
    logger logger,
    settings Settings,
) *Broker {
    // define internal channel size for producer/client requests
    // to the broker
    chanSize := 20

    var (
        sz           = settings.Events
        minEvents    = settings.FlushMinEvents
        flushTimeout = settings.FlushTimeout
    )

    if minEvents &amp;lt; 1 {
        minEvents = 1
    }
    if minEvents &amp;gt; 1 &amp;amp;&amp;amp; flushTimeout &amp;lt;= 0 {
        minEvents = 1
        flushTimeout = 0
    }
    if minEvents &amp;gt; sz {
        minEvents = sz
    }

    if logger == nil {
        logger = logp.NewLogger(&amp;quot;memqueue&amp;quot;)
    }

    b := &amp;amp;Broker{
        done:   make(chan struct{}),
        logger: logger,

        // broker API channels
        events:    make(chan pushRequest, chanSize),
        requests:  make(chan getRequest),
        pubCancel: make(chan producerCancelRequest, 5),

        // internal broker and ACK handler channels
        acks:          make(chan int),
        scheduledACKs: make(chan chanList),

        waitOnClose: settings.WaitOnClose,

        eventer: settings.Eventer,
    }

    var eventLoop interface {
        run()
        processACK(chanList, int)
    }

    if minEvents &amp;gt; 1 {
        eventLoop = newBufferingEventLoop(b, sz, minEvents, flushTimeout)
    } else {
        eventLoop = newDirectEventLoop(b, sz)
    }

    b.bufSize = sz
    ack := newACKLoop(b, eventLoop.processACK)

    b.wg.Add(2)
    go func() {
        defer b.wg.Done()
        eventLoop.run()
    }()
    go func() {
        defer b.wg.Done()
        ack.run()
    }()

    return b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;broker就是我们的queue，同时创建了一个eventLoop（根据是否有缓存创建不同的结构体，根据配置min_event是否大于1创建BufferingEventLoop或者DirectEventLoop，一般默认都是BufferingEventLoop，即带缓冲的队列。）和ack，调用他们的run函数进行监听&lt;/p&gt;

&lt;p&gt;这边特别说明一下eventLoop的new，我们看带缓存的newBufferingEventLoop&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBufferingEventLoop(b *Broker, size int, minEvents int, flushTimeout time.Duration) *bufferingEventLoop {
    l := &amp;amp;bufferingEventLoop{
        broker:       b,
        maxEvents:    size,
        minEvents:    minEvents,
        flushTimeout: flushTimeout,

        events:    b.events,
        get:       nil,
        pubCancel: b.pubCancel,
        acks:      b.acks,
    }
    l.buf = newBatchBuffer(l.minEvents)

    l.timer = time.NewTimer(flushTimeout)
    if !l.timer.Stop() {
        &amp;lt;-l.timer.C
    }

    return l
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把broker的值很多都赋给了bufferingEventLoop，不知道为什么这么做。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go func() {
    defer b.wg.Done()
    eventLoop.run()
}()
go func() {
    defer b.wg.Done()
    ack.run()
}()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下有缓存的事件处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) run() {
    var (
        broker = l.broker
    )

    for {
        select {
        case &amp;lt;-broker.done:
            return

        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)

        case req := &amp;lt;-l.pubCancel: // producer cancelling active events
            l.handleCancel(&amp;amp;req)

        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)

        case l.schedACKS &amp;lt;- l.pendingACKs:
            l.schedACKS = nil
            l.pendingACKs = chanList{}

        case count := &amp;lt;-l.acks:
            l.handleACK(count)

        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里就可以监听队列中的事件了，BufferingEventLoop是一个实现了Broker、带有各种channel的结构，主要用于将日志发送至consumer消费。 BufferingEventLoop的run方法中，同样是一个无限循环，这里可以认为是一个日志事件的调度中心。&lt;/p&gt;

&lt;p&gt;再来看看ack的调度&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *ackLoop) run() {
    var (
        // log = l.broker.logger

        // Buffer up acked event counter in acked. If acked &amp;gt; 0, acks will be set to
        // the broker.acks channel for sending the ACKs while potentially receiving
        // new batches from the broker event loop.
        // This concurrent bidirectionally communication pattern requiring &#39;select&#39;
        // ensures we can not have any deadlock between the event loop and the ack
        // loop, as the ack loop will not block on any channel
        acked int
        acks  chan int
    )

    for {
        select {
        case &amp;lt;-l.broker.done:
            // TODO: handle pending ACKs?
            // TODO: panic on pending batches?
            return

        case acks &amp;lt;- acked:
            acks, acked = nil, 0

        case lst := &amp;lt;-l.broker.scheduledACKs:
            count, events := lst.count()
            l.lst.concat(&amp;amp;lst)

            // log.Debug(&amp;quot;ACK List:&amp;quot;)
            // for current := l.lst.head; current != nil; current = current.next {
            //  log.Debugf(&amp;quot;  ack entry(seq=%v, start=%v, count=%v&amp;quot;,
            //      current.seq, current.start, current.count)
            // }

            l.batchesSched += uint64(count)
            l.totalSched += uint64(events)

        case &amp;lt;-l.sig:
            acked += l.handleBatchSig()
            if acked &amp;gt; 0 {
                acks = l.broker.acks
            }
        }

        // log.Debug(&amp;quot;ackloop INFO&amp;quot;)
        // log.Debug(&amp;quot;ackloop:   total events scheduled = &amp;quot;, l.totalSched)
        // log.Debug(&amp;quot;ackloop:   total events ack = &amp;quot;, l.totalACK)
        // log.Debug(&amp;quot;ackloop:   total batches scheduled = &amp;quot;, l.batchesSched)
        // log.Debug(&amp;quot;ackloop:   total batches ack = &amp;quot;, l.batchesACKed)

        l.sig = l.lst.channel()
        // if l.sig == nil {
        //  log.Debug(&amp;quot;ackloop: no ack scheduled&amp;quot;)
        // } else {
        //  log.Debug(&amp;quot;ackloop: schedule ack: &amp;quot;, l.lst.head.seq)
        // }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有处理信号就发送给regestry进行记录，关于&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/#registry和ack-机制&#34;&gt;registry在下面详细说明&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;2、output&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;out, err := loadOutput(monitors, makeOutput)
if err != nil {
    return nil, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入loadOutput&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func loadOutput(
    monitors Monitors,
    makeOutput OutputFactory,
) (outputs.Group, error) {
    log := monitors.Logger
    if log == nil {
        log = logp.L()
    }

    if publishDisabled {
        return outputs.Group{}, nil
    }

    if makeOutput == nil {
        return outputs.Group{}, nil
    }

    var (
        metrics  *monitoring.Registry
        outStats outputs.Observer
    )
    if monitors.Metrics != nil {
        metrics = monitors.Metrics.GetRegistry(&amp;quot;output&amp;quot;)
        if metrics != nil {
            metrics.Clear()
        } else {
            metrics = monitors.Metrics.NewRegistry(&amp;quot;output&amp;quot;)
        }
        outStats = outputs.NewStats(metrics)
    }

    outName, out, err := makeOutput(outStats)
    if err != nil {
        return outputs.Fail(err)
    }

    if metrics != nil {
        monitoring.NewString(metrics, &amp;quot;type&amp;quot;).Set(outName)
    }
    if monitors.Telemetry != nil {
        telemetry := monitors.Telemetry.GetRegistry(&amp;quot;output&amp;quot;)
        if telemetry != nil {
            telemetry.Clear()
        } else {
            telemetry = monitors.Telemetry.NewRegistry(&amp;quot;output&amp;quot;)
        }
        monitoring.NewString(telemetry, &amp;quot;name&amp;quot;).Set(outName)
    }

    return out, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边是根据load传进来的makeOutput函数来进行创建的，我们看一下load这个参数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Beat) makeOutputFactory(
    cfg common.ConfigNamespace,
) func(outputs.Observer) (string, outputs.Group, error) {
    return func(outStats outputs.Observer) (string, outputs.Group, error) {
        out, err := b.createOutput(outStats, cfg)
        return cfg.Name(), out, err
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建createOutput&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Beat) createOutput(stats outputs.Observer, cfg common.ConfigNamespace) (outputs.Group, error) {
    if !cfg.IsSet() {
        return outputs.Group{}, nil
    }

    return outputs.Load(b.IdxSupporter, b.Info, stats, cfg.Name(), cfg.Config())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看load&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load creates and configures a output Group using a configuration object..
func Load(
    im IndexManager,
    info beat.Info,
    stats Observer,
    name string,
    config *common.Config,
) (Group, error) {
    factory := FindFactory(name)
    if factory == nil {
        return Group{}, fmt.Errorf(&amp;quot;output type %v undefined&amp;quot;, name)
    }

    if stats == nil {
        stats = NewNilObserver()
    }
    return factory(im, info, stats, config)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见根据配置文件的配置的output的类型进行创建，比如我们用kafka做为output，我们看一下创建函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func init() {
    sarama.Logger = kafkaLogger{}

    outputs.RegisterType(&amp;quot;kafka&amp;quot;, makeKafka)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是makeKafka&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeKafka(
    _ outputs.IndexManager,
    beat beat.Info,
    observer outputs.Observer,
    cfg *common.Config,
) (outputs.Group, error) {
    debugf(&amp;quot;initialize kafka output&amp;quot;)

    config, err := readConfig(cfg)
    if err != nil {
        return outputs.Fail(err)
    }

    topic, err := outil.BuildSelectorFromConfig(cfg, outil.Settings{
        Key:              &amp;quot;topic&amp;quot;,
        MultiKey:         &amp;quot;topics&amp;quot;,
        EnableSingleOnly: true,
        FailEmpty:        true,
    })
    if err != nil {
        return outputs.Fail(err)
    }

    libCfg, err := newSaramaConfig(config)
    if err != nil {
        return outputs.Fail(err)
    }

    hosts, err := outputs.ReadHostList(cfg)
    if err != nil {
        return outputs.Fail(err)
    }

    codec, err := codec.CreateEncoder(beat, config.Codec)
    if err != nil {
        return outputs.Fail(err)
    }

    client, err := newKafkaClient(observer, hosts, beat.IndexPrefix, config.Key, topic, codec, libCfg)
    if err != nil {
        return outputs.Fail(err)
    }

    retry := 0
    if config.MaxRetries &amp;lt; 0 {
        retry = -1
    }
    return outputs.Success(config.BulkMaxSize, retry, client)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接创建了kafka的client给发送的时候使用。&lt;/p&gt;

&lt;p&gt;最后利用上面的两个构建函数来创建我们的pipeline&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p, err := New(beatInfo, monitors, queueBuilder, out, settings)
if err != nil {
    return nil, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实上面函数有的调用是在这个new中进行的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New create a new Pipeline instance from a queue instance and a set of outputs.
// The new pipeline will take ownership of queue and outputs. On Close, the
// queue and outputs will be closed.
func New(
    beat beat.Info,
    monitors Monitors,
    queueFactory queueFactory,
    out outputs.Group,
    settings Settings,
) (*Pipeline, error) {
    var err error

    if monitors.Logger == nil {
        monitors.Logger = logp.NewLogger(&amp;quot;publish&amp;quot;)
    }

    p := &amp;amp;Pipeline{
        beatInfo:         beat,
        monitors:         monitors,
        observer:         nilObserver,
        waitCloseMode:    settings.WaitCloseMode,
        waitCloseTimeout: settings.WaitClose,
        processors:       settings.Processors,
    }
    p.ackBuilder = &amp;amp;pipelineEmptyACK{p}
    p.ackActive = atomic.MakeBool(true)

    if monitors.Metrics != nil {
        p.observer = newMetricsObserver(monitors.Metrics)
    }
    p.eventer.observer = p.observer
    p.eventer.modifyable = true

    if settings.WaitCloseMode == WaitOnPipelineClose &amp;amp;&amp;amp; settings.WaitClose &amp;gt; 0 {
        p.waitCloser = &amp;amp;waitCloser{}

        // waitCloser decrements counter on queue ACK (not per client)
        p.eventer.waitClose = p.waitCloser
    }

    p.queue, err = queueFactory(&amp;amp;p.eventer)
    if err != nil {
        return nil, err
    }

    if count := p.queue.BufferConfig().Events; count &amp;gt; 0 {
        p.eventSema = newSema(count)
    }

    maxEvents := p.queue.BufferConfig().Events
    if maxEvents &amp;lt;= 0 {
        // Maximum number of events until acker starts blocking.
        // Only active if pipeline can drop events.
        maxEvents = 64000
    }
    p.eventSema = newSema(maxEvents)

    p.output = newOutputController(beat, monitors, p.observer, p.queue)
    p.output.Set(out)

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个output的控制器outputController&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newOutputController(
    beat beat.Info,
    monitors Monitors,
    observer outputObserver,
    b queue.Queue,
) *outputController {
    c := &amp;amp;outputController{
        beat:     beat,
        monitors: monitors,
        observer: observer,
        queue:    b,
    }

    ctx := &amp;amp;batchContext{}
    c.consumer = newEventConsumer(monitors.Logger, b, ctx)
    c.retryer = newRetryer(monitors.Logger, observer, nil, c.consumer)
    ctx.observer = observer
    ctx.retryer = c.retryer

    c.consumer.sigContinue()

    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时初始化了eventConsumer和retryer。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventConsumer(
    log *logp.Logger,
    queue queue.Queue,
    ctx *batchContext,
) *eventConsumer {
    c := &amp;amp;eventConsumer{
        logger: log,
        done:   make(chan struct{}),
        sig:    make(chan consumerSignal, 3),
        out:    nil,

        queue:    queue,
        consumer: queue.Consumer(),
        ctx:      ctx,
    }

    c.pause.Store(true)
    go c.loop(c.consumer)
    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在eventConsumer中启动了一个监听程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *eventConsumer) loop(consumer queue.Consumer) {
    log := c.logger

    log.Debug(&amp;quot;start pipeline event consumer&amp;quot;)

    var (
        out    workQueue
        batch  *Batch
        paused = true
    )

    handleSignal := func(sig consumerSignal) {
        switch sig.tag {
        case sigConsumerCheck:

        case sigConsumerUpdateOutput:
            c.out = sig.out

        case sigConsumerUpdateInput:
            consumer = sig.consumer
        }

        paused = c.paused()
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; batch != nil {
            out = c.out.workQueue
        } else {
            out = nil
        }
    }

    for {
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; consumer != nil &amp;amp;&amp;amp; batch == nil {
            out = c.out.workQueue
            queueBatch, err := consumer.Get(c.out.batchSize)
            if err != nil {
                out = nil
                consumer = nil
                continue
            }
            if queueBatch != nil {
                batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
            }

            paused = c.paused()
            if paused || batch == nil {
                out = nil
            }
        }

        select {
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
            continue
        default:
        }

        select {
        case &amp;lt;-c.done:
            log.Debug(&amp;quot;stop pipeline event consumer&amp;quot;)
            return
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
        case out &amp;lt;- batch:
            batch = nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用于消费队列中的事件event，并将其构建成Batch，放到处理队列中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBatch(ctx *batchContext, original queue.Batch, ttl int) *Batch {
    if original == nil {
        panic(&amp;quot;empty batch&amp;quot;)
    }

    b := batchPool.Get().(*Batch)
    *b = Batch{
        original: original,
        ctx:      ctx,
        ttl:      ttl,
        events:   original.Events(),
    }
    return b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看retryer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newRetryer(
    log *logp.Logger,
    observer outputObserver,
    out workQueue,
    c *eventConsumer,
) *retryer {
    r := &amp;amp;retryer{
        logger:     log,
        observer:   observer,
        done:       make(chan struct{}),
        sig:        make(chan retryerSignal, 3),
        in:         retryQueue(make(chan batchEvent, 3)),
        out:        out,
        consumer:   c,
        doneWaiter: sync.WaitGroup{},
    }
    r.doneWaiter.Add(1)
    go r.loop()
    return r
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样启动了监听程序，用于重试。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *retryer) loop() {
    defer r.doneWaiter.Done()
    var (
        out             workQueue
        consumerBlocked bool

        active     *Batch
        activeSize int
        buffer     []*Batch
        numOutputs int

        log = r.logger
    )

    for {
        select {
        case &amp;lt;-r.done:
            return
        case evt := &amp;lt;-r.in:
            var (
                countFailed  int
                countDropped int
                batch        = evt.batch
                countRetry   = len(batch.events)
            )

            if evt.tag == retryBatch {
                countFailed = len(batch.events)
                r.observer.eventsFailed(countFailed)

                decBatch(batch)

                countRetry = len(batch.events)
                countDropped = countFailed - countRetry
                r.observer.eventsDropped(countDropped)
            }

            if len(batch.events) == 0 {
                log.Info(&amp;quot;Drop batch&amp;quot;)
                batch.Drop()
            } else {
                out = r.out
                buffer = append(buffer, batch)
                out = r.out
                active = buffer[0]
                activeSize = len(active.events)
                if !consumerBlocked {
                    consumerBlocked = blockConsumer(numOutputs, len(buffer))
                    if consumerBlocked {
                        log.Info(&amp;quot;retryer: send wait signal to consumer&amp;quot;)
                        r.consumer.sigWait()
                        log.Info(&amp;quot;  done&amp;quot;)
                    }
                }
            }

        case out &amp;lt;- active:
            r.observer.eventsRetry(activeSize)

            buffer = buffer[1:]
            active, activeSize = nil, 0

            if len(buffer) == 0 {
                out = nil
            } else {
                active = buffer[0]
                activeSize = len(active.events)
            }

            if consumerBlocked {
                consumerBlocked = blockConsumer(numOutputs, len(buffer))
                if !consumerBlocked {
                    log.Info(&amp;quot;retryer: send unwait-signal to consumer&amp;quot;)
                    r.consumer.sigUnWait()
                    log.Info(&amp;quot;  done&amp;quot;)
                }
            }

        case sig := &amp;lt;-r.sig:
            switch sig.tag {
            case sigRetryerUpdateOutput:
                r.out = sig.channel
            case sigRetryerOutputAdded:
                numOutputs++
            case sigRetryerOutputRemoved:
                numOutputs--
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后对out进行了设置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *outputController) Set(outGrp outputs.Group) {
    // create new outputGroup with shared work queue
    clients := outGrp.Clients
    queue := makeWorkQueue()
    worker := make([]outputWorker, len(clients))
    for i, client := range clients {
        worker[i] = makeClientWorker(c.observer, queue, client)
    }
    grp := &amp;amp;outputGroup{
        workQueue:  queue,
        outputs:    worker,
        timeToLive: outGrp.Retry + 1,
        batchSize:  outGrp.BatchSize,
    }

    // update consumer and retryer
    c.consumer.sigPause()
    if c.out != nil {
        for range c.out.outputs {
            c.retryer.sigOutputRemoved()
        }
    }
    c.retryer.updOutput(queue)
    for range clients {
        c.retryer.sigOutputAdded()
    }
    c.consumer.updOutput(grp)

    // close old group, so events are send to new workQueue via retryer
    if c.out != nil {
        for _, w := range c.out.outputs {
            w.Close()
        }
    }

    c.out = grp

    // restart consumer (potentially blocked by retryer)
    c.consumer.sigContinue()

    c.observer.updateOutputGroup()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边就是对上面创建的kafka的每个client创建一个监控程序makeClientWorker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeClientWorker(observer outputObserver, qu workQueue, client outputs.Client) outputWorker {
    if nc, ok := client.(outputs.NetworkClient); ok {
        c := &amp;amp;netClientWorker{observer: observer, qu: qu, client: nc}
        go c.run()
        return c
    }
    c := &amp;amp;clientWorker{observer: observer, qu: qu, client: client}
    go c.run()
    return c
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就用于监控workQueue的数据，有数据就通过client的push发送到kafka，到这边pipeline的初始化也就结束了。&lt;/p&gt;

&lt;h2 id=&#34;日志收集&#34;&gt;日志收集&lt;/h2&gt;

&lt;p&gt;Filebeat 不仅支持普通文本日志的作为输入源，还内置支持了 redis 的慢查询日志、stdin、tcp 和 udp 等作为输入源。&lt;/p&gt;

&lt;p&gt;本文只分析下普通文本日志的处理方式，对于普通文本日志，可以按照以下配置方式，指定 log 的输入源信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/*.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 Input 也可以指定多个, 每个 Input 下的 Log 也可以指定多个。&lt;/p&gt;

&lt;p&gt;从收集日志、到发送事件到publisher，其数据流如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/collect.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;filebeat 启动时会开启 Crawler，filebeat抽象出一个Crawler的结构体，对于配置中的每条 Input，Crawler 都会启动一个 Input 进行处理，代码如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Crawler) Start(...){
    ...
    for _, inputConfig := range c.inputConfigs {
        err := c.startInput(pipeline, inputConfig, r.GetStates())
        if err != nil {
            return err
        }
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是创建input，比如我们采集的是log类型的，就是调用log的NewInput来创建，并且启动，定时进行扫描&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p, err := input.New(config, connector, c.beatDone, states, nil)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会根据采集日志的类型来进行注册调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewInput instantiates a new Log
func NewInput(
    cfg *common.Config,
    outlet channel.Connector,
    context input.Context,
) (input.Input, error) {
    cleanupNeeded := true
    cleanupIfNeeded := func(f func() error) {
        if cleanupNeeded {
            f()
        }
    }

    // Note: underlying output.
    //  The input and harvester do have different requirements
    //  on the timings the outlets must be closed/unblocked.
    //  The outlet generated here is the underlying outlet, only closed
    //  once all workers have been shut down.
    //  For state updates and events, separate sub-outlets will be used.
    out, err := outlet.ConnectWith(cfg, beat.ClientConfig{
        Processing: beat.ProcessingConfig{
            DynamicFields: context.DynamicFields,
        },
    })
    if err != nil {
        return nil, err
    }
    defer cleanupIfNeeded(out.Close)

    // stateOut will only be unblocked if the beat is shut down.
    // otherwise it can block on a full publisher pipeline, so state updates
    // can be forwarded correctly to the registrar.
    stateOut := channel.CloseOnSignal(channel.SubOutlet(out), context.BeatDone)
    defer cleanupIfNeeded(stateOut.Close)

    meta := context.Meta
    if len(meta) == 0 {
        meta = nil
    }

    p := &amp;amp;Input{
        config:      defaultConfig,
        cfg:         cfg,
        harvesters:  harvester.NewRegistry(),
        outlet:      out,
        stateOutlet: stateOut,
        states:      file.NewStates(),
        done:        context.Done,
        meta:        meta,
    }

    if err := cfg.Unpack(&amp;amp;p.config); err != nil {
        return nil, err
    }
    if err := p.config.resolveRecursiveGlobs(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to resolve recursive globs in config: %v&amp;quot;, err)
    }
    if err := p.config.normalizeGlobPatterns(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to normalize globs patterns: %v&amp;quot;, err)
    }

    // Create empty harvester to check if configs are fine
    // TODO: Do config validation instead
    _, err = p.createHarvester(file.State{}, nil)
    if err != nil {
        return nil, err
    }

    if len(p.config.Paths) == 0 {
        return nil, fmt.Errorf(&amp;quot;each input must have at least one path defined&amp;quot;)
    }

    err = p.loadStates(context.States)
    if err != nil {
        return nil, err
    }

    logp.Info(&amp;quot;Configured paths: %v&amp;quot;, p.config.Paths)

    cleanupNeeded = false
    go p.stopWhenDone()

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后会调用这个结构体的run函数进行扫描，主要是调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.scan()

// Scan starts a scanGlob for each provided path/glob
func (p *Input) scan() {
    var sortInfos []FileSortInfo
    var files []string

    paths := p.getFiles()

    var err error

    if p.config.ScanSort != &amp;quot;&amp;quot; {
        sortInfos, err = getSortedFiles(p.config.ScanOrder, p.config.ScanSort, getSortInfos(paths))
        if err != nil {
            logp.Err(&amp;quot;Failed to sort files during scan due to error %s&amp;quot;, err)
        }
    }

    if sortInfos == nil {
        files = getKeys(paths)
    }

    for i := 0; i &amp;lt; len(paths); i++ {

        var path string
        var info os.FileInfo

        if sortInfos == nil {
            path = files[i]
            info = paths[path]
        } else {
            path = sortInfos[i].path
            info = sortInfos[i].info
        }

        select {
        case &amp;lt;-p.done:
            logp.Info(&amp;quot;Scan aborted because input stopped.&amp;quot;)
            return
        default:
        }

        newState, err := getFileState(path, info, p)
        if err != nil {
            logp.Err(&amp;quot;Skipping file %s due to error %s&amp;quot;, path, err)
        }



        // Load last state
        lastState := p.states.FindPrevious(newState)

        // Ignores all files which fall under ignore_older
        if p.isIgnoreOlder(newState) {
            logp.Debug(&amp;quot;input&amp;quot;,&amp;quot;ignore&amp;quot;)
            err := p.handleIgnoreOlder(lastState, newState)
            if err != nil {
                logp.Err(&amp;quot;Updating ignore_older state error: %s&amp;quot;, err)
            }
            //close(p.done)
            continue
        }

        // Decides if previous state exists
        if lastState.IsEmpty() {
            logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Start harvester for new file: %s&amp;quot;, newState.Source)
            err := p.startHarvester(newState, 0)
            if err == errHarvesterLimit {
                logp.Debug(&amp;quot;input&amp;quot;, harvesterErrMsg, newState.Source, err)
                continue
            }
            if err != nil {
                logp.Err(harvesterErrMsg, newState.Source, err)
            }
        } else {
            p.harvestExistingFile(newState, lastState)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进行扫描过滤。由于指定的 paths 可以配置多个，而且可以是 Glob 类型，因此 Filebeat 将会匹配到多个配置文件。&lt;/p&gt;

&lt;p&gt;根据配置获取匹配的日志文件，需要注意的是，这里的匹配方式并非正则，而是采用linux glob的规则，和正则还是有一些区别。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;matches, err := filepath.Glob(path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取到了所有匹配的日志文件之后，会经过一些复杂的过滤，例如如果配置了exclude_files则会忽略这类文件，同时还会查询文件的状态，如果文件的最近一次修改时间大于ignore_older的配置，也会不去采集该文件。&lt;/p&gt;

&lt;p&gt;还会对文件进行处理，获取每个文件的状态，构建新的state结构，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;newState, err := getFileState(path, info, p)
if err != nil {
    logp.Err(&amp;quot;Skipping file %s due to error %s&amp;quot;, path, err)
}

func getFileState(path string, info os.FileInfo, p *Input) (file.State, error) {
    var err error
    var absolutePath string
    absolutePath, err = filepath.Abs(path)
    if err != nil {
        return file.State{}, fmt.Errorf(&amp;quot;could not fetch abs path for file %s: %s&amp;quot;, absolutePath, err)
    }
    logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Check file for harvesting: %s&amp;quot;, absolutePath)
    // Create new state for comparison
    newState := file.NewState(info, absolutePath, p.config.Type, p.meta, p.cfg.GetField(&amp;quot;brokerlist&amp;quot;), p.cfg.GetField(&amp;quot;topic&amp;quot;))
    return newState, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时在已经存在的状态中进行对比，如果获取到对于的状态就不重新启动协程进行采集，如果获取一个新的状态就开启新的协程进行采集。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load last state
lastState := p.states.FindPrevious(newState)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Input对象创建时会从registry读取文件状态(主要是offset)， 对于每个匹配到的文件，都会开启一个 Harvester 进行逐行读取，每个 Harvester 都工作在自己的的 goroutine 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err := p.startHarvester(newState, 0)
if err == errHarvesterLimit {
    logp.Debug(&amp;quot;input&amp;quot;, harvesterErrMsg, newState.Source, err)
    continue
}
if err != nil {
    logp.Err(harvesterErrMsg, newState.Source, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看看startHarvester&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// startHarvester starts a new harvester with the given offset
// In case the HarvesterLimit is reached, an error is returned
func (p *Input) startHarvester(state file.State, offset int64) error {
    if p.numHarvesters.Inc() &amp;gt; p.config.HarvesterLimit &amp;amp;&amp;amp; p.config.HarvesterLimit &amp;gt; 0 {
        p.numHarvesters.Dec()
        harvesterSkipped.Add(1)
        return errHarvesterLimit
    }
    // Set state to &amp;quot;not&amp;quot; finished to indicate that a harvester is running
    state.Finished = false
    state.Offset = offset

    // Create harvester with state
    h, err := p.createHarvester(state, func() { p.numHarvesters.Dec() })
    if err != nil {
        p.numHarvesters.Dec()
        return err
    }

    err = h.Setup()
    if err != nil {
        p.numHarvesters.Dec()
        return fmt.Errorf(&amp;quot;error setting up harvester: %s&amp;quot;, err)
    }

    // Update state before staring harvester
    // This makes sure the states is set to Finished: false
    // This is synchronous state update as part of the scan
    h.SendStateUpdate()

    if err = p.harvesters.Start(h); err != nil {
        p.numHarvesters.Dec()
    }
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先创建了Harvester&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// createHarvester creates a new harvester instance from the given state
func (p *Input) createHarvester(state file.State, onTerminate func()) (*Harvester, error) {
    // Each wraps the outlet, for closing the outlet individually
    h, err := NewHarvester(
        p.cfg,
        state,
        p.states,
        func(state file.State) bool {
            return p.stateOutlet.OnEvent(beat.Event{Private: state})
        },
        subOutletWrap(p.outlet),
    )
    if err == nil {
        h.onTerminate = onTerminate
    }
    return h, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;harvester启动时会通过Setup方法创建一系列reader形成读处理链&lt;/p&gt;

&lt;p&gt;关于log类型的reader处理链，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/read.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;opt表示根据配置决定是否创建该reader&lt;/p&gt;

&lt;p&gt;Reader包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Line: 包含os.File，用于从指定offset开始读取日志行。虽然位于处理链的最内部，但其Next函数中实际的处理逻辑（读文件行）却是最新被执行的。&lt;/li&gt;
&lt;li&gt;Encode: 包含Line Reader，将其读取到的行生成Message结构后返回&lt;/li&gt;
&lt;li&gt;JSON, DockerJSON: 将json形式的日志内容decode成字段&lt;/li&gt;
&lt;li&gt;StripNewLine：去除日志行尾部的空白符&lt;/li&gt;
&lt;li&gt;Multiline: 用于读取多行日志&lt;/li&gt;
&lt;li&gt;Limit: 限制单行日志字节数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了Line Reader外，这些reader都实现了Reader接口：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader interface {
    Next() (Message, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reader通过内部包含Reader对象的方式，使Reader形成一个处理链，其实这就是设计模式中的责任链模式。&lt;/p&gt;

&lt;p&gt;各Reader的Next方法的通用形式像是这样：Next方法调用内部Reader对象的Next方法获取Message，然后处理后返回。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *SomeReader) Next() (Message, error) {
    message, err := r.reader.Next()
    if err != nil {
        return message, err
    }

    // do some processing...

    return message, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实Harvester 的工作流程非常简单，harvester从registry记录的文件位置开始读取，就是逐行读取文件，并更新该文件暂时在 Input 中的文件偏移量（注意，并不是 Registrar 中的偏移量），读取完成（读到文件的EOF末尾），组装成事件（beat.Event）后发给Publisher。主要是调用了Harvester的run方法，部分如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    select {
    case &amp;lt;-h.done:
        return nil
    default:
    }

    message, err := h.reader.Next()
    if err != nil {
        switch err {
        case ErrFileTruncate:
            logp.Info(&amp;quot;File was truncated. Begin reading file from offset 0: %s&amp;quot;, h.state.Source)
            h.state.Offset = 0
            filesTruncated.Add(1)
        case ErrRemoved:
            logp.Info(&amp;quot;File was removed: %s. Closing because close_removed is enabled.&amp;quot;, h.state.Source)
        case ErrRenamed:
            logp.Info(&amp;quot;File was renamed: %s. Closing because close_renamed is enabled.&amp;quot;, h.state.Source)
        case ErrClosed:
            logp.Info(&amp;quot;Reader was closed: %s. Closing.&amp;quot;, h.state.Source)
        case io.EOF:
            logp.Info(&amp;quot;End of file reached: %s. Closing because close_eof is enabled.&amp;quot;, h.state.Source)
        case ErrInactive:
            logp.Info(&amp;quot;File is inactive: %s. Closing because close_inactive of %v reached.&amp;quot;, h.state.Source, h.config.CloseInactive)
        case reader.ErrLineUnparsable:
            logp.Info(&amp;quot;Skipping unparsable line in file: %v&amp;quot;, h.state.Source)
            //line unparsable, go to next line
            continue
        default:
            logp.Err(&amp;quot;Read line error: %v; File: %v&amp;quot;, err, h.state.Source)
        }
        return nil
    }

    // Get copy of state to work on
    // This is important in case sending is not successful so on shutdown
    // the old offset is reported
    state := h.getState()
    startingOffset := state.Offset
    state.Offset += int64(message.Bytes)

    // Stop harvester in case of an error
    if !h.onMessage(forwarder, state, message, startingOffset) {
        return nil
    }

    // Update state of harvester as successfully sent
    h.state = state
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，reader.Next()方法会不停的读取日志，如果没有返回异常，则发送日志数据到缓存队列中。&lt;/p&gt;

&lt;p&gt;返回的异常有几种类型，除了读取到EOF外，还会有例如文件一段时间不活跃等情况发生会使harvester goroutine退出，不再采集该文件，并关闭文件句柄。 filebeat为了防止占据过多的采集日志文件的文件句柄，默认的close_inactive参数为5min，如果日志文件5min内没有被修改，上面代码会进入ErrInactive的case，之后该harvester goroutine会被关闭。 这种场景下还需要注意的是，如果某个文件日志采集中被移除了，但是由于此时被filebeat保持着文件句柄，文件占据的磁盘空间会被保留直到harvester goroutine结束。&lt;/p&gt;

&lt;p&gt;不同的harvester goroutine采集到的日志数据都会发送至一个全局的队列queue中，queue的实现有两种：基于内存和基于磁盘的队列，目前基于磁盘的队列还是处于alpha阶段，filebeat默认启用的是基于内存的缓存队列。 每当队列中的数据缓存到一定的大小或者超过了定时的时间（默认1s)，会被注册的client从队列中消费，发送至配置的后端。目前可以设置的client有kafka、elasticsearch、redis等。&lt;/p&gt;

&lt;p&gt;同时，我们需要考虑到，日志型的数据其实是在不断增长和变化的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;会有新的日志在不断产生
可能一个日志文件对应的 Harvester 退出后，又再次有了内容更新。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了解决这两个情况，filebeat 采用了 Input 定时扫描的方式。代码如下，可以看出，Input 扫描的频率是由用户指定的 scan_frequency 配置来决定的 (默认 10s 扫描一次)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Runner) Run() {
    p.input.Run()

    if p.Once {
        return
    }

    for {
        select {
        case &amp;lt;-p.done:
            logp.Info(&amp;quot;input ticker stopped&amp;quot;)
            return
        case &amp;lt;-time.After(p.config.ScanFrequency): // 定时扫描
            logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;Run input&amp;quot;)
            p.input.Run()
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外，如果用户启动时指定了 –once 选项，则扫描只会进行一次，就退出了。&lt;/p&gt;

&lt;p&gt;使用一个简单的流程图可以这样表示&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/collect1.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;处理文件重命名，删除，截断&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取文件信息时会获取文件的device id + indoe作为文件的唯一标识;&lt;/li&gt;
&lt;li&gt;文件收集进度会被持久化，这样当创建Harvester时，首先会对文件作openFile, 以 device id + inode为key在持久化文件中查看当前文件是否被收集过，收集到了什么位置，然后断点续传&lt;/li&gt;
&lt;li&gt;在读取过程中，如果文件被截断，认为文件已经被同名覆盖，将从头开始读取文件&lt;/li&gt;
&lt;li&gt;如果文件被删除，因为原文件已被打开，不影响继续收集，但如果设置了CloseRemoved， 则不会再继续收集&lt;/li&gt;
&lt;li&gt;如果文件被重命名，因为原文件已被打开，不影响继续收集，但如果设置了CloseRenamed ， 则不会再继续收集&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;pipeline调度&#34;&gt;pipeline调度&lt;/h2&gt;

&lt;p&gt;至此，我们可以清楚的知道，Filebeat 是如何采集日志文件。而日志采集过程，Harvest 会将数据写到 Pipeline 中。我们接下来看下数据是如何写入到 Pipeline 中的。&lt;/p&gt;

&lt;p&gt;Haveseter 会将数据写入缓存中，而另一方面 Output 会从缓存将数据读走。整个生产消费的过程都是由 Pipeline 进行调度的，而整个调度过程也非常复杂。&lt;/p&gt;

&lt;p&gt;不同的harvester goroutine采集到的日志数据都会发送至一个全局的队列queue中，Filebeat 的缓存queue目前分为 memqueue 和 spool。memqueue 顾名思义就是内存缓存，spool 则是将数据缓存到磁盘中。本文将基于 memqueue 讲解整个调度过程。&lt;/p&gt;

&lt;p&gt;在下面的pipeline的写入和消费中，在client.go在(*client) publish方法中我们可以看到，事件是通过调用c.producer.Publish(pubEvent)被实际发送的，而producer则通过具体Queue的Producer方法生成。&lt;/p&gt;

&lt;p&gt;队列对象被包含在pipeline.go:Pipeline结构中，其接口的定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Queue interface {
    io.Closer
    BufferConfig() BufferConfig
    Producer(cfg ProducerConfig) Producer
    Consumer() Consumer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要的，Producer方法生成Producer对象，用于向队列中push事件；Consumer方法生成Consumer对象，用于从队列中取出事件。Producer和Consumer接口定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Producer interface {
    Publish(event publisher.Event) bool
    TryPublish(event publisher.Event) bool
    Cancel() int
}

type Consumer interface {
    Get(sz int) (Batch, error)
    Close() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在配置中没有指定队列配置时，默认使用了memqueue作为队列实现，下面我们来看看memqueue及其对应producer和consumer定义：&lt;/p&gt;

&lt;p&gt;Broker结构(memqueue在代码中实际对应的结构名是Broker)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Broker struct {
    done chan struct{}

    logger logger

    bufSize int
    // buf         brokerBuffer
    // minEvents   int
    // idleTimeout time.Duration

    // api channels
    events    chan pushRequest
    requests  chan getRequest
    pubCancel chan producerCancelRequest

    // internal channels
    acks          chan int
    scheduledACKs chan chanList

    eventer queue.Eventer

    // wait group for worker shutdown
    wg          sync.WaitGroup
    waitOnClose bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据是否需要ack分为forgetfullProducer和ackProducer两种producer：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type forgetfullProducer struct {
    broker    *Broker
    openState openState
}

type ackProducer struct {
    broker    *Broker
    cancel    bool
    seq       uint32
    state     produceState
    openState openState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;consumer结构:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type consumer struct {
    broker *Broker
    resp   chan getResponse

    done   chan struct{}
    closed atomic.Bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;queue、producer、consumer三者关系的运作方式如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/queue.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Producer通过Publish或TryPublish事件放入Broker的队列，即结构中的channel对象evetns&lt;/li&gt;
&lt;li&gt;Broker的主事件循环EventLoop将（请求）事件从events channel取出，放入自身结构体对象ringBuffer中。主事件循环有两种类型：

&lt;ul&gt;
&lt;li&gt;直接（不带buffer）事件循环结构directEventLoop：收到事件后尽可能快的转发；&lt;/li&gt;
&lt;li&gt;带buffer事件循环结构bufferingEventLoop：当buffer满或刷新超时时转发。具体使用哪一种取决于memqueue配置项flush.min_events，大于1时使用后者，否则使用前者。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;eventConsumer调用Consumer的Get方法获取事件：

&lt;ul&gt;
&lt;li&gt;首先将获取事件请求（包括请求事件数和用于存放其响应事件的channel resp）放入Broker的请求队列requests中，等待主事件循环EventLoop处理后将事件放入resp；&lt;/li&gt;
&lt;li&gt;获取resp的事件，组装成batch结构后返回&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;eventConsumer将事件放入output对应队列中&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;这部分关于事件在队列中各种channel间的流转，笔者认为是比较消耗性能的，但不清楚设计者这样设计的考量是什么。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;另外值得思考的是，在多个go routine使用队列交互的场景下，libbeat中都使用了go语言channel作为其底层的队列，它是否可以完全替代加锁队列的使用呢？&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;pipeline-的写入&#34;&gt;Pipeline 的写入&lt;/h3&gt;

&lt;p&gt;在Crawler收集日志并转换成事件后，我们继续发送数据，其就会通过调用Publisher对应client的Publish接口将事件送到Publisher，后续的处理流程也都将由libbeat完成，事件的流转如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/event.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们首先看一下事件处理器processor&lt;/p&gt;

&lt;p&gt;在harvester调用client.Publish接口时，其内部会使用配置中定义的processors对事件进行处理，然后才将事件发送到Publisher队列。&lt;/p&gt;

&lt;p&gt;processor包含两种：在Input内定义作为局部（Input独享）的processor，其只对该Input产生的事件生效；在顶层配置中定义作为全局processor，其对全部事件生效。其对应的代码实现方式是： filebeat在使用libbeat pipeline的ConnectWith接口创建client时（factory.go中(*OutletFactory)Create函数），会将Input内部的定义processor作为参数传递给ConnectWith接口。而在ConnectWith实现中，会将参数中的processor和全局processor（在创建pipeline时生成）合并。从这里读者也可以发现，实际上每个Input都独享一个client，其包含一些Input自身的配置定义逻辑。&lt;/p&gt;

&lt;p&gt;任一Processor都实现了Processor接口：Run函数包含处理逻辑，String返回Processor名。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Processor interface {
    Run(event *beat.Event) (*beat.Event, error)
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再看下 Haveseter 是如何将数据写入缓存中的，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/produce-to-pipeline.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Harvester 通过 pipeline 提供的 pipelineClient 将数据写入到 pipeline 中，Haveseter 会将读到的数据会包装成一个 Event 结构体，再递交给 pipeline。&lt;/p&gt;

&lt;p&gt;在 Filebeat 的实现中，pipelineClient 并不直接操作缓存，而是将 event 先写入一个 events channel 中。&lt;/p&gt;

&lt;p&gt;同时，有一个 eventloop 组件，会监听 events channel 的事件到来，等 event 到达时，eventloop 会将其放入缓存中。&lt;/p&gt;

&lt;p&gt;当缓存满的时候，eventloop 直接移除对该 channel 的监听。&lt;/p&gt;

&lt;p&gt;每次 event ACK 或者取消后，缓存不再满了，则 eventloop 会重新监听 events channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// onMessage processes a new message read from the reader.
// This results in a state update and possibly an event would be send.
// A state update first updates the in memory state held by the prospector,
// and finally sends the file.State indirectly to the registrar.
// The events Private field is used to forward the file state update.
//
// onMessage returns &#39;false&#39; if it was interrupted in the process of sending the event.
// This normally signals a harvester shutdown.
func (h *Harvester) onMessage(
    forwarder *harvester.Forwarder,
    state file.State,
    message reader.Message,
    messageOffset int64,
) bool {
    if h.source.HasState() {
        h.states.Update(state)
    }

    text := string(message.Content)
    if message.IsEmpty() || !h.shouldExportLine(text) {
        // No data or event is filtered out -&amp;gt; send empty event with state update
        // only. The call can fail on filebeat shutdown.
        // The event will be filtered out, but forwarded to the registry as is.
        err := forwarder.Send(beat.Event{Private: state})
        return err == nil
    }

    fields := common.MapStr{
        &amp;quot;log&amp;quot;: common.MapStr{
            &amp;quot;offset&amp;quot;: messageOffset, // Offset here is the offset before the starting char.
            &amp;quot;file&amp;quot;: common.MapStr{
                &amp;quot;path&amp;quot;: state.Source,
            },
        },
    }
    fields.DeepUpdate(message.Fields)

    // Check if json fields exist
    var jsonFields common.MapStr
    if f, ok := fields[&amp;quot;json&amp;quot;]; ok {
        jsonFields = f.(common.MapStr)
    }

    var meta common.MapStr
    timestamp := message.Ts
    if h.config.JSON != nil &amp;amp;&amp;amp; len(jsonFields) &amp;gt; 0 {
        id, ts := readjson.MergeJSONFields(fields, jsonFields, &amp;amp;text, *h.config.JSON)
        if !ts.IsZero() {
            // there was a `@timestamp` key in the event, so overwrite
            // the resulting timestamp
            timestamp = ts
        }

        if id != &amp;quot;&amp;quot; {
            meta = common.MapStr{
                &amp;quot;id&amp;quot;: id,
            }
        }
    } else if &amp;amp;text != nil {
        if fields == nil {
            fields = common.MapStr{}
        }
        fields[&amp;quot;message&amp;quot;] = text
    }

    err := forwarder.Send(beat.Event{
        Timestamp: timestamp,
        Fields:    fields,
        Meta:      meta,
        Private:   state,
    })
    return err == nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将数据包装成event直接通过send方法将数据发出去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Send updates the input state and sends the event to the spooler
// All state updates done by the input itself are synchronous to make sure no states are overwritten
func (f *Forwarder) Send(event beat.Event) error {
    ok := f.Outlet.OnEvent(event)
    if !ok {
        logp.Info(&amp;quot;Input outlet closed&amp;quot;)
        return errors.New(&amp;quot;input outlet closed&amp;quot;)
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用Outlet.OnEvent发送data&lt;/p&gt;

&lt;p&gt;点进去发现是一个接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Outlet interface {
       OnEvent(data *util.Data) bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过调试观察，elastic\beats\filebeat\channel\outlet.go实现了这个接口&lt;/p&gt;

&lt;p&gt;outlet在Harvester的run一开始就创建了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;outlet := channel.CloseOnSignal(h.outletFactory(), h.done)
forwarder := harvester.NewForwarder(outlet)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以调用的OnEvent&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (o *outlet) OnEvent(event beat.Event) bool {
    if !o.isOpen.Load() {
        return false
    }

    if o.wg != nil {
        o.wg.Add(1)
    }

    o.client.Publish(event)

    // Note: race condition on shutdown:
    //  The underlying beat.Client is asynchronous. Without proper ACK
    //  handler we can not tell if the event made it &#39;through&#39; or the client
    //  close has been completed before sending. In either case,
    //  we report &#39;false&#39; here, indicating the event eventually being dropped.
    //  Returning false here, prevents the harvester from updating the state
    //  to the most recently published events. Therefore, on shutdown the harvester
    //  might report an old/outdated state update to the registry, overwriting the
    //  most recently
    //  published offset in the registry on shutdown.
    return o.isOpen.Load()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过client.Publish发送数据，client也是一个接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Client interface {
       Publish(Event)
       PublishAll([]Event)
       Close() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调试之后，client使用的是elastic\beats\libbeat\publisher\pipeline\client.go的client对象&lt;/p&gt;

&lt;p&gt;我们来看一下这个client是通过Harvester的参数outletFactory来初始化的，我们来看一下NewHarvester初始化的时候也就是在createHarvester的时候传递的参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// createHarvester creates a new harvester instance from the given state
func (p *Input) createHarvester(state file.State, onTerminate func()) (*Harvester, error) {
    // Each wraps the outlet, for closing the outlet individually
    h, err := NewHarvester(
        p.cfg,
        state,
        p.states,
        func(state file.State) bool {
            return p.stateOutlet.OnEvent(beat.Event{Private: state})
        },
        subOutletWrap(p.outlet),
    )
    if err == nil {
        h.onTerminate = onTerminate
    }
    return h, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是subOutletWrap中的参数p.outlet那就要看以下Input初始化的的时候NewInput传递的参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewInput instantiates a new Log
func NewInput(
    cfg *common.Config,
    outlet channel.Connector,
    context input.Context,
) (input.Input, error) {
    cleanupNeeded := true
    cleanupIfNeeded := func(f func() error) {
        if cleanupNeeded {
            f()
        }
    }

    // Note: underlying output.
    //  The input and harvester do have different requirements
    //  on the timings the outlets must be closed/unblocked.
    //  The outlet generated here is the underlying outlet, only closed
    //  once all workers have been shut down.
    //  For state updates and events, separate sub-outlets will be used.
    out, err := outlet.ConnectWith(cfg, beat.ClientConfig{
        Processing: beat.ProcessingConfig{
            DynamicFields: context.DynamicFields,
        },
    })
    if err != nil {
        return nil, err
    }
    defer cleanupIfNeeded(out.Close)

    // stateOut will only be unblocked if the beat is shut down.
    // otherwise it can block on a full publisher pipeline, so state updates
    // can be forwarded correctly to the registrar.
    stateOut := channel.CloseOnSignal(channel.SubOutlet(out), context.BeatDone)
    defer cleanupIfNeeded(stateOut.Close)

    meta := context.Meta
    if len(meta) == 0 {
        meta = nil
    }

    p := &amp;amp;Input{
        config:      defaultConfig,
        cfg:         cfg,
        harvesters:  harvester.NewRegistry(),
        outlet:      out,
        stateOutlet: stateOut,
        states:      file.NewStates(),
        done:        context.Done,
        meta:        meta,
    }

    if err := cfg.Unpack(&amp;amp;p.config); err != nil {
        return nil, err
    }
    if err := p.config.resolveRecursiveGlobs(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to resolve recursive globs in config: %v&amp;quot;, err)
    }
    if err := p.config.normalizeGlobPatterns(); err != nil {
        return nil, fmt.Errorf(&amp;quot;Failed to normalize globs patterns: %v&amp;quot;, err)
    }

    // Create empty harvester to check if configs are fine
    // TODO: Do config validation instead
    _, err = p.createHarvester(file.State{}, nil)
    if err != nil {
        return nil, err
    }

    if len(p.config.Paths) == 0 {
        return nil, fmt.Errorf(&amp;quot;each input must have at least one path defined&amp;quot;)
    }

    err = p.loadStates(context.States)
    if err != nil {
        return nil, err
    }

    logp.Info(&amp;quot;Configured paths: %v&amp;quot;, p.config.Paths)

    cleanupNeeded = false
    go p.stopWhenDone()

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要看outlet，这个是在Crawler的startInput的时候进行初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Crawler) startInput(
    pipeline beat.Pipeline,
    config *common.Config,
    states []file.State,
) error {
    if !config.Enabled() {
        return nil
    }

    connector := c.out(pipeline)
    p, err := input.New(config, connector, c.beatDone, states, nil)
    if err != nil {
        return fmt.Errorf(&amp;quot;Error while initializing input: %s&amp;quot;, err)
    }
    p.Once = c.once

    if _, ok := c.inputs[p.ID]; ok {
        return fmt.Errorf(&amp;quot;Input with same ID already exists: %d&amp;quot;, p.ID)
    }

    c.inputs[p.ID] = p

    p.Start()

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看out就是crawler创建new的时候传递的值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;crawler, err := crawler.New(
        channel.NewOutletFactory(outDone, wgEvents, b.Info).Create,
        config.Inputs,
        b.Info.Version,
        fb.done,
        *once)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是create返回的pipelineConnector结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *OutletFactory) Create(p beat.Pipeline) Connector {
    return &amp;amp;pipelineConnector{parent: f, pipeline: p}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看pipelineConnector的ConnectWith函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *pipelineConnector) ConnectWith(cfg *common.Config, clientCfg beat.ClientConfig) (Outleter, error) {
    config := inputOutletConfig{}
    if err := cfg.Unpack(&amp;amp;config); err != nil {
        return nil, err
    }

    procs, err := processorsForConfig(c.parent.beatInfo, config, clientCfg)
    if err != nil {
        return nil, err
    }

    setOptional := func(to common.MapStr, key string, value string) {
        if value != &amp;quot;&amp;quot; {
            to.Put(key, value)
        }
    }

    meta := clientCfg.Processing.Meta.Clone()
    fields := clientCfg.Processing.Fields.Clone()

    serviceType := config.ServiceType
    if serviceType == &amp;quot;&amp;quot; {
        serviceType = config.Module
    }

    setOptional(meta, &amp;quot;pipeline&amp;quot;, config.Pipeline)
    setOptional(fields, &amp;quot;fileset.name&amp;quot;, config.Fileset)
    setOptional(fields, &amp;quot;service.type&amp;quot;, serviceType)
    setOptional(fields, &amp;quot;input.type&amp;quot;, config.Type)
    if config.Module != &amp;quot;&amp;quot; {
        event := common.MapStr{&amp;quot;module&amp;quot;: config.Module}
        if config.Fileset != &amp;quot;&amp;quot; {
            event[&amp;quot;dataset&amp;quot;] = config.Module + &amp;quot;.&amp;quot; + config.Fileset
        }
        fields[&amp;quot;event&amp;quot;] = event
    }

    mode := clientCfg.PublishMode
    if mode == beat.DefaultGuarantees {
        mode = beat.GuaranteedSend
    }

    // connect with updated configuration
    clientCfg.PublishMode = mode
    clientCfg.Processing.EventMetadata = config.EventMetadata
    clientCfg.Processing.Meta = meta
    clientCfg.Processing.Fields = fields
    clientCfg.Processing.Processor = procs
    clientCfg.Processing.KeepNull = config.KeepNull
    client, err := c.pipeline.ConnectWith(clientCfg)
    if err != nil {
        return nil, err
    }

    outlet := newOutlet(client, c.parent.wgEvents)
    if c.parent.done != nil {
        return CloseOnSignal(outlet, c.parent.done), nil
    }
    return outlet, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边获取到了pipeline的客户端client&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// ConnectWith create a new Client for publishing events to the pipeline.
// The client behavior on close and ACK handling can be configured by setting
// the appropriate fields in the passed ClientConfig.
// If not set otherwise the defaut publish mode is OutputChooses.
func (p *Pipeline) ConnectWith(cfg beat.ClientConfig) (beat.Client, error) {
    var (
        canDrop      bool
        dropOnCancel bool
        eventFlags   publisher.EventFlags
    )

    err := validateClientConfig(&amp;amp;cfg)
    if err != nil {
        return nil, err
    }

    p.eventer.mutex.Lock()
    p.eventer.modifyable = false
    p.eventer.mutex.Unlock()

    switch cfg.PublishMode {
    case beat.GuaranteedSend:
        eventFlags = publisher.GuaranteedSend
        dropOnCancel = true
    case beat.DropIfFull:
        canDrop = true
    }

    waitClose := cfg.WaitClose
    reportEvents := p.waitCloser != nil

    switch p.waitCloseMode {
    case NoWaitOnClose:

    case WaitOnClientClose:
        if waitClose &amp;lt;= 0 {
            waitClose = p.waitCloseTimeout
        }
    }

    processors, err := p.createEventProcessing(cfg.Processing, publishDisabled)
    if err != nil {
        return nil, err
    }

    client := &amp;amp;client{
        pipeline:     p,
        closeRef:     cfg.CloseRef,
        done:         make(chan struct{}),
        isOpen:       atomic.MakeBool(true),
        eventer:      cfg.Events,
        processors:   processors,
        eventFlags:   eventFlags,
        canDrop:      canDrop,
        reportEvents: reportEvents,
    }

    acker := p.makeACKer(processors != nil, &amp;amp;cfg, waitClose, client.unlink)
    producerCfg := queue.ProducerConfig{
        // Cancel events from queue if acker is configured
        // and no pipeline-wide ACK handler is registered.
        DropOnCancel: dropOnCancel &amp;amp;&amp;amp; acker != nil &amp;amp;&amp;amp; p.eventer.cb == nil,
    }

    if reportEvents || cfg.Events != nil {
        producerCfg.OnDrop = func(event beat.Event) {
            if cfg.Events != nil {
                cfg.Events.DroppedOnPublish(event)
            }
            if reportEvents {
                p.waitCloser.dec(1)
            }
        }
    }

    if acker != nil {
        producerCfg.ACK = acker.ackEvents
    } else {
        acker = newCloseACKer(nilACKer, client.unlink)
    }

    client.acker = acker
    client.producer = p.queue.Producer(producerCfg)

    p.observer.clientConnected()

    if client.closeRef != nil {
        p.registerSignalPropagation(client)
    }

    return client, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的就是client的Publish函数来发送数据，publish方法即发送日志的方法，如果需要在发送前改造日志格式，可在这里添加代码，如下面的解析日志代码。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) Publish(e beat.Event) {
    c.mutex.Lock()
    defer c.mutex.Unlock()

    c.publish(e)
}

func (c *client) publish(e beat.Event) {
    var (
        event   = &amp;amp;e
        publish = true
        log     = c.pipeline.monitors.Logger
    )

    c.onNewEvent()

    if !c.isOpen.Load() {
        // client is closing down -&amp;gt; report event as dropped and return
        c.onDroppedOnPublish(e)
        return
    }

    if c.processors != nil {
        var err error

        event, err = c.processors.Run(event)
        publish = event != nil
        if err != nil {
            // TODO: introduce dead-letter queue?

            log.Errorf(&amp;quot;Failed to publish event: %v&amp;quot;, err)
        }
    }

    if event != nil {
        e = *event
    }

    open := c.acker.addEvent(e, publish)
    if !open {
        // client is closing down -&amp;gt; report event as dropped and return
        c.onDroppedOnPublish(e)
        return
    }

    if !publish {
        c.onFilteredOut(e)
        return
    }

    e = *event
    pubEvent := publisher.Event{
        Content: e,
        Flags:   c.eventFlags,
    }

    if c.reportEvents {
        c.pipeline.waitCloser.inc()
    }

    var published bool
    if c.canDrop {
        published = c.producer.TryPublish(pubEvent)
    } else {
        published = c.producer.Publish(pubEvent)
    }

    if published {
        c.onPublished()
    } else {
        c.onDroppedOnPublish(e)
        if c.reportEvents {
            c.pipeline.waitCloser.dec(1)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上面创建clinet的时候，创建了队列的生产者，也就是之前broker的Producer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Broker) Producer(cfg queue.ProducerConfig) queue.Producer {
    return newProducer(b, cfg.ACK, cfg.OnDrop, cfg.DropOnCancel)
}

func newProducer(b *Broker, cb ackHandler, dropCB func(beat.Event), dropOnCancel bool) queue.Producer {
    openState := openState{
        log:    b.logger,
        isOpen: atomic.MakeBool(true),
        done:   make(chan struct{}),
        events: b.events,
    }

    if cb != nil {
        p := &amp;amp;ackProducer{broker: b, seq: 1, cancel: dropOnCancel, openState: openState}
        p.state.cb = cb
        p.state.dropCB = dropCB
        return p
    }
    return &amp;amp;forgetfulProducer{broker: b, openState: openState}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是forgetfulProducer结构体，调用这个的Publish函数来发送数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *forgetfulProducer) Publish(event publisher.Event) bool {
    return p.openState.publish(p.makeRequest(event))
}

func (st *openState) publish(req pushRequest) bool {
    select {
    case st.events &amp;lt;- req:
        return true
    case &amp;lt;-st.done:
        st.events = nil
        return false
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将数据放到了forgetfulProducer的openState的events中。到此数据就算发送到pipeline中了。&lt;/p&gt;

&lt;p&gt;上文在pipeline的初始化的时候，queue初始化一般默认都是BufferingEventLoop，即带缓冲的队列。BufferingEventLoop是一个实现了Broker、带有各种channel的结构，主要用于将日志发送至consumer消费。 BufferingEventLoop的run方法中，同样是一个无限循环，这里可以认为是一个日志事件的调度中心。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
        select {
        case &amp;lt;-broker.done:
            return
        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)
        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)
        case count := &amp;lt;-l.acks:
            l.handleACK(count)
        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上文中harvester goroutine每次读取到日志数据之后，最终会被发送至bufferingEventLoop中的events chan pushRequest 的channel中，然后触发上面req := &amp;lt;-l.events的case，handleInsert方法会把数据添加至bufferingEventLoop的buf中，buf即memqueue实际缓存日志数据的队列，如果buf长度超过配置的最大值或者bufferingEventLoop中的timer定时器（默认1S）触发了case &amp;lt;-l.idleC，均会调用flushBuffer()方法。
flushBuffer()又会触发req := &amp;lt;-l.get的case，然后运行handleConsumer方法，该方法中最重要的是这一句代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;req.resp &amp;lt;- getResponse{ackChan, events}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里获取到了consumer消费者的response channel，然后发送数据给这个channel。真正到这，才会触发consumer对memqueue的消费。所以，其实memqueue并非一直不停的在被consumer消费，而是在memqueue通知consumer的时候才被消费，我们可以理解为一种脉冲式的发送&lt;/p&gt;

&lt;p&gt;简单的来说就是，每当队列中的数据缓存到一定的大小或者超过了定时的时间（默认1s)，会被注册的client从队列中消费，发送至配置的后端。&lt;/p&gt;

&lt;p&gt;以上是 Pipeline 的写入过程，此时 event 已被写入到了缓存中。&lt;/p&gt;

&lt;p&gt;但是 Output 是如何从缓存中拿到 event 数据的？&lt;/p&gt;

&lt;h3 id=&#34;pipeline-的消费过程&#34;&gt;Pipeline 的消费过程&lt;/h3&gt;

&lt;p&gt;在上文已经提到过，filebeat初始化的时候，就已经创建了一个eventConsumer并在loop无限循环方法里试图从Broker中其实也就是上面的resp中获取日志数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; consumer != nil &amp;amp;&amp;amp; batch == nil {
            out = c.out.workQueue
            queueBatch, err := consumer.Get(c.out.batchSize)
            ...
            batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
        }
        ...
        select {
        case &amp;lt;-c.done:
            return
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
        case out &amp;lt;- batch:
            batch = nil
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面consumer.Get就是消费者consumer从Broker中获取日志数据，然后发送至out的channel中被output client发送，我们看一下Get方法里的核心代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select {
    case c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:
    case &amp;lt;-c.done:
        return nil, io.EOF
    }

    // if request has been send, we do have to wait for a response
    resp := &amp;lt;-c.resp
    return &amp;amp;batch{
        consumer: c,
        events:   resp.buf,
        ack:      resp.ack,
        state:    batchActive,
    }, nil
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;getRequest的结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type getRequest struct {
    sz   int              // request sz events from the broker
    resp chan getResponse // channel to send response to
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;getResponse的结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type getResponse struct {
    ack *ackChan
    buf []publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;getResponse里包含了日志的数据，而getRequest包含了一个发送至消费者的channel。
在上文bufferingEventLoop缓冲队列的handleConsumer方法里接收到的参数为getRequest，里面包含了consumer请求的getResponse channel。
如果handleConsumer不发送数据，consumer.Get方法会一直阻塞在select中，直到flushBuffer，consumer的getResponse channel才会接收到日志数据。&lt;/p&gt;

&lt;p&gt;我们来看看bufferingEventLoop的调度中心&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) run() {
    var (
        broker = l.broker
    )

    for {
        select {
        case &amp;lt;-broker.done:
            return

        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)

        case req := &amp;lt;-l.pubCancel: // producer cancelling active events
            l.handleCancel(&amp;amp;req)

        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)

        case l.schedACKS &amp;lt;- l.pendingACKs:
            l.schedACKS = nil
            l.pendingACKs = chanList{}

        case count := &amp;lt;-l.acks:
            l.handleACK(count)

        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}时候，l.get会得到信息，为什么l.get会得到信息，因为l.get = l.broker.requests&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) flushBuffer() {
    l.buf.flushed = true

    if l.buf.length() == 0 {
        panic(&amp;quot;flushing empty buffer&amp;quot;)
    }

    l.flushList.add(l.buf)
    l.get = l.broker.requests
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看handleConsumer如何处理信息的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) handleConsumer(req *getRequest) {
    buf := l.flushList.head
    if buf == nil {
        panic(&amp;quot;get from non-flushed buffers&amp;quot;)
    }

    count := buf.length()
    if count == 0 {
        panic(&amp;quot;empty buffer in flush list&amp;quot;)
    }

    if sz := req.sz; sz &amp;gt; 0 {
        if sz &amp;lt; count {
            count = sz
        }
    }

    if count == 0 {
        panic(&amp;quot;empty batch returned&amp;quot;)
    }

    events := buf.events[:count]
    clients := buf.clients[:count]
    ackChan := newACKChan(l.ackSeq, 0, count, clients)
    l.ackSeq++

    req.resp &amp;lt;- getResponse{ackChan, events}
    l.pendingACKs.append(ackChan)
    l.schedACKS = l.broker.scheduledACKs

    buf.events = buf.events[count:]
    buf.clients = buf.clients[count:]
    if buf.length() == 0 {
        l.advanceFlushList()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;处理req，并且将数据发送给req的resp，在发送的时候c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:就将c.resp赋值给了req的resp。所以可以获得返回值getResponse，组装成batch发送出去，其实就是放到type workQueue chan *Batch这个barch类型的channel中。&lt;/p&gt;

&lt;p&gt;看一下getResponse&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type getResponse struct {
    ack *ackChan
    buf []publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见ack就是channel，buf就是发送的日志。&lt;/p&gt;

&lt;p&gt;整个消费的过程非常复杂，数据会在多个 channel 之间传递流转，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/consume-from-pipeline.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;首先再介绍两个角色：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;consumer： pipeline 在创建的时候，会同时创建一个 consumer。consumer 负责从缓存中取数据
client worker：负责接收 consumer 传来的数据，并调用 Output 的 Publish 函数进行上报。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;与 producer 类似，consumer 也不直接操作缓存，而是会向 get channel 中写入消费请求。&lt;/p&gt;

&lt;p&gt;consumer 本身是个后台 loop 的过程，这个消费请求会不断进行。&lt;/p&gt;

&lt;p&gt;eventloop 监听 get channel, 拿到之后会从缓存中取数据。并将数据写入到 resp channel 中。&lt;/p&gt;

&lt;p&gt;consumer 从 resp channel 中拿到 event 数据后，又会将其写入到 workQueue。&lt;/p&gt;

&lt;p&gt;workQueue 也是个 channel。client worker 会监听该 channel 上的数据到来，将数据交给 Output client 进行 Publish 上报。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type workQueue chan *Batch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而且，Output 收到的是 Batch Events，即会一次收到一批 Events。BatchSize 由各个 Output 自行决定。&lt;/p&gt;

&lt;p&gt;至此，消息已经递交给了 Output 组件。&lt;/p&gt;

&lt;h3 id=&#34;output&#34;&gt;output&lt;/h3&gt;

&lt;p&gt;Filebeat 并不依赖于 Elasticsearch，可以单独存在。我们可以单独使用 Filebeat 进行日志的上报和搜集。filebeat 内置了常用的 Output 组件, 例如 kafka、Elasticsearch、redis 等。出于调试考虑，也可以输出到 console 和 file。我们可以利用现有的 Output 组件，将日志进行上报。&lt;/p&gt;

&lt;p&gt;当然，我们也可以自定义 Output 组件，让 Filebeat 将日志转发到我们想要的地方。&lt;/p&gt;

&lt;p&gt;在上文提到过，在pipeline初始化的时候，就会设置output的clinet，会创建一个clientWorker或者netClientWorker（可重连，默认就是这个），clientWorker的run方法中，会不停的从consumer发送的channel（就是上面的workQueue）里读取日志数据，然后调用client.Publish批量发送日志。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (w *netClientWorker) run() {
    for !w.closed.Load() {
        reconnectAttempts := 0

        // start initial connect loop from first batch, but return
        // batch to pipeline for other outputs to catch up while we&#39;re trying to connect
        for batch := range w.qu {
            batch.Cancelled()

            if w.closed.Load() {
                logp.Info(&amp;quot;Closed connection to %v&amp;quot;, w.client)
                return
            }

            if reconnectAttempts &amp;gt; 0 {
                logp.Info(&amp;quot;Attempting to reconnect to %v with %d reconnect attempt(s)&amp;quot;, w.client, reconnectAttempts)
            } else {
                logp.Info(&amp;quot;Connecting to %v&amp;quot;, w.client)
            }

            err := w.client.Connect()
            if err != nil {
                logp.Err(&amp;quot;Failed to connect to %v: %v&amp;quot;, w.client, err)
                reconnectAttempts++
                continue
            }

            logp.Info(&amp;quot;Connection to %v established&amp;quot;, w.client)
            reconnectAttempts = 0
            break
        }

        // send loop
        for batch := range w.qu {
            if w.closed.Load() {
                if batch != nil {
                    batch.Cancelled()
                }
                return
            }

            err := w.client.Publish(batch)
            if err != nil {
                logp.Err(&amp;quot;Failed to publish events: %v&amp;quot;, err)
                // on error return to connect loop
                break
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;libbeats库中包含了kafka、elasticsearch、logstash等几种client，它们均实现了client接口：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Client interface {
    Close() error
    Publish(publisher.Batch) error
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然最重要的是实现Publish接口，然后将日志发送出去。比如我们看一下kafka的Publish接口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) Publish(batch publisher.Batch) error {
    events := batch.Events()
    c.observer.NewBatch(len(events))

    ref := &amp;amp;msgRef{
        client: c,
        count:  int32(len(events)),
        total:  len(events),
        failed: nil,
        batch:  batch,
    }

    ch := c.producer.Input()
    for i := range events {
        d := &amp;amp;events[i]
        msg, err := c.getEventMessage(d)
        if err != nil {
            logp.Err(&amp;quot;Dropping event: %v&amp;quot;, err)
            ref.done()
            c.observer.Dropped(1)
            continue
        }

        msg.ref = ref
        msg.initProducerMessage()
        ch &amp;lt;- &amp;amp;msg.msg
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是基本的kafka客户端的使用方法，到此为止，数据也就发送的kakfa了。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;其实在pipleline调度的时候就说明了queue的生产消费的关系，数据在各个channel中进行传输，整个日志数据流转的过程还是表复杂的，在各个channel中进行流转，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/datastream.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;registry和ack-机制&#34;&gt;registry和Ack 机制&lt;/h2&gt;

&lt;p&gt;Filebeat 的可靠性很强，可以保证日志 At least once 的上报，同时也考虑了日志搜集中的各类问题，例如日志断点续读、文件名更改、日志 Truncated 等。&lt;/p&gt;

&lt;p&gt;filebeat 之所以可以保证日志可以 at least once 的上报，就是基于其 Ack 机制。&lt;/p&gt;

&lt;p&gt;简单来说，Ack 机制就是，当 Output Publish 成功之后会调用 ACK，最终 Registrar 会收到 ACK，并修改偏移量。&lt;/p&gt;

&lt;p&gt;而且, Registrar 只会在 Output 调用 batch 的相关信号时，才改变文件偏移量。其中 Batch 对外提供了这些信号：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Batch interface {
    Events() []Event

    // signals
    ACK()
    Drop()
    Retry()
    RetryEvents(events []Event)
    Cancelled()
    CancelledEvents(events []Event)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output 在 Publish 之后，无论失败，必须调用这些函数中的其中一个。&lt;/p&gt;

&lt;p&gt;以下是 Output Publish 成功后调用 Ack 的流程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/ack.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到其中起核心作用的组件是 Ackloop。AckLoop 中有一个 ackChanList，其中每一个 ackChan，对应于转发给 Output 的一个 Batch。
每次新建一个 Batch，同时会建立一个 ackChan，该 ackChan 会被 append 到 ackChanList 中。&lt;/p&gt;

&lt;p&gt;而 AckLoop 每次只监听处于 ackChanList 最头部的 ackChan。&lt;/p&gt;

&lt;p&gt;当 Batch 被 Output 调用 Ack 后，AckLoop 会收到对应 ackChan 上的事件，并将其最终转发给 Registrar。同时，ackChanList 将会 pop 头部的 ackChan，继续监听接下来的 Ack 事件。&lt;/p&gt;

&lt;p&gt;由于 FileBeat 是 At least once 的上报，但并不保证 Exactly once, 因此一条数据可能会被上报多次，所以接收端需要自行进行去重过滤。&lt;/p&gt;

&lt;p&gt;上面状态的修改，主要是filebeat维护了一个registry文件在本地的磁盘，该registry文件维护了所有已经采集的日志文件的状态。 实际上，每当日志数据发送至后端成功后，会返回ack事件。filebeat启动了一个独立的registry协程负责监听该事件，接收到ack事件后会将日志文件的State状态更新至registry文件中，State中的Offset表示读取到的文件偏移量，所以filebeat会保证Offset记录之前的日志数据肯定被后端的日志存储接收到。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;pipeline初始化的ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先是pipeline初始化只有一次，在这个时候只是简单的初始化了pipeline的ack相关信息，这边也创建的一个queue，原始是使用一个queue，这边初始化broker的时候会创建ack.run()来监听，后来改造多kafka发送后这一条queue
是不用的，而且每次连接kafka的时候创建一个新queue的时候，会都会创建一个ack.run()来监听，流程是一样的，改造可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/#支持多kafka的发送&#34;&gt;支持多kafka的发送&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.ackBuilder = &amp;amp;pipelineEmptyACK{p}
p.ackActive = atomic.MakeBool(true)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是在创建queue的时候，默认是使用mem的queue，会创建ack。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ack := newACKLoop(b, eventLoop.processACK)

b.wg.Add(2)
go func() {
    defer b.wg.Done()
    eventLoop.run()
}()
go func() {
    defer b.wg.Done()
    ack.run()
}()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在创建newBufferingEventLoop队列的同时，会newACKLoop并且调用相应结构体的run函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newACKLoop(b *Broker, processACK func(chanList, int)) *ackLoop {
    l := &amp;amp;ackLoop{broker: b}
    l.processACK = processACK
    return l
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ackLoop struct {
    broker *Broker
    sig    chan batchAckMsg
    lst    chanList

    totalACK   uint64
    totalSched uint64

    batchesSched uint64
    batchesACKed uint64

    processACK func(chanList, int)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看一下run函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *ackLoop) run() {
    var (
        // log = l.broker.logger

        // Buffer up acked event counter in acked. If acked &amp;gt; 0, acks will be set to
        // the broker.acks channel for sending the ACKs while potentially receiving
        // new batches from the broker event loop.
        // This concurrent bidirectionally communication pattern requiring &#39;select&#39;
        // ensures we can not have any deadlock between the event loop and the ack
        // loop, as the ack loop will not block on any channel
        acked int
        acks  chan int
    )

    for {
        select {
        case &amp;lt;-l.broker.done:
            // TODO: handle pending ACKs?
            // TODO: panic on pending batches?
            return

        case acks &amp;lt;- acked:
            acks, acked = nil, 0

        case lst := &amp;lt;-l.broker.scheduledACKs:
            count, events := lst.count()
            l.lst.concat(&amp;amp;lst)

            // log.Debug(&amp;quot;ACK List:&amp;quot;)
            // for current := l.lst.head; current != nil; current = current.next {
            //  log.Debugf(&amp;quot;  ack entry(seq=%v, start=%v, count=%v&amp;quot;,
            //      current.seq, current.start, current.count)
            // }

            l.batchesSched += uint64(count)
            l.totalSched += uint64(events)

        case &amp;lt;-l.sig:
            acked += l.handleBatchSig()
            if acked &amp;gt; 0 {
                acks = l.broker.acks
            }
        }

        // log.Debug(&amp;quot;ackloop INFO&amp;quot;)
        // log.Debug(&amp;quot;ackloop:   total events scheduled = &amp;quot;, l.totalSched)
        // log.Debug(&amp;quot;ackloop:   total events ack = &amp;quot;, l.totalACK)
        // log.Debug(&amp;quot;ackloop:   total batches scheduled = &amp;quot;, l.batchesSched)
        // log.Debug(&amp;quot;ackloop:   total batches ack = &amp;quot;, l.batchesACKed)

        l.sig = l.lst.channel()
        // if l.sig == nil {
        //  log.Debug(&amp;quot;ackloop: no ack scheduled&amp;quot;)
        // } else {
        //  log.Debug(&amp;quot;ackloop: schedule ack: &amp;quot;, l.lst.head.seq)
        // }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边其实就是对ack信号的调度处理中心。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;registrar初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后就是启动filebeat的时候可能是要初始化registrar&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Setup registrar to persist state
registrar, err := registrar.New(config.Registry, finishedLogger)
if err != nil {
    logp.Err(&amp;quot;Could not init registrar: %v&amp;quot;, err)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;config.Registry就是registry文件的配置信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Registry struct {
    Path         string        `config:&amp;quot;path&amp;quot;`
    Permissions  os.FileMode   `config:&amp;quot;file_permissions&amp;quot;`
    FlushTimeout time.Duration `config:&amp;quot;flush&amp;quot;`
    MigrateFile  string        `config:&amp;quot;migrate_file&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;构建方法很简单，就是对文件的一些描述赋值给了这个结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New creates a new Registrar instance, updating the registry file on
// `file.State` updates. New fails if the file can not be opened or created.
func New(cfg config.Registry, out successLogger) (*Registrar, error) {
    home := paths.Resolve(paths.Data, cfg.Path)
    migrateFile := cfg.MigrateFile
    if migrateFile != &amp;quot;&amp;quot; {
        migrateFile = paths.Resolve(paths.Data, migrateFile)
    }

    err := ensureCurrent(home, migrateFile, cfg.Permissions)
    if err != nil {
        return nil, err
    }

    dataFile := filepath.Join(home, &amp;quot;filebeat&amp;quot;, &amp;quot;data.json&amp;quot;)
    r := &amp;amp;Registrar{
        registryFile: dataFile,
        fileMode:     cfg.Permissions,
        done:         make(chan struct{}),
        states:       file.NewStates(),
        Channel:      make(chan []file.State, 1),
        flushTimeout: cfg.FlushTimeout,
        out:          out,
        wg:           sync.WaitGroup{},
    }
    return r, r.Init()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后返回Registrar结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Registrar struct {
    Channel      chan []file.State
    out          successLogger
    done         chan struct{}
    registryFile string      // Path to the Registry File
    fileMode     os.FileMode // Permissions to apply on the Registry File
    wg           sync.WaitGroup

    states               *file.States // Map with all file paths inside and the corresponding state
    gcRequired           bool         // gcRequired is set if registry state needs to be gc&#39;ed before the next write
    gcEnabled            bool         // gcEnabled indicates the registry contains some state that can be gc&#39;ed in the future
    flushTimeout         time.Duration
    bufferedStateUpdates int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还进行了初始化，主要是对文件进行了一些检查。然后就是启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Start the registrar
err = registrar.Start()
if err != nil {
    return fmt.Errorf(&amp;quot;Could not start registrar: %v&amp;quot;, err)
}

func (r *Registrar) Start() error {
    // Load the previous log file locations now, for use in input
    err := r.loadStates()
    if err != nil {
        return fmt.Errorf(&amp;quot;Error loading state: %v&amp;quot;, err)
    }

    r.wg.Add(1)
    go r.Run()

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先就是加载了目前存在的文件的状态来赋值给结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// loadStates fetches the previous reading state from the configure RegistryFile file
// The default file is `registry` in the data path.
func (r *Registrar) loadStates() error {
    f, err := os.Open(r.registryFile)
    if err != nil {
        return err
    }

    defer f.Close()

    logp.Info(&amp;quot;Loading registrar data from %s&amp;quot;, r.registryFile)

    states, err := readStatesFrom(f)
    if err != nil {
        return err
    }
    r.states.SetStates(states)
    logp.Info(&amp;quot;States Loaded from registrar: %+v&amp;quot;, len(states))

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后开始监听来更新文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Registrar) Run() {
    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Starting Registrar&amp;quot;)
    // Writes registry on shutdown
    defer func() {
        r.writeRegistry()
        r.wg.Done()
    }()

    var (
        timer  *time.Timer
        flushC &amp;lt;-chan time.Time
    )

    for {
        select {
        case &amp;lt;-r.done:
            logp.Info(&amp;quot;Ending Registrar&amp;quot;)
            return
        case &amp;lt;-flushC:
            flushC = nil
            timer.Stop()
            r.flushRegistry()
        case states := &amp;lt;-r.Channel:
            r.onEvents(states)
            if r.flushTimeout &amp;lt;= 0 {
                r.flushRegistry()
            } else if flushC == nil {
                timer = time.NewTimer(r.flushTimeout)
                flushC = timer.C
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当接受到Registrar中channel的发来的文件状态，就更新结构体的值，如果到时间了就将内存中的值刷新到本地文件中，如果没有就定一个timeout时间后刷新到本地文件中。&lt;/p&gt;

&lt;p&gt;我们可以简单的看一下这个channel&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Channel:      make(chan []file.State, 1),
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是一个文件状态的channel，关于文件状态的结构体如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// State is used to communicate the reading state of a file
type State struct {
    Id          string            `json:&amp;quot;-&amp;quot;` // local unique id to make comparison more efficient
    Finished    bool              `json:&amp;quot;-&amp;quot;` // harvester state
    Fileinfo    os.FileInfo       `json:&amp;quot;-&amp;quot;` // the file info
    Source      string            `json:&amp;quot;source&amp;quot;`
    Offset      int64             `json:&amp;quot;offset&amp;quot;`
    Timestamp   time.Time         `json:&amp;quot;timestamp&amp;quot;`
    TTL         time.Duration     `json:&amp;quot;ttl&amp;quot;`
    Type        string            `json:&amp;quot;type&amp;quot;`
    Meta        map[string]string `json:&amp;quot;meta&amp;quot;`
    FileStateOS file.StateOS
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记录在registry文件中的数据大致如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[{&amp;quot;source&amp;quot;:&amp;quot;/tmp/aa.log&amp;quot;,&amp;quot;offset&amp;quot;:48,&amp;quot;timestamp&amp;quot;:&amp;quot;2019-07-03T13:54:01.298995+08:00&amp;quot;,&amp;quot;ttl&amp;quot;:-1,&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;meta&amp;quot;:null,&amp;quot;FileStateOS&amp;quot;:{&amp;quot;inode&amp;quot;:7048952,&amp;quot;device&amp;quot;:16777220}}]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于文件可能会被改名或移动，filebeat会根据inode和设备号来标志每个日志文件。&lt;/p&gt;

&lt;p&gt;到这边registrar启动也结束了，下面就是监控registrar中channel的数据，在启动的时候还做了一件事情，那就是把channel设置到pipeline中去。&lt;/p&gt;

&lt;p&gt;在构建registrar的时候，通过registrar中channel构建一个结构体registrarLogger&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type registrarLogger struct {
    done chan struct{}
    ch   chan&amp;lt;- []file.State
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是用来交互的结构体,这个结构体中的channel获取的文件状态就是给上面的监听程序进行处理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Make sure all events that were published in
registrarChannel := newRegistrarLogger(registrar)

func newRegistrarLogger(reg *registrar.Registrar) *registrarLogger {
    return &amp;amp;registrarLogger{
        done: make(chan struct{}),
        ch:   reg.Channel,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过这个registrarLogger结构体，做了如下的调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;err = b.Publisher.SetACKHandler(beat.PipelineACKHandler{
    ACKEvents: newEventACKer(finishedLogger, registrarChannel).ackEvents,
})
if err != nil {
    logp.Err(&amp;quot;Failed to install the registry with the publisher pipeline: %v&amp;quot;, err)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先看一下newEventACKer(finishedLogger, registrarChannel)这个结构体的ackEvents函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACKer(stateless statelessLogger, stateful statefulLogger) *eventACKer {
    return &amp;amp;eventACKer{stateless: stateless, stateful: stateful, log: logp.NewLogger(&amp;quot;acker&amp;quot;)}
}

func (a *eventACKer) ackEvents(data []interface{}) {
    stateless := 0
    states := make([]file.State, 0, len(data))
    for _, datum := range data {
        if datum == nil {
            stateless++
            continue
        }

        st, ok := datum.(file.State)
        if !ok {
            stateless++
            continue
        }

        states = append(states, st)
    }

    if len(states) &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateful ack&amp;quot;, &amp;quot;count&amp;quot;, len(states))
        a.stateful.Published(states)
    }

    if stateless &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateless ack&amp;quot;, &amp;quot;count&amp;quot;, stateless)
        a.stateless.Published(stateless)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见是一个eventACKer结构体的函数赋值给了beat.PipelineACKHandler的成员函数，我们再来看一下beat.PipelineACKHandler&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// PipelineACKHandler configures some pipeline-wide event ACK handler.
type PipelineACKHandler struct {
    // ACKCount reports the number of published events recently acknowledged
    // by the pipeline.
    ACKCount func(int)

    // ACKEvents reports the events recently acknowledged by the pipeline.
    // Only the events &#39;Private&#39; field will be reported.
    ACKEvents func([]interface{})

    // ACKLastEvent reports the last ACKed event per pipeline client.
    // Only the events &#39;Private&#39; field will be reported.
    ACKLastEvents func([]interface{})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是一个这样的结构体作为参数，最后我们来看一下SetACKHandler这个函数的调用，首先b的就是libbeat中创建的beat，其中的Publisher就是对应的初始化的Pipeline，看一下Pipeline的SetACKHandler方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// SetACKHandler sets a global ACK handler on all events published to the pipeline.
// SetACKHandler must be called before any connection is made.
func (p *Pipeline) SetACKHandler(handler beat.PipelineACKHandler) error {
    p.eventer.mutex.Lock()
    defer p.eventer.mutex.Unlock()

    if !p.eventer.modifyable {
        return errors.New(&amp;quot;can not set ack handler on already active pipeline&amp;quot;)
    }

    // TODO: check only one type being configured

    cb, err := newPipelineEventCB(handler)
    if err != nil {
        return err
    }

    if cb == nil {
        p.ackBuilder = &amp;amp;pipelineEmptyACK{p}
        p.eventer.cb = nil
        return nil
    }

    p.eventer.cb = cb
    if cb.mode == countACKMode {
        p.ackBuilder = &amp;amp;pipelineCountACK{
            pipeline: p,
            cb:       cb.onCounts,
        }
    } else {
        p.ackBuilder = &amp;amp;pipelineEventsACK{
            pipeline: p,
            cb:       cb.onEvents,
        }
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;newPipelineEventCB是根据传递的不同函数，创建不同mode的pipelineEventCB结构体，启动goroutine来work。我们这边传递的是ACKEvents，设置mode为eventsACKMode&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newPipelineEventCB(handler beat.PipelineACKHandler) (*pipelineEventCB, error) {
    mode := noACKMode
    if handler.ACKCount != nil {
        mode = countACKMode
    }
    if handler.ACKEvents != nil {
        if mode != noACKMode {
            return nil, errors.New(&amp;quot;only one callback can be set&amp;quot;)
        }
        mode = eventsACKMode
    }
    if handler.ACKLastEvents != nil {
        if mode != noACKMode {
            return nil, errors.New(&amp;quot;only one callback can be set&amp;quot;)
        }
        mode = lastEventsACKMode
    }

    // yay, no work
    if mode == noACKMode {
        return nil, nil
    }

    cb := &amp;amp;pipelineEventCB{
        acks:          make(chan int),
        mode:          mode,
        handler:       handler,
        events:        make(chan eventsDataMsg),
        droppedEvents: make(chan eventsDataMsg),
    }
    go cb.worker()
    return cb, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看看worker工作协程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) worker() {
    defer close(p.acks)
    defer close(p.events)
    defer close(p.droppedEvents)

    for {
        select {
        case count := &amp;lt;-p.acks:
            exit := p.collect(count)
            if exit {
                return
            }

            // short circuit dropped events, but have client block until all events
            // have been processed by pipeline ack handler
        case msg := &amp;lt;-p.droppedEvents:
            p.reportEventsData(msg.data, msg.total)
            if msg.sig != nil {
                close(msg.sig)
            }

        case &amp;lt;-p.done:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时对于不同的mode对p.ackBuilder进行了重新构建，因为是代码设置mode为eventsACKMode&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.ackBuilder = &amp;amp;pipelineEventsACK{
    pipeline: p,
    cb:       cb.onEvents,
}

type pipelineEventsACK struct {
    pipeline *Pipeline
    cb       func([]interface{}, int)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里启动就结束了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;input初始化的ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;crawler启动后构建新的input构建的时候，需要获取到pipeline的client，在使用ConnectWith进行构建的时候，会构建client的acker，第一次参数是processors != nil，影响后的结构体的创建，一般是true&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acker := p.makeACKer(processors != nil, &amp;amp;cfg, waitClose, client.unlink)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下makeACKer这个函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *Pipeline) makeACKer(
    canDrop bool,
    cfg *beat.ClientConfig,
    waitClose time.Duration,
    afterClose func(),
) acker {
    var (
        bld   = p.ackBuilder
        acker acker
    )

    sema := p.eventSema
    switch {
    case cfg.ACKCount != nil:
        acker = bld.createCountACKer(canDrop, sema, cfg.ACKCount)
    case cfg.ACKEvents != nil:
        acker = bld.createEventACKer(canDrop, sema, cfg.ACKEvents)
    case cfg.ACKLastEvent != nil:
        cb := lastEventACK(cfg.ACKLastEvent)
        acker = bld.createEventACKer(canDrop, sema, cb)
    default:
        if waitClose &amp;lt;= 0 {
            acker = bld.createPipelineACKer(canDrop, sema)
        } else {
            acker = bld.createCountACKer(canDrop, sema, func(_ int) {})
        }
    }

    if waitClose &amp;lt;= 0 {
        return newCloseACKer(acker, afterClose)
    }
    return newWaitACK(acker, waitClose, afterClose)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要使用p.ackBuilder的create函数，我们在上面SetACKHandler的时候构建了p.ackBuilder，根据cfg配置调用，默认调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acker = bld.createEventACKer(canDrop, sema, cfg.ACKEvents)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *pipelineEventsACK) createEventACKer(canDrop bool, sema *sema, fn func([]interface{})) acker {
    return buildClientEventACK(b.pipeline, canDrop, sema, func(guard *clientACKer) func([]interface{}, int) {
        return func(data []interface{}, acked int) {
            b.cb(data, acked)
            if guard.Active() {
                fn(data)
            }
        }
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用函数buildClientEventACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func buildClientEventACK(
    pipeline *Pipeline,
    canDrop bool,
    sema *sema,
    mk func(*clientACKer) func([]interface{}, int),
) acker {
    guard := &amp;amp;clientACKer{}
    guard.lift(newEventACK(pipeline, canDrop, sema, mk(guard)))
    return guard
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下返回值clientACKer结构体，其成员acker的赋值就是eventDataACK。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACK(
    pipeline *Pipeline,
    canDrop bool,
    sema *sema,
    fn func([]interface{}, int),
) *eventDataACK {
    a := &amp;amp;eventDataACK{pipeline: pipeline, fn: fn}
    a.acker = makeCountACK(pipeline, canDrop, sema, a.onACK)

    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建一个eventDataACK的结构体，fn就是mk(guard)就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func(data []interface{}, acked int) {
    b.cb(data, acked)
    if guard.Active() {
        fn(data)
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后调用makeCountACK来赋值给eventDataACK的acker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeCountACK(pipeline *Pipeline, canDrop bool, sema *sema, fn func(int, int)) acker {
    if canDrop {
        return newBoundGapCountACK(pipeline, sema, fn)
    }
    return newCountACK(pipeline, fn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;canDrop之前说过了，就是ture，所以创建newBoundGapCountACK，将eventDataACK的onACK当参数传递进来。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *eventDataACK) onACK(total, acked int) {
    n := total

    a.mutex.Lock()
    data := a.data[:n]
    a.data = a.data[n:]
    a.mutex.Unlock()

    if len(data) &amp;gt; 0 &amp;amp;&amp;amp; a.pipeline.ackActive.Load() {
        a.fn(data, acked)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;继续newBoundGapCountACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBoundGapCountACK(
    pipeline *Pipeline,
    sema *sema,
    fn func(total, acked int),
) *boundGapCountACK {
    a := &amp;amp;boundGapCountACK{active: true, sema: sema, fn: fn}
    a.acker.init(pipeline, a.onACK)
    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建了一个boundGapCountACK，调用初始化函数，将这个结构体的onACK传进去就是fn&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) init(pipeline *Pipeline, fn func(int, int)) {
    *a = gapCountACK{
        pipeline: pipeline,
        fn:       fn,
        done:     make(chan struct{}),
        drop:     make(chan struct{}),
        acks:     make(chan int, 1),
    }

    init := &amp;amp;gapInfo{}
    a.lst.head = init
    a.lst.tail = init

    go a.ackLoop()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) ackLoop() {
    // close channels, as no more events should be ACKed:
    // - once pipeline is closed
    // - all events of the closed client have been acked/processed by the pipeline

    acks, drop := a.acks, a.drop
    closing := false

    for {
        select {
        case &amp;lt;-a.done:
            closing = true
            a.done = nil
            if a.events.Load() == 0 {
                // stop worker, if all events accounted for have been ACKed.
                // If new events are added after this acker won&#39;t handle them, which may
                // result in duplicates
                return
            }

        case &amp;lt;-a.pipeline.ackDone:
            return

        case n := &amp;lt;-acks:
            empty := a.handleACK(n)
            if empty &amp;amp;&amp;amp; closing &amp;amp;&amp;amp; a.events.Load() == 0 {
                // stop worker, if and only if all events accounted for have been ACKed
                return
            }

        case &amp;lt;-drop:
            // TODO: accumulate multiple drop events + flush count with timer
            a.events.Sub(1)
            a.fn(1, 0)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边启动了一个acker的监听，然后使用这个clientACKer结构体又构建了一个新的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newWaitACK(acker acker, timeout time.Duration, afterClose func()) *waitACK {
    return &amp;amp;waitACK{
        acker:      acker,
        signalAll:  make(chan struct{}, 1),
        signalDone: make(chan struct{}),
        waitClose:  timeout,
        active:     atomic.MakeBool(true),
        afterClose: afterClose,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里连接创建就结束了，创建的acker就是waitACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;client.acker = acker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以pipeline的client的acker就是waitACK。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;publish数据的ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面是有数据的时候将数据发送到pipeline，调用的client的publish函数，在发送数据的时候调用了addEvent。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open := c.acker.addEvent(e, publish)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c.acker也就是上面waitACK的addEvent函数，e就是对应发送的事件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *waitACK) addEvent(event beat.Event, published bool) bool {
    if published {
        a.events.Inc()
    }
    return a.acker.addEvent(event, published)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的是结构体成员acker的addEvent，也就是eventDataACK的addEvent。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *eventDataACK) addEvent(event beat.Event, published bool) bool {
    a.mutex.Lock()
    active := a.pipeline.ackActive.Load()
    if active {
        a.data = append(a.data, event.Private)
    }
    a.mutex.Unlock()

    if active {
        return a.acker.addEvent(event, published)
    }
    return false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边就是将数据传输到了data中，同时调用了其对应的acker的addEvent函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACK(
    pipeline *Pipeline,
    canDrop bool,
    sema *sema,
    fn func([]interface{}, int),
) *eventDataACK {
    a := &amp;amp;eventDataACK{pipeline: pipeline, fn: fn}
    a.acker = makeCountACK(pipeline, canDrop, sema, a.onACK)

    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看到a.acker = makeCountACK(pipeline, canDrop, sema, a.onACK)，再看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeCountACK(pipeline *Pipeline, canDrop bool, sema *sema, fn func(int, int)) acker {
    if canDrop {
        return newBoundGapCountACK(pipeline, sema, fn)
    }
    return newCountACK(pipeline, fn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;canDrop上面说明过了，所以是newBoundGapCountACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBoundGapCountACK(
    pipeline *Pipeline,
    sema *sema,
    fn func(total, acked int),
) *boundGapCountACK {
    a := &amp;amp;boundGapCountACK{active: true, sema: sema, fn: fn}
    a.acker.init(pipeline, a.onACK)
    return a
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以返回的结构体是boundGapCountACK，调用的也是这个结构体的addEvent&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *boundGapCountACK) addEvent(event beat.Event, published bool) bool {
    a.sema.inc()
    return a.acker.addEvent(event, published)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有acker.addEvent，再看boundGapCountACK的acker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type boundGapCountACK struct {
    active bool
    fn     func(total, acked int)

    acker gapCountACK
    sema  *sema
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;是一个gapCountACK的结构体，调用初始化a.acker.init(pipeline, a.onACK)来赋值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) init(pipeline *Pipeline, fn func(int, int)) {
    *a = gapCountACK{
        pipeline: pipeline,
        fn:       fn,
        done:     make(chan struct{}),
        drop:     make(chan struct{}),
        acks:     make(chan int, 1),
    }

    init := &amp;amp;gapInfo{}
    a.lst.head = init
    a.lst.tail = init

    go a.ackLoop()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时启动了一个监听程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) ackLoop() {
    // close channels, as no more events should be ACKed:
    // - once pipeline is closed
    // - all events of the closed client have been acked/processed by the pipeline

    acks, drop := a.acks, a.drop
    closing := false

    for {
        select {
        case &amp;lt;-a.done:
            closing = true
            a.done = nil
            if a.events.Load() == 0 {
                // stop worker, if all events accounted for have been ACKed.
                // If new events are added after this acker won&#39;t handle them, which may
                // result in duplicates
                return
            }

        case &amp;lt;-a.pipeline.ackDone:
            return

        case n := &amp;lt;-acks:
            empty := a.handleACK(n)
            if empty &amp;amp;&amp;amp; closing &amp;amp;&amp;amp; a.events.Load() == 0 {
                // stop worker, if and only if all events accounted for have been ACKed
                return
            }

        case &amp;lt;-drop:
            // TODO: accumulate multiple drop events + flush count with timer
            a.events.Sub(1)
            a.fn(1, 0)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当drop获取到信号的时候，就会调用fn也就是boundGapCountACK的onACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *boundGapCountACK) onACK(total, acked int) {
    a.sema.release(total)
    a.fn(total, acked)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边会调用到fn也就是eventDataACK的onACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *eventDataACK) onACK(total, acked int) {
    n := total

    a.mutex.Lock()
    data := a.data[:n]
    a.data = a.data[n:]
    a.mutex.Unlock()

    if len(data) &amp;gt; 0 &amp;amp;&amp;amp; a.pipeline.ackActive.Load() {
        a.fn(data, acked)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边会调用fn也就是我们创建的ackBuilder的cb成员也就是我们的ackBuilder结构的cb函数，也就是我们的onEvents&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) onEvents(data []interface{}, acked int) {
    p.pushMsg(eventsDataMsg{data: data, total: len(data), acked: acked})
}

func (p *pipelineEventCB) onCounts(total, acked int) {
    p.pushMsg(eventsDataMsg{total: total, acked: acked})
}

func (p *pipelineEventCB) pushMsg(msg eventsDataMsg) {
    if msg.acked == 0 {
        p.droppedEvents &amp;lt;- msg
    } else {
        msg.sig = make(chan struct{})
        p.events &amp;lt;- msg
        &amp;lt;-msg.sig
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取到了file的具体数据。到这边就是继续监听，我们先看addEvent，published肯定是true，正常都是有事件的publish = event != nil&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) addEvent(_ beat.Event, published bool) bool {
    // if gapList is empty and event is being dropped, forward drop event to ack
    // loop worker:

    a.events.Inc()
    if !published {
        a.addDropEvent()
    } else {
        a.addPublishedEvent()
    }

    return true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以看addPublishedEvent，只是给结构体成员send加一&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (a *gapCountACK) addPublishedEvent() {
    // event is publisher -&amp;gt; add a new gap list entry if gap is present in current
    // gapInfo

    a.lst.Lock()

    current := a.lst.tail
    current.Lock()

    if current.dropped &amp;gt; 0 {
        tmp := &amp;amp;gapInfo{}
        a.lst.tail.next = tmp
        a.lst.tail = tmp

        current.Unlock()
        tmp.Lock()
        current = tmp
    }

    a.lst.Unlock()

    current.send++
    current.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边调用的addevent就结束了，下面就是等待output的publish后的返回调用。上面已经有四个相关ack的监听，一个queue消费的监听，一个registry监听，一个是pipeline的监听，一个gapCountACK的ackLoop。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;publish后回调ack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后就是output的publish的时候进行回调了，我们使用的是kafka，kafka在connect的时候会新建两个协程，来监听发送的情况，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) Connect() error {
    c.mux.Lock()
    defer c.mux.Unlock()

    debugf(&amp;quot;connect: %v&amp;quot;, c.hosts)

    // try to connect
    producer, err := sarama.NewAsyncProducer(c.hosts, &amp;amp;c.config)
    if err != nil {
        logp.Err(&amp;quot;Kafka connect fails with: %v&amp;quot;, err)
        return err
    }

    c.producer = producer

    c.wg.Add(2)
    go c.successWorker(producer.Successes())
    go c.errorWorker(producer.Errors())

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们一共可以看到两个处理方式，一个成功一个失败，producer.Successes()和producer.Errors()为这个producer的成功和错误返回channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *client) successWorker(ch &amp;lt;-chan *sarama.ProducerMessage) {
    defer c.wg.Done()
    defer debugf(&amp;quot;Stop kafka ack worker&amp;quot;)

    for libMsg := range ch {
        msg := libMsg.Metadata.(*message)
        msg.ref.done()
    }
}

func (c *client) errorWorker(ch &amp;lt;-chan *sarama.ProducerError) {
    defer c.wg.Done()
    defer debugf(&amp;quot;Stop kafka error handler&amp;quot;)

    for errMsg := range ch {
        msg := errMsg.Msg.Metadata.(*message)
        msg.ref.fail(msg, errMsg.Err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下成功的响应，失败也是一样的，只不过多了一个错误处理，有兴趣可以自己看一下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *msgRef) done() {
    r.dec()
}

func (r *msgRef) dec() {
    i := atomic.AddInt32(&amp;amp;r.count, -1)
    if i &amp;gt; 0 {
        return
    }

    debugf(&amp;quot;finished kafka batch&amp;quot;)
    stats := r.client.observer

    err := r.err
    if err != nil {
        failed := len(r.failed)
        success := r.total - failed
        r.batch.RetryEvents(r.failed)

        stats.Failed(failed)
        if success &amp;gt; 0 {
            stats.Acked(success)
        }

        debugf(&amp;quot;Kafka publish failed with: %v&amp;quot;, err)
    } else {
        r.batch.ACK()
        stats.Acked(r.total)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在else中也就是接受到成功发送信号后调用了batch.ACK()。我们来看一下batch，首先是msg的类型转化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;msg := errMsg.Msg.Metadata.(*message)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;转化为我们定义的kafka的message的结构体message&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type message struct {
    msg sarama.ProducerMessage

    topic string
    key   []byte
    value []byte
    ref   *msgRef
    ts    time.Time

    hash      uint32
    partition int32

    data publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在kafka的client使用publish的时候初始化了ref，给batch赋值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ref := &amp;amp;msgRef{
        client: c,
        count:  int32(len(events)),
        total:  len(events),
        failed: nil,
        batch:  batch,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个batch就是重netClientWorker的qu workQueue中获取的，看一下这个channel是bantch类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type workQueue chan *Batch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个batch就是我们需要找的结构，发送kafka成功后就是调用这个结构他的ACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Batch struct {
    original queue.Batch
    ctx      *batchContext
    ttl      int
    events   []publisher.Event
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们来看一下Batch的ACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *Batch) ACK() {
    b.ctx.observer.outBatchACKed(len(b.events))
    b.original.ACK()
    releaseBatch(b)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边继续调用ACK，我们需要看一下b.original赋值，赋值都会调用newBatch函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newBatch(ctx *batchContext, original queue.Batch, ttl int) *Batch {
    if original == nil {
        panic(&amp;quot;empty batch&amp;quot;)
    }

    b := batchPool.Get().(*Batch)
    *b = Batch{
        original: original,
        ctx:      ctx,
        ttl:      ttl,
        events:   original.Events(),
    }
    return b
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只在eventConsumer消费的时候调用了newBatch，通过get方法获取的queueBatch给了他&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;queueBatch, err := consumer.Get(c.out.batchSize)
if err != nil {
    out = nil
    consumer = nil
    continue
}
if queueBatch != nil {
    batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先consumer是基于mem的，看一下get方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *consumer) Get(sz int) (queue.Batch, error) {
    // log := c.broker.logger

    if c.closed.Load() {
        return nil, io.EOF
    }

    select {
    case c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:
    case &amp;lt;-c.done:
        return nil, io.EOF
    }

    // if request has been send, we do have to wait for a response
    resp := &amp;lt;-c.resp
    return &amp;amp;batch{
        consumer: c,
        events:   resp.buf,
        ack:      resp.ack,
        state:    batchActive,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到返回使用的是结构体batch如下，我们简单看一下需要先向队列请求channel发送getRequest结构体，等待resp的返回来创建下面的结构体。具体的处理逻辑可以查看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat-principle/#pipeline-的消费过程&#34;&gt;pipeline的消费&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type batch struct {
    consumer     *consumer
    events       []publisher.Event
    clientStates []clientState
    ack          *ackChan
    state        ackState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下batch的ACK函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (b *batch) ACK() {
    if b.state != batchActive {
        switch b.state {
        case batchACK:
            panic(&amp;quot;Can not acknowledge already acknowledged batch&amp;quot;)
        default:
            panic(&amp;quot;inactive batch&amp;quot;)
        }
    }

    b.report()
}

func (b *batch) report() {
    b.ack.ch &amp;lt;- batchAckMsg{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是给batch的ack也就是getResponse的ack的ch发送了一个信号batchAckMsg{}。这个ch接收到信号，牵涉到一个完整的消费的调度过程。&lt;/p&gt;

&lt;p&gt;我们先看一下正常的消费调度，在上面说过，首先在有数据发送到queue的时候，consumer会获取这个数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
        if !paused &amp;amp;&amp;amp; c.out != nil &amp;amp;&amp;amp; consumer != nil &amp;amp;&amp;amp; batch == nil {
            out = c.out.workQueue
            queueBatch, err := consumer.Get(c.out.batchSize)
            ...
            batch = newBatch(c.ctx, queueBatch, c.out.timeToLive)
        }
        ...
        select {
        case &amp;lt;-c.done:
            return
        case sig := &amp;lt;-c.sig:
            handleSignal(sig)
        case out &amp;lt;- batch:
            batch = nil
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面consumer.Get就是消费者consumer从Broker中获取日志数据，然后发送至out的channel中被output client发送，我们看一下Get方法里的核心代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *consumer) Get(sz int) (queue.Batch, error) {
    // log := c.broker.logger

    if c.closed.Load() {
        return nil, io.EOF
    }

    select {
    case c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}:
    case &amp;lt;-c.done:
        return nil, io.EOF
    }

    // if request has been send, we do have to wait for a response
    resp := &amp;lt;-c.resp
    return &amp;amp;batch{
        consumer: c,
        events:   resp.buf,
        ack:      resp.ack,
        state:    batchActive,
    }, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当c.broker.requests &amp;lt;- getRequest{sz: sz, resp: c.resp}时候,我们需要看一下bufferingEventLoop的调度中心的响应。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) run() {
    var (
        broker = l.broker
    )

    for {
        select {
        case &amp;lt;-broker.done:
            return

        case req := &amp;lt;-l.events: // producer pushing new event
            l.handleInsert(&amp;amp;req)

        case req := &amp;lt;-l.pubCancel: // producer cancelling active events
            l.handleCancel(&amp;amp;req)

        case req := &amp;lt;-l.get: // consumer asking for next batch
            l.handleConsumer(&amp;amp;req)

        case l.schedACKS &amp;lt;- l.pendingACKs:
            l.schedACKS = nil
            l.pendingACKs = chanList{}

        case count := &amp;lt;-l.acks:
            l.handleACK(count)

        case &amp;lt;-l.idleC:
            l.idleC = nil
            l.timer.Stop()
            if l.buf.length() &amp;gt; 0 {
                l.flushBuffer()
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;l.get会得到信息，为什么l.get会得到信息，因为l.get = l.broker.requests&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) flushBuffer() {
    l.buf.flushed = true

    if l.buf.length() == 0 {
        panic(&amp;quot;flushing empty buffer&amp;quot;)
    }

    l.flushList.add(l.buf)
    l.get = l.broker.requests
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;l.get得到信息后handleConsumer如何处理信息的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *bufferingEventLoop) handleConsumer(req *getRequest) {
    buf := l.flushList.head
    if buf == nil {
        panic(&amp;quot;get from non-flushed buffers&amp;quot;)
    }

    count := buf.length()
    if count == 0 {
        panic(&amp;quot;empty buffer in flush list&amp;quot;)
    }

    if sz := req.sz; sz &amp;gt; 0 {
        if sz &amp;lt; count {
            count = sz
        }
    }

    if count == 0 {
        panic(&amp;quot;empty batch returned&amp;quot;)
    }

    events := buf.events[:count]
    clients := buf.clients[:count]
    ackChan := newACKChan(l.ackSeq, 0, count, clients)
    l.ackSeq++

    req.resp &amp;lt;- getResponse{ackChan, events}
    l.pendingACKs.append(ackChan)
    l.schedACKS = l.broker.scheduledACKs

    buf.events = buf.events[count:]
    buf.clients = buf.clients[count:]
    if buf.length() == 0 {
        l.advanceFlushList()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到获取到数据封装getResponse发送给output，我们这边不看这个数据具体发送到workqueue，而是关心的是ack。&lt;/p&gt;

&lt;p&gt;先看ack构建的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newACKChan(seq uint, start, count int, states []clientState) *ackChan {
    ch := ackChanPool.Get().(*ackChan)
    ch.next = nil
    ch.seq = seq
    ch.start = start
    ch.count = count
    ch.states = states
    return ch
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将这个ackChan新增到chanlist的链表中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;l.pendingACKs.append(ackChan)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将l.schedACKS = l.broker.scheduledACKs，一开始调度的时候l.schedACKS是阻塞的，l.pendingACKs不能写入到l.schedACKS，但是这边进行赋值后就是将l.pendingACKs写入到l.broker.scheduledACKs，这个在初始化的时候是有缓存的。就直接写入了，然后bufferingEventLoop调度中心将这个数据清空，bufferingEventLoop的ack的调度获取到这个chanenl&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *ackLoop) run() {
    var (
        // log = l.broker.logger

        // Buffer up acked event counter in acked. If acked &amp;gt; 0, acks will be set to
        // the broker.acks channel for sending the ACKs while potentially receiving
        // new batches from the broker event loop.
        // This concurrent bidirectionally communication pattern requiring &#39;select&#39;
        // ensures we can not have any deadlock between the event loop and the ack
        // loop, as the ack loop will not block on any channel
        acked int
        acks  chan int
    )

    for {
        select {
        case &amp;lt;-l.broker.done:
            // TODO: handle pending ACKs?
            // TODO: panic on pending batches?
            return

        case acks &amp;lt;- acked:
            acks, acked = nil, 0

        case lst := &amp;lt;-l.broker.scheduledACKs:
            count, events := lst.count()
            l.lst.concat(&amp;amp;lst)

            // log.Debug(&amp;quot;ACK List:&amp;quot;)
            // for current := l.lst.head; current != nil; current = current.next {
            //  log.Debugf(&amp;quot;  ack entry(seq=%v, start=%v, count=%v&amp;quot;,
            //      current.seq, current.start, current.count)
            // }

            l.batchesSched += uint64(count)
            l.totalSched += uint64(events)

        case &amp;lt;-l.sig:
            acked += l.handleBatchSig()
            if acked &amp;gt; 0 {
                acks = l.broker.acks
            }
        }

        // log.Debug(&amp;quot;ackloop INFO&amp;quot;)
        // log.Debug(&amp;quot;ackloop:   total events scheduled = &amp;quot;, l.totalSched)
        // log.Debug(&amp;quot;ackloop:   total events ack = &amp;quot;, l.totalACK)
        // log.Debug(&amp;quot;ackloop:   total batches scheduled = &amp;quot;, l.batchesSched)
        // log.Debug(&amp;quot;ackloop:   total batches ack = &amp;quot;, l.batchesACKed)

        l.sig = l.lst.channel()
        // if l.sig == nil {
        //  log.Debug(&amp;quot;ackloop: no ack scheduled&amp;quot;)
        // } else {
        //  log.Debug(&amp;quot;ackloop: schedule ack: &amp;quot;, l.lst.head.seq)
        // }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;lst := &amp;lt;-l.broker.scheduledACKs就是获取到那个请求是新建的ackchan的channel，也就是获取到了batch回调的ack的函数的信号。也就是&amp;lt;-l.sig获取到了信号，调用handleBatchSig&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// handleBatchSig collects and handles a batch ACK/Cancel signal. handleBatchSig
// is run by the ackLoop.
func (l *ackLoop) handleBatchSig() int {
    lst := l.collectAcked()

    count := 0
    for current := lst.front(); current != nil; current = current.next {
        count += current.count
    }

    if count &amp;gt; 0 {
        if e := l.broker.eventer; e != nil {
            e.OnACK(count)
        }

        // report acks to waiting clients
        l.processACK(lst, count)
    }

    for !lst.empty() {
        releaseACKChan(lst.pop())
    }

    // return final ACK to EventLoop, in order to clean up internal buffer
    l.broker.logger.Debug(&amp;quot;ackloop: return ack to broker loop:&amp;quot;, count)

    l.totalACK += uint64(count)
    l.broker.logger.Debug(&amp;quot;ackloop:  done send ack&amp;quot;)
    return count
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;l.broker.eventer我们可以追溯一下，broker就是创建queue的是newACKLoop的时候传递的，broker也是在这个时候初始化的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b := &amp;amp;Broker{
        done:   make(chan struct{}),
        logger: logger,

        // broker API channels
        events:    make(chan pushRequest, chanSize),
        requests:  make(chan getRequest),
        pubCancel: make(chan producerCancelRequest, 5),

        // internal broker and ACK handler channels
        acks:          make(chan int),
        scheduledACKs: make(chan chanList),

        waitOnClose: settings.WaitOnClose,

        eventer: settings.Eventer,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;eventer是create的时候传递的，create回传的时候是create的方法，真正调用是在pipeline初始化的时候new新建pipeline的时候&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p.queue, err = queueFactory(&amp;amp;p.eventer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以说l.broker.eventer就是p.eventer也就是结构体pipelineEventer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type pipelineEventer struct {
    mutex      sync.Mutex
    modifyable bool

    observer  queueObserver
    waitClose *waitCloser
    cb        *pipelineEventCB
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在pipeline中设置registry的时候p.eventer.cb就是我们创建的pipelineEventCB：p.eventer.cb = cb&lt;/p&gt;

&lt;p&gt;到这边可以看出e.OnACK(count)就是调用pipelineEventer的成员函数OnACK&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (e *pipelineEventer) OnACK(n int) {
    e.observer.queueACKed(n)

    if wc := e.waitClose; wc != nil {
        wc.dec(n)
    }
    if e.cb != nil {
        e.cb.reportQueueACK(n)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候就调用我们最初用的pipelineEventCB的reportQueueACK函数，就是将acked发送到了p.acks &amp;lt;- acked中，这个时候pipelineEventCB的监听程序监听到acks信号。&lt;/p&gt;

&lt;p&gt;收到ack的channel信息，调用collect函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) collect(count int) (exit bool) {
    var (
        signalers []chan struct{}
        data      []interface{}
        acked     int
        total     int
    )

    for acked &amp;lt; count {
        var msg eventsDataMsg
        select {
        case msg = &amp;lt;-p.events:
        case msg = &amp;lt;-p.droppedEvents:
        case &amp;lt;-p.done:
            exit = true
            return
        }

        if msg.sig != nil {
            signalers = append(signalers, msg.sig)
        }
        total += msg.total
        acked += msg.acked

        if count-acked &amp;lt; 0 {
            panic(&amp;quot;ack count mismatch&amp;quot;)
        }

        switch p.mode {
        case eventsACKMode:
            data = append(data, msg.data...)

        case lastEventsACKMode:
            if L := len(msg.data); L &amp;gt; 0 {
                data = append(data, msg.data[L-1])
            }
        }
    }

    // signal clients we processed all active ACKs, as reported by queue
    for _, sig := range signalers {
        close(sig)
    }
    p.reportEventsData(data, total)
    return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重pipelineEventCB中的events和droppedEvents中读取数据信息，然后进行上报reportEventsData&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (p *pipelineEventCB) reportEventsData(data []interface{}, total int) {
    // report ACK back to the beat
    switch p.mode {
    case countACKMode:
        p.handler.ACKCount(total)
    case eventsACKMode:
        p.handler.ACKEvents(data)
    case lastEventsACKMode:
        p.handler.ACKLastEvents(data)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边就调用到一开始的ACKEvents函数，对数据进行处理，其实这些数据就是[]file.State文件信息。在创建pipelineEventCB的时候，也就是在pipeline使用set函数的时候，我们这边传递的是ACKEvents，所以调用的是newEventACKer(finishedLogger, registrarChannel)这个结构体的ackEvents函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newEventACKer(stateless statelessLogger, stateful statefulLogger) *eventACKer {
    return &amp;amp;eventACKer{stateless: stateless, stateful: stateful, log: logp.NewLogger(&amp;quot;acker&amp;quot;)}
}

func (a *eventACKer) ackEvents(data []interface{}) {
    stateless := 0
    states := make([]file.State, 0, len(data))
    for _, datum := range data {
        if datum == nil {
            stateless++
            continue
        }

        st, ok := datum.(file.State)
        if !ok {
            stateless++
            continue
        }

        states = append(states, st)
    }

    if len(states) &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateful ack&amp;quot;, &amp;quot;count&amp;quot;, len(states))
        a.stateful.Published(states)
    }

    if stateless &amp;gt; 0 {
        a.log.Debugw(&amp;quot;stateless ack&amp;quot;, &amp;quot;count&amp;quot;, stateless)
        a.stateless.Published(stateless)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用的这个registrarLogger的Published来完成文件状态的推送&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (l *registrarLogger) Published(states []file.State) {
    select {
    case &amp;lt;-l.done:
        // set ch to nil, so no more events will be send after channel close signal
        // has been processed the first time.
        // Note: nil channels will block, so only done channel will be actively
        //       report &#39;closed&#39;.
        l.ch = nil
    case l.ch &amp;lt;- states:
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;文件持久化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;registrarLogger的channel获取的信息是如何处理的？其实是Registrar的channel接受到了信息，在一开始Registrar就启动了监听channel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    select {
    case &amp;lt;-r.done:
        logp.Info(&amp;quot;Ending Registrar&amp;quot;)
        return
    case &amp;lt;-flushC:
        flushC = nil
        timer.Stop()
        r.flushRegistry()
    case states := &amp;lt;-r.Channel:
        r.onEvents(states)
        if r.flushTimeout &amp;lt;= 0 {
            r.flushRegistry()
        } else if flushC == nil {
            timer = time.NewTimer(r.flushTimeout)
            flushC = timer.C
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接收到文件状态后调用onEvents&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// onEvents processes events received from the publisher pipeline
func (r *Registrar) onEvents(states []file.State) {
    r.processEventStates(states)
    r.bufferedStateUpdates += len(states)

    // check if we need to enable state cleanup
    if !r.gcEnabled {
        for i := range states {
            if states[i].TTL &amp;gt;= 0 || states[i].Finished {
                r.gcEnabled = true
                break
            }
        }
    }

    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Registrar state updates processed. Count: %v&amp;quot;, len(states))

    // new set of events received -&amp;gt; mark state registry ready for next
    // cleanup phase in case gc&#39;able events are stored in the registry.
    r.gcRequired = r.gcEnabled
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过processEventStates来处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// processEventStates gets the states from the events and writes them to the registrar state
func (r *Registrar) processEventStates(states []file.State) {
    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Processing %d events&amp;quot;, len(states))

    ts := time.Now()
    for i := range states {
        r.states.UpdateWithTs(states[i], ts)
        statesUpdate.Add(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就是states的更新UpdateWithTs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// UpdateWithTs updates a state, assigning the given timestamp.
// If previous state didn&#39;t exist, new one is created
func (s *States) UpdateWithTs(newState State, ts time.Time) {
    s.Lock()
    defer s.Unlock()

    id := newState.ID()
    index := s.findPrevious(id)
    newState.Timestamp = ts

    if index &amp;gt;= 0 {
        s.states[index] = newState
    } else {
        // No existing state found, add new one
        s.idx[id] = len(s.states)
        s.states = append(s.states, newState)
        logp.Debug(&amp;quot;input&amp;quot;, &amp;quot;New state added for %s&amp;quot;, newState.Source)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边states的状态就发生变化，再来看看states的初始化操作，其实就是在Registrar的New的时候调用了NewStates进行了初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r := &amp;amp;Registrar{
    registryFile: dataFile,
    fileMode:     cfg.Permissions,
    done:         make(chan struct{}),
    states:       file.NewStates(),
    Channel:      make(chan []file.State, 1),
    flushTimeout: cfg.FlushTimeout,
    out:          out,
    wg:           sync.WaitGroup{},
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Registrar的for循环的时候，定时会对状态进行写文件操作，调用flushRegistry的writeRegistry来完成文件的持久化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Registrar) flushRegistry() {
    if err := r.writeRegistry(); err != nil {
        logp.Err(&amp;quot;Writing of registry returned error: %v. Continuing...&amp;quot;, err)
    }

    if r.out != nil {
        r.out.Published(r.bufferedStateUpdates)
    }
    r.bufferedStateUpdates = 0
}

// writeRegistry writes the new json registry file to disk.
func (r *Registrar) writeRegistry() error {
    // First clean up states
    r.gcStates()
    states := r.states.GetStates()
    statesCurrent.Set(int64(len(states)))

    registryWrites.Inc()

    tempfile, err := writeTmpFile(r.registryFile, r.fileMode, states)
    if err != nil {
        registryFails.Inc()
        return err
    }

    err = helper.SafeFileRotate(r.registryFile, tempfile)
    if err != nil {
        registryFails.Inc()
        return err
    }

    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Registry file updated. %d states written.&amp;quot;, len(states))
    registrySuccess.Inc()

    return nil
}

func writeTmpFile(baseName string, perm os.FileMode, states []file.State) (string, error) {
    logp.Debug(&amp;quot;registrar&amp;quot;, &amp;quot;Write registry file: %s (%v)&amp;quot;, baseName, len(states))

    tempfile := baseName + &amp;quot;.new&amp;quot;
    f, err := os.OpenFile(tempfile, os.O_RDWR|os.O_CREATE|os.O_TRUNC|os.O_SYNC, perm)
    if err != nil {
        logp.Err(&amp;quot;Failed to create tempfile (%s) for writing: %s&amp;quot;, tempfile, err)
        return &amp;quot;&amp;quot;, err
    }

    defer f.Close()

    encoder := json.NewEncoder(f)

    if err := encoder.Encode(states); err != nil {
        logp.Err(&amp;quot;Error when encoding the states: %s&amp;quot;, err)
        return &amp;quot;&amp;quot;, err
    }

    // Commit the changes to storage to avoid corrupt registry files
    if err = f.Sync(); err != nil {
        logp.Err(&amp;quot;Error when syncing new registry file contents: %s&amp;quot;, err)
        return &amp;quot;&amp;quot;, err
    }

    return tempfile, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对以上的过程最一个简单的总结&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/registry.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;特殊情况&#34;&gt;特殊情况&lt;/h3&gt;

&lt;p&gt;1.如果filebeat异常重启，每次采集harvester启动的时候都会读取registry文件，从上次记录的状态继续采集，确保不会从头开始重复发送所有的日志文件。
当然，如果日志发送过程中，还没来得及返回ack，filebeat就挂掉，registry文件肯定不会更新至最新的状态，那么下次采集的时候，这部分的日志就会重复发送，所以这意味着filebeat只能保证at least once，无法保证不重复发送。
还有一个比较异常的情况是，linux下如果老文件被移除，新文件马上创建，很有可能它们有相同的inode，而由于filebeat根据inode来标志文件记录采集的偏移，会导致registry里记录的其实是被移除的文件State状态，这样新的文件采集却从老的文件Offset开始，从而会遗漏日志数据。
为了尽量避免inode被复用的情况，同时防止registry文件随着时间增长越来越大，建议使用clean_inactive和clean_remove配置将长时间未更新或者被删除的文件State从registry中移除。&lt;/p&gt;

&lt;p&gt;2.在harvester读取日志中，会更新registry的状态处理一些异常场景。例如，如果一个日志文件被清空，filebeat会在下一次Reader.Next方法中返回ErrFileTruncate异常，将inode标志文件的Offset置为0，结束这次harvester，重新启动新的harvester，虽然文件不变，但是registry中的Offset为0，采集会从头开始。&lt;/p&gt;

&lt;p&gt;3.如果使用容器部署filebeat，需要将registry文件挂载到宿主机上，否则容器重启后registry文件丢失，会使filebeat从头开始重复采集日志文件。&lt;/p&gt;

&lt;h4 id=&#34;日志重复&#34;&gt;日志重复&lt;/h4&gt;

&lt;p&gt;Filebeat对于收集到的数据（即event）的传输保证的是&amp;rdquo;at least once&amp;rdquo;，而不是&amp;rdquo;exactly once&amp;rdquo;，也就是Filebeat传输的数据是有可能有重复的。这里我们讨论一下可能产生重复数据的一些场景，我大概将其分为两类。&lt;/p&gt;

&lt;p&gt;第一类：Filebeat重传导致数据重复。重传是因为Filebeat要保证数据至少发送一次，进而避免数据丢失。具体来说就是每条event发送到output后都要等待ack，只有收到ack了才会认为数据发送成功，然后将状态记录到registry。当然实际操作的时候为了高效是批量发送，批量确认的。而造成重传的场景（也就是没有收到ack）非常多，而且很多都不可避免，比如后端不可达、网络传输失败、程序突然挂掉等等。&lt;/p&gt;

&lt;p&gt;第二类：配置不当或操作不当导致文件重复收集。Filebeat感知文件有没有被收集过靠的是registry文件里面记录的状态，如果一个文件已经被收集过了，但因为各种原因它的状态从registry文件中被移除了，而恰巧这个文件还在收集范围内，那就会再收集一次。&lt;/p&gt;

&lt;p&gt;对于第一类产生的数据重复一般不可避免，而第二类可以避免，但总的来说，Filebeat提供的是at least once的机制，所以我们在使用时要明白数据是可能重复的。如果业务上不能接受数据重复，那就要在Filebeat之后的流程中去重。&lt;/p&gt;

&lt;h4 id=&#34;数据丢失&#34;&gt;数据丢失&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;inode重用的问题&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果一个文件达到了限制（比如大小），不是重新创建一个新的文件写，而是将这个文件truncate掉继续复用（当然实际中这种场景好像比较少，但也并非没有），Filebeat下次来检查这个文件是否有变动的时候，这个文件的大小如果大于之前记录的offset，也会发生上面的情况。这个问题在github上面是有issue的，但目前还没有解决，官方回复是Filebeat的整个机制在重构中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;还有一些其它情况，比如文件数太多，Filebeat的处理能力有限，在还没来得及处理的时候这些文件就被删掉了（比如rotate给老化掉了）也会造成数据丢失。还有就是后端不可用，所以Filebeat还在重试，但源文件被删了，那数据也就丢了。因为Filebeat的重试并非一直发送已经收集到内存里面的event，必要的时候会重新从源文件读，比如程序重启。这些情况的话，只要不限制Filebeat的收集能力，同时保证后端的可用性，网络的可用性，一般问题不大。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;总结-1&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;其实重数据发送到内存队列中这一套完整的功能就是由libbeat完成的，正常流程如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/output.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;日志采集状态监控&#34;&gt;日志采集状态监控&lt;/h2&gt;

&lt;p&gt;我们之前讲到 Registrar 会记录每个文件的状态，当 Filebeat 启动时，会从 Registrar 恢复文件处理状态。&lt;/p&gt;

&lt;p&gt;其实在 filebeat 运行过程中，Input 组件也记录了文件状态。不一样的是，Registrar 是持久化存储，而 Input 中的文件状态仅表示当前文件的读取偏移量，且修改时不会同步到磁盘中。&lt;/p&gt;

&lt;p&gt;每次，Filebeat 刚启动时，Input 都会载入 Registrar 中记录的文件状态，作为初始状态。Input 中的状态有两个非常重要：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;offset: 代表文件当前读取的 offset，从 Registrar 中初始化。Harvest 读取文件后，会同时修改 offset。
finished: 代表该文件对应的 Harvester 是否已经结束，Harvester 开始时置为 false，结束时置为 true。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于每次定时扫描到的文件，概括来说，会有三种大的情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input 找不到该文件状态的记录, 说明是新增文件，则开启一个 Harvester，从头开始解析该文件
如果可以找到文件状态，且 finished 等于 false。这个说明已经有了一个 Harvester 在处理了，这种情况直接忽略就好了。
如果可以找到文件状态，且 finished 等于 true。说明之前有 Harvester 处理过，但已经处理结束了。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于这种第三种情况，我们需要考虑到一些异常情况，Filebeat 是这么处理的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;如果 offset 大于当前文件大小：说明文件被 Truncate 过，此时按做一个新文件处理，直接从头开始解析该文件
如果 offset 小于当前文件大小，说明文件内容有新增，则从上次 offset 处继续读即可。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于第二种情况，Filebeat 似乎有一个逻辑上的问题: 如果文件被 Truncate 过，后来又新增了数据，且文件大小也比之前 offset 大，那么 Filebeat 是检查不出来这个问题的。&lt;/p&gt;

&lt;h2 id=&#34;句柄保持&#34;&gt;句柄保持&lt;/h2&gt;

&lt;p&gt;Filebeat 甚至可以处理文件名修改的问题。即使一个日志的文件名被修改过，Filebeat 重启后，也能找到该文件，从上次读过的地方继续读。&lt;/p&gt;

&lt;p&gt;这是因为 Filebeat 除了在 Registrar 存储了文件名，还存储了文件的唯一标识。对于 Linux 来说，这个文件的唯一标识就是该文件的 inode ID + device ID。&lt;/p&gt;

&lt;h2 id=&#34;重载&#34;&gt;重载&lt;/h2&gt;

&lt;p&gt;重载是在Crawler中启动的，首先是新建的InputsFactory的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c.InputsFactory = input.NewRunnerFactory(c.out, r, c.beatDone)

// RunnerFactory is a factory for registrars
type RunnerFactory struct {
    outlet    channel.Factory
    registrar *registrar.Registrar
    beatDone  chan struct{}
}

// NewRunnerFactory instantiates a new RunnerFactory
func NewRunnerFactory(outlet channel.Factory, registrar *registrar.Registrar, beatDone chan struct{}) *RunnerFactory {
    return &amp;amp;RunnerFactory{
        outlet:    outlet,
        registrar: registrar,
        beatDone:  beatDone,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后如果配置了重载，就会新建Reloader结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Reloader is used to register and reload modules
type Reloader struct {
    pipeline beat.Pipeline
    config   DynamicConfig
    path     string
    done     chan struct{}
    wg       sync.WaitGroup
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新建的过程如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if configInputs.Enabled() {
    c.inputReloader = cfgfile.NewReloader(pipeline, configInputs)
    if err := c.inputReloader.Check(c.InputsFactory); err != nil {
        return err
    }

    go func() {
        c.inputReloader.Run(c.InputsFactory)
    }()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动reloader的run方法并且将RunnerFactory作为参数传递进去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Run runs the reloader
func (rl *Reloader) Run(runnerFactory RunnerFactory) {
    logp.Info(&amp;quot;Config reloader started&amp;quot;)

    list := NewRunnerList(&amp;quot;reload&amp;quot;, runnerFactory, rl.pipeline)

    rl.wg.Add(1)
    defer rl.wg.Done()

    // Stop all running modules when method finishes
    defer list.Stop()

    gw := NewGlobWatcher(rl.path)

    // If reloading is disable, config files should be loaded immediately
    if !rl.config.Reload.Enabled {
        rl.config.Reload.Period = 0
    }

    overwriteUpdate := true

    for {
        select {
        case &amp;lt;-rl.done:
            logp.Info(&amp;quot;Dynamic config reloader stopped&amp;quot;)
            return

        case &amp;lt;-time.After(rl.config.Reload.Period):
            debugf(&amp;quot;Scan for new config files&amp;quot;)
            configReloads.Add(1)

            //扫描所有的配置文件
            files, updated, err := gw.Scan()
            if err != nil {
                // In most cases of error, updated == false, so will continue
                // to next iteration below
                logp.Err(&amp;quot;Error fetching new config files: %v&amp;quot;, err)
            }

            // no file changes
            if !updated &amp;amp;&amp;amp; !overwriteUpdate {
                overwriteUpdate = false
                continue
            }

            // Load all config objects 加载所有配置文件
            configs, _ := rl.loadConfigs(files)

            debugf(&amp;quot;Number of module configs found: %v&amp;quot;, len(configs))

            //启动加载程序
            if err := list.Reload(configs); err != nil {
                // Make sure the next run also updates because some runners were not properly loaded
                overwriteUpdate = true
            }
        }

        // Path loading is enabled but not reloading. Loads files only once and then stops.
        if !rl.config.Reload.Enabled {
            logp.Info(&amp;quot;Loading of config files completed.&amp;quot;)
            select {
            case &amp;lt;-rl.done:
                logp.Info(&amp;quot;Dynamic config reloader stopped&amp;quot;)
                return
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见有一个定时的循环程序来获取所有的配置文件，交给RunnerList的reload的来加载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Reload the list of runners to match the given state
func (r *RunnerList) Reload(configs []*reload.ConfigWithMeta) error {
    r.mutex.Lock()
    defer r.mutex.Unlock()

    var errs multierror.Errors

    startList := map[uint64]*reload.ConfigWithMeta{}
    //获取正在运行的runner到stopList
    stopList := r.copyRunnerList()

    r.logger.Debugf(&amp;quot;Starting reload procedure, current runners: %d&amp;quot;, len(stopList))

    // diff current &amp;amp; desired state, create action lists
    for _, config := range configs {
        hash, err := HashConfig(config.Config)
        if err != nil {
            r.logger.Errorf(&amp;quot;Unable to hash given config: %s&amp;quot;, err)
            errs = append(errs, errors.Wrap(err, &amp;quot;Unable to hash given config&amp;quot;))
            continue
        }

        //如果配置文件还在，就重stopList中删除，继续采集，剩下的在stopList中的下面会停止采集
        if _, ok := stopList[hash]; ok {
            delete(stopList, hash)
        } else {
            //如果不在stopList中，说明是新的文件，如果不是重复的就加入到startList中，下来开始采集
            if _,ok := r.runners[hash]; !ok{
                startList[hash] = config
            }
        }
    }

    r.logger.Debugf(&amp;quot;Start list: %d, Stop list: %d&amp;quot;, len(startList), len(stopList))

    // Stop removed runners
    for hash, runner := range stopList {
        r.logger.Debugf(&amp;quot;Stopping runner: %s&amp;quot;, runner)
        delete(r.runners, hash)
        go runner.Stop()
    }

    // Start new runners
    for hash, config := range startList {
        // Pass a copy of the config to the factory, this way if the factory modifies it,
        // that doesn&#39;t affect the hash of the original one.
        c, _ := common.NewConfigFrom(config.Config)
        runner, err := r.factory.Create(r.pipeline, c, config.Meta)
        if err != nil {
            r.logger.Errorf(&amp;quot;Error creating runner from config: %s&amp;quot;, err)
            errs = append(errs, errors.Wrap(err, &amp;quot;Error creating runner from config&amp;quot;))
            continue
        }

        r.logger.Debugf(&amp;quot;Starting runner: %s&amp;quot;, runner)
        r.runners[hash] = runner
        runner.Start()
    }

    return errs.Err()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边的create就是上面传进来的InputsFactory的结构体的成员函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Create creates a input based on a config
func (r *RunnerFactory) Create(
    pipeline beat.Pipeline,
    c *common.Config,
    meta *common.MapStrPointer,
) (cfgfile.Runner, error) {
    connector := r.outlet(pipeline)
    p, err := New(c, connector, r.beatDone, r.registrar.GetStates(), meta)
    if err != nil {
        // In case of error with loading state, input is still returned
        return p, err
    }

    return p, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看看new函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// New instantiates a new Runner
func New(
    conf *common.Config,
    connector channel.Connector,
    beatDone chan struct{},
    states []file.State,
    dynFields *common.MapStrPointer,
) (*Runner, error) {
    input := &amp;amp;Runner{
        config:   defaultConfig,
        wg:       &amp;amp;sync.WaitGroup{},
        done:     make(chan struct{}),
        Once:     false,
        beatDone: beatDone,
    }

    var err error
    if err = conf.Unpack(&amp;amp;input.config); err != nil {
        return nil, err
    }

    var h map[string]interface{}
    conf.Unpack(&amp;amp;h)
    input.ID, err = hashstructure.Hash(h, nil)
    if err != nil {
        return nil, err
    }

    var f Factory
    f, err = GetFactory(input.config.Type)
    if err != nil {
        return input, err
    }

    context := Context{
        States:        states,
        Done:          input.done,
        BeatDone:      input.beatDone,
        DynamicFields: dynFields,
        Meta:          nil,
    }
    var ipt Input
    ipt, err = f(conf, connector, context)
    if err != nil {
        return input, err
    }
    input.input = ipt

    return input, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看见就是新建流程中的新建runner，下面调用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;runner.Start()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是正常的采集流程，到这边重载也就结束了。&lt;/p&gt;

&lt;h1 id=&#34;基本使用与特性&#34;&gt;基本使用与特性&lt;/h1&gt;

&lt;h2 id=&#34;filebeat自动reload更新&#34;&gt;filebeat自动reload更新&lt;/h2&gt;

&lt;p&gt;目前filebeat支持reload input配置，module配置，但reload的机制只有定时更新。&lt;/p&gt;

&lt;p&gt;在配置中打开reload.enable之后，还可以配置reload.period表示自动reload配置的时间间隔。&lt;/p&gt;

&lt;p&gt;filebeat在启动时，会创建一个专门用于reload的协程。对于每个正在运行的harvester，filebeat会将其加入一个全局的Runner列表，每次到了定时的间隔后，会触发一次配置文件的diff判断，如果是需要停止的加入stopRunner列表，然后逐个关闭，新的则加入startRunner列表，启动新的Runner。&lt;/p&gt;

&lt;h2 id=&#34;filebeat对kubernetes的支持&#34;&gt;filebeat对kubernetes的支持&lt;/h2&gt;

&lt;p&gt;filebeat官方文档提供了在kubernetes下基于daemonset的部署方式，最主要的一个配置如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- type: docker
      containers.ids:
      - &amp;quot;*&amp;quot;
      processors:
        - add_kubernetes_metadata:
            in_cluster: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即设置输入input为docker类型。由于所有的容器的标准输出日志默认都在节点的/var/lib/docker/containers/&lt;containerId&gt;/*-json.log路径，所以本质上采集的是这类日志文件。&lt;/p&gt;

&lt;p&gt;和传统的部署方式有所区别的是，如果服务部署在kubernetes上，我们查看和检索日志的维度不能仅仅局限于节点和服务，还需要有podName，containerName等，所以每条日志我们都需要打标增加kubernetes的元信息才发送至后端。&lt;/p&gt;

&lt;p&gt;filebeat会在配置中增加了add_kubernetes_metadata的processor的情况下，启动监听kubernetes的watch服务，监听所有kubernetes pod的变更，然后将归属本节点的pod最新的事件同步至本地的缓存中。&lt;/p&gt;

&lt;p&gt;节点上一旦发生容器的销毁创建，/var/lib/docker/containers/下会有目录的变动，filebeat根据路径提取出containerId，再根据containerId从本地的缓存中找到pod信息，从而可以获取到podName、label等数据，并加到日志的元信息fields中。&lt;/p&gt;

&lt;p&gt;filebeat还有一个beta版的功能autodiscover，autodiscover的目的是把分散到不同节点上的filebeat配置文件集中管理。目前也支持kubernetes作为provider，本质上还是监听kubernetes事件然后采集docker的标准输出文件。&lt;/p&gt;

&lt;p&gt;大致架构如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/log/filebeat/k8slog.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但是在实际生产环境使用中，仅采集容器的标准输出日志还是远远不够，我们往往还需要采集容器挂载出来的自定义日志目录，还需要控制每个服务的日志采集方式以及更多的定制化功能。&lt;/p&gt;

&lt;h2 id=&#34;性能分析与调优&#34;&gt;性能分析与调优&lt;/h2&gt;

&lt;p&gt;虽然beats系列主打轻量级，虽然用golang写的filebeat的内存占用确实比较基于jvm的logstash等好太多，但是事实告诉我们其实没那么简单。&lt;/p&gt;

&lt;p&gt;正常启动filebeat，一般确实只会占用3、40MB内存，但是在轻舟容器云上偶发性的我们也会发现某些节点上的filebeat容器内存占用超过配置的pod limit限制（一般设置为200MB），并且不停的触发的OOM。&lt;/p&gt;

&lt;p&gt;究其原因，一般容器化环境中，特别是裸机上运行的容器个数可能会比较多，导致创建大量的harvester去采集日志。如果没有很好的配置filebeat，会有较大概率导致内存急剧上升。
当然，filebeat内存占据较大的部分还是memqueue，所有采集到的日志都会先发送至memqueue聚集，再通过output发送出去。每条日志的数据在filebeat中都被组装为event结构，filebeat默认配置的memqueue缓存的event个数为4096，可通过queue.mem.events设置。默认最大的一条日志的event大小限制为10MB，可通过max_bytes设置。4096 * 10MB = 40GB，可以想象，极端场景下，filebeat至少占据40GB的内存。特别是配置了multiline多行模式的情况下，如果multiline配置有误，单个event误采集为上千条日志的数据，很可能导致memqueue占据了大量内存，致使内存爆炸。&lt;/p&gt;

&lt;p&gt;所以，合理的配置日志文件的匹配规则，限制单行日志大小，根据实际情况配置memqueue缓存的个数，才能在实际使用中规避filebeat的内存占用过大的问题。&lt;/p&gt;

&lt;p&gt;有些文章说filebeat内存消耗很少,不会超过100M, 这简直是不负责任的胡说,假如带着这样的认识把filebeat部署到生产服务器上就等着哭吧.&lt;/p&gt;

&lt;p&gt;那怎么样才能避免以上内存灾难呢?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;每个日志生产环境生产的日志大小,爆发量都不一样, 要根据自己的日志特点设定合适的event值;什么叫合适,至少能避免内存&amp;gt;200MB的灾难;&lt;/li&gt;
&lt;li&gt;在不知道日志实际情况(单条大小,爆发量), 务必把event设置上,建议128或者256;&lt;/li&gt;
&lt;li&gt;合理的配置日志文件的匹配规则，是否因为通配符的原因，造成同时监控数量巨大的文件，这种情况应该避免用通配符监控无用的文件。&lt;/li&gt;
&lt;li&gt;规范日志，限制单行日志大小，是否文件的单行内容巨大，确定是否需要改造文件内容，或者将其过滤&lt;/li&gt;
&lt;li&gt;限制cpu&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面的一系列操作可以做如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max_procs: 2
queue:
  mem:
    events: 512
    flush.min_events: 256
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;限制cpu为2core，内存最大为512*10M～=5G&lt;/p&gt;

&lt;p&gt;限制cpu的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max_procs，限制filebeat的进程数量，其实是内核数，建议手动设为1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;限制内存的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;queue.mem.events消息队列的大小，默认值是4096，这个参数在6.0以前的版本是spool-size，通过命令行，在启动时进行配置
max_message_bytes 单条消息的大小, 默认值是10M
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;filebeat最大的可能占用的内存是max_message_bytes * queue.mem.events = 40G，考虑到这个queue是用于存储encode过的数据，raw数据也是要存储的，所以，在没有对内存进行限制的情况下，最大的内存占用情况是可以达到超过80G。&lt;/p&gt;

&lt;h2 id=&#34;内存使用过多的情况&#34;&gt;内存使用过多的情况&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;非常频繁的rotate日志&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于实时大量产生内容的文件，比如日志，常用的做法往往是将日志文件进行rotate，根据策略的不同，每隔一段时间或者达到固定大小之后，将日志rotate。
这样，在文件目录下可能会产生大量的日志文件。
如果我们使用通配符的方式，去监控该目录，则filebeat会启动大量的harvester实例去采集文件。但是，请记住，我这里不是说这样一定会产生内存泄漏，只是在这里观测到了内存泄漏而已，不是说这是造成内存泄漏的原因。&lt;/p&gt;

&lt;p&gt;当filebeat运行了几个月之后，占用了超过10个G的内存。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;因为multiline导致内存占用过多&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;multiline.pattern: &amp;lsquo;^[[:space:]]+|^Caused by:|^.+Exception:|^\d+\serror，比如这个配置，认为空格或者制表符开头的line是上一行的附加内容，需要作为多行模式，存储到同一个event当中。当你监控的文件刚巧在文件的每一行带有一个空格时，会错误的匹配多行，造成filebeat解析过后，单条event的行数达到了上千行，大小达到了10M，并且在这过程中使用的是正则表达式，每一条event的处理都会极大的消耗内存。因为大多数的filebeat output是需应答的，buffer这些event必然会大量的消耗内存。&lt;/p&gt;

&lt;h2 id=&#34;解读日志中的监控数据&#34;&gt;解读日志中的监控数据&lt;/h2&gt;

&lt;p&gt;其实filebeat的日志，已经包含了很多参数用于实时观测filebeat的资源使用情况，（下面是6.0版本的，6.5版本之后，整个日志格式变了，从kv格式变成了json对象格式）&lt;/p&gt;

&lt;p&gt;里面的参数主要分成三个部分：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;beat.*，包含memstats.gc_next，memstats.memory_alloc，memstats.memory_total，这个是所有beat组件都有的指标，是filebeat继承来的，主要是内存相关的，我们这里特别关注memstats.memory_alloc，alloc的越多，占用内存越大
filebeat.*，这部分是filebeat特有的指标，通过event相关的指标，我们知道吞吐，通过harvester，我们知道正在监控多少个文件，未消费event堆积的越多，havester创建的越多，消耗内存越大
libbeat.*，也是beats组件通用的指标，包含outputs和pipeline等信息。这里要主要当outputs发生阻塞的时候，会直接影响queue里面event的消费，造成内存堆积
registrar，filebeat将监控文件的状态放在registry文件里面，当监控文件非常多的时候，比如10万个，而且没有合理的设置close_inactive参数，这个文件能达到100M，载入内存后，直接占用内存
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在6.5之后都是json，但也是kv结构，可以对应查看。&lt;/p&gt;

&lt;h2 id=&#34;如何对filebeat进行扩展开发&#34;&gt;如何对filebeat进行扩展开发&lt;/h2&gt;

&lt;p&gt;一般情况下filebeat可满足大部分的日志采集需求，但是仍然避免不了一些特殊的场景需要我们对filebeat进行定制化开发，当然filebeat本身的设计也提供了良好的扩展性。
beats目前只提供了像elasticsearch、kafka、logstash等几类output客户端，如果我们想要filebeat直接发送至其他后端，需要定制化开发自己的output。同样，如果需要对日志做过滤处理或者增加元信息，也可以自制processor插件。
无论是增加output还是写个processor，filebeat提供的大体思路基本相同。一般来讲有3种方式：&lt;/p&gt;

&lt;p&gt;1.直接fork filebeat，在现有的源码上开发。output或者processor都提供了类似Run、Stop等的接口，只需要实现该类接口，然后在init方法中注册相应的插件初始化方法即可。当然，由于golang中init方法是在import包时才被调用，所以需要在初始化filebeat的代码中手动import。&lt;/p&gt;

&lt;p&gt;2.filebeat还提供了基于golang plugin的插件机制，需要把自研的插件编译成.so共享链接库，然后在filebeat启动参数中通过-plugin指定库所在路径。不过实际上一方面golang plugin还不够成熟稳定，一方面自研的插件依然需要依赖相同版本的libbeat库，而且还需要相同的golang版本编译，坑可能更多，不太推荐。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控日志系列---- 容器日志采集方案</title>
          <link>https://kingjcy.github.io/post/monitor/log/collect/collect-scheme/</link>
          <pubDate>Sun, 08 Jul 2018 19:45:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/log/collect/collect-scheme/</guid>
          <description>&lt;p&gt;容器由于其特殊性，在日志采集上有着不同的解决方案，目前主要还是以探针采集为主。&lt;/p&gt;

&lt;h1 id=&#34;日志采集演进&#34;&gt;日志采集演进&lt;/h1&gt;

&lt;p&gt;容器日志采集方案一直不断的演进，纵览当前容器日志收集的场景，无非就是两种方式：一是直接采集Docker标准输出，容器内的服务将日志信息写到标准输出，这样通过Docker的log driver可以发送到相应的收集程序中；二是延续传统的日志写入方式，容器内的服务将日志直接写到普通文件中，通过Docker volume将日志文件映射到Host上，日志采集程序就可以收集它。&lt;/p&gt;

&lt;h2 id=&#34;docker-log-driver&#34;&gt;docker log driver&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;docker logs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;docker logs edc-k8s-demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，Docker的日志会发送到容器的标准输出设备（STDOUT）和标准错误设备（STDERR），其中STDOUT和STDERR实际上就是容器的控制台终端。如果想要持续看到新打印出的日志信息，那么可以加上 -f 参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker logs -f edc-k8s-demo
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Docker logging driver&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Docker还提供了其他的一些机制允许我们从运行的容器中提取日志，这些机制统称为 logging driver。&lt;/p&gt;

&lt;p&gt;对Docker而言，其默认的logging driver是json-file，如果在启动时没有特别指定，都会使用这个默认的logging driver。json-file会将我们在控制台通过docker logs命名看到的日志都保存在一个json文件中，我们可以在服务器Host上的容器目录中找到这个json文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;容器日志路径：/var/lib/docker/containers/&amp;lt;container-id&amp;gt;/&amp;lt;container-id&amp;gt;-json.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了json-file，Docker还支持以下多种logging dirver&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;none  No logs are available for the container and docker logs does not return any output.&lt;/li&gt;
&lt;li&gt;local Logs are stored in a custom format designed for minimal overhead.&lt;/li&gt;
&lt;li&gt;json-file The logs are formatted as JSON. The default logging driver for Docker.&lt;/li&gt;
&lt;li&gt;syslog    Writes logging messages to the syslog facility. The syslog daemon must be running on the host machine.&lt;/li&gt;
&lt;li&gt;journald  Writes log messages to journald. The journald daemon must be running on the host machine.&lt;/li&gt;
&lt;li&gt;gelf  Writes log messages to a Graylog Extended Log Format (GELF) endpoint such as Graylog or Logstash.&lt;/li&gt;
&lt;li&gt;fluentd   Writes log messages to fluentd (forward input). The fluentd daemon must be running on the host machine.&lt;/li&gt;
&lt;li&gt;awslogs   Writes log messages to Amazon CloudWatch Logs.&lt;/li&gt;
&lt;li&gt;splunk    Writes log messages to splunk using the HTTP Event Collector.&lt;/li&gt;
&lt;li&gt;etwlogs   Writes log messages as Event Tracing for Windows (ETW) events. Only available on Windows platforms.&lt;/li&gt;
&lt;li&gt;gcplogs   Writes log messages to Google Cloud Platform (GCP) Logging.&lt;/li&gt;
&lt;li&gt;logentries    Writes log messages to Rapid7 Logentries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以在容器启动时通过加上 &amp;ndash;log-driver 来指定使用哪个具体的 logging driver，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --log-driver=syslog ......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想要设置默认的logging driver，那么则需要修改Docker daemon的启动脚本，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;log-driver&amp;quot;: &amp;quot;json-file&amp;quot;,
  &amp;quot;log-opts&amp;quot;: {
    &amp;quot;labels&amp;quot;: &amp;quot;production_status&amp;quot;,
    &amp;quot;env&amp;quot;: &amp;quot;os,customer&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个logging driver都有一些自己特定的log-opt，使用时可以参考具体官方文档。&lt;/p&gt;

&lt;p&gt;可见，第一种方式足够简单，直接配置相关的Log Driver就可以，但是这种方式也有些劣势：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当主机的容器密度比较高的时候，对Docker Engine的压力比较大，毕竟容器标准输出都要通过Docker Engine来处理。&lt;/li&gt;
&lt;li&gt;尽管原则上，我们希望遵循一容器部署一个服务的原则，但是有时候特殊情况不可避免容器内有多个业务服务，这时候很难做到所有服务都向标准输出写日志，这就需要用到前面所说的第二种场景模式。&lt;/li&gt;
&lt;li&gt;虽然我们可以先选择很多种Log Driver，但是有些Log Driver会破坏Docker原生的体验，比如docker logs无法直接看到容器日志。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;docker-volume&#34;&gt;docker volume&lt;/h2&gt;

&lt;p&gt;通过对第一种方案的摸索，存在着很多的问题与不方便，所以目前我们大多数采集还是使用第二种方案，文件采集的方式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;第三方采集方案&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面都是将日志文件落到STDOUT和STDERR，我们采集都是基于这个，其实在我们应用编程的时候，完全可以将日志文件落到容器的对应的目录下，落盘然后使用第三方采集组件比如filebeat、fluentd等采集，统一管理。&lt;/p&gt;

&lt;h1 id=&#34;容器日志采集方案&#34;&gt;容器日志采集方案&lt;/h1&gt;

&lt;p&gt;根据上面的基本描述，容器日志采集有很多种方式，每种方式都用不同实现方案，适用于不同的场景。&lt;/p&gt;

&lt;h2 id=&#34;logdriver&#34;&gt;LogDriver&lt;/h2&gt;

&lt;p&gt;DockerEngine 本身具有 LogDriver 功能，可通过配置不同的 LogDriver 将容器的 stdout 通过 DockerEngine 写入到远端存储，以此达到日志采集的目的。这种方式的可定制化、灵活性、资源隔离性都很低，一般不建议在生产环境中使用，上面我们已经说明不使用的原因。&lt;/p&gt;

&lt;h2 id=&#34;http&#34;&gt;http&lt;/h2&gt;

&lt;p&gt;业务直写是在应用中集成日志采集的 SDK，通过 SDK 直接将日志发送到服务端。这种方式省去了落盘采集的逻辑，也不需要额外部署 Agent，对于系统的资源消耗最低，但由于业务和日志 SDK 强绑定，整体灵活性很低，一般只有日志量极大的场景中使用，这是一种特殊的场景，我们会在特殊情况下使用。&lt;/p&gt;

&lt;h2 id=&#34;deamonset模式&#34;&gt;deamonset模式&lt;/h2&gt;

&lt;p&gt;DaemonSet 方式在每个 node 节点上只运行一个日志 agent(&lt;a href=&#34;https://kingjcy.github.io/post/monitor/log/collect/filebeat/filebeat/&#34;&gt;filebeat&lt;/a&gt;,fluentd,flume,fluentbit)，采集这个节点上所有的日志。DaemonSet 相对资源占用要小很多，但扩展性、租户隔离性受限，比较适用于功能单一或业务不是很多的集群；&lt;/p&gt;

&lt;p&gt;正常规模的采集可以适应，日志分类明确、功能较单一的集群，大规模的集群采集速度就跟不上了，而且没有办法做到垂直扩展无上限。&lt;/p&gt;

&lt;p&gt;当然完整的方案还是有很多需要做的工作，比如如何做发现，如何动态变更，这就是一个完成的平台建设了，这些每个公司都有自己的建设，就不太好说了。&lt;/p&gt;

&lt;h2 id=&#34;sidecar模式&#34;&gt;sidecar模式&lt;/h2&gt;

&lt;p&gt;Sidecar 方式为每个 POD 单独部署日志 agent，这个 agent 只负责一个业务应用的日志采集。Sidecar 相对资源占用较多，但灵活性以及多租户隔离性较强，建议大型的 K8s 集群或作为 PaaS 平台为多个业务方服务的集群使用该方式。&lt;/p&gt;

&lt;p&gt;适用于大型、混合型、PAAS型集群的日志采集，是一种水平扩展消耗更多资源来增加采集速度的方案，但是方案就比较复杂。&lt;/p&gt;

&lt;p&gt;当然完整的方案还是有很多需要做的工作，比如如何做发现，如何动态变更，这就是一个完成的平台建设了，这些每个公司都有自己的建设，也就不太好说了。&lt;/p&gt;

&lt;h1 id=&#34;网络采集性能数据&#34;&gt;网络采集性能数据&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;有赞&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;重flume发展到自研rsyslog-hub和http服务&lt;/p&gt;

&lt;p&gt;17年平均每秒产生日志1.1万条，峰值1.5万条，每天的日志量约9亿条，占用空间2.4T左右&lt;/p&gt;

&lt;p&gt;19年每天都会产生百亿级别的日志量（据统计，平均每秒产生 50 万条日志，峰值每秒可达 80 万条）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;七牛云&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;自研logkit&lt;/p&gt;

&lt;p&gt;17年现在日均数据流入量超 250 TB，3650 亿条，其中最大的客户日均数据流入量超过 45 TB。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;b站&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;17年目前集群规模20台机器，接入业务200+，单日日志量10T+。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;阿里云&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;自研logtail（重内核都得到的优化和充分利用）&lt;/p&gt;

&lt;p&gt;速度达到160M/s&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Operator</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/</link>
          <pubDate>Tue, 12 Jun 2018 16:57:05 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/</guid>
          <description>&lt;p&gt;Prometheus-Operator是一套为了方便整合prometheus和kubernetes的开源方案，使用Prometheus-Operator可以非常简单的在kubernetes集群中部署Prometheus服务，用户能够使用简单的声明性配置来配置和管理Prometheus实例，这些配置将响应、创建、配置和管理Prometheus监控实例。&lt;/p&gt;

&lt;h1 id=&#34;operator&#34;&gt;operator&lt;/h1&gt;

&lt;p&gt;Operator是由CoreOS公司开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的一些专业知识，比如创建一个数据库的Operator，则必须对创建的数据库的各种运维方式非常了解，创建Operator的关键是CRD（自定义资源）的设计。&lt;/p&gt;

&lt;p&gt;CRD是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在YAML文件里定义的那些spec都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。&lt;/p&gt;

&lt;p&gt;Operator是将运维人员对软件操作的知识给代码化，同时利用 Kubernetes 强大的抽象来管理大规模的软件应用。目前CoreOS官方提供了几种Operator的实现，其中就包括我们今天的主角：Prometheus Operator，Operator的核心实现就是基于 Kubernetes 的以下两个概念：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;资源：对象的状态定义&lt;/li&gt;
&lt;li&gt;控制器：观测、分析和行动，以调节资源的分布&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然我们如果有对应的需求也完全可以自己去实现一个Operator，接下来我们就来给大家详细介绍下Prometheus-Operator的使用方法&lt;/p&gt;

&lt;h1 id=&#34;原理&#34;&gt;原理&lt;/h1&gt;

&lt;h2 id=&#34;基本架构&#34;&gt;基本架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/operator/operator.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1、Operator： 根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心，也就是我们常用的控制器，可见operater让prometheus更加k8s。&lt;/p&gt;

&lt;p&gt;2、Prometheus：声明 Prometheus deployment 期望的状态，Operator 确保这个 deployment 运行时一直与定义保持一致。这边是一个资源类型，和下一个具体的prometheus是有区别的。&lt;/p&gt;

&lt;p&gt;3、Prometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。&lt;/p&gt;

&lt;p&gt;4、ServiceMonitor：声明指定监控的服务，描述了一组被 Prometheus 监控的目标列表。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。&lt;/p&gt;

&lt;p&gt;5、Service：简单的说就是 Prometheus 监控的对象。&lt;/p&gt;

&lt;p&gt;6、Alertmanager：定义 AlertManager deployment 期望的状态，Operator 确保这个 deployment 运行时一直与定义保持一致。&lt;/p&gt;

&lt;p&gt;这边涉及来operater定义的几种crd类型。&lt;/p&gt;

&lt;h2 id=&#34;crd&#34;&gt;CRD&lt;/h2&gt;

&lt;p&gt;Prometheus Operater 定义了如下的六类自定义资源（CRD）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Prometheus：部署prometheus
ServiceMonitor：服务发现拉去列表基于service
Alertmanager：部署alertmanager
PrometheusRule：告警规则
ThanosRuler：部署thanos
PodMonitor：服务发现拉去列表基于pod
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;prometheus&#34;&gt;Prometheus&lt;/h3&gt;

&lt;p&gt;Prometheus 自定义资源（CRD）声明了在 Kubernetes 集群中运行的 Prometheus 的期望设置。包含了副本数量，持久化存储，以及 Prometheus 实例发送警告到的 Alertmanagers等配置选项。&lt;/p&gt;

&lt;p&gt;每一个 Prometheus 资源，Operator 都会在相同 namespace 下部署成一个正确配置的 StatefulSet，Prometheus 的 Pod 都会挂载一个名为 &lt;prometheus-name&gt; 的 Secret，里面包含了 Prometheus 的配置。Operator 根据包含的 ServiceMonitor 生成配置，并且更新含有配置的 Secret。无论是对 ServiceMonitors 或者 Prometheus 的修改，都会持续不断的被按照前面的步骤更新。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Prometheus
metadata: # 略
spec:
  alerting:
    alertmanagers:
    - name: prometheus-prometheus-oper-alertmanager # 定义该 Prometheus 对接的 Alertmanager 集群的名字, 在 default 这个 namespace 中
      namespace: default
      pathPrefix: /
      port: web
  baseImage: quay.io/prometheus/prometheus
  replicas: 2 # 定义该 Proemtheus “集群”有两个副本，说是集群，其实 Prometheus 自身不带集群功能，这里只是起两个完全一样的 Prometheus 来避免单点故障
  ruleSelector: # 定义这个 Prometheus 需要使用带有 prometheus=k8s 且 role=alert-rules 标签的 PrometheusRule
    matchLabels:
      prometheus: k8s
      role: alert-rules
  serviceMonitorNamespaceSelector: {} # 定义这些 Prometheus 在哪些 namespace 里寻找 ServiceMonitor
  serviceMonitorSelector: # 定义这个 Prometheus 需要使用带有 k8s-app=node-exporter 标签的 ServiceMonitor，不声明则会全部选中
    matchLabels:
      k8s-app: node-exporter
  version: v2.10.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;servicemonitor&#34;&gt;ServiceMonitor&lt;/h3&gt;

&lt;p&gt;ServiceMonitor 自定义资源(CRD)能够声明如何监控一组动态服务的定义。它使用标签选择定义一组需要被监控的服务。主要通过Selector来依据 Labels 选取对应的Service的endpoints，并让 Prometheus Server 通过 Service 进行拉取指标。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: ServiceMonitor
metadata:
  labels:
    k8s-app: node-exporter # 这个 ServiceMonitor 对象带有 k8s-app=node-exporter 标签，因此会被 Prometheus 选中
  name: node-exporter
  namespace: default
spec:
  selector:
    matchLabels: # 定义需要监控的 Endpoints，带有 app=node-exporter 且 k8s-app=node-exporter标签的 Endpoints 会被选中
      app: node-exporter
      k8s-app: node-exporter
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    interval: 30s # 定义这些 Endpoints 需要每 30 秒抓取一次
    targetPort: 9100 # 定义这些 Endpoints 的指标端口为 9100
    scheme: https
  jobLabel: k8s-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spec 的 endpoints 部分用于配置需要收集 metrics 的 Endpoints 的端口和其他参数。endpoints（小写）是 ServiceMonitor CRD 中的一个字段，而 Endpoints（大写）是 Kubernetes 资源类型。&lt;/p&gt;

&lt;p&gt;Spec 下的 namespaceSelector 可以现在允许发现 Endpoints 对象的命名空间。要发现所有命名空间下的目标，namespaceSelector 必须为空。&lt;/p&gt;

&lt;h3 id=&#34;alertmanager&#34;&gt;Alertmanager&lt;/h3&gt;

&lt;p&gt;Alertmanager 自定义资源(CRD)声明在 Kubernetes 集群中运行的 Alertmanager 的期望设置。它也提供了配置副本集和持久化存储的选项。&lt;/p&gt;

&lt;p&gt;每一个 Alertmanager 资源，Operator 都会在相同 namespace 下部署成一个正确配置的 StatefulSet。Alertmanager pods 配置挂载一个名为 &lt;alertmanager-name&gt; 的 Secret， 使用 alertmanager.yaml key 对作为配置文件。&lt;/p&gt;

&lt;p&gt;当有两个或更多配置的副本时，Operator 可以高可用性模式运行Alertmanager实例。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Alertmanager #  一个 Alertmanager 对象
metadata:
  name: prometheus-prometheus-oper-alertmanager
spec:
  baseImage: quay.io/prometheus/alertmanager
  replicas: 3      # 定义该 Alertmanager 集群的节点数为 3
  version: v0.17.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;prometheusrule&#34;&gt;PrometheusRule&lt;/h3&gt;

&lt;p&gt;PrometheusRule CRD 声明一个或多个 Prometheus 实例需要的 Prometheus rule。&lt;/p&gt;

&lt;p&gt;Alerts 和 recording rules 可以保存并应用为 yaml 文件，可以被动态加载而不需要重启。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PrometheusRule
metadata:
  labels: # 定义该 PrometheusRule 的 label, 显然它会被 Prometheus 选中
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
spec:
  groups:
  - name: k8s.rules
    rules: # 定义了一组规则，其中只有一条报警规则，用来报警 kubelet 是不是挂了
    - alert: KubeletDown
      annotations:
        message: Kubelet has disappeared from Prometheus target discovery.
      expr: |
        absent(up{job=&amp;quot;kubelet&amp;quot;} == 1)
      for: 15m
      labels:
        severity: critical
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;thanosruler&#34;&gt;ThanosRuler&lt;/h3&gt;

&lt;p&gt;ThanosRuler 自定义资源(CRD)声明在 Kubernetes 集群中运行的 thanos 的期望设置。它也提供了配置副本集和持久化存储的选项。&lt;/p&gt;

&lt;h3 id=&#34;podmonitor&#34;&gt;PodMonitor&lt;/h3&gt;

&lt;p&gt;直接对接pod。&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;h2 id=&#34;部署prometheus-operater&#34;&gt;部署prometheus-operater&lt;/h2&gt;

&lt;p&gt;operater安装直接使用yaml安装就好了，先clone项目&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/coreos/prometheus-operator.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在项目目录下有一个bundle.yaml定义了各种crd资源和operater的镜像启动配置清单，直接运行就好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:prometheus-operator chunyinjiang$ kubectl apply -f bundle.yaml
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
serviceaccount/prometheus-operator created
service/prometheus-operator created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见在default的namespace下创建了prometheus-operator的sa，service，deployment应用，还有授权role以及CRD。&lt;/p&gt;

&lt;p&gt;最新的版本官方将资源&lt;a href=&#34;https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus&#34;&gt;https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus&lt;/a&gt; 迁移到了独立的git仓库中：&lt;a href=&#34;https://github.com/coreos/kube-prometheus.git，&#34;&gt;https://github.com/coreos/kube-prometheus.git，&lt;/a&gt; 我们也可以直接使用这里面setup的yaml文件来部署prometheus-operater，这个项目中还有prometheus相关生态的部署yaml，可以参考使用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/coreos/kube-prometheus.git
cd manifests/setup
$ ls
00namespace-namespace.yaml                                         node-exporter-clusterRole.yaml
0prometheus-operator-0alertmanagerCustomResourceDefinition.yaml    node-exporter-daemonset.yaml
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后直接部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:setup chunyinjiang$ kubectl apply -f .
namespace/monitoring created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
service/prometheus-operator created
serviceaccount/prometheus-operator created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get all -n monitoring
NAME                                       READY   STATUS    RESTARTS   AGE
pod/prometheus-operator-6f98f66b89-dggn6   2/2     Running   0          20h

NAME                          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
service/prometheus-operator   ClusterIP   None         &amp;lt;none&amp;gt;        8443/TCP   20h

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/prometheus-operator   1/1     1            1           20h

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/prometheus-operator-6f98f66b89   1         1         1       20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看crd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get crd | grep monitoring
alertmanagers.monitoring.coreos.com     2020-06-19T11:34:06Z
podmonitors.monitoring.coreos.com       2020-06-19T11:34:06Z
prometheuses.monitoring.coreos.com      2020-06-19T11:34:06Z
prometheusrules.monitoring.coreos.com   2020-06-19T11:34:06Z
servicemonitors.monitoring.coreos.com   2020-06-19T11:34:07Z
thanosrulers.monitoring.coreos.com      2020-06-19T11:34:07Z
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;部署prometheus生态&#34;&gt;部署prometheus生态&lt;/h2&gt;

&lt;p&gt;直接使用kube-prometheus的yaml进行部署&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl apply -f .
alertmanager.monitoring.coreos.com/main created
secret/alertmanager-main created
service/alertmanager-main created
serviceaccount/alertmanager-main created
servicemonitor.monitoring.coreos.com/alertmanager created
secret/grafana-datasources created
configmap/grafana-dashboard-apiserver created
configmap/grafana-dashboard-cluster-total created
configmap/grafana-dashboard-controller-manager created
configmap/grafana-dashboard-k8s-resources-cluster created
configmap/grafana-dashboard-k8s-resources-namespace created
configmap/grafana-dashboard-k8s-resources-node created
configmap/grafana-dashboard-k8s-resources-pod created
configmap/grafana-dashboard-k8s-resources-workload created
configmap/grafana-dashboard-k8s-resources-workloads-namespace created
configmap/grafana-dashboard-kubelet created
configmap/grafana-dashboard-namespace-by-pod created
configmap/grafana-dashboard-namespace-by-workload created
configmap/grafana-dashboard-node-cluster-rsrc-use created
configmap/grafana-dashboard-node-rsrc-use created
configmap/grafana-dashboard-nodes created
configmap/grafana-dashboard-persistentvolumesusage created
configmap/grafana-dashboard-pod-total created
configmap/grafana-dashboard-prometheus-remote-write created
configmap/grafana-dashboard-prometheus created
configmap/grafana-dashboard-proxy created
configmap/grafana-dashboard-scheduler created
configmap/grafana-dashboard-statefulset created
configmap/grafana-dashboard-workload-total created
configmap/grafana-dashboards created
deployment.apps/grafana created
service/grafana created
serviceaccount/grafana created
servicemonitor.monitoring.coreos.com/grafana created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
deployment.apps/kube-state-metrics created
service/kube-state-metrics created
serviceaccount/kube-state-metrics created
servicemonitor.monitoring.coreos.com/kube-state-metrics created
clusterrole.rbac.authorization.k8s.io/node-exporter created
clusterrolebinding.rbac.authorization.k8s.io/node-exporter created
daemonset.apps/node-exporter created
service/node-exporter created
serviceaccount/node-exporter created
servicemonitor.monitoring.coreos.com/node-exporter created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io configured
clusterrole.rbac.authorization.k8s.io/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created
clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created
clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created
configmap/adapter-config created
deployment.apps/prometheus-adapter created
rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created
service/prometheus-adapter created
serviceaccount/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/prometheus-k8s created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus-operator created
prometheus.monitoring.coreos.com/k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s-config created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
prometheusrule.monitoring.coreos.com/prometheus-k8s-rules created
service/prometheus-k8s created
serviceaccount/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus created
servicemonitor.monitoring.coreos.com/kube-apiserver created
servicemonitor.monitoring.coreos.com/coredns created
servicemonitor.monitoring.coreos.com/kube-controller-manager created
servicemonitor.monitoring.coreos.com/kube-scheduler created
servicemonitor.monitoring.coreos.com/kubelet created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get all -n monitoring
NAME                                       READY   STATUS              RESTARTS   AGE
pod/alertmanager-main-0                    0/2     ContainerCreating   0          3m16s
pod/alertmanager-main-1                    0/2     ContainerCreating   0          3m16s
pod/alertmanager-main-2                    0/2     ContainerCreating   0          3m16s
pod/grafana-5c55845445-7tdhk               0/1     ContainerCreating   0          3m15s
pod/kube-state-metrics-957fd6c75-sqntg     0/3     ContainerCreating   0          3m14s
pod/node-exporter-tnftm                    0/2     ContainerCreating   0          3m14s
pod/prometheus-adapter-5cdcdf9c8d-xpxz4    1/1     Running             0          3m15s
pod/prometheus-k8s-0                       0/3     ContainerCreating   0          3m13s
pod/prometheus-k8s-1                       0/3     ContainerCreating   0          3m13s
pod/prometheus-operator-6f98f66b89-dggn6   2/2     Running             0          20h

NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
service/alertmanager-main       ClusterIP   10.106.202.8    &amp;lt;none&amp;gt;        9093/TCP                     3m17s
service/alertmanager-operated   ClusterIP   None            &amp;lt;none&amp;gt;        9093/TCP,9094/TCP,9094/UDP   3m17s
service/grafana                 ClusterIP   10.98.82.99     &amp;lt;none&amp;gt;        3000/TCP                     3m16s
service/kube-state-metrics      ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP,9443/TCP            3m16s
service/node-exporter           ClusterIP   None            &amp;lt;none&amp;gt;        9100/TCP                     3m15s
service/prometheus-adapter      ClusterIP   10.98.119.241   &amp;lt;none&amp;gt;        443/TCP                      3m15s
service/prometheus-k8s          ClusterIP   10.104.199.30   &amp;lt;none&amp;gt;        9090/TCP                     3m14s
service/prometheus-operated     ClusterIP   None            &amp;lt;none&amp;gt;        9090/TCP                     3m15s
service/prometheus-operator     ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP                     20h

NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/node-exporter   1         1         0       1            0           kubernetes.io/os=linux   3m15s

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana               0/1     1            0           3m16s
deployment.apps/kube-state-metrics    0/1     1            0           3m16s
deployment.apps/prometheus-adapter    1/1     1            1           3m15s
deployment.apps/prometheus-operator   1/1     1            1           20h

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-5c55845445               1         1         0       3m16s
replicaset.apps/kube-state-metrics-957fd6c75     1         1         0       3m16s
replicaset.apps/prometheus-adapter-5cdcdf9c8d    1         1         1       3m15s
replicaset.apps/prometheus-operator-6f98f66b89   1         1         1       20h

NAME                                 READY   AGE
statefulset.apps/alertmanager-main   0/3     3m17s
statefulset.apps/prometheus-k8s      0/2     3m15s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到资源正在创建，拉去镜像可能需要一点事件，其中 alertmanager 和 prometheus 是用 StatefulSet 控制器管理的。&lt;/p&gt;

&lt;p&gt;可以看到上面针对 grafana 和 prometheus 都创建了一个类型为 ClusterIP 的 Service，当然如果我们想要在外网访问这两个服务的话可以通过创建对应的 Ingress 对象或者使用 NodePort 类型的 Service，我们这里为了简单，直接使用 NodePort 类型的服务即可，编辑 grafana 和 prometheus-k8s 这两个 Service，将服务类型更改为 NodePort:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type: ClusterIp   --&amp;gt; NodePort

MacBook-Pro:manifests chunyinjiang$ kubectl edit svc prometheus-k8s -n monitoring
service/prometheus-k8s edited
MacBook-Pro:manifests chunyinjiang$ kubectl edit svc grafana -n monitoring
service/grafana edited
MacBook-Pro:manifests chunyinjiang$ kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.106.85.214   &amp;lt;none&amp;gt;        9093/TCP                     95m
alertmanager-operated   ClusterIP   None            &amp;lt;none&amp;gt;        9093/TCP,9094/TCP,9094/UDP   95m
grafana                 NodePort    10.106.80.1     &amp;lt;none&amp;gt;        3000:30267/TCP               95m
kube-state-metrics      ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP,9443/TCP            95m
node-exporter           ClusterIP   None            &amp;lt;none&amp;gt;        9100/TCP                     95m
prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      95m
prometheus-k8s          NodePort    10.103.177.44   &amp;lt;none&amp;gt;        9090:31174/TCP               95m
prometheus-operated     ClusterIP   None            &amp;lt;none&amp;gt;        9090/TCP                     95m
prometheus-operator     ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP                     22h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更改完成后，我们就可以通过NodeIP:NodePort去访问上面的两个服务了，比如查看 prometheus 的 targets 页面：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/operator/operater.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;至此基本的prometheus生态组件就部署好了，但是可以看到kube-controller-manager 和 kube-scheduler 这两个系统组件并没有taeget，这就和 ServiceMonitor 的定义有关系了，我们刚好研究一下ServiceMonitor&lt;/p&gt;

&lt;p&gt;我们查看ServiceMonitor这种crd的资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get servicemonitors.monitoring.coreos.com -n monitoring
NAME                      AGE
alertmanager              101m
coredns                   101m
grafana                   101m
kube-apiserver            101m
kube-controller-manager   101m
kube-scheduler            101m
kube-state-metrics        101m
kubelet                   101m
node-exporter             101m
prometheus                101m
prometheus-operator       101m
MacBook-Pro:manifests chunyinjiang$ kubectl describe servicemonitors.monitoring.coreos.com kube-scheduler -n monitoring
Name:         kube-scheduler
Namespace:    monitoring
Labels:       k8s-app=kube-scheduler
Annotations:  API Version:  monitoring.coreos.com/v1
Kind:         ServiceMonitor
Metadata:
  Creation Timestamp:  2020-06-20T08:04:50Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:kubectl.kubernetes.io/last-applied-configuration:
        f:labels:
          .:
          f:k8s-app:
      f:spec:
        .:
        f:endpoints:
        f:jobLabel:
        f:namespaceSelector:
          .:
          f:matchNames:
        f:selector:
          .:
          f:matchLabels:
            .:
            f:k8s-app:
    Manager:         kubectl
    Operation:       Update
    Time:            2020-06-20T08:04:50Z
  Resource Version:  862846
  Self Link:         /apis/monitoring.coreos.com/v1/namespaces/monitoring/servicemonitors/kube-scheduler
  UID:               07132145-1db1-4847-a2a1-347cc014a80e
Spec:
  Endpoints:
    Interval:  30s
    Port:      http-metrics
  Job Label:   k8s-app
  Namespace Selector:
    Match Names:
      kube-system
  Selector:
    Match Labels:
      k8s-app:  kube-scheduler
Events:         &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到每个监控的应用都是使用ServiceMonitor部署了。我们再来看看对应的资源配置清单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-serviceMonitorKubeScheduler.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: kube-scheduler
  name: kube-scheduler
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s
    port: http-metrics
  jobLabel: k8s-app
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      k8s-app: kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是一个典型的 ServiceMonitor 资源文件的声明方式，上面我们通过selector.matchLabels在 kube-system 这个命名空间下面匹配具有k8s-app=kube-scheduler这样的 Service，我们来看看service&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc -n kube-system -L k8s-app
NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE    K8S-APP
kube-dns         ClusterIP   10.96.0.10      &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP         11d    kube-dns
kubelet          ClusterIP   None            &amp;lt;none&amp;gt;        10250/TCP,10255/TCP,4194/TCP   26h    kubelet
metrics-server   ClusterIP   10.111.196.64   &amp;lt;none&amp;gt;        443/TCP                        2d6h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是我们系统中根本就没有对应的 Service，所以我们需要手动创建一个 Service：（prometheus-kubeSchedulerService.yaml）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-scheduler
  labels:
    k8s-app: kube-scheduler
spec:
  selector:
    component: kube-scheduler
  ports:
  - name: http-metrics
    port: 10251
    targetPort: 10251
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10251是kube-scheduler组件 metrics 数据所在的端口，10252是kube-controller-manager组件的监控数据所在端口。&lt;/p&gt;

&lt;p&gt;其中最重要的是上面 labels 和 selector 部分，labels 区域的配置必须和我们上面的 ServiceMonitor 对象中的 selector 保持一致，selector下面配置的是component=kube-scheduler，为什么会是这个 label 标签呢，我们可以去 describe 下 kube-scheduelr 这个 Pod：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pod kube-scheduler-minikube -n kube-system
Name:                 kube-scheduler-minikube
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 minikube/192.168.99.101
Start Time:           Sat, 20 Jun 2020 17:25:48 +0800
Labels:               component=kube-scheduler
                      tier=control-plane
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到这个 Pod 具有component=kube-scheduler和tier=control-plane这两个标签，而前面这个标签具有更唯一的特性，所以使用前面这个标签较好，这样上面创建的 Service 就可以和我们的 Pod 进行关联了，直接创建即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl apply -f prometheus-kubeSchedulerService.yaml
service/kube-scheduler created
MacBook-Pro:manifests chunyinjiang$ kubectl get svc -n kube-system -L k8s-app
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                        AGE    K8S-APP
kube-dns         ClusterIP   10.96.0.10       &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP         14d    kube-dns
kube-scheduler   ClusterIP   10.105.229.159   &amp;lt;none&amp;gt;        10251/TCP                      3m5s   kube-scheduler
kubelet          ClusterIP   None             &amp;lt;none&amp;gt;        10250/TCP,10255/TCP,4194/TCP   4d1h   kubelet
metrics-server   ClusterIP   10.111.196.64    &amp;lt;none&amp;gt;        443/TCP                        5d4h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 targets 下面 kube-scheduler 的状态，可以看到已经发现了，并且有数据了。&lt;/p&gt;

&lt;p&gt;kube-controller-manager 也是一样的操作。下面我们就可以通过自定义的grafana视图来进行监控了。&lt;/p&gt;

&lt;h2 id=&#34;部署详情&#34;&gt;部署详情&lt;/h2&gt;

&lt;h3 id=&#34;prometheus-1&#34;&gt;prometheus&lt;/h3&gt;

&lt;p&gt;prometheus的所有信息都能重prometheus的ui界面进行查看，主要查看status的状态，我们重容器中查看一下。&lt;/p&gt;

&lt;p&gt;上面我们知道使用有状态的statefulset部署两个prometheus，我们来看一下他们的具体情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get pod -n monitoring | grep prometheus-k8s
prometheus-k8s-0                       3/3     Running   17         3d1h
prometheus-k8s-1                       3/3     Running   17         3d1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个pod中有三个容器，可以使用 kubectl describe pod prometheus-k8s-0 -n monitoring来查看这三个容器分别是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus：quay.io/prometheus/prometheus:v2.17.2
prometheus-config-reloader：quay.io/coreos/prometheus-config-reloader:v0.39.0
rules-configmap-reloader：jimmidyson/configmap-reload:v0.3.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们到prometheus中看看，可以理解这个就是启动了prometheus的实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl exec -ti prometheus-k8s-0 -c prometheus -n monitoring -- sh
/prometheus $ ps -ef | grep prome
    1 1000     11:13 /bin/prometheus --web.console.templates=/etc/prometheus/consoles --web.console.libraries=/etc/prometheus/console_libraries --config.file=/etc/prometheus/config_out/prometheus.env.yaml --storage.tsdb.path=/prometheus --storage.tsdb.retention.time=24h --web.enable-lifecycle --storage.tsdb.no-lockfile --web.route-prefix=/
   52 1000      0:00 grep prome
/prometheus $ cat /etc/prometheus/config_out/prometheus.env.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在节点中可以看到&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以看到每个job就是监控的一个组件，也就是serviceMonitor。主要监听组件alertmanager，coredns，grafana，kube-apiserver，kube-controller-manager，kube-scheduler，kube-state-metrics，kubelet，node-exporter，prometheus，prometheus-operator。&lt;/li&gt;
&lt;li&gt;在配置文件中并没有使用hash的模式来分集群进行采集，这边两个prometheus节点是双采，解决来单点问题&lt;/li&gt;
&lt;li&gt;使用的是kubernetes_sd_configs的服务发现模式&lt;/li&gt;
&lt;li&gt;数据存储24h，存储在/prometheus目录下&lt;/li&gt;
&lt;li&gt;监听端口9090，我们可以使用nodeport或者ingress来对外暴露&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个简单的部署只适合简单的小集群的使用，使用大集群的监控还需要将数据进行分片采集，远程存储聚合等方案。&lt;/p&gt;

&lt;p&gt;我们再看看config的container是做什么的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti prometheus-k8s-0 -c prometheus-config-reloader -n monitoring -- sh
/ $ ps -ef
PID   USER     TIME  COMMAND
    1 1000      0:01 /bin/prometheus-config-reloader --log-format=logfmt --reload-url=http://localhost:9090/-/reload --config-file=/etc/prometheus/config/prometheus.yaml.gz --config-envsubst-file=/etc/prometheus/config_out/pro
$ kubectl exec -ti prometheus-k8s-0 -c rules-configmap-reloader -n monitoring -- sh
/ $ ps -ef
\PID   USER     TIME  COMMAND
    1 1000      0:00 /configmap-reload --webhook-url=http://localhost:9090/-/reload --volume-dir=/etc/prometheus/rules/prometheus-k8s-rulefiles-0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实就是对配置文件和rule文件进行热加载。&lt;/p&gt;

&lt;h3 id=&#34;grafana&#34;&gt;grafana&lt;/h3&gt;

&lt;p&gt;grafana也是直接在k8s中用deployment进行部署的，只有一个节点&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod -n monitoring | grep grafana
grafana-5c55845445-bnln8               1/1     Running   2          3d3h
MacBook-Pro:manifests chunyinjiang$ kubectl exec -ti grafana-5c55845445-bnln8 -n monitoring -- sh
/usr/share/grafana $ ps -ef | grep grafana
    1 nobody    4:36 grafana-server --homepath=/usr/share/grafana --config=/etc/grafana/grafana.ini --packaging=docker cfg:default.log.mode=console cfg:default.paths.data=/var/lib/grafana cfg:default.paths.logs=/var/log/grafana cfg:default.paths.plugins=/var/lib/grafana/plugins cfg:default.paths.provisioning=/etc/grafana/provisioning
   30 nobody    0:00 grep grafana
/usr/share/grafana $ cat /etc/grafana/grafana.ini
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到一些内容&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;监听端口3000，我们可以使用nodeport或者ingress来对外暴露&lt;/li&gt;
&lt;li&gt;没有使用mysql数据库，使用sqlite数据库&lt;/li&gt;
&lt;li&gt;直接通过域名访问prometheus：&lt;a href=&#34;http://prometheus-k8s.monitoring.svc:9090&#34;&gt;http://prometheus-k8s.monitoring.svc:9090&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;alertmanager-1&#34;&gt;alertmanager&lt;/h3&gt;

&lt;p&gt;alertmanager使用的也是statefulset的方式进行部署的，我们看一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MacBook-Pro:manifests chunyinjiang$ kubectl get pod -n monitoring | grep alert
alertmanager-main-0                    2/2     Running   6          3d3h
alertmanager-main-1                    2/2     Running   7          3d3h
alertmanager-main-2                    2/2     Running   5          3d3h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个pod也有两个container。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;alertmanager：quay.io/prometheus/alertmanager:v0.20.0
config-reloader：jimmidyson/configmap-reload:v0.3.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;config-reloader肯定就是配置加载，我们看看alertmanager的实例吧&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti alertmanager-main-0 -c alertmanager -n monitoring -- sh
/alertmanager $ ps
PID   USER     TIME  COMMAND
    1 1000      2:12 /bin/alertmanager --config.file=/etc/alertmanager/config/alertmanager.yaml --cluster.listen-address=[172.17.0.35]:9094 --storage.path=/alertmanager --data.retention=120h --web.listen-address=:9093 --web.ro
   21 1000      0:00 sh
   26 1000      0:00 ps
/alertmanager $ cat /etc/alertmanager/config/alertmanager.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;adapter&#34;&gt;adapter&lt;/h3&gt;

&lt;p&gt;kubernetes apiserver 提供了两种 api 用于监控指标相关的操作，&lt;/p&gt;

&lt;p&gt;k8s-prometheus-adapter是将prometheus的metrics数据格式转换成k8s API接口能识别的格式，同时通过apiservice扩展的模式（声明apiservice）注册到kube-apiserver来给k8s进行调用。&lt;/p&gt;

&lt;p&gt;查看adapter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get all -n monitoring | grep adapter
pod/prometheus-adapter-66b9c9dd58-6bdbm    1/1     Running   0          14h
service/prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      8d
deployment.apps/prometheus-adapter    1/1     1            1           8d
replicaset.apps/prometheus-adapter-5cdcdf9c8d    0         0         0       8d
replicaset.apps/prometheus-adapter-66b9c9dd58    1         1         1       14h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用deployment来部署了两个副本的k8s-prometheus-adapter，然后启动了一个service。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ps -ef | grep adapter | grep -v grep
dbus     23281 23263  0 00:13 ?        00:00:20 /adapter --cert-dir=/var/run/serving-cert --config=/etc/adapter/config.yaml --logtostderr=true --metrics-relist-interval=1m --prometheus-url=http://192.168.99.101:31174/ --secure-port=6443
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--lister-kubeconfig=&amp;lt;path-to-kubeconfig&amp;gt;: 指定通信的kubeconfig
--metrics-relist-interval=&amp;lt;duration&amp;gt;: 获取指标的间隔，应该大于prometheus的采集间隔时间。
--prometheus-url=&amp;lt;url&amp;gt;: 连接到Prometheus的URL。
--config=&amp;lt;yaml-file&amp;gt; (-c): 配置文件，主要是Prometheus指标和关联的Kubernetes资源，以及如何在自定义指标API中显示这些指标。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再来看看配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rules:
- seriesQuery: &#39;nginx_vts_server_requests_total&#39;
  seriesFilters: []
  resources:
    overrides:
      kubernetes_namespace:
        resource: namespace
      kubernetes_pod_name:
        resource: pod
  name:
    matches: &amp;quot;^(.*)_total&amp;quot;
    as: &amp;quot;${1}_per_second&amp;quot;
  metricsQuery: (sum(rate(&amp;lt;&amp;lt;.Series&amp;gt;&amp;gt;{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}[1m])) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个带参数的 Prometheus 查询，其中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;seriesQuery：查询 Prometheus 的语句，通过这个查询语句查询到的所有指标都可以用于 HPA
seriesFilters：查询到的指标可能会存在不需要的，可以通过它过滤掉。
resources：通过 seriesQuery 查询到的只是指标，如果需要查询某个 Pod 的指标，肯定要将它的名称和所在的命名空间作为指标的标签进行查询，resources 就是将指标的标签和 k8s 的资源类型关联起来，最常用的就是 pod 和 namespace。有两种添加标签的方式，一种是 overrides，另一种是 template。

    overrides：它会将指标中的标签和 k8s 资源关联起来。上面示例中就是将指标中的 pod 和 namespace 标签和 k8s 中的 pod 和 namespace 关联起来，因为 pod 和 namespace 都属于核心 api 组，所以不需要指定 api 组。当我们查询某个 pod 的指标时，它会自动将 pod 的名称和名称空间作为标签加入到查询条件中。比如 nginx: {group: &amp;quot;apps&amp;quot;, resource: &amp;quot;deployment&amp;quot;} 这么写表示的就是将指标中 nginx 这个标签和 apps 这个 api 组中的 deployment 资源关联起来；
    template：通过 go 模板的形式。比如template: &amp;quot;kube_&amp;lt;&amp;lt;.Group&amp;gt;&amp;gt;_&amp;lt;&amp;lt;.Resource&amp;gt;&amp;gt;&amp;quot; 这么写表示，假如 &amp;lt;&amp;lt;.Group&amp;gt;&amp;gt; 为 apps，&amp;lt;&amp;lt;.Resource&amp;gt;&amp;gt; 为 deployment，那么它就是将指标中 kube_apps_deployment 标签和 deployment 资源关联起来。

name：用来给指标重命名的，之所以要给指标重命名是因为有些指标是只增的，比如以 total 结尾的指标。这些指标拿来做 HPA 是没有意义的，我们一般计算它的速率，以速率作为值，那么此时的名称就不能以 total 结尾了，所以要进行重命名。

    matches：通过正则表达式来匹配指标名，可以进行分组
    as：默认值为 $1，也就是第一个分组。as 为空就是使用默认值的意思。

metricsQuery：这就是 Prometheus 的查询语句了，前面的 seriesQuery 查询是获得 HPA 指标。当我们要查某个指标的值时就要通过它指定的查询语句进行了。可以看到查询语句使用了速率和分组，这就是解决上面提到的只增指标的问题。

    Series：表示指标名称
    LabelMatchers：附加的标签，目前只有 pod 和 namespace 两种，因此我们要在之前使用 resources 进行关联
    GroupBy：就是 pod 名称，同样需要使用 resources 进行关联。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看一下项目的的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-adapter-configMap.yaml
apiVersion: v1
data:
  config.yaml: |-
    &amp;quot;resourceRules&amp;quot;:
      &amp;quot;cpu&amp;quot;:
        &amp;quot;containerLabel&amp;quot;: &amp;quot;container&amp;quot;
        &amp;quot;containerQuery&amp;quot;: &amp;quot;sum(irate(container_cpu_usage_seconds_total{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;,container!=\&amp;quot;POD\&amp;quot;,container!=\&amp;quot;\&amp;quot;,pod!=\&amp;quot;\&amp;quot;}[5m])) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;nodeQuery&amp;quot;: &amp;quot;sum(1 - irate(node_cpu_seconds_total{mode=\&amp;quot;idle\&amp;quot;}[5m]) * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;resources&amp;quot;:
          &amp;quot;overrides&amp;quot;:
            &amp;quot;namespace&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;namespace&amp;quot;
            &amp;quot;node&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;node&amp;quot;
            &amp;quot;pod&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;pod&amp;quot;
      &amp;quot;memory&amp;quot;:
        &amp;quot;containerLabel&amp;quot;: &amp;quot;container&amp;quot;
        &amp;quot;containerQuery&amp;quot;: &amp;quot;sum(container_memory_working_set_bytes{&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;,container!=\&amp;quot;POD\&amp;quot;,container!=\&amp;quot;\&amp;quot;,pod!=\&amp;quot;\&amp;quot;}) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;nodeQuery&amp;quot;: &amp;quot;sum(node_memory_MemTotal_bytes{job=\&amp;quot;node-exporter\&amp;quot;,&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;} - node_memory_MemAvailable_bytes{job=\&amp;quot;node-exporter\&amp;quot;,&amp;lt;&amp;lt;.LabelMatchers&amp;gt;&amp;gt;}) by (&amp;lt;&amp;lt;.GroupBy&amp;gt;&amp;gt;)&amp;quot;
        &amp;quot;resources&amp;quot;:
          &amp;quot;overrides&amp;quot;:
            &amp;quot;instance&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;node&amp;quot;
            &amp;quot;namespace&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;namespace&amp;quot;
            &amp;quot;pod&amp;quot;:
              &amp;quot;resource&amp;quot;: &amp;quot;pod&amp;quot;
      &amp;quot;window&amp;quot;: &amp;quot;5m&amp;quot;
kind: ConfigMap
metadata:
  name: adapter-config
  namespace: monitoring
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置了两个规则，可以到prometheus的界面查看一下是可以查到的，下面我们需要提供给k8s，我们就需要使用聚合api，先注册apiservice&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl api-versions  | grep metrics
metrics.k8s.io/v1beta1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们就可以重/apis/metrics.k8s.io/v1beta1这个URL来获取指标，这是给resource metrics 使用的,主要是提供核心指标，这边是指向k8s-prometheus-adapter，所以是prometheus的采集的指标。&lt;/p&gt;

&lt;p&gt;还有是自定义指标，只要是prometheus采集的指标，都可以在上面的配置文件配置，然后都可以通过这个接口查询到，这种情况其实一般使用custom.metrics.k8s.io api接口来操作，&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-autoscaler/#基于prometheus&#34;&gt;具体使用可以在hpa场景下查看&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get --raw=&amp;quot;/apis/metrics.k8s.io/v1beta1&amp;quot;
{
    &amp;quot;kind&amp;quot;:&amp;quot;APIResourceList&amp;quot;,
    &amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,
    &amp;quot;groupVersion&amp;quot;:&amp;quot;metrics.k8s.io/v1beta1&amp;quot;,
    &amp;quot;resources&amp;quot;:[
        {
            &amp;quot;name&amp;quot;:&amp;quot;nodes&amp;quot;,
            &amp;quot;singularName&amp;quot;:&amp;quot;&amp;quot;,
            &amp;quot;namespaced&amp;quot;:false,
            &amp;quot;kind&amp;quot;:&amp;quot;NodeMetrics&amp;quot;,
            &amp;quot;verbs&amp;quot;:[
                &amp;quot;get&amp;quot;,
                &amp;quot;list&amp;quot;
            ]
        },
        {
            &amp;quot;name&amp;quot;:&amp;quot;pods&amp;quot;,
            &amp;quot;singularName&amp;quot;:&amp;quot;&amp;quot;,
            &amp;quot;namespaced&amp;quot;:true,
            &amp;quot;kind&amp;quot;:&amp;quot;PodMetrics&amp;quot;,
            &amp;quot;verbs&amp;quot;:[
                &amp;quot;get&amp;quot;,
                &amp;quot;list&amp;quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看出来这个接口主要获取了核心资源指标，比如nodes，pods，可以具体去查看一下。&lt;/p&gt;

&lt;h3 id=&#34;其他组件&#34;&gt;其他组件&lt;/h3&gt;

&lt;p&gt;主要是给proemtheus的采集的探针&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kube-state-metrics主要是为了暴露集群的一些状态
node-exporter主要是获取主机信息
其他组件集成prometheus的库，通过端口直接暴露metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;servicemonitor-1&#34;&gt;serviceMonitor&lt;/h3&gt;

&lt;p&gt;将所有组件的监控通过serviceMonitor来暴露采集。&lt;/p&gt;

&lt;h1 id=&#34;使用&#34;&gt;使用&lt;/h1&gt;

&lt;h2 id=&#34;自定义servicemonitor&#34;&gt;自定义servicemonitor&lt;/h2&gt;

&lt;p&gt;添加一个自定义监控的步骤&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;第一步建立一个 ServiceMonitor 对象，用于 Prometheus 添加监控项
第二步为 ServiceMonitor 对象关联 metrics 数据接口的一个 Service 对象
第三步确保 Service 对象可以正确获取到 metrics 数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们以监控etcd为实例&lt;/p&gt;

&lt;p&gt;etcd 集群一般情况下，为了安全都会开启 https 证书认证的方式，所以要想让 Prometheus 访问到 etcd 集群的监控数据，就需要提供相应的证书校验。查看证书&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pod etcd-minikube  -n kube-system
...
    Command:
      etcd
      --advertise-client-urls=https://192.168.99.101:2379
      --cert-file=/var/lib/minikube/certs/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/minikube/etcd
      --initial-advertise-peer-urls=https://192.168.99.101:2380
      --initial-cluster=minikube=https://192.168.99.101:2380
      --key-file=/var/lib/minikube/certs/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://192.168.99.101:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://192.168.99.101:2380
      --name=minikube
      --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/var/lib/minikube/certs/etcd/peer.key
      --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到证书都在/var/lib/minikube/certs/etcd/下面，我们也可以通过kubectl get pod etcd-minikube  -n kube-system -o yaml来获取对应的配置，我们再来看看这个目录下的证书&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -lrth /var/lib/minikube/certs/etcd/
total 32K
-rw------- 1 root root 1.7K Jun  9 01:38 ca.key
-rw-r--r-- 1 root root 1017 Jun  9 01:38 ca.crt
-rw------- 1 root root 1.7K Jun  9 01:38 server.key
-rw-r--r-- 1 root root 1.2K Jun  9 01:38 server.crt
-rw------- 1 root root 1.7K Jun  9 01:38 peer.key
-rw-r--r-- 1 root root 1.2K Jun  9 01:38 peer.crt
-rw------- 1 root root 1.7K Jun  9 01:38 healthcheck-client.key
-rw-r--r-- 1 root root 1.1K Jun  9 01:38 healthcheck-client.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们需要将证书放到secret中给promehteus使用验证，创建secret就需要把这些证书拉到本地来进行创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret generic etcd-certs -n monitoring --from-file=./healthcheck-client.crt --from-file=./healthcheck-client.key --from-file=./ca.crt
secret/etcd-certs created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后将上面创建的 etcd-certs 对象配置到 prometheus 资源对象中，直接更新 prometheus 资源对象即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get prometheus -n monitoring
NAME   VERSION   REPLICAS   AGE
k8s    v2.17.2   2          3d22h
$ kubectl edit prometheus k8s -n monitoring
prometheus.monitoring.coreos.com/k8s edited
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要在spec中新增secret给prometheus使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nodeSelector:
  beta.kubernetes.io/os: linux
replicas: 2
secrets:
- etcd-certs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新完成后，我们就可以在 Prometheus 的 Pod 中获取到上面创建的 etcd 证书文件了，具体的路径我们可以进入 Pod 中查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -it prometheus-k8s-0 -c prometheus /bin/sh -n monitoring
/prometheus $ ls -lrth /etc/prometheus/secrets/etcd-certs/
total 0
lrwxrwxrwx    1 root     root          29 Jun 24 06:48 healthcheck-client.key -&amp;gt; ..data/healthcheck-client.key
lrwxrwxrwx    1 root     root          29 Jun 24 06:48 healthcheck-client.crt -&amp;gt; ..data/healthcheck-client.crt
lrwxrwxrwx    1 root     root          13 Jun 24 06:48 ca.crt -&amp;gt; ..data/ca.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面就是创建ServiceMonitor资源配置清单prometheus-serviceMonitorEtcd.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: etcd-k8s
  namespace: monitoring
  labels:
    k8s-app: etcd-k8s
spec:
  jobLabel: k8s-app
  endpoints:
  - port: port
    interval: 30s
    scheme: https
    tlsConfig:
      caFile: /etc/prometheus/secrets/etcd-certs/ca.crt
      certFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.crt
      keyFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.key
      insecureSkipVerify: true
  selector:
    matchLabels:
      k8s-app: etcd
  namespaceSelector:
    matchNames:
    - kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在 monitoring 命名空间下面创建了名为 etcd-k8s 的 ServiceMonitor 对象&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;匹配 kube-system 这个命名空间下面的具有 k8s-app=etcd 这个 label 标签的 Service&lt;/li&gt;
&lt;li&gt;jobLabel 表示用于检索 job 任务名称的标签&lt;/li&gt;
&lt;li&gt;和前面不太一样的地方是 endpoints 属性的写法，配置上访问 etcd 的相关证书，endpoints 属性下面可以配置很多抓取的参数，比如 relabel、proxyUrl，tlsConfig 表示用于配置抓取监控数据端点的 tls 认证，由于证书 serverName 和 etcd 中签发的可能不匹配，所以加上了 insecureSkipVerify=true&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后就是创建来这个资源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f prometheus-serviceMonitorEtcd.yaml
servicemonitor.monitoring.coreos.com &amp;quot;etcd-k8s&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个时候promehteus的配置文件中就新增一个job为etcd的监控，target是etcd的service，但是现在还没有关联的对应的 Service 对象，所以需要我们去手动创建一个 Service 对象prometheus-etcdService.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: port
    port: 2379
    protocol: TCP

---
apiVersion: v1
kind: Endpoints
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
subsets:
- addresses:
  - ip: 10.151.30.57
    nodeName: etc-master
  ports:
  - name: port
    port: 2379
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们这里创建的 Service 没有采用前面通过 label 标签的形式去匹配 Pod 的做法，因为前面我们说过很多时候我们创建的 etcd 集群是独立于集群之外的，这种情况下面我们就需要自定义一个 Endpoints，要注意 metadata 区域的内容要和 Service 保持一致，Service 的 clusterIP 设置为 None&lt;/p&gt;

&lt;p&gt;Endpoints 的 subsets 中填写 etcd 集群的地址即可&lt;/p&gt;

&lt;p&gt;创建&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f prometheus-etcdService.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面讲解的是独立于k8s之外的监控访问，前提是需要把网络打通，也是可以直接使用endpoint进行配置的，当然在集群内的监控常规就是匹配的pod的label。&lt;/p&gt;

&lt;p&gt;比如我们把etcd运行在k8s上，我们创建的service就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-etcdService.yaml
apiVersion: v1
kind: Service
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
spec:
  selector:
    component: etcd
  ports:
  - name: port
    port: 2379
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体在上面的schduler讲解过，不多说了，到此一个完整的监控就新增好了。&lt;/p&gt;

&lt;h2 id=&#34;自定义告警规则&#34;&gt;自定义告警规则&lt;/h2&gt;

&lt;p&gt;我们首先查看prometheus部署的时候的alert的配置，可以在prometheus的ui界面的config下查到&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;alerting:
  alert_relabel_configs:
  - separator: ;
    regex: prometheus_replica
    replacement: $1
    action: labeldrop
  alertmanagers:
  - kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - monitoring
    scheme: http
    path_prefix: /
    timeout: 10s
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name]
      separator: ;
      regex: alertmanager-main
      replacement: $1
      action: keep
    - source_labels: [__meta_kubernetes_endpoint_port_name]
      separator: ;
      regex: web
      replacement: $1
      action: keep
rule_files:
- /etc/prometheus/rules/prometheus-k8s-rulefiles-0/*.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过角色为 endpoints 的 kubernetes 的服务发现机制来知道需要发送的alert的地址&lt;/li&gt;
&lt;li&gt;匹配的是服务名为 alertmanager-main，端口名为 web 的 Service 服务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来看一下operator部署的svc&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.106.85.214   &amp;lt;none&amp;gt;        9093/TCP                     4d
alertmanager-operated   ClusterIP   None            &amp;lt;none&amp;gt;        9093/TCP,9094/TCP,9094/UDP   4d
grafana                 NodePort    10.106.80.1     &amp;lt;none&amp;gt;        3000:30267/TCP               4d
kube-state-metrics      ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP,9443/TCP            4d
node-exporter           ClusterIP   None            &amp;lt;none&amp;gt;        9100/TCP                     4d
prometheus-adapter      ClusterIP   10.97.73.122    &amp;lt;none&amp;gt;        443/TCP                      4d
prometheus-k8s          NodePort    10.103.177.44   &amp;lt;none&amp;gt;        9090:31174/TCP               4d
prometheus-operated     ClusterIP   None            &amp;lt;none&amp;gt;        9090/TCP                     4d
prometheus-operator     ClusterIP   None            &amp;lt;none&amp;gt;        8443/TCP                     4d20h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确实有一个alertmanager-main的svc，我们查看一下他的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe svc alertmanager-main -n monitoring
Name:              alertmanager-main
Namespace:         monitoring
Labels:            alertmanager=main
Annotations:       Selector:  alertmanager=main,app=alertmanager
Type:              ClusterIP
IP:                10.106.85.214
Port:              web  9093/TCP
TargetPort:        web/TCP
Endpoints:         172.17.0.22:9093,172.17.0.7:9093,172.17.0.9:9093
Session Affinity:  ClientIP
Events:            &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到服务名正是 alertmanager-main，Port 定义的名称也是 web，符合上面的规则，所以 Prometheus 和 AlertManager 组件就正确关联上了。我们就可以将告警发送到对应的alertmanaager了。&lt;/p&gt;

&lt;p&gt;再来看告警规则在/etc/prometheus/rules/prometheus-k8s-rulefiles-0/目录下面所有的 YAML 文件。我们在部署prometheus的时候有一个规则资源配置清单prometheus-rules.yaml，就是我们现在看到的告警规则。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
  namespace: monitoring
spec:
  groups:
  - name: k8s.rules
    rules:
    - expr: |
        sum(rate(container_cpu_usage_seconds_total{job=&amp;quot;kubelet&amp;quot;, image!=&amp;quot;&amp;quot;, container_name!=&amp;quot;&amp;quot;}[5m])) by (namespace)
      record: namespace:container_cpu_usage_seconds_total:sum_rate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PrometheusRule 的 name 为 prometheus-k8s-rules，namespace 为 monitoring，我们可以猜想到我们创建一个 PrometheusRule 资源对象后，会自动在上面的 prometheus-k8s-rulefiles-0 目录下面生成一个对应的&lt;namespace&gt;-&lt;name&gt;.yaml文件，所以如果以后我们需要自定义一个报警选项的话，只需要定义一个 PrometheusRule 资源对象即可。&lt;/p&gt;

&lt;p&gt;至于为什么 Prometheus 能够识别这个 PrometheusRule 资源对象呢？&lt;/p&gt;

&lt;p&gt;创建的 prometheus 这个资源对象里面有非常重要的一个属性 ruleSelector，用来匹配 rule 规则的过滤器，要求匹配具有 prometheus=k8s 和 role=alert-rules 标签的 PrometheusRule 资源对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruleSelector:
  matchLabels:
    prometheus: k8s
    role: alert-rules
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以自定义一个报警规则，还需要需要创建一个具有 prometheus=k8s 和 role=alert-rules 标签的 PrometheusRule 对象。&lt;/p&gt;

&lt;p&gt;我们以etcd为例来自定义一个告警：如果不可用的 etcd 数量超过了一半那么就触发报警&lt;/p&gt;

&lt;p&gt;创建文件 prometheus-etcdRules.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: etcd-rules
  namespace: monitoring
spec:
  groups:
  - name: etcd
    rules:
    - alert: EtcdClusterUnavailable
      annotations:
        summary: etcd cluster small
        description: If one more etcd peer goes down the cluster will be unavailable
      expr: |
        count(up{job=&amp;quot;etcd&amp;quot;} == 0) &amp;gt; (count(up{job=&amp;quot;etcd&amp;quot;}) / 2 - 1)
      for: 3m
      labels:
        severity: critical
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建好了，我们就可以看到在对应目录下生成了一份yaml告警规则文件。&lt;/p&gt;

&lt;h2 id=&#34;配置告警方式&#34;&gt;配置告警方式&lt;/h2&gt;

&lt;p&gt;我们可以通过 AlertManager 的配置文件去配置各种报警接收器，首先我们将 alertmanager-main 这个 Service 改为 NodePort 类型的 Service，和前面的修改是一样的操作，修改完成后我们可以在页面上的 status 路径下面查看 AlertManager 的配置信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  resolve_timeout: 5m
  http_config: {}
  smtp_hello: localhost
  smtp_require_tls: true
  pagerduty_url: https://events.pagerduty.com/v2/enqueue
  hipchat_api_url: https://api.hipchat.com/
  opsgenie_api_url: https://api.opsgenie.com/
  wechat_api_url: https://qyapi.weixin.qq.com/cgi-bin/
  victorops_api_url: https://alert.victorops.com/integrations/generic/20131114/alert/
route:
  receiver: Default
  group_by:
  - namespace
  routes:
  - receiver: Watchdog
    match:
      alertname: Watchdog
  - receiver: Critical
    match:
      severity: critical
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
inhibit_rules:
- source_match:
    severity: critical
  target_match_re:
    severity: warning|info
  equal:
  - namespace
  - alertname
- source_match:
    severity: warning
  target_match_re:
    severity: info
  equal:
  - namespace
  - alertname
receivers:
- name: Default
- name: Watchdog
- name: Critical
templates: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些配置信息实际上是来自于我们之前在prometheus-operator/contrib/kube-prometheus/manifests目录下面创建的 alertmanager-secret.yaml 文件，将文件中 alertmanager.yaml 对应的 value 值做一个 base64 解码，内容和上面查看的配置信息是一致的。&lt;/p&gt;

&lt;p&gt;果我们想要添加自己的接收器，或者模板消息，我们就可以更改这个文件，比如我们添加了两个接收器，默认的通过邮箱进行发送，对于 CoreDNSDown 这个报警我们通过 webhook 来进行发送。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global:
  resolve_timeout: 5m
  smtp_smarthost: &#39;smtp.163.com:25&#39;
  smtp_from: &#39;ych_1024@163.com&#39;
  smtp_auth_username: &#39;ych_1024@163.com&#39;
  smtp_auth_password: &#39;&amp;lt;邮箱密码&amp;gt;&#39;
  smtp_hello: &#39;163.com&#39;
  smtp_require_tls: false
route:
  group_by: [&#39;job&#39;, &#39;severity&#39;]
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: default
  routes:
  - receiver: webhook
    match:
      alertname: CoreDNSDown
receivers:
- name: &#39;default&#39;
  email_configs:
  - to: &#39;517554016@qq.com&#39;
    send_resolved: true
- name: &#39;webhook&#39;
  webhook_configs:
  - url: &#39;http://dingtalk-hook.kube-ops:5000&#39;
    send_resolved: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上面文件保存为 alertmanager.yaml，然后使用这个文件创建一个 Secret 对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 先将之前的 secret 对象删除
$ kubectl delete secret alertmanager-main -n monitoring
secret &amp;quot;alertmanager-main&amp;quot; deleted
$ kubectl create secret generic alertmanager-main --from-file=alertmanager.yaml -n monitoring
secret &amp;quot;alertmanager-main&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以完成发送的配置了。&lt;/p&gt;

&lt;h2 id=&#34;自动发现配置&#34;&gt;自动发现配置&lt;/h2&gt;

&lt;p&gt;如果在我们的 Kubernetes 集群中有了很多的 Service/Pod，那么我们都需要一个一个的去建立一个对应的 ServiceMonitor 对象来进行监控吗？这样岂不是又变得麻烦起来了？&lt;/p&gt;

&lt;p&gt;我们可以通过添加额外的配置来进行服务发现进行自动监控。配置如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;kubernetes-service-endpoints&#39;
  kubernetes_sd_configs:
  - role: endpoints
  relabel_configs:
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
    action: replace
    target_label: __scheme__
    regex: (https?)
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
    action: replace
    target_label: __metrics_path__
    regex: (.+)
  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
    action: replace
    target_label: __address__
    regex: ([^:]+)(?::\d+)?;(\d+)
    replacement: $1:$2
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    action: replace
    target_label: kubernetes_name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上面文件直接保存为 prometheus-additional.yaml，然后通过这个文件创建一个对应的 Secret 对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret generic additional-configs --from-file=prometheus-additional.yaml -n monitoring
secret &amp;quot;additional-configs&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建完成后，会将上面配置信息进行 base64 编码后作为 prometheus-additional.yaml 这个 key 对应的值存在&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get secret additional-configs -n monitoring -o yaml
apiVersion: v1
data:
  prometheus-additional.yaml: LSBqb2JfbmFtZTogJ2t1YmVybmV0ZXMtc2VydmljZS1lbmRwb2ludHMnCiAga3ViZXJuZXRlc19zZF9jb25maWdzOgogIC0gcm9sZTogZW5kcG9pbnRzCiAgcmVsYWJlbF9jb25maWdzOgogIC0gc291cmNlX2xhYmVsczogW19fbWV0YV9rdWJlcm5ldGVzX3NlcnZpY2VfYW5ub3RhdGlvbl9wcm9tZXRoZXVzX2lvX3NjcmFwZV0KICAgIGFjdGlvbjoga2VlcAogICAgcmVnZXg6IHRydWUKICAtIHNvdXJjZV9sYWJlbHM6IFtfX21ldGFfa3ViZXJuZXRlc19zZXJ2aWNlX2Fubm90YXRpb25fcHJvbWV0aGV1c19pb19zY2hlbWVdCiAgICBhY3Rpb246IHJlcGxhY2UKICAgIHRhcmdldF9sYWJlbDogX19zY2hlbWVfXwogICAgcmVnZXg6IChodHRwcz8pCiAgLSBzb3VyY2VfbGFiZWxzOiBbX19tZXRhX2t1YmVybmV0ZXNfc2VydmljZV9hbm5vdGF0aW9uX3Byb21ldGhldXNfaW9fcGF0aF0KICAgIGFjdGlvbjogcmVwbGFjZQogICAgdGFyZ2V0X2xhYmVsOiBfX21ldHJpY3NfcGF0aF9fCiAgICByZWdleDogKC4rKQogIC0gc291cmNlX2xhYmVsczogW19fYWRkcmVzc19fLCBfX21ldGFfa3ViZXJuZXRlc19zZXJ2aWNlX2Fubm90YXRpb25fcHJvbWV0aGV1c19pb19wb3J0XQogICAgYWN0aW9uOiByZXBsYWNlCiAgICB0YXJnZXRfbGFiZWw6IF9fYWRkcmVzc19fCiAgICByZWdleDogKFteOl0rKSg/OjpcZCspPzsoXGQrKQogICAgcmVwbGFjZW1lbnQ6ICQxOiQyCiAgLSBhY3Rpb246IGxhYmVsbWFwCiAgICByZWdleDogX19tZXRhX2t1YmVybmV0ZXNfc2VydmljZV9sYWJlbF8oLispCiAgLSBzb3VyY2VfbGFiZWxzOiBbX19tZXRhX2t1YmVybmV0ZXNfbmFtZXNwYWNlXQogICAgYWN0aW9uOiByZXBsYWNlCiAgICB0YXJnZXRfbGFiZWw6IGt1YmVybmV0ZXNfbmFtZXNwYWNlCiAgLSBzb3VyY2VfbGFiZWxzOiBbX19tZXRhX2t1YmVybmV0ZXNfc2VydmljZV9uYW1lXQogICAgYWN0aW9uOiByZXBsYWNlCiAgICB0YXJnZXRfbGFiZWw6IGt1YmVybmV0ZXNfbmFtZQo=
kind: Secret
metadata:
  creationTimestamp: 2018-12-20T14:50:35Z
  name: additional-configs
  namespace: monitoring
  resourceVersion: &amp;quot;41814998&amp;quot;
  selfLink: /api/v1/namespaces/monitoring/secrets/additional-configs
  uid: 9bbe22c5-0466-11e9-a777-525400db4df7
type: Opaque
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们只需要在声明创建 prometheus 的资源对象文件中添加上这个额外的配置，其实就是通过secret将这段配置挂载到prometheus上去，作为prometheus的配置文件的一部分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  additionalScrapeConfigs:
    name: additional-configs
    key: prometheus-additional.yaml
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以自动发现service信息了，这边还有一个权限的问题，Prometheus 绑定了一个名为 prometheus-k8s 的 ServiceAccount 对象，而这个对象绑定的是一个名为 prometheus-k8s 的 ClusterRole，有对 Service 或者 Pod 的 list 权限，所以报错了，要解决这个问题，我们只需要添加上需要的权限即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - nodes
  - services
  - endpoints
  - pods
  - nodes/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - configmaps
  - nodes/metrics
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就有权限了，只需要我们在 Service 的annotation区域添加prometheus.io/scrape=true的声明，就会在服务创建的时候被自动发现。&lt;/p&gt;

&lt;h2 id=&#34;数据持久化&#34;&gt;数据持久化&lt;/h2&gt;

&lt;p&gt;查看生成的 Prometheus Pod 的挂载情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod prometheus-k8s-0 -n monitoring -o yaml
......
    volumeMounts:
    - mountPath: /etc/prometheus/config_out
      name: config-out
      readOnly: true
    - mountPath: /prometheus
      name: prometheus-k8s-db
......
  volumes:
......
  - emptyDir: {}
    name: prometheus-k8s-db
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到 Prometheus 的数据目录 /prometheus 实际上是通过 emptyDir 进行挂载的，我们知道 emptyDir 挂载的数据的生命周期和 Pod 生命周期一致的，所以如果 Pod 挂掉了，数据也就丢失了。&lt;/p&gt;

&lt;p&gt;对应线上的监控数据肯定需要做数据的持久化的，们的 Prometheus 最终是通过 Statefulset 控制器进行部署的，所以我们这里需要通过 storageclass 来做数据持久化，首先创建一个 StorageClass 对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: prometheus-data-db
provisioner: fuseim.pri/ifs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中 provisioner=fuseim.pri/ifs，则是因为我们集群中使用的是 nfs 作为存储后端，将该文件保存为 prometheus-storageclass.yaml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f prometheus-storageclass.yaml
storageclass.storage.k8s.io &amp;quot;prometheus-data-db&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在 prometheus 的 CRD 资源对象中添加如下配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;storage:
  volumeClaimTemplate:
    spec:
      storageClassName: prometheus-data-db
      resources:
        requests:
          storage: 10Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意这里的 storageClassName 名字为上面我们创建的 StorageClass 对象名称，然后更新 prometheus 这个 CRD 资源。更新完成后会自动生成两个 PVC 和 PV 资源对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pvc -n monitoring
NAME                                 STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE
prometheus-k8s-db-prometheus-k8s-0   Bound     pvc-0cc03d41-047a-11e9-a777-525400db4df7   10Gi       RWO            prometheus-data-db   8m
prometheus-k8s-db-prometheus-k8s-1   Bound     pvc-1938de6b-047b-11e9-a777-525400db4df7   10Gi       RWO            prometheus-data-db   1m
$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                           STORAGECLASS         REASON    AGE
pvc-0cc03d41-047a-11e9-a777-525400db4df7   10Gi       RWO            Delete           Bound       monitoring/prometheus-k8s-db-prometheus-k8s-0   prometheus-data-db             2m
pvc-1938de6b-047b-11e9-a777-525400db4df7   10Gi       RWO            Delete           Bound       monitoring/prometheus-k8s-db-prometheus-k8s-1   prometheus-data-db             1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在我们再去看 Prometheus Pod 的数据目录就可以看到是关联到一个 PVC 对象上了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod prometheus-k8s-0 -n monitoring -o yaml
......
    volumeMounts:
    - mountPath: /etc/prometheus/config_out
      name: config-out
      readOnly: true
    - mountPath: /prometheus
      name: prometheus-k8s-db
......
  volumes:
......
  - name: prometheus-k8s-db
    persistentVolumeClaim:
      claimName: prometheus-k8s-db-prometheus-k8s-0
......
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在即使我们的 Pod 挂掉了，数据也不会丢失了。&lt;/p&gt;

&lt;h1 id=&#34;使用场景&#34;&gt;使用场景&lt;/h1&gt;

&lt;p&gt;什么时候prometheus使用的物理机部署的集群？&lt;/p&gt;

&lt;p&gt;1、取决于生产环境，比如要求prometheus不光需要监控k8s还需要监控kvm的机器&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;k8s的服务发现主要是通过定义serviceMonitor，或者service配置备注做自动发现，本质其实就是通过service来暴露指标，但是对于kvm没有这些机制，如何对kvm环境下的机器做服务发现就是一个问题&lt;/li&gt;
&lt;li&gt;k8s和kvm的网络打通也是一个问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、监控规模，数据量导致的稳定性&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当集群规模小，监控数据少的情况下部署单点的prometheus是够用的，但是如果监控规模扩展，数据量很大的时候，对资源，比如cpu和memory的要求比较高，对k8s来说是一个很重的应用，对于本身的稳定性也是一个很重要的考验。&lt;/li&gt;
&lt;li&gt;当规模达到一定的时候，需要分布式集群来处理，在k8s部署分布式集群也是有着很多的问题&lt;/li&gt;
&lt;li&gt;当规模扩大时候，分组也是一个很大的问题，单个prometheus的的采集分配也是问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以如果只是监控k8s使用operator部署k8s上都可以优化，但是如果加上kvm是很有必要在物理机上部署prometheus的监控的，需要设计完成的架构和实现方案。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus Principle</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/</link>
          <pubDate>Sun, 13 May 2018 17:56:03 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/</guid>
          <description>&lt;p&gt;本篇文章主要是对prometheus的一些原理进行解析。&lt;/p&gt;

&lt;h1 id=&#34;启动流程解析&#34;&gt;启动流程解析&lt;/h1&gt;

&lt;p&gt;Prometheus 启动过程中，主要包含服务组件初始化，服务组件配置应用及启动各个服务组件三个部分，下面基于版本 v2.7.1，详细分析这三部分内容。&lt;/p&gt;

&lt;h2 id=&#34;服务组件初始化&#34;&gt;服务组件初始化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Storage组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prometheus的Storage组件是时序数据库，包含两个：localStorage和remoteStorage。localStorage当前版本指TSDB，用于对metrics的本地存储存储，remoteStorage用于metrics的远程存储，其中fanoutStorage作为localStorage和remoteStorage的读写代理服务器。&lt;/p&gt;

&lt;p&gt;初始化流程如下，代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;localStorage  = &amp;amp;tsdb.ReadyStorage{} //本地存储
remoteStorage = remote.NewStorage(log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;remote&amp;quot;), //远端存储 localStorage.StartTime, time.Duration(cfg.RemoteFlushDeadline))
fanoutStorage = storage.NewFanout(logger, localStorage, remoteStorage) //读写代理服务器
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;notifier 组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;notifier组件用于发送告警信息给AlertManager，通过方法notifier.NewManager完成初始化&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;notifierManager = notifier.NewManager(&amp;amp;cfg.notifier, log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;notifier&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;discoveryManagerScrape组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;discoveryManagerScrape组件用于服务发现，当前版本支持多种服务发现系统，比如kuberneters等，通过方法discovery.NewManager完成初始化。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;discoveryManagerScrape  = discovery.NewManager(ctxScrape, log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;discovery manager scrape&amp;quot;), discovery.Name(&amp;quot;scrape&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;discoveryManagerNotify组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;discoveryManagerNotify组件用于告警通知服务发现，比如AlertManager服务．也是通过方法discovery.NewManager完成初始化，不同的是，discoveryManagerNotify服务于notify，而discoveryManagerScrape服务于scrape。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;discoveryManagerNotify  = discovery.NewManager(ctxNotify, log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;discovery manager notify&amp;quot;), discovery.Name(&amp;quot;notify&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;scrapeManager组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrapeManager组件利用discoveryManagerScrape组件发现的targets，抓取对应targets的所有metrics，并将抓取的metrics存储到fanoutStorage中，通过方法scrape.NewManager完成初始化。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrapeManager = scrape.NewManager(log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;scrape manager&amp;quot;), fanoutStorage)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;queryEngine组件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;queryEngine组件用于rules查询和计算，通过方法promql.NewEngine完成初始化。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;opts = promql.EngineOpts{
    Logger:        log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;query engine&amp;quot;),
    Reg:           prometheus.DefaultRegisterer,
    MaxConcurrent: cfg.queryConcurrency,　　　　　　　//最大并发查询个数
    MaxSamples:    cfg.queryMaxSamples,
    Timeout:       time.Duration(cfg.queryTimeout),　//查询超时时间
}
queryEngine = promql.NewEngine(opts)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;ruleManager组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ruleManager组件通过方法rules.NewManager完成初始化．其中rules.NewManager的参数涉及多个组件：存储，queryEngine和notifier，整个流程包含rule计算和发送告警。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruleManager = rules.NewManager(&amp;amp;rules.ManagerOptions{
    Appendable:      fanoutStorage,                        //存储器
    TSDB:            localStorage,　　　　　　　　　　　　　　//本地时序数据库TSDB
    QueryFunc:       rules.EngineQueryFunc(queryEngine, fanoutStorage), //rules计算
    NotifyFunc:      sendAlerts(notifierManager, cfg.web.ExternalURL.String()),　//告警通知
    Context:         ctxRule,　//用于控制ruleManager组件的协程
    ExternalURL:     cfg.web.ExternalURL,　//通过Web对外开放的URL
    Registerer:      prometheus.DefaultRegisterer,
    Logger:          log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;rule manager&amp;quot;),
    OutageTolerance: time.Duration(cfg.outageTolerance), //当prometheus重启时，保持alert状态（https://ganeshvernekar.com/gsoc-2018/persist-for-state/）
    ForGracePeriod:  time.Duration(cfg.forGracePeriod),
    ResendDelay:     time.Duration(cfg.resendDelay),
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Web组件初始化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Web组件用于为Storage组件，queryEngine组件，scrapeManager组件， ruleManager组件和notifier 组件提供外部HTTP访问方式，也就是我们经常访问的prometheus的界面。&lt;/p&gt;

&lt;p&gt;初始化代码如下，代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfg.web.Context = ctxWeb
cfg.web.TSDB = localStorage.Get
cfg.web.Storage = fanoutStorage
cfg.web.QueryEngine = queryEngine
cfg.web.ScrapeManager = scrapeManager
cfg.web.RuleManager = ruleManager
cfg.web.Notifier = notifierManager

cfg.web.Version = &amp;amp;web.PrometheusVersion{
    Version:   version.Version,
    Revision:  version.Revision,
    Branch:    version.Branch,
    BuildUser: version.BuildUser,
    BuildDate: version.BuildDate,
    GoVersion: version.GoVersion,
}

cfg.web.Flags = map[string]string{}

// Depends on cfg.web.ScrapeManager so needs to be after cfg.web.ScrapeManager = scrapeManager
webHandler := web.New(log.With(logger, &amp;quot;component&amp;quot;, &amp;quot;web&amp;quot;), &amp;amp;cfg.web)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;服务组件配置应用&#34;&gt;服务组件配置应用&lt;/h2&gt;

&lt;p&gt;除了服务组件ruleManager用的方法是Update，其他服务组件的在匿名函数中通过各自的ApplyConfig方法，实现配置的管理。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reloaders := []func(cfg *config.Config) error{
    remoteStorage.ApplyConfig, //存储配置
    webHandler.ApplyConfig,    //web配置
    notifierManager.ApplyConfig, //notifier配置
    scrapeManager.ApplyConfig,　　//scrapeManger配置
　　//从配置文件中提取Section:scrape_configs
    func(cfg *config.Config) error {
        c := make(map[string]sd_config.ServiceDiscoveryConfig)
        for _, v := range cfg.ScrapeConfigs {
            c[v.JobName] = v.ServiceDiscoveryConfig
        }
        return discoveryManagerScrape.ApplyConfig(c)
    },
    //从配置文件中提取Section:alerting
    func(cfg *config.Config) error {
        c := make(map[string]sd_config.ServiceDiscoveryConfig)
        for _, v := range cfg.AlertingConfig.AlertmanagerConfigs {
            // AlertmanagerConfigs doesn&#39;t hold an unique identifier so we use the config hash as the identifier.
            b, err := json.Marshal(v)
            if err != nil {
                return err
            }
            c[fmt.Sprintf(&amp;quot;%x&amp;quot;, md5.Sum(b))] = v.ServiceDiscoveryConfig
        }
        return discoveryManagerNotify.ApplyConfig(c)
    },
    //从配置文件中提取Section:rule_files
    func(cfg *config.Config) error {
        // Get all rule files matching the configuration paths.
        var files []string
        for _, pat := range cfg.RuleFiles {
            fs, err := filepath.Glob(pat)
            if err != nil {
                // The only error can be a bad pattern.
                return fmt.Errorf(&amp;quot;error retrieving rule files for %s: %s&amp;quot;, pat, err)
            }
            files = append(files, fs...)
        }
        return ruleManager.Update(time.Duration(cfg.GlobalConfig.EvaluationInterval), files)
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务组件remoteStorage，webHandler，notifierManager和ScrapeManager的ApplyConfig方法，参数cfg *config.Config中传递的配置文件，是整个文件prometheus.yml&lt;/p&gt;

&lt;p&gt;代码文件prometheus/scrape/manager.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg *config.Config) error {
   .......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务组件discoveryManagerScrape和discoveryManagerNotify的ApplyConfig方法，参数中传递的配置文件，是文件中的一个Section&lt;/p&gt;

&lt;p&gt;代码文件prometheus/discovery/manager.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg map[string]sd_config.ServiceDiscoveryConfig) error {
     ......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以，需要利用匿名函数提前处理下，取出对应的Section。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//从配置文件中提取Section:scrape_configs
func(cfg *config.Config) error {
    c := make(map[string]sd_config.ServiceDiscoveryConfig)
    for _, v := range cfg.ScrapeConfigs {
        c[v.JobName] = v.ServiceDiscoveryConfig
    }
    return discoveryManagerScrape.ApplyConfig(c)
},
//从配置文件中提取Section:alerting
func(cfg *config.Config) error {
    c := make(map[string]sd_config.ServiceDiscoveryConfig)
    for _, v := range cfg.AlertingConfig.AlertmanagerConfigs {
        // AlertmanagerConfigs doesn&#39;t hold an unique identifier so we use the config hash as the identifier.
        b, err := json.Marshal(v)
        if err != nil {
            return err
        }
        c[fmt.Sprintf(&amp;quot;%x&amp;quot;, md5.Sum(b))] = v.ServiceDiscoveryConfig
    }
    return discoveryManagerNotify.ApplyConfig(c)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务组件ruleManager，在匿名函数中提取出Section:rule_files&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//从配置文件中提取Section:rule_files
func(cfg *config.Config) error {
    // Get all rule files matching the configuration paths.
    var files []string
    for _, pat := range cfg.RuleFiles {
        fs, err := filepath.Glob(pat)
        if err != nil {
            // The only error can be a bad pattern.
            return fmt.Errorf(&amp;quot;error retrieving rule files for %s: %s&amp;quot;, pat, err)
        }
        files = append(files, fs...)
    }
    return ruleManager.Update(time.Duration(cfg.GlobalConfig.EvaluationInterval), files)
},
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;利用该组件内置的Update方法完成配置管理。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/rules/manager.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Update(interval time.Duration, files []string) error {
  .......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，通过reloadConfig方法，加载各个服务组件的配置项&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func reloadConfig(filename string, logger log.Logger, rls ...func(*config.Config) error) (err error) {
    level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Loading configuration file&amp;quot;, &amp;quot;filename&amp;quot;, filename)

    defer func() {
        if err == nil {
            configSuccess.Set(1)
            configSuccessTime.SetToCurrentTime()
        } else {
            configSuccess.Set(0)
        }
    }()

    conf, err := config.LoadFile(filename)
    if err != nil {
        return fmt.Errorf(&amp;quot;couldn&#39;t load configuration (--config.file=%q): %v&amp;quot;, filename, err)
    }

    failed := false
　　//通过一个for循环，加载各个服务组件的配置项
    for _, rl := range rls {
        if err := rl(conf); err != nil {
            level.Error(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Failed to apply configuration&amp;quot;, &amp;quot;err&amp;quot;, err)
            failed = true
        }
    }
    if failed {
        return fmt.Errorf(&amp;quot;one or more errors occurred while applying the new configuration (--config.file=%q)&amp;quot;, filename)
    }
    promql.SetDefaultEvaluationInterval(time.Duration(conf.GlobalConfig.EvaluationInterval))
    level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Completed loading of configuration file&amp;quot;, &amp;quot;filename&amp;quot;, filename)
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;服务组件启动&#34;&gt;服务组件启动&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;实例化&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里引用了github.com/oklog/oklog/pkg/group包，实例化一个对象g&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// &amp;quot;github.com/oklog/oklog/pkg/group&amp;quot;
var g group.Group
{
　　......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对象g中包含各个服务组件的入口，通过调用Add方法把把这些入口添加到对象g中，以组件scrapeManager为例。&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    // Scrape manager.
　　//通过方法Add，把ScrapeManager组件添加到g中
    g.Add(
        func() error {
            // When the scrape manager receives a new targets list
            // it needs to read a valid config for each job.
            // It depends on the config being in sync with the discovery manager so
            // we wait until the config is fully loaded.
            &amp;lt;-reloadReady.C
　　　　　　　//ScrapeManager组件的启动函数
            err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
            level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape manager stopped&amp;quot;)
            return err
        },
        func(err error) {
            // Scrape manager needs to be stopped before closing the local TSDB
            // so that it doesn&#39;t try to write samples to a closed storage.
            level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Stopping scrape manager...&amp;quot;)
            scrapeManager.Stop()
        },
    )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;run&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过对象g，调用方法run，启动所有服务组件&lt;/p&gt;

&lt;p&gt;代码文件prometheus/cmd/prometheus/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if err := g.Run(); err != nil {
    level.Error(logger).Log(&amp;quot;err&amp;quot;, err)
    os.Exit(1)
}
level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;See you next time!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，Prometheus的启动过程分析完成。&lt;/p&gt;

&lt;h1 id=&#34;内部结构&#34;&gt;内部结构&lt;/h1&gt;

&lt;p&gt;Prometheus的内部主要分为三大块，Retrieval是负责定时去暴露的目标页面上去抓取采样指标数据，Storage是负责将采样数据写磁盘，PromQL是Prometheus提供的查询语言模块。当然还有其他的一些组件，可以参考上面组件的初始化，比如一些web，notify等。&lt;/p&gt;

&lt;h2 id=&#34;retrieval&#34;&gt;Retrieval&lt;/h2&gt;

&lt;h3 id=&#34;采集实现&#34;&gt;采集实现&lt;/h3&gt;

&lt;p&gt;​Prometheus采集数据使用pull模式，通过HTTP协议去采集指标，只要应用系统能够提供HTTP接口就可以接入监控系统。&lt;/p&gt;

&lt;p&gt;​拉取目标称之为scrape，一个scrape一般对应一个进程，如下为scrape相关的配置。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;配置文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;scrape_interval:     15s
scrape_configs:
  - job_name: &#39;test_server_name&#39;
    static_configs:
    - targets: [&#39;localhost:8886&#39;]
      labels:
        project: &#39;test_server&#39;
        environment: &#39;test&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该配置描述：每15秒去拉取一次上报数据，拉取目标为localhost:8886。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;读取配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ScrapeConfig的结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ScrapeConfig struct {
   //  作业名称
   JobName string `yaml:&amp;quot;job_name&amp;quot;`
   // 同名lable，是否覆盖处理
   HonorLabels bool `yaml:&amp;quot;honor_labels,omitempty&amp;quot;`
   HonorTimestamps bool `yaml:&amp;quot;honor_timestamps&amp;quot;`
   // 采集目标url参数
   Params url.Values `yaml:&amp;quot;params,omitempty&amp;quot;`
   // 采集周期
   ScrapeInterval model.Duration `yaml:&amp;quot;scrape_interval,omitempty&amp;quot;`
   // 采集超时时间
   ScrapeTimeout model.Duration `yaml:&amp;quot;scrape_timeout,omitempty&amp;quot;`
   // 目标 URl path
   MetricsPath string `yaml:&amp;quot;metrics_path,omitempty&amp;quot;`
   Scheme string `yaml:&amp;quot;scheme,omitempty&amp;quot;`
   SampleLimit uint `yaml:&amp;quot;sample_limit,omitempty&amp;quot;`
   // 服务发现配置
   ServiceDiscoveryConfig sd_config.ServiceDiscoveryConfig `yaml:&amp;quot;,inline&amp;quot;`
   // 客户端http client 配置
   HTTPClientConfig       config_util.HTTPClientConfig     `yaml:&amp;quot;,inline&amp;quot;`
   // 目标重置规则
   RelabelConfigs []*relabel.Config `yaml:&amp;quot;relabel_configs,omitempty&amp;quot;`
   // 指标重置规则
   MetricRelabelConfigs []*relabel.Config `yaml:&amp;quot;metric_relabel_configs,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读取配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg *config.Config) error {
    m.mtxScrape.Lock()
    defer m.mtxScrape.Unlock()
    // 初始化map结构，用于保存配置
    c := make(map[string]*config.ScrapeConfig)
    for _, scfg := range cfg.ScrapeConfigs {
    // 配置读取维度
        c[scfg.JobName] = scfg
    }
    m.scrapeConfigs = c
    // 设置 所有时间序列和警告与外部通信时用的外部标签 external_labels
    if err := m.setJitterSeed(cfg.GlobalConfig.ExternalLabels); err != nil {
        return err
    }

    // 如果配置已经更改，清理历史配置，重新加载到池子中
    var failed bool
    for name, sp := range m.scrapePools {
    // 如果当前job不存在，则删除
        if cfg, ok := m.scrapeConfigs[name]; !ok {
            sp.stop()
            delete(m.scrapePools, name)
        } else if !reflect.DeepEqual(sp.config, cfg) {
      // 如果配置变更，重新启动reload，进行加载
            err := sp.reload(cfg)
            if err != nil {
                level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error reloading scrape pool&amp;quot;, &amp;quot;err&amp;quot;, err, &amp;quot;scrape_pool&amp;quot;, name)
                failed = true
            }
        }
    }
    // 失败 return
    if failed {
        return errors.New(&amp;quot;failed to apply the new configuration&amp;quot;)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus 中，将任意一个独立的数据源（target）称之为实例（instance）。包含相同类型的实例的集合称之为作业（job)，从读取配置中，我们也能看到，以job为key。所以注意job在业务侧的使用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Scrape Manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;​
添加Scrape Manager 到 run.Group启动。reloadReady.C的作用是当Manager接收到一组数据采集目标(target)的时候，他需要为每个job读取有效的配置。因此这里等待所有配置加载完成，进行下一步。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;g.Add(
            func() error {
        // 当所有配置都准备好
                &amp;lt;-reloadReady.C
                // 启动scrapeManager
                err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
                level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape manager stopped&amp;quot;)
                return err
            },
            func(err error) {
        // 失败处理
                level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Stopping scrape manager...&amp;quot;)
                scrapeManager.Stop()
            },
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;加载Targets&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加载targets，如果targets更新，会触发重新加载，reloader的加载发生在后台，所以并不会影响target的更新，(配置文件中配置的target是依赖discoveryManagerScrape.ApplyConfig&amp;copy;进行加载的，后面分析target服务发现的时候详细分析)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Run(tsets &amp;lt;-chan map[string][]*targetgroup.Group) error {
    go m.reloader()
    for {
        select {
    // 触发重新加载目标。添加新增
        case ts := &amp;lt;-tsets:
            m.updateTsets(ts)

            select {
      // 关闭 Scrape Manager 处理信号
            case m.triggerReload &amp;lt;- struct{}{}:
            default:
            }

        case &amp;lt;-m.graceShut:
            return nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;scrape pool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;顺着Run继续阅读，reload为每一组tatget生成一个对应的scrape pool管理targets集合，scrapePool结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type scrapePool struct {
   appendable Appendable
   logger     log.Logger
     // 读写锁
   mtx    sync.RWMutex
   // Scrape 配置
   config *config.ScrapeConfig
   // http client
   client *http.Client

   // 正在运行的target
   activeTargets  map[uint64]*Target
   // 无效的target
   droppedTargets []*Target
   // 所有运行的loop
   loops          map[uint64]loop
   // 取消
   cancel         context.CancelFunc
     // 创建loop
   newLoop func(scrapeLoopOptions) loop
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;执行reload&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;m.reloade的流程也很简单，setName指我们配置中的job，如果scrapePools不存在该job，则添加，添加前也是先校验该job的配置是否存在，不存在则报错，创建scrape pool。总结看就是为每个job创建与之对应的scrape pool&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) reload() {
   //加锁
   m.mtxScrape.Lock()
   var wg sync.WaitGroup
   for setName, groups := range m.targetSets {
       //检查该scrape是否存在scrapePools，不存在则创建
      if _, ok := m.scrapePools[setName]; !ok {
         //读取该scrape的配置
         scrapeConfig, ok := m.scrapeConfigs[setName]
         if !ok {
            // 未读取到该scrape的配置打印错误
            level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error reloading target set&amp;quot;, &amp;quot;err&amp;quot;, &amp;quot;invalid config id:&amp;quot;+setName)
            // 跳出
            continue
         }
         // 创建该scrape的scrape pool
         sp, err := newScrapePool(scrapeConfig, m.append, m.jitterSeed, log.With(m.logger, &amp;quot;scrape_pool&amp;quot;, setName))
         if err != nil {
            level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;error creating new scrape pool&amp;quot;, &amp;quot;err&amp;quot;, err, &amp;quot;scrape_pool&amp;quot;, setName)
            continue
         }
         // 保存
         m.scrapePools[setName] = sp
      }

      wg.Add(1)
            // 并行运行，提升性能。
      go func(sp *scrapePool, groups []*targetgroup.Group) {
         sp.Sync(groups)
         wg.Done()
      }(m.scrapePools[setName], groups)

   }
   // 释放锁
   m.mtxScrape.Unlock()
   // 阻塞，等待所有pool运行完毕
   wg.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;创建scrape pool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrape pool利用newLoop去为该job下的所有target生成对应的loop：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newScrapePool(cfg *config.ScrapeConfig, app Appendable, jitterSeed uint64, logger log.Logger) (*scrapePool, error) {
    targetScrapePools.Inc()
    if logger == nil {
        logger = log.NewNopLogger()
    }
  // 创建http client，用于执行数据抓取
    client, err := config_util.NewClientFromConfig(cfg.HTTPClientConfig, cfg.JobName)
    if err != nil {
        targetScrapePoolsFailed.Inc()
        return nil, errors.Wrap(err, &amp;quot;error creating HTTP client&amp;quot;)
    }
    // 设置buffers
    buffers := pool.New(1e3, 100e6, 3, func(sz int) interface{} { return make([]byte, 0, sz) })
  // 设置scrapePool的一些基础属性
    ctx, cancel := context.WithCancel(context.Background())
    sp := &amp;amp;scrapePool{
        cancel:        cancel,
        appendable:    app,
        config:        cfg,
        client:        client,
        activeTargets: map[uint64]*Target{},
        loops:         map[uint64]loop{},
        logger:        logger,
    }
  // newLoop用于生层loop，主要处理对应的target，可以理解为，每个target对应一个loop。
    sp.newLoop = func(opts scrapeLoopOptions) loop {
        // Update the targets retrieval function for metadata to a new scrape cache.
        cache := newScrapeCache()
        opts.target.setMetadataStore(cache)

        return newScrapeLoop(
            ctx,
            opts.scraper,
            log.With(logger, &amp;quot;target&amp;quot;, opts.target),
            buffers,
            func(l labels.Labels) labels.Labels {
                return mutateSampleLabels(l, opts.target, opts.honorLabels, opts.mrc)
            },
            func(l labels.Labels) labels.Labels { return mutateReportSampleLabels(l, opts.target) },
            func() storage.Appender {
                app, err := app.Appender()
                if err != nil {
                    panic(err)
                }
                return appender(app, opts.limit)
            },
            cache,
            jitterSeed,
            opts.honorTimestamps,
        )
    }

    return sp, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;group转化为target&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrape pool创建完成后，则通过sp.Sync执行，使用该job对应的pool遍历Group，使其转换为target&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go func(sp *scrapePool, groups []*targetgroup.Group) {
   sp.Sync(groups)
   wg.Done()
}(m.scrapePools[setName], groups)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sync函数解读如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sp *scrapePool) Sync(tgs []*targetgroup.Group) {
   start := time.Now()

   var all []*Target
   // 加锁
   sp.mtx.Lock()
   sp.droppedTargets = []*Target{}
   // 遍历所有Group
   for _, tg := range tgs {
        // 转化对应 targets
      targets, err := targetsFromGroup(tg, sp.config)
      if err != nil {
         level.Error(sp.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;creating targets failed&amp;quot;, &amp;quot;err&amp;quot;, err)
         continue
      }
      // 将所有有效targets添加到all，等待处理
      for _, t := range targets {
         // 检查该target的lable是否有效
         if t.Labels().Len() &amp;gt; 0 {
            // 添加到all队列中
            all = append(all, t)
         } else if t.DiscoveredLabels().Len() &amp;gt; 0 {
            // 记录无效target
            sp.droppedTargets = append(sp.droppedTargets, t)
         }
      }
   }
   // 解锁
   sp.mtx.Unlock()
   // 处理all队列，执行scarape同步操作
   sp.sync(all)

   targetSyncIntervalLength.WithLabelValues(sp.config.JobName).Observe(
      time.Since(start).Seconds(),
   )
   targetScrapePoolSyncsCounter.WithLabelValues(sp.config.JobName).Inc()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;生成loop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在sync最后，调用了当前scrape pool的sync去处理all队列中的target，添加新的target，删除失效的target。实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sp *scrapePool) sync(targets []*Target) {
  // 加锁
    sp.mtx.Lock()
    defer sp.mtx.Unlock()

    var (
    // target 标记
        uniqueTargets   = map[uint64]struct{}{}
    // 采集周期
        interval        = time.Duration(sp.config.ScrapeInterval)
    // 采集超时时间
        timeout         = time.Duration(sp.config.ScrapeTimeout)
        limit           = int(sp.config.SampleLimit)
    // 重复lable是否覆盖
        honorLabels     = sp.config.HonorLabels
        honorTimestamps = sp.config.HonorTimestamps
        mrc             = sp.config.MetricRelabelConfigs
    )
    // 遍历all队列中的所有target
    for _, t := range targets {
    // 赋值，避免range的坑
        t := t
    // 生成对应的hash（对该hash算法感兴趣可以看下这里的源码）
        hash := t.hash()
    // 标记
        uniqueTargets[hash] = struct{}{}
        // 判断该taget是否已经在运行了。如果没有则运行该target对应的loop，将该loop加入activeTargets中
        if _, ok := sp.activeTargets[hash]; !ok {
            s := &amp;amp;targetScraper{Target: t, client: sp.client, timeout: timeout}
            l := sp.newLoop(scrapeLoopOptions{
                target:          t,
                scraper:         s,
                limit:           limit,
                honorLabels:     honorLabels,
                honorTimestamps: honorTimestamps,
                mrc:             mrc,
            })

            sp.activeTargets[hash] = t
            sp.loops[hash] = l
            // 启动该loop
            go l.run(interval, timeout, nil)
        } else {
      // 该target对应的loop已经运行，设置最新的标签信息
            sp.activeTargets[hash].SetDiscoveredLabels(t.DiscoveredLabels())
        }
    }

    var wg sync.WaitGroup

  // 停止并且移除无效的targets与对应的loops
  // 遍历activeTargets正在执行的Target
    for hash := range sp.activeTargets {
    // 检查该hash对应的标记是否存在，放过不存在执行清除逻辑
        if _, ok := uniqueTargets[hash]; !ok {
            wg.Add(1)
      // 异步清除
            go func(l loop) {
                // 停止该loop
                l.stop()
                // 执行完成
                wg.Done()
            }(sp.loops[hash])
            // 从loops中删除该hash对应的loop
            delete(sp.loops, hash)
      // 从activeTargets中删除该hash对应的target
            delete(sp.activeTargets, hash)
        }
    }
  // 等待所有执行完成
    wg.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;运行loop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;scrape pool对应的sync的实现中可以看到，如果该target没有运行，则启动该target对应的loop，执行l.run，通过一个goroutine来执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sl *scrapeLoop) run(interval, timeout time.Duration, errc chan&amp;lt;- error) {
  // 偏移量相关设置
    select {
    case &amp;lt;-time.After(sl.scraper.offset(interval, sl.jitterSeed)):
        // Continue after a scraping offset.
    case &amp;lt;-sl.scrapeCtx.Done():
        close(sl.stopped)
        return
    }

    var last time.Time
    // 根据interval设置定时器
    ticker := time.NewTicker(interval)
    defer ticker.Stop()

mainLoop:
    for {
        select {
        case &amp;lt;-sl.ctx.Done():
            close(sl.stopped)
            return
        case &amp;lt;-sl.scrapeCtx.Done():
            break mainLoop
        default:
        }

        var (
            start             = time.Now()
            scrapeCtx, cancel = context.WithTimeout(sl.ctx, timeout)
        )

        // 记录第一次
        if !last.IsZero() {
            targetIntervalLength.WithLabelValues(interval.String()).Observe(
                time.Since(last).Seconds(),
            )
        }
        // 根据上次拉取数据的大小，设置buffer空间
        b := sl.buffers.Get(sl.lastScrapeSize).([]byte)
        buf := bytes.NewBuffer(b)
        // 读取数据，设置到buffer中
        contentType, scrapeErr := sl.scraper.scrape(scrapeCtx, buf)
    // 取消，结束scrape
        cancel()

        if scrapeErr == nil {
            b = buf.Bytes()
            if len(b) &amp;gt; 0 {
        // 记录本次Scrape大小
                sl.lastScrapeSize = len(b)
            }
        } else {
      // 错误处理
            level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape failed&amp;quot;, &amp;quot;err&amp;quot;, scrapeErr.Error())
            if errc != nil {
                errc &amp;lt;- scrapeErr
            }
        }

        // 生成数据，存储指标
        total, added, seriesAdded, appErr := sl.append(b, contentType, start)
        if appErr != nil {
            level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;append failed&amp;quot;, &amp;quot;err&amp;quot;, appErr)

            if _, _, _, err := sl.append([]byte{}, &amp;quot;&amp;quot;, start); err != nil {
                level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;append failed&amp;quot;, &amp;quot;err&amp;quot;, err)
            }
        }
        // 对象复用
        sl.buffers.Put(b)

        if scrapeErr == nil {
            scrapeErr = appErr
        }
        // 上报指标，进行统计
        if err := sl.report(start, time.Since(start), total, added, seriesAdded, scrapeErr); err != nil {
            level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;appending scrape report failed&amp;quot;, &amp;quot;err&amp;quot;, err)
        }
    // 重置时间位置
        last = start

        select {
        case &amp;lt;-sl.ctx.Done():
            close(sl.stopped)
            return
        case &amp;lt;-sl.scrapeCtx.Done():
            break mainLoop
        case &amp;lt;-ticker.C:
        }
    }

    close(sl.stopped)

    sl.endOfRunStaleness(last, ticker, interval)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;拉取数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;依赖scrape实现数据的抓取，使用GET方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (s *targetScraper) scrape(ctx context.Context, w io.Writer) (string, error) {
    if s.req == nil {
    // 新建Http Request
        req, err := http.NewRequest(&amp;quot;GET&amp;quot;, s.URL().String(), nil)
        if err != nil {
            return &amp;quot;&amp;quot;, err
        }
    // 设置请求头
        req.Header.Add(&amp;quot;Accept&amp;quot;, acceptHeader)
        req.Header.Add(&amp;quot;Accept-Encoding&amp;quot;, &amp;quot;gzip&amp;quot;)
        req.Header.Set(&amp;quot;User-Agent&amp;quot;, userAgentHeader)
        req.Header.Set(&amp;quot;X-Prometheus-Scrape-Timeout-Seconds&amp;quot;, fmt.Sprintf(&amp;quot;%f&amp;quot;, s.timeout.Seconds()))

        s.req = req
    }
    // 发起请求
    resp, err := s.client.Do(s.req.WithContext(ctx))
    if err != nil {
        return &amp;quot;&amp;quot;, err
    }
    defer func() {
        io.Copy(ioutil.Discard, resp.Body)
        resp.Body.Close()
    }()
    // 错误处理
    if resp.StatusCode != http.StatusOK {
        return &amp;quot;&amp;quot;, errors.Errorf(&amp;quot;server returned HTTP status %s&amp;quot;, resp.Status)
    }
    // 检查Content-Encoding
    if resp.Header.Get(&amp;quot;Content-Encoding&amp;quot;) != &amp;quot;gzip&amp;quot; {
    // copy buffer到w
        _, err = io.Copy(w, resp.Body)
        if err != nil {
            return &amp;quot;&amp;quot;, err
        }
        return resp.Header.Get(&amp;quot;Content-Type&amp;quot;), nil
    }
    if s.gzipr == nil {
        s.buf = bufio.NewReader(resp.Body)
        s.gzipr, err = gzip.NewReader(s.buf)
        if err != nil {
            return &amp;quot;&amp;quot;, err
        }
    } else {
        s.buf.Reset(resp.Body)
        if err = s.gzipr.Reset(s.buf); err != nil {
            return &amp;quot;&amp;quot;, err
        }
    }

    _, err = io.Copy(w, s.gzipr)
    s.gzipr.Close()
    if err != nil {
        return &amp;quot;&amp;quot;, err
    }
    return resp.Header.Get(&amp;quot;Content-Type&amp;quot;), nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每一个job有一个与之对应的scrape pool，每一个target有一个与之对应的loop，每个loop内部执 Http Get请求拉取数据。通过一些控制参数，控制采集周期以及结束等逻辑。&lt;/p&gt;

&lt;h3 id=&#34;数据规范&#34;&gt;数据规范&lt;/h3&gt;

&lt;h4 id=&#34;数据模型&#34;&gt;数据模型&lt;/h4&gt;

&lt;p&gt;Prometheus与其他主流时序数据库一样，在数据模型定义上，也会包含metric name、一个或多个labels（同tags）以及metric value&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/datamodel.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图是所有数据点分布的一个简单视图，横轴是时间，纵轴是时间线，区域内每个点就是数据点。Prometheus每次接收数据，收到的是图中区域内纵向的一条线。这个表述很形象，在同一时刻，每条时间线只会产生一个数据点，但同时会有多条时间线产生数据，把这些数据点连在一起，就是一条竖线。这个特征很重要，影响数据写入和压缩的优化策略。&lt;/p&gt;

&lt;h3 id=&#34;探针数据&#34;&gt;探针数据&lt;/h3&gt;

&lt;p&gt;都体现在client_golang的库中，直接去&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/&#34;&gt;client_golang&lt;/a&gt;文章中参考。&lt;/p&gt;

&lt;h2 id=&#34;promql&#34;&gt;PromQL&lt;/h2&gt;

&lt;p&gt;PromQL 是 Prom 中的查询语言，提供了简洁的、贴近自然语言的语法实现时序数据的分析计算。&lt;/p&gt;

&lt;h3 id=&#34;原理&#34;&gt;原理&lt;/h3&gt;

&lt;p&gt;PromQL 表达式输入是一段文本，Prom 会解析这段文本，将它转化为一个结构化的语法树对象，进而实现相应的数据计算逻辑。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(avg_over_time(go_goroutines{job=&amp;quot;prometheus&amp;quot;}[5m])) by (instance)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述表达式可以从外往内分解为三层：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(…) by (instance)：序列纵向分组合并序列（包含相同的 instance 会分配到一组）
avg_over_time(…)
go_goroutines{job=&amp;quot;prometheus&amp;quot;}[5m]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;时间点对象MatrixSelector 对象，是获取时序数据的基础结构&lt;/li&gt;
&lt;li&gt;获取时间段里面的数据，通过iterator 是序列筛选结果的顺序访问接口，获取某个时间点往前的一段历史数据，这是一个二维矩阵 (matrix)，进而由外层函数将这段历史数据汇总成一个 vector&lt;/li&gt;
&lt;li&gt;实现对一段数据的汇总，然后求平均值&lt;/li&gt;
&lt;li&gt;最后来看关键字（keyword）sum 的实现，这里注意 sum 不是函数（Function）而是关键字。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;sum(avg_over_time(go_goroutines{job=&amp;ldquo;prometheus&amp;rdquo;}[5m])) by (instance) 计算过程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/promql.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;PromQL 有三个很简单的原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;任意 PromQL 返回的结果都不是原始数据，即使查询一个具体的 Metric（如 go_goroutines），结果也不是原始数据&lt;/li&gt;
&lt;li&gt;任意 Metrics 经过 Function 计算后会丢失&lt;code&gt;__name__&lt;/code&gt;Label&lt;/li&gt;
&lt;li&gt;子序列间具备完全相同的 Label/Value 键值对（可以有不同的&lt;code&gt;__name__&lt;/code&gt;）才能进行代数运算&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;storage&#34;&gt;storage&lt;/h2&gt;

&lt;h3 id=&#34;源码解读&#34;&gt;源码解读&lt;/h3&gt;

&lt;p&gt;真正存储指标的是storage.Appender，在scrape与storage之间有一层缓存。缓存主要的作用是过滤错误的指标。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type scrapeCache struct {
  iter uint64                           // scrape批次
    successfulCount int                   // 成功保存的元数据数
    series map[string]*cacheEntry         // 缓存解析的相关数据
    droppedSeries map[string]*uint64      // 缓存无效指标
    seriesCur  map[uint64]labels.Labels     // 本次采集指标
    seriesPrev map[uint64]labels.Labels   // 上次采集指标

    metaMtx  sync.Mutex
    metadata map[string]*metaEntry
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建scrapeCache，调用newScrapeLoop，初始化scrapeLoop，会判断scrapeCache是否为空，如果为nil，调用newScrapeCache对cache进行初始化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if cache == nil {
        cache = newScrapeCache()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;newScrapeCache()如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func newScrapeCache() *scrapeCache {
    return &amp;amp;scrapeCache{
        series:        map[string]*cacheEntry{},
        droppedSeries: map[string]*uint64{},
        seriesCur:     map[uint64]labels.Labels{},
        seriesPrev:    map[uint64]labels.Labels{},
        metadata:      map[string]*metaEntry{},
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;scrapeCache 方法介绍，这里简介各个fun的作用，详细代码不做注解。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 根据met信息，获取对应的cacheEntry
func (c *scrapeCache) get(met string) (*cacheEntry, bool)
// 根据met创建cacheEntry节点
func (c *scrapeCache) addRef(met string, ref uint64, lset labels.Labels, hash uint64)
// 添加无效指标，met作为key
func (c *scrapeCache) addDropped(met string)
// 根据met，检查该指标是否有效
func (c *scrapeCache) getDropped(met string) bool
// 添加当前采集指标
func (c *scrapeCache) trackStaleness(hash uint64, lset labels.Labels)
// 检查指标状态
func (c *scrapeCache) forEachStale(f func(labels.Labels) bool)
// 缓存清理
func (c *scrapeCache) iterDone(flushCache bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;存储过程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;分析scrapeLoop.append是如何实现存储数据的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (sl *scrapeLoop)  append(b []byte, contentType string, ts time.Time) (total, added, seriesAdded int, err error) {
    var (
    // 获取指标存储组件
        app            = sl.appender()
    // 获取解析组件
        p              = textparse.New(b, contentType)
        defTime        = timestamp.FromTime(ts)
        numOutOfOrder  = 0
        numDuplicates  = 0
        numOutOfBounds = 0
    )
    var sampleLimitErr error

loop:
    for {
        var et textparse.Entry
    // 开始遍历，遍历到EOF(字节流尾部)，终止遍历
        if et, err = p.Next(); err != nil {
            if err == io.EOF {
                err = nil
            }
            break
        }
    // 以下Entry类型跳过
        switch et {
        case textparse.EntryType:
            sl.cache.setType(p.Type())
            continue
        case textparse.EntryHelp:
            sl.cache.setHelp(p.Help())
            continue
        case textparse.EntryUnit:
            sl.cache.setUnit(p.Unit())
            continue
        case textparse.EntryComment:
            continue
        default:
        }
        total++

        t := defTime
    // 获取指标label，时间戳（如果设置了），当前样本值
        met, tp, v := p.Series()
    // 如果设置了honorTimestamps，时间戳设置为nil
        if !sl.honorTimestamps {
            tp = nil
        }
    // 如果时间戳不为空，更新当前t
        if tp != nil {
            t = *tp
        }
        // 检查该指标值是否有效，无效则直接跳过当前处理
        if sl.cache.getDropped(yoloString(met)) {
            continue
        }
    // 根据当前met获取对应的cacheEntry结构
        ce, ok := sl.cache.get(yoloString(met))
    // 如果从缓存中获取，则执行指标的存储操作
        if ok {
      // 指标存储
            switch err = app.AddFast(ce.lset, ce.ref, t, v); err {
            case nil:
        // 如果不带时间戳
                if tp == nil {
          // 存储该不带时间戳的指标到seriesCur中。
                    sl.cache.trackStaleness(ce.hash, ce.lset)
                }
       // 未找到错误，重置ok为false，执行!ok逻辑
            case storage.ErrNotFound:
                ok = false
      // 乱序样本
            case storage.ErrOutOfOrderSample:
        // 乱序样本错误记录，并上报
                numOutOfOrder++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of order sample&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfOrder.Inc()
                continue
      // 重复样本
            case storage.ErrDuplicateSampleForTimestamp:
        // 重复样本错误记录，并上报
                numDuplicates++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Duplicate sample for timestamp&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleDuplicate.Inc()
                continue
      // 存储越界
            case storage.ErrOutOfBounds:
        // 存储越界错误记录，并上报
                numOutOfBounds++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of bounds metric&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfBounds.Inc()
                continue
      // 超出样本限制错误
            case errSampleLimit:
        // 如果我们达到上限也要继续解析输出，所以我们要上报正确的样本总量
                sampleLimitErr = err
                added++
                continue
      // 未知情况，终止loop
            default:
                break loop
            }
        }
    // 在缓存中未查找到，
        if !ok {
            var lset labels.Labels
            // 生成mets
            mets := p.Metric(&amp;amp;lset)
      // 生成hash值
            hash := lset.Hash()

            // 根据配置重置label set
            lset = sl.sampleMutator(lset)

            // 如果label set为空，则表明该mets为非法指标
            if lset == nil {
        // 添加mets到无效指标字典中
                sl.cache.addDropped(mets)
                continue
            }

            var ref uint64
      // 存储指标
            ref, err = app.Add(lset, t, v)

      // 错误处理同上，不重复描述
            switch err {
            case nil:
      // 乱序样本
            case storage.ErrOutOfOrderSample:
                err = nil
                numOutOfOrder++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of order sample&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfOrder.Inc()
                continue
      // 重复样本
            case storage.ErrDuplicateSampleForTimestamp:
                err = nil
                numDuplicates++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Duplicate sample for timestamp&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleDuplicate.Inc()
                continue
      // 存储越界
            case storage.ErrOutOfBounds:
                err = nil
                numOutOfBounds++
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Out of bounds metric&amp;quot;, &amp;quot;series&amp;quot;, string(met))
                targetScrapeSampleOutOfBounds.Inc()
                continue
      // 样本限制
            case errSampleLimit:
                sampleLimitErr = err
                added++
                continue
            default:
                level.Debug(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;unexpected error&amp;quot;, &amp;quot;series&amp;quot;, string(met), &amp;quot;err&amp;quot;, err)
                break loop
            }
            if tp == nil {
                // 存储该不带时间戳的指标到seriesCur中。
                sl.cache.trackStaleness(hash, lset)
            }
        // 缓存该指标到series中
            sl.cache.addRef(mets, ref, lset, hash)
            seriesAdded++
        }
        added++
    }
  // 错误相关处理，不做分析。
    if sampleLimitErr != nil {
        if err == nil {
            err = sampleLimitErr
        }
        targetScrapeSampleLimit.Inc()
    }
    if numOutOfOrder &amp;gt; 0 {
        level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Error on ingesting out-of-order samples&amp;quot;, &amp;quot;num_dropped&amp;quot;, numOutOfOrder)
    }
    if numDuplicates &amp;gt; 0 {
        level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Error on ingesting samples with different value but same timestamp&amp;quot;, &amp;quot;num_dropped&amp;quot;, numDuplicates)
    }
    if numOutOfBounds &amp;gt; 0 {
        level.Warn(sl.l).Log(&amp;quot;msg&amp;quot;, &amp;quot;Error on ingesting samples that are too old or are too far into the future&amp;quot;, &amp;quot;num_dropped&amp;quot;, numOutOfBounds)
    }
    if err == nil {
    // 指标状态检查。
        sl.cache.forEachStale(func(lset labels.Labels) bool {
            // 标记存储中的过期指标
            _, err = app.Add(lset, defTime, math.Float64frombits(value.StaleNaN))
            switch err {
      // 以下错误不做处理
            case storage.ErrOutOfOrderSample, storage.ErrDuplicateSampleForTimestamp:
                err = nil
            }
            return err == nil
        })
    }
    if err != nil {
    // 出现错误，存储组件进行回滚
        app.Rollback()
        return total, added, seriesAdded, err
    }
  // 存储提交
    if err := app.Commit(); err != nil {
        return total, added, seriesAdded, err
    }

    // 执行缓存清理相关工作
    sl.cache.iterDone(len(b) &amp;gt; 0)

    return total, added, seriesAdded, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;整个存储逻辑都围绕着过滤无效指标进行。特殊点在于存储的时候指标分为有时间戳与无时间戳两种情况。&lt;/p&gt;

&lt;p&gt;1、有时间戳&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;解析指标数据通过Series()&lt;/li&gt;
&lt;li&gt;利用getDropped判断指标是否有效，无效则跳出处理&lt;/li&gt;
&lt;li&gt;通过get查找对应cacheEntry，如果找到利用app.AddFast直接存储样本值。如果未找到，使用sampleMutator进行解析重置，判断lset是否为空，为空则使用addDropped添加到无效字典中，跳出当前处理，如果有效则使用app.Add存储指标。(可以看到，通过get找到使用AddFast存储，未找到使用Add存储，感兴趣可以看下两个fun实现的区别)&lt;/li&gt;
&lt;li&gt;通过forEachStale检查指标是否过期。&lt;/li&gt;
&lt;li&gt;app.Add标记过期指标&lt;/li&gt;
&lt;li&gt;调用iterDone进行相关缓存清理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、无时间戳&lt;/p&gt;

&lt;p&gt;每次存储后，如果不带时间戳都会调用trackStaleness，存储指标到seriesCur中&lt;/p&gt;

&lt;p&gt;这里seriesCur与seriesPrev的作用就是处理指标label是否过期的。forEachStale实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *scrapeCache) forEachStale(f func(labels.Labels) bool) {
    for h, lset := range c.seriesPrev {
        if _, ok := c.seriesCur[h]; !ok {
            if !f(lset) {
                break
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果seriesPrev中的指标(label)存在于seriesPrev，则不处理，如果不存在，则说明过期。其中在iterDone中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 交换seriesPrev与seriesCur
c.seriesPrev, c.seriesCur = c.seriesCur, c.seriesPrev

// 清空当前指标缓存列表
for k := range c.seriesCur {
    delete(c.seriesCur, k)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以，每次存储处理后，都会交换seriesPrev与seriesCur，然后清空seriesCur。下次存储在做比较。如果命中过期规则，则标记该样本值为StaleNaN。&lt;/p&gt;

&lt;h3 id=&#34;local-storage&#34;&gt;local storage&lt;/h3&gt;

&lt;h4 id=&#34;v2&#34;&gt;v2&lt;/h4&gt;

&lt;p&gt;Prometheus 1.0版本的TSDB（V2存储引擎）使用了G家的LevelDB来做索引(PromSQL重度依赖LevelDB)，并且使用了和Facebook Gorilla一样的压缩算法，能够将16个字节的数据点压缩到平均1.37个字节。&lt;/p&gt;

&lt;p&gt;V2存储引擎对于大量的采样数据有自己的存储层，Prometheus为每个时序数据创建一个本地文件，以1024byte大小的chunk来组织。写到head chunk，写满1KB，就再生成新的块，完成的块，是不可再变更的 , 根据配置文件的设置，有一部份chunk会被保留在内存里，按照LRU算法，定期将块写进磁盘文件内。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;缺陷&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;文件数会随着时间线的数量同比增长，慢慢会耗尽inode。&lt;/li&gt;
&lt;li&gt;即便使用了Chunk写优化，若一次写入涉及的时间线过多，IOPS要求还是会很高。&lt;/li&gt;
&lt;li&gt;每个文件不可能会时刻保持open状态，一次查询可能需要重新打开大量文件，增大查询延迟。&lt;/li&gt;
&lt;li&gt;数据回收需要从大量文件扫描和重写数据，耗时较长。&lt;/li&gt;
&lt;li&gt;数据需要在内存中积累一定时间以Chunk写，V2会采用定时写Checkpoint的机制来尽量保证内存中数据不丢失。但通常记录Checkpoint的时间大于能承受的数据丢失的时间窗口，并且在节点恢复时从checkpoint restore数据的时间也会很长。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;优化策略&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Chunk写，热数据内存缓存&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prometheus一次性接收到的数据是一条竖线，包含很多的数据点，但是这些数据点属于不同的时间线。而当前的设计是一条时间线对应一个独立的文件，所以每次写入都会需要向很多不同的文件写入极少量的数据。针对这个问题，V2存储引擎的优化策略是Chunk写，针对单个时间线的写入必须是批量写，那就需要数据在时间线维度累积一定时间后才能凑到一定量的数据点。Chunk写策略带来的好处除了批量写外，还能优化热数据查询效率以及数据压缩率。V2存储引擎使用了和Facebook Gorilla一样的压缩算法，能够将16个字节的数据点压缩到平均1.37个字节，节省12倍内存和空间。Chunk写就要求数据一定要在服务器内存里积累一定的时间，即热数据基本都在内存中，查询效率很高。&lt;/p&gt;

&lt;h4 id=&#34;v3&#34;&gt;v3&lt;/h4&gt;

&lt;p&gt;Prometheus 2.0版本引入了全新的V3存储引擎，提供了更高的写入和查询性能。&lt;/p&gt;

&lt;p&gt;V3引擎完全重新设计，但是也延续了v2的一些优化策略，也来解决V2引擎中存在的这些问题。V3引擎可以看做是一个简单版、针对时序数据场景优化过后的LSM，可以带着LSM的设计思想来理解，先看一下V3引擎中数据的文件目录结构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/data.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;data目录下存放所有的数据，data目录的下一级目录是以&amp;rsquo;b-&amp;lsquo;为前缀，顺序自增的ID为后缀的目录，代表Block。每个Block下有chunks、index和meta.json，chunks目录下存放chunk的数据。这个chunk和V2的chunk是一个概念，唯一的不同是一个chunk内会包含很多的时间线，而不再只是一条。index是这个block下对chunk的索引，可以支持根据某个label快速定位到时间线以及数据所在的chunk。meta.json是一个简单的关于block数据和状态的一个描述文件。要理解V3引擎的设计思想，只需要搞明白几个问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk文件的存储格式？&lt;/li&gt;
&lt;li&gt;index的存储格式，如何实现快速查找？&lt;/li&gt;
&lt;li&gt;为何最后一个block没有chunk目录但有一个wal目录？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/block.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Prometheus将数据按时间维度切分为多个block，每个block被认为是独立的一个数据库，覆盖不同的时间范围的数据，完全没有交叉。每个Block下chunk内的数据dump到文件后即不可再修改，只有最近的一个block允许接收新数据。最新的block内数据写入会先写到一个内存的结构，为了保证数据不丢失，会先写一份WAL（write ahead log）。&lt;/p&gt;

&lt;p&gt;V3完全借鉴了LSM的设计思想，针对时序数据特征做了一些优化，带来很多好处：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当查询一个时间范围的数据时，可快速排除无关的block。每个block有独立的index，能够有效解决V2内遇到的『无效时间线 Series Churn』的问题。&lt;/li&gt;
&lt;li&gt;内存数据dump到chunk file，可高效采用大块数据顺序写，对SSD和HDD都很友好。&lt;/li&gt;
&lt;li&gt;和V2一样，最近的数据在内存内，最近的数据也是最热的数据，在内存可支持最高效的查询。&lt;/li&gt;
&lt;li&gt;老数据的回收变得非常简单和高效，只需要删除少量目录。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/principle/block1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;V3内block以两个小时的跨度来切割，这个时间跨度不能太大，也不能太小。太大的话若内存中要保留两个小时数据，则内存占用会比较大。太小的话会导致太多的block，查询时需要对更多的文件做查询。所以两个小时是一个综合考虑后决定的值，但是当查询大跨度时间范围时，仍不可避免需要跨多个文件，例如查询一周时间跨度需要84个文件。V3也是采用了LSM一样的compaction策略来做查询优化，把小的block合并为大的block，compaction期间也可做其他一些事，例如删除过期数据或重构chunk数据以支持更高效的查询。InfluxDB也有多种不同的compaction策略，在不同的时刻使用。&lt;/p&gt;

&lt;p&gt;prometheus重2.0版本开始使用了V3引擎，V3没有和V2一样采用LevelDB，在已经持久化的Block，Index已经固定下来，不可修改。而对于最新的还在写数据的block，V3则会把所有的索引全部hold在内存，维护一个内存结构，等到这个block被关闭，再持久化到文件。这样做会比较简单一点，内存里维护时间线到ID的映射以及label到ID列表的映射，查询效率会很高。而且Prometheus对Label的基数会有一个假设：『a real-world dataset of ~4.4 million series with about 12 labels each has less than 5,000 unique labels』，这个全部保存在内存也是一个很小的量级，完全没有问题。InfluxDB采用的是类似的策略，而其他一些TSDB则直接使用ElasticSearch作为索引引擎。&lt;/p&gt;

&lt;p&gt;针对时序数据这种写多读少的场景，类LSM的存储引擎还是有不少优势的。有些TSDB直接基于开源的LSM引擎分布式数据库例如Hbase或Cassandra，也有自己基于LevelDB/RocksDB研发，或者再像InfluxDB和Prometheus一样纯自研，因为时序数据这一特定场景还是可以做更多的优化，例如索引、compaction策略等。Prometheus V3引擎的设计思想和InfluxDB真的很像，优化思路高度一致，后续在有新的需求的出现后，会有更多变化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prometheus将Timeseries数据按2小时一个block进行存储。每个block由一个目录组成，该目录里包含：一个或者多个chunk文件（保存timeseries数据）、一个metadata文件、一个index文件（通过metric name和labels查找timeseries数据在chunk文件的位置）。最新写入的数据保存在内存block中，达到2小时后写入磁盘。为了防止程序崩溃导致数据丢失，实现了WAL（write-ahead-log）机制，将timeseries原始数据追加写入log中进行持久化。删除timeseries时，删除条目会记录在独立的tombstone文件中，而不是立即从chunk文件删除。启动时会以写入日志(WAL)的方式来实现重播，从而恢复数据。&lt;/p&gt;

&lt;p&gt;这些2小时的block会在后台压缩成更大的block，数据压缩合并成更高level的block文件后删除低level的block文件。这个和leveldb、rocksdb等LSM树的思路一致。&lt;/p&gt;

&lt;p&gt;这些设计和Gorilla的设计高度相似，所以Prometheus几乎就是等于一个缓存TSDB。它本地存储的特点决定了它不能用于long-term数据存储，只能用于短期窗口的timeseries数据保存和查询，并且不具有高可用性（宕机会导致历史数据无法读取）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;具体形式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、磁盘文件结构&lt;/p&gt;

&lt;p&gt;内存中的block&lt;/p&gt;

&lt;p&gt;内存中的block数据未刷盘时，block目录下面主要保存wal文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./data/01BKGV7JBM69T2G1BGBGM6KB12
./data/01BKGV7JBM69T2G1BGBGM6KB12/meta.json
./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000002
./data/01BKGV7JBM69T2G1BGBGM6KB12/wal/000001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;持久化的block&lt;/p&gt;

&lt;p&gt;持久化的block目录下wal文件被删除，timeseries数据保存在chunk文件里。index用于索引timeseries在wal文件里的位置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./data/01BKGV7JC0RY8A6MACW02A2PJD
./data/01BKGV7JC0RY8A6MACW02A2PJD/meta.json
./data/01BKGV7JC0RY8A6MACW02A2PJD/index
./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks
./data/01BKGV7JC0RY8A6MACW02A2PJD/chunks/000001
./data/01BKGV7JC0RY8A6MACW02A2PJD/tombstones
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、mmap&lt;/p&gt;

&lt;p&gt;使用mmap读取压缩合并后的大文件（不占用太多句柄），建立进程虚拟地址和文件偏移的映射关系，只有在查询读取对应的位置时才将数据真正读到物理内存。绕过文件系统page cache，减少了一次数据拷贝。查询结束后，对应内存由Linux系统根据内存压力情况自动进行回收，在回收之前可用于下一次查询命中。因此使用mmap自动管理查询所需的的内存缓存，具有管理简单，处理高效的优势。
从这里也可以看出，它并不是完全基于内存的TSDB，和Gorilla的区别在于查询历史数据需要读取磁盘文件。&lt;/p&gt;

&lt;p&gt;3、Compaction&lt;/p&gt;

&lt;p&gt;Compaction主要操作包括合并block、删除过期数据、重构chunk数据。其中合并多个block成为更大的block，可以有效减少block个数，当查询覆盖的时间范围较长时，避免需要合并很多block的查询结果。
为提高删除效率，删除时序数据时，会记录删除的位置，只有block所有数据都需要删除时，才将block整个目录删除。因此block合并的大小也需要进行限制，避免保留了过多已删除空间（额外的空间占用）。比较好的方法是根据数据保留时长，按百分比（如10%）计算block的最大时长。&lt;/p&gt;

&lt;p&gt;4、Inverted Index&lt;/p&gt;

&lt;p&gt;Inverted Index（倒排索引）基于其内容的子集提供数据项的快速查找。简而言之，我可以查看所有标签为app=“nginx”的数据，而不必遍历每一个timeseries，并检查是否包含该标签。
为此，每个时间序列key被分配一个唯一的ID，通过它可以在恒定的时间内检索，在这种情况下，ID就是正向索引。
举个栗子：如ID为9,10,29的series包含label app=&amp;ldquo;nginx&amp;rdquo;，则lable &amp;ldquo;nginx&amp;rdquo;的倒排索引为[9,10,29]用于快速查询包含该label的series。&lt;/p&gt;

&lt;p&gt;5、存储配置&lt;/p&gt;

&lt;p&gt;对于本地存储，prometheus提供了一些配置项，主要包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--storage.tsdb.path: 存储数据的目录，默认为data/，如果要挂外部存储，可以指定该目录
--storage.tsdb.retention.time: 数据过期清理时间，默认保存15天
--storage.tsdb.retention.size: 实验性质，声明数据块的最大值，不包括wal文件，如512MB
--storage.tsdb.retention: 已被废弃，改为使用storage.tsdb.retention.time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus将所有当前使用的块保留在内存中。此外，它将最新使用的块保留在内存中，最大内存可以通过storage.local.memory-chunks标志配置。&lt;/p&gt;

&lt;p&gt;监测当前使用的内存量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_chunks
process_resident_memory_bytes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监测当前使用的存储指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_series: 时间序列持有的内存当前块数量
prometheus_local_storage_memory_chunks: 在内存中持久块的当前数量


prometheus_local_storage_chunks_to_persist: 当前仍然需要持久化到磁盘的的内存块数量
prometheus_local_storage_persistence_urgency_score: 紧急程度分数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、性能&lt;/p&gt;

&lt;p&gt;在文章Writing a Time Series Database from Scratch里，作者给出了benchmark测试结果为Macbook Pro上写入达到2000万每秒。这个数据比Gorilla论文中的目标7亿次写入每分钟（1000千多万每秒）提供了更高的单机性能。&lt;/p&gt;

&lt;h3 id=&#34;remote-storage&#34;&gt;remote storage&lt;/h3&gt;

&lt;p&gt;Prometheus 的设计者非常看重监控系统自身的稳定性，所以 Prometheus 仅仅依赖了本地文件系统，而这就决定了 Prometheus 自身并不适合存储长期数据。本地存储的优势就是运维简单,缺点就是无法海量的metrics持久化和数据存在丢失的风险，我们在实际使用过程中，出现过几次wal文件损坏，无法再写入的问题。&lt;/p&gt;

&lt;p&gt;所以 Prometheus 提供了 remote read 和 remote write 的接口，让用户自己去实现对接，prometheus以两种方式与远程存储系统集成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prometheus可以以标准格式将其提取的样本写入远程URL。&lt;/li&gt;
&lt;li&gt;Prometheus可以以标准格式从远程URL读取（返回）样本数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/prometheus/20190315.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Adapter 是一个中间组件，Prometheus 与 Adapter 之间通过由 Prometheus 定义的标准格式发送和接收数据，Adapter 与外部存储系统之间的通信可以自定义，目前 Prometheus 和 Adapter 之间通过 grpc 通信。Prometheus 将 samples 发送到 Adapter。为了提高效率，samples 会在队列中先缓存，再打包发送给 Adapter，所以一个读请求中包含了 start_timestamp，end_timestamp 和 label_matchers，response 则包含所有 match 到的 time series，也就是说，Prometheus 仅通过 Adapter 来获取时间序列，进一步的处理都在 Prometheus 中完成。&lt;/p&gt;

&lt;p&gt;remote read 和 remote write 的配置还没有稳定，我们从代码中来一探究竟，HTTPClientConfig 可以用来配置 HTTP 相关的 auth 信息，proxy 方式，以及 tls。WriteRelabelConfigs 用在发送过程中对 timeseries 进行 relabel。QueueConfig 定义了发送队列的 batch size，queue 数量，发送失败时的重试次数与等待时间等参数。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;write&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prometheus 默认定义了 1000 个 queue，batch size 为 100，预期可以达到 1M samples/s 的发送速率。Prometheus 输出了一些 queue 相关的指标，例如 failed_samples_total, dropped_samples_total，如果这两个指标的 rate 大于 0，就需要说明 Remote Storage 出现了问题导致发送失败，或者队列满了导致 samples 被丢弃掉。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReadRecent 如果为 false，Prometheus 会在处理查询时比较本地存储中最早的数据的 timestamp 与 query 的 start timestamp，如果发现需要的数据都在本地存储中，则会跳过对 Remote Storage 的查询。&lt;/p&gt;

&lt;p&gt;社区中支持prometheus远程读写的方案&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;AppOptics: write
Chronix: write
Cortex: read and write
CrateDB: read and write
Elasticsearch: write
Gnocchi: write
Graphite: write
InfluxDB: read and write
OpenTSDB: write
PostgreSQL/TimescaleDB: read and write
SignalFx: write
clickhouse: read and write
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前远程存储使用最多的是influxdb（收费），opentsdb（依赖hbase），m3db（不稳定）,VM(很优秀的存储查询性能)。&lt;/p&gt;

&lt;h2 id=&#34;服务发现&#34;&gt;服务发现&lt;/h2&gt;

&lt;p&gt;Prometheus目前支持以下平台的动态发现能力：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;容器编排系统：kubernetes&lt;/li&gt;
&lt;li&gt;云平台：EC2、Azure、OpenStack&lt;/li&gt;
&lt;li&gt;服务发现：DNS、Zookeeper、Consul 等。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;加载配置
​
ServiceDiscoveryConfig配置结构如下：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type ServiceDiscoveryConfig struct {
    // 静态服务发现配置
    StaticConfigs []*targetgroup.Group `yaml:&amp;quot;static_configs,omitempty&amp;quot;`
    // DNS服务发现配置
    DNSSDConfigs []*dns.SDConfig `yaml:&amp;quot;dns_sd_configs,omitempty&amp;quot;`
    // 配置文件服务发现配置
    FileSDConfigs []*file.SDConfig `yaml:&amp;quot;file_sd_configs,omitempty&amp;quot;`
    // Consul服务发现配置
    ConsulSDConfigs []*consul.SDConfig `yaml:&amp;quot;consul_sd_configs,omitempty&amp;quot;`
    // zookeeper Serverset 服务发现配置
    ServersetSDConfigs []*zookeeper.ServersetSDConfig `yaml:&amp;quot;serverset_sd_configs,omitempty&amp;quot;`
    // zookeeper Nerve 服务发现配置
    NerveSDConfigs []*zookeeper.NerveSDConfig `yaml:&amp;quot;nerve_sd_configs,omitempty&amp;quot;`
    // 根据Marathon API 服务发现配置
    MarathonSDConfigs []*marathon.SDConfig `yaml:&amp;quot;marathon_sd_configs,omitempty&amp;quot;`
    // 根据Kubernetes API 服务发现配置
    KubernetesSDConfigs []*kubernetes.SDConfig `yaml:&amp;quot;kubernetes_sd_configs,omitempty&amp;quot;`
    // GCE 服务发现配置
    GCESDConfigs []*gce.SDConfig `yaml:&amp;quot;gce_sd_configs,omitempty&amp;quot;`
    // EC2服务发现配置
    EC2SDConfigs []*ec2.SDConfig `yaml:&amp;quot;ec2_sd_configs,omitempty&amp;quot;`
    // Openstack 服务发现配置
    OpenstackSDConfigs []*openstack.SDConfig `yaml:&amp;quot;openstack_sd_configs,omitempty&amp;quot;`
    // Azure 服务发现配置
    AzureSDConfigs []*azure.SDConfig `yaml:&amp;quot;azure_sd_configs,omitempty&amp;quot;`
    // Triton 服务发现配置
    TritonSDConfigs []*triton.SDConfig `yaml:&amp;quot;triton_sd_configs,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​
在Prometheus初始化过程中，通过执行discoveryManagerScrape.ApplyConfig进行服务发现相关配置的加载。&lt;/p&gt;

&lt;p&gt;移除目前正在运行的providers，根据新的provided 配置，启动新的providers。。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) ApplyConfig(cfg map[string]sd_config.ServiceDiscoveryConfig) error {
    // 加锁
    m.mtx.Lock()
  // 函数结束后 解锁
    defer m.mtx.Unlock()
  // 遍历已存在target
    for pk := range m.targets {
        if _, ok := cfg[pk.setName]; !ok {
      // 删除标签
            discoveredTargets.DeleteLabelValues(m.name, pk.setName)
        }
    }
  // 取消所有Discoverer
    m.cancelDiscoverers()
    for name, scfg := range cfg {
    // 根据scfg，注册服务发现实例
        m.registerProviders(scfg, name)
    // 设置标签
        discoveredTargets.WithLabelValues(m.name, name).Set(0)
    }
    for _, prov := range m.providers {
    // 启动服务发现实例
        m.startProvider(m.ctx, prov)
    }

    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;注册Providers&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中m.registerProviders的主要作用就是根据cfg（配置）注册所有provider实例，保存在m.providers&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) registerProviders(cfg sd_config.ServiceDiscoveryConfig, setName string) {
    // 标签
    var added bool
  // 加载Providers的add方法
    add := func(cfg interface{}, newDiscoverer func() (Discoverer, error)) {
    // 读取cfg类型
        t := reflect.TypeOf(cfg).String()
        for _, p := range m.providers {
      // 检查该cfg是否加载过
            if reflect.DeepEqual(cfg, p.config) {
        // 如果加载过，记录该job
                p.subs = append(p.subs, setName)
        // 变更标签状态
                added = true
        // 跳出
                return
            }
        }
        // 创建一个Discoverer实例
        d, err := newDiscoverer()
        if err != nil {
            level.Error(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Cannot create service discovery&amp;quot;, &amp;quot;err&amp;quot;, err, &amp;quot;type&amp;quot;, t)
            failedConfigs.WithLabelValues(m.name).Inc()
            return
        }
        // 创建一个provider
        provider := provider{
      // 生成provider名称
            name:   fmt.Sprintf(&amp;quot;%s/%d&amp;quot;, t, len(m.providers)),
      // 关联对应的Discoverer实例（比如DNS、zk等）
            d:      d,
      // 关联配置
            config: cfg,
      // 关联job
            subs:   []string{setName},
        }
    // 添加该provider到m.providers队列中
        m.providers = append(m.providers, &amp;amp;provider)
        // 更新标签
        added = true
    }
    // 遍历DNS配置，生成该Discoverer
    for _, c := range cfg.DNSSDConfigs {
        add(c, func() (Discoverer, error) {
            return dns.NewDiscovery(*c, log.With(m.logger, &amp;quot;discovery&amp;quot;, &amp;quot;dns&amp;quot;)), nil
        })
    }
  .
  .
  .
  .
  .
  .
  // 类似配置遍历省略，感兴趣可以阅读源码查看
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;启动Provider&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在ApplyConfig，执行m.startProvider(m.ctx, prov)启动provider。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) startProvider(ctx context.Context, p *provider) {
    level.Info(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Starting provider&amp;quot;, &amp;quot;provider&amp;quot;, p.name, &amp;quot;subs&amp;quot;, fmt.Sprintf(&amp;quot;%v&amp;quot;, p.subs))
    ctx, cancel := context.WithCancel(ctx)
  // 记录发现的服务
    updates := make(chan []*targetgroup.Group)
    // 添加取消方法
    m.discoverCancel = append(m.discoverCancel, cancel)
    // 执行run  每个服务发现都有自己的run方法。
    go p.d.Run(ctx, updates)
  // 更新发现的服务
    go m.updater(ctx, p, updates)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里分析DNS 服务发现对应的Run方法。需要标注下，DNS对应的Discovery其实是refresh中的Discovery的Run实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d.Discovery = refresh.NewDiscovery(
        logger,
        &amp;quot;dns&amp;quot;,
        time.Duration(conf.RefreshInterval),
        d.refresh,
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (d *Discovery) Run(ctx context.Context, ch chan&amp;lt;- []*targetgroup.Group) {
  // 首次进入，执行更新
    tgs, err := d.refresh(ctx)
    if err != nil {
        level.Error(d.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Unable to refresh target groups&amp;quot;, &amp;quot;err&amp;quot;, err.Error())
    } else {
        select {
        case ch &amp;lt;- tgs:
        case &amp;lt;-ctx.Done():
            return
        }
    }
    // 创建定时器
    ticker := time.NewTicker(d.interval)
    defer ticker.Stop()

    for {
        select {
        case &amp;lt;-ticker.C:
      // 定时执行更新，如果发现变化，通过ch发出更新信息
            tgs, err := d.refresh(ctx)
            if err != nil {
                level.Error(d.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Unable to refresh target groups&amp;quot;, &amp;quot;err&amp;quot;, err.Error())
                continue
            }

            select {
      // 发送 变化的targets
            case ch &amp;lt;- tgs:
            case &amp;lt;-ctx.Done():
                return
            }
        case &amp;lt;-ctx.Done():
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;更新服务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当服务发现变化的targets时，通过updates chan进行更新。最终更新Discovery Manager的targets。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) updater(ctx context.Context, p *provider, updates chan []*targetgroup.Group) {
   for {

      select {
      case &amp;lt;-ctx.Done():
         return
      // 接收updates数据
      case tgs, ok := &amp;lt;-updates:
         receivedUpdates.WithLabelValues(m.name).Inc()
         if !ok {
            level.Debug(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;discoverer channel closed&amp;quot;, &amp;quot;provider&amp;quot;, p.name)
            return
         }
                 // 更新targets
         for _, s := range p.subs {
            m.updateGroup(poolKey{setName: s, provider: p.name}, tgs)
         }

         select {
         // 发送更新通知
         case m.triggerSend &amp;lt;- struct{}{}:
         default:
         }
      }
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;启动discovery manager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;加载完配置，并且完成注册、启动、更新操作后，开始执行discoveryManagerScrape.Run方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Run() error {
  // 后台处理
    go m.sender()
    for range m.ctx.Done() {
        m.cancelDiscoverers()
        return m.ctx.Err()
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定时执行，当接收到服务发现的更新通知，通过m.allGroups()同步服务快照信息到scrapeManager&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) sender() {
  // 创建定时器
    ticker := time.NewTicker(m.updatert)
    defer ticker.Stop()

    for {
        select {
        case &amp;lt;-m.ctx.Done():
            return
        case &amp;lt;-ticker.C:
            select {
      // 检测到更新
            case &amp;lt;-m.triggerSend:
                sentUpdates.WithLabelValues(m.name).Inc()
                select {
        // 通过allGroups同步服务快照信息到scrapeManager
                case m.syncCh &amp;lt;- m.allGroups():
                default:
                    delayedUpdates.WithLabelValues(m.name).Inc()
                    level.Debug(m.logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;discovery receiver&#39;s channel was full so will retry the next cycle&amp;quot;)
                    select {
                    case m.triggerSend &amp;lt;- struct{}{}:
                    default:
                    }
                }
            default:
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;关联ScrapeManager&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关联&lt;/p&gt;

&lt;p&gt;在ScrapeManager在启动的时候会关联discoveryManagerScrape.SyncCh()。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func() error {
                &amp;lt;-reloadReady.C
                // 关联 discoveryManagerScrape 的 syncCh
                err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
                level.Info(logger).Log(&amp;quot;msg&amp;quot;, &amp;quot;Scrape manager stopped&amp;quot;)
                return err
            },
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更新&lt;/p&gt;

&lt;p&gt;更新ScrapeManager的targets&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) Run(tsets &amp;lt;-chan map[string][]*targetgroup.Group) error {
    go m.reloader()
    for {
        select {
    // 收到更新targets
        case ts := &amp;lt;-tsets:
            m.updateTsets(ts)

            select {
            case m.triggerReload &amp;lt;- struct{}{}:
            default:
            }

        case &amp;lt;-m.graceShut:
            return nil
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行更新&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (m *Manager) updateTsets(tsets map[string][]*targetgroup.Group) {
    m.mtxScrape.Lock()
  // 替换新的 tagets
    m.targetSets = tsets
    m.mtxScrape.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;discoveryManager在加载配置的时候，顺便完成provider的注册、启动、以及discovery的自更新通知操作。discoveryManager与ScrapeManager通过discoveryManager的syncCh通道来关联同步。&lt;/p&gt;

&lt;p&gt;整个服务发现的流程很值得学习，尤其是discoveryManager支持多种服务发现的扩展配置的相关设设计很值得学习。&lt;/p&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;h2 id=&#34;集群&#34;&gt;集群&lt;/h2&gt;

&lt;p&gt;Prometheus有两个比较著名的扩展版一个是cortex，另一个是thanos。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos&lt;/a&gt;在github的简介是『Highly available Prometheus setup with long term storage capabilities』，它基于Prometheus的最大改进是底层存储可扩展支持对象存储，例如AWS的S3，使得单机容量可扩展。这个得益于Prometheus 2.0中V3引擎的特性，持久化的Chunk文件是immutable的，所以能够很容易迁移到对象存储上。从它的设计文档里可以看出，它引入了一个Sidecar节点，与Prometheus server结对部署，主要作用将本地数据backup到远端的对象存储。当然数据被切割到本地和对象存储内后，为了支持统一的查询接口，又引入了Store层。Store层支持标准查询接口，屏蔽了底层是对象存储的细节，同时做了一些查询优化例如对Index的缓存。thanos中包含多个类型的节点，包括Prometheus Server、Sidecar、Store node、Rule node、compactor和query layer，其中只有query layer能水平扩展，因为其是无状态的。也就是说，单个实例的Prometheus其写入能力还是会有瓶颈，cotex相比它则在scalability上改进了更多。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/&#34;&gt;cortex&lt;/a&gt;在github的简介是『A multitenant, horizontally scalable Prometheus as a Service』，几个关键词：多租户、水平扩展及服务化。&lt;/p&gt;

&lt;p&gt;现在还可以使用远程存储聚合的方式来实现集群，比如做的比较好的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Ioutil</title>
          <link>https://kingjcy.github.io/post/golang/go-ioutil/</link>
          <pubDate>Sat, 13 Jan 2018 11:04:07 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-ioutil/</guid>
          <description>&lt;p&gt;ioutil主要是提供了一些常用、方便的IO操作函数。&lt;/p&gt;

&lt;h1 id=&#34;ioutil&#34;&gt;ioutil&lt;/h1&gt;

&lt;p&gt;ioutil针对reader和writer这两个接口封装的基础操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Discard 是一个 io.Writer 接口，调用它的 Write 方法将不做任何事情
// 并且始终成功返回。
var Discard io.Writer = devNull(0)

// ReadAll 读取 r 中的所有数据，返回读取的数据和遇到的错误。
// 如果读取成功，则 err 返回 nil，而不是 EOF，因为 ReadAll 定义为读取
// 所有数据，所以不会把 EOF 当做错误处理。
func ReadAll(r io.Reader) ([]byte, error)

// ReadFile 读取文件中的所有数据，返回读取的数据和遇到的错误。
// 如果读取成功，则 err 返回 nil，而不是 EOF
func ReadFile(filename string) ([]byte, error)

// WriteFile 向文件中写入数据，写入前会清空文件。
// 如果文件不存在，则会以指定的权限创建该文件。
// 返回遇到的错误。
func WriteFile(filename string, data []byte, perm os.FileMode) error

// ReadDir 读取指定目录中的所有目录和文件（不包括子目录）。
// 返回读取到的文件信息列表和遇到的错误，列表是经过排序的。
func ReadDir(dirname string) ([]os.FileInfo, error)

// NopCloser 将 r 包装为一个 ReadCloser 类型，但 Close 方法不做任何事情。
func NopCloser(r io.Reader) io.ReadCloser

// TempFile 在 dir 目录中创建一个以 prefix 为前缀的临时文件，并将其以读
// 写模式打开。返回创建的文件对象和遇到的错误。
// 如果 dir 为空，则在默认的临时目录中创建文件（参见 os.TempDir），多次
// 调用会创建不同的临时文件，调用者可以通过 f.Name() 获取文件的完整路径。
// 调用本函数所创建的临时文件，应该由调用者自己删除。
func TempFile(dir, prefix string) (f *os.File, err error)

// TempDir 功能同 TempFile，只不过创建的是目录，返回目录的完整路径。
func TempDir(dir, prefix string) (name string, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;示例&#34;&gt;示例&lt;/h1&gt;

&lt;h2 id=&#34;读取目录&#34;&gt;读取目录&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    rd, err := ioutil.ReadDir(&amp;quot;/&amp;quot;)
    fmt.Println(err)
    for _, fi := range rd {
        if fi.IsDir() {
            fmt.Printf(&amp;quot;[%s]\n&amp;quot;, fi.Name())

        } else {
            fmt.Println(fi.Name())
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;临时目录-临时文件&#34;&gt;临时目录、临时文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    // 创建临时目录
    dir, err := ioutil.TempDir(&amp;quot;&amp;quot;, &amp;quot;Test&amp;quot;)
    if err != nil {
        fmt.Println(err)
    }
    defer os.Remove(dir) // 用完删除
    fmt.Printf(&amp;quot;%s\n&amp;quot;, dir)

    // 创建临时文件
    f, err := ioutil.TempFile(dir, &amp;quot;Test&amp;quot;)
    if err != nil {
        fmt.Println(err)
    }
    defer os.Remove(f.Name()) // 用完删除
    fmt.Printf(&amp;quot;%s\n&amp;quot;, f.Name())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;读取文件&#34;&gt;读取文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;io/ioutil&amp;quot;
)

func main() {
    b, err := ioutil.ReadFile(&amp;quot;test.log&amp;quot;)
    if err != nil {
        fmt.Print(err)
    }
    fmt.Println(b)
    str := string(b)
    fmt.Println(str)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;写文件&#34;&gt;写文件&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
   &amp;quot;io/ioutil&amp;quot;
)

func check(e error) {
   if e != nil {
       panic(e)
   }
}

func main() {

   d1 := []byte(&amp;quot;hello\ngo\n&amp;quot;)
   err := ioutil.WriteFile(&amp;quot;test.txt&amp;quot;, d1, 0644)
   check(err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读取文件和写文件内容还可以使用&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/#文件io&#34;&gt;os包&lt;/a&gt;来处理。一般也是使用os标准库来处理。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Bytes</title>
          <link>https://kingjcy.github.io/post/golang/go-bytes/</link>
          <pubDate>Mon, 25 Dec 2017 14:28:17 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-bytes/</guid>
          <description>&lt;p&gt;该包定义了一些操作 byte slice 的便利操作。因为字符串可以表示为 []byte，因此，bytes 包定义的函数、方法等和 strings 包很类似，所以讲解时会和 strings 包类似甚至可以直接参考。&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;h2 id=&#34;是否存在某个子-slice&#34;&gt;是否存在某个子 slice&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 子 slice subslice 在 b 中，返回 true
func Contains(b, subslice []byte) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数的内部调用了 bytes.Index 函数（在后面会讲解）:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Contains(b, subslice []byte) bool {
    return Index(b, subslice) != -1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;题外：对比 strings.Contains 你会发现，一个判断 &amp;gt;=0，一个判断 != -1，可见库不是一个人写的，没有做到一致性。&lt;/p&gt;

&lt;h2 id=&#34;byte-出现次数&#34;&gt;[]byte 出现次数&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// slice sep 在 s 中出现的次数（无重叠）
func Count(s, sep []byte) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和 strings 实现不同，此包中的 Count 核心代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;count := 0
c := sep[0]
i := 0
t := s[:len(s)-n+1]
for i &amp;lt; len(t) {
    // 判断 sep 第一个字节是否在 t[i:] 中
    // 如果在，则比较之后相应的字节
    if t[i] != c {
        o := IndexByte(t[i:], c)
        if o &amp;lt; 0 {
            break
        }
        i += o
    }
    // 执行到这里表示 sep[0] == t[i]
    if n == 1 || Equal(s[i:i+n], sep) {
        count++
        i += n
        continue
    }
    i++
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;runes-类型转换&#34;&gt;Runes 类型转换&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 将 []byte 转换为 []rune
func Runes(s []byte) []rune
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数将 []byte 转换为 []rune ，适用于汉字等多字节字符，示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b:=[]byte(&amp;quot;你好，世界&amp;quot;)
for k,v:=range b{
    fmt.Printf(&amp;quot;%d:%s |&amp;quot;,k,string(v))
}
r:=bytes.Runes(b)
for k,v:=range r{
    fmt.Printf(&amp;quot;%d:%s|&amp;quot;,k,string(v))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0:ä |1:½ |2:  |3:å |4:¥ |5:½ |6:ï |7:¼ |8:  |9:ä |10:¸ |11:  |12:ç |13:  |14: |
0:你|1:好|2:，|3:世|4:界|
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;其它函数&#34;&gt;其它函数&lt;/h2&gt;

&lt;p&gt;其它大部分函数、方法与 strings 包下的函数、方法类似，只是数据源从 string 变为了 []byte ，请参考 strings 包的用法。&lt;/p&gt;

&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;

&lt;h2 id=&#34;reader-类型&#34;&gt;Reader 类型&lt;/h2&gt;

&lt;p&gt;看到名字就能猜到，这是实现了 io 包中的接口。其实这就是缓存io，对缓存中的[]byte进行读写操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader struct {
    s        []byte
    i        int64 // 当前读取下标
    prevRune int   // 前一个字符的下标，也可能 &amp;lt; 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bytes 包下的 Reader 类型实现了 io 包下的 Reader, ReaderAt, RuneReader, RuneScanner, ByteReader, ByteScanner, ReadSeeker, Seeker, WriterTo 等多个接口。主要用于 Read 数据。&lt;/p&gt;

&lt;p&gt;我们需要在通过 bytes.NewReader 方法来初始化 bytes.Reader 类型的对象。初始化时传入 []byte 类型的数据。NewReader 函数签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(b []byte) *Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果直接声明该对象了，可以通过 Reset 方法重新写入数据，示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x:=[]byte(&amp;quot;你好，世界&amp;quot;)

r1:=bytes.NewReader(x)
d1:=make([]byte,len(x))
n,_:=r1.Read(d1)
fmt.Println(n,string(d1))

r2:=bytes.Reader{}
r2.Reset(x)
d2:=make([]byte,len(x))
n,_=r2.Read(d2)
fmt.Println(n,string(d2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;15 你好，世界
15 你好，世界
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reader 包含了 8 个读取相关的方法，实现了前面提到的 io 包下的 9 个接口（ReadSeeker 接口内嵌 Reader 和 Seeker 两个接口）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 读取数据至 b
func (r *Reader) Read(b []byte) (n int, err error)
// 读取一个字节
func (r *Reader) ReadByte() (byte, error)
// 读取一个字符
func (r *Reader) ReadRune() (ch rune, size int, err error)
// 读取数据至 w
func (r *Reader) WriteTo(w io.Writer) (n int64, err error)
// 进度下标指向前一个字节，如果 r.i &amp;lt;= 0 返回错误。
func (r *Reader) UnreadByte()
// 进度下标指向前一个字符，如果 r.i &amp;lt;= 0 返回错误，且只能在每次 ReadRune 方法后使用一次，否则返回错误。
func (r *Reader) UnreadRune()
// 读取 r.s[off:] 的数据至b，该方法忽略进度下标 i，不使用也不修改。
func (r *Reader) ReadAt(b []byte, off int64) (n int, err error)
// 根据 whence 的值，修改并返回进度下标 i ，当 whence == 0 ，进度下标修改为 off，当 whence == 1 ，进度下标修改为 i+off，当 whence == 2 ，进度下标修改为 len[s]+off.
// off 可以为负数，whence 的只能为 0，1，2，当 whence 为其他值或计算后的进度下标越界，则返回错误。
func (r *Reader) Seek(offset int64, whence int) (int64, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x := []byte(&amp;quot;你好，世界&amp;quot;)
r1 := bytes.NewReader(x)

ch, size, _ := r1.ReadRune()
fmt.Println(size, string(ch))
_ = r1.UnreadRune()
ch, size, _ = r1.ReadRune()
fmt.Println(size, string(ch))
_ = r1.UnreadRune()

by, _ := r1.ReadByte()
fmt.Println(by)
_ = r1.UnreadByte()
by, _ = r1.ReadByte()
fmt.Println(by)
_ = r1.UnreadByte()

d1 := make([]byte, 6)
n, _ := r1.Read(d1)
fmt.Println(n, string(d1))

d2 := make([]byte, 6)
n, _ = r1.ReadAt(d2, 0)
fmt.Println(n, string(d2))

w1 := &amp;amp;bytes.Buffer{}
_, _ = r1.Seek(0, 0)
_, _ = r1.WriteTo(w1)
fmt.Println(w1.String())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3 你
3 你
228
228
6 你好
6 你好
你好，世界
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;buffer-类型&#34;&gt;Buffer 类型&lt;/h2&gt;

&lt;p&gt;buffer类型也实现了缓存io，也是对[]byte进行读写操作，提供了多种实现化函数对象，个人感觉是对string，byte的reader的综合使用实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Buffer struct {
    buf      []byte
    off      int
    lastRead readOp
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上一个示例的最后，我们使用了 bytes.Buffer 类型，该类型实现了 io 包下的 ByteScanner, ByteWriter, ReadWriter, Reader, ReaderFrom, RuneReader, RuneScanner, StringWriter, Writer, WriterTo 等接口，可以方便的进行读写操作。&lt;/p&gt;

&lt;p&gt;对象可读取数据为 buf[off : len(buf)], off 表示进度下标，lastRead 表示最后读取的一个字符所占字节数，方便 Unread* 相关操作。&lt;/p&gt;

&lt;p&gt;Buffer 可以通过 3 中方法初始化对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := bytes.NewBufferString(&amp;quot;Hello World&amp;quot;)
b := bytes.NewBuffer([]byte(&amp;quot;Hello World&amp;quot;))
c := bytes.Buffer{}

fmt.Println(a)
fmt.Println(b)
fmt.Println(c)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Hello World
Hello World
{[] 0 0}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Buffer 包含了 21 个读写相关的方法，大部分同名方法的用法与前面讲的类似，这里只讲演示其中的 3 个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 读取到字节 delim 后，以字节数组的形式返回该字节及前面读取到的字节。如果遍历 b.buf 也找不到匹配的字节，则返回错误(一般是 EOF)
func (b *Buffer) ReadBytes(delim byte) (line []byte, err error)
// 读取到字节 delim 后，以字符串的形式返回该字节及前面读取到的字节。如果遍历 b.buf 也找不到匹配的字节，则返回错误(一般是 EOF)
func (b *Buffer) ReadString(delim byte) (line string, err error)
// 截断 b.buf , 舍弃 b.off+n 之后的数据。n == 0 时，调用 Reset 方法重置该对象，当 n 越界时（n &amp;lt; 0 || n &amp;gt; b.Len() ）方法会触发 panic.
func (b *Buffer) Truncate(n int)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := bytes.NewBufferString(&amp;quot;Good Night&amp;quot;)

x, err := a.ReadBytes(&#39;t&#39;)
if err != nil {
    fmt.Println(&amp;quot;delim:t err:&amp;quot;, err)
} else {
    fmt.Println(string(x))
}

a.Truncate(0)
a.WriteString(&amp;quot;Good Night&amp;quot;)
fmt.Println(a.Len())
a.Truncate(5)
fmt.Println(a.Len())
y, err := a.ReadString(&#39;N&#39;)
if err != nil {
    fmt.Println(&amp;quot;delim:N err:&amp;quot;, err)
} else {
    fmt.Println(y)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Good Night
10
5
delim:N err: EOF
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- zabbix源码阅读</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbixcode/</link>
          <pubDate>Sat, 25 Nov 2017 09:52:47 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbixcode/</guid>
          <description>&lt;p&gt;阅读源码，解析基本原理。&lt;/p&gt;

&lt;h1 id=&#34;流程&#34;&gt;流程&lt;/h1&gt;

&lt;p&gt;一个监控系统运行的大概的流程是这样的：&lt;/p&gt;

&lt;p&gt;agentd需要安装到被监控的主机上，它负责定期收集各项数据，并发送到zabbix server端，zabbix server将数据存储到数据库中，zabbix web根据数据在前端进行展现和绘图。这里agentd收集数据分为主动和被动两种模式：&lt;/p&gt;

&lt;p&gt;主动：agent请求server获取主动的监控项列表，并主动将监控项内需要检测的数据提交给server/proxy&lt;/p&gt;

&lt;p&gt;被动：server向agent请求获取监控项的数据，agent返回数据。&lt;/p&gt;

&lt;h1 id=&#34;主动监测&#34;&gt;主动监测&lt;/h1&gt;

&lt;p&gt;通信过程如下：&lt;/p&gt;

&lt;p&gt;zabbix首先向ServerActive配置的IP请求获取active items，获取并提交active tiems数据值server或者proxy。很多人会提出疑问：zabbix多久获取一次active items？它会根据配置文件中的RefreshActiveChecks的频率进行，如果获取失败，那么将会在60秒之后重试。分两个部分：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/zabbix/z1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1.获取ACTIVE ITEMS列表&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Agent打开TCP连接（主动检测变成Agent打开）&lt;/li&gt;
&lt;li&gt;Agent请求items检测列表&lt;/li&gt;
&lt;li&gt;Server返回items列表&lt;/li&gt;
&lt;li&gt;Agent 处理响应&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;li&gt;Agent开始收集数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2.主动检测提交数据过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Agent建立TCP连接&lt;/li&gt;
&lt;li&gt;Agent提交items列表收集的数据&lt;/li&gt;
&lt;li&gt;Server处理数据，并返回响应状态&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;被动监测&#34;&gt;被动监测&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/zabbix/z2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通信过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server打开一个TCP连接&lt;/li&gt;
&lt;li&gt;Server发送请求agent.ping\n&lt;/li&gt;
&lt;li&gt;Agent接收到请求并且响应&lt;code&gt;&amp;lt;HEADER&amp;gt;&amp;lt;DATALEN&amp;gt;1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Server处理接收到的数据1&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里，有人可以看出来，被动模式每次都需要打开一个tcp连接，这样当监控项越来越多时，就会出现server端性能问题了。&lt;/p&gt;

&lt;p&gt;比如not supported items通信过程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server打开一个TCP连接&lt;/li&gt;
&lt;li&gt;Server发送请求&lt;code&gt;vfs.fs.size[ no]\n&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Agent接收请求并且返回响应数据 &lt;code&gt;&amp;lt;HEADER&amp;gt;&amp;lt;DATALEN&amp;gt;ZBX_NOTSUPPORTED\0Cannot obtain filesystem information: [2] No such file or directory&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Server接收并处理数据, 将item的状态改为“ not supported ”&lt;/li&gt;
&lt;li&gt;关闭TCP连接&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;还有人会问，那实际监控中是用主动的还是被动的呢？这里主要涉及两个地方：&lt;/p&gt;

&lt;p&gt;1、新建监控项目时，选择的是zabbix代理还是zabbix端点代理程式（主动式），前者是被动模式，后者是主动模式。&lt;/p&gt;

&lt;p&gt;2、agentd配置文件中StartAgents参数的设置，如果为0，表示禁止被动模式，否则开启。一般建议不要设置为0，因为监控项目很多时，可以部分使用主动，部分使用被动模式。&lt;/p&gt;

&lt;h1 id=&#34;常用的监控架构平台&#34;&gt;常用的监控架构平台&lt;/h1&gt;

&lt;p&gt;1、server-agentd模式：&lt;/p&gt;

&lt;p&gt;这个是最简单的架构了，常用于监控主机比较少的情况下。&lt;/p&gt;

&lt;p&gt;2、server-proxy-agentd模式：&lt;/p&gt;

&lt;p&gt;这个常用于比较多的机器，使用proxy进行分布式监控，有效的减轻server端的压力。&lt;/p&gt;

&lt;h1 id=&#34;组件解析&#34;&gt;组件解析&lt;/h1&gt;

&lt;h2 id=&#34;agent&#34;&gt;Agent&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;入口函数：zabbix_agentd.c:MAIN_ZABBIX_ENTRY&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;采集线程：stats.c: ZBX_THREAD_ENTRY(collector_thread, args)，采集数据&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监听线程：listener.c: ZBX_THREAD_ENTRY(listener_thread, args)，监听端口（根据加密格式）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;发送线程：active.c:ZBX_THREAD_ENTRY(active_checks_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.发送报文函数: active.c:send_buffer，消息体为消息头+json格式的消息体，根据加密配置，分为不加密，cert加密和psk加密。Json的编码可以在这个函数里看。
2.加密可以使用openssl的库，主要实现在tls.c:zbx_tls_connect函数中。
3.消息头的编码：comms.c:zbx_tcp_send_ext，包括” ZBXD”+1字节flag+32位json消息长度+32位0x00，在发送json体的时候，使用了zlib的compress函数进行压缩，对端接收的时候使用uncompress进行了解压缩。
4.1字节flag有以下取值：

        {
        ZBX_TCP_PROTOCOL(0x01)
        ZBX_TCP_PROTOCOL |ZBX_TCP_COMPRESS (0x03)
        0x00
        }
        当flag&amp;amp; ZBX_TCP_COMPRESS!=0时，发送报文需要对消息体进行compress压缩，接收报文需要对消息体进行uncompress解压缩
        #define ZBX_TCP_PROTOCOL        0x01
        #define ZBX_TCP_COMPRESS        0x02
        当flag==0时，报文没有消息头，只有json消息体

5.消息长度

        发送报文时，如果加密，消息体最长16K
        #define ZBX_TLS_MAX_REC_LEN 16384
        如果不加密，没有限制，写json串时动态申请内存
        接收报文时，最大长度128M，根据接收的消息长度循环动态申请内存
        #define ZBX_MAX_RECV_DATA_SIZE  (128 * ZBX_MEBIBYTE)

6.json编码中request的类型

        #define ZBX_PROTO_VALUE_GET_ACTIVE_CHECKS   &amp;quot;active checks&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_CONFIG        &amp;quot;proxy config&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_HEARTBEAT     &amp;quot;proxy heartbeat&amp;quot;
        #define ZBX_PROTO_VALUE_SENDER_DATA     &amp;quot;sender data&amp;quot;
        #define ZBX_PROTO_VALUE_AGENT_DATA      &amp;quot;agent data&amp;quot;
        #define ZBX_PROTO_VALUE_COMMAND         &amp;quot;command&amp;quot;
        #define ZBX_PROTO_VALUE_JAVA_GATEWAY_INTERNAL   &amp;quot;java gateway internal&amp;quot;
        #define ZBX_PROTO_VALUE_JAVA_GATEWAY_JMX    &amp;quot;java gateway jmx&amp;quot;
        #define ZBX_PROTO_VALUE_GET_QUEUE       &amp;quot;queue.get&amp;quot;
        #define ZBX_PROTO_VALUE_GET_STATUS      &amp;quot;status.get&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_DATA      &amp;quot;proxy data&amp;quot;
        #define ZBX_PROTO_VALUE_PROXY_TASKS     &amp;quot;proxy tasks&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;proxy&#34;&gt;Proxy&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;入口函数：zabbix_proxy.c:MAIN_ZABBIX_ENTRY&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置同步线程：proxyconfig.c: ZBX_THREAD_ENTRY(proxyconfig_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.主要处理函数：proxyconfig.c: process_configuration_sync
2.发送request为#define ZBX_PROTO_VALUE_PROXY_CONFIG        &amp;quot;proxy config&amp;quot;的配置同步请求消息，消息头和消息体的格式和agent消息格式一样，见agent段落的第3节
3.接收对端的配置同步响应消息，并解析消息体中的json段
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;心跳线程：heartbeat.c:ZBX_THREAD_ENTRY(heart_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;发送request为#define ZBX_PROTO_VALUE_PROXY_HEARTBEAT       &amp;quot;proxy heartbeat&amp;quot;的心跳消息
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;发送线程：datasender.c: ZBX_THREAD_ENTRY(datasender_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.主要处理函数：datasender.c: proxy_data_sender
2.发送request为#define ZBX_PROTO_VALUE_PROXY_DATA      &amp;quot;proxy data&amp;quot;的消息，消息头和消息体的格式和agent消息格式一样，见agent段落的第3节
3.发送的消息体包括下面4个类型的数据，数据源主要从db中获取
    #define ZBX_DATASENDER_AVAILABILITY     0x0001
    #define ZBX_DATASENDER_HISTORY          0x0002
    #define ZBX_DATASENDER_DISCOVERY        0x0004
    #define ZBX_DATASENDER_AUTOREGISTRATION     0x0008
4.从数据库中获取remotetasks，zbx_tm_get_remote_tasks，根据获取的task组织json消息体zbx_tm_json_serialize_tasks
5.接收对端的响应消息，解析消息体中的json段，并更新db中的task数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;poller线程：poller.c: ZBX_THREAD_ENTRY(poller_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.poller.c: get_values，从队列中获取数据项串并解析substitute_simple_macros，根据接口类型(snmp,java等)获取数值get_values_snmp，get_values_java
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;trapper线程：trapper.c:ZBX_THREAD_ENTRY(trapper_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.解析各类响应消息并对应处理：trapper.c:process_trap
2.消息体格式分为json格式，ZBX_GET_ACTIVE_CHECKS开头格式，xml格式，host:key:value格式
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pinger线程：pinger.c:ZBX_THREAD_ENTRY(pinger_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.从snmp或者java接口中获取数据
2.Icmp.c:process_ping，写数据到zbx_get_thread_id()i.pinger文件中
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;housekeeper_thread线程：housekeeper.c:ZBX_THREAD_ENTRY(pinger_thread, args)
    1.连接数据库删除历史数据&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;discoverer线程：httppoller.c:ZBX_THREAD_ENTRY(httppoller_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.数据库操作，获取新主机
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dbsyncer线程：dbsyncer.c:ZBX_THREAD_ENTRY(dbsyncer_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.同步数据库和内存
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;snmptrapper线程：snmptrapper.c: ZBX_THREAD_ENTRY(snmptrapper_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.读取snmptrapper文件中的数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;selfmon线程：selfmon.c: ZBX_THREAD_ENTRY(selfmon_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.收集selfmon统计数据
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vmware线程：selfmon.c: ZBX_THREAD_ENTRY(vmware_thread, args)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.收集vmware统计数据，使用soap协议
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;sever-proxy的交互&#34;&gt;Sever: proxy的交互&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;入口函数Proxypoll.c:ZBX_THREAD_ENTRY(proxypoller_thread, args)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;主要处理函数process_proxy，发送报文send_data_to_proxy，接收报文recv_data_from_proxy，回proxy响应zbx_send_proxy_data_response，报文格式仍然为json格式，同agent的第3部分&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
      
    
      
        <item>
          <title>UML</title>
          <link>https://kingjcy.github.io/post/architecture/map/uml/</link>
          <pubDate>Wed, 08 Nov 2017 11:40:49 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/map/uml/</guid>
          <description>&lt;p&gt;UML（Unified Modeling Language）是一种统一建模语言，为面向对象开发系统的产品进行说明、可视化、和编制文档的一种标准语言。下面将对UML的九种图的基本概念进行介绍以及各个图的使用场景。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念　　&lt;/h1&gt;

&lt;p&gt;如下图所示，UML图分为用例视图、设计视图、进程视图、实现视图和拓扑视图，又可以静动分为静态视图和动态视图。&lt;/p&gt;

&lt;p&gt;静态图分为：用例图，类图，对象图，包图，构件图，部署图。&lt;/p&gt;

&lt;p&gt;动态图分为：状态图，活动图，协作图，序列图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;用例图-usecase-diagrams&#34;&gt;用例图（UseCase Diagrams）&lt;/h2&gt;

&lt;p&gt;用例图主要回答了两个问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、是谁用软件。
2、软件的功能。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从用户的角度描述了系统的功能，并指出各个功能的执行者，强调用户的使用者，系统为执行者完成哪些功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;类图-class-diagrams&#34;&gt;类图（Class Diagrams）&lt;/h2&gt;

&lt;p&gt;用户根据用例图抽象成类，描述类的内部结构和类与类之间的关系，是一种静态结构图。 在UML类图中，常见的有以下几种关系: 泛化（Generalization）, 实现（Realization），关联（Association)，聚合（Aggregation），组合(Composition)，依赖(Dependency)。&lt;/p&gt;

&lt;p&gt;各种关系的强弱顺序： 泛化 = 实现 &amp;gt; 组合 &amp;gt; 聚合 &amp;gt; 关联 &amp;gt; 依赖&lt;/p&gt;

&lt;p&gt;1.泛化&lt;/p&gt;

&lt;p&gt;泛化关系：是一种继承关系，表示一般与特殊的关系，它指定了子类如何继承父类的所有特征和行为。例如：老虎是动物的一种，即有老虎的特性也有动物的共性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.实现&lt;/p&gt;

&lt;p&gt;实现关系：是一种类与接口的关系，表示类是接口所有特征和行为的实现。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3.关联&lt;/p&gt;

&lt;p&gt;关联关系：是一种拥有的关系，它使一个类知道另一个类的属性和方法；如：老师与学生，丈夫与妻子关联可以是双向的，也可以是单向的。双向的关联可以有两个箭头或者没有箭头，单向的关联有一个箭头。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.共享聚合　&lt;/p&gt;

&lt;p&gt;聚合关系：是整体与部分的关系，且部分可以离开整体而单独存在。如车和轮胎是整体和部分的关系，轮胎离开车仍然可以存在。&lt;/p&gt;

&lt;p&gt;聚合关系是关联关系的一种，是强的关联关系；关联和聚合在语法上无法区分，必须考察具体的逻辑关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;5.组合集合&lt;/p&gt;

&lt;p&gt;组合关系：是整体与部分的关系，但部分不能离开整体而单独存在。如公司和部门是整体和部分的关系，没有公司就不存在部门。&lt;/p&gt;

&lt;p&gt;组合关系是关联关系的一种，是比聚合关系还要强的关系，它要求普通的聚合关系中代表整体的对象负责代表部分的对象的生命周期。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml6.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;6.依赖　　&lt;/p&gt;

&lt;p&gt;依赖关系：是一种使用的关系，即一个类的实现需要另一个类的协助，所以要尽量不使用双向的互相依赖.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;7 各种类图关系&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml8.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;对象图-object-diagrams&#34;&gt;对象图（Object Diagrams）&lt;/h2&gt;

&lt;p&gt;描述的是参与交互的各个对象在交互过程中某一时刻的状态。对象图可以被看作是类图在某一时刻的实例。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml9.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;状态图-statechart-diagrams&#34;&gt;状态图（Statechart Diagrams）&lt;/h2&gt;

&lt;p&gt;一种由状态、变迁、事件和活动组成的状态机，用来描述类的对象所有可能的状态以及时间发生时状态的转移条件。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml10.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;活动图-activity-diagrams&#34;&gt;活动图（Activity Diagrams）：&lt;/h2&gt;

&lt;p&gt;状态图的一种特殊情况，这些状态大都处于活动状态。本质是一种流程图，它描述了活动到活动的控制流。　　　　&lt;/p&gt;

&lt;p&gt;交互图强调的是对象到对象的控制流，而活动图则强调的是从活动到活动的控制流。&lt;/p&gt;

&lt;p&gt;活动图是一种表述过程基理、业务过程以及工作流的技术。它可以用来对业务过程、工作流建模，也可以对用例实现甚至是程序实现来建模。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml11.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1.带泳道的活动图&lt;/p&gt;

&lt;p&gt;泳道表明每个活动是由哪些人或哪些部门负责完成。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml12.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.带对象流的活动图&lt;/p&gt;

&lt;p&gt;用活动图描述某个对象时，可以把涉及到的对象放置在活动图中，并用一个依赖将其连接到进行创建、修改和撤销的动作状态或者活动状态上，对象的这种使用方法就构成了对象流。对象流用带有箭头的虚线表示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml13.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;序列图-时序图-sequence-diagrams&#34;&gt;序列图-时序图（Sequence Diagrams）&lt;/h2&gt;

&lt;p&gt;交互图的一种，描述了对象之间消息发送的先后顺序，强调时间顺序。&lt;/p&gt;

&lt;p&gt;序列图的主要用途是把用例表达的需求，转化为进一步、更加正式层次的精细表达。用例常常被细化为一个或者更多的序列图。同时序列图更有效地描述如何分配各个类的职责以及各类具有相应职责的原因。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml14.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;消息用从一个对象的生命线到另一个对象生命线的箭头表示。箭头以时间顺序在图中从上到下排列。&lt;/p&gt;

&lt;p&gt;序列图中涉及的元素：&lt;/p&gt;

&lt;p&gt;1.角色&lt;/p&gt;

&lt;p&gt;系统角色，可以是人、及其甚至其他的系统或者子系统&lt;/p&gt;

&lt;p&gt;2.对象&lt;/p&gt;

&lt;p&gt;对象包括三种命名方式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  第一种方式包括对象名和类名；

  第二中方式只显示类名不显示对象名，即表示他是一个匿名对象；

  第三种方式只显示对象名不显示类。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.生命线&lt;/p&gt;

&lt;p&gt;生命线在顺序图中表示为从对象图标向下延伸的一条虚线，表示对象存在的时间。&lt;/p&gt;

&lt;p&gt;生命线名称可带下划线。当使用下划线时，意味着序列图中的生命线代表一个类的特定实例。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml15.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.控制焦点&lt;/p&gt;

&lt;p&gt;控制焦点是顺序图中表示时间段的符号，在这个时间段内对象将执行相应的操作。用小矩形表示&lt;/p&gt;

&lt;p&gt;5.同步消息&lt;/p&gt;

&lt;p&gt;同步等待消息&lt;/p&gt;

&lt;p&gt;消息的发送者把控制传递给消息的接收者，然后停止活动，等待消息的接收者放弃或者返回控制。用来表示同步的意义。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml16.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;6.异步消息&lt;/p&gt;

&lt;p&gt;异步发送消息，不需等待&lt;/p&gt;

&lt;p&gt;消息发送者通过消息把信号传递给消息的接收者，然后继续自己的活动，不等待接受者返回消息或者控制。异步消息的接收者和发送者是并发工作的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml17.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;7.注释&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml18.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;8.约束&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml19.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;9.组合　　&lt;/p&gt;

&lt;p&gt;组合片段用来解决交互执行的条件及方式。它允许在序列图中直接表示逻辑组件，用于通过指定条件或子进程的应用区域，为任何生命线的任何部分定义特殊条件和子进程。常用的组合片段有：抉择、选项、循环、并行。&lt;/p&gt;

&lt;h2 id=&#34;协作图-collaboration-diagrams&#34;&gt;协作图（Collaboration Diagrams）&lt;/h2&gt;

&lt;p&gt;交互图的一种，描述了收发消息的对象的组织关系，强调对象之间的合作关系。时序图按照时间顺序布图，而写作图按照空间结构布图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml20.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;构件图-component-diagrams&#34;&gt;构件图（Component Diagrams）：&lt;/h2&gt;

&lt;p&gt;构件图是用来表示系统中构件与构件之间，类或接口与构件之间的关系图。其中，构建图之间的关系表现为依赖关系，定义的类或接口与类之间的关系表现为依赖关系或实现关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml21.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;部署图-deployment-diagrams&#34;&gt;部署图（Deployment Diagrams）：&lt;/h2&gt;

&lt;p&gt;描述了系统运行时进行处理的结点以及在结点上活动的构件的配置。强调了物理设备以及之间的连接关系。&lt;/p&gt;

&lt;p&gt;部署模型的目的：&lt;/p&gt;

&lt;p&gt;描述一个具体应用的主要部署结构，通过对各种硬件，在硬件中的软件以及各种连接协议的显示，可以很好的描述系统是如何部署的；平衡系统运行时的计算资源分布；可以通过连接描述组织的硬件网络结构或者是嵌入式系统等具有多种硬件和软件相关的系统运行模型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/architecture/map/uml/uml22.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Net/Http 应用层</title>
          <link>https://kingjcy.github.io/post/golang/go-net-http/</link>
          <pubDate>Tue, 26 Sep 2017 17:05:22 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-net-http/</guid>
          <description>&lt;p&gt;http包提供了HTTP协议的客户端和服务端的实现。&lt;/p&gt;

&lt;h1 id=&#34;http客户端&#34;&gt;HTTP客户端&lt;/h1&gt;

&lt;h2 id=&#34;直接使用http方法&#34;&gt;直接使用http方法&lt;/h2&gt;

&lt;p&gt;直接使用http方法，其实就是使用标准库默认的结构体client，transport等来实现请求。&lt;/p&gt;

&lt;p&gt;http包中封装了Get、Head、Post和PostForm函数可以直接发出HTTP/ HTTPS请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resp, err := http.Get(&amp;quot;http://example.com/&amp;quot;)
...
resp, err := http.Post(&amp;quot;http://example.com/upload&amp;quot;, &amp;quot;image/jpeg&amp;quot;, &amp;amp;buf)
...
resp, err := http.PostForm(&amp;quot;http://example.com/form&amp;quot;,
    url.Values{&amp;quot;key&amp;quot;: {&amp;quot;Value&amp;quot;}, &amp;quot;id&amp;quot;: {&amp;quot;123&amp;quot;}})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序在使用完回复后必须关闭回复的主体。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resp, err := http.Get(&amp;quot;http://example.com/&amp;quot;)
if err != nil {
    // handle error
}
defer resp.Body.Close()
body, err := ioutil.ReadAll(resp.Body)
// ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;原理解析&#34;&gt;原理解析&lt;/h3&gt;

&lt;p&gt;http直接提供的Post等方法实现在client.go文件中，以Post为例，其他都是一样&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Post(url, contentType string, body io.Reader) (resp *Response, err error) {
    return DefaultClient.Post(url, contentType, body)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际是调用了默认结构体client的Post方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var DefaultClient = &amp;amp;Client{}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再来看Post方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) Post(url, contentType string, body io.Reader) (resp *Response, err error) {
    req, err := NewRequest(&amp;quot;POST&amp;quot;, url, body)
    if err != nil {
        return nil, err
    }
    req.Header.Set(&amp;quot;Content-Type&amp;quot;, contentType)
    return c.Do(req)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据url和请求体body新建一个reqest，然后调用DefaultClient的Do方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) Do(req *Request) (*Response, error) {
    return c.do(req)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用内部的do方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) do(req *Request) (retres *Response, reterr error) {
    if testHookClientDoResult != nil {
        defer func() { testHookClientDoResult(retres, reterr) }()
    }
    if req.URL == nil {
        req.closeBody()
        return nil, &amp;amp;url.Error{
            Op:  urlErrorOp(req.Method),
            Err: errors.New(&amp;quot;http: nil Request.URL&amp;quot;),
        }
    }

    var (
        deadline      = c.deadline()
        reqs          []*Request
        resp          *Response
        copyHeaders   = c.makeHeadersCopier(req)
        reqBodyClosed = false // have we closed the current req.Body?

        // Redirect behavior:
        redirectMethod string
        includeBody    bool
    )
    uerr := func(err error) error {
        // the body may have been closed already by c.send()
        if !reqBodyClosed {
            req.closeBody()
        }
        var urlStr string
        if resp != nil &amp;amp;&amp;amp; resp.Request != nil {
            urlStr = stripPassword(resp.Request.URL)
        } else {
            urlStr = stripPassword(req.URL)
        }
        return &amp;amp;url.Error{
            Op:  urlErrorOp(reqs[0].Method),
            URL: urlStr,
            Err: err,
        }
    }
    for {
        // For all but the first request, create the next
        // request hop and replace req.
        if len(reqs) &amp;gt; 0 {
            loc := resp.Header.Get(&amp;quot;Location&amp;quot;)
            if loc == &amp;quot;&amp;quot; {
                resp.closeBody()
                return nil, uerr(fmt.Errorf(&amp;quot;%d response missing Location header&amp;quot;, resp.StatusCode))
            }
            u, err := req.URL.Parse(loc)
            if err != nil {
                resp.closeBody()
                return nil, uerr(fmt.Errorf(&amp;quot;failed to parse Location header %q: %v&amp;quot;, loc, err))
            }
            host := &amp;quot;&amp;quot;
            if req.Host != &amp;quot;&amp;quot; &amp;amp;&amp;amp; req.Host != req.URL.Host {
                // If the caller specified a custom Host header and the
                // redirect location is relative, preserve the Host header
                // through the redirect. See issue #22233.
                if u, _ := url.Parse(loc); u != nil &amp;amp;&amp;amp; !u.IsAbs() {
                    host = req.Host
                }
            }
            ireq := reqs[0]
            req = &amp;amp;Request{
                Method:   redirectMethod,
                Response: resp,
                URL:      u,
                Header:   make(Header),
                Host:     host,
                Cancel:   ireq.Cancel,
                ctx:      ireq.ctx,
            }
            if includeBody &amp;amp;&amp;amp; ireq.GetBody != nil {
                req.Body, err = ireq.GetBody()
                if err != nil {
                    resp.closeBody()
                    return nil, uerr(err)
                }
                req.ContentLength = ireq.ContentLength
            }

            // Copy original headers before setting the Referer,
            // in case the user set Referer on their first request.
            // If they really want to override, they can do it in
            // their CheckRedirect func.
            copyHeaders(req)

            // Add the Referer header from the most recent
            // request URL to the new one, if it&#39;s not https-&amp;gt;http:
            if ref := refererForURL(reqs[len(reqs)-1].URL, req.URL); ref != &amp;quot;&amp;quot; {
                req.Header.Set(&amp;quot;Referer&amp;quot;, ref)
            }
            err = c.checkRedirect(req, reqs)

            // Sentinel error to let users select the
            // previous response, without closing its
            // body. See Issue 10069.
            if err == ErrUseLastResponse {
                return resp, nil
            }

            // Close the previous response&#39;s body. But
            // read at least some of the body so if it&#39;s
            // small the underlying TCP connection will be
            // re-used. No need to check for errors: if it
            // fails, the Transport won&#39;t reuse it anyway.
            const maxBodySlurpSize = 2 &amp;lt;&amp;lt; 10
            if resp.ContentLength == -1 || resp.ContentLength &amp;lt;= maxBodySlurpSize {
                io.CopyN(ioutil.Discard, resp.Body, maxBodySlurpSize)
            }
            resp.Body.Close()

            if err != nil {
                // Special case for Go 1 compatibility: return both the response
                // and an error if the CheckRedirect function failed.
                // See https://golang.org/issue/3795
                // The resp.Body has already been closed.
                ue := uerr(err)
                ue.(*url.Error).URL = loc
                return resp, ue
            }
        }

        reqs = append(reqs, req)
        var err error
        var didTimeout func() bool
        //调用 send
        if resp, didTimeout, err = c.send(req, deadline); err != nil {
            // c.send() always closes req.Body
            reqBodyClosed = true
            if !deadline.IsZero() &amp;amp;&amp;amp; didTimeout() {
                err = &amp;amp;httpError{
                    // TODO: early in cycle: s/Client.Timeout exceeded/timeout or context cancelation/
                    err:     err.Error() + &amp;quot; (Client.Timeout exceeded while awaiting headers)&amp;quot;,
                    timeout: true,
                }
            }
            return nil, uerr(err)
        }

        var shouldRedirect bool
        redirectMethod, shouldRedirect, includeBody = redirectBehavior(req.Method, resp, reqs[0])
        if !shouldRedirect {
            return resp, nil
        }

        req.closeBody()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用send方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) send(req *Request, deadline time.Time) (resp *Response, didTimeout func() bool, err error) {
    if c.Jar != nil {
        for _, cookie := range c.Jar.Cookies(req.URL) {
            req.AddCookie(cookie)
        }
    }
    resp, didTimeout, err = send(req, c.transport(), deadline)
    if err != nil {
        return nil, didTimeout, err
    }
    if c.Jar != nil {
        if rc := resp.Cookies(); len(rc) &amp;gt; 0 {
            c.Jar.SetCookies(req.URL, rc)
        }
    }
    return resp, nil, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边需要确定实现transport的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Client) transport() RoundTripper {
    if c.Transport != nil {
        return c.Transport
    }
    return DefaultTransport
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用默认的DefaultTransport（如果transport自定义了，就使用自定义的，否则使用默认的），这边这个接口调用就是DefaultTransport，也就是Transport.go中的Transport结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var DefaultTransport RoundTripper = &amp;amp;Transport{
    Proxy: ProxyFromEnvironment,
    DialContext: (&amp;amp;net.Dialer{
        Timeout:   30 * time.Second,
        KeepAlive: 30 * time.Second,
        DualStack: true,
    }).DialContext,
    MaxIdleConns:          100,
    IdleConnTimeout:       90 * time.Second,
    TLSHandshakeTimeout:   10 * time.Second,
    ExpectContinueTimeout: 1 * time.Second,
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看Transport结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Transport struct {
    idleMu     sync.Mutex
    wantIdle   bool                                // user has requested to close all idle conns
    idleConn   map[connectMethodKey][]*persistConn // most recently used at end
    idleConnCh map[connectMethodKey]chan *persistConn
    idleLRU    connLRU

    reqMu       sync.Mutex
    reqCanceler map[*Request]func(error)

    altMu    sync.Mutex   // guards changing altProto only
    altProto atomic.Value // of nil or map[string]RoundTripper, key is URI scheme

    connCountMu          sync.Mutex
    connPerHostCount     map[connectMethodKey]int
    connPerHostAvailable map[connectMethodKey]chan struct{}

    // Proxy specifies a function to return a proxy for a given
    // Request. If the function returns a non-nil error, the
    // request is aborted with the provided error.
    //
    // The proxy type is determined by the URL scheme. &amp;quot;http&amp;quot;,
    // &amp;quot;https&amp;quot;, and &amp;quot;socks5&amp;quot; are supported. If the scheme is empty,
    // &amp;quot;http&amp;quot; is assumed.
    //
    // If Proxy is nil or returns a nil *URL, no proxy is used.
    Proxy func(*Request) (*url.URL, error)

    // DialContext specifies the dial function for creating unencrypted TCP connections.
    // If DialContext is nil (and the deprecated Dial below is also nil),
    // then the transport dials using package net.
    //
    // DialContext runs concurrently with calls to RoundTrip.
    // A RoundTrip call that initiates a dial may end up using
    // a connection dialed previously when the earlier connection
    // becomes idle before the later DialContext completes.
    DialContext func(ctx context.Context, network, addr string) (net.Conn, error)

    // Dial specifies the dial function for creating unencrypted TCP connections.
    //
    // Dial runs concurrently with calls to RoundTrip.
    // A RoundTrip call that initiates a dial may end up using
    // a connection dialed previously when the earlier connection
    // becomes idle before the later Dial completes.
    //
    // Deprecated: Use DialContext instead, which allows the transport
    // to cancel dials as soon as they are no longer needed.
    // If both are set, DialContext takes priority.
    Dial func(network, addr string) (net.Conn, error)

    // DialTLS specifies an optional dial function for creating
    // TLS connections for non-proxied HTTPS requests.
    //
    // If DialTLS is nil, Dial and TLSClientConfig are used.
    //
    // If DialTLS is set, the Dial hook is not used for HTTPS
    // requests and the TLSClientConfig and TLSHandshakeTimeout
    // are ignored. The returned net.Conn is assumed to already be
    // past the TLS handshake.
    DialTLS func(network, addr string) (net.Conn, error)

    // TLSClientConfig specifies the TLS configuration to use with
    // tls.Client.
    // If nil, the default configuration is used.
    // If non-nil, HTTP/2 support may not be enabled by default.
    TLSClientConfig *tls.Config

    // TLSHandshakeTimeout specifies the maximum amount of time waiting to
    // wait for a TLS handshake. Zero means no timeout.
    TLSHandshakeTimeout time.Duration

    // DisableKeepAlives, if true, disables HTTP keep-alives and
    // will only use the connection to the server for a single
    // HTTP request.
    //
    // This is unrelated to the similarly named TCP keep-alives.
    DisableKeepAlives bool

    // DisableCompression, if true, prevents the Transport from
    // requesting compression with an &amp;quot;Accept-Encoding: gzip&amp;quot;
    // request header when the Request contains no existing
    // Accept-Encoding value. If the Transport requests gzip on
    // its own and gets a gzipped response, it&#39;s transparently
    // decoded in the Response.Body. However, if the user
    // explicitly requested gzip it is not automatically
    // uncompressed.
    DisableCompression bool

    // MaxIdleConns controls the maximum number of idle (keep-alive)
    // connections across all hosts. Zero means no limit.
    MaxIdleConns int

    // MaxIdleConnsPerHost, if non-zero, controls the maximum idle
    // (keep-alive) connections to keep per-host. If zero,
    // DefaultMaxIdleConnsPerHost is used.
    MaxIdleConnsPerHost int

    // MaxConnsPerHost optionally limits the total number of
    // connections per host, including connections in the dialing,
    // active, and idle states. On limit violation, dials will block.
    //
    // Zero means no limit.
    //
    // For HTTP/2, this currently only controls the number of new
    // connections being created at a time, instead of the total
    // number. In practice, hosts using HTTP/2 only have about one
    // idle connection, though.
    MaxConnsPerHost int

    // IdleConnTimeout is the maximum amount of time an idle
    // (keep-alive) connection will remain idle before closing
    // itself.
    // Zero means no limit.
    IdleConnTimeout time.Duration

    // ResponseHeaderTimeout, if non-zero, specifies the amount of
    // time to wait for a server&#39;s response headers after fully
    // writing the request (including its body, if any). This
    // time does not include the time to read the response body.
    ResponseHeaderTimeout time.Duration

    // ExpectContinueTimeout, if non-zero, specifies the amount of
    // time to wait for a server&#39;s first response headers after fully
    // writing the request headers if the request has an
    // &amp;quot;Expect: 100-continue&amp;quot; header. Zero means no timeout and
    // causes the body to be sent immediately, without
    // waiting for the server to approve.
    // This time does not include the time to send the request header.
    ExpectContinueTimeout time.Duration

    // TLSNextProto specifies how the Transport switches to an
    // alternate protocol (such as HTTP/2) after a TLS NPN/ALPN
    // protocol negotiation. If Transport dials an TLS connection
    // with a non-empty protocol name and TLSNextProto contains a
    // map entry for that key (such as &amp;quot;h2&amp;quot;), then the func is
    // called with the request&#39;s authority (such as &amp;quot;example.com&amp;quot;
    // or &amp;quot;example.com:1234&amp;quot;) and the TLS connection. The function
    // must return a RoundTripper that then handles the request.
    // If TLSNextProto is not nil, HTTP/2 support is not enabled
    // automatically.
    TLSNextProto map[string]func(authority string, c *tls.Conn) RoundTripper

    // ProxyConnectHeader optionally specifies headers to send to
    // proxies during CONNECT requests.
    ProxyConnectHeader Header

    // MaxResponseHeaderBytes specifies a limit on how many
    // response bytes are allowed in the server&#39;s response
    // header.
    //
    // Zero means to use a default limit.
    MaxResponseHeaderBytes int64

    // nextProtoOnce guards initialization of TLSNextProto and
    // h2transport (via onceSetNextProtoDefaults)
    nextProtoOnce sync.Once
    h2transport   h2Transport // non-nil if http2 wired up
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;中文讲解&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Transport struct {
    // Proxy指定一个对给定请求返回代理的函数。
    // 如果该函数返回了非nil的错误值，请求的执行就会中断并返回该错误。
    // 如果Proxy为nil或返回nil的*URL置，将不使用代理。
    Proxy func(*Request) (*url.URL, error)

    // Dial指定创建TCP连接的拨号函数。如果Dial为nil，会使用net.Dial。
    //Dial获取一个tcp 连接，也就是net.Conn结构，你就记住可以往里面写request
    //然后从里面搞到response就行了
    Dial func(network, addr string) (net.Conn, error)

    // TLSClientConfig指定用于tls.Client的TLS配置信息。
    // 如果该字段为nil，会使用默认的配置信息。
    TLSClientConfig *tls.Config

    // TLSHandshakeTimeout指定等待TLS握手完成的最长时间。零值表示不设置超时。
    TLSHandshakeTimeout time.Duration

    // 如果DisableKeepAlives为真，会禁止不同HTTP请求之间TCP连接的重用。
    DisableKeepAlives bool

    // 如果DisableCompression为真，会禁止Transport在请求中没有Accept-Encoding头时，
    // 主动添加&amp;quot;Accept-Encoding: gzip&amp;quot;头，以获取压缩数据。
    // 如果Transport自己请求gzip并得到了压缩后的回复，它会主动解压缩回复的主体。
    // 但如果用户显式的请求gzip压缩数据，Transport是不会主动解压缩的。
    DisableCompression bool

    // 如果MaxIdleConnsPerHost!=0，会控制每个主机下的最大闲置连接。
    // 如果MaxIdleConnsPerHost==0，会使用DefaultMaxIdleConnsPerHost。
    MaxIdleConnsPerHost int

    // ResponseHeaderTimeout指定在发送完请求（包括其可能的主体）之后，
    // 等待接收服务端的回复的头域的最大时间。零值表示不设置超时。
    // 该时间不包括获取回复主体的时间。
    ResponseHeaderTimeout time.Duration

    // 内含隐藏或非导出字段



    //保存从 connectMethodKey （代表着不同的协议 不同的host，也就是不同的请求）到 persistConn 的映射
    idleConn   map[connectMethodKey][]*persistConn // most recently used at end
    //用来在并发http请求的时候在多个 goroutine 里面相互发送持久连接，也就是说， 这些持久连接是可以重复利用的， 你的http请求用某个persistConn用完了，通过这个channel发送给其他http请求使用这个persistConn，然后我们找到transport的RoundTrip方法
    idleConnCh map[connectMethodKey]chan *persistConn
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用内部send方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func send(ireq *Request, rt RoundTripper, deadline time.Time) (resp *Response, didTimeout func() bool, err error) {
    req := ireq // req is either the original request, or a modified fork

    if rt == nil {
        req.closeBody()
        return nil, alwaysFalse, errors.New(&amp;quot;http: no Client.Transport or DefaultTransport&amp;quot;)
    }

    if req.URL == nil {
        req.closeBody()
        return nil, alwaysFalse, errors.New(&amp;quot;http: nil Request.URL&amp;quot;)
    }

    if req.RequestURI != &amp;quot;&amp;quot; {
        req.closeBody()
        return nil, alwaysFalse, errors.New(&amp;quot;http: Request.RequestURI can&#39;t be set in client requests.&amp;quot;)
    }

    // forkReq forks req into a shallow clone of ireq the first
    // time it&#39;s called.
    forkReq := func() {
        if ireq == req {
            req = new(Request)
            *req = *ireq // shallow clone
        }
    }

    // Most the callers of send (Get, Post, et al) don&#39;t need
    // Headers, leaving it uninitialized. We guarantee to the
    // Transport that this has been initialized, though.
    if req.Header == nil {
        forkReq()
        req.Header = make(Header)
    }

    if u := req.URL.User; u != nil &amp;amp;&amp;amp; req.Header.Get(&amp;quot;Authorization&amp;quot;) == &amp;quot;&amp;quot; {
        username := u.Username()
        password, _ := u.Password()
        forkReq()
        req.Header = ireq.Header.clone()
        req.Header.Set(&amp;quot;Authorization&amp;quot;, &amp;quot;Basic &amp;quot;+basicAuth(username, password))
    }

    if !deadline.IsZero() {
        forkReq()
    }
    stopTimer, didTimeout := setRequestCancel(req, rt, deadline)

    resp, err = rt.RoundTrip(req)
    if err != nil {
        stopTimer()
        if resp != nil {
            log.Printf(&amp;quot;RoundTripper returned a response &amp;amp; error; ignoring response&amp;quot;)
        }
        if tlsErr, ok := err.(tls.RecordHeaderError); ok {
            // If we get a bad TLS record header, check to see if the
            // response looks like HTTP and give a more helpful error.
            // See golang.org/issue/11111.
            if string(tlsErr.RecordHeader[:]) == &amp;quot;HTTP/&amp;quot; {
                err = errors.New(&amp;quot;http: server gave HTTP response to HTTPS client&amp;quot;)
            }
        }
        return nil, didTimeout, err
    }
    if !deadline.IsZero() {
        resp.Body = &amp;amp;cancelTimerBody{
            stop:          stopTimer,
            rc:            resp.Body,
            reqDidTimeout: didTimeout,
        }
    }
    return resp, nil, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用DefaultTransport也就是Transport.go中的Transport结构体的RoundTrip方法（当出现自定义的时候，就调用对应的Transport的RoundTrip方法，这边直接使用这个借口就是DefaultTransport），可见使用golang net/http库发送http请求，最后都是调用 http transport的 RoundTrip方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// roundTrip implements a RoundTripper over HTTP.
func (t *Transport) roundTrip(req *Request) (*Response, error) {
    t.nextProtoOnce.Do(t.onceSetNextProtoDefaults)
    ctx := req.Context()
    trace := httptrace.ContextClientTrace(ctx)

    if req.URL == nil {
        req.closeBody()
        return nil, errors.New(&amp;quot;http: nil Request.URL&amp;quot;)
    }
    if req.Header == nil {
        req.closeBody()
        return nil, errors.New(&amp;quot;http: nil Request.Header&amp;quot;)
    }
    scheme := req.URL.Scheme
    isHTTP := scheme == &amp;quot;http&amp;quot; || scheme == &amp;quot;https&amp;quot;
    if isHTTP {
        for k, vv := range req.Header {
            if !httpguts.ValidHeaderFieldName(k) {
                return nil, fmt.Errorf(&amp;quot;net/http: invalid header field name %q&amp;quot;, k)
            }
            for _, v := range vv {
                if !httpguts.ValidHeaderFieldValue(v) {
                    return nil, fmt.Errorf(&amp;quot;net/http: invalid header field value %q for key %v&amp;quot;, v, k)
                }
            }
        }
    }

    if t.useRegisteredProtocol(req) {
        altProto, _ := t.altProto.Load().(map[string]RoundTripper)
        if altRT := altProto[scheme]; altRT != nil {
            if resp, err := altRT.RoundTrip(req); err != ErrSkipAltProtocol {
                return resp, err
            }
        }
    }
    if !isHTTP {
        req.closeBody()
        return nil, &amp;amp;badStringError{&amp;quot;unsupported protocol scheme&amp;quot;, scheme}
    }
    if req.Method != &amp;quot;&amp;quot; &amp;amp;&amp;amp; !validMethod(req.Method) {
        return nil, fmt.Errorf(&amp;quot;net/http: invalid method %q&amp;quot;, req.Method)
    }
    if req.URL.Host == &amp;quot;&amp;quot; {
        req.closeBody()
        return nil, errors.New(&amp;quot;http: no Host in request URL&amp;quot;)
    }

    for {
        select {
        case &amp;lt;-ctx.Done():
            req.closeBody()
            return nil, ctx.Err()
        default:
        }

        // treq gets modified by roundTrip, so we need to recreate for each retry.
        treq := &amp;amp;transportRequest{Request: req, trace: trace}
        cm, err := t.connectMethodForRequest(treq)
        if err != nil {
            req.closeBody()
            return nil, err
        }

        // Get the cached or newly-created connection to either the
        // host (for http or https), the http proxy, or the http proxy
        // pre-CONNECTed to https server. In any case, we&#39;ll be ready
        // to send it requests.
        pconn, err := t.getConn(treq, cm)
        if err != nil {
            t.setReqCanceler(req, nil)
            req.closeBody()
            return nil, err
        }

        var resp *Response
        if pconn.alt != nil {
            // HTTP/2 path.
            t.decHostConnCount(cm.key()) // don&#39;t count cached http2 conns toward conns per host
            t.setReqCanceler(req, nil)   // not cancelable with CancelRequest
            resp, err = pconn.alt.RoundTrip(req)
        } else {
            resp, err = pconn.roundTrip(treq)
        }
        if err == nil {
            return resp, nil
        }
        if !pconn.shouldRetryRequest(req, err) {
            // Issue 16465: return underlying net.Conn.Read error from peek,
            // as we&#39;ve historically done.
            if e, ok := err.(transportReadFromServerError); ok {
                err = e.err
            }
            return nil, err
        }
        testHookRoundTripRetried()

        // Rewind the body if we&#39;re able to.
        if req.GetBody != nil {
            newReq := *req
            var err error
            newReq.Body, err = req.GetBody()
            if err != nil {
                return nil, err
            }
            req = &amp;amp;newReq
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前面对输入的错误处理部分我们忽略， 其实就2步，先获取一个TCP长连接，所谓TCP长连接就是三次握手建立连接后不close而是一直保持重复使用（节约环保） 然后调用这个持久连接persistConn 这个struct的roundTrip方法。我们先看获取连接&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (t *Transport) getConn(req *Request, cm connectMethod) (*persistConn, error) {
    if pc := t.getIdleConn(cm); pc != nil {
        // set request canceler to some non-nil function so we
        // can detect whether it was cleared between now and when
        // we enter roundTrip
        t.setReqCanceler(req, func() {})
        return pc, nil
    }

    type dialRes struct {
        pc  *persistConn
        err error
    }
    dialc := make(chan dialRes)
    //定义了一个发送 persistConn的channel

    prePendingDial := prePendingDial
    postPendingDial := postPendingDial

    handlePendingDial := func() {
        if prePendingDial != nil {
            prePendingDial()
        }
        go func() {
            if v := &amp;lt;-dialc; v.err == nil {
                t.putIdleConn(v.pc)
            }
            if postPendingDial != nil {
                postPendingDial()
            }
        }()
    }

    cancelc := make(chan struct{})
    t.setReqCanceler(req, func() { close(cancelc) })

    // 启动了一个goroutine, 这个goroutine 获取里面调用dialConn搞到
    // persistConn, 然后发送到上面建立的channel  dialc里面，
    go func() {
        pc, err := t.dialConn(cm)
        dialc &amp;lt;- dialRes{pc, err}
    }()

    idleConnCh := t.getIdleConnCh(cm)
    select {
    case v := &amp;lt;-dialc:
        // dialc 我们的 dial 方法先搞到通过 dialc通道发过来了
        return v.pc, v.err
    case pc := &amp;lt;-idleConnCh:
        // 这里代表其他的http请求用完了归还的persistConn通过idleConnCh这个
        // channel发送来的
        handlePendingDial()
        return pc, nil
    case &amp;lt;-req.Cancel:
        handlePendingDial()
        return nil, errors.New(&amp;quot;net/http: request canceled while waiting for connection&amp;quot;)
    case &amp;lt;-cancelc:
        handlePendingDial()
        return nil, errors.New(&amp;quot;net/http: request canceled while waiting for connection&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里面的代码写的很有讲究 , 上面代码里面我也注释了， 定义了一个发送 persistConn的channel dialc， 启动了一个goroutine, 这个goroutine 获取里面调用dialConn搞到persistConn, 然后发送到dialc里面，主协程goroutine在 select里面监听多个channel,看看哪个通道里面先发过来 persistConn，就用哪个，然后return。&lt;/p&gt;

&lt;p&gt;这里要注意的是 idleConnCh 这个通道里面发送来的是其他的http请求用完了归还的persistConn， 如果从这个通道里面搞到了，dialc这个通道也等着发呢，不能浪费，就通过handlePendingDial这个方法把dialc通道里面的persistConn也发到idleConnCh，等待后续给其他http请求使用。&lt;/p&gt;

&lt;p&gt;每个新建的persistConn的时候都把tcp连接里地输入流，和输出流用br（br *bufio.Reader）,和bw(bw *bufio.Writer)包装了一下，往bw写就写到tcp输入流里面了，读输出流也是通过br读，并启动了读循环和写循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pconn.br = bufio.NewReader(noteEOFReader{pconn.conn, &amp;amp;pconn.sawEOF})
pconn.bw = bufio.NewWriter(pconn.conn)
go pconn.readLoop()
go pconn.writeLoop()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再看pconn.roundTrip 调用这个持久连接persistConn 这个struct的roundTrip方法。先瞄一下 persistConn 这个struct&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type persistConn struct {
    t        *Transport
    cacheKey connectMethodKey
    conn     net.Conn
    tlsState *tls.ConnectionState
    br       *bufio.Reader       // 从tcp输出流里面读
    sawEOF   bool                // whether we&#39;ve seen EOF from conn; owned by readLoop
    bw       *bufio.Writer       // 写到tcp输入流
     reqch    chan requestAndChan // 主goroutine 往channnel里面写，读循环从     
                                 // channnel里面接受
    writech  chan writeRequest   // 主goroutine 往channnel里面写                                      
                                 // 写循环从channel里面接受
    closech  chan struct{}       // 通知关闭tcp连接的channel 

    writeErrCh chan error

    lk                   sync.Mutex // guards following fields
    numExpectedResponses int
    closed               bool // whether conn has been closed
    broken               bool // an error has happened on this connection; marked broken so it&#39;s not reused.
    canceled             bool // whether this conn was broken due a CancelRequest
    // mutateHeaderFunc is an optional func to modify extra
    // headers on each outbound request before it&#39;s written. (the
    // original Request given to RoundTrip is not modified)
    mutateHeaderFunc func(Header)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;里面是各种channel, 用的是出神入化， 各位要好好理解一下，这里有三个goroutine，有两个channel writeRequest 和 requestAndChan&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type writeRequest struct {
    req *transportRequest
    ch  chan&amp;lt;- error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主goroutine 往writeRequest里面写，写循环从writeRequest里面接受&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type responseAndError struct {
    res *Response
    err error
}

type requestAndChan struct {
    req *Request
    ch  chan responseAndError
    addedGzip bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主goroutine 往requestAndChan里面写，读循环从requestAndChan里面接受。&lt;/p&gt;

&lt;p&gt;注意这里的channel都是双向channel，也就是channel 的struct里面有一个chan类型的字段， 比如 reqch chan requestAndChan 这里的 requestAndChan 里面的 ch chan responseAndError。&lt;/p&gt;

&lt;p&gt;这个是很牛叉，主 goroutine 通过 reqch 发送requestAndChan 给读循环，然后读循环搞到response后通过 requestAndChan 里面的通道responseAndError把response返给主goroutine，所以我画了一个双向箭头。&lt;/p&gt;

&lt;p&gt;我们研究一下代码，我理解下来其实就是三个goroutine通过channel互相协作的过程。&lt;/p&gt;

&lt;p&gt;主循环：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pc *persistConn) roundTrip(req *transportRequest) (resp *Response, err error) {
    ... 忽略
    // Write the request concurrently with waiting for a response,
    // in case the server decides to reply before reading our full
    // request body.
    writeErrCh := make(chan error, 1)
    pc.writech &amp;lt;- writeRequest{req, writeErrCh}
    //把request发送给写循环
    resc := make(chan responseAndError, 1)
    pc.reqch &amp;lt;- requestAndChan{req.Request, resc, requestedGzip}
    //发送给读循环
    var re responseAndError
    var respHeaderTimer &amp;lt;-chan time.Time
    cancelChan := req.Request.Cancel
WaitResponse:
    for {
        select {
        case err := &amp;lt;-writeErrCh:
            if isNetWriteError(err) {
                //写循环通过这个channel报告错误
                select {
                case re = &amp;lt;-resc:
                    pc.close()
                    break WaitResponse
                case &amp;lt;-time.After(50 * time.Millisecond):
                    // Fall through.
                }
            }
            if err != nil {
                re = responseAndError{nil, err}
                pc.close()
                break WaitResponse
            }
            if d := pc.t.ResponseHeaderTimeout; d &amp;gt; 0 {
                timer := time.NewTimer(d)
                defer timer.Stop() // prevent leaks
                respHeaderTimer = timer.C
            }
        case &amp;lt;-pc.closech:
            // 如果长连接挂了， 这里的channel有数据， 进入这个case, 进行处理

            select {
            case re = &amp;lt;-resc:
                if fn := testHookPersistConnClosedGotRes; fn != nil {
                    fn()
                }
            default:
                re = responseAndError{err: errClosed}
                if pc.isCanceled() {
                    re = responseAndError{err: errRequestCanceled}
                }
            }
            break WaitResponse
        case &amp;lt;-respHeaderTimer:
            pc.close()
            re = responseAndError{err: errTimeout}
            break WaitResponse
            // 如果timeout，这里的channel有数据， break掉for循环
        case re = &amp;lt;-resc:
            break WaitResponse
           // 获取到读循环的response, break掉 for循环
        case &amp;lt;-cancelChan:
            pc.t.CancelRequest(req.Request)
            cancelChan = nil
        }
    }

    if re.err != nil {
        pc.t.setReqCanceler(req.Request, nil)
    }
    return re.res, re.err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段代码主要就干了三件事&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;主goroutine -&amp;gt;requestAndChan -&amp;gt; 读循环goroutine

主goroutine -&amp;gt;writeRequest-&amp;gt; 写循环goroutine

主goroutine 通过select 监听各个channel上的数据， 比如请求取消， timeout，长连接挂了，写流出错，读流出错， 都是其他goroutine 发送过来的， 跟中断一样，然后相应处理，上面也提到了，有些channel是主goroutine通过channel发送给其他goroutine的struct里面包含的channel, 比如 case err := &amp;lt;-writeErrCh: case re = &amp;lt;-resc:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读循环代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pc *persistConn) readLoop() {

    ... 忽略
    alive := true
    for alive {

        ... 忽略
        rc := &amp;lt;-pc.reqch

        var resp *Response
        if err == nil {
            resp, err = ReadResponse(pc.br, rc.req)
            if err == nil &amp;amp;&amp;amp; resp.StatusCode == 100 {
                //100  Continue  初始的请求已经接受，客户应当继续发送请求的其 
                // 余部分
                resp, err = ReadResponse(pc.br, rc.req)
                // 读pc.br（tcp输出流）中的数据，这里的代码在response里面
                //解析statusCode，头字段， 转成标准的内存中的response 类型
                //  http在tcp数据流里面，head和body以 /r/n/r/n分开， 各个头
                // 字段 以/r/n分开
            }
        }

        if resp != nil {
            resp.TLS = pc.tlsState
        }

        ...忽略
        //上面处理一些http协议的一些逻辑行为，
        rc.ch &amp;lt;- responseAndError{resp, err} //把读到的response返回给    
                                             //主goroutine

        .. 忽略
        //忽略部分， 处理cancel req中断， 发送idleConnCh归还pc（持久连接）到持久连接池中（map）    
    pc.close()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;无关代码忽略，这段代码主要干了一件事情&lt;/p&gt;

&lt;p&gt;读循环goroutine 通过channel requestAndChan 接受主goroutine发送的request(rc := &amp;lt;-pc.reqch), 并从tcp输出流中读取response， 然后反序列化到结构体中， 最后通过channel 返给主goroutine (rc.ch &amp;lt;- responseAndError{resp, err} )&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (pc *persistConn) writeLoop() {
    for {
        select {
        case wr := &amp;lt;-pc.writech:   //接受主goroutine的 request
            if pc.isBroken() {
                wr.ch &amp;lt;- errors.New(&amp;quot;http: can&#39;t write HTTP request on broken connection&amp;quot;)
                continue
            }
            err := wr.req.Request.write(pc.bw, pc.isProxy, wr.req.extra)   //写入tcp输入流
            if err == nil {
                err = pc.bw.Flush()
            }
            if err != nil {
                pc.markBroken()
                wr.req.Request.closeBody()
            }
            pc.writeErrCh &amp;lt;- err 
            wr.ch &amp;lt;- err         //  出错的时候返给主goroutineto 
        case &amp;lt;-pc.closech:
            return
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写循环就更简单了，select channel中主gouroutine的request，然后写入tcp输入流，如果出错了，channel 通知调用者。&lt;/p&gt;

&lt;p&gt;整体看下来，过程都很简单，但是代码中有很多值得我们学习的地方，比如高并发请求如何复用tcp连接，这里是连接池的做法，如果使用多个 goroutine相互协作完成一个http请求，出现错误的时候如何通知调用者中断错误，代码风格也有很多可以借鉴的地方。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;http.Client 表示一个http client端，用来处理HTTP相关的工作，例如cookies, redirect, timeout等工作，其内部包含一个Transport，tranport用来建立一个连接，其中维护了一个空闲连接池idleConn map[connectMethodKey][]*persistConn，其中的每个成员都是一个persistConn对象，persistConn是个具体的连接实例，包含了连接的上下文，会启动两个groutine分别执行readLoop和writeLoop, 每当transport调用roundTrip的时候，就会从连接池中选择一个空闲的persistConn，然后调用其roundTrip方法，将读写请求通过channel分别发送到readLoop和writeLoop中，然后会进行select各个channel的信息，包括连接关闭，请求超时，writeLoop出错， readLoop返回读取结果等。在writeLoop中发送请求，在readLoop中获取response并通过channe返回给roundTrip函数中，并再次将自己加入到idleConn中，等待下次请求到来。&lt;/p&gt;

&lt;h2 id=&#34;自定义client&#34;&gt;自定义client&lt;/h2&gt;

&lt;p&gt;在上面我们说到调用结构体的成员函数都是默认的结构体的成员函数，但是如果我们有一些特殊的需求，我们就需要重新定义这些结构体，然后实现自己的逻辑，整个http请求也就会按着我们的逻辑进行处理，这也是我们实现一些功能的必要手段。最基本的就是自定义client，也是我们编程常用的，深入一些就需要了解一些传输transport等。&lt;/p&gt;

&lt;p&gt;1、要管理HTTP客户端的头域、重定向策略和其他设置，创建一个Client：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;client := &amp;amp;http.Client{
    CheckRedirect: redirectPolicyFunc,
}
resp, err := client.Get(&amp;quot;http://example.com&amp;quot;)
// ...
req, err := http.NewRequest(&amp;quot;GET&amp;quot;, &amp;quot;http://example.com&amp;quot;, nil)
// ...
req.Header.Add(&amp;quot;If-None-Match&amp;quot;, `W/&amp;quot;wyzzy&amp;quot;`)
resp, err := client.Do(req)
// ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是在上面的基础上增加了对client结构体的设置，而不是使用DefaultClient，我们来看一下client的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Client struct {
    // Transport指定执行独立、单次HTTP请求的机制。
    // 如果Transport为nil，则使用DefaultTransport。
    Transport RoundTripper
    // CheckRedirect指定处理重定向的策略。
    // 如果CheckRedirect不为nil，客户端会在执行重定向之前调用本函数字段。
    // 参数req和via是将要执行的请求和已经执行的请求（切片，越新的请求越靠后）。
    // 如果CheckRedirect返回一个错误，本类型的Get方法不会发送请求req，
    // 而是返回之前得到的最后一个回复和该错误。（包装进url.Error类型里）
    //
    // 如果CheckRedirect为nil，会采用默认策略：连续10此请求后停止。
    CheckRedirect func(req *Request, via []*Request) error
    // Jar指定cookie管理器。
    // 如果Jar为nil，请求中不会发送cookie，回复中的cookie会被忽略。
    Jar CookieJar
    // Timeout指定本类型的值执行请求的时间限制。
    // 该超时限制包括连接时间、重定向和读取回复主体的时间。
    // 计时器会在Head、Get、Post或Do方法返回后继续运作并在超时后中断回复主体的读取。
    //
    // Timeout为零值表示不设置超时。
    //
    // Client实例的Transport字段必须支持CancelRequest方法，
    // 否则Client会在试图用Head、Get、Post或Do方法执行请求时返回错误。
    // 本类型的Transport字段默认值（DefaultTransport）支持CancelRequest方法。
    Timeout time.Duration
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要是对这些结构体中的成员的如何运用才是重点，然后就调用client的Get，Do等方法就是上面的执行逻辑，这边只是简单的client的处理，后面的逻辑依然使用的是默认的Transport。&lt;/p&gt;

&lt;p&gt;2、要管理代理、TLS配置、keep-alive、压缩和其他设置，创建一个Transport：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tr := &amp;amp;http.Transport{
    TLSClientConfig:    &amp;amp;tls.Config{RootCAs: pool},
    DisableCompression: true,
}
client := &amp;amp;http.Client{Transport: tr}
resp, err := client.Get(&amp;quot;https://example.com&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Client和Transport类型都可以安全的被多个go程同时使用。出于效率考虑，应该一次建立、尽量重用。&lt;/p&gt;

&lt;p&gt;这边在client的基础上对client的transport的管理代理、TLS配置、keep-alive、压缩和其他设置，然后后面的逻辑中主要是切换到自定义的transport的逻辑运行。&lt;/p&gt;

&lt;h1 id=&#34;http服务端&#34;&gt;http服务端&lt;/h1&gt;

&lt;h2 id=&#34;http-status&#34;&gt;http status&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;const (
    StatusContinue           = 100
    StatusSwitchingProtocols = 101
    StatusOK                   = 200
    StatusCreated              = 201
    StatusAccepted             = 202
    StatusNonAuthoritativeInfo = 203
    StatusNoContent            = 204
    StatusResetContent         = 205
    StatusPartialContent       = 206
    StatusMultipleChoices   = 300
    StatusMovedPermanently  = 301
    StatusFound             = 302
    StatusSeeOther          = 303
    StatusNotModified       = 304
    StatusUseProxy          = 305
    StatusTemporaryRedirect = 307
    StatusBadRequest                   = 400
    StatusUnauthorized                 = 401
    StatusPaymentRequired              = 402
    StatusForbidden                    = 403
    StatusNotFound                     = 404
    StatusMethodNotAllowed             = 405
    StatusNotAcceptable                = 406
    StatusProxyAuthRequired            = 407
    StatusRequestTimeout               = 408
    StatusConflict                     = 409
    StatusGone                         = 410
    StatusLengthRequired               = 411
    StatusPreconditionFailed           = 412
    StatusRequestEntityTooLarge        = 413
    StatusRequestURITooLong            = 414
    StatusUnsupportedMediaType         = 415
    StatusRequestedRangeNotSatisfiable = 416
    StatusExpectationFailed            = 417
    StatusTeapot                       = 418
    StatusInternalServerError     = 500
    StatusNotImplemented          = 501
    StatusBadGateway              = 502
    StatusServiceUnavailable      = 503
    StatusGatewayTimeout          = 504
    StatusHTTPVersionNotSupported = 505
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们比较常用的就是404（服务未发现），503（服务不可用）等。&lt;/p&gt;

&lt;h2 id=&#34;http-header&#34;&gt;http header&lt;/h2&gt;

&lt;p&gt;Header代表HTTP头域的键值对。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Header map[string][]string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基本操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Get(key string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get返回键对应的第一个值，如果键不存在会返回&amp;rdquo;&amp;ldquo;。如要获取该键对应的值切片，请直接用规范格式的键访问map。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Set(key, value string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set添加键值对到h，如键已存在则会用只有新值一个元素的切片取代旧值切片。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Add(key, value string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add添加键值对到h，如键已存在则会将新的值附加到旧值切片后面。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Del(key string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Del删除键值对。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (h Header) Write(w io.Writer) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write以有线格式将头域写入w。&lt;/p&gt;

&lt;h2 id=&#34;用于http客户端和服务端的结构体&#34;&gt;用于http客户端和服务端的结构体&lt;/h2&gt;

&lt;p&gt;type Request&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Request struct {
    // Method指定HTTP方法（GET、POST、PUT等）。对客户端，&amp;quot;&amp;quot;代表GET。
    Method string
    // URL在服务端表示被请求的URI，在客户端表示要访问的URL。
    //
    // 在服务端，URL字段是解析请求行的URI（保存在RequestURI字段）得到的，
    // 对大多数请求来说，除了Path和RawQuery之外的字段都是空字符串。
    // （参见RFC 2616, Section 5.1.2）
    //
    // 在客户端，URL的Host字段指定了要连接的服务器，
    // 而Request的Host字段（可选地）指定要发送的HTTP请求的Host头的值。
    URL *url.URL
    // 接收到的请求的协议版本。本包生产的Request总是使用HTTP/1.1
    Proto      string // &amp;quot;HTTP/1.0&amp;quot;
    ProtoMajor int    // 1
    ProtoMinor int    // 0
    // Header字段用来表示HTTP请求的头域。如果头域（多行键值对格式）为：
    //  accept-encoding: gzip, deflate
    //  Accept-Language: en-us
    //  Connection: keep-alive
    // 则：
    //  Header = map[string][]string{
    //      &amp;quot;Accept-Encoding&amp;quot;: {&amp;quot;gzip, deflate&amp;quot;},
    //      &amp;quot;Accept-Language&amp;quot;: {&amp;quot;en-us&amp;quot;},
    //      &amp;quot;Connection&amp;quot;: {&amp;quot;keep-alive&amp;quot;},
    //  }
    // HTTP规定头域的键名（头名）是大小写敏感的，请求的解析器通过规范化头域的键名来实现这点。
    // 在客户端的请求，可能会被自动添加或重写Header中的特定的头，参见Request.Write方法。
    Header Header
    // Body是请求的主体。
    //
    // 在客户端，如果Body是nil表示该请求没有主体买入GET请求。
    // Client的Transport字段会负责调用Body的Close方法。
    //
    // 在服务端，Body字段总是非nil的；但在没有主体时，读取Body会立刻返回EOF。
    // Server会关闭请求的主体，ServeHTTP处理器不需要关闭Body字段。
    Body io.ReadCloser
    // ContentLength记录相关内容的长度。
    // 如果为-1，表示长度未知，如果&amp;gt;=0，表示可以从Body字段读取ContentLength字节数据。
    // 在客户端，如果Body非nil而该字段为0，表示不知道Body的长度。
    ContentLength int64
    // TransferEncoding按从最外到最里的顺序列出传输编码，空切片表示&amp;quot;identity&amp;quot;编码。
    // 本字段一般会被忽略。当发送或接受请求时，会自动添加或移除&amp;quot;chunked&amp;quot;传输编码。
    TransferEncoding []string
    // Close在服务端指定是否在回复请求后关闭连接，在客户端指定是否在发送请求后关闭连接。
    Close bool
    // 在服务端，Host指定URL会在其上寻找资源的主机。
    // 根据RFC 2616，该值可以是Host头的值，或者URL自身提供的主机名。
    // Host的格式可以是&amp;quot;host:port&amp;quot;。
    //
    // 在客户端，请求的Host字段（可选地）用来重写请求的Host头。
    // 如过该字段为&amp;quot;&amp;quot;，Request.Write方法会使用URL字段的Host。
    Host string
    // Form是解析好的表单数据，包括URL字段的query参数和POST或PUT的表单数据。
    // 本字段只有在调用ParseForm后才有效。在客户端，会忽略请求中的本字段而使用Body替代。
    Form url.Values
    // PostForm是解析好的POST或PUT的表单数据。
    // 本字段只有在调用ParseForm后才有效。在客户端，会忽略请求中的本字段而使用Body替代。
    PostForm url.Values
    // MultipartForm是解析好的多部件表单，包括上传的文件。
    // 本字段只有在调用ParseMultipartForm后才有效。
    // 在客户端，会忽略请求中的本字段而使用Body替代。
    MultipartForm *multipart.Form
    // Trailer指定了会在请求主体之后发送的额外的头域。
    //
    // 在服务端，Trailer字段必须初始化为只有trailer键，所有键都对应nil值。
    // （客户端会声明哪些trailer会发送）
    // 在处理器从Body读取时，不能使用本字段。
    // 在从Body的读取返回EOF后，Trailer字段会被更新完毕并包含非nil的值。
    // （如果客户端发送了这些键值对），此时才可以访问本字段。
    //
    // 在客户端，Trail必须初始化为一个包含将要发送的键值对的映射。（值可以是nil或其终值）
    // ContentLength字段必须是0或-1，以启用&amp;quot;chunked&amp;quot;传输编码发送请求。
    // 在开始发送请求后，Trailer可以在读取请求主体期间被修改，
    // 一旦请求主体返回EOF，调用者就不可再修改Trailer。
    //
    // 很少有HTTP客户端、服务端或代理支持HTTP trailer。
    Trailer Header
    // RemoteAddr允许HTTP服务器和其他软件记录该请求的来源地址，一般用于日志。
    // 本字段不是ReadRequest函数填写的，也没有定义格式。
    // 本包的HTTP服务器会在调用处理器之前设置RemoteAddr为&amp;quot;IP:port&amp;quot;格式的地址。
    // 客户端会忽略请求中的RemoteAddr字段。
    RemoteAddr string
    // RequestURI是被客户端发送到服务端的请求的请求行中未修改的请求URI
    // （参见RFC 2616, Section 5.1）
    // 一般应使用URI字段，在客户端设置请求的本字段会导致错误。
    RequestURI string
    // TLS字段允许HTTP服务器和其他软件记录接收到该请求的TLS连接的信息
    // 本字段不是ReadRequest函数填写的。
    // 对启用了TLS的连接，本包的HTTP服务器会在调用处理器之前设置TLS字段，否则将设TLS为nil。
    // 客户端会忽略请求中的TLS字段。
    TLS *tls.ConnectionState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Request类型代表一个服务端接受到的或者客户端发送出去的HTTP请求。&lt;/p&gt;

&lt;p&gt;Request各字段的意义和用途在服务端和客户端是不同的。除了字段本身上方文档，还可参见Request.Write方法和RoundTripper接口的文档。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Response struct {
    Status     string // 例如&amp;quot;200 OK&amp;quot;
    StatusCode int    // 例如200
    Proto      string // 例如&amp;quot;HTTP/1.0&amp;quot;
    ProtoMajor int    // 例如1
    ProtoMinor int    // 例如0
    // Header保管头域的键值对。
    // 如果回复中有多个头的键相同，Header中保存为该键对应用逗号分隔串联起来的这些头的值
    // （参见RFC 2616 Section 4.2）
    // 被本结构体中的其他字段复制保管的头（如ContentLength）会从Header中删掉。
    //
    // Header中的键都是规范化的，参见CanonicalHeaderKey函数
    Header Header
    // Body代表回复的主体。
    // Client类型和Transport类型会保证Body字段总是非nil的，即使回复没有主体或主体长度为0。
    // 关闭主体是调用者的责任。
    // 如果服务端采用&amp;quot;chunked&amp;quot;传输编码发送的回复，Body字段会自动进行解码。
    Body io.ReadCloser
    // ContentLength记录相关内容的长度。
    // 其值为-1表示长度未知（采用chunked传输编码）
    // 除非对应的Request.Method是&amp;quot;HEAD&amp;quot;，其值&amp;gt;=0表示可以从Body读取的字节数
    ContentLength int64
    // TransferEncoding按从最外到最里的顺序列出传输编码，空切片表示&amp;quot;identity&amp;quot;编码。
    TransferEncoding []string
    // Close记录头域是否指定应在读取完主体后关闭连接。（即Connection头）
    // 该值是给客户端的建议，Response.Write方法的ReadResponse函数都不会关闭连接。
    Close bool
    // Trailer字段保存和头域相同格式的trailer键值对，和Header字段相同类型
    Trailer Header
    // Request是用来获取此回复的请求
    // Request的Body字段是nil（因为已经被用掉了）
    // 这个字段是被Client类型发出请求并获得回复后填充的
    Request *Request
    // TLS包含接收到该回复的TLS连接的信息。 对未加密的回复，本字段为nil。
    // 返回的指针是被（同一TLS连接接收到的）回复共享的，不应被修改。
    TLS *tls.ConnectionState
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Response代表一个HTTP请求的回复&lt;/p&gt;

&lt;p&gt;type ResponseWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ResponseWriter interface {
    // Header返回一个Header类型值，该值会被WriteHeader方法发送。
    // 在调用WriteHeader或Write方法后再改变该对象是没有意义的。
    Header() Header
    // WriteHeader该方法发送HTTP回复的头域和状态码。
    // 如果没有被显式调用，第一次调用Write时会触发隐式调用WriteHeader(http.StatusOK)
    // WriterHeader的显式调用主要用于发送错误码。
    WriteHeader(int)
    // Write向连接中写入作为HTTP的一部分回复的数据。
    // 如果被调用时还未调用WriteHeader，本方法会先调用WriteHeader(http.StatusOK)
    // 如果Header中没有&amp;quot;Content-Type&amp;quot;键，
    // 本方法会使用包函数DetectContentType检查数据的前512字节，将返回值作为该键的值。
    Write([]byte) (int, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResponseWriter接口被HTTP处理器用于构造HTTP回复。这个一般用于服务端处理请求&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;p&gt;正常我们使用的返回方式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
 &amp;quot;net/http&amp;quot;
)

func main() {

 http.HandleFunc(&amp;quot;/&amp;quot;, func (w http.ResponseWriter, r *http.Request){


   w.Header().Set(&amp;quot;name&amp;quot;, &amp;quot;my name is smallsoup&amp;quot;)
   w.WriteHeader(500)
   w.Write([]byte(&amp;quot;hello world\n&amp;quot;))

 })

 http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;type CloseNotifier&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type CloseNotifier interface {
    // CloseNotify返回一个通道，该通道会在客户端连接丢失时接收到唯一的值
    CloseNotify() &amp;lt;-chan bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HTTP处理器ResponseWriter接口参数的下层如果实现了CloseNotifier接口，可以让用户检测下层的连接是否停止。如果客户端在回复准备好之前关闭了连接，该机制可以用于取消服务端耗时较长的操作。&lt;/p&gt;

&lt;h2 id=&#34;http-服务端使用和原理解析&#34;&gt;http 服务端使用和原理解析&lt;/h2&gt;

&lt;p&gt;ListenAndServe使用指定的监听地址和处理器启动一个HTTP服务端。处理器参数通常是nil，这表示采用包变量DefaultServeMux作为处理器。Handle和HandleFunc函数可以向DefaultServeMux添加处理器。如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.Handle(&amp;quot;/foo&amp;quot;, fooHandler)
http.HandleFunc(&amp;quot;/bar&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintf(w, &amp;quot;Hello, %q&amp;quot;, html.EscapeString(r.URL.Path))
})
log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenAndServe该方法用于在指定的TCP网络地址addr进行监听，然后调用服务端处理程序来处理传入的连接请求。该方法有两个参数：第一个参数addr 即监听地址；第二个参数表示服务端处理程序，通常为空，这意味着服务端调用 http.DefaultServeMux 进行处理，而服务端编写的业务逻辑处理程序 http.Handle() 或 http.HandleFunc() 默认注入 http.DefaultServeMux 中。&lt;/p&gt;

&lt;p&gt;理解HTTP相关的网络应用，主要关注两个地方-客户端(client)和服务端(server)，两者的交互主要是client的request以及server的response,主要就在于如何接受client的request并向client返回response。&lt;/p&gt;

&lt;p&gt;接收request的过程中，最重要的莫过于路由（router），即实现一个Multiplexer器。Go http中既可以使用内置的mutilplexer &amp;mdash; DefautServeMux，也可以自定义。Multiplexer路由的目的就是为了找到处理器函数（handler），后者将对request进行处理，同时构建response&lt;/p&gt;

&lt;p&gt;流程为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Clinet -&amp;gt; Requests -&amp;gt;  Multiplexer(router) -&amp;gt; handler  -&amp;gt; Response -&amp;gt; Clinet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于一个http服务，大致需要理解这两个封装的过程就可以理解上面的实现了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.首先需要注册路由，即提供url模式和handler函数的映射.
2.其次就是实例化一个server对象，并开启对客户端的监听。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再看go http服务的代码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/&amp;quot;, indexHandler) -----即是注册路由。
http.ListenAndServe(&amp;quot;127.0.0.1:8000&amp;quot;, nil)---启动server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server := &amp;amp;Server{Addr: addr, Handler: handler}
server.ListenAndServe()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;注册路由&#34;&gt;注册路由&lt;/h3&gt;

&lt;p&gt;net/http包暴露的注册路由的api很简单&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/&amp;quot;, indexHandler) -----即是注册路由。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HandlerFunc是一个函数类型，如下定义，同时实现了Handler接口的ServeHTTP方法。使用HandlerFunc类型包装一下路由定义的indexHandler函数，其目的就是为了让这个函数也实现ServeHTTP方法，即转变成一个handler处理器(函数)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type HandlerFunc func(ResponseWriter, *Request)

// ServeHTTP calls f(w, r).
func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) {
    f(w, r)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们最开始写的例子中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.HandleFunc(&amp;quot;/&amp;quot;,Indexhandler)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样 IndexHandler 函数也有了ServeHTTP方法。&lt;/p&gt;

&lt;p&gt;ServeMux和handler处理器函数的连接桥梁就是Handler接口。ServeMux的ServeHTTP方法实现了寻找注册路由的handler的函数（可以看下面的监控服务流程），并调用该handler的ServeHTTP方法。ServeHTTP方法就是真正处理请求和构造响应的地方。&lt;/p&gt;

&lt;p&gt;Go其实支持外部实现的路由器 ListenAndServe的第二个参数就是 用以配置外部路由器的，它是一个Handler接口，即外部路由器只要实现了Handler接口就可以,我们可以在自己实现 的路由器的ServHTTP里面实现自定义路由功能。&lt;/p&gt;

&lt;p&gt;如下代码所示，我们自己实现了一个简易的路由器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import ( 
    &amp;quot;fmt&amp;quot;
    &amp;quot;net/http&amp;quot; 
    )
type MyMux struct { }

func (p *MyMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path == &amp;quot;/&amp;quot; {
    sayhelloName(w, r)
    return 
}
    http.NotFound(w, r)
    return 
}

func sayhelloName(w http.ResponseWriter, r *http.Request) { f
    mt.Fprintf(w, &amp;quot;Hello myroute!&amp;quot;)
}

func main() {
    mux := &amp;amp;MyMux{}
    http.ListenAndServe(&amp;quot;:9090&amp;quot;, mux) 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;multiplexer&#34;&gt;multiplexer&lt;/h3&gt;

&lt;p&gt;http.HandleFunc选取了DefaultServeMux作为multiplexer：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) {
    DefaultServeMux.HandleFunc(pattern, handler)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DefaultServeMux是ServeMux的一个实例。当然http包也提供了NewServeMux方法创建一个ServeMux实例，默认则创建一个DefaultServeMux：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// NewServeMux allocates and returns a new ServeMux.
func NewServeMux() *ServeMux { return new(ServeMux) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DefaultServeMux的代码定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// DefaultServeMux is the default ServeMux used by Serve.
var DefaultServeMux = &amp;amp;defaultServeMux
var defaultServeMux ServeMux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然也可以是其他可以实现的实例 ，比如上面实现的mux。&lt;/p&gt;

&lt;p&gt;路由结构体ServeMux&lt;/p&gt;

&lt;p&gt;ServeMux的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ServeMux struct {
    mu    sync.RWMutex                      //锁，由于请求涉及到并发处理，因此这里需要一个锁机制
    m     map[string]muxEntry               // 路由规则，一个string对应一个mux实体，这里的string就是注册的路由
    hosts bool 
}

type muxEntry struct {
    explicit bool                // 是否精确匹配
    h        Handler              // 这个路由表达式对应哪个handler
    pattern  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ServeMux结构中最重要的字段为m，这是一个map，key是一些url模式，value是一个muxEntry结构，后者里定义存储了具体的url模式和handler。&lt;/p&gt;

&lt;p&gt;当然，所谓的ServeMux也实现了ServeHTTP接口，也算是一个handler，不过ServeMux的ServeHTTP方法不是用来处理request和respone，而是用来找到路由注册的handler，可以看服务监听时候的调用过程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// HandleFunc registers the handler function for the given pattern.
func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) {
    if handler == nil {
        panic(&amp;quot;http: nil handler&amp;quot;)
    }
    mux.Handle(pattern, HandlerFunc(handler))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ServeMux的Handle方法，将会对pattern和handler函数做一个map映射：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Handle registers the handler for the given pattern.
// If a handler already exists for pattern, Handle panics.
func (mux *ServeMux) Handle(pattern string, handler Handler) {
    mux.mu.Lock()
    defer mux.mu.Unlock()

    if pattern == &amp;quot;&amp;quot; {
        panic(&amp;quot;http: invalid pattern &amp;quot; + pattern)
    }
    if handler == nil {
        panic(&amp;quot;http: nil handler&amp;quot;)
    }
    if mux.m[pattern].explicit {
        panic(&amp;quot;http: multiple registrations for &amp;quot; + pattern)
    }

    if mux.m == nil {
        mux.m = make(map[string]muxEntry)
    }
    mux.m[pattern] = muxEntry{explicit: true, h: handler, pattern: pattern}

    if pattern[0] != &#39;/&#39; {
        mux.hosts = true
    }

    // Helpful behavior:
    // If pattern is /tree/, insert an implicit permanent redirect for /tree.
    // It can be overridden by an explicit registration.
    n := len(pattern)
    if n &amp;gt; 0 &amp;amp;&amp;amp; pattern[n-1] == &#39;/&#39; &amp;amp;&amp;amp; !mux.m[pattern[0:n-1]].explicit {
        // If pattern contains a host name, strip it and use remaining
        // path for redirect.
        path := pattern
        if pattern[0] != &#39;/&#39; {
            // In pattern, at least the last character is a &#39;/&#39;, so
            // strings.Index can&#39;t be -1.
            path = pattern[strings.Index(pattern, &amp;quot;/&amp;quot;):]
        }
        url := &amp;amp;url.URL{Path: path}
        mux.m[pattern[0:n-1]] = muxEntry{h: RedirectHandler(url.String(), StatusMovedPermanently), pattern: pattern}
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Handle函数的主要目的在于把handler和pattern模式绑定到map[string]muxEntry的map上，其中muxEntry保存了更多pattern和handler的信息，还记得前面讨论的Server结构吗？Server的m字段就是map[string]muxEntry这样一个map。&lt;/p&gt;

&lt;p&gt;此时，pattern和handler的路由注册完成。接下来就是如何开始server的监听，以接收客户端的请求。&lt;/p&gt;

&lt;h3 id=&#34;启动服务&#34;&gt;启动服务&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;http.ListenAndServe(&amp;quot;127.0.0.1:8000&amp;quot;, nil)---启动server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server := &amp;amp;Server{Addr: addr, Handler: handler}

server.ListenAndServe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注册好路由之后，启动web服务还需要开启服务器监听。http的ListenAndServer方法中可以看到创建了一个Server对象，并调用了Server对象的同名方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenAndServe(addr string, handler Handler) error {
    server := &amp;amp;Server{Addr: addr, Handler: handler}
    return server.ListenAndServe()
}
// ListenAndServe listens on the TCP network address srv.Addr and then
// calls Serve to handle requests on incoming connections.
// Accepted connections are configured to enable TCP keep-alives.
// If srv.Addr is blank, &amp;quot;:http&amp;quot; is used.
// ListenAndServe always returns a non-nil error.
func (srv *Server) ListenAndServe() error {
    addr := srv.Addr
    if addr == &amp;quot;&amp;quot; {
        addr = &amp;quot;:http&amp;quot;
    }
    ln, err := net.Listen(&amp;quot;tcp&amp;quot;, addr)
    if err != nil {
        return err
    }
    return srv.Serve(tcpKeepAliveListener{ln.(*net.TCPListener)})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Server的ListenAndServe方法中，会初始化监听地址Addr，同时调用Listen方法设置监听。最后将监听的TCP对象传入Serve方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Serve accepts incoming connections on the Listener l, creating a
// new service goroutine for each. The service goroutines read requests and
// then call srv.Handler to reply to them.
//
// For HTTP/2 support, srv.TLSConfig should be initialized to the
// provided listener&#39;s TLS Config before calling Serve. If
// srv.TLSConfig is non-nil and doesn&#39;t include the string &amp;quot;h2&amp;quot; in
// Config.NextProtos, HTTP/2 support is not enabled.
//
// Serve always returns a non-nil error. After Shutdown or Close, the
// returned error is ErrServerClosed.
func (srv *Server) Serve(l net.Listener) error {
    defer l.Close()
    if fn := testHookServerServe; fn != nil {
        fn(srv, l)
    }
    var tempDelay time.Duration // how long to sleep on accept failure

    if err := srv.setupHTTP2_Serve(); err != nil {
        return err
    }

    srv.trackListener(l, true)
    defer srv.trackListener(l, false)

    baseCtx := context.Background() // base is always background, per Issue 16220
    ctx := context.WithValue(baseCtx, ServerContextKey, srv)
    for {
        rw, e := l.Accept()
        if e != nil {
            select {
            case &amp;lt;-srv.getDoneChan():
                return ErrServerClosed
            default:
            }
            if ne, ok := e.(net.Error); ok &amp;amp;&amp;amp; ne.Temporary() {
                if tempDelay == 0 {
                    tempDelay = 5 * time.Millisecond
                } else {
                    tempDelay *= 2
                }
                if max := 1 * time.Second; tempDelay &amp;gt; max {
                    tempDelay = max
                }
                srv.logf(&amp;quot;http: Accept error: %v; retrying in %v&amp;quot;, e, tempDelay)
                time.Sleep(tempDelay)
                continue
            }
            return e
        }
        tempDelay = 0
        c := srv.newConn(rw)
        c.setState(c.rwc, StateNew) // before Serve can return
        go c.serve(ctx)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监听开启之后，一旦客户端请求到达，创建一个conn结构体，这个conn中保留了这次请求的信息，go就开启一个协程serve处理请求，主要逻辑都在serve方法之中。&lt;/p&gt;

&lt;p&gt;serve方法比较长，其主要职能就是，创建一个上下文对象，然后调用Listener的Accept方法用来　获取连接数据并使用newConn方法创建连接对象。最后使用goroutein协程的方式处理连接请求。因为每一个连接都开起了一个协程，请求的上下文都不同，同时又保证了go的高并发。serve也是一个长长的方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Serve a new connection.
func (c *conn) serve(ctx context.Context) {
    c.remoteAddr = c.rwc.RemoteAddr().String()
    ctx = context.WithValue(ctx, LocalAddrContextKey, c.rwc.LocalAddr())
    defer func() {
        if err := recover(); err != nil &amp;amp;&amp;amp; err != ErrAbortHandler {
            const size = 64 &amp;lt;&amp;lt; 10
            buf := make([]byte, size)
            buf = buf[:runtime.Stack(buf, false)]
            c.server.logf(&amp;quot;http: panic serving %v: %v\n%s&amp;quot;, c.remoteAddr, err, buf)
        }
        if !c.hijacked() {
            c.close()
            c.setState(c.rwc, StateClosed)
        }
    }()

    if tlsConn, ok := c.rwc.(*tls.Conn); ok {
        if d := c.server.ReadTimeout; d != 0 {
            c.rwc.SetReadDeadline(time.Now().Add(d))
        }
        if d := c.server.WriteTimeout; d != 0 {
            c.rwc.SetWriteDeadline(time.Now().Add(d))
        }
        if err := tlsConn.Handshake(); err != nil {
            c.server.logf(&amp;quot;http: TLS handshake error from %s: %v&amp;quot;, c.rwc.RemoteAddr(), err)
            return
        }
        c.tlsState = new(tls.ConnectionState)
        *c.tlsState = tlsConn.ConnectionState()
        if proto := c.tlsState.NegotiatedProtocol; validNPN(proto) {
            if fn := c.server.TLSNextProto[proto]; fn != nil {
                h := initNPNRequest{tlsConn, serverHandler{c.server}}
                fn(c.server, tlsConn, h)
            }
            return
        }
    }

    // HTTP/1.x from here on.

    ctx, cancelCtx := context.WithCancel(ctx)
    c.cancelCtx = cancelCtx
    defer cancelCtx()

    c.r = &amp;amp;connReader{conn: c}
    c.bufr = newBufioReader(c.r)
    c.bufw = newBufioWriterSize(checkConnErrorWriter{c}, 4&amp;lt;&amp;lt;10)

    for {
        w, err := c.readRequest(ctx)
        if c.r.remain != c.server.initialReadLimitSize() {
            // If we read any bytes off the wire, we&#39;re active.
            c.setState(c.rwc, StateActive)
        }
        if err != nil {
            const errorHeaders = &amp;quot;\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n&amp;quot;

            if err == errTooLarge {
                // Their HTTP client may or may not be
                // able to read this if we&#39;re
                // responding to them and hanging up
                // while they&#39;re still writing their
                // request. Undefined behavior.
                const publicErr = &amp;quot;431 Request Header Fields Too Large&amp;quot;
                fmt.Fprintf(c.rwc, &amp;quot;HTTP/1.1 &amp;quot;+publicErr+errorHeaders+publicErr)
                c.closeWriteAndWait()
                return
            }
            if isCommonNetReadError(err) {
                return // don&#39;t reply
            }

            publicErr := &amp;quot;400 Bad Request&amp;quot;
            if v, ok := err.(badRequestError); ok {
                publicErr = publicErr + &amp;quot;: &amp;quot; + string(v)
            }

            fmt.Fprintf(c.rwc, &amp;quot;HTTP/1.1 &amp;quot;+publicErr+errorHeaders+publicErr)
            return
        }

        // Expect 100 Continue support
        req := w.req
        if req.expectsContinue() {
            if req.ProtoAtLeast(1, 1) &amp;amp;&amp;amp; req.ContentLength != 0 {
                // Wrap the Body reader with one that replies on the connection
                req.Body = &amp;amp;expectContinueReader{readCloser: req.Body, resp: w}
            }
        } else if req.Header.get(&amp;quot;Expect&amp;quot;) != &amp;quot;&amp;quot; {
            w.sendExpectationFailed()
            return
        }

        c.curReq.Store(w)

        if requestBodyRemains(req.Body) {
            registerOnHitEOF(req.Body, w.conn.r.startBackgroundRead)
        } else {
            if w.conn.bufr.Buffered() &amp;gt; 0 {
                w.conn.r.closeNotifyFromPipelinedRequest()
            }
            w.conn.r.startBackgroundRead()
        }

        // HTTP cannot have multiple simultaneous active requests.[*]
        // Until the server replies to this request, it can&#39;t read another,
        // so we might as well run the handler in this goroutine.
        // [*] Not strictly true: HTTP pipelining. We could let them all process
        // in parallel even if their responses need to be serialized.
        // But we&#39;re not going to implement HTTP pipelining because it
        // was never deployed in the wild and the answer is HTTP/2.
        serverHandler{c.server}.ServeHTTP(w, w.req)
        w.cancelCtx()
        if c.hijacked() {
            return
        }
        w.finishRequest()
        if !w.shouldReuseConnection() {
            if w.requestBodyLimitHit || w.closedRequestBodyEarly() {
                c.closeWriteAndWait()
            }
            return
        }
        c.setState(c.rwc, StateIdle)
        c.curReq.Store((*response)(nil))

        if !w.conn.server.doKeepAlives() {
            // We&#39;re in shutdown mode. We might&#39;ve replied
            // to the user without &amp;quot;Connection: close&amp;quot; and
            // they might think they can send another
            // request, but such is life with HTTP/1.1.
            return
        }

        if d := c.server.idleTimeout(); d != 0 {
            c.rwc.SetReadDeadline(time.Now().Add(d))
            if _, err := c.bufr.Peek(4); err != nil {
                return
            }
        }
        c.rwc.SetReadDeadline(time.Time{})
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用defer定义了函数退出时，连接关闭相关的处理。然后就是读取连接的网络数据，并处理读取完毕时候的状态。接下来就是调用serverHandler{c.server}.ServeHTTP(w, w.req)方法处理请求了。最后就是请求处理完毕的逻辑。serverHandler是一个重要的结构，它近有一个字段，即Server结构，同时它也实现了Handler接口方法ServeHTTP，并在该接口方法中做了一个重要的事情，初始化multiplexer路由多路复用器。如果server对象没有指定Handler，则使用默认的DefaultServeMux作为路由Multiplexer。并调用初始化Handler的ServeHTTP方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// serverHandler delegates to either the server&#39;s Handler or
// DefaultServeMux and also handles &amp;quot;OPTIONS *&amp;quot; requests.
type serverHandler struct {
    srv *Server
}

func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) {
    handler := sh.srv.Handler
    if handler == nil {
        handler = DefaultServeMux
    }
    if req.RequestURI == &amp;quot;*&amp;quot; &amp;amp;&amp;amp; req.Method == &amp;quot;OPTIONS&amp;quot; {
        handler = globalOptionsHandler{}
    }
    handler.ServeHTTP(rw, req)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里DefaultServeMux的ServeHTTP方法其实也是定义在ServeMux结构中的，相关代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Find a handler on a handler map given a path string.
// Most-specific (longest) pattern wins.
func (mux *ServeMux) match(path string) (h Handler, pattern string) {
    // Check for exact match first.
    v, ok := mux.m[path]
    if ok {
        return v.h, v.pattern
    }

    // Check for longest valid match.
    var n = 0
    for k, v := range mux.m {
        if !pathMatch(k, path) {
            continue
        }
        if h == nil || len(k) &amp;gt; n {
            n = len(k)
            h = v.h
            pattern = v.pattern
        }
    }
    return
}
func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) {

    // CONNECT requests are not canonicalized.
    if r.Method == &amp;quot;CONNECT&amp;quot; {
        return mux.handler(r.Host, r.URL.Path)
    }

    // All other requests have any port stripped and path cleaned
    // before passing to mux.handler.
    host := stripHostPort(r.Host)
    path := cleanPath(r.URL.Path)
    if path != r.URL.Path {
        _, pattern = mux.handler(host, path)
        url := *r.URL
        url.Path = path
        return RedirectHandler(url.String(), StatusMovedPermanently), pattern
    }

    return mux.handler(host, r.URL.Path)
}

// handler is the main implementation of Handler.
// The path is known to be in canonical form, except for CONNECT methods.
func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) {
    mux.mu.RLock()
    defer mux.mu.RUnlock()

    // Host-specific pattern takes precedence over generic ones
    if mux.hosts {
        h, pattern = mux.match(host + path)
    }
    if h == nil {
        h, pattern = mux.match(path)
    }
    if h == nil {
        h, pattern = NotFoundHandler(), &amp;quot;&amp;quot;
    }
    return
}

// ServeHTTP dispatches the request to the handler whose
// pattern most closely matches the request URL.
func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) {
    if r.RequestURI == &amp;quot;*&amp;quot; {
        if r.ProtoAtLeast(1, 1) {
            w.Header().Set(&amp;quot;Connection&amp;quot;, &amp;quot;close&amp;quot;)
        }
        w.WriteHeader(StatusBadRequest)
        return
    }
    h, _ := mux.Handler(r)
    h.ServeHTTP(w, r)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mux的ServeHTTP方法通过调用其Handler方法寻找注册到路由上的handler函数，并调用该函数的ServeHTTP方法，本例则是IndexHandler函数。&lt;/p&gt;

&lt;p&gt;mux的Handler方法对URL简单的处理，然后调用handler方法，后者会创建一个锁，同时调用match方法返回一个handler和pattern。&lt;/p&gt;

&lt;p&gt;在match方法中，mux的m字段是map[string]muxEntry图，后者存储了pattern和handler处理器函数，因此通过迭代m寻找出注册路由的patten模式与实际url匹配的handler函数并返回。&lt;/p&gt;

&lt;p&gt;返回的结构一直传递到mux的ServeHTTP方法，接下来调用handler函数的ServeHTTP方法，即IndexHandler函数，然后把response写到http.RequestWirter对象返回给客户端。&lt;/p&gt;

&lt;p&gt;上述函数运行结束即serverHandler{c.server}.ServeHTTP(w, w.req)运行结束。接下来就是对请求处理完毕之后上希望和连接断开的相关逻辑。&lt;/p&gt;

&lt;p&gt;至此，Golang中一个完整的http服务介绍完毕，包括注册路由，开启监听，处理连接，路由处理函数。
多数的web应用基于HTTP协议，客户端和服务器通过request-response的方式交互。一个server并不可少的两部分莫过于路由注册和连接处理。Golang通过一个ServeMux实现了的multiplexer路由多路复用器来管理路由。同时提供一个Handler接口提供ServeHTTP用来实现handler处理其函数，后者可以处理实际request并构造response。&lt;/p&gt;

&lt;h3 id=&#34;总结-1&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;理解go中的http服务，最重要就是要理解Multiplexer和handler，Golang中的Multiplexer基于ServeMux结构，同时也实现了Handler接口。下面对几个重要概念说明，两个重要的结构体和一个接口&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Handler类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Golang没有继承，类多态的方式可以通过接口实现。所谓接口则是定义声明了函数签名，任何结构只要实现了与接口函数签名相同的方法，就等同于实现了接口。go的http服务都是基于handler进行处理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Handler interface {
    ServeHTTP(ResponseWriter, *Request)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;任何结构体，只要实现了ServeHTTP方法，这个结构就可以称之为handler对象。ServeMux会使用handler并调用其ServeHTTP方法处理请求并返回响应。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;handler处理器(函数)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;handler处理器(函数)-就是HandleFunc的第二个参数，是一个函数： 具有func(w http.ResponseWriter, r *http.Requests)签名的函数，经过HandlerFunc结构包装的handler函数，它实现了ServeHTTP接口方法的函数。调用handler处理器的ServeHTTP方法时，即调用handler函数本身。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;handler对象：实现了Handler接口ServeHTTP方法的结构。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ServeMux和handler处理器函数的连接桥梁就是Handler接口。ServeMux的ServeHTTP方法实现了寻找注册路由的handler的函数，并调用该handler的ServeHTTP方法。ServeHTTP方法就是真正处理请求和构造响应的地方。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Server结构体&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从http.ListenAndServe的源码可以看出，它还是创建了一个server对象，并调用server对象的ListenAndServe方法来实现监听路由：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenAndServe(addr string, handler Handler) error {
    server := &amp;amp;Server{Addr: addr, Handler: handler}
    return server.ListenAndServe()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看server的结构如下，其实上面已经解释过：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server struct {
    Addr         string        
    Handler      Handler       
    ReadTimeout  time.Duration 
    WriteTimeout time.Duration 
    TLSConfig    *tls.Config   

    MaxHeaderBytes int

    TLSNextProto map[string]func(*Server, *tls.Conn, Handler)

    ConnState func(net.Conn, ConnState)
    ErrorLog *log.Logger
    disableKeepAlives int32     nextProtoOnce     sync.Once 
    nextProtoErr      error     
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;server结构存储了服务器处理请求常见的字段。其中Handler字段也保留Handler接口。如果Server接口没有提供Handler结构对象，那么会使用DefautServeMux做multiplexer，后面再做分析。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;路由结构体ServeMux&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ServeMux的源码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ServeMux struct {
    mu    sync.RWMutex                      //锁，由于请求涉及到并发处理，因此这里需要一个锁机制
    m     map[string]muxEntry               // 路由规则，一个string对应一个mux实体，这里的string就是注册的路由
    hosts bool 
}

type muxEntry struct {
    explicit bool                // 是否精确匹配
    h        Handler              // 这个路由表达式对应哪个handler
    pattern  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ServeMux结构中最重要的字段为m，这是一个map，key是一些url模式，value是一个muxEntry结构，后者里定义存储了具体的url模式和handler。&lt;/p&gt;

&lt;p&gt;当然，所谓的ServeMux也实现了ServeHTTP接口，也算是一个handler，不过ServeMux的ServeHTTP方法不是用来处理request和respone，而是用来找到路由注册的handler&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Go代码的执行流程&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;过对http包的分析之后，现在让我们来梳理一下整个的代码执行过程。&lt;/p&gt;

&lt;p&gt;1、首先调用Http.HandleFunc&lt;/p&gt;

&lt;p&gt;按顺序做了几件事:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1 调用了DefaultServerMux的HandleFunc
2 调用了DefaultServerMux的Handle
3 往DefaultServeMux的map[string]muxEntry中增加对应的handler和路由规则
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、其次调用http.ListenAndServe(&amp;rdquo;:9090&amp;rdquo;, nil)&lt;/p&gt;

&lt;p&gt;按顺序做了几件事情:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1 实例化Server
2 调用Server的ListenAndServe()
3 调用net.Listen(&amp;quot;tcp&amp;quot;, addr)监听端口
4 启动一个for循环，在循环体中Accept请求
5 对每个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve()
6 读取每个请求的内容w, err := c.readRequest()
7 判断handler是否为空，如果没有设置handler(这个例子就没有设置handler)，handler就设置为 DefaultServeMux
8 调用handler的ServeHttp
9 在这个例子中，下面就进入到DefaultServerMux.ServeHttp
10 根据request选择handler，并且进入到这个handler的ServeHTTP mux.handler(r).ServeHTTP(w, r)
11 选择handler:
    A 判断是否有路由能满足这个request(循环遍历ServerMux的muxEntry)
    B 如果有路由满足，调用这个路由handler的ServeHttp
    C 如果没有路由满足，调用NotFoundHandler的ServeHttp
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自定义server&#34;&gt;自定义server&lt;/h2&gt;

&lt;p&gt;要管理服务端的行为，可以创建一个自定义的Server：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s := &amp;amp;http.Server{
    Addr:           &amp;quot;:8080&amp;quot;,
    Handler:        myHandler,
    ReadTimeout:    10 * time.Second,
    WriteTimeout:   10 * time.Second,
    MaxHeaderBytes: 1 &amp;lt;&amp;lt; 20,
}
log.Fatal(s.ListenAndServe())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也是上面的流程，就是新增了一个server结构体的，做对应的操作，来看一下server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server struct {
    Addr           string        // TCP address to listen on, &amp;quot;:http&amp;quot; if empty
    Handler        Handler       // handler to invoke, http.DefaultServeMux if nil
    ReadTimeout    time.Duration // maximum duration before timing out read of the request
    WriteTimeout   time.Duration // maximum duration before timing out write of the response
    MaxHeaderBytes int           // maximum size of request headers, DefaultMaxHeaderBytes if 0
    TLSConfig      *tls.Config   // optional TLS config, used by ListenAndServeTLS

    // TLSNextProto optionally specifies a function to take over
    // ownership of the provided TLS connection when an NPN
    // protocol upgrade has occurred.  The map key is the protocol
    // name negotiated. The Handler argument should be used to
    // handle HTTP requests and will initialize the Request&#39;s TLS
    // and RemoteAddr if not already set.  The connection is
    // automatically closed when the function returns.
    TLSNextProto map[string]func(*Server, *tls.Conn, Handler)

    // ConnState specifies an optional callback function that is
    // called when a client connection changes state. See the
    // ConnState type and associated constants for details.
    ConnState func(net.Conn, ConnState)

    // ErrorLog specifies an optional logger for errors accepting
    // connections and unexpected behavior from handlers.
    // If nil, logging goes to os.Stderr via the log package&#39;s
    // standard logger.
    ErrorLog *log.Logger
    // contains filtered or unexported fields
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;都是什么作用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server struct {
    Addr           string        // 监听的TCP地址，如果为空字符串会使用&amp;quot;:http&amp;quot;
    Handler        Handler       // 调用的处理器，如为nil会调用http.DefaultServeMux
    ReadTimeout    time.Duration // 请求的读取操作在超时前的最大持续时间
    WriteTimeout   time.Duration // 回复的写入操作在超时前的最大持续时间
    MaxHeaderBytes int           // 请求的头域最大长度，如为0则用DefaultMaxHeaderBytes
    TLSConfig      *tls.Config   // 可选的TLS配置，用于ListenAndServeTLS方法
    // TLSNextProto（可选地）指定一个函数来在一个NPN型协议升级出现时接管TLS连接的所有权。
    // 映射的键为商谈的协议名；映射的值为函数，该函数的Handler参数应处理HTTP请求，
    // 并且初始化Handler.ServeHTTP的*Request参数的TLS和RemoteAddr字段（如果未设置）。
    // 连接在函数返回时会自动关闭。
    TLSNextProto map[string]func(*Server, *tls.Conn, Handler)
    // ConnState字段指定一个可选的回调函数，该函数会在一个与客户端的连接改变状态时被调用。
    // 参见ConnState类型和相关常数获取细节。
    ConnState func(net.Conn, ConnState)
    // ErrorLog指定一个可选的日志记录器，用于记录接收连接时的错误和处理器不正常的行为。
    // 如果本字段为nil，日志会通过log包的标准日志记录器写入os.Stderr。
    ErrorLog *log.Logger
    // 内含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Server
func (s *Server) SetKeepAlivesEnabled(v bool)
func (srv *Server) Serve(l net.Listener) error
func (srv *Server) ListenAndServe() error
func (srv *Server) ListenAndServeTLS(certFile, keyFile string) error

func (*Server) SetKeepAlivesEnabled
func (s *Server) SetKeepAlivesEnabled(v bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SetKeepAlivesEnabled控制是否允许HTTP闲置连接重用（keep-alive）功能。默认该功能总是被启用的。只有资源非常紧张的环境或者服务端在关闭进程中时，才应该关闭该功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Server) Serve
func (srv *Server) Serve(l net.Listener) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Serve会接手监听器l收到的每一个连接，并为每一个连接创建一个新的服务go程。该go程会读取请求，然后调用srv.Handler回复请求。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Server) ListenAndServe
func (srv *Server) ListenAndServe() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenAndServe监听srv.Addr指定的TCP地址，并且会调用Serve方法接收到的连接。如果srv.Addr为空字符串，会使用&amp;rdquo;:http&amp;rdquo;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Server) ListenAndServeTLS
func (srv *Server) ListenAndServeTLS(certFile, keyFile string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenAndServeTLS监听srv.Addr确定的TCP地址，并且会调用Serve方法处理接收到的连接。必须提供证书文件和对应的私钥文件。如果证书是由权威机构签发的，certFile参数必须是顺序串联的服务端证书和CA证书。如果srv.Addr为空字符串，会使用&amp;rdquo;:https&amp;rdquo;。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Prometheus入门</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus/</link>
          <pubDate>Thu, 29 Jun 2017 16:31:54 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus/</guid>
          <description>&lt;p&gt;Prometheus，它最早是借鉴了 Google 的 Borgmon 系统，完全是开源的，也是CNCF 下继 K8S 之后第二个项目。它们的开发人员都是原 Google 的 SRE，通过 HTTP 的方式来做数据收集，对其最深远的应该是其被设计成一个 self sustained 的系统，也就是说它是完全独立的系统，不需要外部依赖。&lt;/p&gt;

&lt;h1 id=&#34;时序数据库的发展&#34;&gt;时序数据库的发展&lt;/h1&gt;

&lt;h2 id=&#34;时序数据&#34;&gt;时序数据&lt;/h2&gt;

&lt;p&gt;时序数据的种类：常规和不规则。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;开发人员比较常见和熟悉的是常规时间序列，它只在规定的时间间隔内进行测量，如每10秒钟一次，通常会发生在传感器中，定期读取数据。常规时间序列代表了一些基本的原始事件流或分发。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不规则时间序列则对应离散事件，主要是针对API，例如股票交易。如果要以1分钟间隔计算API的平均响应时间，可以聚合各个请求以生成常规时间序列。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;关系型数据库和nosql&#34;&gt;关系型数据库和nosql&lt;/h2&gt;

&lt;p&gt;使用mysql或者分布式数据库cassandra等，数据频繁插入操作，数据量很大，查询困难，还需要不停的进行分区分表，在应用级获取的时候需要要大量的代码控制，所以需要一个时序数据库。&lt;/p&gt;

&lt;p&gt;nosql可以很好的处理的大规模数据的处理查询，但是缺乏规范的sql，现在虽然每种nosql都得到了广泛的应用，但是其实都是缓存数据库的思想，每中nosql都要有自己学习的成本，当然这个并不是使用时序数据库的理由，相反，缓存数据库在很多场景下都是得到的重用，但是针对一些特殊场景，比如以时间为主轴的数据，观察变化趋势的，优化后的时序数据库则拥有了更好的数据存储处理查询能力&lt;/p&gt;

&lt;p&gt;时间序列数据跟关系型数据库有太多不同，但是很多公司并不想放弃关系型数据库。于是就产生了一些特殊的用法，比如：用 MySQL 的 VividCortex, 用 Postgres 的 TimescaleDB；当然，还有人依赖K-V、NoSQL数据库或者列式数据库的，比如：OpenTSDB的HBase，而Druid则是一个不折不扣的列式存储系统；更多人觉得特殊的问题需要特殊的解决方法，于是很多时间序列数据库从头写起，不依赖任何现有的数据库, 比如： Graphite，InfluxDB。&lt;/p&gt;

&lt;p&gt;时序数据库基本上是基于缓存（nosql思想）的基础上处理大规模的数据，并且在一些场景，比如以时间为主轴的数据变化趋势：自动驾驶，交易，监控等行业，就需要时序数据库进行大规模的数据处理，用于跟踪历史数据。&lt;/p&gt;

&lt;p&gt;现在生活中时序的场景很多很多，所以时序数据库很受需要，已经成为发展最快的一种数据库。&lt;/p&gt;

&lt;p&gt;下面我们来全面对比一下关系数据库和时序数据库&lt;/p&gt;

&lt;p&gt;时序数据库&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;数据写入&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;时间是一个主坐标轴，数据通常按照时间顺序抵达&lt;/li&gt;
&lt;li&gt;大多数测量是在观察后的几秒或几分钟内写入的，抵达的数据几乎总是作为新条目被记录&lt;/li&gt;
&lt;li&gt;95％到99％的操作是写入，有时更高&lt;/li&gt;
&lt;li&gt;更新几乎没有&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据读取&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;随机位置的单个测量读取、删除操作几乎没有&lt;/li&gt;
&lt;li&gt;读取和删除是批量的，从某时间点开始的一段时间内&lt;/li&gt;
&lt;li&gt;时间段内读取的数据有可能非常巨大&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据存储&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据结构简单，价值随时间推移迅速降低&lt;/li&gt;
&lt;li&gt;通过压缩、移动、删除等手段降低存储成本&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而关系数据库主要应对的数据特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据写入：大多数操作都是DML操作，插入、更新、删除等；&lt;/li&gt;
&lt;li&gt;数据读取：读取逻辑一般都比较复杂；&lt;/li&gt;
&lt;li&gt;数据存储：很少压缩，一般也不设置数据生命周期管理。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;针对这些特点，致使我们使用时序数据库，我们来看一下需要使用时序数据库的主要的特点&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基本上都是插入，没有更新的需求。&lt;/li&gt;
&lt;li&gt;数据基本上都有时间属性，随着时间的推移不断产生新的数据。&lt;/li&gt;
&lt;li&gt;数据量大，每秒钟需要写入千万、上亿条数据&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为什么要使用时序数据库？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;因为数据量大，并且大部分都是写入的要求，并且要求性能特别高，并且有时间属性。这类数据使用时序数据库的特殊处理方式（以缓存为基础，以时间为主轴来存储数据），比较快捷高效&lt;/li&gt;
&lt;li&gt;数据重复性特别大，使用压缩来降低存储成本。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;时序数据库&#34;&gt;时序数据库&lt;/h2&gt;

&lt;h3 id=&#34;基本概念&#34;&gt;基本概念&lt;/h3&gt;

&lt;p&gt;一些基本概念(不同的时序数据库称呼略有不同)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Metric:  度量，相当于关系型数据库中的 table。&lt;/li&gt;
&lt;li&gt;Data point:  数据点，相当于关系型数据库中的 row。&lt;/li&gt;
&lt;li&gt;Timestamp：时间戳，代表数据点产生的时间。&lt;/li&gt;
&lt;li&gt;Field:  度量下的不同字段。比如位置这个度量具有经度和纬度两个 field。一般情况下存放的是随时间戳而变化的数据。&lt;/li&gt;
&lt;li&gt;Tag:  标签。一般存放的是不随时间戳变化的信息。timestamp 加上所有的 tags 可以视为 table 的 primary key。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如采集有关风的数据，度量为 Wind，每条数据都有时间戳timestamp，两个字段 field：direction(风向)、speed(风速)，两个tag：sensor(传感器编号)、city(城市)。&lt;/p&gt;

&lt;p&gt;业务方常见需求&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取最新状态，查询最近的数据(例如传感器最新的状态)&lt;/li&gt;
&lt;li&gt;展示区间统计，指定时间范围，查询统计信息，例如平均值，最大值，最小值，计数等。。。&lt;/li&gt;
&lt;li&gt;获取异常数据，根据指定条件，筛选异常数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;常见业务场景&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;监控软件系统： 虚拟机、容器、服务、应用&lt;/li&gt;
&lt;li&gt;监控物理系统： 水文监控、制造业工厂中的设备监控、国家安全相关的数据监控、通讯监控、传感器数据、血糖仪、血压变化、心率等&lt;/li&gt;
&lt;li&gt;资产跟踪应用： 汽车、卡车、物理容器、运货托盘&lt;/li&gt;
&lt;li&gt;金融交易系统： 传统证券、新兴的加密数字货币&lt;/li&gt;
&lt;li&gt;事件应用程序： 跟踪用户、客户的交互数据&lt;/li&gt;
&lt;li&gt;商业智能工具： 跟踪关键指标和业务的总体健康情况&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在互联网行业中，也有着非常多的时序数据，例如用户访问网站的行为轨迹，应用程序产生的日志数据等等。&lt;/p&gt;

&lt;h3 id=&#34;主流时序数据库&#34;&gt;主流时序数据库&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;influxdb，opentsdb，Graphite，prometheus，HiTSDB，LinDB
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;InfluxDB：很多公司都在用，包括饿了么有部分监控系统也是用的InfluxDB。其优点在于支持多维和多字段，存储也根据TSDB的特点做了优化，不过开源的部分并不支持。很多公司自己做集群化，但大多基于指标名来，这样就会有单指的热点问题。现在饿了么也是类似的做法，但热点问题很严重，大的指标已经用了最好的服务器，可查询性能还是不够理想，如果做成按Series Sharding，那成本还是有一点高；&lt;/li&gt;
&lt;li&gt;Graphite：根据指标写入及查询，计算函数很多，但很难支持多维，包括机房或多集群的查询。原来饿了么把业务层的监控指标存储在Graphite中，并工作的很好，不过多活之后基本已经很难满足一些需求了，由于其存储结构的特点，很占IO，根据目前线上的数据写放大差不多几十倍以上；&lt;/li&gt;
&lt;li&gt;OpenTSDB：基于HBase，优点在于存储层不用自己考虑，做好查询聚合就可以，也会存在HBase的热点问题等。在以前公司也用基于HBase实现的TSDB来解决OpenTSDB的一些问题， 如热点、部分查询聚合下放到HBase等，目的是优化其查询性能，但依赖HBase/HDFS还是很重；&lt;/li&gt;
&lt;li&gt;HiTSDB：阿里提供的TSDB，存储也是用HBase，在数据结构及Index上面做了很多优化，具体没有研究。&lt;/li&gt;
&lt;li&gt;LinDB：饿了么轻量级分布式时序数据库，基础组件如下

&lt;ul&gt;
&lt;li&gt;LinProxy主要做一些SQL的解析及一些中间结合的再聚合计算，如果不是跨集群，LinProxy可以不需要，对于单集群的每个节点都内嵌了一个LinProxy来提供查询服务；&lt;/li&gt;
&lt;li&gt;LinDB Client主要用于数据的写入，也有一些查询的API；&lt;/li&gt;
&lt;li&gt;LinStorage的每个节点组成一个集群，节点之间进行复制，并有副本的Leader节点提供读写服务，这点设计主要是参考Kafka的设计，可以把LinDB理解成类Kafka的数据写入复制+底层时间序列的存储层；&lt;/li&gt;
&lt;li&gt;LinMaster主要负责database、shard、replica的分配，所以LinStorage存储的调度及MetaData（目前存储Zookeeper中）的管理；由于LinStorage Node都是对等的，所以我们基于Zookeeper在集群的节点选一个成为Master，每个Node把自身的状态以心跳的方式上报到Master上，Master根据这些状态进行调度，如果Master挂了，自动再选一个Master出来，这个过程基本对整个服务是无损的，所以用户基本无感知。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;prometheus&#34;&gt;prometheus&lt;/h1&gt;

&lt;h2 id=&#34;安装编译&#34;&gt;安装编译&lt;/h2&gt;

&lt;p&gt;可以通过源码编译也可以通过下载二进制包，还可以通过docker启动，如果是源码编译很简单，clone下代码make build一下就行，会产生二进制文件prometheus，&lt;/p&gt;

&lt;p&gt;下载tar包二进制文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar xvfz prometheus-*.tar.gz
cd prometheus-*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./prometheus --config.file=prometheus.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;常用启动参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--storage.tsdb.path指定的路径存储文件，默认为./data
--web.listen-address=0.0.0.0:9090 指定监听的ip和端口
--config.file=/opt/prometheus-2.4.2.linux-amd64-k8s/prometheus.yml 指定启动的配置文件
--storage.tsdb.retention=10d 指定数据存储时间
--log.level=info 指定日志等级
--query.max-concurrency=2000 指定查询并发数量
--web.max-connections=4096 指定连接数
--web.read-timeout=40s 界面查询超时时间
--query.timeout=40s 指定查询超时时间
--query.lookback-delta=3600s 查询最长多少时间范围内的点
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;全部启动参数：2.4.2版本的详细说明，随着升级会有对应的变化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@promessitapp05 k8s-prometheus-2.4.2.linux-amd64-k8s]# ./prometheus -h
usage: prometheus [&amp;lt;flags&amp;gt;]

The Prometheus monitoring server

Flags:
  -h, --help                     Show context-sensitive help (also try --help-long and --help-man).
      --version                  Show application version.
      --config.file=&amp;quot;prometheus.yml&amp;quot;
                                 Prometheus configuration file path.
      --web.listen-address=&amp;quot;0.0.0.0:9090&amp;quot;
                                 Address to listen on for UI, API, and telemetry.
      --web.read-timeout=5m      Maximum duration before timing out read of the request, and closing idle connections.
      --web.max-connections=512  Maximum number of simultaneous connections.
      --web.external-url=&amp;lt;URL&amp;gt;   The URL under which Prometheus is externally reachable (for example, if Prometheus is served via a reverse proxy). Used for generating relative and
                                 absolute links back to Prometheus itself. If the URL has a path portion, it will be used to prefix all HTTP endpoints served by Prometheus. If
                                 omitted, relevant URL components will be derived automatically.
      --web.route-prefix=&amp;lt;path&amp;gt;  Prefix for the internal routes of web endpoints. Defaults to path of --web.external-url.
      --web.user-assets=&amp;lt;path&amp;gt;   Path to static asset directory, available at /user.
      --web.enable-lifecycle     Enable shutdown and reload via HTTP request.
      --web.enable-admin-api     Enable API endpoints for admin control actions.
      --web.console.templates=&amp;quot;consoles&amp;quot;
                                 Path to the console template directory, available at /consoles.
      --web.console.libraries=&amp;quot;console_libraries&amp;quot;
                                 Path to the console library directory.
      --storage.tsdb.path=&amp;quot;data/&amp;quot;
                                 Base path for metrics storage.
      --storage.tsdb.retention=15d
                                 How long to retain samples in storage.
      --storage.tsdb.no-lockfile
                                 Do not create lockfile in data directory.
      --storage.remote.flush-deadline=&amp;lt;duration&amp;gt;
                                 How long to wait flushing sample on shutdown or config reload.
      --storage.remote.read-sample-limit=5e7
                                 Maximum overall number of samples to return via the remote read interface, in a single query. 0 means no limit.
      --rules.alert.for-outage-tolerance=1h
                                 Max time to tolerate prometheus outage for restoring &#39;for&#39; state of alert.
      --rules.alert.for-grace-period=10m
                                 Minimum duration between alert and restored &#39;for&#39; state. This is maintained only for alerts with configured &#39;for&#39; time greater than grace period.
      --rules.alert.resend-delay=1m
                                 Minimum amount of time to wait before resending an alert to Alertmanager.
      --alertmanager.notification-queue-capacity=10000
                                 The capacity of the queue for pending Alertmanager notifications.
      --alertmanager.timeout=10s
                                 Timeout for sending alerts to Alertmanager.
      --query.lookback-delta=5m  The delta difference allowed for retrieving metrics during expression evaluations.就是查询当前时间前多长时间的数据中最新的一个数据，当配置较小的时候，可能采集间隔过大而获取不到数据。
      --query.timeout=2m         Maximum time a query may take before being aborted.
      --query.max-concurrency=20
                                 Maximum number of queries executed concurrently.
      --log.level=info           Only log messages with the given severity or above. One of: [debug, info, warn, error]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d--name=prometheus     --publish=9090:9090-v /etc/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml     -v /var/prometheus/storage:/prometheus     prom/prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;部署&#34;&gt;部署&lt;/h2&gt;

&lt;p&gt;1、就是上面的二进制或者docker直接启动&lt;/p&gt;

&lt;p&gt;2、k8s部署&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;直接使用这个项目中的yaml文件&lt;a href=&#34;https://github.com/giantswarm/prometheus&#34;&gt;https://github.com/giantswarm/prometheus&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Prometheus Operator部署&lt;/p&gt;

&lt;p&gt;具体可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-operater/&#34;&gt;Prometheus Operator&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;配置文件&#34;&gt;配置文件&lt;/h2&gt;

&lt;p&gt;通常的配置文件如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# my global config全局配置
global:
  scrape_interval:     15s # By default, scrape targets every 15 seconds.采集频率
  evaluation_interval: 15s # By default, scrape targets every 15 seconds.规则计算的频率
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  # 给全局指标增加一个label
  external_labels:
      monitor: &#39;codelab-monitor&#39;

# Load and evaluate rules in this file every &#39;evaluation_interval&#39; seconds.
# 告警规则文件
rule_files:
  # - &amp;quot;first.rules&amp;quot;
  # - &amp;quot;second.rules&amp;quot;
  - &amp;quot;alert.rules&amp;quot;
  # - &amp;quot;record.rules&amp;quot;


#lertmanager configuration
# altermanager服务器的配置，所有的地址都要配置
alerting:
  alertmanagers:
  - static_configs:
    - targets: [&#39;10.242.182.161:9093&#39;,&#39;10.242.182.166:9093&#39;]


# A scrape configuration containing exactly one endpoint to scrape:
# Here it&#39;s Prometheus itself.
# 采集配置
scrape_configs:
  # The job name is added as a label `job=&amp;lt;job_name&amp;gt;` to any timeseries scraped from this config.
  # job的名字
  - job_name: &#39;windows-test&#39;

    # Override the global default and scrape targets from this job every 5 seconds.
    # 每个job可以单独设置采集频率，但是这个不能在label中设置，也就是说只能一个job一个采集频率，不能一个target一个采集频率
    scrape_interval: 1s

    # metrics_path defaults to &#39;/metrics&#39;，
    # 可以设置采集路经,默认是metrics，这个参数可以在label中设置
    metrics_path: /probe

    # Optional HTTP URL parameters.
    # params:
    #  [ &amp;lt;string&amp;gt;: [&amp;lt;string&amp;gt;, ...] ]
    # target的URL的请求参数，比如http://10.27.241.4:10260/metrics?all，就是k/v结构
    params:
        all: [&amp;quot;&amp;quot;]

    # 这边还有一个match的使用方法
    # 只采集job是node_exporter_1的数据。
    params:
      match[]:
        - &#39;{job=~&amp;quot;node_exporter_1&amp;quot;}&#39;

    # scheme defaults to &#39;http&#39;.
    # 可以设置http的方式，默认http，这个参数也可以在label中设置
    scheme： http

    # 静态target的配置，也可以使用其他的服务发现，但是都是job统一级别的
    static_configs:
      - targets: [&#39;192.168.3.1:9090&#39;,&#39;192.168.3.120:9090&#39;]
      # 可以直接设置采集数据的标签
        labels:
            appid : &#39;mycat&#39;



    # Sets the `Authorization` header on every scrape request with the
    # configured username and password.
    # password and password_file are mutually exclusive.
    # basic_auth:
    #  [ username: &amp;lt;string&amp;gt; ]
    #  [ password: &amp;lt;secret&amp;gt; ]
    #  [ password_file: &amp;lt;string&amp;gt; ]
    # 访问https的时候可以带上用户名和密码

    basic_auth:
      username: &amp;quot;admin&amp;quot;
      password: &amp;quot;Pwd123456&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面是默认的使用方式，使用的是static_configs直接静态配置ip，也可以使用一些服务发现来动态更新IP。&lt;/p&gt;

&lt;h3 id=&#34;服务发现&#34;&gt;服务发现&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;static_configs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;static_configs直接静态配置ip&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;文件服务发现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;file_sd_config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;node&#39;
file_sd_configs:
  - files:
    - /opt/promes/harbor-prometheus-2.4.2.linux-amd64/discoveries/node/discovery.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是使用了json文件的服务发现，可以把对应的target和label写入json文件，这边就可以使用一些模版生产工具（consul-template）来生成对应的json文件&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;kubernetes_sd_configs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;主要参考&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/monitor-scheme/k8s/#使用k8s的服务发现&#34;&gt;k8s监控方案中的prometheus in k8s的配置文件解析&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;这里我们主要关注在kubernetes下的采集目标发现的配置，Prometheus支持通过kubernetes的Rest API动态发现采集的目标Target信息，包括kubernetes下的node,service,pod,endpoints等信息。&lt;/p&gt;

&lt;p&gt;kubernets_sd_config下面role类型中的任何一个都能在发现目标上配置：&lt;/p&gt;

&lt;p&gt;节点node&lt;/p&gt;

&lt;p&gt;这个node角色发现带有地址的每一个集群节点一个目标，都指向Kublelet的HTTP端口。这个目标地址默认为Kubernetes节点对象的第一个现有地址，地址类型为NodeInernalIP, NodeExternalIP, NodeLegacyHostIP和NodeHostName。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_node_name: 节点对象的名称
__meta_kubernetes_node_label_&amp;lt;labelname&amp;gt;: 节点对象的每个标签
__meta_kubernetes_node_annotation_&amp;lt;annotationname&amp;gt;: 节点对象的每个注释
_meta_kubernetes_node_address&amp;lt;address_type&amp;gt;: 如果存在，每一个节点对象类型的第一个地址
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，对于节点的instance标签，将会被设置成从API服务中获取的节点名称。&lt;/p&gt;

&lt;p&gt;服务service&lt;/p&gt;

&lt;p&gt;对于每个服务每个服务端口，service角色发现一个目标。对于一个服务的黑盒监控是通常有用的。这个地址被设置成这个服务的Kubernetes DNS域名, 以及各自的服务端口。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_namespace: 服务对象的命名空间
__meta_kubernetes_service_name: 服务对象的名称
__meta_kubernetes_service_label_&amp;lt;labelname&amp;gt;: 服务对象的标签。
__meta_kubernetes_service_annotation_&amp;lt;annotationname&amp;gt;: 服务对象的注释
__meta_kubernetes_service_port_name: 目标服务端口的名称
__meta_kubernetes_service_port_number: 目标服务端口的数量
__meta_kubernetes_service_port_portocol: 目标服务端口的协议
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note：这里Service中同样标注了 prometheus.io/scrape: ‘true’从而确保prometheus会采集数据。&lt;/p&gt;

&lt;p&gt;pod&lt;/p&gt;

&lt;p&gt;pod角色发现所有的pods，并暴露它们的容器作为目标。对于每一个容器的声明端口，单个目标被生成。 如果一个容器没有指定端口，每个容器的无端口目标都是通过relabeling手动添加端口而创建的。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_namespace: pod对象的命名空间
__meta_kubernetes_pod_name: pod对象的名称
__meta_kubernetes_pod_ip: pod对象的IP地址
__meta_kubernetes_pod_label_&amp;lt;labelname&amp;gt;: pod对象的标签
__meta_kubernetes_pod_annotation_&amp;lt;annotationname&amp;gt;: pod对象的注释
__meta_kubernetes_pod_container_name: 目标地址的容器名称
__meta_kubernetes_pod_container_port_name: 容器端口名称
__meta_kubernetes_pod_container_port_number: 容器端口的数量
__meta_kubernetes_pod_container_port_protocol: 容器端口的协议
__meta_kubernetes_pod_ready: 设置pod ready状态为true或者false
__meta_kubernetes_pod_node_name: pod调度的node名称
__meta_kubernetes_pod_host_ip: 节点对象的主机IP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;endpoints端点&lt;/p&gt;

&lt;p&gt;endpoints角色发现来自于一个服务的列表端点目标。对于每一个终端地址，一个目标被一个port发现。如果这个终端被写入到pod中，这个节点的所有其他容器端口，未绑定到端点的端口，也会被目标发现。&lt;/p&gt;

&lt;p&gt;可用的meta标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_kubernetes_namespace: 端点对象的命名空间
__meta_kubernetes_endpoints_name: 端点对象的名称
对于直接从端点列表中获取的所有目标，下面的标签将会被附加上。
__meta_kubernetes_endpoint_ready: endpoint ready状态设置为true或者false。
__meta_kubernetes_endpoint_port_name: 端点的端口名称
__meta_kubernetes_endpoint_port_protocol: 端点的端口协议

如果端点属于一个服务，这个角色的所有标签：服务发现被附加上。
对于在pod中的所有目标，这个角色的所有表掐你：pod发现被附加上
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于Kuberntes发现，看看下面的配置选项：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# The information to access the Kubernetes API.

# The API server addresses. If left empty, Prometheus is assumed to run inside
# of the cluster and will discover API servers automatically and use the pod&#39;s
# CA certificate and bearer token file at /var/run/secrets/kubernetes.io/serviceaccount/.
[ api_server: &amp;lt;host&amp;gt; ]

# The Kubernetes role of entities that should be discovered.
role: &amp;lt;role&amp;gt;

# Optional authentication information used to authenticate to the API server.
# Note that `basic_auth`, `bearer_token` and `bearer_token_file` options are
# mutually exclusive.

# Optional HTTP basic authentication information.
basic_auth:
  [ username: &amp;lt;string&amp;gt; ]
  [ password: &amp;lt;string&amp;gt; ]

# Optional bearer token authentication information.
[ bearer_token: &amp;lt;string&amp;gt; ]

# Optional bearer token file authentication information.
[ bearer_token_file: &amp;lt;filename&amp;gt; ]

# TLS configuration.
tls_config:
  [ &amp;lt;tls_config&amp;gt; ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;role&amp;gt;&lt;/code&gt;必须是endpoints, service, pod或者node。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;consul服务发现&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;consul_sd_configs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;TEST_NEW_1&#39;
    scrape_interval:     30s
    consul_sd_configs:
      - server: &#39;192.47.178.100:9996&#39;
        services: [&#39;node_exporter_1&#39;]
    relabel_configs:
    - source_labels: [&#39;__meta_consul_service&#39;]
      regex:         &#39;(.*)&#39;
      target_label:  &#39;job&#39;
      replacement:   &#39;PROMES_$1&#39;
    - source_labels: [&#39;__meta_consul_node&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;instance&#39;
      replacement:   &#39;$4&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;appId&#39;
      replacement:   &#39;$1&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;ldc&#39;
      replacement:   &#39;$2&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;env&#39;
      replacement:   &#39;$3&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;ip&#39;
      replacement:   &#39;$4&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;softType&#39;
      replacement:   &#39;$5&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;software&#39;
      replacement:   &#39;$6&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;exporter&#39;
      replacement:   &#39;$7&#39;
    - source_labels: [&#39;__meta_consul_tags&#39;]
      regex:         &#39;,(.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*),&#39;
      target_label:  &#39;exporterVersion&#39;
      replacement:   &#39;$8&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由上面的配置可见，配置consul的服务器的地址和对应的services的名字就可以匹配到api注册需要采集对应的配置。&lt;/p&gt;

&lt;p&gt;consul服务发现中支持一下内部使用的metadata：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__meta_consul_address: the address of the target
__meta_consul_dc: the datacenter name for the target
__meta_consul_tagged_address_&amp;lt;key&amp;gt;: each node tagged address key value of the target
__meta_consul_metadata_&amp;lt;key&amp;gt;: each node metadata key value of the target
__meta_consul_node: the node name defined for the target
__meta_consul_service_address: the service address of the target
__meta_consul_service_id: the service ID of the target
__meta_consul_service_metadata_&amp;lt;key&amp;gt;: each service metadata key value of the target
__meta_consul_service_port: the service port of the target
__meta_consul_service: the name of the service the target belongs to
__meta_consul_tags: the list of tags of the target joined by the tag separator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过注册tags的编号来替换对应内部专门使用的变量的值，来完成label的注册。&lt;/p&gt;

&lt;p&gt;这种注册和服务发现的模式，需要一直去请求consul的api，当数据量大的时候，会出现超时现象的性能瓶颈，影响采集的动态更新，小规模使用比较好，services还有自检的功能，但是大规模，直接使用k/v结构存储注册数据，作为数据来来使用，然后使用第三方模版工具(consul-template)生成json文件，来完成动态更新，etcd+confd也是类似的模式。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;其他还有很多服务发现，没有用过，先不做说明。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;prometheus的relabeling机制&#34;&gt;Prometheus的Relabeling机制&lt;/h3&gt;

&lt;p&gt;在Prometheus所有的Target实例中，都包含一些默认的Metadata标签信息。可以通过Prometheus UI的Targets页面中查看这些实例的Metadata标签的内容：&lt;/p&gt;

&lt;p&gt;默认情况下，当Prometheus加载Target实例完成后，这些Target时候都会包含一些默认的标签：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__address__：当前Target实例的访问地址&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;
__scheme__：采集目标服务访问地址的HTTP Scheme，HTTP或者HTTPS
__metrics_path__：采集目标服务访问地址的访问路径
__param_&amp;lt;name&amp;gt;：采集任务目标服务的中包含的请求参数
__name__是特定的label标签，代表了metric name。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这些标签将会告诉Prometheus如何从该Target实例中获取监控数据。除了这些默认的标签以外，我们还可以为Target添加自定义的标签，也就是我们平常使用的label&lt;/p&gt;

&lt;p&gt;一般来说，Target以&lt;code&gt;__&lt;/code&gt;作为前置的标签是作为系统内部使用的，因此这些标签不会被写入到样本数据中。不过这里有一些例外，例如，我们会发现所有通过Prometheus采集的样本数据中都会包含一个名为instance的标签，该标签的内容对应到Target实例的&lt;code&gt;__address__&lt;/code&gt;。 这里实际上是发生了一次标签的重写处理。&lt;/p&gt;

&lt;p&gt;这种发生在采集样本数据之前，对Target实例的标签进行重写的机制在Prometheus被称为Relabeling。&lt;/p&gt;

&lt;p&gt;Promtheus允许用户在采集任务设置中通过relabel_configs来添加自定义的Relabeling过程。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;relabel_config&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;relabel_config的作用就是将时间序列中 label 的值做一个替换，具体的替换规则有配置决定，默认 job 的值是 job_name，&lt;code&gt;__address__&lt;/code&gt;的值为&lt;code&gt;&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;，instance的值默认就是 &lt;code&gt;__address__，__param_&amp;lt;name&amp;gt;&lt;/code&gt;的值就是请求url中&lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt;的值 &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;blackbox&#39;
metrics_path: /probe
params:
  module: [http_2xx]  # Look for a HTTP 200 response.
static_configs:
  - targets: [&amp;quot;https://test.com/api/projects&amp;quot;]
relabel_configs:
  - source_labels: [__address__]
    target_label: __param_target
  - source_labels: [__param_target]
    target_label: instance
  - target_label: __address__
    replacement: 10.243.129.101:9115  # The blackbox exporter&#39;s real hostname:port.
basic_auth:
  username: &amp;quot;admin&amp;quot;
  password: &amp;quot;Pwd123456&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这个配置的意思就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;__param_target = __address__ ，&amp;lt;- https://test.com/api/projects
instance = __param_target &amp;lt;- https://test.com/api/projects
__address__ = 10.243.129.101:9115。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus最后是根据&lt;code&gt;__address__&lt;/code&gt;来作为采集的地址来拉去数据的。可以看出默认情况下，targets将地址给了&lt;code&gt;__address__&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;具体规则做一个简单的说明，其实就是relabel_action所决定的&lt;/p&gt;

&lt;p&gt;&lt;relabel_action&gt; determines the relabeling action to take:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;replace: Match regex against the concatenated source_labels. Then, set target_label to replacement, with match group references (${1}, ${2}, &amp;hellip;) in replacement substituted by their value. If regex does not match, no replacement takes place.&lt;/li&gt;
&lt;li&gt;keep: Drop targets for which regex does not match the concatenated source_labels.&lt;/li&gt;
&lt;li&gt;drop: Drop targets for which regex matches the concatenated source_labels.&lt;/li&gt;
&lt;li&gt;hashmod: Set target_label to the modulus of a hash of the concatenated source_labels.&lt;/li&gt;
&lt;li&gt;labelmap: Match regex against all label names. Then copy the values of the matching labels to label names given by replacement with match group references (${1}, ${2}, &amp;hellip;) in replacement substituted by their value.&lt;/li&gt;
&lt;li&gt;labeldrop: Match regex against all label names. Any label that matches will be removed from the set of labels.&lt;/li&gt;
&lt;li&gt;labelkeep: Match regex against all label names. Any label that does not match will be removed from the set of labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;重新贴标签的工作如下（对应每行数据）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;定义源标签列表。&lt;/li&gt;
&lt;li&gt;对于每个目标，这些标签的值与分隔符连接。&lt;/li&gt;
&lt;li&gt;正则表达式与结果字符串匹配。&lt;/li&gt;
&lt;li&gt;基于这些匹配的新值被分配给另一个标签。&lt;/li&gt;
&lt;li&gt;可以为每个刮擦配置定义多个重新标记规则。简单的将两个标签压成一个，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例看起来如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;relabel_configs:
- source_labels: [&#39;label_a&#39;, &#39;label_b&#39;]
  separator:     &#39;;&#39;
  regex:         &#39;(.*);(.*)&#39;
  replacement:   &#39;${1}-${2}&#39;
  target_label:  &#39;label_c&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这条规则用标签集转换目标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;job&amp;quot;: &amp;quot;job1&amp;quot;,
  &amp;quot;label_a&amp;quot;: &amp;quot;foo&amp;quot;,
  &amp;quot;label_b&amp;quot;: &amp;quot;bar&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;成为标签集的目标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;job&amp;quot;: &amp;quot;job1&amp;quot;,
  &amp;quot;label_a&amp;quot;: &amp;quot;foo&amp;quot;,
  &amp;quot;label_b&amp;quot;: &amp;quot;bar&amp;quot;,
  &amp;quot;label_c&amp;quot;: &amp;quot;foo-bar&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;separator&lt;/p&gt;

&lt;p&gt;意思是如果有多个&lt;code&gt;source_label([__address__,jod])&lt;/code&gt;的时候用separator去连接几个值&lt;/p&gt;

&lt;p&gt;regex&lt;/p&gt;

&lt;p&gt;意思是符合这个正则表达式的source_label会被赋值给replacement再赋值给target_label&lt;/p&gt;

&lt;p&gt;也可以在采集的时候drop掉某些label&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#如下是删除一个原来的标签
- action: labeldrop
  regex: job
- action: labeldrop
  regex: soft.*
- action: labeldrop
  regex: exporter.*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以在采集的时候不采集一类指标符合正则表达式，使用的是一个新的域标签metric_relabel_configs&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metric_relabel_configs:
- source_labels: [ __name__ ]
  regex: &#39;go.*&#39;
  action: drop
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;hashmod&#34;&gt;hashmod&lt;/h3&gt;

&lt;p&gt;hashmod是基于服务发现的基础中的一种分布式集群的实现方式，多个prometheus实例来平均分配采集任务，完成prometheus的水平扩展。&lt;/p&gt;

&lt;p&gt;可以结合lb来负载均衡，也可以来指定ip去采集对应的数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: ibmmq
metrics_path: /metrics
params:
  module: [ibm-mq]
file_sd_configs:
  - files:
    - /opt/prometheus/discoveries/discovery.json
relabel_configs:
- source_labels: [__address__]
  modulus:       3    # 0 slaves
  target_label:  __tmp_hash
  action:        hashmod
- source_labels: [__tmp_hash]
  regex:         ^2$  # This is the 2nd slave
  action:        keep
- source_labels: [__address__]
  target_label: __param_target
- source_labels: [__param_target]
  target_label: instance
- target_label: __address__
  replacement: 10.47.247.214:9115
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当relabel_config设置为hashmod时，Promtheus会根据modulus的值作为系数，计算source_labels值的hash值。&lt;/p&gt;

&lt;p&gt;根据当前Target实例&lt;strong&gt;address&lt;/strong&gt;的值以4作为系数，这样每个Target实例都会包含一个新的标签tmp_hash，并且该值的范围在1~4之间。&lt;/p&gt;

&lt;p&gt;如果relabel的操作只是为了产生一个临时变量，以作为下一个relabel操作的输入，那么我们可以使用__tmp作为标签名的前缀，通过该前缀定义的标签就不会写入到Target或者采集到的样本的标签中。&lt;/p&gt;

&lt;p&gt;上面的可以理解为&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置的第一个 souce_labels 是对同一个任务抓取目标的 LabelSet 进行预处理，具体而言就是将抓取目标地址进行 hashmod, 并将 hashmod 的值存到一个自定义字段 __tmp_hash 中。&lt;/li&gt;
&lt;li&gt;配置的第二个 souce_labels 对预处理后的抓取目标进行筛选，只选取 __tmp_hash 值满足正则匹配的，例子中 hashmod != 2 将全部被忽略。&lt;/li&gt;
&lt;li&gt;通过以上两步，就非常容易对相同 job 的抓取目标进行散列，从而抓取命中的部分。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以采用 hashmod 配置，使用同样的配置列表，将抓取目标散列到不同的 Prometheus server 中去, 从而很好实现 Prometheus 数据收集的水平扩展。&lt;/p&gt;

&lt;h3 id=&#34;远程读写&#34;&gt;远程读写&lt;/h3&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#remote_read:
#  - url: &amp;quot;http://localhost:7201/api/v1/prom/remote/read&amp;quot;
    # To test reading even when local Prometheus has the data
#    read_recent: true
#remote_write:
#  - url: &amp;quot;http://localhost:7201/api/v1/prom/remote/write&amp;quot;
#  - url: &amp;quot;http://10.47.178.80:9268/write&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是m3db的远程读写的配置，prometheus采集的数据就会直接发生到prometheus的apadter中，然后通过调用m3db的接口，将数据存储在m3db中，查询也直接在m3db中查询数据。&lt;/p&gt;

&lt;p&gt;远程读写是prometheus的一个扩展功能，prometheus自身主要是做时序数据库，关于存储提供了一个&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/#remote-storage&#34;&gt;可扩展性的方案&lt;/a&gt;，可以自己实现，目前已经有很多项目支持prometheus远程存储，比如&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/m3db/&#34;&gt;m3db&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/remotestore/cortex/&#34;&gt;cortex&lt;/a&gt;,&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM&lt;/a&gt;等，目前VM在这一块做的还是比较好的。&lt;/p&gt;

&lt;h3 id=&#34;支持密钥文件校验-也可以跳过密钥校验&#34;&gt;支持密钥文件校验，也可以跳过密钥校验&lt;/h3&gt;

&lt;p&gt;配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: k8s-etcd
file_sd_configs:
  - files:
    - /opt/prometheus/discoveries/discovery-etcd.json
scheme: https
tls_config:
  ca_file: /opt/prometheus/ssl/etcd-ca.pem
  cert_file: /opt/prometheus/ssl/etcd.pem
  key_file:  /opt/k8s-prometheus/ssl/etcd-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;带着etcd的密钥证书去验证采集。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: k8s-other
file_sd_configs:
  - files:
    - /opt/prometheus-2.4.2.linux-amd64/discoveries/discovery-k8s.json
tls_config:
  insecure_skip_verify: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以直接跳过验证，前提是跳过验证能拉到数据。&lt;/p&gt;

&lt;h3 id=&#34;prometheus支持yml文件的服务发现实现路径重新设置&#34;&gt;prometheus支持yml文件的服务发现实现路径重新设置&lt;/h3&gt;

&lt;p&gt;I achieved this by using file_sd_config option. All targets are described in separate file(s), which can be either in YML or JSON format.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus.yml:

scrape_configs:
  - job_name: &#39;dummy&#39;  # This will be overridden in targets.yml
    file_sd_configs:
      - files:
        - targets.yml



targets.yml:

- targets: [&#39;host1:9999&#39;]
  labels:
    job: my_job
    __metrics_path__: /path1

- targets: [&#39;host2:9999&#39;]
  labels:
    job: my_job  # can belong to the same job
    __metrics_path__: /path2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;reload&#34;&gt;reload&lt;/h3&gt;

&lt;p&gt;Prometheus can reload its configuration at runtime.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kill -HUP pid
curl -X POST http://IP/-/reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus可以在运行时重新加载它的配置。 如果新配置格式不正确，则更改将不会应用。 通过向Prometheus进程发送SIGHUP或向/-/reload端点发送HTTP POST请求（启用&amp;ndash;web.enable-lifecycle标志时）来触发配置reload。 这也将重新加载任何配置的规则文件。&lt;/p&gt;

&lt;p&gt;我个人更倾向于采用 curl -X POST 的方式，因为每次 reload 过后， pid 会改变，使用 kill 方式需要找到当前进程号。
从 2.0 开始，hot reload 功能是默认关闭的，如需开启，需要在启动 Prometheus 的时候，添加 &amp;ndash;web.enable-lifecycle 参数。&lt;/p&gt;

&lt;h2 id=&#34;高级特性&#34;&gt;高级特性&lt;/h2&gt;

&lt;h3 id=&#34;prometheus分布式&#34;&gt;prometheus分布式&lt;/h3&gt;

&lt;p&gt;1、目前prometheus处理百万级的数据是完全没有问题的，也就是一千个服务器，一千个指标，以10S的频率去采集完全没有问题的，如果量级上去了，可以分业务进行多个prometheus进行采集使用，如果需要聚合，就需要使用prometheus的联邦集群，如果已经分业务但是量级还是不够，就是需要分group采集，然后聚合，其实也是分布式的概念。&lt;/p&gt;

&lt;p&gt;2、&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/thanos/&#34;&gt;thanos&lt;/a&gt;+hashmod实现分布式采集聚合查询。&lt;/p&gt;

&lt;p&gt;3、自己的想法，想开发一个类似于redis cluster的分片的集群，使用raft算法，目前并没有相关的实现方案。&lt;/p&gt;

&lt;p&gt;4、使用远程读写，比如目前性能比较优秀的&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/cluster/victoriametrics/&#34;&gt;VM&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;federation-联合&#34;&gt;FEDERATION(联合)&lt;/h3&gt;

&lt;p&gt;Federation允许一个Prometheus从另一个Prometheus中拉取某些指定的时序数据，Federation是Prometheus提供的扩展机制，允许Prometheus从一个节点扩展到多个节点，实际使用中一般会扩展成树状的层级结构。下面是Prometheus官方文档中对federation的配置示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- job_name: &#39;federate&#39;
  scrape_interval: 15s

  honor_labels: true
  metrics_path: &#39;/federate&#39;

  params:
    &#39;match[]&#39;:
      - &#39;{job=&amp;quot;prometheus&amp;quot;}&#39;
      - &#39;{__name__=~&amp;quot;job:.*&amp;quot;}&#39;

  static_configs:
    - targets:
      - &#39;source-prometheus-1:9090&#39;
      - &#39;source-prometheus-2:9090&#39;
      - &#39;source-prometheus-3:9090&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段配置所属的Prometheus将从source-prometheus-1 ~ 3这3个Prometheus的/federate端点拉取监控数据。 match[]参数指定了只拉取带有job=”prometheus标签的指标或者名称以job开头的指标。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;federation的使用&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、物理使用&lt;/p&gt;

&lt;p&gt;就是上面使用方式，将几个prometheus的数据聚合到一个prometheus中，往往就是使用几个性能差的机器来采集部分数据，然后使用性能好的来聚合，也缓解了探针连接和拉去的压力。&lt;/p&gt;

&lt;p&gt;2、k8s使用federation&lt;/p&gt;

&lt;p&gt;要实现对Kubernetes集群的监控，因为Kubernetes的rbac机制以及证书认证，当然是把Prometheus部署在Kubernetes集群上最方便。可是很多监控系统是以k8s集群外部的Prometheus为主的，grafana和告警都是使用这个外部的Prometheus，如果还需要在Kubernetes集群内部部署一个Prometheus的话一定要把它连通外部的Prometheus联合起来，好在Prometheus支持Federation。&lt;/p&gt;

&lt;p&gt;前面已经介绍了将使用Prometheus federation的形式，k8s集群外部的Prometheus从k8s集群中Prometheus拉取监控数据，外部的Prometheus才是监控数据的存储。 k8s集群中部署Prometheus的数据存储层可以简单的使用emptyDir,数据只保留24小时(或更短时间)即可，部署在k8s集群上的这个Prometheus实例即使发生故障也可以放心的让它在集群节点中漂移。&lt;/p&gt;

&lt;p&gt;federation也只能在数据量不是太大的情况下使用，如果数据量太大，聚合到prometheus中单实例还是有着各种瓶颈，并不适合后期的聚合查询使用。&lt;/p&gt;

&lt;h3 id=&#34;prometheus高可用&#34;&gt;prometheus高可用&lt;/h3&gt;

&lt;p&gt;目前prometheus解决单点故障还是使用的是多份一致数据，启动多个prometheus对同一个数据进行采集，保留多分数据，但是数据是一致的，时序数据对一致性要求不高，可以容忍数据的部分丢失，对外是一个service。&lt;/p&gt;

&lt;h3 id=&#34;adapter&#34;&gt;adapter&lt;/h3&gt;

&lt;p&gt;adapter就是一个适配器，通用的功能就是为了适配，在prometheus中有很多需要使用的地方，在&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle/#remote-storage&#34;&gt;远程存储&lt;/a&gt;中是一种使用方式，可以将数据转化到其他数据库适配的格式发送到对应的数据库中，还可以转换适配其他一些应用，还有我们使用的&lt;a href=&#34;https://github.com/DirectXMan12/k8s-prometheus-adapter&#34;&gt;k8s-prometheus-adapter&lt;/a&gt;也是一种&lt;a href=&#34;https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-autoscaler/#基于prometheus&#34;&gt;方式&lt;/a&gt;，用于k8s重prometheus拉去指标。&lt;/p&gt;

&lt;h2 id=&#34;监控方案选择&#34;&gt;监控方案选择&lt;/h2&gt;

&lt;p&gt;一直纠结于选择Prometheus还是Open-falcon。这两者都是非常棒的新一代监控解决方案，后者是小米公司开源的，目前包括小米、金山云、美团、京东金融、赶集网等都在使用Open-Falcon，最大区别在于前者采用的是pull的方式获取数据，后者使用push的方式，暂且不说这两种方式的优缺点。简单说下我喜欢Prometheus的原因：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;开箱即用，部署运维非常方便&lt;/li&gt;
&lt;li&gt;prometheus的社区非常活跃&lt;/li&gt;
&lt;li&gt;自带服务发现功能&lt;/li&gt;
&lt;li&gt;简单的文本存储格式，进行二次开发非常方便。&lt;/li&gt;
&lt;li&gt;最重要的一点，他的报警插件我非常喜欢，带有分组、报警抑制、静默提醒机制。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里并没有贬低open-falcon的意思，还是那句老话适合自己的才是最好的。&lt;/p&gt;

&lt;h2 id=&#34;prometheus二次开发项目&#34;&gt;prometheus二次开发项目&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/yunlzheng/prometheus-pusher&#34;&gt;prometheus改造&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;使用总结&#34;&gt;使用总结&lt;/h2&gt;

&lt;p&gt;1、 正确关闭Prometheus有助于降低启动延迟的风险。那你怎么做的？&lt;/p&gt;

&lt;p&gt;如果没有干净地关闭普罗米修斯理论上应该能够在启动时正常恢复，但是它可能需要更长的时间，或者你可能会在软件堆栈的某处遇到一个模糊的错误，这会导致问题。因此，最好让普罗米修斯自己一个个关闭对应程序，直接使用kill pid，不要加-9，然后等待停止所需的时间，这通常不会花费太多时间。&lt;/p&gt;

&lt;p&gt;2、 prometheus只支持数值，可以为正可以为负，字符串只能作为标签。&lt;/p&gt;

&lt;p&gt;在Prometheus的世界里面，所有的数值都是64bit的。每条时间序列里面记录的其实就是64bit timestamp(时间戳) + 64bit value(采样值)。&lt;/p&gt;

&lt;p&gt;3、Prometheus有着非常高效的时间序列数据存储方法，每个采样数据仅仅占用3.5byte左右空间，上百万条时间序列，30秒间隔，保留60天，大概花了200多G（引用官方PPT）&lt;/p&gt;

&lt;p&gt;我们实际环境中，Node Exporter 有 251 个测量点，Prometheus 服务本身有 775 个测量点。每一千个时间序列大约需要 1M 内存。每条数据占用了1K的空间，可见加了很多标签在里面，数据量还是很可观的。&lt;/p&gt;

&lt;p&gt;4、metrics&lt;/p&gt;

&lt;p&gt;指标名称只能由ASCII字符、数字、下划线以及冒号组成并必须符合正则表达式&lt;code&gt;[a-zA-Z:][a-zA-Z0-9_:]*&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;标签的名称只能由ASCII字符、数字以及下划线组成并满足正则表达式&lt;code&gt;[a-zA-Z_][a-zA-Z0-9_]*&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;其中以&lt;code&gt;__&lt;/code&gt;作为前缀的标签，是系统保留的关键字，只能在系统内部使用。标签的值则可以包含任何Unicode编码的字符。在Prometheus的底层实现中指标名称实际上是以&lt;code&gt;__name__=&amp;lt;metric name&amp;gt;&lt;/code&gt;的形式保存在数据库中的，因此以下两种方式均表示的同一条time-series：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;api_http_requests_total{method=&amp;quot;POST&amp;quot;, handler=&amp;quot;/messages&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等同于：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{__name__=&amp;quot;api_http_requests_total&amp;quot;，method=&amp;quot;POST&amp;quot;, handler=&amp;quot;/messages&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pro将所有数据保存为timeseries data，用metric name和label区分，label是在metric name上的更细维度的划分，其中的每一个实例是由一个float64和timestamp组成，只不过timestamp是隐式加上去的，有时候不会显示出来，如下面所示(数据来源于pro暴露的监控数据，访问&lt;a href=&#34;http://localhost:9090/metrics&#34;&gt;http://localhost:9090/metrics&lt;/a&gt; 可得），其中go_gc_duration_seconds是metrics name,quantile=&amp;ldquo;0.5&amp;rdquo;是key-value pair的label，而后面的值是float64 value。
pro为了方便client library的使用提供了四种数据类型： Counter, Gauge, Histogram, Summary, 简单理解就是Counter对数据只增不减，Gauage可增可减，Histogram,Summary提供跟多的统计信息。下面的实例中注释部分# TYPE go_gc_duration_seconds summary 标识出这是一个summary对象。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile=&amp;quot;0.5&amp;quot;} 0.000107458
go_gc_duration_seconds{quantile=&amp;quot;0.75&amp;quot;} 0.000200112
go_gc_duration_seconds{quantile=&amp;quot;1&amp;quot;} 0.000299278
go_gc_duration_seconds_sum 0.002341738
go_gc_duration_seconds_count 18
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 107
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在我们的使用场景中，大部分监控使用Counter来记录，例如接口请求次数、消息队列数量、重试操作次数等。比较推荐多使用Counter类型采集，因为Counter类型不会在两次采集间隔中间丢失信息。&lt;/p&gt;

&lt;p&gt;一小部分使用Gauge，如在线人数、协议流量、包大小等。Gauge模式比较适合记录无规律变化的数据，而且两次采集之间可能会丢失某些数值变化的情况。随着时间周期的粒度变大，丢失关键变化的情况也会增多。&lt;/p&gt;

&lt;p&gt;还有一小部分使用Histogram和Summary，用于统计平均延迟、请求延迟占比和分布率。另外针对Historgram，不论是打点还是查询对服务器的CPU消耗比较高，通过查询时查询结果的返回耗时会有十分直观的感受。&lt;/p&gt;

&lt;p&gt;5、PromQL&lt;/p&gt;

&lt;p&gt;直接通过类似于PromQL表达式httprequeststotal查询时间序列时，返回值中只会包含该时间序列中的最新的一个样本值，这样的返回结果我们称之为瞬时向量。而相应的这样的表达式称之为瞬时向量表达式。&lt;/p&gt;

&lt;p&gt;而如果我们想过去一段时间范围内的样本数据时，我们则需要使用区间向量表达式。区间向量表达式和瞬时向量表达式之间的差异在于在区间向量表达式中我们需要定义时间选择的范围，时间范围通过时间范围选择器[]进行定义。例如，通过以下表达式可以选择最近5分钟内的所有样本数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http_request_total{}[5m]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对比&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http_request_total{} # 瞬时向量表达式，选择当前最新的数据
http_request_total{}[5m] # 区间向量表达式，选择以当前时间为基准，5分钟内的数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、http api&lt;/p&gt;

&lt;p&gt;Prometheus API使用了JSON格式的响应内容。 当API调用成功后将会返回2xx的HTTP状态码。&lt;/p&gt;

&lt;p&gt;反之，当API调用失败时可能返回以下几种不同的HTTP状态码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;404 Bad Request：当参数错误或者缺失时。

422 Unprocessable Entity 当表达式无法执行时。

503 Service Unavailiable 当请求超时或者被中断时。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所有的API请求均使用以下的JSON格式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;status&amp;quot;: &amp;quot;success&amp;quot; | &amp;quot;error&amp;quot;,
  &amp;quot;data&amp;quot;: &amp;lt;data&amp;gt;,
​
  // Only set if status is &amp;quot;error&amp;quot;. The data field may still hold
  // additional data.
  &amp;quot;errorType&amp;quot;: &amp;quot;&amp;lt;string&amp;gt;&amp;quot;,
  &amp;quot;error&amp;quot;: &amp;quot;&amp;lt;string&amp;gt;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;瞬时数据查询&lt;/p&gt;

&lt;p&gt;通过使用QUERY API我们可以查询PromQL在特定时间点下的计算结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /api/v1/query
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;URL请求参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query=：PromQL表达式。

time=：用于指定用于计算PromQL的时间戳。可选参数，默认情况下使用当前系统时间。

timeout=：超时设置。可选参数，默认情况下使用-query,timeout的全局设置。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如使用以下表达式查询表达式up在时间点2015-07-01T20:10:51.781Z的计算结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &#39;http://localhost:9090/api/v1/query?query=up&amp;amp;time=2015-07-01T20:10:51.781Z&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;区间数据查询&lt;/p&gt;

&lt;p&gt;使用QUERY_RANGE API我们则可以直接查询PromQL表达式在一段时间返回内的计算结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GET /api/v1/query_range
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;URL请求参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query=: PromQL表达式。

start=: 起始时间。

end=: 结束时间。

step=: 查询步长。

timeout=: 超时设置。可选参数，默认情况下使用-query,timeout的全局设置。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当使用QUERY_RANGE API查询PromQL表达式时，返回结果一定是一个区间向量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;resultType&amp;quot;: &amp;quot;matrix&amp;quot;,
  &amp;quot;result&amp;quot;: &amp;lt;value&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要注意的是，在QUERY_RANGE API中PromQL只能使用瞬时向量选择器类型的表达式。&lt;/p&gt;

&lt;p&gt;7、sum&lt;/p&gt;

&lt;p&gt;sum_over_time(range-vector): 范围向量内每个度量指标的求和值。&lt;/p&gt;

&lt;p&gt;sum不能用于时间范围的求和，只能用于不同维度之间的求和&lt;/p&gt;

&lt;p&gt;8、编码方式和压缩比&lt;/p&gt;

&lt;p&gt;prometheus目前提供了三种算法(主要是为了压缩数据)用于块的编码,可以通过-storage.local.chunk-encoding-version进行配置.参数的有效值为0,1,2.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk-encoding为0时,采用的是一种叫做delta encoding的算法.早期的prometheus存储层用的就是该实现.&lt;/li&gt;
&lt;li&gt;chunk-encoding为1时,是一种改进型的double-delta encoding算法,目前的额prometheus默认使用该编码方式.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这两种编码方式对每个块使用固定的字节长度,这样有利于随机读取.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chunk-encoding为2时,使用的则是可变长的编码方式.这种编码比起上面两种方式,特点在于牺牲压缩速度换取了压缩率.facebook的时间序列数据库Beringei采用的编码方式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面展示了压缩同样大小的数据对比(文档说样本很大,但没说具体多少):&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;编码类型&lt;/th&gt;
&lt;th&gt;压缩后样本大小&lt;/th&gt;
&lt;th&gt;所用时间&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3.3bytes&lt;/td&gt;
&lt;td&gt;2.9s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1.3bytes&lt;/td&gt;
&lt;td&gt;4.9s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;测试:&lt;/p&gt;

&lt;p&gt;官方给出在生产环境中,每个样本加上索引信息后的大小一般为3-4bytes,我们可以做下测试看看实际的样本有多大,因为数据文件是经过处理后写入磁盘的,所以没办法查看单个样本的大小,只能采集一段时间的数据后计算.&lt;/p&gt;

&lt;p&gt;测试的监控目标的有两个,一个是prometheus本身的信息,一个是node-exporter输出的硬件数据,我们的分别访问host:port/metrics获取采集到的数据内容.在这个例子中,每进行一次采集,prometheus server就会取回145756 bytes的数据.(即访问两个/metrics接口返回的数据相加)&lt;/p&gt;

&lt;p&gt;五次测试得出的结果为:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;用时&lt;/th&gt;
&lt;th&gt;抓取频率&lt;/th&gt;
&lt;th&gt;数据变化量(bytes)&lt;/th&gt;
&lt;th&gt;原始大小(bytes)&lt;/th&gt;
&lt;th&gt;压缩率&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;第一次&lt;/td&gt;
&lt;td&gt;10min&lt;/td&gt;
&lt;td&gt;5s +1003520&lt;/td&gt;
&lt;td&gt;17490720&lt;/td&gt;
&lt;td&gt;94%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第二次&lt;/td&gt;
&lt;td&gt;20min&lt;/td&gt;
&lt;td&gt;5s +1597440&lt;/td&gt;
&lt;td&gt;34981440&lt;/td&gt;
&lt;td&gt;95%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第三次&lt;/td&gt;
&lt;td&gt;155min&lt;/td&gt;
&lt;td&gt;5s +4243456&lt;/td&gt;
&lt;td&gt;271106160&lt;/td&gt;
&lt;td&gt;98%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第四次&lt;/td&gt;
&lt;td&gt;10min&lt;/td&gt;
&lt;td&gt;1s +1658880&lt;/td&gt;
&lt;td&gt;17490720&lt;/td&gt;
&lt;td&gt;90%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;第五次&lt;/td&gt;
&lt;td&gt;20min&lt;/td&gt;
&lt;td&gt;1s +3481600&lt;/td&gt;
&lt;td&gt;34981440&lt;/td&gt;
&lt;td&gt;90%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;按照抓取频率5s,压缩率90%进行粗略估算.&lt;/p&gt;

&lt;p&gt;假设检测的数据为系统的硬件指标,即node-exporter的输出(145756个字节),且集群中有10台机器,那么24个小时的数据量将不超过200m.假设监控数据保留1个月,那么大概需要6-7G左右的空间&lt;/p&gt;

&lt;p&gt;9、内存使用&lt;/p&gt;

&lt;p&gt;prometheus在内存里保存了最近使用的chunks，具体chunks的最大个数可以通过storage.local.memory-chunks来设定，默认值为1048576，即1048576个chunk，大小为1G。 除了采用的数据，prometheus还需要对数据进行各种运算，因此整体内存开销肯定会比配置的local.memory-chunks大小要来的大，因此官方建议要预留3倍的local.memory-chunks的内存大小。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;As a rule of thumb, you should have at least three times more RAM available than needed by the memory chunks alone
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过server的metrics去查看prometheus_local_storage_memory_chunks以及process_resident_memory_byte两个指标值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.prometheus_local_storage_memory_chunks

    The current number of chunks in memory, excluding cloned chunks 目前内存中暴露的chunks的个数

2.process_resident_memory_byte

    Resident memory size in bytes 驻存在内存的数据大小

3.prometheus_local_storage_persistence_urgency_score 介于0-1之间，当该值小于等于0.7时，prometheus离开rushed模式。 当大于0.8的时候，进入rushed模式

4.prometheus_local_storage_rushed_mode 1表示进入了rushed mode，0表示没有。进入了rushed模式的话，prometheus会利用storage.local.series-sync-strategy以及storage.local.checkpoint-interval的配置加速chunks的持久化。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监测当前使用的内存量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_chunks
process_resident_memory_bytes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;监测当前使用的存储指标：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prometheus_local_storage_memory_series: 时间序列持有的内存当前块数量
prometheus_local_storage_memory_chunks: 在内存中持久块的当前数量


prometheus_local_storage_chunks_to_persist: 当前仍然需要持久化到磁盘的的内存块数量
prometheus_local_storage_persistence_urgency_score: 紧急程度分数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10、prometheus的target采用的是长连接的方式，会和target的机器端口一直保持连接。&lt;/p&gt;

&lt;p&gt;11、一般我们可以使用prometheus_egine_query_duration_seconds来评估prometheus整体的响应时间，如果响应过慢，可能是promql使用不当造成的，比如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大量使用join来组合指标或者增加label&lt;/li&gt;
&lt;li&gt;大范围时间查询，step很小，导致数据量很大&lt;/li&gt;
&lt;li&gt;rate时，range duration要大于step，否则会丢失数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;12、wal中文件太多，句柄不够用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level=error ts=2019-07-05T02:29:56.706Z caller=main.go:717 err=&amp;quot;opening storage failed: read WAL: open WAL segments: open segment:00020174 in dir:/data/wal: open /data/wal/00020174: too many open files&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;wal中文件太多，句柄不够用，需要打开句柄，句柄不够用可能导致压缩block出错，报错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level=error ts=2019-07-05T01:58:01.826Z caller=main.go:717 err=&amp;quot;opening storage failed: block dir: \&amp;quot;/data/01DEN382CDGHQR91QKNDHT77M8\&amp;quot;: open /data/01DEN382CDGHQR91QKNDHT77M8/meta.json: no such file or directory&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以需要在机器使用之前设置一下参数&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;锁定内存&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;logMessage &amp;quot;lock mem&amp;quot;
echo &amp;quot;esadmin hard memlock unlimited&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf
echo &amp;quot;esadmin soft memlock unlimited&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;修改最大文件描述数&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;logMessage &amp;quot;file description &amp;quot;
echo &amp;quot;esadmin soft nofile 65536&amp;quot;  &amp;gt;&amp;gt;/etc/security/limits.conf
echo &amp;quot;esadmin hard nofile 131072&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf

#修改最大线程数
logMessage &amp;quot;max thread size &amp;quot;
echo &amp;quot;esadmin soft nproc 2048 &amp;quot;&amp;gt;&amp;gt; /etc/security/limits.conf
echo &amp;quot;esadmin hard nproc 4096 &amp;quot;&amp;gt;&amp;gt; /etc/security/limits.conf
echo &amp;quot;esadmin soft nproc 2048 &amp;quot;&amp;gt;&amp;gt; /etc/security/limits.d/90-nproc.conf

#修改内存映射区域最大数
logMessage &amp;quot;max mem count &amp;quot;
echo &amp;quot;vm.max_map_count=655360&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf
sysctl -p
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;函数与常用表达式&#34;&gt;函数与常用表达式&lt;/h3&gt;

&lt;h4 id=&#34;操作符&#34;&gt;操作符&lt;/h4&gt;

&lt;p&gt;或&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;up{exporterName=~&amp;quot;etcd|etcd-event&amp;quot; ,cluster_name=~&amp;quot;k8s_xingang_02&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正则匹配,全量配置.*&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;up{exporterName=~&amp;quot;etcd.*&amp;quot; ,cluster_name=~&amp;quot;k8s_xingang_02&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;函数&#34;&gt;函数&lt;/h4&gt;

&lt;p&gt;PromQL 有三个很简单的原则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;任意 PromQL 返回的结果都不是原始数据，即使查询一个具体的 Metric（如 go_goroutines），结果也不是原始数据&lt;/li&gt;
&lt;li&gt;任意 Metrics 经过 Function 计算后会丢失 &lt;code&gt;__name__ Label&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;子序列间具备完全相同的 Label/Value 键值对（可以有不同的 &lt;code&gt;__name__&lt;/code&gt;）才能进行代数运算&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;rate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;(last值-first值)/时间差s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;irate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;(last值-last前一个值)/时间戳差值
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以cpu的使用率常用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;irate(node_cpu_seconds_total{mode=&amp;quot;idle&amp;quot;,ip=~&amp;quot;$ip&amp;quot;}[2m]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;avg&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;avg 同一时间的多条数据的平均值&lt;/li&gt;
&lt;li&gt;avg_over_time(range-vector): 范围向量内同一个度量指标不同时间的多条数据的平均值。&lt;/li&gt;
&lt;li&gt;同理的还有max，min等&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;相减&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边有一个两个指标相减的问题，必须是统一维度的才能相互计算，不能直接用指标value计算，可以对指标进行sum，max，rate等计算后进行加减乘除&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;increase()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;increase(v range-vector)函数，  度量指标：last值-first值,increase的返回值类型只能是counters，主要作用是增加图表和数据的可读性，使用rate记录规则的使用率，以便持续跟踪数据样本值的变化。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;idelta()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;idelta(v range-vector)函数，输入一个范围向量，返回key: value = 度量指标： 每最后两个样本值差值。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;label_replace&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;label_replace给指标的label新生成一个指标名的指标&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;label_replace(v instant-vector, dst_label string, replacement string, src_label string, regex string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将正则表达式与标签值src_label匹配。如果匹配，则返回时间序列，标签值dst_label被替换的扩展替换。$1替换为第一个匹配子组，$2替换为第二个等。如果正则表达式不匹配，则时间序列不会更改。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;label_replace(redis_remote_replication_dest_repl_offset{},&amp;quot;destldcId&amp;quot;,&amp;quot;$1&amp;quot;, &amp;quot;ldcId&amp;quot;, &amp;quot;(.*)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;by&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当指标中的label发生变化的时候，哪怕是同一个指标名，在promethes也是两个数据，如果将变化的两条数据衔接起来，这个时候就使用by，by就是按着制订的维度来获取指标，可以摒弃不一样的label，这样就能是一条数据了，这样就可以使得时序图连接起来，例如ntp的client变更&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sum(ntp_offset{ip=~&amp;quot;$ip&amp;quot;})by(ip)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;by还可以用于表格的聚合，对于相同label的数据可以聚合在一个表格中的一条数据，所以用by获取到不通指标数据中的相同的label，就可以实现不同value的展示，但是label一样，就是一条数据。&lt;/p&gt;

&lt;p&gt;也可以sum不加by的数据可用和任何数据聚合，其实也就是聚合后少的标签可用和多的标签进行聚合。&lt;/p&gt;

&lt;p&gt;还可以使用or，当两个数据是对立的时候，一个出现另一个就不会出来。这样也能使得数据出来&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;topk(5, rate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, rate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval]))
topk(5, rate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval]))
topk(5, irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m])) or topk(5, irate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m]))
topk(5, irate(redis_command_call_duration_seconds_count{ softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m])) or topk(5, irate(redis_commands_total{softType=&amp;quot;Redis&amp;quot;,ip=&amp;quot;$ip&amp;quot;} [5m]))
sum by (cmd)( rate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or sum by (cmd) (irate(redis_command_call_duration_seconds_count{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval])) or sum by (cmd) (irate(redis_commands_total{appId=&amp;quot;$appId&amp;quot;, softType=&amp;quot;Redis&amp;quot;} [$interval]))
redis_memory_fragmentation_ratio{ip=&amp;quot;$ip&amp;quot;}  or redis_memory_used_rss_bytes{ip=&amp;quot;$ip&amp;quot;} / redis_memory_used_bytes{ip=&amp;quot;$ip&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;原理解析&#34;&gt;原理解析&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/prometheus/prometheus-principle&#34;&gt;prometheus原理解析&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>架构系列---- 并发安全</title>
          <link>https://kingjcy.github.io/post/architecture/concurrencesafe/</link>
          <pubDate>Sun, 09 Apr 2017 19:25:10 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/architecture/concurrencesafe/</guid>
          <description>&lt;p&gt;并发安全，就是多个并发体在同一段时间内访问同一个共享数据，共享数据能被正确处理。&lt;/p&gt;

&lt;h1 id=&#34;并发不安全&#34;&gt;并发不安全&lt;/h1&gt;

&lt;p&gt;最典型的案例:卖票超售&lt;/p&gt;

&lt;p&gt;设想有一家电影院，有两个售票窗口，售票员售票时候先看一下当前剩余票数是否大于0，如果大于0则售出票。&lt;/p&gt;

&lt;p&gt;此时票数剩下一张票，两个售票窗口同时来了顾客，两个售票人都看了一下剩余票数还有一张，不约而同地收下顾客的钱，余票还剩一张，但是却售出了两张票，就会出现致命的问题。&lt;/p&gt;

&lt;h1 id=&#34;如何做到并发安全&#34;&gt;如何做到并发安全&lt;/h1&gt;

&lt;p&gt;目前最最主流的办法就是加锁就行操作，其实售票的整个操作同时间内只能一个人进行，在我看来归根到底加锁其实就是让查询和售票两个步骤原子化，只能一块执行，不能被其他程序中断，让这步操作变成串行化。下面就介绍一下使查询和售票原子化的常见程序操作：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;锁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;锁的做法就是每次进入这段变量共享的程序片段，都要先获取一下锁，如果获取成功则可以继续执行，如果获取失败则阻塞，直到其他并发体把锁给释放，程序得到执行调度才可以执行下去。&lt;/p&gt;

&lt;p&gt;锁本质上就是让并发体创建一个程序临界区，临界区一次只能进去一个并发体，伪代码示意如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lock()
totalNum = getTotalNum()
if totalNum &amp;gt; 0
    # 则售出一张票
    totalNum = totalNum - 1
else
    failedToSold()
unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读锁与写锁&lt;/p&gt;

&lt;p&gt;读锁也叫共享锁，写锁也叫排它锁，锁的概念被发明了之后，人们就想着如果我很多个并发体大部分时间都是读，如果就把变量读取的时候也要建立临界区，那就有点太大题小做了。于是人们发明了读锁，一个临界区如果加上了读锁，其他并发体执行到相同的临界区都可以加上读锁，执行下去，但不能加上写锁。这样就保证了可以多个并发体并发读取而又不会互相干扰。&lt;/p&gt;

&lt;p&gt;在golang中也是提供了mutex的锁机制。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;队列&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;队列也是解决并发不安全的做法。多个并发体去获取队列里的元素，然后进行处理，这种做法和上锁其实大同小异，本质都是把并发的操作串行化，同一个数据同一个时刻只能交给一个并发体去处理,伪代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 第一个获取到队列的元素就可以进行下去
isCanSold = canSoldList.pop()
totalNum = getTotalNum()
if totalNum &amp;gt; 0
    # 则售出一张票
    totalNum = totalNum - 1
else
    failedToSold()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在golang中也提供了队列机制，也就是Goroutine 通过 channel 进行安全读写共享变量。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;CAS&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CAS（compare and swap），先比对，然后再进行交换，和数据库里的乐观锁的做法很相似。&lt;/p&gt;

&lt;p&gt;乐观锁&lt;/p&gt;

&lt;p&gt;数据库里的乐观锁并不是真的使用了锁的机制，而是一种程序的实现思路。
乐观锁的想法是，每次拿取数据再去修改的时候很乐观，认为其他人不会去修改这个数据，表另外维护一个额外版本号的字段。
查数据的时候记录下该数据的版本号，如果成功修改的话，会修改该数据的版本号，如果修改的时候版本号和查询的时候版本号不一致，则认为数据已经被修改过，会重新尝试查询再次操作。&lt;/p&gt;

&lt;p&gt;设我们表有一个user表，除了必要的字段，还有一个字段version，表如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;id  username    money   version
1   a   10  100
2   b   20  100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候我们需要修改a的余额-10元，执行事务语句如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while
    select @money = money, @version = version from user where username = a;
    if @money &amp;lt; 10
        print(&#39;余额成功&#39;)
        break
    # 扣费前的预操作
    paied()
    # 实行扣费
    update user set money = money - 10, version = version + 1 where username = a and version = @version
    # 影响条数等于1，证明执行成功
    if @@ROWCOUNT == 1
        print(&#39;扣费成功&#39;)
        break
    else
        rollback
        print(&#39;扣费失败，重新进行尝试&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;乐观锁的做法就是使用版本的形式，每次写数据的时候会比对一下最开始的版本号，如果不同则证明有问题。&lt;/p&gt;

&lt;p&gt;CAS的做法也是一样的，在代码里面的实现稍有一点不同，由于SQL每条语句都是原子性，查询对应版本号的数据再更新的这个条件是原子性的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;update user set money = money - 10, version = version + 1 where username = a and version = @version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是在代码里面两条查询和赋值两个语句不是原子性的，需要有特定的函数让cpu底层把两个操作变成一个原子操作，在go里面有atomic包支持实现，是这样实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for {
    user := getUserByName(A)
    version := user.version
    paied()
    if atomic.CompareAndSwapInt32(&amp;amp;user.version, version, version + 1) {
        user.money -= 10
    } else {
        rollback()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;atomic.CompareAndSwapInt32需要依次传入要比较变量的地址，旧变量的值，修改后变量的值，函数会判断旧变量的值是否与现在变量的地址是否相同，相同则把新变量的值写入到该变量。
CAS的好处是不需要程序去创建临界区，而是让CPU去把两个指令变成原子性操作，性能更好，但是如果变量会被频繁更改的话，重试的次数变多反而会使得效率不如加锁高。&lt;/p&gt;

&lt;p&gt;在golang中也提供了CAS机制，也就是Goroutine 通过 atomic进行安全读写共享变量。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Zabbix基本使用</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix/</link>
          <pubDate>Sat, 04 Mar 2017 17:54:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix/</guid>
          <description>&lt;p&gt;zabbix是目前各大互联网公司使用最广泛的开源监控之一,其历史最早可追溯到1998年,在业内拥有各种成熟的解决方案.&lt;/p&gt;

&lt;h1 id=&#34;网站可用性&#34;&gt;网站可用性&lt;/h1&gt;

&lt;p&gt;在软件系统的高可靠性（也称为可用性，英文描述为HA，High Available）里有个衡量其可靠性的标准——X个9，这个X是代表数字3~5。X个9表示在软件系统1年时间的使用过程中，系统可以正常使用时间与总时间（1年）之比，我们通过下面的计算来感受下X个9在不同级别的可靠性差异。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1个9：(1-90%)*365=36.5天，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是36.5天
2个9：(1-99%)*365=3.65天 ， 表示该软件系统在连续运行1年时间里最多可能的业务中断时间是3.65天
3个9：(1-99.9%)*365*24=8.76小时，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是8.76小时。
4个9：(1-99.99%)*365*24=0.876小时=52.6分钟，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是52.6分钟。
5个9：(1-99.999%)*365*24*60=5.26分钟，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是5.26分钟。
6个9：(1-99.9999%)*365*24*60*60=31秒， 示该软件系统在连续运行1年时间里最多可能的业务中断时间是31秒
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前能达到4个9就很好了。&lt;/p&gt;

&lt;h1 id=&#34;组件&#34;&gt;组件&lt;/h1&gt;

&lt;p&gt;zabbix属于CS架构,Server端基于C语言编写,相比其他语言具有一定的性能优势(在数据量不大的情况下!).Web管理端则使用了PHP. 而其client端有各种流行语言的库实现,方便使用其API&lt;/p&gt;

&lt;p&gt;在数据的存储方面,zabbix使用了关系性数据库,包括SQLite,MySQL,PostgreSQL,Oracle,DB2&lt;/p&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;yum安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;zabbix的安装比较繁琐,但也不算困难(主要是因为网上提供的资料足够多)&lt;/p&gt;

&lt;p&gt;我们需要一种关系型关系型数据库,目前提供的选择有MySQL,SQLite, PostgreSQL,Oracle,DB2&lt;/p&gt;

&lt;p&gt;接下来需要安装PHP的运行环境,Web服务器可是使用Apache或者Nginx都可以.&lt;/p&gt;

&lt;p&gt;最后一步是安装zabbix服务.&lt;/p&gt;

&lt;p&gt;完整的安装教程可以参考:&lt;a href=&#34;http://support.supermap.com.cn/DataWarehouse/WebDocHelp/icm/Appdix/Zabbix_server/Zabbix_Installation.htm&#34;&gt;zabbix安装指南&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;主要步骤&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;配置zabbix官方yum源，还有base和epel源&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install zabbix-server-mysql zabbix-get
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;初始化database&lt;/p&gt;

&lt;p&gt;导入zabbix-server-mysql包中的create.sql来初始化数据库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm -ql zabbix-server-mysql
mysql -uroot -p -Dzabbix &amp;lt; create.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以查看表了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置服务端配置文件并启动&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装web&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install httpd php php-mysql php-mbstring php-gd php-bamath php-ladp php-xml
yum install zabbix-web-mysql zabbix-web
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后访问　　&lt;a href=&#34;http://zabbix-web-ip/zabbix/setup.php进行zabbix初始化&#34;&gt;http://zabbix-web-ip/zabbix/setup.php进行zabbix初始化&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装zabbix-agent&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install zabbix-agent zabbix-sender
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置客户端端配置文件并启动&lt;/p&gt;

&lt;p&gt;服务端快速安装脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#clsn

#设置解析 注意：网络条件较好时，可以不用自建yum源
# echo &#39;10.0.0.1 mirrors.aliyuncs.com mirrors.aliyun.com repo.zabbix.com&#39; &amp;gt;&amp;gt; /etc/hosts

#安装zabbix源、aliyun YUM源
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
rpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm

#安装zabbix
yum install -y zabbix-server-mysql zabbix-web-mysql

#安装启动 mariadb数据库
yum install -y  mariadb-server
systemctl start mariadb.service

#创建数据库
mysql -e &#39;create database zabbix character set utf8 collate utf8_bin;&#39;
mysql -e &#39;grant all privileges on zabbix.* to zabbix@localhost identified by &amp;quot;zabbix&amp;quot;;&#39;

#导入数据
zcat /usr/share/doc/zabbix-server-mysql-3.0.13/create.sql.gz|mysql -uzabbix -pzabbix zabbix

#配置zabbixserver连接mysql
sed -i.ori &#39;115a DBPassword=zabbix&#39; /etc/zabbix/zabbix_server.conf

#添加时区
sed -i.ori &#39;18a php_value date.timezone  Asia/Shanghai&#39; /etc/httpd/conf.d/zabbix.conf

#解决中文乱码
yum -y install wqy-microhei-fonts
\cp /usr/share/fonts/wqy-microhei/wqy-microhei.ttc /usr/share/fonts/dejavu/DejaVuSans.ttf

#启动服务
systemctl start zabbix-server
systemctl start httpd

#写入开机自启动
chmod +x /etc/rc.d/rc.local
cat &amp;gt;&amp;gt;/etc/rc.d/rc.local&amp;lt;&amp;lt;EOF
systemctl start mariadb.service
systemctl start httpd
systemctl start zabbix-server
EOF

#输出信息
echo &amp;quot;浏览器访问 http://`hostname -I|awk &#39;{print $1}&#39;`/zabbix&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端快速部署脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#clsn

#设置解析
echo &#39;10.0.0.1 mirrors.aliyuncs.com mirrors.aliyun.com repo.zabbix.com&#39; &amp;gt;&amp;gt; /etc/hosts

#安装zabbix源、aliyu nYUM源
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
rpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm

#安装zabbix客户端
yum install zabbix-agent -y
sed -i.ori &#39;s#Server=127.0.0.1#Server=172.16.1.61#&#39; /etc/zabbix/zabbix_agentd.conf
systemctl start  zabbix-agent.service

#写入开机自启动
chmod +x /etc/rc.d/rc.local
cat &amp;gt;&amp;gt;/etc/rc.d/rc.local&amp;lt;&amp;lt;EOF
systemctl start  zabbix-agent.service
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;编译安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;系统环境&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    OS:         centos7.5
    software：  zabbix 4.0 LTS
    DBSever:    MariaDB-10.2.15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一、需要先把数据库装上，这里用到的是mariadb 二进制包安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、下载二进制包，
     官网的下载路径：
  wget http://mirrors.neusoft.edu.cn/mariadb//mariadb-10.2.15/bintar-linux-x86_64/mariadb-10.2.15-linux-x86_64.tar.gz

2、添加组和用户
  [root@node2 ~]# groupadd -r -g 306 mysql
  [root@node2 ~]# useradd -g mysql -u 306 -r mysql

3、解压mariadb二进制包到/usr/local下去
   [root@node2 ~]# tar xf mariadb-10.2.15-linux-x86_64.tar.gz -C /usr/local/

4、进入到/usr/local下面创建mysql的软连接
   [root@node2 ~]# cd /usr/local/
   [root@node2 /usr/local]# ln -s mariadb-10.2.15-linux-x86_64/ mysql

5、修改mysql的相对应的属主和属组权限
    [root@node2 /usr/local]# chown -R root.mysql mysql/

6、创建数据文件的存放路径，并修改所属组的权限为mysql
     [root@node2 ~]#   cd /app/
     [root@node2 /app]# mkdir mydata
     [root@node2 ]#  chown -R mysql.mysql  /app

7、初始化数据库，指定好数据文件的存放路径和用户
       [root@node2 ]# cd /usr/local/mysql/
       [root@node2 /usr/local/mysql/]# scripts/mysql_install_db --datadir=/app/mydata --user=mysql

8、拷贝mariadb的启动脚本到/etc/rc.d/init.d下命名为mysqld
       [root@node2 /usr/local/mysql/]# cp support-files/mysql.server /etc/rc.d/init.d/mysqld

9、把mysqld设置为开机启动
       [root@node2 /usr/local/mysql/]# chkconfig --add mysqld

10、创建mariadb的配置文件存放路径，并拷贝模版文件到这个目录下命名为my.cnf
      [root@node2 /usr/local/mysql/]# mkdir /etc/mysql
      [root@node2 /usr/local/mysql/]#cp support-files/my-large.cnf /etc/mysql/my.cnf

11、配置系统环境变量，重读配置文件让它生效
      [root@node2 /usr/local/mysql/]# vim /etc/profile.d/mysql.sh
      [root@node2 /usr/local/mysql/]#export PATH=/usr/local/mysql/bin:$PATH
      [root@node2 /usr/local/mysql/]# . /etc/profile.d/mysql.sh

12、修改mariadb的配置文件需要增加几条内容
      [root@node2 /usr/local/mysql/]# vim /etc/mysql/my.cnf
          lower_case_table_names = 1
          character-set-server = utf8
          datadir = /app/mydata
          innodb_file_per_table = on
          skip_name_resolve = o

13、启动数据库服务
      [root@node2 /usr/local/mysql/]#  service mysqld start

14、查看mariadb的服务端口是否正常监听
    [root@node2 /app]#ss -tnl
    State      Recv-Q Send-Q       Local Address:Port                      Peer Address:Port
    LISTEN     0      128                      *:52874                                *:*
    LISTEN     0      128                      *:11211                                *:*
    LISTEN     0      128                      *:111                                  *:*
    LISTEN     0      128                      *:22                                   *:*
    LISTEN     0      128              127.0.0.1:631                                  *:*
    LISTEN     0      100              127.0.0.1:25                                   *:*
    LISTEN     0      80                      :::3306                                :::*

15、数据库的安全初始操作，设置完之后就可以先创建zabbix相关的库和用户
    [root@node2 /app]#mysql_secure_installation
    [root@node2 /app]#mysql -uroot -p
16、创建zabbix库
    MariaDB [(none)]&amp;gt; create database zabbix character set utf8 collate utf8_bin;
17、给zabbix库授权并指定用户
    MariaDB [(none)]&amp;gt; grant all privileges on zabbix.* to zabbix@&#39;192.168.137.%&#39; identified by &#39;123456&#39;;

18、在另一台主机上测试用zabbix用是否能正常登陆数据库
    [root@node7 ~]#mysql -uzabbix -p123456 -h192.168.137.54
    Welcome to the MariaDB monitor.  Commands end with ; or \g.
    Your MariaDB connection id is 12
    Server version: 10.2.15-MariaDB-log MariaDB Server

    Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

    Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.

    MariaDB [(none)]&amp;gt; show databases;
    +--------------------+
    | Database           |
    +--------------------+
    | information_schema |
    | zabbix             |
    +--------------------+
    2 rows in set (0.00 sec)
    MariaDB [(none)]&amp;gt;
19、在zabbix server主机上导入zabbix自带的三个表，路径在/root/zabbix-4.0.1/database/mysql下后缀为.sql的三个文件
    [root@node6 ~/zabbix-4.0.1]#ls -l database/mysql/
    total 5816
    -rw-r--r-- 1 1001 1001 3795433 Oct 30 01:36 data.sql
    -rw-r--r-- 1 1001 1001 1978341 Oct 30 01:36 images.sql
    -rw-r--r-- 1 root root   15323 Nov 26 22:44 Makefile
    -rw-r--r-- 1 1001 1001     392 Oct 30 01:36 Makefile.am
    -rw-r--r-- 1 1001 1001   15711 Oct 30 01:36 Makefile.in
    -rw-r--r-- 1 1001 1001  140265 Oct 30 01:36 schema.sql

20、导入sql文件是有先后顺序的，先导schema.sql、images.sql、data.sql.
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456 zabbix &amp;lt; schema.sql
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456 zabbix &amp;lt; images.sql
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456 zabbix &amp;lt; data.sql

21、进到数据库里面查看zabbix库是否导入成功
    [root@node6 ~/zabbix-4.0.1/database/mysql]#mysql -uzabbix -h192.168.137.54 -p123456
    MariaDB [(none)]&amp;gt; use zabbix
    MariaDB [zabbix]&amp;gt; show tables;
    +----------------------------+
    | Tables_in_zabbix           |
    +----------------------------+
    | acknowledges               |
    | actions                    |
    | alerts                     |
    | application_discovery      |
    | application_prototype      |
    | application_template       |
    | applications               |
    | auditlog                   |
    | auditlog_details           |
    | autoreg_host               |
    | conditions                 |
    | config                     |
    | corr_condition             |
    | corr_condition_group       |
    .......
    | users                      |
    | users_groups               |
    | usrgrp                     |
    | valuemaps                  |
    | widget                     |
    | widget_field               |
    +----------------------------+
    144 rows in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;二、编译zabbix&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、安装编译环境所需要的依赖包组
    [root@node6 ~]#yum install gcc  libxml2-devel libevent-devel net-snmp net-snmp-devel  curl  curl-devel php  php-bcmath  php-mbstring mariadb mariadb-devel –y

    还需要安装一些php的依赖包后续在网页端安装zabbix时需要用到所以先提前安装好
    [root@node6 ~]#yum install php-gettext php-session php-ctype php-xmlreader php-xmlwrer php-xml php-net-socket php-gd php-mysql -y

2、安装jdk环境，装的是jdk-8u191-linux-x64.rpm的包，要不后面编译时会报Java找不到。
    [root@node6 ~]#yum -y install jdk-8u191-linux-x64.rpm

3、创建zabbix用户
    [root@node6 ~]#useradd zabbix -s /sbin/nologin

4、下载zabbix的源码包
    [root@node6 ~]#wget http://192.168.137.53/yum/zabbix/zabbix-4.0.1.tar.gz

5、解压源码包，并进入到解压后的目录里去
    [root@node6 ~]#tar xf zabbix-4.0.1.tar.gz
    [root@node6 ~]#cd zabbix-4.0.1/
    [root@node6 ~/zabbix-4.0.1]#

6、开始编译安装zabbix
    [root@node6 ~/zabbix-4.0.1./configure  \
    --prefix=/usr/local/zabbix  \
    --enable-server  \
    --enable-agent  \
    --with-mysql   \
    --with-net-snmp  \
    --with-libcurl  \
    --with-libxml2  \
    --enable-java

7、执行make install
    [root@node6 ~/zabbix-4.0.1]#make -j 2 &amp;amp;&amp;amp; make install

8、拷贝启动脚本文件到/etc/init.d目录下
    [root@node6 ~/zabbix-4.0.1]#cp misc/init.d/fedora/core/* /etc/init.d/

9、拷贝过去的脚本需要修改下目录路径，server和agent都需要改
    [root@node6 ~/zabbix-4.0.1]#vim /etc/init.d/zabbix_server
        22         BASEDIR=/usr/local
    改成：
        22         BASEDIR=/usr/local/zabbix

    agent启动脚本修改也是一样
        [root@node6 ~/zabbix-4.0.1vim /etc/init.d/zabbix_agentd
        22         BASEDIR=/usr/local
    改成：
        22         BASEDIR=/usr/local/zabbix

10、创建zabbix的日志存放路径和修改/usr/local/zabbix的所属主为zabbix
    [root@node6 ~/zabbix-4.0.1]#mkdir /var/log/zabbix
    [root@node6 ~/zabbix-4.0.1]#chown -R zabbix.zabbix /var/log/zabbix
    [root@node6 ~/zabbix-4.0.1]#ll /var/log/zabbix/ -d
    drwxr-xr-x 2 zabbix zabbix 6 Nov 27 09:17 /var/log/zabbix/
    [root@node6 ~]#chown -R zabbix.zabbix /usr/local/zabbix/
    [root@node6 ~]#ll -d /usr/local/zabbix/
    drwxr-xr-x 7 zabbix zabbix 64 Nov 26 22:45 /usr/local/zabbix/

11、修改配置文件
    [root@node6 ~/zabbix-4.0.1]#vim /usr/local/zabbix/etc/zabbix_server.conf
    ListenPort=10051   启用监听端口，不过默认也是启用的。

    LogFile=/var/log/zabbix/zabbix_server.log    修改日志存放路径，默认是在/tmp下

    LogFileSize=5   开启日志滚动，单位为MB、达到指定值之后就生成新的日志文件。
    DebugLevel=4   日志级别等级，4为debug，利于排除错误，排错之后可以改成3级别的。
    PidFile=/usr/local/zabbix/zabbix_server.pid   zabbix pid文件路径默认为tmp下需要改成安装目录，并且安装目录的所属组要改成zabbix用户
    # SocketDir=/tmp
    User=zabbix                    启动的用户默认也是zabbix,如果要改成root的话 还需要修改一项
    # AllowRoot=0                  需要改成1才能使用root来启动，默认0的话是被禁止用root启动，不过最好别用root
    SocketDir=/usr/local/zabbix   socket 文件存放路径默认在/tmp下
    DBHost=192.168.137.54          数据库地址必须要填
    DBName=zabbix                  数据库名称
    DBUser=zabbix                  数据库连接用户
    DBPassword=123456              数据库连接密码，建议在生产中密码不要太简单了。
    DBPort=3306                    数据库端口，其实也不用开默认就是3306

12、启动zabbix、并查看端口是否正常监听
    [root@node6 ~/zabbix-4.0.1]#service zabbix_server start
    Reloading systemd:                                         [  OK  ]
    Starting zabbix_server (via systemctl):                    [  OK  ]
    [root@node6 ~/zabbix-4.0.1]#ss -tnl
    State       Recv-Q Send-Q          Local Address:Port                Peer Address:Port
    LISTEN      0      128                         *:10051                    *:*
    LISTEN      0      128                         *:111                      *:*
    LISTEN      0      128                         *:22                       *:*
    LISTEN      0      100                 127.0.0.1:25                       *:*

13、装前端展示端
    [root@node6 ~/zabbix-4.0.1]#yum -y install httpd

14、在httpd的默认工作目录下创建一个zabbix目录
    [root@node6 ~/zabbix-4.0.1]#mkdir /var/www/html/zabbix

15、从zabbix解压包里面把php的所有文件拷贝到/var/www/html/zabbix目录下
    [root@node6 ~/zabbix-4.0.1]#cp -a frontends/php/* /var/www/html/zabbix/

16、启动httpd、查看端口是否正常监听
    [root@node6 ~]#systemctl start httpd
    [root@node6 ~]#ss -tnl
    State       Recv-Q Send-Q          Local Address:Port                         Peer Address:Port
    LISTEN      0      128                         *:10051                                   *:*
    LISTEN      0      128                         *:111                                     *:*
    LISTEN      0      128                         *:22                                      *:*
    LISTEN      0      100                 127.0.0.1:25                                      *:*
    LISTEN      0      128                        :::111                                    :::*
    LISTEN      0      128                        :::80                                     :::*

17、通过网页来安装zabbix
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;zabbix使用&#34;&gt;zabbix使用&lt;/h1&gt;

&lt;p&gt;zabbix的使用基本上都是在界面完成操作的，比较简单，基本使用流程&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;添加需要监控的主机&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;为监控的主机添加监控项，也就是key，zabbix自身带有很多设定好的监控项，直接选择就好，比如cpu，内存，都是界面操作&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监控项中可以直接输入参数，来获取指定的数据&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;监控项可以自定义key，主要设定key，和执行的脚本命令command，可见zabbix都是通过执行命令来获取监控数据的&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix有对应的告警机制，也就是触发器，设置触发器也就是表达式，达到阈值，就会产生事件，然后可以通过各种通信方式发送，都是支持界面操作。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;zabbix对比promethes&#34;&gt;zabbix对比promethes&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;zabbix采集数据只能通过脚本命令，比较局限，基本都是物理机上的一些命令，所以zabbix比较适合物理机的监控，prometheus不但能够监控物理机，更适合云环境（频繁变动），比如k8s，&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据存储在mysql等关系型数据库中，存储有限，而且很难扩展监控维度，prometheus则是一个时序数据库，还可以远程存储，更适合&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix监控界面不够实时，相比于grafana也是一点都不美观，而且定制化特别难，而grafana则是得到公认的可编辑可扩展美观软件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix集群规模有限，上线为10000个节点，但是promtheus监控节点可以有更大的规模，速度也快。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zabbix已经发展比较成熟，确实在管理界面上比较完善。但是prometheus比较灵活。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;zabbix的报文协议&#34;&gt;zabbix的报文协议&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;cmppingloss[&lt;target&gt;,&lt;packets&gt;,&lt;interval&gt;,&lt;size&gt;,&lt;timeout&gt;] 目标服务器，包数量，包发送间隔，包大小，超时&lt;/li&gt;
&lt;li&gt;value是string，一般是出错信息&lt;/li&gt;
&lt;li&gt;redis.cpunu.discovery这个是一个做发现的配置，最后生成了如下的配置可以舍去  ￼&lt;/li&gt;
&lt;li&gt;state表示在key不支持，或者是监控数据的过程中出错时候会出来&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;zabbix配置文件&#34;&gt;zabbix配置文件&lt;/h1&gt;

&lt;p&gt;zabbix的配置文件一般有三种：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zabbixserver的配置文件zabbix_server.conf

zabbixproxy的配置文件zabbix_proxy.conf

zabbix_agentd的配置文件zabbix_agentd.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.zabbixserver的配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NodeID=0 #分布式节点id号，0代表是独立服务器，默认是被注释掉的，不强制配置
ListenPort=10051 #zabbix server的端口，默认是10051，可以自行修改，
范围是1024-32767 ，一般默认即可
SourceIP=  #连接的源ip地址，默认为空，默认即可
LogFile=/tmp/zabbix_server.log #日志文件的存放位置
LogFileSize=1 #日志文件的大小，单位为MB，当设置为0时，表示不仅行日志轮询，
默认设置为1，默认即可
DebugLevel=3 #指定调试级别，默认即可
PidFile=/tmp/zabbix_server.pid #pid文件的存放位置
DBHost=localhost #数据库主机名，当设置为localhost时，连接mysql通过sock
DBName=zabbix #指定存放zabbix数据数据库的名字
DBUser=zabbix #指定连接数据库的用户名
DBPassword=123456 #用户连接数据库需要的密码
DBSocket=/var/lib/mysql/mysql.sock #前文主机设置为localhost，用户
连接数据库所用的sock位置，
DBPort=3306 #数据库的端口号，当用sock连接时，无关紧要，当通过网络连接时需设置
StartPollers=5 #默认即可
StartIPMIPollers=0 #使用IPMI协议时，用到的参数
StartTrappers=5 #打开的进程数，
StartPingers=1 同上
StartDiscoverers=1
StartHTTPPollers=1
JavaGateway=127.0.0.1 #JavaGateway的ip地址或主机名
JavaGatewayPort=10052 #JavaGateway的端口号
StartJavaPollers=5 #开启连接javagatey的进程数
SNMPTrapperFile=/tmp/zabbix_traps.tmp
StartSNMPTrapper=0 #如果设置为1，snmp trapper进程就会开启
ListenIP=0.0.0.0 #监听来自trapper的ip地址
ListenIP=127.0.0.1
HousekeepingFrequency=1 #zabbix执行Housekeeping的频率，单位为hours
MaxHousekeeperDelete=500 #每次最多删除历史数据的行
SenderFrequency=30 #zabbix试图发送未发送的警报的时间，单位为秒
CacheSize=8M #缓存的大小
CacheUpdateFrequency=60#执行更新缓存配置的时间，单位为秒数
StartDBSyncers=4
HistoryCacheSize=8M
TrendCacheSize=4M
HistoryTextCacheSize=16M
NodeNoEvents=0
NodeNoHistory=0
Timeout=3
TrapperTimeout=300
UnreachablePeriod=45
UnavailableDelay=60
UnreachableDelay=15
AlertScriptsPath=/usr/local/zabbix/shell #脚本的存放路径
FpingLocation=/usr/local/sbin/fping #fping指令的绝对路径
SSHKeyLocation=
LogSlowQueries=0
TmpDir=/tmp
Include=/usr/local/etc/zabbix_server.general.conf
Include=/usr/local/etc/zabbix_server.conf.d/ #子配置文件路径
StartProxyPollers=1 #在zabbix proxy被动模式下用此参数
ProxyConfigFrequency=3600#同上
ProxyDataFrequency=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ListenPort=10051 #监听端口



LogFile=/opt/zabbix/logs/zabbix_server.log

LogFileSize=1024

DebugLevel=3

PidFile=/var/run/zabbix/zabbix_server.pid

#mysql 数据库配置
DBHost=10.243.51.107
DBName=zabbix
DBUser=zabbix
DBPassword=zabbix@suning
DBPort=3306


StartPollers=500

StartIPMIPollers=1

StartPollersUnreachable=100

StartTrappers=100

StartPingers=50

StartDiscoverers=10

StartHTTPPollers=10

StartTimers=10










SNMPTrapperFile=/opt/zabbix/zabbix_traps.tmp

StartSNMPTrapper=1

# 监听地址
ListenIP=0.0.0.0

CacheSize=8G

CacheUpdateFrequency=3600

StartDBSyncers=50

HistoryCacheSize=2G

TrendCacheSize=2G


ValueCacheSize=10G


Timeout=25
TrapperTimeout=120
UnreachablePeriod=300
UnavailableDelay=60
UnreachableDelay=60
AlertScriptsPath=/opt/zabbix/alertscripts
ExternalScripts=/opt/zabbix/externalscripts
FpingLocation=/usr/sbin/fping


LogSlowQueries=10

StartProxyPollers=100
ProxyConfigFrequency=3600
ProxyDataFrequency=30
AllowRoot=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.zabbixagentd的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PidFile=/tmp/zabbix_agentd.pid #pid文件的存放位置
LogFile=/tmp/zabbix_agentd.log #日志文件的位置
LogFileSize=1 #当日志文件达到多大时进行轮询操作
DebugLevel=3 #日志信息级别
SourceIP= #连接的源ip地址，默认为空，即可
EnableRemoteCommands=0 #是否允许zabbix server端的远程指令，
0表示不允许，
1表示允许
LogRemoteCommands=0 #是否开启日志记录shell命令作为警告 0表示不允许，1表示允许
Server=127.0.0.1 #zabbix server的ip地址或主机名，可同时列出多个，需要用逗号隔开
ListenPort=10050 #zabbix agent监听的端口
ListenIP=0.0.0.0 #zabbix agent监听的ip地址
StartAgents=3 #zabbix agent开启进程数
ServerActive=127.0.0.1 #开启主动检查
Hostname=Zabbix server#在zabbix server前端配置时指定的主机名要相同，最重要的配置
RefreshActiveChecks=120 #主动检查刷新的时间，单位为秒数
BufferSend=5 #数据缓冲的时间
BufferSize=100 #zabbix agent数据缓冲区的大小，当达到该值便会发送所有的数据到zabbix server
MaxLinesPerSecond=100 #zabbix agent发送给zabbix server最大的数据行
AllowRoot=0 #是否允许zabbix agent 以root用户运行
Timeout=3 #设定处理超时的时间
Include=/usr/local/etc/zabbix_agentd.userparams.conf
Include=/usr/local/etc/zabbix_agentd.conf.d/ #包含子配置文件的路径
UnsafeUserParameters=0 #是否允许所有字符参数的传递
UserParameter= #指定用户自定义参数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PidFile=/var/run/zabbix/zabbix_agentd.pid

LogFile=/var/log/zabbix/zabbix_agentd.log

LogFileSize=0
Server=10.243.51.50

# 推送指标连接的服务器，格式如下addr:port
ServerActive=10.243.51.48

Hostname=10.243.51.50

HostMetadataItem=system.uname
Timeout=15

AllowRoot=1

Include=/etc/zabbix/zabbix_agentd.d/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.zabbixproxy的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Server=192.168.70.133 #指定zabbix server的ip地址或主机名
Hostname=zabbix-proxy-1.35 #定义监控代理的主机名，需和zabbix server前端配置时指定的节点名相同
LogFile=/tmp/zabbix_proxy.log #指定日志文件的位置
PidFile=/tmp/zabbix_proxy.pid #pid文件的位置
DBName=zabbix_proxy #数据库名
DBUser=zabbix #连接数据库的用户
DBPassword=123456#连接数据库用户的密码
ConfigFrequency=60 #zabbix proxy从zabbix server取得配置数据的频率
DataSenderFrequency=60 #zabbix proxy发送监控到的数据给zabbix server的频率
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际使用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#连接server的地址
Server=10.243.51.48

ServerPort=10052

Hostname=10.243.51.48

#启动监听的地址和端口
ListenPort=10051
ListenIP=0.0.0.0

LogFile=/opt/zabbix/logs/zabbix_proxy.log

LogFileSize=1024

DebugLevel=3

PidFile=/var/run/zabbix/zabbix_proxy.pid

#自带数据库
DBHost=localhost
DBName=zabbix_proxy
DBUser=zabbix
DBPassword=zabbix@suning
DBSocket=/opt/mysql/run/mysqld.sock
DBPort=3306



ProxyOfflineBuffer=1


ConfigFrequency=3600

DataSenderFrequency=20


StartPollers=300

StartIPMIPollers=10

StartPollersUnreachable=100

StartTrappers=100

StartPingers=20

StartDiscoverers=50

StartHTTPPollers=100




StartVMwareCollectors=10

VMwareFrequency=60

VMwareCacheSize=256M

CacheSize=8G

StartDBSyncers=10

HistoryCacheSize=2G

HistoryTextCacheSize=2G

Timeout=30

TrapperTimeout=300

UnreachablePeriod=300

UnavailableDelay=60

UnreachableDelay=15

ExternalScripts=/opt/zabbix/externalscripts

FpingLocation=/usr/sbin/fping

LogSlowQueries=0


AllowRoot=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;官网配置文件：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_proxy&#34;&gt;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_proxy&lt;/a&gt;
&lt;a href=&#34;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_server&#34;&gt;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_server&lt;/a&gt;
&lt;a href=&#34;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_agentd&#34;&gt;https://www.zabbix.com/documentation/2.2/manual/appendix/config/zabbix_agentd&lt;/a&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Strings</title>
          <link>https://kingjcy.github.io/post/golang/go-strings/</link>
          <pubDate>Wed, 12 Oct 2016 19:37:30 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-strings/</guid>
          <description>&lt;p&gt;平时在开发过程中， 和字符串打交道还是比较多的，比如分割， 去除， 替换等等常用的方法， 这些都是由strings包来提供的。&lt;/p&gt;

&lt;h1 id=&#34;基本应用&#34;&gt;基本应用&lt;/h1&gt;

&lt;h2 id=&#34;字符串比较&#34;&gt;字符串比较&lt;/h2&gt;

&lt;p&gt;Compare 函数，用于比较两个字符串的大小，如果两个字符串相等，返回为 0。如果 a 小于 b ，返回 -1 ，反之返回 1 。不推荐使用这个函数，直接使用 == != &amp;gt; &amp;lt; &amp;gt;= &amp;lt;= 等一系列运算符更加直观。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Compare(a, b string) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EqualFold 函数，计算 s 与 t 忽略字母大小写后是否相等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func EqualFold(s, t string) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := &amp;quot;gopher&amp;quot;
b := &amp;quot;hello world&amp;quot;
fmt.Println(strings.Compare(a, b))
fmt.Println(strings.Compare(a, a))
fmt.Println(strings.Compare(b, a))

fmt.Println(strings.EqualFold(&amp;quot;GO&amp;quot;, &amp;quot;go&amp;quot;))
fmt.Println(strings.EqualFold(&amp;quot;壹&amp;quot;, &amp;quot;一&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-1
0
1
true
false
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;是否存在某个字符或子串&#34;&gt;是否存在某个字符或子串&lt;/h2&gt;

&lt;p&gt;有三个函数做这件事：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 子串 substr 在 s 中，返回 true
func Contains(s, substr string) bool
// chars 中任何一个 Unicode 代码点在 s 中，返回 true
func ContainsAny(s, chars string) bool
// Unicode 代码点 r 在 s 中，返回 true
func ContainsRune(s string, r rune) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里对 ContainsAny 函数进行一下说明，看如下例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.ContainsAny(&amp;quot;team&amp;quot;, &amp;quot;i&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;failure&amp;quot;, &amp;quot;u &amp;amp; i&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;in failure&amp;quot;, &amp;quot;s g&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;foo&amp;quot;, &amp;quot;&amp;quot;))
fmt.Println(strings.ContainsAny(&amp;quot;&amp;quot;, &amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;false
true
true
false
false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说，第二个参数 chars 中任意一个字符（Unicode Code Point）如果在第一个参数 s 中存在，则返回 true。&lt;/p&gt;

&lt;p&gt;查看这三个函数的源码，发现它们只是调用了相应的 Index 函数（子串出现的位置），然后和 0 作比较返回 true 或 fale。如，Contains：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Contains(s, substr string) bool {
  return Index(s, substr) &amp;gt;= 0
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;index则使用了我们常用的字符串匹配算法的rk算法。&lt;/p&gt;

&lt;h2 id=&#34;子串出现次数-字符串匹配&#34;&gt;子串出现次数 ( 字符串匹配 )&lt;/h2&gt;

&lt;p&gt;在数据结构与算法中，可能会讲解以下字符串匹配算法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;朴素匹配算法
KMP 算法
Rabin-Karp 算法
Boyer-Moore 算法
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有其他的算法，这里不一一列举，感兴趣的可以网上搜一下。&lt;/p&gt;

&lt;p&gt;在 Go 中，查找子串出现次数即字符串模式匹配，根据长度，分别实现的是BF和 Rabin-Karp 算法。Count 函数的签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Count(s, sep string) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 Count 的实现中，处理了几种特殊情况，属于字符匹配预处理的一部分。这里要特别说明一下的是当 sep 为空时，Count 的返回值是：utf8.RuneCountInString(s) + 1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Count(&amp;quot;cheese&amp;quot;, &amp;quot;e&amp;quot;))
fmt.Println(len(&amp;quot;谷歌中国&amp;quot;))
fmt.Println(strings.Count(&amp;quot;谷歌中国&amp;quot;, &amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3
12
5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 Rabin-Karp 算法的实现，有兴趣的可以看看 Count 的源码。&lt;/p&gt;

&lt;p&gt;另外，Count 是计算子串在字符串中出现的无重叠的次数，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Count(&amp;quot;fivevev&amp;quot;, &amp;quot;vev&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串分割为-string&#34;&gt;字符串分割为[]string&lt;/h2&gt;

&lt;p&gt;这个需求很常见，倒不一定是为了得到[]string。&lt;/p&gt;

&lt;p&gt;该包提供了六个三组分割函数：Fields和FieldsFunc、Split和SplitAfter、SplitN和SplitAfterN。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Fields和FieldsFunc&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Fields(s string) []string
func FieldsFunc(s string, f func(rune) bool) []string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fields 用一个或多个连续的空格分隔字符串 s，返回子字符串的数组（slice）。如果字符串 s 只包含空格，则返回空列表 ([]string 的长度为 0）。其中，空格的定义是 unicode.IsSpace，之前已经介绍过。&lt;/p&gt;

&lt;p&gt;常见间隔符包括：&amp;rsquo;\t&amp;rsquo;, &amp;lsquo;\n&amp;rsquo;, &amp;lsquo;\v&amp;rsquo;, &amp;lsquo;\f&amp;rsquo;, &amp;lsquo;\r&amp;rsquo;, &amp;lsquo; &amp;lsquo;, U+0085 (NEL), U+00A0 (NBSP)&lt;/p&gt;

&lt;p&gt;由于是用空格分隔，因此结果中不会含有空格或空子字符串，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;Fields are: %q&amp;quot;, strings.Fields(&amp;quot;  foo bar  baz   &amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Fields are: [&amp;quot;foo&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;baz&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FieldsFunc通过实现一个回调函数来指定分隔字符串 s 的字符。比如上面的例子，我们通过 FieldsFunc 来实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.FieldsFunc(&amp;quot;  foo bar  baz   &amp;quot;, unicode.IsSpace))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上，Fields 函数就是调用 FieldsFunc 实现的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Fields(s string) []string {
  return FieldsFunc(s, unicode.IsSpace)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Split和SplitAfter、SplitN和SplitAfterN&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;之所以将这四个函数放在一起讲，是因为它们都是通过一个同一个内部函数来实现的。它们的函数签名及其实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Split(s, sep string) []string { return genSplit(s, sep, 0, -1) }
func SplitAfter(s, sep string) []string { return genSplit(s, sep, len(sep), -1) }
func SplitN(s, sep string, n int) []string { return genSplit(s, sep, 0, n) }
func SplitAfterN(s, sep string, n int) []string { return genSplit(s, sep, len(sep), n) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它们都调用了 genSplit 函数。&lt;/p&gt;

&lt;p&gt;这四个函数都是通过 sep 进行分割，返回[]string。如果 sep 为空，相当于分成一个个的 UTF-8 字符，如 Split(&amp;ldquo;abc&amp;rdquo;,&amp;ldquo;&amp;rdquo;)，得到的是[a b c]。&lt;/p&gt;

&lt;p&gt;Split(s, sep) 和 SplitN(s, sep, -1) 等价；SplitAfter(s, sep) 和 SplitAfterN(s, sep, -1) 等价。&lt;/p&gt;

&lt;p&gt;那么，Split 和 SplitAfter 有啥区别呢？通过这两句代码的结果就知道它们的区别了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.SplitAfter(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;quot;foo&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;baz&amp;quot;]
[&amp;quot;foo,&amp;quot; &amp;quot;bar,&amp;quot; &amp;quot;baz&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也就是说，Split 会将 s 中的 sep 去掉，而 SplitAfter 会保留 sep。&lt;/p&gt;

&lt;p&gt;带 N 的方法可以通过最后一个参数 n 控制返回的结果中的 slice 中的元素个数，当 n &amp;lt; 0 时，返回所有的子字符串；当 n == 0 时，返回的结果是 nil；当 n &amp;gt; 0 时，表示返回的 slice 中最多只有 n 个元素，其中，最后一个元素不会分割，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.SplitN(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;, 2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;quot;foo&amp;quot; &amp;quot;bar,baz&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外看一下官方文档提供的例子，注意一下输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;a,b,c&amp;quot;, &amp;quot;,&amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;a man a plan a canal panama&amp;quot;, &amp;quot;a &amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot; xyz &amp;quot;, &amp;quot;&amp;quot;))
fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;&amp;quot;, &amp;quot;Bernardo O&#39;Higgins&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;]
[&amp;quot;&amp;quot; &amp;quot;man &amp;quot; &amp;quot;plan &amp;quot; &amp;quot;canal panama&amp;quot;]
[&amp;quot; &amp;quot; &amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot; &amp;quot; &amp;quot;]
[&amp;quot;&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串是否有某个前缀或后缀&#34;&gt;字符串是否有某个前缀或后缀&lt;/h2&gt;

&lt;p&gt;这两个函数比较简单，源码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// s 中是否以 prefix 开始
func HasPrefix(s, prefix string) bool {
  return len(s) &amp;gt;= len(prefix) &amp;amp;&amp;amp; s[0:len(prefix)] == prefix
}
// s 中是否以 suffix 结尾
func HasSuffix(s, suffix string) bool {
  return len(s) &amp;gt;= len(suffix) &amp;amp;&amp;amp; s[len(s)-len(suffix):] == suffix
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 prefix 或 suffix 为 &amp;ldquo;&amp;rdquo; , 返回值总是 true。&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.HasPrefix(&amp;quot;Gopher&amp;quot;, &amp;quot;Go&amp;quot;))
fmt.Println(strings.HasPrefix(&amp;quot;Gopher&amp;quot;, &amp;quot;C&amp;quot;))
fmt.Println(strings.HasPrefix(&amp;quot;Gopher&amp;quot;, &amp;quot;&amp;quot;))
fmt.Println(strings.HasSuffix(&amp;quot;Amigo&amp;quot;, &amp;quot;go&amp;quot;))
fmt.Println(strings.HasSuffix(&amp;quot;Amigo&amp;quot;, &amp;quot;Ami&amp;quot;))
fmt.Println(strings.HasSuffix(&amp;quot;Amigo&amp;quot;, &amp;quot;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;true
false
true
true
false
true
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符或子串在字符串中出现的位置&#34;&gt;字符或子串在字符串中出现的位置&lt;/h2&gt;

&lt;p&gt;有一序列函数与该功能有关：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 在 s 中查找 sep 的第一次出现，返回第一次出现的索引
func Index(s, sep string) int
// 在 s 中查找字节 c 的第一次出现，返回第一次出现的索引
func IndexByte(s string, c byte) int
// chars 中任何一个 Unicode 代码点在 s 中首次出现的位置
func IndexAny(s, chars string) int
// 查找字符 c 在 s 中第一次出现的位置，其中 c 满足 f(c) 返回 true
func IndexFunc(s string, f func(rune) bool) int
// Unicode 代码点 r 在 s 中第一次出现的位置
func IndexRune(s string, r rune) int

// 有三个对应的查找最后一次出现的位置
func LastIndex(s, sep string) int
func LastIndexByte(s string, c byte) int
func LastIndexAny(s, chars string) int
func LastIndexFunc(s string, f func(rune) bool) int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一序列函数，只举 IndexFunc 的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;han := func(c rune) bool {
    return unicode.Is(unicode.Han, c) // 汉字
}
fmt.Println(strings.IndexFunc(&amp;quot;Hello, world&amp;quot;, han))
fmt.Println(strings.IndexFunc(&amp;quot;Hello, 世界&amp;quot;, han))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-1
7
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串-join-操作&#34;&gt;字符串 JOIN 操作&lt;/h2&gt;

&lt;p&gt;将字符串数组（或 slice）连接起来可以通过 Join 实现，函数签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Join(a []string, sep string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;假如没有这个库函数，我们自己实现一个，我们会这么实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Join(str []string, sep string) string {
  // 特殊情况应该做处理
  if len(str) == 0 {
      return &amp;quot;&amp;quot;
  }
  if len(str) == 1 {
      return str[0]
  }
  buffer := bytes.NewBufferString(str[0])
  for _, s := range str[1:] {
      buffer.WriteString(sep)
      buffer.WriteString(s)
  }
  return buffer.String()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里，我们使用了 bytes 包的 Buffer 类型，避免大量的字符串连接操作（因为 Go 中字符串是不可变的）。我们再看一下标准库的实现：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Join(a []string, sep string) string {
  if len(a) == 0 {
      return &amp;quot;&amp;quot;
  }
  if len(a) == 1 {
      return a[0]
  }
  n := len(sep) * (len(a) - 1)
  for i := 0; i &amp;lt; len(a); i++ {
      n += len(a[i])
  }

  b := make([]byte, n)
  bp := copy(b, a[0])
  for _, s := range a[1:] {
      bp += copy(b[bp:], sep)
      bp += copy(b[bp:], s)
  }
  return string(b)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;标准库的实现没有用 bytes 包，当然也不会简单的通过 + 号连接字符串。Go 中是不允许循环依赖的，标准库中很多时候会出现代码拷贝，而不是引入某个包。这里 Join 的实现方式挺好，我个人观点认为，不直接使用 bytes 包，也是不想依赖 bytes 包（其实 bytes 中的实现也是 copy 方式）。&lt;/p&gt;

&lt;p&gt;简单使用示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(Join([]string{&amp;quot;name=xxx&amp;quot;, &amp;quot;age=xx&amp;quot;}, &amp;quot;&amp;amp;&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name=xxx&amp;amp;age=xx
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符串重复几次&#34;&gt;字符串重复几次&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func Repeat(s string, count int) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 s 重复 count 次，如果 count 为负数或返回值长度 len(s)*count 超出 string 上限会导致 panic，这个函数使用很简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(&amp;quot;ba&amp;quot; + strings.Repeat(&amp;quot;na&amp;quot;, 2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;banana
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;字符替换&#34;&gt;字符替换&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;map&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Map(mapping func(rune) rune, s string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Map 函数，将 s 的每一个字符按照 mapping 的规则做映射替换，如果 mapping 返回值 &amp;lt;0 ，则舍弃该字符。该方法只能对每一个字符做处理，但处理方式很灵活，可以方便的过滤，筛选汉字等。&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mapping := func(r rune) rune {
    switch {
    case r &amp;gt;= &#39;A&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;Z&#39;: // 大写字母转小写
        return r + 32
    case r &amp;gt;= &#39;a&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;z&#39;: // 小写字母不处理
        return r
    case unicode.Is(unicode.Han, r): // 汉字换行
        return &#39;\n&#39;
    }
    return -1 // 过滤所有非字母、汉字的字符
}
fmt.Println(strings.Map(mapping, &amp;quot;Hello你#￥%……\n（&#39;World\n,好Hello^(&amp;amp;(*界gopher...&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hello
world
hello
gopher
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;replace&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;进行字符串替换时，考虑到性能问题，能不用正则尽量别用，应该用这里的函数。&lt;/p&gt;

&lt;p&gt;字符串替换的函数签名如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 用 new 替换 s 中的 old，一共替换 n 个。
// 如果 n &amp;lt; 0，则不限制替换次数，即全部替换
func Replace(s, old, new string, n int) string
// 该函数内部直接调用了函数 Replace(s, old, new , -1)
func ReplaceAll(s, old, new string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Replace(&amp;quot;oink oink oink&amp;quot;, &amp;quot;k&amp;quot;, &amp;quot;ky&amp;quot;, 2))
fmt.Println(strings.Replace(&amp;quot;oink oink oink&amp;quot;, &amp;quot;oink&amp;quot;, &amp;quot;moo&amp;quot;, -1))
fmt.Println(strings.ReplaceAll(&amp;quot;oink oink oink&amp;quot;, &amp;quot;oink&amp;quot;, &amp;quot;moo&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;oinky oinky oink
moo moo moo
moo moo moo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我们希望一次替换多个，比如我们希望替换 This is &lt;b&gt;HTML&lt;/b&gt; 中的 &amp;lt; 和 &amp;gt; 为 &amp;lt; 和 &amp;gt;，可以调用上面的函数两次。但标准库提供了另外的方法进行这种替换。&lt;/p&gt;

&lt;h2 id=&#34;大小写转换&#34;&gt;大小写转换&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func ToLower(s string) string
func ToLowerSpecial(c unicode.SpecialCase, s string) string
func ToUpper(s string) string
func ToUpperSpecial(c unicode.SpecialCase, s string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;大小写转换包含了 4 个相关函数，ToLower,ToUpper 用于大小写转换。ToLowerSpecial,ToUpperSpecial 可以转换特殊字符的大小写。 举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.ToLower(&amp;quot;HELLO WORLD&amp;quot;))
fmt.Println(strings.ToLower(&amp;quot;Ā Á Ǎ À&amp;quot;))
fmt.Println(strings.ToLowerSpecial(unicode.TurkishCase, &amp;quot;壹&amp;quot;))
fmt.Println(strings.ToLowerSpecial(unicode.TurkishCase, &amp;quot;HELLO WORLD&amp;quot;))
fmt.Println(strings.ToLower(&amp;quot;Önnek İş&amp;quot;))
fmt.Println(strings.ToLowerSpecial(unicode.TurkishCase, &amp;quot;Önnek İş&amp;quot;))

fmt.Println(strings.ToUpper(&amp;quot;hello world&amp;quot;))
fmt.Println(strings.ToUpper(&amp;quot;ā á ǎ à&amp;quot;))
fmt.Println(strings.ToUpperSpecial(unicode.TurkishCase, &amp;quot;一&amp;quot;))
fmt.Println(strings.ToUpperSpecial(unicode.TurkishCase, &amp;quot;hello world&amp;quot;))
fmt.Println(strings.ToUpper(&amp;quot;örnek iş&amp;quot;))
fmt.Println(strings.ToUpperSpecial(unicode.TurkishCase, &amp;quot;örnek iş&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hello world
ā á ǎ à
壹
hello world
önnek iş
önnek iş
HELLO WORLD
Ā Á Ǎ À       // 汉字拼音有效
一           //  汉字无效
HELLO WORLD
ÖRNEK IŞ
ÖRNEK İŞ    // 有细微差别
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;标题处理&#34;&gt;标题处理&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func Title(s string) string
func ToTitle(s string) string
func ToTitleSpecial(c unicode.SpecialCase, s string) string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;标题处理包含 3 个相关函数，其中 Title 会将 s 每个单词的首字母大写，不处理该单词的后续字符。ToTitle 将 s 的每个字母大写。ToTitleSpecial 将 s 的每个字母大写，并且会将一些特殊字母转换为其对应的特殊大写字母。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(strings.Title(&amp;quot;hElLo wOrLd&amp;quot;))
fmt.Println(strings.ToTitle(&amp;quot;hElLo wOrLd&amp;quot;))
fmt.Println(strings.ToTitleSpecial(unicode.TurkishCase, &amp;quot;hElLo wOrLd&amp;quot;))
fmt.Println(strings.Title(&amp;quot;āáǎà ōóǒò êēéěè&amp;quot;))
fmt.Println(strings.ToTitle(&amp;quot;āáǎà ōóǒò êēéěè&amp;quot;))
fmt.Println(strings.ToTitleSpecial(unicode.TurkishCase, &amp;quot;āáǎà ōóǒò êēéěè&amp;quot;))
fmt.Println(strings.Title(&amp;quot;dünyanın ilk borsa yapısı Aizonai kabul edilir&amp;quot;))
fmt.Println(strings.ToTitle(&amp;quot;dünyanın ilk borsa yapısı Aizonai kabul edilir&amp;quot;))
fmt.Println(strings.ToTitleSpecial(unicode.TurkishCase, &amp;quot;dünyanın ilk borsa yapısı Aizonai kabul edilir&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HElLo WOrLd
HELLO WORLD
HELLO WORLD
Āáǎà Ōóǒò Êēéěè
ĀÁǍÀ ŌÓǑÒ ÊĒÉĚÈ
ĀÁǍÀ ŌÓǑÒ ÊĒÉĚÈ
Dünyanın Ilk Borsa Yapısı Aizonai Kabul Edilir
DÜNYANIN ILK BORSA YAPISI AIZONAI KABUL EDILIR
DÜNYANIN İLK BORSA YAPISI AİZONAİ KABUL EDİLİR
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;修剪&#34;&gt;修剪&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 将 s 左侧和右侧中匹配 cutset 中的任一字符的字符去掉
func Trim(s string, cutset string) string
// 将 s 左侧的匹配 cutset 中的任一字符的字符去掉
func TrimLeft(s string, cutset string) string
// 将 s 右侧的匹配 cutset 中的任一字符的字符去掉
func TrimRight(s string, cutset string) string
// 如果 s 的前缀为 prefix 则返回去掉前缀后的 string , 否则 s 没有变化。
func TrimPrefix(s, prefix string) string
// 如果 s 的后缀为 suffix 则返回去掉后缀后的 string , 否则 s 没有变化。
func TrimSuffix(s, suffix string) string
// 将 s 左侧和右侧的间隔符去掉。常见间隔符包括：&#39;\t&#39;, &#39;\n&#39;, &#39;\v&#39;, &#39;\f&#39;, &#39;\r&#39;, &#39; &#39;, U+0085 (NEL)
func TrimSpace(s string) string
// 将 s 左侧和右侧的匹配 f 的字符去掉
func TrimFunc(s string, f func(rune) bool) string
// 将 s 左侧的匹配 f 的字符去掉
func TrimLeftFunc(s string, f func(rune) bool) string
// 将 s 右侧的匹配 f 的字符去掉
func TrimRightFunc(s string, f func(rune) bool) string
包含了 9 个相关函数用于修剪字符串。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x := &amp;quot;!!!@@@你好,!@#$ Gophers###$$$&amp;quot;
fmt.Println(strings.Trim(x, &amp;quot;@#$!%^&amp;amp;*()_+=-&amp;quot;))
fmt.Println(strings.TrimLeft(x, &amp;quot;@#$!%^&amp;amp;*()_+=-&amp;quot;))
fmt.Println(strings.TrimRight(x, &amp;quot;@#$!%^&amp;amp;*()_+=-&amp;quot;))
fmt.Println(strings.TrimSpace(&amp;quot; \t\n Hello, Gophers \n\t\r\n&amp;quot;))
fmt.Println(strings.TrimPrefix(x, &amp;quot;!&amp;quot;))
fmt.Println(strings.TrimSuffix(x, &amp;quot;$&amp;quot;))

f := func(r rune) bool {
    return !unicode.Is(unicode.Han, r) // 非汉字返回 true
}
fmt.Println(strings.TrimFunc(x, f))
fmt.Println(strings.TrimLeftFunc(x, f))
fmt.Println(strings.TrimRightFunc(x, f))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;你好,!@#$ Gophers
你好,!@#$ Gophers###$$$
!!!@@@你好,!@#$ Gophers
Hello, Gophers
!!@@@你好,!@#$ Gophers###$$$
!!!@@@你好,!@#$ Gophers###$$
你好
你好,!@#$ Gophers###$$$
!!!@@@你好
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;

&lt;h2 id=&#34;replacer-类型&#34;&gt;Replacer 类型&lt;/h2&gt;

&lt;p&gt;这是一个结构，没有导出任何字段，实例化通过 func NewReplacer(oldnew &amp;hellip;string) *Replacer 函数进行，其中不定参数 oldnew 是 old-new 对，即进行多个替换。如果 oldnew 长度与奇数，会导致 panic.&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r := strings.NewReplacer(&amp;quot;&amp;lt;&amp;quot;, &amp;quot;&amp;amp;lt;&amp;quot;, &amp;quot;&amp;gt;&amp;quot;, &amp;quot;&amp;amp;gt;&amp;quot;)
fmt.Println(r.Replace(&amp;quot;This is &amp;lt;b&amp;gt;HTML&amp;lt;/b&amp;gt;!&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This is &amp;amp;lt;b&amp;amp;gt;HTML&amp;amp;lt;/b&amp;amp;gt;!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，Replacer 还提供了另外一个方法，它在替换之后将结果写入 io.Writer 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *Replacer) WriteString(w io.Writer, s string) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reader-类型&#34;&gt;Reader 类型&lt;/h2&gt;

&lt;p&gt;看到名字就能猜到，这是实现了 io 包中的接口。其实这就是缓存io，对缓存中的string进行读写操作。&lt;/p&gt;

&lt;p&gt;它实现了 io.Reader（Read 方法），io.ReaderAt（ReadAt 方法），io.Seeker（Seek 方法），io.WriterTo（WriteTo 方法），io.ByteReader（ReadByte 方法），io.ByteScanner（ReadByte 和 UnreadByte 方法），io.RuneReader（ReadRune 方法） 和 io.RuneScanner（ReadRune 和 UnreadRune 方法）。&lt;/p&gt;

&lt;p&gt;Reader 结构如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Reader struct {
    s        string    // Reader 读取的数据来源
    i        int // current reading index（当前读的索引位置）
    prevRune int // index of previous rune; or &amp;lt; 0（前一个读取的 rune 索引位置）
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见 Reader 结构没有导出任何字段，而是提供一个实例化方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(s string) *Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方法接收一个字符串，返回的 Reader 实例就是从该参数字符串读数据。&lt;/p&gt;

&lt;p&gt;在后面学习了 &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/#reader-类型&#34;&gt;bytes&lt;/a&gt; 包之后，可以知道 bytes.NewBufferString 有类似的功能，不过，如果只是为了读取，NewReader 会更高效。&lt;/p&gt;

&lt;h2 id=&#34;builder-类型&#34;&gt;Builder 类型&lt;/h2&gt;

&lt;p&gt;这个类型也是缓存io的一种实现方式。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Builder struct {
    addr *Builder // of receiver, to detect copies by value
    buf  []byte
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该类型实现了 io 包下的 Writer, ByteWriter, StringWriter 等接口，可以向该对象内写入数据，Builder 没有实现 Reader 等接口，所以该类型不可读，但提供了 String 方法可以获取对象内的数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 该方法向 b 写入一个字节
func (b *Builder) WriteByte(c byte) error
// WriteRune 方法向 b 写入一个字符
func (b *Builder) WriteRune(r rune) (int, error)
// WriteRune 方法向 b 写入字节数组 p
func (b *Builder) Write(p []byte) (int, error)
// WriteRune 方法向 b 写入字符串 s
func (b *Builder) WriteString(s string) (int, error)
// Len 方法返回 b 的数据长度。
func (b *Builder) Len() int
// Cap 方法返回 b 的 cap。
func (b *Builder) Cap() int
// Grow 方法将 b 的 cap 至少增加 n (可能会更多)。如果 n 为负数，会导致 panic。
func (b *Builder) Grow(n int)
// Reset 方法将 b 清空 b 的所有内容。
func (b *Builder) Reset()
// String 方法将 b 的数据以 string 类型返回。
func (b *Builder) String() string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Builder 有 4 个与写入相关的方法，这 4 个方法的 error 都总是为 nil.&lt;/p&gt;

&lt;p&gt;Builder 的 cap 会自动增长，一般不需要手动调用 Grow 方法。&lt;/p&gt;

&lt;p&gt;String 方法可以方便的获取 Builder 的内容。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b := strings.Builder{}
_ = b.WriteByte(&#39;7&#39;)
n, _ := b.WriteRune(&#39;夕&#39;)
fmt.Println(n)
n, _ = b.Write([]byte(&amp;quot;Hello, World&amp;quot;))
fmt.Println(n)
n, _ = b.WriteString(&amp;quot;你好，世界&amp;quot;)
fmt.Println(n)
fmt.Println(b.Len())
fmt.Println(b.Cap())
b.Grow(100)
fmt.Println(b.Len())
fmt.Println(b.Cap())
fmt.Println(b.String())
b.Reset()
fmt.Println(b.String())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3
12
15
31
32
31
164
7夕Hello, World你好，世界
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;这边主要是要注意，使用返回值作为新值，原来值是不变的。&lt;/strong&gt;&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Strconv</title>
          <link>https://kingjcy.github.io/post/golang/go-strconv/</link>
          <pubDate>Wed, 12 Oct 2016 19:33:24 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-strconv/</guid>
          <description>&lt;p&gt;strconv包实现了基本数据类型和其字符串表示的相互转换。&lt;/p&gt;

&lt;h1 id=&#34;基本使用&#34;&gt;基本使用&lt;/h1&gt;

&lt;p&gt;strconv主要就是字符之间的转化，我们直接看我们经常的使用就好。&lt;/p&gt;

&lt;h2 id=&#34;parseint&#34;&gt;ParseInt&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func ParseInt(s string, base int, bitSize int) (i int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回字符串表示的整数值，接受正负号。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;base指定进制（2到36），如果base为0，则会从字符串前置判断，&amp;rdquo;0x&amp;rdquo;是16进制，&amp;rdquo;0&amp;rdquo;是8进制，否则是10进制；&lt;/li&gt;
&lt;li&gt;bitSize指定结果必须能无溢出赋值的整数类型，0、8、16、32、64 分别代表 int、int8、int16、int32、int64；返回的err是*NumErr类型的，如果语法有误，err.Error = ErrSyntax；如果结果超出类型范围err.Error = ErrRange。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;int和string的转化&#34;&gt;int和string的转化&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;int转string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;s := strconv.Itoa(i)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等价于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s := strconv.FormatInt(int64(i), 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;int64转string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;i := int64(123)
s := strconv.FormatInt(i, 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二个参数为基数，可选2~36&lt;/p&gt;

&lt;p&gt;注：对于无符号整形，可以使用FormatUint(i uint64, base int)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;string转int&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;i, err := strconv.Atoi(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;string转int64&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;i, err := strconv.ParseInt(s, 10, 64)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二个参数为基数（2~36），第三个参数位大小表示期望转换的结果类型，其值可以为0, 8, 16, 32和64，分别对应 int, int8, int16, int32和int64&lt;/p&gt;

&lt;h2 id=&#34;float相关&#34;&gt;float相关&lt;/h2&gt;

&lt;p&gt;float转string：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;v := 3.1415926535
s1 := strconv.FormatFloat(v, &#39;E&#39;, -1, 32)//float32s2 := strconv.FormatFloat(v, &#39;E&#39;, -1, 64)//float64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数原型及参数含义具体可查看：&lt;a href=&#34;https://golang.org/pkg/strconv/#FormatFloat&#34;&gt;https://golang.org/pkg/strconv/#FormatFloat&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;string转float：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s := &amp;quot;3.1415926535&amp;quot;
v1, err := strconv.ParseFloat(v, 32)
v2, err := strconv.ParseFloat(v, 64)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;error相关&#34;&gt;error相关&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;error转string&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;err.Error()
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Io</title>
          <link>https://kingjcy.github.io/post/golang/go-io/</link>
          <pubDate>Sat, 30 Jul 2016 20:39:03 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-io/</guid>
          <description>&lt;p&gt;io包提供了所有需要交互的输入输出模式的基础。&lt;/p&gt;

&lt;h1 id=&#34;基本概念&#34;&gt;基本概念&lt;/h1&gt;

&lt;h2 id=&#34;stream&#34;&gt;stream&lt;/h2&gt;

&lt;p&gt;我们先介绍一下stream的概念。stream就是数据流，数据流的概念其实非常基础，最早是在通讯领域使用的概念，这个概念最初在 1998 年由 Henzinger 在文献中提出，他将数据流定义为 “只能以事先规定好的顺序被读取一次的数据的一个序列”&lt;/p&gt;

&lt;p&gt;数据流就是由数据形成的流，就像由水形成的水流，非常形象，现代语言中，基本上都会有流的支持，比如 C++ 的 iostream，Node.js 的 stream 模块，以及 golang 的 io 包。&lt;/p&gt;

&lt;p&gt;Stream in Golang与流密切相关的就是 bufio io io/ioutil 这几个包：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、io 为 IO 原语（I/O primitives）提供基本的接口
2、io/ioutil 封装一些实用的 I/O 函数
3、fmt 实现格式化 I/O，类似 C 语言中的 printf 和 scanf
4、bufio 实现带缓冲I/O
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;io&#34;&gt;io&lt;/h2&gt;

&lt;p&gt;io 包为 I/O 原语提供了基本的接口。在 io 包中最重要的是两个接口：Reader 和 Writer 接口。本章所提到的各种 IO 包，都跟这两个接口有关，也就是说，只要满足这两个接口，它就可以使用 IO 包的功能。&lt;/p&gt;

&lt;h1 id=&#34;接口&#34;&gt;接口&lt;/h1&gt;

&lt;h2 id=&#34;读取器&#34;&gt;读取器&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;type Reader interface {
    //Read() 方法有两个返回值，一个是读取到的字节数，一个是发生错误时的错误。如果资源内容已全部读取完毕，应该返回 io.EOF 错误。
    Read(p []byte) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;io.Reader 表示一个读取器，它将数据从某个资源读取到传输缓冲区p。在缓冲区中，数据可以被流式传输和使用。&lt;/p&gt;

&lt;p&gt;实现这个接口需要实现如下功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Read 将 len(p) 个字节读取到 p 中。它返回读取的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)） 以及任何遇到的错误。

&lt;ul&gt;
&lt;li&gt;即使 Read 返回的 n &amp;lt; len(p)，它也会在调用过程中占用 len(p) 个字节作为暂存空间。&lt;/li&gt;
&lt;li&gt;若可读取的数据不到 len(p) 个字节，Read 会返回可用数据，而不是等待更多数据。&lt;/li&gt;
&lt;li&gt;当读取的时候没有数据也没有EOF的时候，会阻塞在这边等待。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;当 Read 在成功读取 n &amp;gt; 0 个字节后遇到一个错误或 EOF (end-of-file)，它会返回读取的字节数。

&lt;ul&gt;
&lt;li&gt;它可能会同时在本次的调用中返回一个non-nil错误,或在下一次的调用中返回这个错误（且 n 为 0）。&lt;/li&gt;
&lt;li&gt;一般情况下, Reader会返回一个非0字节数n, 若 n = len(p) 个字节从输入源的结尾处由 Read 返回，Read可能返回 err == EOF 或者 err == nil。并且之后的 Read() 都应该返回 (n:0, err:EOF)。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;调用者在考虑错误之前应当首先处理返回的数据。这样做可以正确地处理在读取一些字节后产生的 I/O 错误，同时允许EOF的出现。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于要用作读取器的类型，它必须实现 io.Reader 接口的唯一一个方法 Read(p []byte)。换句话说，只要实现了 Read(p []byte) ，那它就是一个读取器，使用标准库中已经实现的读写器，来举例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    reader := strings.NewReader(&amp;quot;Clear is better than clever&amp;quot;)
    p := make([]byte, 4)

    for {
        n, err := reader.Read(p)
        if err != nil{
            if err == io.EOF {
                fmt.Println(&amp;quot;EOF:&amp;quot;, n)
                break
            }
            fmt.Println(err)
            os.Exit(1)
        }
        fmt.Println(n, string(p[:n]))
    }
}

输出打印的内容：

4 Clea
4 r is
4  bet
4 ter 
4 than
4  cle
3 ver
EOF: 0 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;自定义reader&#34;&gt;自定义Reader&lt;/h3&gt;

&lt;p&gt;现在，让我们看看如何自己实现一个。它的功能是从流中过滤掉非字母字符。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type alphaReader struct {
    // 资源
    src string
    // 当前读取到的位置 
    cur int
}

// 创建一个实例
func newAlphaReader(src string) *alphaReader {
    return &amp;amp;alphaReader{src: src}
}

// 过滤函数
func alpha(r byte) byte {
    if (r &amp;gt;= &#39;A&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;Z&#39;) || (r &amp;gt;= &#39;a&#39; &amp;amp;&amp;amp; r &amp;lt;= &#39;z&#39;) {
        return r
    }
    return 0
}

// Read 方法，read函数是阻塞的
func (a *alphaReader) Read(p []byte) (int, error) {
    // 当前位置 &amp;gt;= 字符串长度 说明已经读取到结尾 返回 EOF
    if a.cur &amp;gt;= len(a.src) {
        return 0, io.EOF
    }

    // x 是剩余未读取的长度
    x := len(a.src) - a.cur
    n, bound := 0, 0
    if x &amp;gt;= len(p) {
        // 剩余长度超过缓冲区大小，说明本次可完全填满缓冲区
        bound = len(p)
    } else if x &amp;lt; len(p) {
        // 剩余长度小于缓冲区大小，使用剩余长度输出，缓冲区不补满
        bound = x
    }

    buf := make([]byte, bound)
    for n &amp;lt; bound {
        // 每次读取一个字节，执行过滤函数
        if char := alpha(a.src[a.cur]); char != 0 {
            buf[n] = char
        }
        n++
        a.cur++
    }
    // 将处理后得到的 buf 内容复制到 p 中
    copy(p, buf)
    return n, nil
}

func main() {
    reader := newAlphaReader(&amp;quot;Hello! It&#39;s 9am, where is the sun?&amp;quot;)
    p := make([]byte, 4)
    for {
        n, err := reader.Read(p)
        if err == io.EOF {
            break
        }
        fmt.Print(string(p[:n]))
    }
    fmt.Println()
}
输出打印的内容：
HelloItsamwhereisthesun
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;tcp粘包拆包&#34;&gt;TCP粘包拆包&lt;/h3&gt;

&lt;p&gt;这边讲解一下TCP粘包拆包问题，先看下面这个实例&lt;/p&gt;

&lt;p&gt;服务端代码 server/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    l, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:4044&amp;quot;)
    if err != nil {
        panic(err)
    }
    fmt.Println(&amp;quot;listen to 4044&amp;quot;)
    for {
        // 监听到新的连接，创建新的 goroutine 交给 handleConn函数 处理
        conn, err := l.Accept()
        if err != nil {
            fmt.Println(&amp;quot;conn err:&amp;quot;, err)
        } else {
            go handleConn(conn)
        }
    }
}

func handleConn(conn net.Conn) {
    defer conn.Close()
    defer fmt.Println(&amp;quot;关闭&amp;quot;)
    fmt.Println(&amp;quot;新连接：&amp;quot;, conn.RemoteAddr())

    result := bytes.NewBuffer(nil)
    var buf [1024]byte
    for {
        n, err := conn.Read(buf[0:])
        result.Write(buf[0:n])
        if err != nil {
            if err == io.EOF {
                continue
            } else {
                fmt.Println(&amp;quot;read err:&amp;quot;, err)
                break
            }
        } else {
            fmt.Println(&amp;quot;recv:&amp;quot;, result.String())
        }
        result.Reset()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端代码 client/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    data := []byte(&amp;quot;[这里才是一个完整的数据包]&amp;quot;)
    conn, err := net.DialTimeout(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:4044&amp;quot;, time.Second*30)
    if err != nil {
        fmt.Printf(&amp;quot;connect failed, err : %v\n&amp;quot;, err.Error())
        return
    }
    for i := 0; i &amp;lt;1000; i++ {
        _, err = conn.Write(data)
        if err != nil {
            fmt.Printf(&amp;quot;write failed , err : %v\n&amp;quot;, err)
            break
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;listen to 4044
新连接： [::1]:53079
recv: [这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据�
recv: �][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包][这里才是一个完整的数据包][这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
...省略其它的...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从服务端的控制台输出可以看出，存在三种类型的输出：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一种是正常的一个数据包输出。&lt;/li&gt;
&lt;li&gt;一种是多个数据包“粘”在了一起，我们定义这种读到的包为粘包。&lt;/li&gt;
&lt;li&gt;一种是一个数据包被“拆”开，形成一个破碎的包，我们定义这种包为半包。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为什么会出现半包和粘包？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;客户端一段时间内发送包的速度太多，服务端没有全部处理完。于是数据就会积压起来，产生粘包。&lt;/li&gt;
&lt;li&gt;定义的读的buffer不够大，而数据包太大或者由于粘包产生，服务端不能一次全部读完，产生半包。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;什么时候需要考虑处理半包和粘包？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TCP连接是长连接，即一次连接多次发送数据。&lt;/li&gt;
&lt;li&gt;每次发送的数据是结构的，比如 JSON格式的数据 或者 数据包的协议是由我们自己定义的（包头部包含实际数据长度、协议魔数等）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解决思路&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;定长分隔(每个数据包最大为该长度，不足时使用特殊字符填充) ，但是数据不足时会浪费传输资源&lt;/li&gt;
&lt;li&gt;使用特定字符来分割数据包，但是若数据中含有分割字符则会出现Bug&lt;/li&gt;
&lt;li&gt;在数据包中添加长度字段，弥补了以上两种思路的不足，推荐使用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过上述分析，我们最好通过第三种思路来解决拆包粘包问题。&lt;/p&gt;

&lt;p&gt;Golang的bufio库中有为我们提供了Scanner，来解决这类分割数据的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Scanner
Scanner provides a convenient interface for reading data such as a file of newline-delimited lines of text. Successive calls to the Scan method will step through the &#39;tokens&#39; of a file, skipping the bytes between the tokens. The specification of a token is defined by a split function of type SplitFunc; the default split function breaks the input into lines with line termination stripped. Split functions are defined in this package for scanning a file into lines, bytes, UTF-8-encoded runes, and space-delimited words. The client may instead provide a custom split function.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;简单来讲即是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Scanner为 读取数据 提供了方便的 接口。连续调用Scan方法会逐个得到文件的“tokens”，跳过 tokens 之间的字节。token 的规范由 SplitFunc 类型的函数定义。我们可以改为提供自定义拆分功能。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来看看 SplitFunc 类型的函数是什么样子的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SplitFunc func(data []byte, atEOF bool) (advance int, token []byte, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    // An artificial input source.
    const input = &amp;quot;1234 5678 1234567901234567890&amp;quot;
    scanner := bufio.NewScanner(strings.NewReader(input))
    // Create a custom split function by wrapping the existing ScanWords function.
    split := func(data []byte, atEOF bool) (advance int, token []byte, err error) {
        advance, token, err = bufio.ScanWords(data, atEOF)
        if err == nil &amp;amp;&amp;amp; token != nil {
            _, err = strconv.ParseInt(string(token), 10, 32)
        }
        return
    }
    // Set the split function for the scanning operation.
    scanner.Split(split)
    // Validate the input
    for scanner.Scan() {
        fmt.Printf(&amp;quot;%s\n&amp;quot;, scanner.Text())
    }

    if err := scanner.Err(); err != nil {
        fmt.Printf(&amp;quot;Invalid input: %s&amp;quot;, err)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;于是，我们可以这样改写我们的程序：&lt;/p&gt;

&lt;p&gt;服务端代码 server/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    l, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:4044&amp;quot;)
    if err != nil {
        panic(err)
    }
    fmt.Println(&amp;quot;listen to 4044&amp;quot;)
    for {
        conn, err := l.Accept()
        if err != nil {
            fmt.Println(&amp;quot;conn err:&amp;quot;, err)
        } else {
            go handleConn2(conn)
        }
    }
}

func packetSlitFunc(data []byte, atEOF bool) (advance int, token []byte, err error) {
        // 检查 atEOF 参数 和 数据包头部的四个字节是否 为 0x123456(我们定义的协议的魔数)
    if !atEOF &amp;amp;&amp;amp; len(data) &amp;gt; 6 &amp;amp;&amp;amp; binary.BigEndian.Uint32(data[:4]) == 0x123456 {
        var l int16
                // 读出 数据包中 实际数据 的长度(大小为 0 ~ 2^16)
        binary.Read(bytes.NewReader(data[4:6]), binary.BigEndian, &amp;amp;l)
        pl := int(l) + 6
        if pl &amp;lt;= len(data) {
            return pl, data[:pl], nil
        }
    }
    return
}

func handleConn2(conn net.Conn) {
    defer conn.Close()
    defer fmt.Println(&amp;quot;关闭&amp;quot;)
    fmt.Println(&amp;quot;新连接：&amp;quot;, conn.RemoteAddr())
    result := bytes.NewBuffer(nil)
        var buf [65542]byte // 由于 标识数据包长度 的只有两个字节 故数据包最大为 2^16+4(魔数)+2(长度标识)
    for {
        n, err := conn.Read(buf[0:])
        result.Write(buf[0:n])
        if err != nil {
            if err == io.EOF {
                continue
            } else {
                fmt.Println(&amp;quot;read err:&amp;quot;, err)
                break
            }
        } else {
            scanner := bufio.NewScanner(result)
            scanner.Split(packetSlitFunc)
            for scanner.Scan() {
                fmt.Println(&amp;quot;recv:&amp;quot;, string(scanner.Bytes()[6:]))
            }
        }
        result.Reset()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;复制代码客户端代码 client/main.go&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    data := []byte(&amp;quot;[这里才是一个完整的数据包]&amp;quot;)
    l := len(data)
    fmt.Println(l)
    magicNum := make([]byte, 4)
    binary.BigEndian.PutUint32(magicNum, 0x123456)
    lenNum := make([]byte, 2)
    binary.BigEndian.PutUint16(lenNum, uint16(l))
    packetBuf := bytes.NewBuffer(magicNum)
    packetBuf.Write(lenNum)
    packetBuf.Write(data)
    conn, err := net.DialTimeout(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:4044&amp;quot;, time.Second*30)
    if err != nil {
        fmt.Printf(&amp;quot;connect failed, err : %v\n&amp;quot;, err.Error())
                return
    }
    for i := 0; i &amp;lt;1000; i++ {
        _, err = conn.Write(packetBuf.Bytes())
        if err != nil {
            fmt.Printf(&amp;quot;write failed , err : %v\n&amp;quot;, err)
            break
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;复制代码运行结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;listen to 4044
新连接： [::1]:55738
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
recv: [这里才是一个完整的数据包]
...省略其它的...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;编写器&#34;&gt;编写器&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;type Writer
type Writer interface {
    //Write() 方法有两个返回值，一个是写入到目标资源的字节数，一个是发生错误时的错误。
    Write(p []byte) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;io.Writer 表示一个编写器，它从缓冲区读取数据，并将数据写入目标资源。&lt;/p&gt;

&lt;p&gt;实现这个接口就需要实现如下的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Write 将 len(p) 个字节从 p 中写入到基本数据流中。它返回从 p 中被写入的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)）以及任何遇到的引起写入提前停止的错误。若 Write 返回的 n &amp;lt; len(p)，它就必须返回一个 非nil 的错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于要用作编写器的类型，必须实现 io.Writer 接口的唯一一个方法 Write(p []byte),同样，只要实现了 Write(p []byte) ，那它就是一个编写器。举例，标准库&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    proverbs := []string{
        &amp;quot;Channels orchestrate mutexes serialize&amp;quot;,
        &amp;quot;Cgo is not Go&amp;quot;,
        &amp;quot;Errors are values&amp;quot;,
        &amp;quot;Don&#39;t panic&amp;quot;,
    }
    var writer bytes.Buffer

    for _, p := range proverbs {
        n, err := writer.Write([]byte(p))
        if err != nil {
            fmt.Println(err)
            os.Exit(1)
        }
        if n != len(p) {
            fmt.Println(&amp;quot;failed to write data&amp;quot;)
            os.Exit(1)
        }
    }

    fmt.Println(writer.String())
}
输出打印的内容：
Channels orchestrate mutexes serializeCgo is not GoErrors are valuesDon&#39;t panic
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;自定义writer&#34;&gt;自定义Writer&lt;/h3&gt;

&lt;p&gt;下面我们来实现一个名为 chanWriter 的自定义 io.Writer ，它将其内容作为字节序列写入 channel 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type chanWriter struct {
    // ch 实际上就是目标资源
    ch chan byte
}

func newChanWriter() *chanWriter {
    return &amp;amp;chanWriter{make(chan byte, 1024)}
}

func (w *chanWriter) Chan() &amp;lt;-chan byte {
    return w.ch
}

func (w *chanWriter) Write(p []byte) (int, error) {
    n := 0
    // 遍历输入数据，按字节写入目标资源
    for _, b := range p {
        w.ch &amp;lt;- b
        n++
    }
    return n, nil
}

func (w *chanWriter) Close() error {
    close(w.ch)
    return nil
}

func main() {
    writer := newChanWriter()
    go func() {
        defer writer.Close()
        writer.Write([]byte(&amp;quot;Stream &amp;quot;))
        writer.Write([]byte(&amp;quot;me!&amp;quot;))
    }()
    for c := range writer.Chan() {
        fmt.Printf(&amp;quot;%c&amp;quot;, c)
    }
    fmt.Println()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要使用这个 Writer，只需在函数 main() 中调用 writer.Write()（在单独的goroutine中）。&lt;/p&gt;

&lt;p&gt;因为 chanWriter 还实现了接口 io.Closer ，所以调用方法 writer.Close() 来正确地关闭channel，以避免发生泄漏和死锁。&lt;/p&gt;

&lt;h2 id=&#34;closer&#34;&gt;closer&lt;/h2&gt;

&lt;p&gt;Closer 接口包装了基本的 Close 方法，用于关闭数据读写。Close 一般用于关闭文件，关闭通道，关闭连接，关闭数据库等，在不同的标准库实现中实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Closer interface {
    Close() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;seeker&#34;&gt;seeker&lt;/h2&gt;

&lt;p&gt;Seeker 接口包装了基本的 Seek 方法，用于移动数据的读写指针。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Seeker interface {
    Seek(offset int64, whence int) (ret int64, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seek 设置下一次读写操作的指针位置，每次的读写操作都是从指针位置开始的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;whence 的含义：

&lt;ul&gt;
&lt;li&gt;如果 whence 为 0：表示从数据的开头开始移动指针。&lt;/li&gt;
&lt;li&gt;如果 whence 为 1：表示从数据的当前指针位置开始移动指针。&lt;/li&gt;
&lt;li&gt;如果 whence 为 2：表示从数据的尾部开始移动指针。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;offset 是指针移动的偏移量。&lt;/li&gt;
&lt;li&gt;返回新指针位置和遇到的错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;whence 的值，在 io 包中定义了相应的常量，应该使用这些常量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
  SeekStart   = 0 // seek relative to the origin of the file
  SeekCurrent = 1 // seek relative to the current offset
  SeekEnd     = 2 // seek relative to the end
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而原先 os 包中的常量已经被标注为Deprecated&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Deprecated: Use io.SeekStart, io.SeekCurrent, and io.SeekEnd.
const (
  SEEK_SET int = 0 // seek relative to the origin of the file
  SEEK_CUR int = 1 // seek relative to the current offset
  SEEK_END int = 2 // seek relative to the end
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;组合接口&#34;&gt;组合接口&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;type ReadWriter interface {
    Reader
    Writer
}

type ReadSeeker interface {
    Reader
    Seeker
}

type WriteSeeker interface {
    Writer
    Seeker
}

type ReadWriteSeeker interface {
    Reader
    Writer
    Seeker
}

type ReadCloser interface {
    Reader
    Closer
}

type WriteCloser interface {
    Writer
    Closer
}

type ReadWriteCloser interface {
    Reader
    Writer
    Closer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些接口的作用是：有些时候同时需要某两个接口的所有功能，即必须同时实现了某两个接口的类型才能够被传入使用。可见，io 包中有大量的“小接口”，这样方便组合为“大接口”。&lt;/p&gt;

&lt;h2 id=&#34;其他接口&#34;&gt;其他接口&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;ReaderFrom&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReaderFrom 接口包装了基本的 ReadFrom 方法，用于从 r 中读取数据存入自身。直到遇到 EOF 或读取出错为止，返回读取的字节数和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ReaderFrom interface {
    ReadFrom(r Reader) (n int64, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ReadFrom 从 r 中读取数据，直到 EOF 或发生错误。其返回值 n 为读取的字节数。除 io.EOF 之外，在读取过程中遇到的任何错误也将被返回。&lt;/li&gt;
&lt;li&gt;如果 ReaderFrom 可用，Copy 函数就会使用它。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实例：将文件中的数据全部读取（显示在标准输出）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file, err := os.Open(&amp;quot;writeAt.txt&amp;quot;)
if err != nil {
    panic(err)
}
defer file.Close()
writer := bufio.NewWriter(os.Stdout)
writer.ReadFrom(file)
writer.Flush()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然，我们可以通过 ioutil 包的 ReadFile 函数获取文件全部内容。其实，跟踪一下 ioutil.ReadFile 的源码，会发现其实也是通过 ReadFrom 方法实现（用的是 bytes.Buffer，它实现了 ReaderFrom 接口）。&lt;/p&gt;

&lt;p&gt;如果不通过 ReadFrom 接口来做这件事，而是使用 io.Reader 接口，我们有两种思路：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;先获取文件的大小（File 的 Stat 方法），之后定义一个该大小的 []byte，通过 Read 一次性读取&lt;/li&gt;
&lt;li&gt;定义一个小的 []byte，不断的调用 Read 方法直到遇到 EOF，将所有读取到的 []byte 连接到一起&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;WriterTo&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;WriterTo 接口包装了基本的 WriteTo 方法，用于将自身的数据写入 w 中。直到数据全部写入完毕或遇到错误为止，返回写入的字节数和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type WriterTo interface {
    WriteTo(w Writer) (n int64, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;WriteTo 将数据写入 w 中，直到没有数据可写或发生错误。其返回值 n 为写入的字节数。 在写入过程中遇到的任何错误也将被返回。&lt;/li&gt;
&lt;li&gt;如果 WriterTo 可用，Copy 函数就会使用它。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;读者是否发现，其实 ReaderFrom 和 WriterTo 接口的方法接收的参数是 io.Reader 和 io.Writer 类型。根据 io.Reader 和 io.Writer 接口的讲解，对该接口的使用应该可以很好的掌握。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ReaderAt&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReaderAt 接口包装了基本的 ReadAt 方法，用于将自身的数据写入 p 中。ReadAt 忽略之前的读写位置，从起始位置的 off 偏移处开始读取。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ReaderAt interface {
    ReadAt(p []byte, off int64) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ReadAt 从基本输入源的偏移量 off 处开始，将 len(p) 个字节读取到 p 中。它返回读取的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)）以及任何遇到的错误。&lt;/li&gt;
&lt;li&gt;当 ReadAt 返回的 n &amp;lt; len(p) 时，它就会返回一个 非nil 的错误来解释 为什么没有返回更多的字节。在这一点上，ReadAt 比 Read 更严格。&lt;/li&gt;
&lt;li&gt;即使 ReadAt 返回的 n &amp;lt; len(p)，它也会在调用过程中使用 p 的全部作为暂存空间。若可读取的数据不到 len(p) 字节，ReadAt 就会阻塞,直到所有数据都可用或一个错误发生。 在这一点上 ReadAt 不同于 Read。&lt;/li&gt;
&lt;li&gt;若 n = len(p) 个字节从输入源的结尾处由 ReadAt 返回，Read可能返回 err == EOF 或者 err == nil&lt;/li&gt;
&lt;li&gt;若 ReadAt 携带一个偏移量从输入源读取，ReadAt 应当既不影响偏移量也不被它所影响。&lt;/li&gt;
&lt;li&gt;可对相同的输入源并行执行 ReadAt 调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;标准库上面说的很多都是实现了这个接口，简单示例代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reader := strings.NewReader(&amp;quot;Go语言中文网&amp;quot;)
p := make([]byte, 6)
n, err := reader.ReadAt(p, 2)
if err != nil {
    panic(err)
}
fmt.Printf(&amp;quot;%s, %d\n&amp;quot;, p, n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;语言, 6
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;WriterAt&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;WriterAt 接口包装了基本的 WriteAt 方法，用于将 p 中的数据写入自身。ReadAt 忽略之前的读写位置，从起始位置的 off 偏移处开始写入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type WriterAt interface {
    WriteAt(p []byte, off int64) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要实现接口的功能&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;WriteAt 从 p 中将 len(p) 个字节写入到偏移量 off 处的基本数据流中。它返回从 p 中被写入的字节数 n（0 &amp;lt;= n &amp;lt;= len(p)）以及任何遇到的引起写入提前停止的错误。若 WriteAt 返回的 n &amp;lt; len(p)，它就必须返回一个 非nil 的错误。&lt;/li&gt;
&lt;li&gt;若 WriteAt 携带一个偏移量写入到目标中，WriteAt 应当既不影响偏移量也不被它所影响。&lt;/li&gt;
&lt;li&gt;若被写区域没有重叠，可对相同的目标并行执行 WriteAt 调用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;os.File 实现了 WriterAt 接口，实例如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file, err := os.Create(&amp;quot;writeAt.txt&amp;quot;)
if err != nil {
    panic(err)
}
defer file.Close()
file.WriteString(&amp;quot;Golang中文社区——这里是多余&amp;quot;)
n, err := file.WriteAt([]byte(&amp;quot;Go语言中文网&amp;quot;), 24)
if err != nil {
    panic(err)
}
fmt.Println(n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Golang中文社区——Go语言中文网。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分析：file.WriteString(&amp;ldquo;Golang中文社区——这里是多余&amp;rdquo;) 往文件中写入 Golang中文社区——这里是多余，之后 file.WriteAt([]byte(&amp;ldquo;Go语言中文网&amp;rdquo;), 24) 在文件流的 offset=24 处写入 Go语言中文网（会覆盖该位置的内容）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ByteReader和ByteWriter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ByteReader 接口包装了基本的 ReadByte 方法，用于从自身读出一个字节。返回读出的字节和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ByteReader interface {
    ReadByte() (c byte, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ByteWriter 接口包装了基本的 WriteByte 方法，用于将一个字节写入自身返回遇到的错误&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ByteWriter interface {
    WriteByte(c byte) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这组接口在标准库中也有实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bufio.Reader/Writer 分别实现了io.ByteReader 和 io.ByteWriter
bytes.Buffer 同时实现了 io.ByteReader 和 io.ByteWriter
bytes.Reader 实现了 io.ByteReader
strings.Reader 实现了 io.ByteReader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ch byte
fmt.Scanf(&amp;quot;%c\n&amp;quot;, &amp;amp;ch)

buffer := new(bytes.Buffer)
err := buffer.WriteByte(ch)
if err == nil {
    fmt.Println(&amp;quot;写入一个字节成功！准备读取该字节……&amp;quot;)
    newCh, _ := buffer.ReadByte()
    fmt.Printf(&amp;quot;读取的字节：%c\n&amp;quot;, newCh)
} else {
    fmt.Println(&amp;quot;写入错误&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;ByteScanner&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ByteScanner 在 ByteReader 的基础上增加了一个 UnreadByte 方法，用于撤消最后一次的 ReadByte 操作，以便下次的 ReadByte 操作可以读出与前一次一样的数据。UnreadByte 之前必须是 ReadByte 才能撤消成功，否则可能会返回一个错误信息（根据不同的需求，UnreadByte 也可能返回 nil，允许随意调用 UnreadByte，但只有最后一次的 ReadByte 可以被撤销，其它 UnreadByte 不执行任何操作）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ByteScanner interface {
    ByteReader
    UnreadByte() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;RuneReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;RuneReader 接口包装了基本的 ReadRune 方法，用于从自身读取一个 UTF-8 编码的字符到 r 中。返回读取的字符、字符的编码长度和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type RuneReader interface {
    ReadRune() (r rune, size int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;RuneScanner&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;RuneScanner 在 RuneReader 的基础上增加了一个 UnreadRune 方法，用于撤消最后一次的 ReadRune 操作，以便下次的 ReadRune 操作可以读出与前一次一样的数据。UnreadRune 之前必须是 ReadRune 才能撤消成功，否则可能会返回一个错误信息（根据不同的需求，UnreadRune 也可能返回 nil，允许随意调用 UnreadRune，但只有最后一次的 ReadRune 可以被撤销，其它 UnreadRune 不执行任何操作）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type RuneScanner interface {
    RuneReader
    UnreadRune() error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;bytes.NewBuffer 实现了很多基本的接口，可以通过 bytes 包学习接口的实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    buf := bytes.NewBuffer([]byte(&amp;quot;Hello World!&amp;quot;))
    b := make([]byte, buf.Len())

    n, err := buf.Read(b)
    fmt.Printf(&amp;quot;%s   %v\n&amp;quot;, b[:n], err)
    // Hello World!   &amp;lt;nil&amp;gt;

    buf.WriteString(&amp;quot;ABCDEFG\n&amp;quot;)
    buf.WriteTo(os.Stdout)
    // ABCDEFG

    n, err = buf.Write(b)
    fmt.Printf(&amp;quot;%d   %s   %v\n&amp;quot;, n, buf.String(), err)
    // 12   Hello World!   &amp;lt;nil&amp;gt;

    c, err := buf.ReadByte()
    fmt.Printf(&amp;quot;%c   %s   %v\n&amp;quot;, c, buf.String(), err)
    // H   ello World!   &amp;lt;nil&amp;gt;

    c, err = buf.ReadByte()
    fmt.Printf(&amp;quot;%c   %s   %v\n&amp;quot;, c, buf.String(), err)
    // e   llo World!   &amp;lt;nil&amp;gt;

    err = buf.UnreadByte()
    fmt.Printf(&amp;quot;%s   %v\n&amp;quot;, buf.String(), err)
    // ello World!   &amp;lt;nil&amp;gt;

    err = buf.UnreadByte()
    fmt.Printf(&amp;quot;%s   %v\n&amp;quot;, buf.String(), err)
    // ello World!   bytes.Buffer: UnreadByte: previous operation was not a read
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;

&lt;p&gt;io包中定义了很多原生的类型。都是实现了上面的接口，可以直接创建使用的类型。&lt;/p&gt;

&lt;h2 id=&#34;sectionreader-类型&#34;&gt;SectionReader 类型&lt;/h2&gt;

&lt;p&gt;SectionReader 是一个 struct（没有任何导出的字段），实现了 Read, Seek 和 ReadAt，同时，内嵌了 ReaderAt 接口。结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type SectionReader struct {
    r     ReaderAt    // 该类型最终的 Read/ReadAt 最终都是通过 r 的 ReadAt 实现
    base  int64        // NewSectionReader 会将 base 设置为 off
    off   int64        // 从 r 中的 off 偏移处开始读取数据
    limit int64        // limit - off = SectionReader 流的长度
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从名称我们可以猜到，该类型读取数据流中部分数据。看一下常见的创建函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewSectionReader(r ReaderAt, off int64, n int64) *SectionReader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewSectionReader 返回一个 SectionReader，它从 r 中的偏移量 off 处读取 n 个字节后以 EOF 停止。也就是说，SectionReader 只是内部（内嵌）ReaderAt 表示的数据流的一部分：从 off 开始后的 n 个字节。这个类型的作用是：方便重复操作某一段 (section) 数据流；或者同时需要 ReadAt 和 Seek 的功能。&lt;/p&gt;

&lt;h2 id=&#34;limitedreader-类型&#34;&gt;LimitedReader 类型&lt;/h2&gt;

&lt;p&gt;LimitedReader 结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type LimitedReader struct {
    R Reader // underlying reader，最终的读取操作通过 R.Read 完成
    N int64  // max bytes remaining
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从 R 读取但将返回的数据量限制为 N 字节。每调用一次 Read 都将更新 N 来反应新的剩余数量。也就是说，最多只能返回 N 字节数据。LimitedReader 只实现了 Read 方法（Reader 接口）。&lt;/p&gt;

&lt;p&gt;使用示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;content := &amp;quot;This Is LimitReader Example&amp;quot;
reader := strings.NewReader(content)
limitReader := &amp;amp;io.LimitedReader{R: reader, N: 8}
for limitReader.N &amp;gt; 0 {
    tmp := make([]byte, 2)
    limitReader.Read(tmp)
    fmt.Printf(&amp;quot;%s&amp;quot;, tmp)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This Is
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，通过该类型可以达到 只允许读取一定长度数据 的目的。&lt;/p&gt;

&lt;p&gt;在 io 包中，LimitReader 函数的实现其实就是调用 LimitedReader：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LimitReader(r Reader, n int64) Reader { return &amp;amp;LimitedReader{r, n} }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pipereader-和-pipewriter-类型&#34;&gt;PipeReader 和 PipeWriter 类型&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;PipeReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PipeReader（一个没有任何导出字段的 struct）是管道的读取端。它实现了 io.Reader 和 io.Closer 接口。结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PipeReader struct {
    p *pipe
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 PipeReader.Read 方法的说明：从管道中读取数据。该方法会堵塞，直到管道写入端开始写入数据或写入端被关闭。如果写入端关闭时带有 error（即调用 CloseWithError 关闭），该Read返回的 err 就是写入端传递的error；否则 err 为 EOF。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;PipeWriter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PipeWriter（一个没有任何导出字段的 struct）是管道的写入端。它实现了 io.Writer 和 io.Closer 接口。结构定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PipeWriter struct {
    p *pipe
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于 PipeWriter.Write 方法的说明：写数据到管道中。该方法会堵塞，直到管道读取端读完所有数据或读取端被关闭。如果读取端关闭时带有 error（即调用 CloseWithError 关闭），该Write返回的 err 就是读取端传递的error；否则 err 为 ErrClosedPipe。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Pipe&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;io.Pipe() 用于创建一个同步的内存管道 (synchronous in-memory pipe)，函数签名：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Pipe() (*PipeReader, *PipeWriter)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它将 io.Reader 连接到 io.Writer。一端的读取匹配另一端的写入，直接在这两端之间复制数据；它没有内部缓存。它对于并行调用 Read 和 Write 以及其它函数或 Close 来说都是安全的。一旦等待的 I/O 结束，Close 就会完成。并行调用 Read 或并行调用 Write 也同样安全：同种类的调用将按顺序进行控制。&lt;/p&gt;

&lt;p&gt;正因为是同步的，因此不能在一个 goroutine 中进行读和写。&lt;/p&gt;

&lt;p&gt;读关闭管道&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *PipeReader) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读关闭管道并传入错误信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (r *PipeReader) CloseWithError(err error) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从管道中读取数据，如果管道被关闭，则会返会一个错误信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、如果写入端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。&lt;/li&gt;
&lt;li&gt;2、如果写入端通过 Close 方法关闭了管道，则返回 io.EOF。&lt;/li&gt;
&lt;li&gt;3、如果是读取端关闭了管道，则返回 io.ErrClosedPipe。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例：管道（读取端关闭）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r, w := io.Pipe()
    // 启用一个例程进行读取
    go func() {
        buf := make([]byte, 5)
        for n, err := 0, error(nil); err == nil; {
            n, err = r.Read(buf)
            r.CloseWithError(errors.New(&amp;quot;管道被读取端关闭&amp;quot;))
            fmt.Printf(&amp;quot;读取：%d, %v, %s\n&amp;quot;, n, err, buf[:n])
        }
    }()
    // 主例程进行写入
    n, err := w.Write([]byte(&amp;quot;Hello World !&amp;quot;))
    fmt.Printf(&amp;quot;写入：%d, %v\n&amp;quot;, n, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写关闭管道&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (w *PipeWriter) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写关闭管道并传入错误信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (w *PipeWriter) CloseWithError(err error) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;向管道中写入数据，如果管道被关闭，则会返会一个错误信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、如果读取端通过 CloseWithError 方法关闭了管道，则返回关闭时传入的错误信息。&lt;/li&gt;
&lt;li&gt;2、如果读取端通过 Close 方法关闭了管道，则返回 io.ErrClosedPipe。&lt;/li&gt;
&lt;li&gt;3、如果是写入端关闭了管道，则返回 io.ErrClosedPipe。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例：管道（写入端关闭）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r, w := io.Pipe()
    // 启用一个例程进行读取
    go func() {
        buf := make([]byte, 5)
        for n, err := 0, error(nil); err == nil; {
            n, err = r.Read(buf)
            fmt.Printf(&amp;quot;读取：%d, %v, %s\n&amp;quot;, n, err, buf[:n])
        }
    }()
    // 主例程进行写入
    n, err := w.Write([]byte(&amp;quot;Hello World !&amp;quot;))
    fmt.Printf(&amp;quot;写入：%d, %v\n&amp;quot;, n, err)

    w.CloseWithError(errors.New(&amp;quot;管道被写入端关闭&amp;quot;))
    n, err = w.Write([]byte(&amp;quot;Hello World !&amp;quot;))
    fmt.Printf(&amp;quot;写入：%d, %v\n&amp;quot;, n, err)
    time.Sleep(time.Second * 1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;综合使用实例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    pipeReader, pipeWriter := io.Pipe()
    go PipeWrite(pipeWriter)
    go PipeRead(pipeReader)
    time.Sleep(30 * time.Second)
}

func PipeWrite(writer *io.PipeWriter){
    data := []byte(&amp;quot;Go语言中文网&amp;quot;)
    for i := 0; i &amp;lt; 3; i++{
        n, err := writer.Write(data)
        if err != nil{
            fmt.Println(err)
            return
        }
        fmt.Printf(&amp;quot;写入字节 %d\n&amp;quot;,n)
    }
    writer.CloseWithError(errors.New(&amp;quot;写入段已关闭&amp;quot;))
}

func PipeRead(reader *io.PipeReader){
    buf := make([]byte, 128)
    for{
        fmt.Println(&amp;quot;接口端开始阻塞5秒钟...&amp;quot;)
        time.Sleep(5 * time.Second)
        fmt.Println(&amp;quot;接收端开始接受&amp;quot;)
        n, err := reader.Read(buf)
        if err != nil{
            fmt.Println(err)
            return
        }
        fmt.Printf(&amp;quot;收到字节: %d\n buf内容: %s\n&amp;quot;,n,buf)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;函数&#34;&gt;函数&lt;/h1&gt;

&lt;p&gt;io包中也有一下原生实现可以使用的函数。其实都是直接操作结构体的函数。&lt;/p&gt;

&lt;h2 id=&#34;writestring&#34;&gt;WriteString&lt;/h2&gt;

&lt;p&gt;WriteString 将字符串 s 写入到 w 中，返回写入的字节数和遇到的错误。如果 w 实现了 WriteString 方法，则优先使用该方法将 s 写入 w 中。否则，将 s 转换为 []byte，然后调用 w.Write 方法将数据写入 w 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func WriteString(w Writer, s string) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;readatleast&#34;&gt;ReadAtLeast&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadAtLeast&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReadAtLeast 从 r 中读取数据到 buf 中，要求至少读取 min 个字节。返回读取的字节数和遇到的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ReadAtLeast(r Reader, buf []byte, min int) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 min 超出了 buf 的容量，则 err 返回 io.ErrShortBuffer，否则：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、读出的数据长度 == 0  ，则 err 返回 EOF。&lt;/li&gt;
&lt;li&gt;2、读出的数据长度 &amp;lt;  min，则 err 返回 io.ErrUnexpectedEOF。&lt;/li&gt;
&lt;li&gt;3、读出的数据长度 &amp;gt;= min，则 err 返回 nil。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;ReadFull&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReadFull 的功能和 ReadAtLeast 一样，只不过 min = len(buf)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ReadFull(r Reader, buf []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：WriteString、ReadAtLeast、ReadFull&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    io.WriteString(os.Stdout, &amp;quot;Hello World!\n&amp;quot;)
    // Hello World!

    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    b := make([]byte, 15)

    n, err := io.ReadAtLeast(r, b, 20)
    fmt.Printf(&amp;quot;%q   %d   %v\n&amp;quot;, b[:n], n, err)
    // &amp;quot;&amp;quot;   0   short buffer

    r.Seek(0, 0)
    b = make([]byte, 15)

    n, err = io.ReadFull(r, b)
    fmt.Printf(&amp;quot;%q   %d   %v\n&amp;quot;, b[:n], n, err)
    // &amp;quot;Hello World!&amp;quot;   12   unexpected EOF
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;LimitReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;LimitReader 对 r 进行封装，使其最多只能读取 n 个字节的数据。相当于对 r 做了一个切片 r[:n] 返回。底层实现是一个 *LimitedReader（只有一个 Read 方法）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LimitReader(r Reader, n int64) Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;MultiReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;MultiReader 将多个 Reader 封装成一个单独的 Reader，多个 Reader 会按顺序读取，当多个 Reader 都返回 EOF 之后，单独的 Reader 才返回 EOF，否则返回读取过程中遇到的任何错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MultiReader(readers ...Reader) Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;MultiWriter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;MultiWriter 将向自身写入的数据同步写入到所有 writers 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MultiWriter(writers ...Writer) Writer
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;TeeReader&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;TeeReader 对 r 进行封装，使 r 在读取数据的同时，自动向 w 中写入数据。它是一个无缓冲的 Reader，所以对 w 的写入操作必须在 r 的 Read 操作结束之前完成。所有写入时遇到的错误都会被作为 Read 方法的 err 返回。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func TeeReader(r Reader, w Writer) Reader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 LimitReader&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    lr := io.LimitReader(r, 5)

    n, err := io.Copy(os.Stdout, lr)  // Hello
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err) // 5   &amp;lt;nil&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 MultiReader&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r1 := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    r2 := strings.NewReader(&amp;quot;ABCDEFG&amp;quot;)
    r3 := strings.NewReader(&amp;quot;abcdefg&amp;quot;)
    b := make([]byte, 15)
    mr := io.MultiReader(r1, r2, r3)

    for n, err := 0, error(nil); err == nil; {
        n, err = mr.Read(b)
        fmt.Printf(&amp;quot;%q\n&amp;quot;, b[:n])
    }
    // &amp;quot;Hello World!&amp;quot;
    // &amp;quot;ABCDEFG&amp;quot;
    // &amp;quot;abcdefg&amp;quot;
    // &amp;quot;&amp;quot;

    r1.Seek(0, 0)
    r2.Seek(0, 0)
    r3.Seek(0, 0)
    mr = io.MultiReader(r1, r2, r3)
    io.Copy(os.Stdout, mr)
    // Hello World!ABCDEFGabcdefg
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 MultiWriter&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!\n&amp;quot;)
    mw := io.MultiWriter(os.Stdout, os.Stdout, os.Stdout)

    r.WriteTo(mw)
    // Hello World!
    // Hello World!
    // Hello World!
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例 TeeReader&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    b := make([]byte, 15)
    tr := io.TeeReader(r, os.Stdout)

    n, err := tr.Read(b)                  // Hello World!
    fmt.Printf(&amp;quot;\n%s   %v\n&amp;quot;, b[:n], err) // Hello World!   &amp;lt;nil&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;copy&#34;&gt;Copy&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;CopyN&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CopyN 从 src 中复制 n 个字节的数据到 dst 中，返回复制的字节数和遇到的错误。只有当 written = n 时，err 才返回 nil。如果 dst 实现了 ReadFrom 方法，则优先调用该方法执行复制操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func CopyN(dst Writer, src Reader, n int64) (written int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Copy&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Copy 从 src 中复制数据到 dst 中，直到所有数据都复制完毕，返回复制的字节数和遇到的错误。如果复制过程成功结束，则 err 返回 nil，而不是 EOF，因为 Copy 的定义为“直到所有数据都复制完毕”，所以不会将 EOF 视为错误返回。如果 src 实现了 WriteTo 方法，则调用 src.WriteTo(dst) 复制数据，否则如果 dst 实现了 ReadeFrom 方法，则调用 dst.ReadeFrom(src) 复制数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Copy(dst Writer, src Reader) (written int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;CopyBuffer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CopyBuffer 相当于 Copy，只不 Copy 在执行的过程中会创建一个临时的缓冲区来中转数据，而 CopyBuffer 则可以单独提供一个缓冲区，让多个复制操作共用同一个缓冲区，避免每次复制操作都创建新的缓冲区。如果 buf == nil，则 CopyBuffer 会自动创建缓冲区。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func CopyBuffer(dst Writer, src Reader, buf []byte) (written int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例：CopyN、Copy、CopyBuffer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    r := strings.NewReader(&amp;quot;Hello World!&amp;quot;)
    buf := make([]byte, 32)

    n, err := io.CopyN(os.Stdout, r, 5) // Hello
    fmt.Printf(&amp;quot;\n%d   %v\n\n&amp;quot;, n, err) // 5   &amp;lt;nil&amp;gt;

    r.Seek(0, 0)
    n, err = io.Copy(os.Stdout, r)      // Hello World!
    fmt.Printf(&amp;quot;\n%d   %v\n\n&amp;quot;, n, err) // 12   &amp;lt;nil&amp;gt;

    r.Seek(0, 0)
    r2 := strings.NewReader(&amp;quot;ABCDEFG&amp;quot;)
    r3 := strings.NewReader(&amp;quot;abcdefg&amp;quot;)

    n, err = io.CopyBuffer(os.Stdout, r, buf) // Hello World!
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err)         // 12   &amp;lt;nil&amp;gt;

    n, err = io.CopyBuffer(os.Stdout, r2, buf) // ABCDEFG
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err)          // 7   &amp;lt;nil&amp;gt;

    n, err = io.CopyBuffer(os.Stdout, r3, buf) // abcdefg
    fmt.Printf(&amp;quot;\n%d   %v\n&amp;quot;, n, err)          // 7   &amp;lt;nil&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数还是我们在网络消息流量转发的时候还是经常使用的。&lt;/p&gt;

&lt;h1 id=&#34;场景举例&#34;&gt;场景举例&lt;/h1&gt;

&lt;h2 id=&#34;base64编码成字符串&#34;&gt;base64编码成字符串&lt;/h2&gt;

&lt;p&gt;encoding/base64包中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewEncoder(enc *Encoding, w io.Writer) io.WriteCloser
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个用来做base64编码，但是仔细观察发现，它需要一个io.Writer作为输出目标，并用返回的WriteCloser的Write方法将结果写入目标，下面是Go官方文档的例子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input := []byte(&amp;quot;foo\x00bar&amp;quot;)
encoder := base64.NewEncoder(base64.StdEncoding, os.Stdout)
encoder.Write(input)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个例子是将结果写入到Stdout，如果我们希望得到一个字符串呢？可以用bytes.Buffer作为目标io.Writer：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input := []byte(&amp;quot;foo\x00bar&amp;quot;)
buffer := new(bytes.Buffer)
encoder := base64.NewEncoder(base64.StdEncoding, buffer)
encoder.Write(input)
fmt.Println(string(buffer.Bytes())
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;byte和struct之间正反序列化&#34;&gt;[]byte和struct之间正反序列化&lt;/h2&gt;

&lt;p&gt;这种场景经常用在基于字节的协议上，比如有一个具有固定长度的结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Protocol struct {
    Version     uint8
    BodyLen     uint16
    Reserved    [2]byte
    Unit        uint8
    Value       uint32
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过一个[]byte来反序列化得到这个Protocol，一种思路是遍历这个[]byte，然后逐一赋值。其实在encoding/binary包中有个方便的方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Read(r io.Reader, order ByteOrder, data interface{}) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个方法从一个io.Reader中读取字节，并已order指定的端模式，来给填充data（data需要是fixed-sized的结构或者类型）。要用到这个方法首先要有一个io.Reader，从上面的图中不难发现，我们可以这么写：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var p Protocol
var bin []byte
//...
binary.Read(bytes.NewReader(bin), binary.LittleEndian, &amp;amp;p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;换句话说，我们将一个[]byte转成了一个io.Reader。&lt;/p&gt;

&lt;p&gt;反过来，我们需要将Protocol序列化得到[]byte，使用encoding/binary包中有个对应的Write方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Write(w io.Writer, order ByteOrder, data interface{}) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过将[]byte转成一个io.Writer即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var p Protocol
buffer := new(bytes.Buffer)
//...
binary.Writer(buffer, binary.LittleEndian, p)
bin := buffer.Bytes()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;从流中按行读取&#34;&gt;从流中按行读取&lt;/h2&gt;

&lt;p&gt;比如对于常见的基于文本行的HTTP协议的读取，我们需要将一个流按照行来读取。本质上，我们需要一个基于缓冲的读写机制（读一些到缓冲，然后遍历缓冲中我们关心的字节或字符）。在Go中有一个bufio的包可以实现带缓冲的读写：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewReader(rd io.Reader) *Reader
func (b *Reader) ReadString(delim byte) (string, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个ReadString方法从io.Reader中读取字符串，直到delim，就返回delim和之前的字符串。如果将delim设置为\n，相当于按行来读取了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var conn net.Conn
//...
reader := NewReader(conn)
for {
    line, err := reader.ReadString([]byte(&#39;\n&#39;))
    //...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;string-to-byte&#34;&gt;string to byte&lt;/h2&gt;

&lt;p&gt;花式技（zuo）巧（si）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;string转[]byte
a := &amp;quot;Hello, playground&amp;quot;
fmt.Println([]byte(a))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等价于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a := &amp;quot;Hello, playground&amp;quot;
buf := new(bytes.Buffer)
buf.ReadFrom(strings.NewReader(a))
fmt.Println(buf.Bytes())
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;标准库中实现的读取器和编写器的实例&#34;&gt;标准库中实现的读取器和编写器的实例&lt;/h1&gt;

&lt;p&gt;目前，Go 文档中还没有直接列出实现了某个接口的所有类型。不过，我们可以通过查看标准库文档，列出实现了 io.Reader 或 io.Writer 接口的类型（导出的类型）：（注：godoc 命令支持额外参数 -analysis ，能列出都有哪些类型实现了某个接口，相关参考 godoc -h 或 Static analysis features of godoc。另外，还有一个地址：&lt;a href=&#34;http://docs.studygolang.com。&#34;&gt;http://docs.studygolang.com。&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/&#34;&gt;os.File&lt;/a&gt; 同时实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/#reader-类型&#34;&gt;strings.Reader&lt;/a&gt; 实现了 io.Reader&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bufio/&#34;&gt;bufio.Reader/Writer&lt;/a&gt; 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/&#34;&gt;bytes.Buffer&lt;/a&gt; 同时实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/&#34;&gt;bytes.Reader&lt;/a&gt; 实现了 io.Reader&lt;/li&gt;
&lt;li&gt;compress/gzip.Reader/Writer 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;crypto/cipher.StreamReader/StreamWriter 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;crypto/tls.Conn 同时实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;encoding/csv.Reader/Writer 分别实现了 io.Reader 和 io.Writer&lt;/li&gt;
&lt;li&gt;mime/multipart.Part 实现了 io.Reader&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net/&#34;&gt;net.Conn&lt;/a&gt; 分别实现了 io.Reader 和 io.Writer(Conn接口定义了Read/Write)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此之外，io 包本身也有这两个接口的实现类型。如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实现了 Reader 的类型：&lt;a href=&#34;#limitedreader-类型&#34;&gt;LimitedReader&lt;/a&gt;、&lt;a href=&#34;#pipereader-和-pipewriter-类型&#34;&gt;PipeReader&lt;/a&gt;、&lt;a href=&#34;#sectionreader-类型&#34;&gt;SectionReader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实现了 Writer 的类型：&lt;a href=&#34;#pipereader-和-pipewriter-类型&#34;&gt;PipeWriter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上类型中，常用的类型有，文件IO，缓冲IO，网络IO，在标准库中都有实现&lt;/p&gt;

&lt;p&gt;网络io/文件io/标准io&amp;ndash;其实就是操作网络数据和文件中的数据
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net/&#34;&gt;net.Conn&lt;/a&gt;, &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/&#34;&gt;os.Stdin&lt;/a&gt;, &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-os/&#34;&gt;os.File&lt;/a&gt;: 网络、标准输入输出、文件的流读取，对应&amp;mdash;frp就是基于这个基础上实现的&lt;/p&gt;

&lt;p&gt;缓存io&amp;ndash;其实就是操作缓存中的string，[]byte
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/#reader-类型&#34;&gt;strings.Reader&lt;/a&gt;，&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-strings/#builder-类型&#34;&gt;strings.Builder&lt;/a&gt;: 把字符串抽象成Reader
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/#reader-类型&#34;&gt;bytes.Reader&lt;/a&gt;: 把[]byte抽象成Reader
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bytes/#buffer-类型&#34;&gt;bytes.Buffer&lt;/a&gt;: 把[]byte抽象成Reader和Writer&lt;/p&gt;

&lt;p&gt;缓存io&amp;ndash;还是使用缓存，但是主要是对io.reader实例进行读写
- &lt;a href=&#34;https://kingjcy.github.io/post/golang/go-bufio/&#34;&gt;bufio.Reader/Writer&lt;/a&gt;: 抽象成带缓冲的流读取（比如按行读写）&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Go Net 协议层</title>
          <link>https://kingjcy.github.io/post/golang/go-net/</link>
          <pubDate>Mon, 11 Jul 2016 17:34:34 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-net/</guid>
          <description>&lt;p&gt;网络编程是go语言使用的一个核心模块。golang的网络封装使用对于底层socket或者上层的http，甚至是web服务都很友好。&lt;/p&gt;

&lt;h1 id=&#34;net&#34;&gt;net&lt;/h1&gt;

&lt;p&gt;net包提供了可移植的网络I/O接口，包括TCP/IP、UDP、域名解析和Unix域socket等方式的通信。其中每一种通信方式都使用 xxConn 结构体来表示，诸如IPConn、TCPConn等，这些结构体都实现了Conn接口，Conn接口实现了基本的读、写、关闭、获取远程和本地地址、设置timeout等功能。&lt;/p&gt;

&lt;p&gt;conn的接口定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Conn interface {
    // Read从连接中读取数据
    // Read方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    Read(b []byte) (n int, err error)
    // Write从连接中写入数据
    // Write方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    Write(b []byte) (n int, err error)
    // Close方法关闭该连接
    // 并会导致任何阻塞中的Read或Write方法不再阻塞并返回错误
    Close() error
    // 返回本地网络地址
    LocalAddr() Addr
    // 返回远端网络地址
    RemoteAddr() Addr
    // 设定该连接的读写deadline，等价于同时调用SetReadDeadline和SetWriteDeadline
    // deadline是一个绝对时间，超过该时间后I/O操作就会直接因超时失败返回而不会阻塞
    // deadline对之后的所有I/O操作都起效，而不仅仅是下一次的读或写操作
    // 参数t为零值表示不设置期限
    SetDeadline(t time.Time) error
    // 设定该连接的读操作deadline，参数t为零值表示不设置期限
    SetReadDeadline(t time.Time) error
    // 设定该连接的写操作deadline，参数t为零值表示不设置期限
    // 即使写入超时，返回值n也可能&amp;gt;0，说明成功写入了部分数据
    SetWriteDeadline(t time.Time) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后每种类型都是对应的结构体实现这些接口。&lt;/p&gt;

&lt;p&gt;还有一个常用的接口定义PacketConn&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PacketConn interface {
    // ReadFrom方法从连接读取一个数据包，并将有效信息写入b
    // ReadFrom方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    // 返回写入的字节数和该数据包的来源地址
    ReadFrom(b []byte) (n int, addr Addr, err error)
    // WriteTo方法将有效数据b写入一个数据包发送给addr
    // WriteTo方法可能会在超过某个固定时间限制后超时返回错误，该错误的Timeout()方法返回真
    // 在面向数据包的连接中，写入超时非常罕见
    WriteTo(b []byte, addr Addr) (n int, err error)
    // Close方法关闭该连接
    // 会导致任何阻塞中的ReadFrom或WriteTo方法不再阻塞并返回错误
    Close() error
    // 返回本地网络地址
    LocalAddr() Addr
    // 设定该连接的读写deadline
    SetDeadline(t time.Time) error
    // 设定该连接的读操作deadline，参数t为零值表示不设置期限
    // 如果时间到达deadline，读操作就会直接因超时失败返回而不会阻塞
    SetReadDeadline(t time.Time) error
    // 设定该连接的写操作deadline，参数t为零值表示不设置期限
    // 如果时间到达deadline，写操作就会直接因超时失败返回而不会阻塞
    // 即使写入超时，返回值n也可能&amp;gt;0，说明成功写入了部分数据
    SetWriteDeadline(t time.Time) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ip&#34;&gt;ip&lt;/h2&gt;

&lt;p&gt;使用IPConn结构体来表示，它实现了Conn、PacketConn两种接口。使用如下两个函数进行Dial（连接）和Listen（监听）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialIP(netProto string, laddr, raddr *IPAddr) (*IPConn, error)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DialIP在网络协议netProto上连接本地地址laddr和远端地址raddr，netProto必须是&amp;rdquo;ip&amp;rdquo;、&amp;rdquo;ip4&amp;rdquo;或&amp;rdquo;ip6&amp;rdquo;后跟冒号和协议名或协议号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenIP(netProto string, laddr *IPAddr) (*IPConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenIP创建一个接收目的地是本地地址laddr的IP数据包的网络连接，返回的*IPConn的ReadFrom和WriteTo方法可以用来发送和接收IP数据包。（每个包都可获取来源址或者设置目标地址）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、IPAddr类型&lt;/p&gt;

&lt;p&gt;位于iprawsock.go中在net包的许多函数和方法会返回一个指向IPAddr的指针。这不过只是一个包含IP类型的结构体。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type IPAddr struct {
    IP   IP
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个类型的另一个主要用途是通过IP主机名执行DNS查找。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ResolveIPAddr
ResolveIPAddr有两个参数第一个参数.必须为&amp;quot;ip&amp;quot;,&amp;quot;ip4&amp;quot;,&amp;quot;ip6&amp;quot;,第二个参数多为要解析的域名.返回一个IPAddr的指针类型

addr, _ := net.ResolveIPAddr(&amp;quot;ip&amp;quot;, &amp;quot;www.baidu.com&amp;quot;)
fmt.Println(addr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ip.go 中还定义了三个类型.分别是IP,IPMask,IPNet&lt;/p&gt;

&lt;p&gt;2、IP类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type IP []byte
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IP类型被定义为一个字节数组。 ParseIP(String) 可以将字符窜转换为一个IP类型.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name := &amp;quot;127.0.0.1&amp;quot;
addr := net.ParseIP(name)
fmt.Println(addr.IsLoopback())// IsLoopback reports whether ip is a loopback address.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、IPMask类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// An IP mask is an IP address.
type IPMask []byte
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个掩码的字符串形式是一个十六进制数，如掩码255.255.0.0为ffff0000。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func IPv4Mask(a, b, c, d byte) IPMask :用一个4字节的IPv4地址来创建一个掩码.
func CIDRMask(ones, bits int) IPMask : 用ones和bits来创建一个掩码
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4、IPNet类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// An IPNet represents an IP network.
type IPNet struct {
    IP   IP     // network number
    Mask IPMask // network mask
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由IP类型和IPMask组成一个网段,其字符串形式是CIDR地址,如:“192.168.100.1/24”或“2001:DB8::/ 48”&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    mask := net.IPv4Mask(byte(255), byte(255), byte(255), byte(0))
    ip := net.ParseIP(&amp;quot;192.168.1.125&amp;quot;).Mask(mask)
    in := &amp;amp;net.IPNet{ip, mask}
    fmt.Println(in)         //  192.168.1.0/24
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;实例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这边插播一个经常使用的实例：获取本地IP&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net&amp;quot;
    &amp;quot;os&amp;quot;
)
func main() {
    addrs, err := net.InterfaceAddrs()
    if err != nil {
        fmt.Println(err)
        os.Exit(1)
    }
    for _, address := range addrs {
        // 检查ip地址判断是否回环地址
        if ipnet, ok := address.(*net.IPNet); ok &amp;amp;&amp;amp; !ipnet.IP.IsLoopback() {
            if ipnet.IP.To4() != nil {
                fmt.Println(ipnet.IP.String())
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tcp&#34;&gt;tcp&lt;/h2&gt;

&lt;p&gt;使用TCPConn结构体来表示，它实现了Conn接口。&lt;/p&gt;

&lt;p&gt;使用DialTCP进行Dial操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialTCP(net string, laddr, raddr *TCPAddr) (*TCPConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DialTCP在网络协议net上连接本地地址laddr和远端地址raddr。net必须是&amp;rdquo;tcp&amp;rdquo;、&amp;rdquo;tcp4&amp;rdquo;、&amp;rdquo;tcp6&amp;rdquo;；如果laddr不是nil，将使用它作为本地地址，否则自动选择一个本地地址。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenTCP(net string, laddr *TCPAddr) (*TCPListener, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用 ListenTCP函数进行Listen，产生一个TCPListener结构体，使用TCPListener的AcceptTCP方法建立通信链路，得到TCPConn。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;TCPAddr类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;位于tcpsock.go中TCPAddr类型包含一个IP和一个port的结构:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type TCPAddr struct {
    IP   IP
    Port int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResolveTCPAddr&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ResolveTCPAddr(net, addr string) (*TCPAddr, os.Error) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该函数用来创建一个TCPAddr,第一个参数为,tcp,tcp4或者tcp6,addr是一个字符串，由主机名或IP地址，以及&amp;rdquo;:&amp;ldquo;后跟随着端口号组成，例如： &amp;ldquo;www.google.com:80&amp;rdquo; 或 &amp;lsquo;127.0.0.1:22&amp;rdquo;。如果地址是一个IPv6地址，由于已经有冒号，主机部分，必须放在方括号内, 例如：&amp;rdquo;[::1]:23&amp;rdquo;. 另一种特殊情况是经常用于服务器, 主机地址为0, 因此，TCP地址实际上就是端口名称, 例如：&amp;rdquo;:80&amp;rdquo; 用来表示HTTP服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;addr, _ := net.ResolveTCPAddr(&amp;quot;tcp&amp;quot;, &amp;quot;www.baidu.com:80&amp;quot;)
fmt.Println(addr)   //220.181.111.147:80
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;udp&#34;&gt;udp&lt;/h2&gt;

&lt;p&gt;使用UDPConn接口体来表示，它实现了Conn、PacketConn两种接口。使用如下两个函数进行Dial和Listen。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialUDP(net string, laddr, raddr *UDPAddr) (*UDPConn, error)    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;DialTCP在网络协议net上连接本地地址laddr和远端地址raddr。net必须是&amp;rdquo;udp&amp;rdquo;、&amp;rdquo;udp4&amp;rdquo;、&amp;rdquo;udp6&amp;rdquo;；如果laddr不是nil，将使用它作为本地地址，否则自动选择一个本地地址。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenUDP(net string, laddr *UDPAddr) (*UDPConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ListenUDP创建一个接收目的地是本地地址laddr的UDP数据包的网络连接。net必须是&amp;rdquo;udp&amp;rdquo;、&amp;rdquo;udp4&amp;rdquo;、&amp;rdquo;udp6&amp;rdquo;；如果laddr端口为0，函数将选择一个当前可用的端口，可以用Listener的Addr方法获得该端口。返回的*UDPConn的ReadFrom和WriteTo方法可以用来发送和接收UDP数据包（每个包都可获得来源地址或设置目标地址）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;类型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、UDPAddr类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type UDPAddr struct {
    IP   IP
    Port int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResolveUDPAddr同样的功能&lt;/p&gt;

&lt;p&gt;2、UnixAddr类型&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type UnixAddr struct {
    Name string
    Net  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ResolveUnixAddr同样的功能&lt;/p&gt;

&lt;h2 id=&#34;unix&#34;&gt;unix&lt;/h2&gt;

&lt;p&gt;UnixConn实现了Conn、PacketConn两种接口，其中unix又分为SOCK_DGRAM、SOCK_STREAM。&lt;/p&gt;

&lt;p&gt;1.对于unix（SOCK_DGRAM），使用如下两个函数进行Dial和Listen。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialUnix(net string, laddr, raddr *UnixAddr) (*UnixConn, error)    

func ListenUnixgram(net string, laddr *UnixAddr) (*UnixConn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.对于unix（SOCK_STREAM）&lt;/p&gt;

&lt;p&gt;客户端使用DialUnix进行Dial操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialUnix(net string, laddr, raddr *UnixAddr) (*UnixConn, error)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;服务端使用ListenUnix函数进行Listen操作，然后使用UnixListener进行AcceptUnix&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenUnix(net string, laddr *UnixAddr) (*UnixListener, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;函数整合&#34;&gt;函数整合&lt;/h1&gt;

&lt;p&gt;为了使用方便，golang将上面一些重复的操作集中到一个函数中。在参数中制定上面不同协议类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ListenPacket(net, laddr string) (PacketConn, error)　
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数用于侦听ip、udp、unix（DGRAM）等协议，返回一个PacketConn接口，同样根据侦听的协议不同，这个接口可以包含IPCon、UDPConn、UnixConn等，它们都实现了PacketConn。可以发现与ip、unix（stream）协议不同，直接返回的是xxConn，不是间接的通过Listener进行Accept操作后，才得到一个Conn。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Listen(net, laddr string) (Listener, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数用于侦听tcp、unix（stream）等协议，返回一个Listener接口、根据侦听的协议不同，这个接口可以包含TCPListener、UnixListener等，它们都实现了Listener接口，然后通过调用其Accept方法可以得到Conn接口，进行通信。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Dial(network, address string) (Conn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数对于所有的协议都是相同的操作，返回一个Conn接口，根据协议的不同实际上包含IPConn、UDPConn、UnixConn、IPConn，它们都实现了Conn接口&lt;/p&gt;

&lt;h1 id=&#34;基本c-s功能&#34;&gt;基本c/s功能&lt;/h1&gt;

&lt;p&gt;在 Unix/Linux 中的 Socket 编程主要通过调用 listen, accept, write read 等函数来实现的. 具体如下图所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/unix_socket.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;服务端&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;服务端listen, accept&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func connHandler(c net.Conn) {
    for {
        cnt, err := c.Read(buf)
        c.Write(buf)
    }
}
func main() {
    server, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:1208&amp;quot;)
    for {
        conn, err := server.Accept()
        go connHandler(conn)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直接使用net的listen返回的就是对应协议已经定义好的结构体，比如tcp&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type TCPListener struct {
    fd *netFD
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个结构体实现了listener接口的所有接口，所以可以作为返回值返回。其他协议类型也是一样。&lt;/p&gt;

&lt;p&gt;accept后返回的conn是一个存储着连接信息的结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Network file descriptor.
type netFD struct {
    pfd poll.FD

    // immutable until Close
    family      int
    sotype      int
    isConnected bool // handshake completed or use of association with peer
    net         string
    laddr       Addr
    raddr       Addr
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;客户端&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;客户端dial&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func connHandler(c net.Conn) {
    for {
        c.Write(...)
        c.Read(...)
    }
}
func main() {
    conn, err := net.Dial(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:1208&amp;quot;)
    connHandler(conn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Dial(net, addr string) (Conn, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中net参数是网络协议的名字， addr参数是IP地址或域名，而端口号以“:”的形式跟随在地址
或域名的后面，端口号可选。如果连接成功，返回连接对象，否则返回error。&lt;/p&gt;

&lt;p&gt;Dial() 函数支持如下几种网络协议：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;tcp&amp;quot; 、 &amp;quot;tcp4&amp;quot; （仅限IPv4）、 &amp;quot;tcp6&amp;quot; （仅限IPv6）、 &amp;quot;udp&amp;quot; 、 &amp;quot;udp4&amp;quot;（仅限IPv4）、 &amp;quot;udp6&amp;quot;（仅限IPv6）、 &amp;quot;ip&amp;quot; 、 &amp;quot;ip4&amp;quot;（仅限IPv4）和&amp;quot;ip6&amp;quot;（仅限IPv6）。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以直接用相关协议的函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func DialTCP(net string, laddr, raddr *TCPAddr) (c *TCPConn, err error)
func DialUDP(net string, laddr, raddr *UDPAddr) (c *UDPConn, err error)
func DialIP(netProto string, laddr, raddr *IPAddr) (*IPConn, error)
func DialUnix(net string, laddr, raddr *UnixAddr) (c *UnixConn, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;特性功能&#34;&gt;特性功能&lt;/h2&gt;

&lt;p&gt;1、控制TCP连接&lt;/p&gt;

&lt;p&gt;TCP连接有很多控制函数，我们平常用到比较多的有如下几个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *TCPConn) SetTimeout(nsec int64) os.Error
func (c *TCPConn) SetKeepAlive(keepalive bool) os.Error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一个函数用来设置超时时间，客户端和服务器端都适用，当超过设置的时间时那么该链接就失效。&lt;/p&gt;

&lt;p&gt;第二个函数用来设置客户端是否和服务器端一直保持着连接，即使没有任何的数据发送&lt;/p&gt;

&lt;h1 id=&#34;实例&#34;&gt;实例&lt;/h1&gt;

&lt;p&gt;从零开始写Socket Server： Socket-Client框架&lt;/p&gt;

&lt;p&gt;在golang中，网络协议已经被封装的非常完好了，想要写一个Socket的Server，我们并不用像其他语言那样需要为socket、bind、listen、receive等一系列操作头疼，只要使用Golang中自带的net包即可很方便的完成连接等操作~&lt;/p&gt;

&lt;p&gt;在这里，给出一个最最基础的基于Socket的Server的写法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
)


func main() {

//建立socket，监听端口
    netListen, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:1024&amp;quot;)
    CheckError(err)
    defer netListen.Close()

    Log(&amp;quot;Waiting for clients&amp;quot;)
    for {
        conn, err := netListen.Accept()
        if err != nil {
            continue
        }

        Log(conn.RemoteAddr().String(), &amp;quot; tcp connect success&amp;quot;)
        handleConnection(conn)
    }
}
//处理连接
func handleConnection(conn net.Conn) {

    buffer := make([]byte, 2048)

    for {

        n, err := conn.Read(buffer)

        if err != nil {
            Log(conn.RemoteAddr().String(), &amp;quot; connection error: &amp;quot;, err)
            return
        }


        Log(conn.RemoteAddr().String(), &amp;quot;receive data string:\n&amp;quot;, string(buffer[:n]))

    }

}
func Log(v ...interface{}) {
    log.Println(v...)
}

func CheckError(err error) {
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Fatal error: %s&amp;quot;, err.Error())
        os.Exit(1)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;唔，抛除Go语言里面10行代码有5行error的蛋疼之处,你可以看到，Server想要建立并接受一个Socket，其核心流程就是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netListen, err := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:1024&amp;quot;)
conn, err := netListen.Accept()
n, err := conn.Read(buffer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这三步，通过Listen、Accept 和Read，我们就成功的绑定了一个端口，并能够读取从该端口传来的内容~&lt;/p&gt;

&lt;p&gt;这边插播一个内容，关于read是阻塞的，如果读取不到内容，代码会阻塞在这边，直到有内容可以读取，包括connection断掉返回的io.EOF,一般对这个都有特殊处理。一般重conn读取数据也是在for循环中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;net&amp;quot;
)

func main(){
    ln, err := net.Listen(&amp;quot;tcp&amp;quot;,&amp;quot;127.0.0.1:10051&amp;quot;)

    if err != nil {
        panic(err)
    }

    for {
        conn, _ := ln.Accept() //The loop will be held here
        fmt.Println(&amp;quot;get connect&amp;quot;)
        go handleread(conn)


    }
}

func handleread(conn net.Conn){
    defer conn.Close()

    var tatalBuffer  []byte
    var all int
    for {
        buffer := make([]byte, 2)
        n,err := conn.Read(buffer)
        if err == io.EOF{
            fmt.Println(err,n)
            break
        }

        tatalBuffer = append(tatalBuffer,buffer...)
        all += n

        fmt.Println(string(buffer),n,string(tatalBuffer[:all]),all)
    }



}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这个例子中，会重conn中两个字符循环读取内容，这边slice不会动态扩容，所以需要使用append来获取全部内容。&lt;/p&gt;

&lt;p&gt;还有一点，buffer := make([]byte, 2)这个代码，放在for循环中，浪费内存，可以放在gor循环外部，然后使用n来截取buf[:n]可以解决buf最后一部分重复的问题。&lt;/p&gt;

&lt;p&gt;插播结束，回到server。&lt;/p&gt;

&lt;p&gt;Server写好之后，接下来就是Client方面啦，我手写一个HelloWorld给大家：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net&amp;quot;
    &amp;quot;os&amp;quot;
)

func sender(conn net.Conn) {
        words := &amp;quot;hello world!&amp;quot;
        conn.Write([]byte(words))
    fmt.Println(&amp;quot;send over&amp;quot;)

}



func main() {
    server := &amp;quot;127.0.0.1:1024&amp;quot;
    tcpAddr, err := net.ResolveTCPAddr(&amp;quot;tcp4&amp;quot;, server)
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Fatal error: %s&amp;quot;, err.Error())
        os.Exit(1)
    }

    conn, err := net.DialTCP(&amp;quot;tcp&amp;quot;, nil, tcpAddr)
    if err != nil {
        fmt.Fprintf(os.Stderr, &amp;quot;Fatal error: %s&amp;quot;, err.Error())
        os.Exit(1)
    }


    fmt.Println(&amp;quot;connect success&amp;quot;)
    sender(conn)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，Client这里的关键在于&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tcpAddr, err := net.ResolveTCPAddr(&amp;quot;tcp4&amp;quot;, server)
conn, err := net.DialTCP(&amp;quot;tcp&amp;quot;, nil, tcpAddr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这两步，主要是负责解析端口和连接。&lt;/p&gt;

&lt;p&gt;这边插播一个tcp协议的三次握手图，加强理解。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/golang/net/tcp_open_close.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;其实我们最常用的还是&lt;a href=&#34;https://kingjcy.github.io/post/golang/go-net-http/&#34;&gt;http协议&lt;/a&gt;，也即是应用层的协议，其实http协议是在tcp协议的基础上进行封装，最终还是使用的这边基本的网络IO，所以在网络传输中，网络IO的基本协议的实现是基础。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Os</title>
          <link>https://kingjcy.github.io/post/golang/go-os/</link>
          <pubDate>Thu, 02 Jun 2016 09:52:35 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-os/</guid>
          <description>&lt;p&gt;os包中实现了不依赖平台的操作系统函数接口(平台无关的接口)，设计向Unix风格，但是错误处理是go风格，当os包使用时，如果失败之后返回错误类型而不是错误数量,返回错误值而非错误码,可以包含更多信息。&lt;/p&gt;

&lt;h1 id=&#34;os&#34;&gt;os&lt;/h1&gt;

&lt;p&gt;os 依赖于 syscall。在实际编程中，我们应该总是优先使用 os 中提供的功能，而不是 syscall。&lt;/p&gt;

&lt;p&gt;os包提供了操作系统函数的不依赖平台的接口。一般都是linux下的一些基本命令的操作，比如文件，目录操作之类。&lt;/p&gt;

&lt;p&gt;我们运行程序常用的命令行参数就是在这个包中可以获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var Args []string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Args保管了命令行参数，第一个是程序名。&lt;/p&gt;

&lt;h2 id=&#34;文件io&#34;&gt;文件io&lt;/h2&gt;

&lt;p&gt;文件IO就是对文件的读写操作，我们先了解一些os中的基本概念。&lt;/p&gt;

&lt;h3 id=&#34;基本概念&#34;&gt;基本概念&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;文件描述符&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有 I/O 操作以文件描述符 ( 一个非负整数 , 通常是小整数 ) 来指代打开的文件。文件描述符用以表示所有类型的已打开文件，包括管道（pipe）、FIFO、socket、终端、设备和普通文件。&lt;/p&gt;

&lt;p&gt;在 Go 中，文件描述符封装在 os.File 结构中，通过 File.Fd() 可以获得底层的文件描述符：fd。&lt;/p&gt;

&lt;p&gt;File结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type File struct {
    *file
}
// file is the real representation of *File.
// The extra level of indirection ensures that no clients of os
// can overwrite this data, which could cause the finalizer
// to close the wrong file descriptor.
type file struct {
    fd      int
    name    string
    dirinfo *dirInfo // nil unless directory being read
}

// Auxiliary information if the File describes a directory
type dirInfo struct {
    buf  []byte // buffer for directory I/O
    nbuf int    // length of buf; return value from Getdirentries
    bufp int    // location of next record in buf.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;标准定义&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;按照惯例，大多数程序都期望能够使用 3 种标准的文件描述符：0- 标准输入；1- 标准输出；2- 标准错误。os 包提供了 3 个 File 对象，分别代表这 3 种标准描述符：Stdin、Stdout 和 Stderr，它们对应的文件名分别是：/dev/stdin、/dev/stdout 和 /dev/stderr。&lt;/p&gt;

&lt;h3 id=&#34;基本操作&#34;&gt;基本操作&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func NewFile(fd uintptr, name string) *File
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewFile使用给出的Unix文件描述符和名称创建一个文件。&lt;/p&gt;

&lt;p&gt;正常使用create来创建一个文件，比如文件不存在，就创建一个&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file,er:=os.Open(&amp;quot;xxx&amp;quot;)
defer func(){file.Close()}()
if er!=nil &amp;amp;&amp;amp; os.IfNotExist(er
r){
  file = os.Create(&amp;quot;xx&amp;quot;)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;打开&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Open(name string) (file *File, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open打开一个文件用于读取。如果操作成功，返回的文件对象的方法可用于读取数据；对应的文件描述符具有O_RDONLY模式。如果出错，错误底层类型是*PathError。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func OpenFile(name string, flag int, perm FileMode) (file *File, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OpenFile是一个更一般性的文件打开函数，大多数调用者都应用Open或Create代替本函数。它会使用指定的选项（如O_RDONLY等）、指定的模式（如0666等）打开指定名称的文件。如果操作成功，返回的文件对象可用于I/O。如果出错，错误底层类型是*PathError。&lt;/p&gt;

&lt;p&gt;位掩码参数 flag 用于指定文件的访问模式，可用的值在 os 中定义为常量（以下值并非所有操作系统都可用）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    O_RDONLY int = syscall.O_RDONLY // 只读模式打开文件
    O_WRONLY int = syscall.O_WRONLY // 只写模式打开文件
    O_RDWR   int = syscall.O_RDWR   // 读写模式打开文件
    O_APPEND int = syscall.O_APPEND // 写操作时将数据附加到文件尾部
    O_CREATE int = syscall.O_CREAT  // 如果不存在将创建一个新文件
    O_EXCL   int = syscall.O_EXCL   // 和 O_CREATE 配合使用，文件必须不存在
    O_SYNC   int = syscall.O_SYNC   // 打开文件用于同步 I/O
    O_TRUNC  int = syscall.O_TRUNC  // 如果可能，打开时清空文件
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O_TRUNC这个参数可以用来清空文件，如果可以的话，还可以用这个函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;os.Truncate(name, size)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Truncate(size int64) error
size 填0 就把文件清空了。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面有详细的说明&lt;/p&gt;

&lt;p&gt;位掩码参数 perm 指定了文件的模式和权限位，类型是 os.FileMode，文件模式位常量定义在 os 中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    // 单字符是被 String 方法用于格式化的属性缩写。
    ModeDir        FileMode = 1 &amp;lt;&amp;lt; (32 - 1 - iota) // d: 目录
    ModeAppend                                     // a: 只能写入，且只能写入到末尾
    ModeExclusive                                  // l: 用于执行
    ModeTemporary                                  // T: 临时文件（非备份文件）
    ModeSymlink                                    // L: 符号链接（不是快捷方式文件）
    ModeDevice                                     // D: 设备
    ModeNamedPipe                                  // p: 命名管道（FIFO）
    ModeSocket                                     // S: Unix 域 socket
    ModeSetuid                                     // u: 表示文件具有其创建者用户 id 权限
    ModeSetgid                                     // g: 表示文件具有其创建者组 id 的权限
    ModeCharDevice                                 // c: 字符设备，需已设置 ModeDevice
    ModeSticky                                     // t: 只有 root/ 创建者能删除 / 移动文件

    // 覆盖所有类型位（用于通过 &amp;amp; 获取类型位），对普通文件，所有这些位都不应被设置
    ModeType = ModeDir | ModeSymlink | ModeNamedPipe | ModeSocket | ModeDevice
    ModePerm FileMode = 0777 // 覆盖所有 Unix 权限位（用于通过 &amp;amp; 获取类型位）
)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;read&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Read(b []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Read 方法从 f 中读取最多 len(b) 字节数据并写入 b。它返回读取的字节数和可能遇到的任何错误。文件终止标志是读取 0 个字节且返回值 err 为 io.EOF。&lt;/p&gt;

&lt;p&gt;从方法声明可以知道，File 实现了 io.Reader 接口。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) ReadAt(b []byte, off int64) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ReadAt 从指定的位置（相对于文件开始位置）读取长度为 len(b) 个字节数据并写入 b。它返回读取的字节数和可能遇到的任何错误。当 n&amp;lt;len(b) 时，本方法总是会返回错误；如果是因为到达文件结尾，返回值 err 会是 io.EOF。它对应的系统调用是 pread。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var chunks []byte
buf := make([]byte, 1024)
var count = 0
for {
    n, err := f.Read(buf)
    if err != nil &amp;amp;&amp;amp; err != io.EOF {
        panic(err)
    }
    if 0 == n {
        break
    }
    count = count + n
    chunks = append(chunks, buf[:n]...)
}
r.logger.Debugf(&amp;quot;read file content : %s&amp;quot;,string(chunks[:count]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边这个实例主要是要说明一下几个重点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、buf必须make，不然会panic
2、read必须for循环，直到io.EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;write&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Write(b []byte) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write 向文件中写入 len(b) 字节数据。它返回写入的字节数和可能遇到的任何错误。如果返回值 n!=len(b)，本方法会返回一个非 nil 的错误。&lt;/p&gt;

&lt;p&gt;从方法声明可以知道，File 实现了 io.Writer 接口。&lt;/p&gt;

&lt;p&gt;Write 与 WriteAt 的区别同 Read 与 ReadAt 的区别一样。为了方便，还提供了 WriteString 方法，它实际是对 Write 的封装。&lt;/p&gt;

&lt;p&gt;注意：Write 调用成功并不能保证数据已经写入磁盘，因为内核会缓存磁盘的 I/O 操作。如果希望立刻将数据写入磁盘（一般场景不建议这么做，因为会影响性能），有两种办法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 打开文件时指定 `os.O_SYNC`；
2. 调用 `File.Sync()` 方法。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：File.Sync() 底层调用的是 fsync 系统调用，这会将数据和元数据都刷到磁盘；如果只想刷数据到磁盘（比如，文件大小没变，只是变了文件数据），需要自己封装，调用 fdatasync（syscall.Fdatasync） 系统调用。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;close&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;close() 系统调用关闭一个打开的文件描述符，并将其释放回调用进程，供该进程继续使用。当进程终止时，将自动关闭其已打开的所有文件描述符。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Close() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;os.File.Close() 是对 close() 的封装。我们应该养成关闭不需要的文件的良好编程习惯。文件描述符是资源，Go 的 gc 是针对内存的，并不会自动回收资源，如果不关闭文件描述符，长期运行的服务可能会把文件描述符耗尽。&lt;/p&gt;

&lt;p&gt;以下两种情况会导致 Close 返回错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 关闭一个未打开的文件；
2. 两次关闭同一个文件；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通常，我们不会去检查 Close 的错误&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;seek&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Seek(offset int64, whence int) (ret int64, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Seek 设置下一次读 / 写的位置。offset 为相对偏移量，而 whence 决定相对位置：0 为相对文件开头，1 为相对当前位置，2 为相对文件结尾。它返回新的偏移量（相对开头）和可能的错误。使用中，whence 应该使用 os 包中的常量：SEEK_SET、SEEK_CUR 和 SEEK_END&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file.Seek(0, os.SEEK_SET)    // 文件开始处
file.Seek(0, SEEK_END)        // 文件结尾处的下一个字节
file.Seek(-1, SEEK_END)        // 文件最后一个字节
file.Seek(-10, SEEK_CUR)     // 当前位置前 10 个字节
file.Seek(1000, SEEK_END)    // 文件结尾处的下 1001 个字节
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;trucate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;trucate 和 ftruncate 系统调用将文件大小设置为 size 参数指定的值；Go 语言中相应的包装函数是 os.Truncate 和 os.File.Truncate。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Truncate(name string, size int64) error
func (f *File) Truncate(size int64) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果文件当前长度大于参数 size，调用将丢弃超出部分，若小于参数 size，调用将在文件尾部添加一系列空字节或是一个文件空洞。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;remove&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Remove(name string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove删除name指定的文件或目录。如果出错，会返回*PathError底层类型的错误。&lt;/p&gt;

&lt;h2 id=&#34;文件属性&#34;&gt;文件属性&lt;/h2&gt;

&lt;h3 id=&#34;文件信息&#34;&gt;文件信息&lt;/h3&gt;

&lt;p&gt;可以通过包里的函数 Stat、Lstat 和 File.Stat 可以得到os.FileInfo 接口的信息。这三个函数对应三个系统调用：stat、lstat 和 fstat。&lt;/p&gt;

&lt;p&gt;这三个函数的区别：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;stat 会返回所命名文件的相关信息。&lt;/li&gt;
&lt;li&gt;lstat 与 stat 类似，区别在于如果文件是符号链接，那么所返回的信息针对的是符号链接自身（而非符号链接所指向的文件）。&lt;/li&gt;
&lt;li&gt;fstat 则会返回由某个打开文件描述符（Go 中则是当前打开文件 File）所指代文件的相关信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stat 和 Lstat 无需对其所操作的文件本身拥有任何权限，但针对指定 name 的父目录要有执行（搜索）权限。而只要 File 对象 ok，File.Stat 总是成功。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Stat() (fi FileInfo, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stat返回描述文件f的FileInfo类型值。如果出错，错误底层类型是*PathError。这个方法也可以用于检查文件是否有问题，上面说到文件的信息是存储在FileInfo 接口中的，我们来看一下这个接口&lt;/p&gt;

&lt;p&gt;FileInfo是一个接口，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// A FileInfo describes a file and is returned by Stat and Lstat.
type FileInfo interface {
    Name() string       // base name of the file 文件的名字（不含扩展名）
    Size() int64        // length in bytes for regular files; system-dependent for others  普通文件返回值表示其大小；其他文件的返回值含义各系统不同
    Mode() FileMode     // file mode bits   文件的模式位
    ModTime() time.Time // modification time    文件的修改时间
    IsDir() bool        // abbreviation for Mode().IsDir()  等价于 Mode().IsDir()
    Sys() interface{}   // underlying data source (can return nil)  底层数据来源（可以返回 nil）
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该接口提供了一个sys函数，Sys() 底层数据的 C 语言 结构 statbuf 格式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct stat {
  dev_t    st_dev;    // 设备 ID
  ino_t    st_ino;    // 文件 i 节点号
  mode_t    st_mode;    // 位掩码，文件类型和文件权限
  nlink_t    st_nlink;    // 硬链接数
  uid_t    st_uid;    // 文件属主，用户 ID
  gid_t    st_gid;    // 文件属组，组 ID
  dev_t    st_rdev;    // 如果针对设备 i 节点，则此字段包含主、辅 ID
  off_t    st_size;    // 常规文件，则是文件字节数；符号链接，则是链接所指路径名的长度，字节为单位；对于共享内存对象，则是对象大小
  blksize_t    st_blsize;    // 分配给文件的总块数，块大小为 512 字节
  blkcnt_t    st_blocks;    // 实际分配给文件的磁盘块数量
  time_t    st_atime;        // 对文件上次访问时间
  time_t    st_mtime;        // 对文件上次修改时间
  time_t    st_ctime;        // 文件状态发生改变的上次时间
}
Go 中 syscal.Stat_t 与该结构对应。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果我们要获取 FileInfo 接口没法直接返回的信息，比如想获取文件的上次访问时间，示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fileInfo, err := os.Stat(&amp;quot;test.log&amp;quot;)
if err != nil {
  log.Fatal(err)
}
sys := fileInfo.Sys()
stat := sys.(*syscall.Stat_t)
fmt.Println(time.Unix(stat.Atimespec.Unix()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常返回的是实现这个接口的结构体，也就是fileStat，如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// A fileStat is the implementation of FileInfo returned by Stat and Lstat.
type fileStat struct {
    name    string
    size    int64
    mode    FileMode
    modTime time.Time
    sys     syscall.Stat_t
}

func (fs *fileStat) Size() int64        { return fs.size }
func (fs *fileStat) Mode() FileMode     { return fs.mode }
func (fs *fileStat) ModTime() time.Time { return fs.modTime }
func (fs *fileStat) Sys() interface{}   { return &amp;amp;fs.sys }

func sameFile(fs1, fs2 *fileStat) bool {
    return fs1.sys.Dev == fs2.sys.Dev &amp;amp;&amp;amp; fs1.sys.Ino == fs2.sys.Ino
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中有一个syscall.Stat_t，源于syscall的结构体，这个结构体是需要区分系统的，不同的系统调用不一样，不然编译不通过，报错如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;registry/delete.go:49:27: stat.Ctimespec undefined (type *syscall.Stat_t has no field or method Ctimespec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;是因为在linux下结构体成名名是Ctim，在drawin下是Ctimespec，导致跨平台编译报错。&lt;/p&gt;

&lt;h3 id=&#34;文件时间&#34;&gt;文件时间&lt;/h3&gt;

&lt;p&gt;通过包里的Chtimes函数可以显式改变文件的访问时间和修改时间。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Chtimes(name string, atime time.Time, mtime time.Time) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chtimes 修改 name 指定的文件对象的访问时间和修改时间，类似 Unix 的 utime() 或 utimes() 函数。底层的文件系统可能会截断 / 舍入时间单位到更低的精确度。如果出错，会返回 *PathError 类型的错误。在 Unix 中，底层实现会调用 utimenstat()，它提供纳秒级别的精度&lt;/p&gt;

&lt;h3 id=&#34;文件权限&#34;&gt;文件权限&lt;/h3&gt;

&lt;p&gt;系统调用 chown、lchown 和 fchown 可用来改变文件的属主和属组，Go 中os包中对应的函数或方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Chown(name string, uid, gid int) error
func Lchown(name string, uid, gid int) error
func (f *File) Chown(uid, gid int) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它们的区别和上文提到的 Stat 相关函数类似。&lt;/p&gt;

&lt;p&gt;在文件相关操作报错时，可以通过 os.IsPermission 检查是否是权限的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func IsPermission(err error) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回一个布尔值说明该错误是否表示因权限不足要求被拒绝。ErrPermission 和一些系统调用错误会使它返回真。&lt;/p&gt;

&lt;p&gt;另外，syscall.Access 可以获取文件的权限。这对应系统调用 access。&lt;/p&gt;

&lt;p&gt;os.Chmod 和 os.File.Chmod 可以修改文件权限（包括 sticky 位），分别对应系统调用 chmod 和 fchmod。&lt;/p&gt;

&lt;h2 id=&#34;目录&#34;&gt;目录&lt;/h2&gt;

&lt;p&gt;在 Unix 文件系统中，目录的存储方式类似于普通文件。目录和普通文件的区别有二：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在其 i-node 条目中，会将目录标记为一种不同的文件类型。&lt;/li&gt;
&lt;li&gt;目录是经特殊组织而成的文件。本质上说就是一个表格，包含文件名和 i-node 标号&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;目录操作&#34;&gt;目录操作&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Mkdir(name string, perm FileMode) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mkdir 使用指定的权限和名称创建一个目录。如果出错，会返回 *PathError 类型的错误。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;name 参数指定了新目录的路径名，可以是相对路径，也可以是绝对路径。如果已经存在，则调用失败并返回 os.ErrExist 错误。&lt;/li&gt;
&lt;li&gt;perm 参数指定了新目录的权限。对该位掩码值的指定方式和 os.OpenFile 相同，也可以直接赋予八进制数值。注意，perm 值还将于进程掩码相与（&amp;amp;）。如果 perm 中设置了 sticky 位，那么将对新目录设置该权限。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为 Mkdir 所创建的只是路径名中的最后一部分，如果父目录不存在，创建会失败。os.MkdirAll 用于递归创建所有不存在的目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func MkdirAll(path string, perm FileMode) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MkdirAll使用指定的权限和名称创建一个目录，包括任何必要的上级目录，并返回nil，否则返回错误。权限位perm会应用在每一个被本函数创建的目录上。如果path指定了一个已经存在的目录，MkdirAll不做任何操作并返回nil。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;删除&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Remove(name string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove 删除 name 指定的文件或目录。如果出错，会返回 *PathError 类型的错误。如果目录不为空，Remove 会返回失败。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func RemoveAll(path string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RemoveAll 删除 path 指定的文件，或目录及它包含的任何下级对象。它会尝试删除所有东西，除非遇到错误并返回。如果 path 指定的对象不存在，RemoveAll 会返回 nil 而不返回错误。&lt;/p&gt;

&lt;p&gt;RemoveAll 的内部实现逻辑如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;调用 Remove 尝试进行删除，如果成功或返回 path 不存在，则直接返回 nil；&lt;/li&gt;
&lt;li&gt;调用 Lstat 获取 path 信息，以便判断是否是目录。注意，这里使用 Lstat，表示不对符号链接解引用；&lt;/li&gt;
&lt;li&gt;调用 Open 打开目录，递归读取目录中内容，执行删除操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;读&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Readdirnames(n int) (names []string, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readdirnames 读取目录 f 的内容，返回一个最多有 n 个成员的[]string，切片成员为目录中文件对象的名字，采用目录顺序。对本函数的下一次调用会返回上一次调用未读取的内容的信息。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果 n&amp;gt;0，Readdirnames 函数会返回一个最多 n 个成员的切片。这时，如果 Readdirnames 返回一个空切片，它会返回一个非 nil 的错误说明原因。如果到达了目录 f 的结尾，返回值 err 会是 io.EOF。&lt;/li&gt;
&lt;li&gt;如果 n&amp;lt;=0，Readdirnames 函数返回目录中剩余所有文件对象的名字构成的切片。此时，如果 Readdirnames 调用成功（读取所有内容直到结尾），它会返回该切片和 nil 的错误值。如果在到达结尾前遇到错误，会返回之前成功读取的名字构成的切片和该错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Readdir 内部会调用 Readdirnames，将得到的 names 构造路径，通过 Lstat 构造出 []FileInfo。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (f *File) Readdir(n int) (fi []FileInfo, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;连接&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Link(oldname, newname string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Link 创建一个名为 newname 指向 oldname 的硬链接。如果出错，会返回 *LinkError 类型的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Symlink(oldname, newname string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Symlink 创建一个名为 newname 指向 oldname 的符号链接。如果出错，会返回 *LinkError 类型的错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Readlink(name string) (string, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Readlink 获取 name 指定的符号链接指向的文件的路径。如果出错，会返回 *PathError 类型的错误。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;更改文件名&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func Rename(oldpath, newpath string) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Rename 修改一个文件的名字或移动一个文件。如果 newpath 已经存在，则替换它。注意，可能会有一些个操作系统特定的限制。&lt;/p&gt;

&lt;h1 id=&#34;os-singal&#34;&gt;os/singal&lt;/h1&gt;

&lt;h2 id=&#34;类型&#34;&gt;类型&lt;/h2&gt;

&lt;p&gt;Signal是一个接口，所有的信号都实现了这个接口，可以直接传递，我们传递信号的时候，需要定义这个类型的channel来传递信号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Signal interface {
    String() string
    Signal() // to distinguish from other Stringers
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;syscall 包中定义了所有的信号常量，比如syscall.SIGINT，其实就是一个int的数字信号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SIGINT    = Signal(0x2)
type Signal int

func (s Signal) Signal() {}

func (s Signal) String() string {
    if 0 &amp;lt;= s &amp;amp;&amp;amp; int(s) &amp;lt; len(signals) {
        str := signals[s]
        if str != &amp;quot;&amp;quot; {
            return str
        }
    }
    return &amp;quot;signal &amp;quot; + itoa(int(s))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;函数&#34;&gt;函数&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Notify&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;singnal主要是用于信号的传递，一般程序中需要使用信号的时候使用。主要使用下面两个方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Notify(c chan&amp;lt;- os.Signal, sig ...os.Signal)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notify函数让signal包将输入信号转发到c。如果没有列出要传递的信号，会将所有输入信号传递到c；否则只传递列出的输入信号。&lt;/p&gt;

&lt;p&gt;signal包不会为了向c发送信息而阻塞（就是说如果发送时c阻塞了，signal包会直接放弃）：调用者应该保证c有足够的缓存空间可以跟上期望的信号频率。对使用单一信号用于通知的通道，缓存为1就足够了。&lt;/p&gt;

&lt;p&gt;可以使用同一通道多次调用Notify：每一次都会扩展该通道接收的信号集。可以使用同一信号和不同通道多次调用Notify：每一个通道都会独立接收到该信号的一个拷贝。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import &amp;quot;fmt&amp;quot;
import &amp;quot;os&amp;quot;
import &amp;quot;os/signal&amp;quot;
import &amp;quot;syscall&amp;quot;
func main() {
    sigs := make(chan os.Signal, 1)
    done := make(chan bool, 1)
    signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)

    go func() {
        sig := &amp;lt;-sigs
        fmt.Println()
        fmt.Println(sig)
        done &amp;lt;- true
    }()

    fmt.Println(&amp;quot;awaiting signal&amp;quot;)
    &amp;lt;-done
    fmt.Println(&amp;quot;exiting&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;stop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;唯一从信号集去除信号的方法是调用Stop。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Stop(c chan&amp;lt;- os.Signal)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stop函数让signal包停止向c转发信号。它会取消之前使用c调用的所有Notify的效果。当Stop返回后，会保证c不再接收到任何信号。&lt;/p&gt;

&lt;h1 id=&#34;os-exec&#34;&gt;os/exec&lt;/h1&gt;

&lt;h2 id=&#34;进程io&#34;&gt;进程io&lt;/h2&gt;

&lt;p&gt;exec包用于执行外部命令。它包装了os.StartProcess函数以便更容易的修正输入和输出，使用管道连接I/O。主要用于创建一个子进程来执行相关的命令。创建子进程一定要wait，不能出现僵死进程。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;调用脚本命令&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在golang标准库中提供了两种方式可以用来启动进程调用脚本&lt;/p&gt;

&lt;p&gt;第一种是在os库中的Process类型，Process类型包含一系列方法用来启动进程并对进程进行操作（参考： &lt;a href=&#34;https://golang.org/pkg/os/#Process）&#34;&gt;https://golang.org/pkg/os/#Process）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;示例 使用Process执行脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os&amp;quot;
)

func main() {
    shellPath := &amp;quot;/home/xx/test.sh&amp;quot;
    argv := make([]string, 1) 
    attr := new(os.ProcAttr)
    newProcess, err := os.StartProcess(shellPath, argv, attr)  //运行脚本
    if err != nil {
        fmt.Println(err)
    }
    fmt.Println(&amp;quot;Process PID&amp;quot;, newProcess.Pid)
    processState, err := newProcess.Wait() //等待命令执行完
    if err != nil {
        fmt.Println(err)
    }
    fmt.Println(&amp;quot;processState PID:&amp;quot;, processState.Pid())//获取PID
    fmt.Println(&amp;quot;ProcessExit:&amp;quot;, processState.Exited())//获取进程是否退出
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二种是在os/exec库种通过Cmd类型的各个函数实现对脚本的调用，实际上Cmd是对Process中各种方法的高层次封装（参考： &lt;a href=&#34;https://golang.org/pkg/os/exec/）&#34;&gt;https://golang.org/pkg/os/exec/）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1、LookPath&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LookPath(file string) (string, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在环境变量PATH指定的目录中搜索可执行文件，如file中有斜杠，则只在当前目录搜索。返回完整路径或者相对于当前目录的一个相对路径。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    output, err := exec.LookPath(&amp;quot;ls&amp;quot;)
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf(output)
}

output:

[ `go run test.go` | done: 616.254982ms ]
  /bin/ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、Cmd&lt;/p&gt;

&lt;p&gt;Cmd代表一个正在准备或者在执行中的外部命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Cmd struct {
    // Path是将要执行的命令的路径。
    //
    // 该字段不能为空，如为相对路径会相对于Dir字段。
    Path string
    // Args保管命令的参数，包括命令名作为第一个参数；如果为空切片或者nil，相当于无参数命令。
    //
    // 典型用法下，Path和Args都应被Command函数设定。
    Args []string
    // Env指定进程的环境，如为nil，则是在当前进程的环境下执行。
    Env []string
    // Dir指定命令的工作目录。如为空字符串，会在调用者的进程当前目录下执行。
    Dir string
    // Stdin指定进程的标准输入，如为nil，进程会从空设备读取（os.DevNull）
    Stdin io.Reader
    // Stdout和Stderr指定进程的标准输出和标准错误输出。
    //
    // 如果任一个为nil，Run方法会将对应的文件描述符关联到空设备（os.DevNull）
    //
    // 如果两个字段相同，同一时间最多有一个线程可以写入。
    Stdout io.Writer
    Stderr io.Writer
    // ExtraFiles指定额外被新进程继承的已打开文件流，不包括标准输入、标准输出、标准错误输出。
    // 如果本字段非nil，entry i会变成文件描述符3+i。
    //
    // BUG: 在OS X 10.6系统中，子进程可能会继承不期望的文件描述符。
    // http://golang.org/issue/2603
    ExtraFiles []*os.File
    // SysProcAttr保管可选的、各操作系统特定的sys执行属性。
    // Run方法会将它作为os.ProcAttr的Sys字段传递给os.StartProcess函数。
    SysProcAttr *syscall.SysProcAttr
    // Process是底层的，只执行一次的进程。
    Process *os.Process
    // ProcessState包含一个已经存在的进程的信息，只有在调用Wait或Run后才可用。
    ProcessState *os.ProcessState
    // 内含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用Command来创建cmd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Command(name string, arg ...string) *Cmd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数返回一个*Cmd，用于使用给出的参数执行name指定的程序。返回值只设定了Path和Args两个参数。&lt;/p&gt;

&lt;p&gt;如果name不含路径分隔符，将使用LookPath获取完整路径；否则直接使用name。参数arg不应包含命令名&lt;/p&gt;

&lt;p&gt;使用Run运行cmd命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Run() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run执行c包含的命令，并阻塞直到完成。如果命令成功执行，stdin、stdout、stderr的转交没有问题，并且返回状态码为0，方法的返回值为nil；如果命令没有执行或者执行失败，会返回错误；&lt;/p&gt;

&lt;p&gt;使用Start和wait来运行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Start() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start开始执行c包含的命令，但并不会等待该命令完成即返回。可以配合使用Wait方法来达到和Run一样的效果。wait方法会返回命令的返回状态码并在命令返回后释放相关的资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Wait() error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait会阻塞直到该命令执行完成，该命令必须是被Start方法开始执行的。&lt;/p&gt;

&lt;p&gt;通过Run的源码可以看出其实Run方法内部也是调用了Start和Wait方法。Run方法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Run() error {
    if err := c.Start(); err != nil {
        return err
    }
    return c.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    cmd := exec.Command(&amp;quot;tr&amp;quot;, &amp;quot;a-z&amp;quot;, &amp;quot;A-Z&amp;quot;)
    cmd.Stdin = strings.NewReader(&amp;quot;abc def&amp;quot;)
    var out bytes.Buffer
    cmd.Stdout = &amp;amp;out
    err := cmd.Run()
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;GOGOGO: %q\n&amp;quot;, out.String())
}

output:

[ `go run test.go` | done: 286.798242ms ]
  GOGOGO: &amp;quot;ABC DEF&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用Output输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) Output() ([]byte, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行命令并返回标准输出的切片。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Cmd) CombinedOutput() ([]byte, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行命令并返回标准输出和错误输出合并的切片.&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    out, err := exec.Command(&amp;quot;date&amp;quot;).Output()
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;The date is %s\n&amp;quot;, out)
}

output:

[ `go run test.go` | done: 585.495467ms ]
  The date is Tue Aug  1 19:24:11 CST 2017
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用pipe&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Cmd) StdinPipe
func (c *Cmd) StdinPipe() (io.WriteCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StdinPipe方法返回一个在命令Start后与命令标准输入关联的管道。Wait方法获知命令结束后会关闭这个管道。必要时调用者可以调用Close方法来强行关闭管道，例如命令在输入关闭后才会执行返回时需要显式关闭管道。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Cmd) StdoutPipe
func (c *Cmd) StdoutPipe() (io.ReadCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StdoutPipe方法返回一个在命令Start后与命令标准输出关联的管道。Wait方法获知命令结束后会关闭这个管道，一般不需要显式的关闭该管道。但是在从管道读取完全部数据之前调用Wait是错误的；同样使用StdoutPipe方法时调用Run函数也是错误的。例子如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Cmd) StderrPipe
func (c *Cmd) StderrPipe() (io.ReadCloser, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StderrPipe方法返回一个在命令Start后与命令标准错误输出关联的管道。Wait方法获知命令结束后会关闭这个管道，一般不需要显式的关闭该管道。但是在从管道读取完全部数据之前调用Wait是错误的；同样使用StderrPipe方法时调用Run函数也是错误的。请参照StdoutPipe的例子。&lt;/p&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    cmd := exec.Command(&amp;quot;echo&amp;quot;, &amp;quot;-n&amp;quot;, `{&amp;quot;Name&amp;quot;: &amp;quot;Bob&amp;quot;, &amp;quot;Age&amp;quot;: 32}`)
    stdout, err := cmd.StdoutPipe()
    if err != nil {
        log.Fatal(err)
    }
    if err := cmd.Start(); err != nil {
        log.Fatal(err)
    }
    var person struct {
        Name string
        Age  int
    }
    json.NewDecoder(r)
    if err := json.NewDecoder(stdout).Decode(&amp;amp;person); err != nil {
        log.Fatal(err)
    }
    if err := cmd.Wait(); err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;%s is %d years old\n&amp;quot;, person.Name, person.Age)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取命令返回值&lt;/p&gt;

&lt;p&gt;实际上脚本或命令执行完后，会将结果返回到ProcessState中的status去， 但是status不是export的，所以我们需要通过一些手段将脚本返回值从syscall.WaitStatus找出来&lt;/p&gt;

&lt;p&gt;ProcessState定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ProcessState struct {
    pid    int                // The process&#39;s id.
    status syscall.WaitStatus // System-dependent status info.
    rusage *syscall.Rusage
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于上面使用Cmd的例子，可以在进程退出后可以通过以下语句获取到返回值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(&amp;quot;Exit Code&amp;quot;, command.ProcessState.Sys().(syscall.WaitStatus).ExitStatus())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用Process方式的也可以通过对ProcessState通过相同的方式获取到返回结果。&lt;/p&gt;

&lt;h1 id=&#34;os-user&#34;&gt;os/user&lt;/h1&gt;

&lt;p&gt;os/user 模块的主要作用是通过用户名或者 id 从而获取系统用户的相关属性。&lt;/p&gt;

&lt;h2 id=&#34;类型-1&#34;&gt;类型&lt;/h2&gt;

&lt;p&gt;User 结构体&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type User struct {
    Uid      string
    Gid      string
    Username string
    Name     string
    HomeDir  string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;User 代表一个用户账户：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Uid ：用户的 ID&lt;/li&gt;
&lt;li&gt;Gid ：用户所属组的 ID，如果属于多个组，那么此 ID 为主组的 ID&lt;/li&gt;
&lt;li&gt;Username ：用户名&lt;/li&gt;
&lt;li&gt;Name ：属组名称，如果属于多个组，那么此名称为主组的名称&lt;/li&gt;
&lt;li&gt;HomeDir ：用户的宿主目录&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;方法&#34;&gt;方法&lt;/h2&gt;

&lt;p&gt;返回当前用户。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Current() (*User, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过用户名查找用户，如果没有找到这个用户那么将返回 UnknownUserError 错误类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Lookup(username string) (*User, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过用户 ID 查找用户，如果没有找到这个用户那么将返回 UnknownUserIdError 错误类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func LookupId(uid string) (*User, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;os/user&amp;quot;
    &amp;quot;reflect&amp;quot;
)

func main() {
    fmt.Println(&amp;quot;== 测试 Current 正常情况 ==&amp;quot;)
    if u, err := user.Current(); err == nil {
        fmt.Println(&amp;quot;用户ID: &amp;quot; + u.Uid)
        fmt.Println(&amp;quot;主组ID: &amp;quot; + u.Gid)
        fmt.Println(&amp;quot;用户名: &amp;quot; + u.Username)
        fmt.Println(&amp;quot;主组名: &amp;quot; + u.Name)
        fmt.Println(&amp;quot;家目录: &amp;quot; + u.HomeDir)
    }

    fmt.Println(&amp;quot;== 测试 Lookup 正常情况 ==&amp;quot;)
    if u, err := user.Lookup(&amp;quot;root&amp;quot;); err == nil {
        fmt.Println(&amp;quot;用户ID: &amp;quot; + u.Uid)
        fmt.Println(&amp;quot;主组ID: &amp;quot; + u.Gid)
        fmt.Println(&amp;quot;用户名: &amp;quot; + u.Username)
        fmt.Println(&amp;quot;主组名: &amp;quot; + u.Name)
        fmt.Println(&amp;quot;家目录: &amp;quot; + u.HomeDir)
    }
    fmt.Println(&amp;quot;== 测试 Lookup 异常情况 ==&amp;quot;)
    if _, err := user.Lookup(&amp;quot;roo&amp;quot;); err == nil {
    } else {
        fmt.Println(&amp;quot;错误信息: &amp;quot; + err.Error())
        fmt.Print(&amp;quot;错误类型: &amp;quot;)
        fmt.Println(reflect.TypeOf(err))
    }

    fmt.Println(&amp;quot;== 测试 LookupId 正常情况 ==&amp;quot;)
    if u, err := user.LookupId(&amp;quot;0&amp;quot;); err == nil {
        fmt.Println(&amp;quot;用户ID: &amp;quot; + u.Uid)
        fmt.Println(&amp;quot;主组ID: &amp;quot; + u.Gid)
        fmt.Println(&amp;quot;用户名: &amp;quot; + u.Username)
        fmt.Println(&amp;quot;主组名: &amp;quot; + u.Name)
        fmt.Println(&amp;quot;家目录: &amp;quot; + u.HomeDir)
    }
    fmt.Println(&amp;quot;== 测试 LookupId 异常情况 ==&amp;quot;)
    if _, err := user.LookupId(&amp;quot;10000&amp;quot;); err == nil {
    } else {
        fmt.Println(&amp;quot;错误信息: &amp;quot; + err.Error())
        fmt.Print(&amp;quot;错误类型: &amp;quot;)
        fmt.Println(reflect.TypeOf(err))
    }

}
输出结果如下：

== 测试 Current 正常情况 ==
用户ID: 0
主组ID: 0
用户名: root
主组名: root
家目录: /root
== 测试 Lookup 正常情况 ==
用户ID: 0
主组ID: 0
用户名: root
主组名: root
家目录: /root
== 测试 Lookup 异常情况 ==
错误信息: user: unknown user roo
错误类型: user.UnknownUserError
== 测试 LookupId 正常情况 ==
用户ID: 0
主组ID: 0
用户名: root
主组名: root
家目录: /root
== 测试 LookupId 异常情况 ==
错误信息: user: unknown userid 10000
错误类型: user.UnknownUserIdError
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>Golang使用系列---- Fmt</title>
          <link>https://kingjcy.github.io/post/golang/go-fmt/</link>
          <pubDate>Mon, 30 May 2016 11:57:05 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-fmt/</guid>
          <description>&lt;p&gt;fmt是实现了格式化的I/O函数，这点类似Ｃ语言中的printf和scanf，但是更加简单。&lt;/p&gt;

&lt;h1 id=&#34;print&#34;&gt;Print&lt;/h1&gt;

&lt;h2 id=&#34;基本函数&#34;&gt;基本函数&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Print&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;// Print 将参数列表 a 中的各个参数转换为字符串并写入到标准输出中。
// 非字符串参数之间会添加空格，返回写入的字节数。
func Print(a ...interface{}) (n int, err error)

// Println 功能类似 Print，只不过最后会添加一个换行符。
// 所有参数之间会添加空格，返回写入的字节数。
func Println(a ...interface{}) (n int, err error)

// Printf 将参数列表 a 填写到格式字符串 format 的占位符中。
// 填写后的结果写入到标准输出中，返回写入的字节数。
func Printf(format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Fprint&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过将转换结果写入到 w 中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Fprint(w io.Writer, a ...interface{}) (n int, err error)
func Fprintln(w io.Writer, a ...interface{}) (n int, err error)
func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Sprint&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过将转换结果以字符串形式返回。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Sprint(a ...interface{}) string
func Sprintln(a ...interface{}) string
func Sprintf(format string, a ...interface{}) string
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Errorf&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同 Sprintf，只不过结果字符串被包装成了 error 类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Errorf(format string, a ...interface{}) error
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    fmt.Print(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, 1, 2, 3, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;\n&amp;quot;)
    fmt.Println(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, 1, 2, 3, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)
    fmt.Printf(&amp;quot;ab %d %d %d cd\n&amp;quot;, 1, 2, 3)
    // ab1 2 3cd
    // a b 1 2 3 c d
    // ab 1 2 3 cd

    if err := percent(30, 70, 90, 160); err != nil {
        fmt.Println(err)
    }
    // 30%
    // 70%
    // 90%
    // 数值 160 超出范围（100）
}

func percent(i ...int) error {
    for _, n := range i {
        if n &amp;gt; 100 {
            return fmt.Errorf(&amp;quot;数值 %d 超出范围（100）&amp;quot;, n)
        }
        fmt.Print(n, &amp;quot;%\n&amp;quot;)
    }
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自定义format类型&#34;&gt;自定义format类型&lt;/h2&gt;

&lt;p&gt;Formatter 由自定义类型实现，用于实现该类型的自定义格式化过程。&lt;/p&gt;

&lt;p&gt;当格式化器需要格式化该类型的变量时，会调用其 Format 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Formatter interface {
    // f 用于获取占位符的旗标、宽度、精度等信息，也用于输出格式化的结果
    // c 是占位符中的动词
    Format(f State, c rune)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由格式化器（Print 之类的函数）实现，用于给自定义格式化过程提供信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type State interface {
    // Formatter 通过 Write 方法将格式化结果写入格式化器中，以便输出。
    Write(b []byte) (ret int, err error)
    // Formatter 通过 Width 方法获取占位符中的宽度信息及其是否被设置。
    Width() (wid int, ok bool)
    // Formatter 通过 Precision 方法获取占位符中的精度信息及其是否被设置。
    Precision() (prec int, ok bool)
    // Formatter 通过 Flag 方法获取占位符中的旗标[+- 0#]是否被设置。
    Flag(c int) bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stringer 由自定义类型实现，用于实现该类型的自定义格式化过程。&lt;/p&gt;

&lt;p&gt;当格式化器需要输出该类型的字符串格式时就会调用其 String 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Stringer interface {
    String() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stringer 由自定义类型实现，用于实现该类型的自定义格式化过程。&lt;/p&gt;

&lt;p&gt;当格式化器需要输出该类型的 Go 语法字符串（%#v）时就会调用其 String 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type GoStringer interface {
    GoString() string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;示例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Ustr string

func (us Ustr) String() string {
    return strings.ToUpper(string(us))
}

func (us Ustr) GoString() string {
    return `&amp;quot;` + strings.ToUpper(string(us)) + `&amp;quot;`
}

func (u Ustr) Format(f fmt.State, c rune) {
    write := func(s string) {
        f.Write([]byte(s))
    }
    switch c {
    case &#39;m&#39;, &#39;M&#39;:
        write(&amp;quot;旗标：[&amp;quot;)
        for s := &amp;quot;+- 0#&amp;quot;; len(s) &amp;gt; 0; s = s[1:] {
            if f.Flag(int(s[0])) {
                write(s[:1])
            }
        }
        write(&amp;quot;]&amp;quot;)
        if v, ok := f.Width(); ok {
            write(&amp;quot; | 宽度：&amp;quot; + strconv.FormatInt(int64(v), 10))
        }
        if v, ok := f.Precision(); ok {
            write(&amp;quot; | 精度：&amp;quot; + strconv.FormatInt(int64(v), 10))
        }
    case &#39;s&#39;, &#39;v&#39;: // 如果使用 Format 函数，则必须自己处理所有格式，包括 %#v
        if c == &#39;v&#39; &amp;amp;&amp;amp; f.Flag(&#39;#&#39;) {
            write(u.GoString())
        } else {
            write(u.String())
        }
    default: // 如果使用 Format 函数，则必须自己处理默认输出
        write(&amp;quot;无效格式：&amp;quot; + string(c))
    }
}

func main() {
    u := Ustr(&amp;quot;Hello World!&amp;quot;)
    // &amp;quot;-&amp;quot; 标记和 &amp;quot;0&amp;quot; 标记不能同时存在
    fmt.Printf(&amp;quot;%-+ 0#8.5m\n&amp;quot;, u) // 旗标：[+- #] | 宽度：8 | 精度：5
    fmt.Printf(&amp;quot;%+ 0#8.5M\n&amp;quot;, u)  // 旗标：[+ 0#] | 宽度：8 | 精度：5
    fmt.Println(u)                // HELLO WORLD!
    fmt.Printf(&amp;quot;%s\n&amp;quot;, u)         // HELLO WORLD!
    fmt.Printf(&amp;quot;%#v\n&amp;quot;, u)        // &amp;quot;HELLO WORLD!&amp;quot;
    fmt.Printf(&amp;quot;%d\n&amp;quot;, u)         // 无效格式：d
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;scan&#34;&gt;Scan&lt;/h1&gt;

&lt;h2 id=&#34;基本函数-1&#34;&gt;基本函数&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Scan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Scan 从标准输入中读取数据，并将数据用空白分割并解析后存入 a 提供的变量中（换行符会被当作空白处理），变量必须以指针传入。当读到 EOF 或所有变量都填写完毕则停止扫描。返回成功解析的参数数量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Scan(a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scanln 和 Scan 类似，只不过遇到换行符就停止扫描。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Scanln(a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scanf 从标准输入中读取数据，并根据格式字符串 format 对数据进行解析，将解析结果存入参数 a 所提供的变量中，变量必须以指针传入。输入端的换行符必须和 format 中的换行符相对应（如果格式字符串中有换行符，则输入端必须输入相应的换行符）。占位符 %c 总是匹配下一个字符，包括空白，比如空格符、制表符、换行符。返回成功解析的参数数量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Scanf(format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Fscan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过从 r 中读取数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Fscan(r io.Reader, a ...interface{}) (n int, err error)
func Fscanln(r io.Reader, a ...interface{}) (n int, err error)
func Fscanf(r io.Reader, format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Sscan&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;功能同上面三个函数，只不过从 str 中读取数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Sscan(str string, a ...interface{}) (n int, err error)
func Sscanln(str string, a ...interface{}) (n int, err error)
func Sscanf(str string, format string, a ...interface{}) (n int, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;// 对于 Scan 而言，回车视为空白
func main() {
    a, b, c := &amp;quot;&amp;quot;, 0, false
    fmt.Scan(&amp;amp;a, &amp;amp;b, &amp;amp;c)
    fmt.Println(a, b, c)
    // 在终端执行后，输入 abc 1 回车 true 回车
    // 结果 abc 1 true
}

// 对于 Scanln 而言，回车结束扫描
func main() {
    a, b, c := &amp;quot;&amp;quot;, 0, false
    fmt.Scanln(&amp;amp;a, &amp;amp;b, &amp;amp;c)
    fmt.Println(a, b, c)
    // 在终端执行后，输入 abc 1 true 回车
    // 结果 abc 1 true
}

// 格式字符串可以指定宽度
func main() {
    a, b, c := &amp;quot;&amp;quot;, 0, false
    fmt.Scanf(&amp;quot;%4s%d%t&amp;quot;, &amp;amp;a, &amp;amp;b, &amp;amp;c)
    fmt.Println(a, b, c)
    // 在终端执行后，输入 1234567true 回车
    // 结果 1234 567 true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从键盘和标准输入 os.Stdin 读取输入，最简单的办法是使用 fmt 包提供的 Scan 和 Sscan 开头的函数。&lt;/p&gt;

&lt;p&gt;从控制台读取输入:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import &amp;quot;fmt&amp;quot;

var (
   firstName, lastName, s string
   i int
   f float32
   input = &amp;quot;56.12 / 5212 / Go&amp;quot;
   format = &amp;quot;%f / %d / %s&amp;quot;
)

func main() {
   fmt.Println(&amp;quot;Please enter your full name: &amp;quot;)
   fmt.Scanln(&amp;amp;firstName, &amp;amp;lastName)
   // fmt.Scanf(&amp;quot;%s %s&amp;quot;, &amp;amp;firstName, &amp;amp;lastName)
   fmt.Printf(&amp;quot;Hi %s %s!\n&amp;quot;, firstName, lastName) // Hi Chris Naegels
   fmt.Sscanf(input, format, &amp;amp;f, &amp;amp;i, &amp;amp;s)
   fmt.Println(&amp;quot;From the string we read: &amp;quot;, f, i, s)
    // 输出结果: From the string we read: 56.12 5212 Go
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scanln 扫描来自标准输入的文本，将空格分隔的值依次存放到后续的参数内，直到碰到换行。&lt;/p&gt;

&lt;p&gt;Scanf 与其类似，除了 Scanf 的第一个参数用作格式字符串，用来决定如何读取。&lt;/p&gt;

&lt;p&gt;Sscan 和以 Sscan 开头的函数则是从字符串读取，除此之外，与 Scanf相同。如果这些函数读取到的结果与您预想的不同，您可以检查成功读入数据的个数和返回的错误。&lt;/p&gt;

&lt;p&gt;也可以使用 bufio 包提供的缓冲读取（buffered reader）来读取数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main
import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;bufio&amp;quot;
    &amp;quot;os&amp;quot;
)

var inputReader *bufio.Reader
var input string
var err error

func main() {
    inputReader = bufio.NewReader(os.Stdin)
    fmt.Println(&amp;quot;Please enter some input: &amp;quot;)
    input, err = inputReader.ReadString(&#39;\n&#39;)
    if err == nil {
        fmt.Printf(&amp;quot;The input was: %s\n&amp;quot;, input)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自定义format类型-1&#34;&gt;自定义format类型&lt;/h2&gt;

&lt;p&gt;Scanner 由自定义类型实现，用于实现该类型的自定义扫描过程。&lt;/p&gt;

&lt;p&gt;当扫描器需要解析该类型的数据时，会调用其 Scan 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Scanner interface {
    // state 用于获取占位符中的宽度信息，也用于从扫描器中读取数据进行解析。
    // verb 是占位符中的动词
    Scan(state ScanState, verb rune) error
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由扫描器（Scan 之类的函数）实现，用于给自定义扫描过程提供数据和信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ScanState interface {
    // ReadRune 从扫描器中读取一个字符，如果用在 Scanln 类的扫描器中，
    // 则该方法会在读到第一个换行符之后或读到指定宽度之后返回 EOF。
    // 返回“读取的字符”和“字符编码所占用的字节数”
    ReadRune() (r rune, size int, err error)
    // UnreadRune 撤消最后一次的 ReadRune 操作，
    // 使下次的 ReadRune 操作得到与前一次 ReadRune 相同的结果。
    UnreadRune() error
    // SkipSpace 为 Scan 方法提供跳过开头空白的能力。
    // 根据扫描器的不同（Scan 或 Scanln）决定是否跳过换行符。
    SkipSpace()
    // Token 用于从扫描器中读取符合要求的字符串，
    // Token 从扫描器中读取连续的符合 f(c) 的字符 c，准备解析。
    // 如果 f 为 nil，则使用 !unicode.IsSpace(c) 代替 f(c)。
    // skipSpace：是否跳过开头的连续空白。返回读取到的数据。
    // 注意：token 指向共享的数据，下次的 Token 操作可能会覆盖本次的结果。
    Token(skipSpace bool, f func(rune) bool) (token []byte, err error)
    // Width 返回占位符中的宽度值以及宽度值是否被设置
    Width() (wid int, ok bool)
    // 因为上面实现了 ReadRune 方法，所以 Read 方法永远不应该被调用。
    // 一个好的 ScanState 应该让 Read 直接返回相应的错误信息。
    Read(buf []byte) (n int, err error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Ustr string

func (u *Ustr) Scan(state fmt.ScanState, verb rune) (err error) {
    var s []byte
    switch verb {
    case &#39;S&#39;:
        s, err = state.Token(true, func(c rune) bool { return &#39;A&#39; &amp;lt;= c &amp;amp;&amp;amp; c &amp;lt;= &#39;Z&#39; })
        if err != nil {
            return
        }
    case &#39;s&#39;, &#39;v&#39;:
        s, err = state.Token(true, func(c rune) bool { return &#39;a&#39; &amp;lt;= c &amp;amp;&amp;amp; c &amp;lt;= &#39;z&#39; })
        if err != nil {
            return
        }
    default:
        return fmt.Errorf(&amp;quot;无效格式：%c&amp;quot;, verb)
    }
    *u = Ustr(s)
    return nil
}

func main() {
    var a, b, c, d, e Ustr
    n, err := fmt.Scanf(&amp;quot;%3S%S%3s%2v%x&amp;quot;, &amp;amp;a, &amp;amp;b, &amp;amp;c, &amp;amp;d, &amp;amp;e)
    fmt.Println(a, b, c, d, e)
    fmt.Println(n, err)
    // 在终端执行后，输入 ABCDEFGabcdefg 回车
    // 结果：
    // ABC DEFG abc de
    // 4 无效格式：x
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>golang使用系列---- Time</title>
          <link>https://kingjcy.github.io/post/golang/go-time/</link>
          <pubDate>Tue, 12 Apr 2016 20:11:01 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/golang/go-time/</guid>
          <description>&lt;p&gt;time包中包括两类时间：时间点（某一时刻）和时长（某一段时间）的基本操作。&lt;/p&gt;

&lt;h1 id=&#34;time&#34;&gt;time&lt;/h1&gt;

&lt;h2 id=&#34;基本结构&#34;&gt;基本结构&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Time&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Time struct {
    wall uint64
    ext  int64
    loc *Location
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;wall  秒&lt;/li&gt;
&lt;li&gt;ext   纳秒&lt;/li&gt;
&lt;li&gt;loc *Location

&lt;ul&gt;
&lt;li&gt;time.UTC utc时间&lt;/li&gt;
&lt;li&gt;time.Local 本地时间&lt;/li&gt;
&lt;li&gt;FixedZone(name string, offset int) *Location   设置时区名,以及与UTC0的时间偏差.返回Location&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Duration&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Duration int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Duration类型代表两个时间点之间经过的时间，以纳秒为单位。可表示的最长时间段大约290年。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;时间常量&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Duration的单位为 nanosecond，为了便于使用，time中定义了时间常量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const (
    Nanosecond Duration = 1
    Microsecond = 1000 * Nanosecond
    Millisecond = 1000 * Microsecond
    Second = 1000 * Millisecond
    Minute = 60 * Second
    Hour = 60 * Minute
)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Ticker&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;type Ticker
type Ticker struct {
    C &amp;lt;-chan Time // 周期性传递时间信息的通道
    // 内含隐藏或非导出字段
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ticker保管一个通道，并每隔一段时间向其传递&amp;rdquo;tick&amp;rdquo;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewTicker
func NewTicker(d Duration) *Ticker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewTicker返回一个新的Ticker，该Ticker包含一个通道字段，并会每隔时间段d就向该通道发送当时的时间。它会调整时间间隔或者丢弃tick信息以适应反应慢的接收者。如果d&amp;lt;=0会panic。关闭该Ticker可以释放相关资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (*Ticker) Stop
func (t *Ticker) Stop()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stop关闭一个Ticker。在关闭后，将不会发送更多的tick信息。Stop不会关闭通道t.C，以避免从该通道的读取不正确的成功。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;总结&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;time.Duration（时长，耗时）&lt;/li&gt;
&lt;li&gt;time.Time（时间点）&lt;/li&gt;
&lt;li&gt;time.C（放时间点的管道）[ Time.C:=make(chan time.Time) ]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;基本函数&#34;&gt;基本函数&lt;/h2&gt;

&lt;p&gt;time包提供了时间的显示和测量用的函数。日历的计算采用的是公历。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Now
func Now() Time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now返回当前本地时间。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (Time) Before
func (t Time) Before(u Time) bool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果t代表的时间点在u之前，返回真；否则返回假。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (Time) Add
func (t Time) Add(d Duration) Time
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add返回时间点t+d。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (Time) Second
func (t Time) Second() int
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回t对应的那一分钟的第几秒，范围[0, 59]。&lt;/p&gt;

&lt;h1 id=&#34;常规使用&#34;&gt;常规使用&lt;/h1&gt;

&lt;h2 id=&#34;sleep&#34;&gt;sleep&lt;/h2&gt;

&lt;p&gt;golang的休眠可以使用time包中的sleep。&lt;/p&gt;

&lt;p&gt;函数原型为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Sleep(d Duration)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面实现休眠2秒功能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;time&amp;quot;
)

func main() {

    fmt.Println(&amp;quot;begin&amp;quot;)
    time.Sleep(time.Duration(2)*time.Second)
    fmt.Println(&amp;quot;end&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;time使用变量的时候需要强制转换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time.Duration(cfg.CTimeOut) * time.Second
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;定时器&#34;&gt;定时器&lt;/h2&gt;

&lt;p&gt;定时器只会传达一次到期事件，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Timer struct {
    C &amp;lt;-chan Time
    r runtimeTimer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每天定时0点执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
    &amp;quot;time&amp;quot;
    &amp;quot;fmt&amp;quot;
)

//定时结算Boottime表数据
func BoottimeTimingSettlement() {
    for {
        now := time.Now()
        // 计算下一个零点
        next := now.Add(time.Hour * 24)
        next = time.Date(next.Year(), next.Month(), next.Day(), 0, 0, 0, 0, next.Location())
        t := time.NewTimer(next.Sub(now))
        &amp;lt;-t.C
        Printf(&amp;quot;定时结算Boottime表数据，结算完成: %v\n&amp;quot;,time.Now())
        //以下为定时执行的操作
        BoottimeSettlement()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;断续器&#34;&gt;断续器&lt;/h2&gt;

&lt;p&gt;周期性的传达到期事件的装置，定时器只会传达一次到期事件，断续器会持续工作直到停止。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Ticker struct {
    C &amp;lt;-chan Time // The channel on which the ticks are delivered.
    r runtimeTimer
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NewTicker(d Duration) *Ticker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NewTicker返回一个新的Ticker，该Ticker包含一个通道字段，并会每隔时间段d就向该通道发送当时的时间。它会调整时间间隔或者丢弃tick信息以适应反应慢的接收者。如果d&amp;lt;=0会panic。关闭该Ticker可以释放相关资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ticker := time.NewTicker(time.Millisecond * 500)
go func() {
    for t := range ticker.C {
        fmt.Println(&amp;quot;Tick at&amp;quot;, t)
    }
}()

time.Sleep(time.Millisecond * 1500)   //阻塞，则执行次数为sleep的休眠时间/ticker的时间
ticker.Stop()    
fmt.Println(&amp;quot;Ticker stopped&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取时间&#34;&gt;获取时间&lt;/h2&gt;

&lt;p&gt;各种现有时间的获取&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    fmt.Printf(&amp;quot;时间戳（秒）：%v;\n&amp;quot;, time.Now().Unix())
    fmt.Printf(&amp;quot;时间戳（纳秒）：%v;\n&amp;quot;,time.Now().UnixNano())
    fmt.Printf(&amp;quot;时间戳（毫秒）：%v;\n&amp;quot;,time.Now().UnixNano() / 1e6)
    fmt.Printf(&amp;quot;时间戳（纳秒转换为秒）：%v;\n&amp;quot;,time.Now().UnixNano() / 1e9)
}


时间戳（秒）：1530027865;
时间戳（纳秒）：1530027865231834600;
时间戳（毫秒）：1530027865231;
时间戳（纳秒转换为秒）：1530027865;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;时间转化&#34;&gt;时间转化&lt;/h2&gt;

&lt;p&gt;处理时间单位自动转化问题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func ParseDuration(s string) (Duration, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;传入字符串，返回响应的时间，其中传入的字符串中的有效时间单位如下：h,m,s,ms,us,ns，其他单位均无效，如果传入无效时间单位，则会返回０&lt;/p&gt;

&lt;p&gt;获取前n天的时间&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//获取两天前的时间
currentTime := time.Now()
oldTime := currentTime.AddDate(0, 0, -2)        //若要获取3天前的时间，则应将-2改为-3
//oldTime 的结果为go的时间time类型，2018-09-25 13:24:58.287714118 +0000 UTC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;比较时间，使用before&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time1 := &amp;quot;2015-03-20 08:50:29&amp;quot;
time2 := &amp;quot;2015-03-21 09:04:25&amp;quot;
//先把时间字符串格式化成相同的时间类型
t1, err := time.Parse(&amp;quot;2006-01-02 15:04:05&amp;quot;, time1)
t2, err := time.Parse(&amp;quot;2006-01-02 15:04:05&amp;quot;, time2)
if err == nil &amp;amp;&amp;amp; t1.Before(t2) {
    //处理逻辑
    fmt.Println(&amp;quot;true&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;获取文件的各种时间&#34;&gt;获取文件的各种时间&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;func main() {
    finfo, _ := os.Stat(filename)
    // Sys()返回的是interface{}，所以需要类型断言，不同平台需要的类型不一样，linux上为*syscall.Stat_t
    stat_t := finfo.Sys().(*syscall.Stat_t)
    fmt.Println(stat_t)
    // atime，ctime，mtime分别是访问时间，创建时间和修改时间，具体参见man 2 stat
    fmt.Println(timespecToTime(stat_t.Atim))
    fmt.Println(timespecToTime(stat_t.Ctim))
    fmt.Println(timespecToTime(stat_t.Mtim))
}
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
      
    
      
        <item>
          <title>监控metrics系列---- Zabbix监控方案</title>
          <link>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix-scheme/</link>
          <pubDate>Fri, 04 Mar 2016 17:54:04 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix-scheme/</guid>
          <description>&lt;p&gt;zabbix是目前各大互联网公司使用最广泛的开源监控之一,其历史最早可追溯到1998年,在业内拥有各种成熟的解决方案，但是对容器的监控还是比较薄弱，我们也不多说，主要用于基础设施VM的监控。&lt;/p&gt;

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kingjcy.github.io/media/monitor/zabbix/zabbix.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;详细说明&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;agent：负载采集数据，所有的采集都在这一个进程中，不像prometheus的exporter有很多。&lt;/li&gt;
&lt;li&gt;proxy：是一个汇聚层，将数据聚合后发送到server。&lt;/li&gt;
&lt;li&gt;server：服务端，用于存储数据，对外进行查询展示。&lt;/li&gt;
&lt;li&gt;DB：数据库，存储数据&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;zabbix的核心组件&#34;&gt;zabbix的核心组件&lt;/h1&gt;

&lt;p&gt;1、zabbix server 负责采集和收取agent采集的信息。&lt;/p&gt;

&lt;p&gt;2、zabbix database   用于存储zabbix的配置信息，监控数据&lt;/p&gt;

&lt;p&gt;3、zabbix web zabbix的管理界面，监控界面，可以独立部署，只要能连接到database就可以&lt;/p&gt;

&lt;p&gt;4、zabbix agent 不数据监控主机主机上，负责采集数据，把数据推送到server或者server来去数据（主动和被动模式，可以同时设置）&lt;/p&gt;

&lt;p&gt;5、zabbix proxy 用于分布式监控，作用就是用于聚合部分数据，最后统一发完server&lt;/p&gt;

&lt;p&gt;zabbix对分布式的数据采集非常好,支持两种分布式架构,一种是Proxy,一种是Node.Proxy作为zabbix server的代理去监控服务器,并发数据汇聚到Zabbix server.而Node本身就是一个完整的Zabbix server, 使用Node可以将多个Zabbix server组成一个具有基层关系的分布式架构.&lt;/p&gt;

&lt;p&gt;两者的区别如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                proxy   Node
轻量级         √       ×
GUI前端           ×       √
是否可以独立运行    √       ×
容易运维            √       ×
本地Admin管理   ×       √
中心化配置       √       ×
产生通知            ×       √
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6、zabbix get 安装服务器上，来测试获取agent的数据的工具&lt;/p&gt;

&lt;p&gt;7、zabbix sender 安装在客户端机器上，用于测试推送数据到server的的工具&lt;/p&gt;

&lt;h1 id=&#34;zabbix监控方式&#34;&gt;Zabbix监控方式&lt;/h1&gt;

&lt;p&gt;1、被动模式&lt;/p&gt;

&lt;p&gt;被动检测：相对于agent而言；agent, server向agent请求获取配置的各监控项相关的数据，agent接收请求、获取数据并响应给server；&lt;/p&gt;

&lt;p&gt;2、主动模式&lt;/p&gt;

&lt;p&gt;主动检测：相对于agent而言；agent(active),agent向server请求与自己相关监控项配置，主动地将server配置的监控项相关的数据发送给server；&lt;/p&gt;

&lt;p&gt;主动监控能极大节约监控server 的资源。&lt;/p&gt;

&lt;h1 id=&#34;zabbix的使用&#34;&gt;zabbix的使用&lt;/h1&gt;

&lt;p&gt;基本安装使用可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix/&#34;&gt;这里&lt;/a&gt;,相关源码解析可以看&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbixcode/&#34;&gt;这里&lt;/a&gt;,这些就不多说了。&lt;/p&gt;

&lt;h1 id=&#34;扩展&#34;&gt;扩展&lt;/h1&gt;

&lt;p&gt;随着系统监控规模的越来越大，zabbix出现越来越多的瓶颈，随着时序数据库的广泛使用，监控已经渐渐切换到了时序数据库，对于原始的zabbix监控项，监控数据如何处理？我们可以将zabbix数据存到时序数据库中，统一使用，相关&lt;a href=&#34;https://kingjcy.github.io/post/monitor/metrics/zabbix/zabbix2tsdb/&#34;&gt;实现方案&lt;/a&gt;就需要自己实现了。&lt;/p&gt;</description>
        </item>
      
    
      
        <item>
          <title>用hugo&#43;github构建自己的blog</title>
          <link>https://kingjcy.github.io/post/tool/hugo-blog-build/</link>
          <pubDate>Fri, 29 Aug 2014 09:29:40 CST</pubDate>
          <author></author>
          <guid>https://kingjcy.github.io/post/tool/hugo-blog-build/</guid>
          <description>&lt;p&gt;这个是我用hugo+github搭建起个人blog写的第一篇文章，有点小兴奋。。。首先把搭建测过程写起来和大家分享一下吧。&lt;/p&gt;

&lt;p&gt;首先，作为一个程序员，不拥有自己搭建的blog，而去用别人搭建好的去注册一下，我是无法接受的！！搭建个人blog需要两个东西：&lt;/p&gt;

&lt;p&gt;1、静态网页生成器，有jekyll，hexo，hugo等，由于最近在玩go语言，所以就选择了hugo，其他的也没有深入了解，后面搭建起来，发现hugo还是比较简单。&lt;/p&gt;

&lt;p&gt;2、github pages 这个是github提供的一个托管工作，相当好用。&lt;/p&gt;

&lt;h1 id=&#34;静态页面生成器hugo&#34;&gt;静态页面生成器hugo&lt;/h1&gt;

&lt;p&gt;这个比较方便的静态页面生成器，首先需要安装，我的系统是centos 64位的.&lt;/p&gt;

&lt;p&gt;现在换成了macos系统了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;install&#34;&gt;install&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、直接下载二进制文件，这也是我说的方便的地方。&lt;/p&gt;

&lt;p&gt;Hugo二进制下载地址：&lt;a href=&#34;https://github.com/spf13/hugo/releases&#34;&gt;https://github.com/spf13/hugo/releases&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2。使用macos系统后直接使用homebrew进行安装更新，这个就是一个类似于linux的yum的工具。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install hugo
brew upgrade hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;use&#34;&gt;use&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;下载下来后，首先要生成自己的站点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`hugo new site mysite`--这边hugo的二进制文件不一定是这个名字，可以起个别名alias来用
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时会在mysite目录下生成一些目录和文件，这边简单的介绍一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、config.toml是网站的配置文件，这是它的作者GitHub联合创始人Tom Preston-Werner 觉得YAML不够优雅，捣鼓出来的一个新格式。如果你不喜欢这种格式，你可以将config.toml替换为YAML格式的config.yaml，或者json格式的config.json。hugo都支持。
2、content目录里放的是你写的markdown文章。
3、layouts目录里放的是网站的模板文件。
4、static目录里放的是一些图片、css、js等资源。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后进入站点目录mysite，新建文档&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`cd mysite`

`hugo new about.md`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这边新建一个md文件会出现在content目录下，一般这个about.md文件是一个关于本站的介绍或者blog个人介绍，在这边将一下md文件的编辑，其实就是MarkDown格式文件的编写，具体的格式可以参考本文的编辑，或者去网上去搜索一下就ok,这边我说几点，我经常记错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1、就是&amp;quot;+++&amp;quot;内的赋值用&amp;quot;=&amp;quot;，&amp;quot;---&amp;quot;内的用&amp;quot;:&amp;quot;。

2、`###`后面必须有空格。

3、有空行才能换行。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般我们写博文，会放在content/post下，正如我这边编写的第一篇文&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`hugo new post/first.md`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后用vim编辑器进行编辑，编辑好后，就可以将你编辑的文字生成静态网页了，当然你肯定需要一个模板，这样可以使你的网页根据美观，这边在讲一下模板的使用&lt;/p&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;模版&#34;&gt;模版&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、模板放在站点的themes下，一般木有这个文件夹，我们需要新增一个&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`mkdir themes`

`cd themes`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2、模板可以到hugo官网上去找,那边可以showcase预览一下自己喜欢的，具体的安装方式也有介绍，就是用&lt;code&gt;git clone&lt;/code&gt;把源码下到themes目录下就好&lt;/p&gt;

&lt;p&gt;官网：&lt;a href=&#34;https://gohugo.io/overview/introduction/&#34;&gt;https://gohugo.io/overview/introduction/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3、编辑模板的配置文件，这个视具体模板，可以参考我的配置&lt;a href=&#34;https://github.com/kingjcy/&#34;&gt;https://github.com/kingjcy/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下面就是生成我们需要的静态网页了，也就是前端的html文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;`hugo --theme=hyde --baseUrl=&amp;quot;http://kingjcy.github.io/&amp;quot;`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不出意外的话，应该在站点目录下生成一个public文件夹，这个就是我们需要的所有文件了，至此第一步已经完成了。可以看见直接编译是hugo，启动一个web服务是hugo server&lt;/p&gt;

&lt;blockquote&gt;
&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;1、huo new XXXX生成文件是可以直接生成自己想要的内容的，取决于模版，默认是archetypes/default.md，可以对其进行修改，变成自己的样子。&lt;/p&gt;

&lt;p&gt;2、使用图片，默认把图片放在media目录下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;![](/media/worklife/baby/XXX.JPG)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;github-pages托管&#34;&gt;github pages托管&lt;/h1&gt;

&lt;p&gt;这个就简单了，因为本身就是github提供现成的东西，首先新增一个repo，命名为：&lt;code&gt;kingjcy.github.io&lt;/code&gt; （kingjcy替换为你的github用户名）。&lt;/p&gt;

&lt;p&gt;然后将第一步的public加入git版本，上传到这个项目，就可以访问你的个人blog：&lt;code&gt;http://kingjcy.github.io/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;至于git版本控制和github直接的传输，这边就不多讲了，如果需要可以参考我的另外一篇博文《git和github的使用》。&lt;/p&gt;

&lt;p&gt;这边简单列举一些过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd public
$ git init
$ git remote add origin https://github.com/kingjcy/kingjcy.github.io.git
$ git add -A
$ git commit -m &amp;quot;first commit&amp;quot;
$ git push -u origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;终于搭建完了，欢迎指正,tks。&lt;/p&gt;</description>
        </item>
      
    
      
    

  </channel>
</rss>

<!DOCTYPE html>

<html lang="zh-cn">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="author" content="fatedier">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="description" content="Kubernetes中有三种网络和三种IP。

">
<meta property="og:url" content="https://kingjcy.github.io/"><meta property="og:type" content="article">
<meta property="og:title" content="云计算K8s组件系列（九）---- 网络 - kingjcy blog"><meta property="og:site_name" content="kingjcy blog">

<title>
    
    云计算K8s组件系列（九）---- 网络
    
</title>

<link rel="stylesheet" href="/onlyone/onlyone.css">
<link rel="shortcut icon" href="/assets/favicon.ico">
<script src="/onlyone/onlyone.js"></script>
<link rel="alternate" type="application/rss+xml" title="RSS" href="/index.xml">
</head>
<body>


<div class="container">
    <header class="nav">
        <nav class="navbar navbar-default">
            <div class="container-fluid">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/">kingjcy blog</a>
                </div>

                <div class="collapse navbar-collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav">
                        <li><a href="/categories/技术文章/">技术文章</a></li>
			<li><a href="/categories/读书笔记/">读书笔记</a></li>
                        <li><a href="/categories/人生感悟/">人生感悟</a></li>
                        <li><a href="/categories/">归档</a></li>
                        <li><a href="/tags/">分类</a></li>
                        <li><a href="/about/">关于我</a></li>
                        <li>
                            <form method="get" style="padding: 8px" action="https://www.google.com/search" target="_blank">
                                <input type="hidden" name="sitesearch" value="https://kingjcy.github.io/"/>
                                <input type="text" class="form-control" name="q" placeholder="Press enter to search">
                            </form>
                        </li>
                    </ul>

                </div>
            </div>
        </nav>
    </header>


<div class="row">
    <div class="col-md-8">
        <article class="post single">

            <header>
                <div class="post-date">
                    2020年04月10日 
                </div>
                <h1 class="post-title">云计算K8s组件系列（九）---- 网络</h1>
            </header>

            <div class="post-content">
                <p>Kubernetes中有三种网络和三种IP。</p>

<p><img src="/media/cloud/k8s/network" alt="" /></p>

<h1 id="docker网络实现">docker网络实现</h1>

<p>想要理解k8s的网络，需要先了解docker的网络实现，用过docker基本都知道，启动docker engine后，主机的网络设备里会有一个docker0的网关，而容器默认情况下会被分配在一个以docker0为网关的虚拟子网中。</p>

<pre><code>root@VM-66-197-ubuntu:/home/ubuntu# ifconfig
...
docker0   Link encap:Ethernet  HWaddr 02:42:ec:43:56:b2
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
...
root@VM-66-197-ubuntu:/home/ubuntu# docker inspect nginx
···
&quot;IPAddress&quot;: &quot;172.17.0.2&quot;,
···
</code></pre>

<p>为了实现上述功能，docker主要用到了linux的Bridge、Network Namespace、VETH。</p>

<ul>
<li>Bridge相当于是一个虚拟网桥，工作在第二层网络。也可以为它配置IP，工作在三层网络。docker0网关就是通过Bridge实现的。</li>
<li>Network Namespace是网络命名空间，通过Network Namespace可以建立一些完全隔离的网络栈。比如通过docker network create xxx就是在建立一个Network Namespace。</li>
<li>VETH是虚拟网卡的接口对，可以把两端分别接在两个不同的Network Namespace中，实现两个原本隔离的Network Namespace的通信。</li>
</ul>

<p>所以总结起来就是：Network Namespace做了容器和宿主机的网络隔离，Bridge分别在容器和宿主机建立一个网关，然后再用VETH将容器和宿主机两个网络空间连接起来。</p>

<p><img src="/media/cloud/docker/network/bridge" alt="" /></p>

<p>这就docker默认的bridge实现方式，对此，docker总结提出来CNM（container network model）理论，还在这个理论的基础上实现了libnetwork。具体docker网络的实现可以查看<a href="/post/cloud/paas/base/docker/docker-network/">这里</a>。</p>

<h1 id="kubernetes网络模型">Kubernetes网络模型</h1>

<p>1、每个Pod拥有唯一的IP（perPodperIp模型）。</p>

<p>2、所有的pod都在一个可以直接连通的、扁平的网络空间中。满足以下条件：</p>

<ul>
<li>任意两个 pod 之间其实是可以直接通信的，无需经过显式地使用 NAT 来接收数据和地址的转换；</li>
<li>node 与 pod 之间是可以直接通信的，无需使用明显的地址转换；</li>
<li>pod 看到自己的 IP 跟别人看见它所用的 IP 是一样的，中间不能经过转换。</li>
</ul>

<p>3、四大目标，其实是在设计一个 K8s 的系统为外部世界提供服务的时候，从网络的角度要想清楚，外部世界如何一步一步连接到容器内部的应用？</p>

<ul>
<li>外部世界和 service 之间是怎么通信的？就是有一个互联网或者是公司外部的一个用户，怎么用到 service？service 特指 K8s 里面的服务概念。</li>
<li>service 如何与它后端的 pod 通讯？</li>
<li>pod 和 pod 之间调用是怎么做到通信的？</li>
<li>最后就是 pod 内部容器与容器之间的通信？</li>
</ul>

<p>最终要达到目标，就是外部世界可以连接到最里面，对容器提供服务。</p>

<h1 id="pod-network">Pod Network</h1>

<h2 id="pod内网络">pod内网络</h2>

<p>Kubernetes的一个Pod中包含有多个容器，这些容器共享一个Network Namespace，更具体的说，是共享一个Network Namespace中的一个IP。创建Pod时，首先会生成一个pause容器，然后其他容器会共享pause容器的网络。</p>

<pre><code>root@kube-2:~# docker ps
CONTAINER ID        IMAGE                                            COMMAND                  CREATED             STATUS              PORTS               NAMES
d2dbb9e288e2        mirrorgooglecontainers/pause-amd64:3.0           &quot;/pause&quot;                 3 weeks ago         Up 3 weeks                              k8s_POD_frontend-647d9fdddf-n4x9w_default_9f275ea8-4853-11e8-8c42-fa163e4a07e5_0
cf1bfff28238        nginx                                            &quot;nginx -g 'daemon of…&quot;   3 weeks ago         Up 3 weeks                              k8s_nginx-demo_frontend-647d9fdddf-n4x9w_default_9f275ea8-4853-11e8-8c42-fa163e4a07e5_0

root@kube-2:~# docker inspect cf1bf
...
            &quot;NetworkMode&quot;: &quot;container:d2dbb9e288e26231759e28e8d4816862c6c57d4d2822a259bee7fcc9a2fd0b20&quot;,
...
</code></pre>

<p>可以看出，在这个Pod中，nginx容器通过&rdquo;NetworkMode&rdquo;: &ldquo;container:d2db&hellip;&ldquo;与pause容器共享了网络，我们可以想到docker我网络的contaienr模式。这时候，相同容器之间的访问只需要用localhost+端口的形式，就像他们是部署在同一台物理机的不同进程一样，可以使用本地IPC进行通信。</p>

<h2 id="pod间通信">pod间通信</h2>

<h3 id="节点内通信">节点内通信</h3>

<p>在每个Kubernetes节点上，都有一个根（root）命名空间（root是作为基准，而不是超级用户）&ndash;root netns。这个命令空间就是我们node节点的网络，包含这个物理网卡eth0。</p>

<p><img src="/media/cloud/k8s/network10" alt="" /></p>

<p>类似的，每个Pod都有其自身的netns，通过一个虚拟的以太网对连接到root netns。这基本上就是一个管道对，一端在root netns内，另一端在Pod的nens内。</p>

<p>我们把Pod端的网络接口叫 eth0，这样Pod就不需要知道底层主机，它认为它拥有自己的根网络设备。另一端命名成比如 vethxxx。你可以用ifconfig 或者 ip a 命令列出你的节点上的所有这些接口。</p>

<p><img src="/media/cloud/k8s/network11" alt="" /></p>

<p>节点上的所有Pod都会完成这个过程。这些Pod要相互通信，就要用到linux的以太网桥 cbr0 了。Docker使用了类似的网桥，称为docker0。</p>

<p><img src="/media/cloud/k8s/network12" alt="" /></p>

<p>所以一个网络数据包要由pod1到pod2。</p>

<ul>
<li>它由pod1中netns的eth0网口离开，通过vethxxx进入root netns。</li>
<li>然后被传到cbr0，cbr0使用ARP请求，说“谁拥有这个IP”，从而发现目标地址。</li>
<li>vethyyy说它有这个IP，因此网桥就知道了往哪里转发这个包。</li>
<li>数据包到达vethyyy，跨过管道对，到达pod2的netns。</li>
</ul>

<p>这就是同一节点内容器间通信的流程。当然也可以用其它方式，但是无疑这是最简单的方式，同时也是Docker单机采用的方式，可以看出只是把container换成了pod。</p>

<h3 id="节点间通信">节点间通信</h3>

<p>正如我前面提到，Pod也需要跨节点可达。Kubernetes不关心如何实现。我们可以使用L2（ARP跨节点），L3（IP路由跨节点，就像云提供商的路由表），Overlay网络，或者甚至信鸽。无所谓，只要流量能到达另一个节点的期望Pod就好。每个节点都为Pod IPs分配了唯一的CIDR块（一段IP地址范围），因此每个Pod都拥有唯一的IP，不会和其它节点上的Pod冲突。</p>

<p><img src="/media/cloud/k8s/network13" alt="" /></p>

<p>一个数据包要从pod1到达pod4（在不同的节点上）</p>

<ul>
<li>它由pod1中netns的eth0网口离开，通过vethxxx进入root netns。</li>
<li>然后被传到cbr0，cbr0通过发送ARP请求来找到目标地址。</li>
<li>本节点上没有Pod拥有pod4的IP地址，因此数据包由cbr0 传到 主网络接口 eth0.</li>
<li>数据包的源地址为pod1，目标地址为pod4，它以这种方式离开node1进入电缆。</li>
<li>路由表有每个节点的CIDR块的路由设定，它把数据包路由到CIDR块包含pod4的IP的节点。</li>
<li>因此数据包到达了node2的主网络接口eth0。现在即使pod4不是eth0的IP，数据包也仍然能转发到cbr0，因为节点配置了IP forwarding enabled。节点的路由表寻找任意能匹配pod4 IP的路由。它发现了 cbr0 是这个节点的CIDR块的目标地址。你可以用route -n命令列出该节点的路由表，它会显示cbr0的路由。</li>
<li>网桥接收了数据包，发送ARP请求，发现目标IP属于vethyyy。</li>
<li>数据包跨过管道对到达pod4。</li>
</ul>

<h1 id="node-network">Node network</h1>

<p>其实就是整个k8s集群node之间的网络，也就是基本的物理机网络，只要各个node之间能相互通信就好。</p>

<h1 id="service-network">Service Network</h1>

<p>service网络主要是kube-proxy来实现构建的,主要是Pod 和 Service 间通信，外部和 Service 间通信，这些都可以使用kube-proxy的各种模式来解决，具体可以看<a href="/post/cloud/paas/base/kubernetes/k8s-proxy/">这里</a>。</p>

<p>service network比较特殊，每个新创建的service会被分配一个service IP，比如在集群中设置环境变量export SERVICE_CLUSTER_IP_RANGE=${SERVICE_CLUSTER_IP_RANGE:-192.168.3.0/24}，并在kube-apiserver的启动参数&ndash;service-cluster-ip-range使用这个环境变量，这个IP的分配范围是192.168.3.0/24。不过这个IP并不“真实”，更像一个“占位符”并且只有入口流量。</p>

<p>在 Service 创建的请求中，可以通过设置 spec.clusterIP 字段来指定自己的集群 IP 地址。 比如，希望替换一个已经已存在的 DNS 条目，或者遗留系统已经配置了一个固定的 IP 且很难重新配置。用户选择的 IP 地址必须合法，并且这个 IP 地址在 service-cluster-ip-range CIDR 范围内，这对 API Server 来说是通过一个标识来指定的。 如果 IP 地址不合法，API Server 会返回 HTTP 状态码 422，表示值不合法。</p>

<h1 id="kubernetes网络开源组件">Kubernetes网络开源组件</h1>

<p>service network的实现依靠kube-proxy，node network依靠的是物理组网，pod network就需要插件来完成了，目前比较通用的就是CNI规范的插件。其实主要是解决两个问题：</p>

<ul>
<li>为容器分配IP地址</li>
<li>不同容器之间的互通</li>
</ul>

<h2 id="基本概念">基本概念</h2>

<p>第2层网络：OSI（Open Systems Interconnections，开放系统互连）网络模型的“数据链路”层。第2层网络会处理网络上两个相邻节点之间的帧传递。第2层网络的一个值得注意的示例是以太网，其中MAC表示为子层。</p>

<p>第3层网络：OSI网络模型的“网络”层。第3层网络的主要关注点，是在第2层连接之上的主机之间路由数据包。IPv4、IPv6和ICMP是第3层网络协议的示例。</p>

<p>IPAM：IP地址管理；这个IP地址管理并不是容器所特有的，传统的网络比如说DHCP其实也是一种IPAM，到了容器时代我们谈IPAM，主流的两种方法： 基于CIDR的IP地址段分配地或者精确为每一个容器分配IP。但总之一旦形成一个容器主机集群之后，上面的容器都要给它分配一个全局唯一的IP地址，这就涉及到IPAM的话题。</p>

<p>Overlay：在现有二层或三层网络之上再构建起来一个独立的网络，这个网络通常会有自己独立的IP地址空间、交换或者路由的实现。</p>

<p>IPSesc：一个点对点的一个加密通信协议，一般会用到Overlay网络的数据通道里。</p>

<p>vxLAN：由VMware、Cisco、RedHat等联合提出的这么一个解决方案，这个解决方案最主要是解决VLAN支持虚拟网络数量（4096）过少的问题。因为在公有云上每一个租户都有不同的VPC，4096明显不够用。就有了vxLAN，它可以支持1600万个虚拟网络，基本上公有云是够用的。</p>

<p>网桥Bridge： 连接两个对等网络之间的网络设备，但在今天的语境里指的是Linux Bridge，就是大名鼎鼎的Docker0这个网桥。</p>

<p>BGP： 代表“边界网关协议”，主干网自治网络的路由协议，今天有了互联网，互联网由很多小的自治网络构成的，自治网络之间的三层路由是由BGP实现的。</p>

<p>SDN、Openflow： 软件定义网络里面的一个术语，比如说我们经常听到的流表、控制平面，或者转发平面都是Openflow里的术语。</p>

<h2 id="容器网络模型">容器网络模型</h2>

<p>容器网络发展到现在，形成了两大阵营，就是Docker的CNM和Google、CoreOS、Kuberenetes主导的CNI。首先明确一点，CNM和CNI并不是网络实现，他们是网络规范和网络体系，从研发的角度他们就是一堆接口，你底层是用Flannel也好、用Calico也好，他们并不关心，CNM和CNI关心的是网络管理的问题。</p>

<h3 id="cnm">CNM</h3>

<p>CNM（Docker LibnetworkContainer Network Model）</p>

<p>Docker Libnetwork的优势就是原生，而且和Docker容器生命周期结合紧密；缺点也可以理解为是原生，被Docker“绑架”。</p>

<pre><code>Docker Swarm overlay
Macvlan &amp; IP networkdrivers
Calico
Contiv
Weave
</code></pre>

<p>目前因为docker很少单独使用，使用CMN规范的也比较少，但是很多都是相通的。</p>

<h3 id="cni">CNI</h3>

<p>CNI的优势是兼容其他容器技术（e.g. rkt）及上层编排系统（Kubernetes &amp; Mesos)，而且社区活跃势头迅猛，Kubernetes加上CoreOS主推，缺点是非Docker原生。</p>

<pre><code>Kubernetes
Weave
Macvlan
Calico
Flannel
Contiv
Mesos CNI
</code></pre>

<p>Kubernetes 在处理网络上，没有选择自己再独立创造一个，而是选择了其中的 CNI作为了自己的网络插件。（至于为什么不选择 CNM，可以看看这篇官方的解释：Why Kubernetes doesn’t use libnetwork）。不使用 CNM 最关键的一点，是 k8s 考虑到CNM 在一定程度上和 container runtime 联系相对比较紧密，不好解耦。
有了 k8s 这种巨无霸的选择之后，后来的很多项目都在 CNM 和 CNI 之间选择了 CNI。</p>

<p>CNI目前已经获得了众多开源项目的青睐，比如 K8S、Memos、Cloud Foundry。同时被Cloud Native Computing Foundation所认可。CNCF 背后有众多的科技大亨，所以可以预见，CNI 将会成为未来容器网络的标准，所以很有必要详细研究了解<a href="/post/cloud/paas/base/kubernetes/k8s-network-cni/">CNI</a></p>

<h2 id="容器网络方案">容器网络方案</h2>

<p>容器网络方案基本就是只有下面几种解决方式，后面的组件实现都是基于这些思路来进行的。</p>

<blockquote>
<p>隧道方案（Overlay Networking）</p>
</blockquote>

<p>隧道方案在IaaS层的网络中应用也比较多，大家共识是随着节点规模的增长复杂度会提升，而且出了网络问题跟踪起来比较麻烦，大规模集群情况下这是需要考虑的一个点。</p>

<pre><code>Weave：UDP广播，本机建立新的BR，通过PCAP互通
Open vSwitch（OVS）：基于VxLan和GRE协议，但是性能方面损失比较严重
Flannel：UDP广播，VxLan
Racher：IPsec
</code></pre>

<blockquote>
<p>底层网络（Underlay Networking）</p>
</blockquote>

<p>underlay的一般是从3层或者2层实现隔离和跨主机容器互通的，出了问题也很容易排查。</p>

<ul>
<li>二层主要是mac的识别，一般二层交换机只要在一个lan中都能通信，使用vlan的话需要在同一个vlan中，如果跨网段进行通信就需要网关，网关其实就是一个中转的三层交换机，在二层的基础上有路由功能</li>
<li>三层主要是ip的路由，我们一般使用三层交换机作为网关，先和网关进行通信，在和目的进行通信</li>
</ul>

<p>常用的产品</p>

<pre><code>Calico：基于BGP协议的路由方案，支持很细致的ACL控制，对混合云亲和度比较高，是一个三层的实现方案。
Macvlan：从逻辑和Kernel层来看隔离性和性能最优的方案，基于二层隔离，所以需要二层路由器支持，大多数云服务商不支持，所以混合云上比较难以实现。
</code></pre>

<h2 id="组件">组件</h2>

<p>其实网络插件解决的是pod network的问题，都是基于CMN或者CNI的管理规范来开发的网络插件。</p>

<h3 id="kubenet">kubenet</h3>

<p><img src="/media/cloud/k8s/kubenet" alt="" /></p>

<p>kubenet 是一个基于 CNI bridge 的网络插件，它为每个容器建立一对 veth pair 并连接到 cbr0 网桥上。kubenet 在 bridge 插件的基础上拓展了很多功能，包括</p>

<ul>
<li>使用 host-local IPAM 插件为容器分配 IP 地址， 并定期释放已分配但未使用的 IP 地址</li>
<li>设置 sysctl net.bridge.bridge-nf-call-iptables = 1</li>
<li>为 Pod IP 创建 SNAT 规则

<ul>
<li>-A POSTROUTING ! -d 10.0.0.0/8 -m comment &ndash;comment &ldquo;kubenet: SNAT for outbound traffic from cluster&rdquo; -m addrtype ! &ndash;dst-type LOCAL -j MASQUERADE</li>
</ul></li>
<li>开启网桥的 hairpin 和 promisc 模式，允许 Pod 访问它自己所在的 Service IP（即通过 NAT 后再访问 Pod 自己）

<ul>
<li>-A OUTPUT -j KUBE-DEDUP</li>
<li>-A KUBE-DEDUP -p IPv4 -s a:58:a:f4:2:1 -o veth+ &ndash;ip-src 10.244.2.1 -j ACCEPT</li>
<li>-A KUBE-DEDUP -p IPv4 -s a:58:a:f4:2:1 -o veth+ &ndash;ip-src 10.244.2.0/24 -j DROP</li>
</ul></li>
<li>HostPort 管理以及设置端口映射</li>
<li>Traffic shaping，支持通过 kubernetes.io/ingress-bandwidth 和 kubernetes.io/egress-bandwidth 等 Annotation 设置 Pod 网络带宽限制</li>
</ul>

<p>未来 kubenet 插件会迁移到标准的 CNI 插件。</p>

<h3 id="flannel">flannel</h3>

<p>Flannel是CoreOS团队针对Kubernetes设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。</p>

<p>在默认的Docker配置中，每个节点上的Docker服务会分别负责所在节点容器的IP分配。这样导致的一个问题是，不同节点上容器可能获得相同的内外IP地址。并使这些容器之间能够之间通过IP地址相互找到，也就是相互ping通。</p>

<p>Flannel的设计目的就是为集群中的所有节点重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，并让属于不同节点上的容器能够直接通过内网IP通信。</p>

<p>Flannel实质上是一种“覆盖网络(overlaynetwork)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持udp、vxlan、host-gw、aws-vpc、gce和alloc路由等数据转发方式，默认的节点间数据通信方式是UDP转发。</p>

<h4 id="flannel网络模型">flannel网络模型</h4>

<p>1、互补冲突的ip</p>

<ul>
<li>flannel利用Kubernetes API或者etcd用于存储整个集群的网络配置，根据配置记录集群使用的网段。</li>
<li>flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配。</li>
</ul>

<p>我们可以查看/run/flannel/subnet.env文件来查看对于的IP分配，在不同的主机上进行分配。</p>

<p><img src="/media/cloud/k8s/flannel" alt="" /></p>

<p>在flannel network中，每个pod都会被分配唯一的ip地址，且每个K8s node的subnet各不重叠，没有交集，如下：</p>

<p><img src="/media/cloud/k8s/flannel1" alt="" /></p>

<p>2、pod间互相访问</p>

<ul>
<li>flanneld将本主机获取的subnet以及用于主机间通信的Public IP通过etcd存储起来，需要时发送给相应模块。</li>
<li>flannel利用各种backend mechanism，例如udp，vxlan等等，跨主机转发容器间的网络流量，完成容器间的跨主机通信。</li>
</ul>

<h4 id="flannel架构">flannel架构</h4>

<p>Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现，flannnel主要支持的后端实现</p>

<ul>
<li>UDP</li>
<li>VXLAN</li>
<li>host-gw</li>
</ul>

<h5 id="udp">UDP</h5>

<p>UDP 模式，是 Flannel 项目最早支持的一种方式，却也是性能最差的一种方式。所以，这个模式目前已经被弃用。</p>

<p><img src="/media/cloud/k8s/network.jpg" alt="" /></p>

<p>可以看到，Flannel UDP 模式提供的其实是一个三层的 Overlay 网络，即：它首先对发出端的 IP 包进行 UDP 封装，然后在接收端进行解封装拿到原始的 IP 包，进而把这个 IP 包转发给目标容器。这就好比，Flannel 在不同宿主机上的两个容器之间打通了一条“隧道”，使得这两个容器可以直接使用 IP 地址进行通信，而无需关心容器和宿主机的分布情况。</p>

<ul>
<li>首先容器发出包，因为该封包的目的地不在本主机subnet内，因此封包会首先通过网桥转发到主机中。</li>
<li>在主机上经过路由匹配，进入网卡flannel0。(需要注意的是flannel0是一个tun设备，它是一种工作在三层的虚拟网络设备，而flanneld是一个proxy，它会监听flannel0并转发流量。)</li>
<li>当封包进入flannel0时，flanneld就可以从flanne0中将封包读出，由于flanne0是三层设备，所以读出的封包仅仅包含IP层的报头及其负载。</li>
<li>最后flanneld会将获取的封包作为负载数据，通过udp socket发往目的主机。</li>
<li>在目的主机的flanneld会监听Public IP所在的设备，从中读取udp封包的负载，并将其放入flannel0设备内。</li>
<li>容器网络封包到达目的主机，之后就可以通过网桥转发到目的容器了。</li>
</ul>

<p>优点：Pod能够跨网段访问</p>

<p>缺点：隔离性不够，udp不能隔离两个网段，性能差。</p>

<blockquote>
<p>实例</p>
</blockquote>

<p>我有两台宿主机。</p>

<ul>
<li>宿主机 Node 1 上有一个容器 container-1，它的 IP 地址是 100.96.1.2，对应的 docker0 网桥的地址是：100.96.1.1/24。</li>
<li>宿主机 Node 2 上有一个容器 container-2，它的 IP 地址是 100.96.2.3，对应的 docker0 网桥的地址是：100.96.2.1/24。</li>
</ul>

<p>我们现在的任务，就是让 container-1 访问 container-2。</p>

<p>这种情况下，container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。</p>

<p>这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示：</p>

<pre><code># 在Node 1上
$ ip route
default via 10.168.0.1 dev eth0
100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.1.0
100.96.1.0/24 dev docker0  proto kernel  scope link  src 100.96.1.1
10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.2
</code></pre>

<p>可以看到，由于我们的 IP 包的目的地址是 100.96.2.3，它匹配不到本机 docker0 网桥对应的 100.96.1.0/24 网段，只能匹配到第二条、也就是 100.96.0.0/16 对应的这条路由规则，从而进入到一个叫作 flannel0 的设备中。</p>

<p>flannel0是一个 TUN 设备（Tunnel 设备）。在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：在操作系统内核和用户应用程序之间传递 IP 包。</p>

<p>像上面提到的情况，当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。这是一个从内核态（Linux 操作系统）向用户态（Flannel 进程）的流动方向。</p>

<p>所以，当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP 包。然后，flanneld 看到了这个 IP 包的目的地址，是 100.96.2.3，就把它发送给了 Node 2 宿主机。</p>

<p>flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3。</p>

<p>在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。在我们的例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示：</p>

<pre><code>$ etcdctl ls /coreos.com/network/subnets
/coreos.com/network/subnets/100.96.1.0-24
/coreos.com/network/subnets/100.96.2.0-24
/coreos.com/network/subnets/100.96.3.0-24
</code></pre>

<p>Docker Daemon 启动时配置如下所示的 bip 参数即可配置对应的子网</p>

<pre><code>$ FLANNEL_SUBNET=100.96.1.1/24
$ dockerd --bip=$FLANNEL_SUBNET ...
</code></pre>

<p>而对于 flanneld 来说，只要 Node 1 和 Node 2 是互通的，那么 flanneld 作为 Node 1 上的一个普通进程，就一定可以通过上述 IP 地址（10.168.0.3）访问到 Node 2，这没有任何问题。</p>

<p>所以说，flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。不难理解，这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址，其实就是宿主机之间的通信。</p>

<p>每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可。</p>

<p>到Node 2上就是反向的处理，由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥，然后到容器。</p>

<pre><code># 在Node 2上
$ ip route
default via 10.168.0.1 dev eth0
100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.2.0
100.96.2.0/24 dev docker0  proto kernel  scope link  src 100.96.2.1
10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.3
</code></pre>

<blockquote>
<p>性能问题</p>
</blockquote>

<p>实际上，相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示：</p>

<p><img src="/media/cloud/k8s/network1.png" alt="" /></p>

<p>我们可以看到：</p>

<ul>
<li>用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态；</li>
<li>IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程；</li>
<li>flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。</li>
</ul>

<p>此外，我们还可以看到，Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。在 Linux 操作系统中，上述这些上下文切换和用户态操作的代价其实是比较高的，这也正是造成 Flannel UDP 模式性能不好的主要原因。所以说，我们在进行系统级编程的时候，有一个非常重要的优化原则，<strong>就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行</strong>。这也是为什么，Flannel 后来支持的VXLAN 模式，逐渐成为了主流的容器网络方案的原因。</p>

<h5 id="vxlan">VXLAN</h5>

<p>VXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）。</p>

<p><img src="/media/cloud/k8s/network1.jpg" alt="" /></p>

<p>VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。</p>

<p>而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。</p>

<p>而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。</p>

<p>每台宿主机上名叫 flannel.1 的设备，就是 VXLAN 所需的 VTEP 设备，它既有 IP 地址，也有 MAC 地址。</p>

<p>优点：效率高</p>

<p>缺点：隔离差</p>

<blockquote>
<p>实例</p>
</blockquote>

<p>上图，我们的 container-1 的 IP 地址是 10.1.15.2，要访问的 container-2 的 IP 地址是 10.1.16.3。</p>

<p>与前面 UDP 模式的流程类似，当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，会先出现在 docker0 网桥，然后被路由到本机 flannel.1 设备进行处理。</p>

<p>为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出口，即：目的宿主机的 VTEP 设备。</p>

<p>而这个设备的信息，正是每台宿主机上的 flanneld 进程负责维护的。比如，当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则：</p>

<pre><code>$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
...
10.1.16.0       10.1.16.0       255.255.255.0   UG    0      0        0 flannel.1
</code></pre>

<p>这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。10.1.16.0 正是 Node 2 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址。</p>

<p>“源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”（当然，这么做还是因为这个 IP 包的目的地址不是本机）。</p>

<p>根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地址查询对应的二层 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示：</p>

<pre><code># 在Node 1上
$ ip neigh show dev flannel.1
10.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT
</code></pre>

<p>这条记录的意思非常明确，即：IP 地址 10.1.16.0，对应的 MAC 地址是 5e:f8:4f:00:e3:37。</p>

<p>可以看到，最新版本的 Flannel 并不依赖 L3 MISS 事件和 ARP 学习，而会在每台节点启动时把它的 VTEP 设备对应的 ARP 记录，直接下放到其他每台宿主机上。</p>

<p>有了这个“目的 VTEP 设备”的 MAC 地址，Linux 内核就可以开始二层封包工作了。这个二层帧的格式，如下所示：</p>

<p><img src="/media/cloud/k8s/network2.jpg" alt="" /></p>

<p>可以看到，Linux 内核会把“目的 VTEP 设备”的 MAC 地址，填写在图中的 Inner Ethernet Header 字段，得到一个二层数据帧。需要注意的是，上述封包过程只是加一个二层头，不会改变“原始 IP 包”的内容。所以图中的 Inner IP Header 字段，依然是 container-2 的 IP 地址，即 10.1.16.3。</p>

<p>但是，上面提到的这些 VTEP 设备的 MAC 地址，对于宿主机网络来说并没有什么实际意义。所以上面封装出来的这个数据帧，并不能在我们的宿主机二层网络里传输。为了方便叙述，我们把它称为“内部数据帧”（Inner Ethernet Frame）。所以接下来，Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，好让它“载着”“内部数据帧”，通过宿主机的 eth0 网卡进行传输。我们把这次要封装出来的、宿主机对应的数据帧称为“外部数据帧”（Outer Ethernet Frame）。为了实现这个“搭便车”的机制，Linux 内核会在“内部数据帧”前面，加上一个特殊的 VXLAN 头，用来表示这个“乘客”实际上是一个 VXLAN 要使用的数据帧。而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。</p>

<p>然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。所以，跟 UDP 模式类似，在宿主机看来，它会以为自己的 flannel.1 设备只是在向另外一台宿主机的 flannel.1 设备，发起了一次普通的 UDP 链接。它哪里会知道，这个 UDP 包里面，其实是一个完整的二层数据帧。</p>

<p>不过，不要忘了，一个 flannel.1 设备只知道另一端的 flannel.1 设备的 MAC 地址，却不知道对应的宿主机地址是什么。也就是说，这个 UDP 包该发给哪台宿主机呢？在这种场景下，flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。不难想到，这个 flannel.1“网桥”对应的 FDB 信息，也是 flanneld 进程负责维护的。它的内容可以通过 bridge fdb 命令查看到，如下所示：</p>

<pre><code># 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询
$ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:37
5e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent
</code></pre>

<p>可以看到，在上面这条 FDB 记录里，指定了这样一条规则，即：发往我们前面提到的“目的 VTEP 设备”（MAC 地址是 5e:f8:4f:00:e3:37）的二层数据帧，应该通过 flannel.1 设备，发往 IP 地址为 10.168.0.3 的主机。显然，这台主机正是 Node 2，UDP 包要发往的目的地就找到了。所以接下来的流程，就是一个正常的、宿主机网络上的封包工作。</p>

<p>接下来，Node 1 上的 flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去。显然，这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡。这时候，Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”，最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。</p>

<blockquote>
<p>数据格式</p>
</blockquote>

<p>我们知道，UDP 包是一个四层数据包，所以 Linux 内核会在它前面加上一个 IP 头，即原理图中的 Outer IP Header，组成一个 IP 包。并且，在这个 IP 头里，会填上前面通过 FDB 查询出来的目的主机的 IP 地址，即 Node 2 的 IP 地址 10.168.0.3。然后，Linux 内核再在这个 IP 包前面加上二层数据帧头，即原理图中的 Outer Ethernet Header，并把 Node 2 的 MAC 地址填进去。这个 MAC 地址本身，是 Node 1 的 ARP 表要学习的内容，无需 Flannel 维护。这时候，我们封装出来的“外部数据帧”的格式，如下所示：</p>

<p><img src="/media/cloud/k8s/network3.jpg" alt="" /></p>

<blockquote>
<p>问题</p>
</blockquote>

<p>如果集群的规模变大，按着每个节点之间都会建立一对vtep，规模会越来越大，性能会是很严重的问题，可以使用vxlan网关来解决，使用分布式vxlan网关也是现在比较流行的解决方案。</p>

<h5 id="host-gw">host-gw</h5>

<h5 id="总结">总结</h5>

<p>flannel更像是经典的桥接模式的扩展。我们知道，在桥接模式中，每台主机的容器都将使用一个默认的网段，容器与容器之间，主机与容器之间都能互相通信。要是，我们能手动配置每台主机的网段，使它们互不冲突。接着再想点办法，将目的地址为非本机容器的流量送到相应主机：如果集群的主机都在一个子网内，就搞一条路由转发过去；若是不在一个子网内，就搞一条隧道转发过去。这样以来，容器的跨网络通信问题就解决了。而flannel做的，其实就是将这些工作自动化了而已。</p>

<h3 id="weave">Weave</h3>

<p>Weave Net是一个多主机容器网络方案，支持去中心化的控制平面，各个host上的wRouter间通过建立Full Mesh的TCP链接，并通过Gossip来同步控制信息。这种方式省去了集中式的K/V Store，能够在一定程度上减低部署的复杂性，Weave将其称为“data centric”，而非RAFT或者Paxos的“algorithm centric”。</p>

<p>数据平面上，Weave通过UDP封装实现L2 Overlay，封装支持两种模式：</p>

<p>1、运行在user space的sleeve mode：通过pcap设备在Linux bridge上截获数据包并由wRouter完成UDP封装，支持对L2 traffic进行加密，还支持Partial Connection，但是性能损失明显。</p>

<p><img src="/media/cloud/k8s/weave" alt="" /></p>

<p>2、运行在kernal space的 fastpath mode：即通过OVS的odp封装VxLAN并完成转发，wRouter不直接参与转发，而是通过下发odp 流表的方式控制转发，这种方式可以明显地提升吞吐量，但是不支持加密等高级功能。</p>

<p><img src="/media/cloud/k8s/weave1" alt="" /></p>

<p>容器网络</p>

<p><img src="/media/cloud/k8s/weave2" alt="" /></p>

<ul>
<li>所有容器都连接到weave网桥</li>
<li>weave网桥通过veth pair连到内核的openvswitch模块</li>
<li>跨主机容器通过openvswitch vxlan通信</li>
<li>policy controller通过配置iptables规则为容器设置网络策略</li>
</ul>

<h3 id="calico">calico</h3>

<p>Calico 是一种容器之间互通的网络方案。在虚拟化平台中，比如 OpenStack、Docker 等都需要实现 workloads 之间互连，但同时也需要对容器做隔离控制，就像在 Internet 中的服务仅开放80端口、公有云的多租户一样，提供隔离和管控机制。而在多数的虚拟化平台实现中，通常都使用二层隔离技术来实现容器的网络，这些二层的技术有一些弊端，比如需要依赖 VLAN、bridge 和隧道等技术，其中 bridge 带来了复杂性，vlan 隔离和 tunnel 隧道则消耗更多的资源并对物理环境有要求，随着网络规模的增大，整体会变得越加复杂。我们尝试把 Host 当作 Internet 中的路由器，同样使用 BGP 同步路由，并使用 iptables 来做安全访问策略，最终设计出了 Calico 方案。</p>

<p>架构</p>

<p><img src="/media/cloud/k8s/calico" alt="" /></p>

<p>Calico网络模型主要工作组件：</p>

<ul>
<li>Felix：运行在每一台 Host 的 agent 进程，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。

<ul>
<li>Felix会监听ECTD中心的存储，从它获取事件，比如说用户在这台机器上加了一个IP，或者是创建了一个容器等。用户创建pod后，Felix负责将其网卡、IP、MAC都设置好，然后在内核的路由表里面写一条，注明这个IP应该到这张网卡。同样如果用户制定了隔离策略，Felix同样会将该策略创建到ACL中，以实现隔离。</li>
</ul></li>
<li>etcd：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用；</li>
<li>BGP Client（BIRD）：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。

<ul>
<li>BIRD是一个标准的路由程序，它会从内核里面获取哪一些IP的路由发生了变化，然后通过标准BGP的路由协议扩散到整个其他的宿主机上，让外界都知道这个IP在这里，你们路由的时候得到这里来。</li>
</ul></li>
<li>BGP Route Reflector：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。</li>
</ul>

<p>由于Calico是一种纯三层的实现，因此可以避免与二层方案相关的数据包封装的操作，中间没有任何的NAT，没有任何的overlay，所以它的转发效率可能是所有方案中最高的，因为它的包直接走原生TCP/IP的协议栈，它的隔离也因为这个栈而变得好做。因为TCP/IP的协议栈提供了一整套的防火墙的规则，所以它可以通过IPTABLES的规则达到比较复杂的隔离逻辑。</p>

<blockquote>
<p>网络模式</p>
</blockquote>

<p>1、IPIP</p>

<p>从字面来理解，就是把一个IP数据包又套在一个IP包里，即把 IP 层封装到 IP 层的一个 tunnel。它的作用其实基本上就相当于一个基于IP层的网桥！一般来说，普通的网桥是基于mac层的，根本不需 IP，而这个 ipip 则是通过两端的路由做一个 tunnel，把两个本来不通的网络通过点对点连接起来。</p>

<p><img src="/media/cloud/k8s/calico1" alt="" /></p>

<p>其实这还是一种Overlay模式，需要使用把 IP 层封装到 IP 层的一个 tunnel。在主机的路由上route -n可以看到这个通道tunX。</p>

<p>2、BGP</p>

<p>在安装calico网络时，默认安装是IPIP网络。calico.yaml文件中，将CALICO_IPV4POOL_IPIP的值修改成 &ldquo;off&rdquo;，就能够替换成BGP网络。</p>

<p>BGP网络相比较IPIP网络，最大的不同之处就是没有了隧道设备 tunl0。 前面介绍过IPIP网络pod之间的流量发送tunl0，然后tunl0发送对端设备。BGP网络中，pod之间的流量直接从网卡发送目的地，减少了tunl0这个环节。</p>

<p><img src="/media/cloud/k8s/calico2" alt="" /></p>

<p>总结对比</p>

<p>IPIP网络：</p>

<ul>
<li>流量：tunlo设备封装数据，形成隧道，承载流量。</li>
<li>适用网络类型：适用于互相访问的pod不在同一个网段中，跨网段访问的场景。外层封装的ip能够解决跨网段的路由问题。</li>
<li>效率：流量需要tunl0设备封装，效率略低</li>
</ul>

<p>BGP网络：</p>

<ul>
<li>流量：使用路由信息导向流量</li>
<li>适用网络类型：适用于互相访问的pod在同一个网段，适用于大型网络。</li>
<li>效率：原生hostGW，效率高</li>
</ul>

<p>缺点</p>

<ul>
<li>租户隔离问题：Calico 的三层方案是直接在 host 上进行路由寻址，那么对于多租户如果使用同一个 CIDR 网络就面临着地址冲突的问题。</li>
<li>路由规模问题：通过路由规则可以看出，路由规模和 pod 分布有关，如果 pod离散分布在 host 集群中，势必会产生较多的路由项。</li>
<li>iptables 规则规模问题：1台 Host 上可能虚拟化十几或几十个容器实例，过多的 iptables 规则造成复杂性和不可调试性，同时也存在性能损耗。</li>
<li>跨子网时的网关路由问题：当对端网络不为二层可达时，需要通过三层路由机时，需要网关支持自定义路由配置，即 pod 的目的地址为本网段的网关地址，再由网关进行跨三层转发。</li>
</ul>

<h3 id="contiv">contiv</h3>

<p>Contiv是思科开源的容器网络方案，是一个用于跨虚拟机、裸机、公有云或私有云的异构容器部署的开源容器网络架构，并与主流容器编排系统集成。Contiv最主要的优势是直接提供了多租户网络，并支持L2(VLAN), L3(BGP), Overlay (VXLAN)以及思科自家的ACI。</p>

<h3 id="其他">其他</h3>

<p>还有很多的网络插件，基本思路都是基于overlay的隧道，或者基于underlay的路由，基本上插件都能支持这两个功能，只是在那个功能上做的更加好，并且还带有其他的辅助功能。其实个人觉得核心解决的问题就是如何进行跨主机网络路由。</p>
            </div>
            
            <div style="border: 1px dashed #e0e0e0; margin-bottom: 15px; padding: 10px 10px 10px 10px; background-color: #fffeee; background-repeat: no-repeat; background-attachment: scroll; background-position: 1% 50%; -moz-background-size: auto auto; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial;">
                <div>
                    <p style="margin-top:0px;">作者：<a target="_blank" href="https://kingjcy.github.io/">kingjcy</a>
                    <br />本文出处：<a target="_blank" href="https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network/">https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-network/</a>
                    <br />
                    文章版权归本人所有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接，否则保留追究法律责任的权利。 </p>
                </div>
            </div>

            <aside>
                
                <ul class="list-inline post-tags">
                    
                    <li>
                        <a href="/tags/cloudnative/">
                            <i class="fa fa-tags"></i>
                            cloudnative
                        </a>
                    </li>
                    
                    <li>
                        <a href="/tags/paas/">
                            <i class="fa fa-tags"></i>
                            paas
                        </a>
                    </li>
                    
                    <li>
                        <a href="/tags/k8s/">
                            <i class="fa fa-tags"></i>
                            k8s
                        </a>
                    </li>
                    
                    <li>
                        <a href="/tags/base/">
                            <i class="fa fa-tags"></i>
                            base
                        </a>
                    </li>
                    
                    <li>
                        <a href="/tags/network/">
                            <i class="fa fa-tags"></i>
                            network
                        </a>
                    </li>
                    
                </ul>

                
                
                <h4 id="real-rels">相关文章</h4>
                <ul class="post-rels" id="real-rels"><li id="li-rels"><a href="/post/cloud/paas/base/kubernetes/k8s-autoscaler/">云计算K8s系列---- K8s autoscaler</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2021年02月04日)</span></li><li id="li-rels"><a href="/post/cloud/paas/base/kubernetes/k8s-kruise/">云计算K8s系列---- kruise</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2021年01月17日)</span></li><li id="li-rels"><a href="/post/cloud/paas/base/kubernetes/k8s-network-cni/">云计算K8s系列---- 网络CNI</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2021年01月17日)</span></li><li id="li-rels"><a href="/post/cloud/paas/base/kubernetes/k8s-controller/">云计算K8s系列---- K8s controller</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2020年11月24日)</span></li><li id="li-rels"><a href="/post/cloud/paas/base/kubernetes/k8s-apiserver/">云计算K8s组件系列（一）---- K8s apiserver 详解</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2020年10月24日)</span></li><li id="li-rels"><a href="/post/cloud/paas/base/kubernetes/k8s-kubelet/">云计算K8s组件系列（四）---- K8s kubelet 详解</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2020年10月20日)</span></li><li id="li-rels"><a href="/post/cloud/paas/base/docker/docker-network/">云计算容器系列---- Docker network</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2020年10月14日)</span></li><li id="li-rels"><a href="/post/computerbase/network/http/">计算机网络系列（八）---- Http</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2020年10月13日)</span></li><li id="li-rels"><a href="/post/cloud/paas/base/kubernetes/k8s-proxy/">云计算K8s组件系列（五）---- K8s proxy 详解</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2020年10月13日)</span></li><li id="li-rels"><a href="/post/cloud/paas/base/kubernetes/k8s-scheduler/">云计算K8s组件系列（二）---- K8s scheduler 详解</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2020年09月24日)</span></li></ul>
            </aside>
                
            
            <footer>
                <nav>
                    <ul class="pager">

                        
                        <li class="previous"><a href="/post/cloud/paas/base/docker/docker-principle/"><span aria-hidden="true">&larr;</span> Prev</a></li>
                        

                        <li><a href="/post/">All Posts</a></li>

                        
                        <li class="next"><a href="/post/distributed/distributed-event/">Next <span aria-hidden="true">&rarr;</span></a></li>
                        

                    </ul>
                </nav>
            </footer>

        </article>
    </div>
    <div class="col-md-4">
        
<aside>
        <div class="toc panel panel-default hidden-xs hidden-sm affix-top" data-spy="affix" data-offset-top="125" data-offset-bottom="300">
            <div class="panel-heading">
                <h2 class="panel-title">Catalog</h2>
            </div>

            <nav id="TableOfContents">
<ul>
<li><a href="#docker网络实现">docker网络实现</a></li>
<li><a href="#kubernetes网络模型">Kubernetes网络模型</a></li>
<li><a href="#pod-network">Pod Network</a>
<ul>
<li><a href="#pod内网络">pod内网络</a></li>
<li><a href="#pod间通信">pod间通信</a>
<ul>
<li><a href="#节点内通信">节点内通信</a></li>
<li><a href="#节点间通信">节点间通信</a></li>
</ul></li>
</ul></li>
<li><a href="#node-network">Node network</a></li>
<li><a href="#service-network">Service Network</a></li>
<li><a href="#kubernetes网络开源组件">Kubernetes网络开源组件</a>
<ul>
<li><a href="#基本概念">基本概念</a></li>
<li><a href="#容器网络模型">容器网络模型</a>
<ul>
<li><a href="#cnm">CNM</a></li>
<li><a href="#cni">CNI</a></li>
</ul></li>
<li><a href="#容器网络方案">容器网络方案</a></li>
<li><a href="#组件">组件</a>
<ul>
<li><a href="#kubenet">kubenet</a></li>
<li><a href="#flannel">flannel</a>
<ul>
<li><a href="#flannel网络模型">flannel网络模型</a></li>
<li><a href="#flannel架构">flannel架构</a>
<ul>
<li><a href="#udp">UDP</a></li>
<li><a href="#vxlan">VXLAN</a></li>
<li><a href="#host-gw">host-gw</a></li>
<li><a href="#总结">总结</a></li>
</ul></li>
</ul></li>
<li><a href="#weave">Weave</a></li>
<li><a href="#calico">calico</a></li>
<li><a href="#contiv">contiv</a></li>
<li><a href="#其他">其他</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
        </div>
</aside>

    </div>
</div>

</div>
<hr>

<footer class="container copy">
    <p>&copy; 2021  kingjcy blog </p>
	<p>Powered by <a href="https://gohugo.io" target="_blank">Hugo</a></p>
</footer>

<script>hljs.initHighlightingOnLoad();</script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?ace3ec99de96c4080ead1eb8d52db3b3";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92600390-2', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>


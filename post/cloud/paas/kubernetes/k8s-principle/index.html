<!DOCTYPE html>

<html lang="zh-cn">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="author" content="fatedier">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="description" content="kubernetes是一种以容器为核心的，自动化部署应用程序的分布式的容器管理系统。

直白的说，Kubernetes用户可以通过编写一个yaml或者json格式的配置文件，也可以通过工具/代码生成或直接请求kubernetes API创建应用，该配置文件中包含了用户想要应用程序保持的状态，不论整个kubernetes集群中的个别主机发生什么问题，都不会影响应用程序的状态，你还可以通过改变该配置文件或请求kubernetes API来改变应用程序的状态。">
<meta property="og:url" content="https://kingjcy.github.io/"><meta property="og:type" content="article">
<meta property="og:title" content="K8s系列---- K8s Principle - kingjcy blog"><meta property="og:site_name" content="kingjcy blog">

<title>
    
    K8s系列---- K8s Principle
    
</title>

<link rel="stylesheet" href="/onlyone/onlyone.css">
<link rel="shortcut icon" href="/assets/favicon.ico">
<script src="/onlyone/onlyone.js"></script>
<link rel="alternate" type="application/rss+xml" title="RSS" href="/index.xml">
</head>
<body>


<div class="container">
    <header class="nav">
        <nav class="navbar navbar-default">
            <div class="container-fluid">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/">kingjcy blog</a>
                </div>

                <div class="collapse navbar-collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav">
                        <li><a href="/categories/技术文章/">技术文章</a></li>
                        <li><a href="/categories/读书笔记/">读书笔记</a></li>
                        <li><a href="/categories/人生感悟/">人生感悟</a></li>
                        <li><a href="/tags/">分类</a></li>
                        <li><a href="/about/">关于我</a></li>
                        <li>
                            <form method="get" style="padding: 8px" action="https://www.google.com/search" target="_blank">
                                <input type="hidden" name="sitesearch" value="kingjcy.github.io"/>
                                <input type="text" class="form-control" name="q" placeholder="Press enter to search">
                            </form>
                        </li>
                    </ul>

                </div>
            </div>
        </nav>
    </header>


<div class="row">
    <div class="col-md-8">
        <article class="post single">

            <header>
                <div class="post-date">
                    2016年11月24日 
                </div>
                <h1 class="post-title">K8s系列---- K8s Principle</h1>
            </header>

            <div class="post-content">
                <p>kubernetes是一种以容器为核心的，自动化部署应用程序的分布式的容器管理系统。</p>

<p>直白的说，Kubernetes用户可以通过编写一个yaml或者json格式的配置文件，也可以通过工具/代码生成或直接请求kubernetes API创建应用，该配置文件中包含了用户想要应用程序保持的状态，不论整个kubernetes集群中的个别主机发生什么问题，都不会影响应用程序的状态，你还可以通过改变该配置文件或请求kubernetes API来改变应用程序的状态。</p>

<h1 id="架构">架构</h1>

<p><img src="/media/cloud/k8s/k8s1.png" alt="" /></p>

<blockquote>
<h2 id="核心组件">核心组件</h2>

<p>master</p>
</blockquote>

<pre><code>apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；可以说是k8s的内部交互的网关总线。
controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；是控制器的管理者，负责很多的控制器。
scheduler负责资源的调度，按照（预选优选）调度策略将Pod调度到相应的机器上；
</code></pre>

<blockquote>
<p>node</p>
</blockquote>

<pre><code>kubelet负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；主要干活的对象，负责和docker进行交互，创建容器，维护容器。
kube-proxy负责为Service提供cluster内部的服务发现和负载均衡；提供了一种网络模式。
docker engine docker引擎，负责docker的创建和管理。
</code></pre>

<blockquote>
<p>数据库</p>
</blockquote>

<pre><code>etcd保存了整个集群的状态；
</code></pre>

<blockquote>
<p>网络</p>
</blockquote>

<pre><code>fannel实现pod网络的互通
</code></pre>

<p>除了核心组件，还有一些推荐的插件Add-ons：</p>

<pre><code>kube-dns负责为整个集群提供DNS服务，主要用于解决igress的负载均衡策略，如何找到相关的容器。从 Kubernetes v1.12 开始，CoreDNS 是推荐的 DNS 服务器，取代了kube-dns。
Ingress Controller为服务提供外网入口，负载均衡。
kube-state-metrics提供资源监控，主要是状态。
Dashboard提供GUI，友好的界面。
Federation,thanos,vm提供集群模式。
</code></pre>

<blockquote>
<h2 id="分层架构">分层架构</h2>
</blockquote>

<p><img src="/media/cloud/k8s/k8s2.png" alt="" /></p>

<p>1、基础设施层：container runtime、网络、存储等</p>

<p>2、核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境。</p>

<p>3、应用层：部署（无状态、有状态应用、Job等）和路由（服务发现、负载均衡等）</p>

<p>4、管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等）</p>

<p>5、接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦</p>

<p>6、生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴。</p>

<pre><code>Kubernetes 外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps 等
Kubernetes 内部：CRI、CNI、CSI、镜像仓库、Cloud Provider、集群自身的配置和管理等。
</code></pre>

<hr />

<h1 id="详细说明">详细说明</h1>

<p>kubernetes架构是master/node，下面我们对每个节点上的组件进行了解</p>

<blockquote>
<h2 id="master">master</h2>
</blockquote>

<p><a href="/posts/cloud/container/kubernetes/k8s-apiserver/">k8s组件系列（一）&mdash;- apiserver详解</a></p>

<p><a href="/posts/cloud/container/kubernetes/k8s-scheduler/">k8s组件系列（二）&mdash;- scheduler详解</a></p>

<p><a href="/posts/cloud/container/kubernetes/k8s-controller-manager/">k8s组件系列（三）&mdash;- controller-manager详解</a></p>

<blockquote>
<h2 id="node组件">node组件</h2>
</blockquote>

<p><a href="/posts/cloud/container/kubernetes/k8s-kubelet/">k8s组件系列（四）&mdash;- kubelet详解</a></p>

<p><a href="/posts/cloud/container/kubernetes/k8s-proxy/">k8s组件系列（五）&mdash;- proxy详解</a></p>

<p><a href="/posts/cloud/container/docker/docker">k8s组件系列（六）&mdash;- docker详解</a></p>

<hr />

<h1 id="基本概念">基本概念</h1>

<h2 id="pod">pod</h2>

<blockquote>
<p>pod</p>
</blockquote>

<p>1、为什么需要pod？pause父容器是做什么的？</p>

<p>Pod的概念，主要是在多个容器中，隐藏了docker中复杂的标志位以及管理docker容器、共享卷及其他docker资源的复杂性。同时也隐藏了不同容器运行环境的差异。</p>

<p>原则上，任何人只需要创建一个父容器就可以配置docker来管理容器组之间的共享问题。这个父容器需要能够准确的知道如何去创建共享运行环境的容器，还能管理这些容器的生命周期。为了实现这个父容器的构想，kubernetes中，用pause容器来作为一个pod中所有容器的父容器。这个pause容器有两个核心的功能：</p>

<pre><code>第一，它提供整个pod的Linux命名空间的基础。业务容器共享pause的ip（pod_ip）和挂载的volume（存储空间）
第二，启用PID命名空间，它在每个pod中都作为PID为1进程，并回收僵尸进程。
</code></pre>

<p><img src="/media/cloud/k8s/pod.png" alt="" />
<img src="/media/cloud/k8s/pod2.png" alt="" /></p>

<p>2、pod是什么？</p>

<p>pod是kubernetes定义的一种操作单位。连接了容器和管理。</p>

<p>pod可以用一个或者多个容器组成。</p>

<pre><code>多个容器（sidecar模式）：在一个Pod中同时运行多个容器。一个Pod中也可以同时封装几个需要紧密耦合互相协作的容器，它们之间共享资源。这些在同一个Pod中的容器可以互相协作成为一个service单位——一个容器共享文件，另一个“sidecar”容器来更新这些文件。Pod将这些容器的存储资源作为一个实体来管理。比如：一种应用的实例，例如一个web站点，包含前端，后端，数据库这三个容器，放在一个pod中对外就是一个web服务。
单个容器：一个Pod中运行一个容器。“每个Pod中一个容器”的模式是最常见的用法；在这种使用方式中，你可以把Pod想象成是单个容器的封装，kuberentes管理的是Pod而不是直接管理容器。
</code></pre>

<p>pod内部共享资源</p>

<ol>
<li><p>Pod中可以共享两种资源：网络和存储（挂载的volume）。</p></li>

<li><p>Pod内部的容器可以使用localhost互相通信。</p></li>

<li><p>可以Pod指定多个共享的Volume。Pod中的所有容器都可以访问共享的volume。Volume也可以用来持久化Pod中的存储资源，以防容器重启后文件丢失。</p></li>
</ol>

<p>3、pod的特性</p>

<ol>
<li><p>你很少会直接在kubernetes中创建单个Pod。因为Pod的生命周期是短暂的，用后即焚的实体。</p></li>

<li><p>Controller可以创建和管理多个Pod，提供副本管理、滚动升级和集群级别的自愈能力。例如，如果一个Node故障，Controller就能自动将该节点上的Pod调度到其他健康的Node上。</p></li>
</ol>

<blockquote>
<p>状态</p>
</blockquote>

<p>pod的五种状态</p>

<pre><code>pending pod已经创建，但是内部镜像还没有完全创建
running 容器已经创建，至少有一个容器处于运行状态
succeeded  pod内容器都成功终止，且不会重启
failed  所有容器已经退出，至少有一个是因为发生错误而退出
Unkown：由于某中原因apiserver无法获取到Pod的状态。通常是由于Master与pod所在的主机失去连接了。
</code></pre>

<p>还有一下其他的状态</p>

<pre><code>CrashLoopBackOff： 容器退出，kubelet正在将它重启
InvalidImageName： 无法解析镜像名称
ImageInspectError： 无法校验镜像
ErrImageNeverPull： 策略禁止拉取镜像
ImagePullBackOff： 正在重试拉取
RegistryUnavailable： 连接不到镜像中心
ErrImagePull： 通用的拉取镜像出错
CreateContainerConfigError： 不能创建kubelet使用的容器配置
CreateContainerError： 创建容器失败
m.internalLifecycle.PreStartContainer  执行hook报错
RunContainerError： 启动容器失败
PostStartHookError： 执行hook报错
ContainersNotInitialized： 容器没有初始化完毕
ContainersNotReady： 容器没有准备完毕
ContainerCreating：容器创建中
PodInitializing：pod 初始化中
DockerDaemonNotReady：docker还没有完全启动
NetworkPluginNotReady： 网络插件还没有完全启动
Terminating： 退出中
</code></pre>

<blockquote>
<p>重启策略</p>
</blockquote>

<p>pod的重启策略：kubelet将根据RestartPolicy的设置来进行相应的操作</p>

<pre><code>Always: 当容器失效时, 由kubelet自动重启该容器
OnFailure: 当容器终止运行且退出码不为0时, 由kubelet自动重启该容器
Never: 不论容器运行状态如何, kubelet都不会重启该容器
</code></pre>

<blockquote>
<p>镜像拉取策略</p>
</blockquote>

<p>支持三种ImagePullPolicy</p>

<pre><code>Always：不管镜像是否存在都会进行一次拉取。
Never：不管镜像是否存在都不会进行拉取
IfNotPresent：只有镜像不存在时，才会进行镜像拉取。
</code></pre>

<p>注意：</p>

<pre><code>默认为IfNotPresent，但:latest标签的镜像默认为Always。
拉取镜像时docker会进行校验，如果镜像中的MD5码没有变，则不会拉取镜像数据。
生产环境中应该尽量避免使用:latest标签，而开发环境中可以借助:latest标签自动拉取最新的镜像。
</code></pre>

<blockquote>
<p>资源限制</p>
</blockquote>

<pre><code>spec.containers[].resources.limits.cpu：CPU上限，可以短暂超过，容器也不会被停止
spec.containers[].resources.limits.memory：内存上限，不可以超过；如果超过，容器可能会被停止或调度到其他资源充足的机器上
spec.containers[].resources.requests.cpu：CPU请求，可以超过
spec.containers[].resources.requests.memory：内存请求，可以超过；但如果超过，容器可能会在Node内存不足时清理
</code></pre>

<p>cpu是以千分之一c为最小单位m，一般设置0.3核就是300m</p>

<p>limit是上限，如果应用超过limit，会kill掉</p>

<p>request给调度用的  调度在选择pod调度到那个node上，会看node上已经调度的pod所声明的request。你如果设置最小需求，那么你node上可能会被调度很多pod, 但是业务大的时候，会使得node的压力比较大，所以设置为正常的时候的需求，request不应该是一个下线值，而是一个运行参考值。</p>

<blockquote>
<p>钩子</p>
</blockquote>

<p>容器生命周期钩子（Container Lifecycle Hooks）监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数。支持两种钩子：</p>

<pre><code>postStart： 容器启动后执行，注意由于是异步执行，它无法保证一定在ENTRYPOINT之后运行。如果失败，容器会被杀死，并根据RestartPolicy决定是否重启
preStop：容器停止前执行，常用于资源清理。如果失败，容器同样也会被杀死
</code></pre>

<p>而钩子的回调函数支持两种方式：</p>

<pre><code>exec：在容器内执行命令
httpGet：向指定URL发起GET请求
</code></pre>

<p>postStart和preStop钩子示例：</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;]
      preStop:
        exec:
          command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]
</code></pre>

<blockquote>
<p>控制器</p>
</blockquote>

<p>pod只是运行的最小单元，我们一般不会直接使用pod。大部分情况下我们都是使用deployment（RS），deamonset，statefulset，job等控制器来完成部署调度使用。</p>

<p><img src="/media/cloud/k8s/pod3.png" alt="" /></p>

<p>在这边做一个简单的说明，每一种控制器都有详细的说明。</p>

<p>1、deploymemnt</p>

<p>deployment是基于rs的基础之上的，根据调度算法，选择合适的node，维持pod的数量，还可以指定调度和优化调度等。一般用于k8s无状态的应用。</p>

<ul>
<li><p>指定node(nodeselector)</p>

<p>通过nodeSelector，一个Pod可以指定它所想要运行的Node节点。</p>

<p>首先给Node加上标签：</p>

<pre><code>kubectl label nodes &lt;your-node-name&gt; disktype=ssd
</code></pre>

<p>接着，指定该Pod只想运行在带有disktype=ssd标签的Node上：</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
</code></pre></li>

<li><p>亲和度调度（nodeaffinity）</p>

<p>目前主要的node affinity：</p>

<pre><code>requiredDuringSchedulingIgnoredDuringExecution
表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。

requiredDuringSchedulingRequiredDuringExecution
表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中RequiredDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，则重新选择符合要求的节点。

preferredDuringSchedulingIgnoredDuringExecution
表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。

preferredDuringSchedulingRequiredDuringExecution
表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中RequiredDuringExecution表示如果后面节点标签发生了变化，满足了条件，则重新调度到满足条件的节点。
</code></pre>

<p>实例</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: gcr.io/google_containers/pause:2.0
</code></pre>

<p>这个 pod 同时定义了 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 两种 nodeAffinity。第一个要求 pod 运行在特定 AZ 的节点上，第二个希望节点最好有对应的 another-node-label-key:another-node-label-value 标签。</p>

<p>这里的匹配逻辑是label在某个列表中，可选的操作符有：</p>

<pre><code>In: label的值在某个列表中
NotIn：label的值不在某个列表中
Exists：某个label存在
DoesNotExist：某个label不存在
Gt：label的值大于某个值（字符串比较）
Lt：label的值小于某个值（字符串比较）
</code></pre>

<p>如果nodeAffinity中nodeSelector有多个选项，节点满足任何一个条件即可；如果matchExpressions有多个选项，则节点必须同时满足这些选项才能运行pod 。</p></li>

<li><p>不被调度</p>

<p>Taints 和 tolerations</p>

<p>Taints 和 tolerations 用于保证 Pod 不被调度到不合适的 Node 上，Taint 应用于 Node 上，而 toleration 则应用于 Pod 上（Toleration 是可选的）。</p>

<pre><code>比如，可以使用 taint 命令给 node1 添加 taints：
kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value2:NoExecute
</code></pre>

<p>实例</p>

<pre><code>[root@kube-manager01 rebuild]# kubectl cordon  kube-node02

node/kube-node02 already cordoned
</code></pre>

<p>通过这个命令会使得node那个字段被设置成node转维护状态，不会被调度新pod</p></li>
</ul>

<p>2、daemonset</p>

<p>DaemonSet保证在每个Node上都运行一个容器副本，常用来部署一些集群的日志、监控或者其他系统管理应用。典型的应用包括：</p>

<pre><code>日志收集，比如fluentd，logstash等
系统监控，比如Prometheus Node Exporter，collectd，New Relic agent，Ganglia gmond等
系统程序，比如kube-proxy, kube-dns, glusterd, ceph等
</code></pre>

<p>daemonset的调度算法和deploymemt基本是差不多的，只不过它会调度到所有的节点上，当然也可以使用nodeselector或者nodeaffinity来进行范围性的调度。</p>

<p>3、job</p>

<p>批处理调度，下面有详述。</p>

<p>4、statefulset</p>

<p>有状态的部署，下面详述。</p>

<blockquote>
<p>扩缩容</p>
</blockquote>

<p>主要是指我们无状态的服务的扩缩容，一般无状态的服务都是使用deployment来部署的，可以直接使用scale来指定副本的数量&ndash;replicas的方式来完成扩缩容。</p>

<p>这是传统的扩缩容，其实都需要手动，如果kubernetes可以通过当时容器使用情况来自动的扩缩容，其实有的可以进行预知，有的根本就是不确定的，纯手工去做也是不现实的人海战术。</p>

<p>1、HPA</p>

<p><img src="/media/cloud/k8s/hpa" alt="" /></p>

<p>Horizontal Pod Autoscaling，简称HPA, Kubernetes通过HPA的设定，实现了容器的弹性伸缩功能。对于Kubernetes中的POD集群来说，HPA可以实现很多自动化功能，比如当POD中业务负载上升的时候，可以创建新的POD来保证业务系统稳定运行，当POD中业务负载下降的时候，可以销毁POD来减少资源的浪费。当前的弹性伸缩的指标包括：CPU，内存，并发数，包传输大小。HPA控制器默认每隔30秒就会运行一次，一旦创建的HPA，我们就可以通过命令查看获取到的当前指标信息。</p>

<p>HPA原来是k8s下面单独的一个项目，现在已经独立在github上了。</p>

<p>首先hpa要创建一个规则，就像我们之前创建ingress的规则一样，里面定义好一个扩容缩容的一个范围然后指定好对象，指定好它的预值，hpa本身就是一个控制器，循环的控制器，它会不断的从metrics server 中去获取这个指标，判断这个预值是不是到达你设置规则的预值，如果是的话，就会去执行这个scale帮你扩容这个副本，如果长期处于一个低使用率的情况下，它会帮你缩容这个副本，这个metrics server的资源来源是来自于cadvisor去拿的，想一下cadvisor可以提供那些指标，hpa可以拿到的，比如cpu,内存的使用率，主要采集你这些的利用率，所以hpa在早期已经支持了对CPU的弹性伸缩</p>

<p>Hpa就是k8s中这个pod水平扩容的一个控制器，但是要实现Pod的扩容，他需要一定的条件，他要拿一定的指标，这里是有预值的，他要判断你的指标，是不是超出这个预值，对你进行缩容扩容，所以要想得到这个指标，你还需要装一个组件，metrics server,在之前呢这个组件的实现是由heapster heapstar现在已经是慢慢弃用了，基本上不怎么去使用heapstat了，所以metrics server来提供这些数据，提供这些资源的利用率。</p>

<p>kubernetesv1.11以后不再支持通过heaspter采集监控数据，支持新的监控数据采集组件metrics-server，比heaspter轻量很多，也不做数据的持久化存储，提供实时的监控数据查询还是很好用的。</p>

<p>metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。从 Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。替代方案如下：</p>

<ol>
<li>用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server；</li>
<li>通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator；</li>
<li>事件传输：使用第三方工具来传输、归档 kubernetes events；</li>
</ol>

<p><strong>实战</strong></p>

<p>基于cpu&ndash;&gt;cadvisor</p>

<p>1、生成HPA控制器</p>

<p>运行这个hpa-example，请求CPU的资源为200m，暴露一个80端口</p>

<pre><code>[root@master ~]# kubectl run php-apache --image=mirrorgooglecontainers/hpa-example --requests=cpu=200m --expose --port=80
</code></pre>

<p>生成了一个HPA的控制器，用于控制自动扩缩容，当deployment资源对象的CPU使用率达到50%时，就进行扩容，最多可以扩容到10个</p>

<pre><code>[root@master ~]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
</code></pre>

<p>查看</p>

<pre><code>[root@master ~]# kubectl get svc | grep php-apache   #查看php-apache对应的svc群集IP
php-apache   ClusterIP   10.99.60.48   &lt;none&gt;        80/TCP    15m
[root@master ~]# kubectl get pod | grep php-apa    #确定当前的pod数量
php-apache-867f97c8cb-vw74k      1/1     Running   0          17m
</code></pre>

<p>2、模拟消耗php-apache的资源，并验证pod是否会自动扩容与缩容</p>

<p>新开启多个终端，对pod进行死循环请求，如下（如果你的系统资源比较充足，可以选择开启多个终端，对pod进行死循环请求，我这里开启了三个终端，同时请求php-apache的pod）：</p>

<pre><code>[root@master ~]# while true; do wget -q -O- 10.99.60.48; done   #每个终端都执行此命令
[root@master ~]# kubectl top pod      #可以通过此命令随时查看pod的负载情况
NAME                             CPU(cores)   MEMORY(bytes)
load-generator-7d549cd44-xm98c   0m           1Mi
php-apache-867f97c8cb-vw74k      208m         20Mi
</code></pre>

<p>也可以不定时的查看pod的数量是否有变化，可以发现php-apache的pod数量变成了10个，并且不会再增加，因为在上面的命令对其限制了最大数</p>

<pre><code>[root@master ~]# kubectl get pod    #在运行死循环请求一段时间后，查看pod数量



NAME                             READY   STATUS              RESTARTS   AGE
load-generator-7d549cd44-xm98c   1/1     Running             1          25m
php-apache-867f97c8cb-4r6sk      1/1     Running             0          19m
php-apache-867f97c8cb-4rcpk      1/1     Running             0          13m
php-apache-867f97c8cb-5pbxf      1/1     Running             0          16m
php-apache-867f97c8cb-8htth      1/1     Running             0          13m
php-apache-867f97c8cb-d94h9      0/1     ContainerCreating   0          13m
php-apache-867f97c8cb-drh52      1/1     Running             0          18m
php-apache-867f97c8cb-f67bs      0/1     ContainerCreating   0          17m
php-apache-867f97c8cb-nxc2r      1/1     Running             0          19m
php-apache-867f97c8cb-vw74k      1/1     Running             0          39m
php-apache-867f97c8cb-wb6l5      0/1     ContainerCreating   0          15m
</code></pre>

<p>当停止死循环请求后，也并不会立即减少pod数量，会等一段时间后减少pod数量，防止流量再次激增。至此，pod副本数量的自动扩缩容就实现了。扩容默认3m，所容默认持续5m</p>

<p>基于prometheus</p>

<blockquote>
<p>限制带宽</p>
</blockquote>

<p>可以通过给Pod增加kubernetes.io/ingress-bandwidth和kubernetes.io/egress-bandwidth这两个annotation来限制Pod的网络带宽</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: qos
  annotations:
    kubernetes.io/ingress-bandwidth: 3M
    kubernetes.io/egress-bandwidth: 4M
spec:
  containers:
  - name: iperf3
    image: networkstatic/iperf3
    command:
    - iperf3
    - -s
</code></pre>

<blockquote>
<p>yaml模版</p>
</blockquote>

<p>其实就是我们声明式操作的资源配置清单，具体参数详解可以查看k8s权威指南的第二章第四节。</p>

<blockquote>
<p>pod的配置管理</p>
</blockquote>

<p>configmap，下面有详解的使用说明</p>

<blockquote>
<p>健康检查</p>
</blockquote>

<p>Kubelet使用liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去（谁的程序还没几个bug呢）。</p>

<p>Kubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态。该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。</p>

<p>一、探测</p>

<p><strong>liveness probe</strong></p>

<p>两种探测都有三种方式</p>

<p>1、基于命令的探测</p>

<p>实例</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    image: gcr.io/google_containers/busybox
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
</code></pre>

<p>该配置文件给Pod配置了一个容器。periodSeconds 规定kubelet要每隔5秒执行一次liveness probe。 initialDelaySeconds 告诉kubelet在第一次执行probe之前要的等待5秒钟。探针检测命令是在容器中执行 cat /tmp/healthy 命令。如果命令执行成功，将返回0，kubelet就会认为该容器是活着的并且很健康。如果返回非0值，kubelet就会杀掉这个容器并重启它。</p>

<p>容器启动时，执行该命令：</p>

<pre><code>/bin/sh -c &quot;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600&quot;
</code></pre>

<p>在容器生命的最初30秒内有一个 /tmp/healthy 文件，在这30秒内 cat /tmp/healthy命令会返回一个成功的返回码。30秒后， cat /tmp/healthy 将返回失败的返回码。</p>

<p>创建Pod：</p>

<pre><code>kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/exec-liveness.yaml
</code></pre>

<p>在30秒内，查看Pod的event：结果显示没有失败的liveness probe：</p>

<pre><code>kubectl describe pod liveness-exec
FirstSeen    LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
24s       24s     1   {default-scheduler }                    Normal      Scheduled   Successfully assigned liveness-exec to worker0
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulling     pulling image &quot;gcr.io/google_containers/busybox&quot;
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulled      Successfully pulled image &quot;gcr.io/google_containers/busybox&quot;
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Created     Created container with docker id 86849c15382e; Security:[seccomp=unconfined]
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Started     Started container with docker id 86849c15382e
</code></pre>

<p>启动35秒后，再次查看pod的event：在最下面有一条信息显示liveness probe失败，容器被删掉并重新创建。</p>

<pre><code>kubectl describe pod liveness-exec
FirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
37s       37s     1   {default-scheduler }                    Normal      Scheduled   Successfully assigned liveness-exec to worker0
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulling     pulling image &quot;gcr.io/google_containers/busybox&quot;
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulled      Successfully pulled image &quot;gcr.io/google_containers/busybox&quot;
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Created     Created container with docker id 86849c15382e; Security:[seccomp=unconfined]
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Started     Started container with docker id 86849c15382e
2s        2s      1   {kubelet worker0}   spec.containers{liveness}   Warning     Unhealthy   Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
</code></pre>

<p>再等30秒，确认容器已经重启：从输出结果来RESTARTS值加1了。</p>

<pre><code>kubectl get pod liveness-exec
NAME            READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m
</code></pre>

<p>2、基于HTTP请求</p>

<p>我们还可以使用HTTP GET请求作为liveness probe。下面是一个基于gcr.io/google_containers/liveness镜像运行了一个容器的Pod的例子http-liveness.yaml：</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    args:
    - /server
    image: gcr.io/google_containers/liveness
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
          - name: X-Custom-Header
            value: Awesome
      initialDelaySeconds: 3
      periodSeconds: 3
</code></pre>

<p>该配置文件只定义了一个容器，livenessProbe 指定kubelet需要每隔3秒执行一次liveness probe。initialDelaySeconds 指定kubelet在该执行第一次探测之前需要等待3秒钟。该探针将向容器中的server的8080端口发送一个HTTP GET请求。如果server的/healthz路径的handler返回一个成功的返回码，kubelet就会认定该容器是活着的并且很健康。如果返回失败的返回码，kubelet将杀掉该容器并重启它。</p>

<p>任何大于200小于400的返回码都会认定是成功的返回码。其他返回码都会被认为是失败的返回码。</p>

<p>最开始的10秒该容器是活着的， /healthz handler返回200的状态码。这之后将返回500的返回码。</p>

<pre><code>http.HandleFunc(&quot;/healthz&quot;, func(w http.ResponseWriter, r *http.Request) {
    duration := time.Now().Sub(started)
    if duration.Seconds() &gt; 10 {
        w.WriteHeader(500)
        w.Write([]byte(fmt.Sprintf(&quot;error: %v&quot;, duration.Seconds())))
    } else {
        w.WriteHeader(200)
        w.Write([]byte(&quot;ok&quot;))
    }
})
</code></pre>

<p>容器启动3秒后，kubelet开始执行健康检查。第一次健康监测会成功，但是10秒后，健康检查将失败，kubelet将杀掉和重启容器。</p>

<p>创建一个Pod来测试一下HTTP liveness检测：</p>

<pre><code>kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/http-liveness.yaml

After 10 seconds, view Pod events to verify that liveness probes have failed and the Container has been restarted:
</code></pre>

<p>10秒后，查看Pod的event，确认liveness probe失败并重启了容器。</p>

<pre><code>kubectl describe pod liveness-http
</code></pre>

<p>3、基于TCP liveness探针</p>

<p>第三种liveness probe使用TCP Socket。 使用此配置，kubelet将尝试在指定端口上打开容器的套接字。 如果可以建立连接，容器被认为是健康的，如果不能就认为是失败的。</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: gcr.io/google_containers/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
</code></pre>

<p>如您所见，TCP检查的配置与HTTP检查非常相似。 此示例同时使用了readiness和liveness probe。 容器启动后5秒钟，kubelet将发送第一个readiness probe。 这将尝试连接到端口8080上的goproxy容器。如果探测成功，则该pod将被标记为就绪。Kubelet将每隔10秒钟执行一次该检查。</p>

<p>除了readiness probe之外，该配置还包括liveness probe。 容器启动15秒后，kubelet将运行第一个liveness probe。 就像readiness probe一样，这将尝试连接到goproxy容器上的8080端口。如果liveness probe失败，容器将重新启动。</p>

<p>4、使用命名的端口</p>

<p>可以使用命名的ContainerPort作为HTTP或TCP liveness检查：</p>

<pre><code>ports:
- name: liveness-port
  containerPort: 8080
  hostPort: 8080

livenessProbe:
  httpGet:
  path: /healthz
  port: liveness-port
</code></pre>

<p><strong>readiness probe</strong></p>

<p>1、定义readiness探针</p>

<p>有时，应用程序暂时无法对外部流量提供服务。 例如，应用程序可能需要在启动期间加载大量数据或配置文件。 在这种情况下，你不想杀死应用程序，但你也不想发送请求。 Kubernetes提供了readiness probe来检测和减轻这些情况。 Pod中的容器可以报告自己还没有准备，不能处理Kubernetes服务发送过来的流量。</p>

<p>Readiness probe的配置跟liveness probe很像。唯一的不同是使用 readinessProbe而不是livenessProbe。</p>

<pre><code>readinessProbe:
  exec:
    command:
    - cat
    - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 5
</code></pre>

<p>Readiness probe的HTTP和TCP的探测器配置跟liveness probe一样。</p>

<p>Readiness和livenss probe可以并行用于同一容器。 使用两者可以确保流量无法到达未准备好的容器，并且容器在失败时重新启动。</p>

<p>二、配置Probe</p>

<p>Probe 中有很多精确和详细的配置，通过它们你能准确的控制liveness和readiness检查：</p>

<pre><code>initialDelaySeconds：容器启动后第一次执行探测是需要等待多少秒。
periodSeconds：执行探测的频率。默认是10秒，最小1秒。
timeoutSeconds：探测超时时间。默认1秒，最小1秒。
successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1。
failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。
</code></pre>

<p>HTTP probe 中可以给 httpGet设置其他配置项：</p>

<pre><code>host：连接的主机名，默认连接到pod的IP。你可能想在http header中设置&quot;Host&quot;而不是使用IP。
scheme：连接使用的schema，默认HTTP。
path: 访问的HTTP server的path。
httpHeaders：自定义请求的header。HTTP运行重复的header。
port：访问的容器的端口名字或者端口号。端口号必须介于1和65535之间。
</code></pre>

<p>对于HTTP探测器，kubelet向指定的路径和端口发送HTTP请求以执行检查。 Kubelet将probe发送到容器的IP地址，除非地址被httpGet中的可选host字段覆盖。 在大多数情况下，你不想设置主机字段。 有一种情况下你可以设置它。 假设容器在127.0.0.1上侦听，并且Pod的hostNetwork字段为true。 然后，在httpGet下的host应该设置为127.0.0.1。 如果你的pod依赖于虚拟主机，这可能是更常见的情况，你不应该是用host，而是应该在httpHeaders中设置Host头。</p>

<h2 id="rc">RC</h2>

<p>Replication Controller 保证了在所有时间内，都有特定数量的Pod副本正在运行，如果太多了，Replication Controller就杀死几个，如果太少了，Replication Controller会新建几个，和直接创建的pod不同的是，Replication Controller会替换掉那些删除的或者被终止的pod，不管删除的原因是什么（维护阿，更新啊，Replication Controller都不关心）。基于这个理由，我们建议即使是只创建一个pod，我们也要使用Replication Controller。Replication Controller 就像一个进程管理器，监管着不同node上的多个pod,而不是单单监控一个node上的pod,Replication Controller 会委派本地容器来启动一些节点上服务（Kubelet ,Docker）。</p>

<p>后来这种kind和kube-controller-manager的模块同名，所以修改为ReplicaSet，一样的功能。</p>

<h2 id="depolymemt">depolymemt</h2>

<p>Deployment为Pod和ReplicaSet提供了一个声明式定义(declarative)方法，用来替代以前的ReplicationController来方便的管理应用。典型的应用场景包括：</p>

<pre><code>定义Deployment来创建Pod和ReplicaSet
滚动升级和回滚应用
扩容和缩容
暂停和继续Deployment
</code></pre>

<p>deploymemt就是使用ReplicaSet来实现的，现在基本都是使用这个。一般适用于无状态的应用部署。</p>

<p>实例：一个简单的nginx应用</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
</code></pre>

<blockquote>
<p>扩容</p>
</blockquote>

<pre><code>kubectl scale deployment nginx-deployment --replicas 10
</code></pre>

<p>如果集群支持 horizontal pod autoscaling 的话，还可以为Deployment设置自动扩展：</p>

<pre><code>kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80
</code></pre>

<blockquote>
<p>回滚：</p>
</blockquote>

<pre><code>kubectl rollout undo deployment/nginx-deployment
</code></pre>

<blockquote>
<p>创建</p>
</blockquote>

<pre><code>$ kubectl create -f docs/user-guide/nginx-deployment.yaml --record
deployment &quot;nginx-deployment&quot; created
</code></pre>

<p>将kubectl的 —record 的flag设置为 true可以在annotation中记录当前命令创建或者升级了该资源。</p>

<p>获取信息</p>

<pre><code>$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         3         3            3           18s

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-2035384211   3         3         0       18s

$ kubectl get pods --show-labels
NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-2035384211-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
nginx-deployment-2035384211-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
nginx-deployment-2035384211-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
</code></pre>

<p>可见是基于rs的基础上实现的。</p>

<blockquote>
<p>更新</p>
</blockquote>

<p>更新镜像也比较简单:</p>

<pre><code>kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
</code></pre>

<p>我们可以使用edit命令来编辑Deployment，修改 .spec.template.spec.containers[0].image ，将nginx:1.7.9 改写成nginx:1.9.1。</p>

<pre><code>$ kubectl edit deployment/nginx-deployment
deployment &quot;nginx-deployment&quot; edited
</code></pre>

<p>查看rollout的状态，只要执行：</p>

<pre><code>$ kubectl rollout status deployment/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment &quot;nginx-deployment&quot; successfully rolled out
</code></pre>

<blockquote>
<p>检查Deployment升级的历史记录</p>
</blockquote>

<p>首先，检查下Deployment的revision：</p>

<pre><code>$ kubectl rollout history deployment/nginx-deployment
deployments &quot;nginx-deployment&quot;:
REVISION    CHANGE-CAUSE
1           kubectl create -f docs/user-guide/nginx-deployment.yaml --record
2           kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
3           kubectl set image deployment/nginx-deployment nginx=nginx:1.91
</code></pre>

<h2 id="statefulset">StatefulSet</h2>

<p>StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计），其应用场景包括</p>

<pre><code>稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现
稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现
有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现
有序收缩，有序删除（即从N-1到0）
</code></pre>

<p>从上面的应用场景可以发现，StatefulSet由以下几个部分组成：</p>

<pre><code>用于定义网络标志（DNS domain）的Headless Service
用于创建PersistentVolumes的volumeClaimTemplates
定义具体应用的StatefulSet
</code></pre>

<p>StatefulSet中每个Pod的DNS格式为statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local，其中</p>

<pre><code>serviceName为Headless Service的名字
0..N-1为Pod所在的序号，从0开始到N-1
statefulSetName为StatefulSet的名字
namespace为服务所在的namespace，Headless Servic和StatefulSet必须在相同的namespace
.cluster.local为Cluster Domain，
</code></pre>

<p>简单示例</p>

<p>以一个简单的nginx服务web.yaml为例：</p>

<pre><code>---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &quot;nginx&quot;
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: gcr.io/google_containers/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      resources:
        requests:
          storage: 1Gi
</code></pre>

<p>创建</p>

<pre><code>$ kubectl create -f web.yaml
service &quot;nginx&quot; created
statefulset &quot;web&quot; created
</code></pre>

<p>查看创建的headless service和statefulset</p>

<pre><code>$ kubectl get service nginx
NAME      CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     None         &lt;none&gt;        80/TCP    1m
$ kubectl get statefulset web
NAME      DESIRED   CURRENT   AGE
web       2         2         2m
</code></pre>

<p>根据volumeClaimTemplates自动创建PVC（在GCE中会自动创建kubernetes.io/gce-pd类型的volume）</p>

<pre><code>$ kubectl get pvc
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-d064a004-d8d4-11e6-b521-42010a800002   1Gi        RWO           16s
www-web-1   Bound     pvc-d06a3946-d8d4-11e6-b521-42010a800002   1Gi        RWO           16s
</code></pre>

<p>查看创建的Pod，他们都是有序的</p>

<pre><code>$ kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          5m
web-1     1/1       Running   0          4m
</code></pre>

<p>使用nslookup查看这些Pod的DNS</p>

<pre><code>$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh
/ # nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.2.10
/ # nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.3.12
/ # nslookup web-0.nginx.default.svc.cluster.local
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx.default.svc.cluster.local
Address 1: 10.244.2.10
</code></pre>

<p>还可以进行其他的操作</p>

<p>扩容</p>

<pre><code>$ kubectl scale statefulset web --replicas=5
</code></pre>

<p>缩容</p>

<pre><code>$ kubectl patch statefulset web -p '{&quot;spec&quot;:{&quot;replicas&quot;:3}}'
</code></pre>

<p>镜像更新（目前还不支持直接更新image，需要patch来间接实现）</p>

<pre><code>$ kubectl patch statefulset web --type='json' -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;gcr.io/google_containers/nginx-slim:0.7&quot;}]'
</code></pre>

<p>删除StatefulSet和Headless Service</p>

<pre><code>$ kubectl delete statefulset web
$ kubectl delete service nginx
</code></pre>

<p>StatefulSet删除后PVC还会保留着，数据不再使用的话也需要删除</p>

<pre><code>$ kubectl delete pvc www-web-0 www-web-1
zookeeper
</code></pre>

<p>另外一个更能说明StatefulSet强大功能的示例为zookeeper.yaml。</p>

<pre><code>---
apiVersion: v1
kind: Service
metadata:
  name: zk-headless
  labels:
    app: zk-headless
spec:
  ports:
  - port: 2888
    name: server
  - port: 3888
    name: leader-election
  clusterIP: None
  selector:
    app: zk
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: zk-config
data:
  ensemble: &quot;zk-0;zk-1;zk-2&quot;
  jvm.heap: &quot;2G&quot;
  tick: &quot;2000&quot;
  init: &quot;10&quot;
  sync: &quot;5&quot;
  client.cnxns: &quot;60&quot;
  snap.retain: &quot;3&quot;
  purge.interval: &quot;1&quot;
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: zk-budget
spec:
  selector:
    matchLabels:
      app: zk
  minAvailable: 2
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: zk
spec:
  serviceName: zk-headless
  replicas: 3
  template:
    metadata:
      labels:
        app: zk
      annotations:
        pod.alpha.kubernetes.io/initialized: &quot;true&quot;
        scheduler.alpha.kubernetes.io/affinity: &gt;
            {
              &quot;podAntiAffinity&quot;: {
                &quot;requiredDuringSchedulingRequiredDuringExecution&quot;: [{
                  &quot;labelSelector&quot;: {
                    &quot;matchExpressions&quot;: [{
                      &quot;key&quot;: &quot;app&quot;,
                      &quot;operator&quot;: &quot;In&quot;,
                      &quot;values&quot;: [&quot;zk-headless&quot;]
                    }]
                  },
                  &quot;topologyKey&quot;: &quot;kubernetes.io/hostname&quot;
                }]
              }
            }
    spec:
      containers:
      - name: k8szk
        imagePullPolicy: Always
        image: gcr.io/google_samples/k8szk:v1
        resources:
          requests:
            memory: &quot;4Gi&quot;
            cpu: &quot;1&quot;
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        env:
        - name : ZK_ENSEMBLE
          valueFrom:
            configMapKeyRef:
              name: zk-config
              key: ensemble
        - name : ZK_HEAP_SIZE
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: jvm.heap
        - name : ZK_TICK_TIME
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: tick
        - name : ZK_INIT_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: init
        - name : ZK_SYNC_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: tick
        - name : ZK_MAX_CLIENT_CNXNS
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: client.cnxns
        - name: ZK_SNAP_RETAIN_COUNT
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: snap.retain
        - name: ZK_PURGE_INTERVAL
          valueFrom:
            configMapKeyRef:
                name: zk-config
                key: purge.interval
        - name: ZK_CLIENT_PORT
          value: &quot;2181&quot;
        - name: ZK_SERVER_PORT
          value: &quot;2888&quot;
        - name: ZK_ELECTION_PORT
          value: &quot;3888&quot;
        command:
        - sh
        - -c
        - zkGenConfig.sh &amp;&amp; zkServer.sh start-foreground
        readinessProbe:
          exec:
            command:
            - &quot;zkOk.sh&quot;
          initialDelaySeconds: 15
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
            - &quot;zkOk.sh&quot;
          initialDelaySeconds: 15
          timeoutSeconds: 5
        volumeMounts:
        - name: datadir
          mountPath: /var/lib/zookeeper
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      resources:
        requests:
          storage: 20Gi
</code></pre>

<p>创建</p>

<pre><code>kubectl create -f zookeeper.yaml
</code></pre>

<p>StatefulSet注意事项</p>

<pre><code>还在beta状态，需要kubernetes v1.5版本以上才支持
所有Pod的Volume必须使用PersistentVolume或者是管理员事先创建好
为了保证数据安全，删除StatefulSet时不会删除Volume
StatefulSet需要一个Headless Service来定义DNS domain，需要在StatefulSet之前创建好
目前StatefulSet还没有feature complete，比如更新操作还需要手动patch。
</code></pre>

<h2 id="daemonset">DaemonSet</h2>

<p>DaemonSet保证在每个Node上都运行一个容器副本，常用来部署一些集群的日志、监控或者其他系统管理应用。典型的应用包括：</p>

<pre><code>日志收集，比如fluentd，logstash等
系统监控，比如Prometheus Node Exporter，collectd，New Relic agent，Ganglia gmond等
系统程序，比如kube-proxy, kube-dns, glusterd, ceph等
</code></pre>

<p>使用Fluentd收集日志的例子：</p>

<pre><code>apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: fluentd
spec:
  template:
    metadata:
      labels:
        app: logging
        id: fluentd
      name: fluentd
    spec:
      containers:
      - name: fluentd-es
        image: gcr.io/google_containers/fluentd-elasticsearch:1.3
        env:
         - name: FLUENTD_ARGS
           value: -qq
        volumeMounts:
         - name: containers
           mountPath: /var/lib/docker/containers
         - name: varlog
           mountPath: /varlog
      volumes:
         - hostPath:
             path: /var/lib/docker/containers
           name: containers
         - hostPath:
             path: /var/log
           name: varlog
</code></pre>

<p>指定Node节点</p>

<p>DaemonSet会忽略Node的unschedulable状态，有两种方式来指定Pod只运行在指定的Node节点上：</p>

<pre><code>nodeSelector：只调度到匹配指定label的Node上
nodeAffinity：功能更丰富的Node选择器，比如支持集合操作
podAffinity：调度到满足条件的Pod所在的Node上
</code></pre>

<p>nodeSelector示例</p>

<p>首先给Node打上标签</p>

<pre><code>kubectl label nodes node-01 disktype=ssd
</code></pre>

<p>然后在daemonset中指定nodeSelector为disktype=ssd：</p>

<pre><code>spec:
  nodeSelector:
    disktype: ssd
</code></pre>

<p>nodeAffinity示例</p>

<p>nodeAffinity目前支持两种：requiredDuringSchedulingIgnoredDuringExecution和preferredDuringSchedulingIgnoredDuringExecution，分别代表必须满足条件和优选条件。比如下面的例子代表调度到包含标签kubernetes.io/e2e-az-name并且值为e2e-az1或e2e-az2的Node上，并且优选还带有标签another-node-label-key=another-node-label-value的Node。</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: gcr.io/google_containers/pause:2.0
</code></pre>

<p>podAffinity示例</p>

<p>podAffinity基于Pod的标签来选择Node，仅调度到满足条件Pod所在的Node上，支持podAffinity和podAntiAffinity。这个功能比较绕，以下面的例子为例：</p>

<p>如果一个“Node所在Zone中包含至少一个带有security=S1标签且运行中的Pod”，那么可以调度到该Node</p>

<p>不调度到“包含至少一个带有security=S2标签且运行中Pod”的Node上</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: kubernetes.io/hostname
  containers:
  - name: with-pod-affinity
    image: gcr.io/google_containers/pause:2.0
</code></pre>

<p>静态Pod</p>

<p>除了DaemonSet，还可以使用静态Pod来在每台机器上运行指定的Pod，这需要kubelet在启动的时候指定manifest目录：</p>

<pre><code>kubelet --pod-manifest-path=/etc/kubernetes/manifests
</code></pre>

<p>然后将所需要的Pod定义文件放到指定的manifest目录中。</p>

<p>注意：静态Pod不能通过API Server来删除，但可以通过删除manifest文件来自动删除对应的Pod。</p>

<h2 id="cronjob">CronJob</h2>

<p>CronJob即定时任务，就类似于Linux系统的crontab，在指定的时间周期运行指定的任务。在Kubernetes 1.5，使用CronJob需要开启batch/v2alpha1 API，即–runtime-config=batch/v2alpha1。</p>

<p>CronJob Spec</p>

<pre><code>.spec.schedule指定任务运行周期，格式同Cron
.spec.jobTemplate指定需要运行的任务，格式同Job
.spec.startingDeadlineSeconds指定任务开始的截止期限
.spec.concurrencyPolicy指定任务的并发策略，支持Allow、Forbid和Replace三个选项
</code></pre>

<p>实例</p>

<pre><code>apiVersion: batch/v2alpha1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
</code></pre>

<p>创建</p>

<pre><code>$ kubectl create -f cronjob.yaml
cronjob &quot;hello&quot; created
</code></pre>

<p>当然，也可以用kubectl run来创建一个CronJob：</p>

<pre><code>kubectl run hello --schedule=&quot;*/1 * * * *&quot; --restart=OnFailure --image=busybox -- /bin/sh -c &quot;date; echo Hello from the Kubernetes cluster&quot;
$ kubectl get cronjob
NAME      SCHEDULE      SUSPEND   ACTIVE    LAST-SCHEDULE
hello     */1 * * * *   False     0         &lt;none&gt;
$ kubectl get jobs
NAME               DESIRED   SUCCESSFUL   AGE
hello-1202039034   1         1            49s
$ pods=$(kubectl get pods --selector=job-name=hello-1202039034 --output=jsonpath={.items..metadata.name} -a)
$ kubectl logs $pods
Mon Aug 29 21:34:09 UTC 2016
Hello from the Kubernetes cluster
</code></pre>

<p>注意，删除cronjob的时候不会自动删除job，这些job可以用kubectl delete job来删除</p>

<pre><code>$ kubectl delete cronjob hello
cronjob &quot;hello&quot; deleted
</code></pre>

<h2 id="job">job</h2>

<p>Job负责批量处理短暂的一次性任务 (short lived one-off tasks)，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束。</p>

<p>Kubernetes支持以下几种Job：</p>

<pre><code>非并行Job：通常创建一个Pod直至其成功结束
固定结束次数的Job：设置.spec.completions，创建多个Pod，直到.spec.completions个Pod成功结束
带有工作队列的并行Job：设置.spec.Parallelism但不设置.spec.completions，当所有Pod结束并且至少一个成功时，Job就认为是成功
</code></pre>

<p>根据.spec.completions和.spec.Parallelism的设置，可以将Job划分为以下几种pattern：</p>

<pre><code>Job类型   使用示例    行为  completions Parallelism
一次性Job  数据库迁移   创建一个Pod直至其成功结束  1   1
固定结束次数的Job  处理工作队列的Pod  依次创建一个Pod运行直至completions个成功结束   2+  1
固定结束次数的并行Job    多个Pod同时处理工作队列   依次创建多个Pod运行直至completions个成功结束   2+  2+
并行Job   多个Pod同时处理工作队列   创建一个或多个Pod直至有一个成功结束 1   2+
</code></pre>

<blockquote>
<p>Job Controller</p>
</blockquote>

<p>Job Controller负责根据Job Spec创建Pod，并持续监控Pod的状态，直至其成功结束。如果失败，则根据restartPolicy（只支持OnFailure和Never，不支持Always）决定是否创建新的Pod再次重试任务。</p>

<p>Job Spec格式</p>

<p>spec.template格式同Pod</p>

<p>RestartPolicy仅支持Never或OnFailure</p>

<p>单个Pod时，默认Pod成功运行后Job即结束</p>

<pre><code>.spec.completions标志Job结束需要成功运行的Pod个数，默认为1
.spec.parallelism标志并行运行的Pod的个数，默认为1
spec.activeDeadlineSeconds标志失败Pod的重试最大时间，超过这个时间不会继续重试
</code></pre>

<p>一个简单的例子：</p>

<pre><code>apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: [&quot;perl&quot;,  &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]
      restartPolicy: Never
$ kubectl create -f ./job.yaml
job &quot;pi&quot; created
$ pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath={.items..metadata.name})
$ kubectl logs $pods
3.141592653589793238462643383279502...
</code></pre>

<p>固定结束次数的Job示例</p>

<pre><code>apiVersion: batch/v1
kind: Job
metadata:
  name: busybox
spec:
  completions: 3
  template:
    metadata:
      name: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        command: [&quot;echo&quot;, &quot;hello&quot;]
      restartPolicy: Never
</code></pre>

<h2 id="label">label</h2>

<blockquote>
<p>label</p>
</blockquote>

<p>Label keys的语法</p>

<pre><code>一个可选前缀+名称，通过/来区分

名称部分是必须的，并且最多63个字符，开始和结束的字符必须是字母或者数字，中间是字母数字和_、-、.。

前缀可选，如指定必须是个DNS子域，一系列的DNS label通过.来划分，长度不超过253个字符，“/”来结尾。如前缀被省略了，这个Label的key被假定为对用户私有的。系统组成部分（比如scheduler,controller-manager,apiserver,kubectl）,必须要指定一个前缀，Kuberentes.io前缀是为K8S内核部分保留的。
</code></pre>

<p>label value语法</p>

<pre><code>长度不超过63个字符。

可以为空

首位字符必须为字母数字字符

中间必须是横线、_、.、数字、字母。
</code></pre>

<p>主要用于label selector对其他的没有任何意义。</p>

<blockquote>
<p>Label选择器</p>
</blockquote>

<p>基于相等性或者不相等性的</p>

<pre><code>environment = production
tier != frontend
</code></pre>

<p>第一个选择所有键等于 environment 值为 production 的资源。后一种选择所有键为 tier 值不等于 frontend 的资源，和那些没有键为 tier 的label的资源。</p>

<p>要过滤所有处于 production 但不是 frontend 的资源，可以使用逗号操作符， environment=production,tier!=frontend 。</p>

<pre><code>environment=production,tier!=frontend
</code></pre>

<p>基于set的条件</p>

<p>基于集合的label条件允许用一组值来过滤键。支持三种操作符: in ， notin ,和 exists(仅针对于key符号) 。例如：</p>

<pre><code>environment in (production, qa)
tier notin (frontend, backend)
partition
!partitio
</code></pre>

<blockquote>
<p>主要使用场景</p>
</blockquote>

<p>1、kube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的pod副本的数量，实现pod数量的自动控制。</p>

<p>2、kube-proxy进程通过service的Label Selector来选择对应的Pod，建立出对应的POd的转发路由表。</p>

<p>3、通过Node定义的Label，使用NodeSelector实现定向调度。</p>

<h2 id="service">service</h2>

<p>Kubernete Service 是一个定义了一组Pod的策略的抽象，我们也有时候叫做宏观服务，也可以是我们常说的微服务，之前说的pod，rs等就是服务。这些被服务标记的Pod都是（一般）通过label Selector决定的。可见service主要提供了负载均衡和服务发现的功能。</p>

<p>我们已经能够通过ReplicaSet来创建一组Pod来提供具有高可用性的服务。虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题：</p>

<pre><code>Pod IP仅仅是集群内可见的虚拟IP，外部无法访问。
Pod IP会随着Pod的销毁而消失，当ReplicaSet对Pod进行动态伸缩时，Pod IP可能随时随地都会变化，这样对于我们访问这个服务带来了难度。
</code></pre>

<p>因此，Kubernetes中的Service对象就是解决以上问题的实现服务发现核心关键。</p>

<blockquote>
<p>yaml模版</p>
</blockquote>

<p>参考k8s权威指南第二章第五节。</p>

<blockquote>
<p>创建</p>
</blockquote>

<p>Service同其他Kubernetes对象一样，也是通过yaml或json文件进行定义。此外，它和其他Controller对象一样，通过Label Selector来确定一个Service将要使用哪些Pod。一个简单的Service定义如下：</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  labels:
    run: nginx
  name: nginx-service
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 81
  selector:
    app: nginx
  type: ClusterIP
</code></pre>

<p>解析</p>

<p>1、通过spec.selector字段确定这个Service将要使用哪些Label。在本例中，这个名为nginx的Service，将会管理所有具有app: nginxLabel的Pod。</p>

<p>2、spec.ports.port: 80表明此Service将会监听80端口，并将所有监听到的请求转发给其管理的Pod。spec.ports.targetPort: 81表明此Service监听到的80端口的请求都会被转发给其管理的Pod的81端口，此字段可以省略，省略后其值会被设置为spec.ports.port的值。</p>

<p>3、type: ClusterIP表面此Service的type，有如下几种</p>

<pre><code>ClusterIP。默认值。给这个Service分配一个Cluster IP，它是Kubernetes系统自动分配的虚拟IP，因此只能在集群内部访问。
NodePort。将Service通过指定的Node上的端口暴露给外部。通过此方法，访问任意一个NodeIP:nodePort都将路由到ClusterIP，从而成功获得该服务。
LoadBalancer。在 NodePort 的基础上，借助 cloud provider 创建一个外部的负载均衡器，并将请求转发到 &lt;NodeIP&gt;:NodePort。此模式只能在云服务器（AWS等）上使用。
ExternalName。将服务通过 DNS CNAME 记录方式转发到指定的域名（通过 spec.externlName 设定）。需要 kube-dns 版本在 1.7 以上。
</code></pre>

<p>实例</p>

<p>1、假如有3个app: nginx Pod运行在3个不同的Node中，那么此时客户端访问任意一个Node的30001端口都能访问到这个nginx服务。</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    run: nginx
spec:
  selector:
    app: nginx
  ports:
  - port: 80
    nodePort: 30001
  type: NodePort
</code></pre>

<p>2、如果云服务商支持外接负载均衡器，则可以通过spec.type=LoadBalancer来定义Service</p>

<pre><code>kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
  clusterIP: 10.0.171.239
  loadBalancerIP: 78.11.24.19
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 146.148.47.155
</code></pre>

<p>创建</p>

<pre><code>kubectl apply -f service.yaml
</code></pre>

<blockquote>
<p>查看service</p>
</blockquote>

<pre><code>kubectl discribe service
</code></pre>

<p><img src="/media/cloud/k8s/service1" alt="" /></p>

<p>这个 IP 地址就是 service 的 IP 地址（clusterIP），这个 IP 地址在集群里面可以被其它 pod 所访问，相当于通过这个 IP 地址提供了统一的一个 pod 的访问入口，以及服务发现。</p>

<p><img src="/media/cloud/k8s/service2" alt="" /></p>

<p>实际的架构如上图所示。在 service 创建之后，它会在集群里面创建一个虚拟的 IP 地址以及端口，在集群里，所有的 pod 和 node 都可以通过这样一个 IP 地址和端口去访问到这个 service。这个 service 会把它选择的 pod 及其 IP 地址都挂载到后端。这样通过 service 的 IP 地址访问时，就可以负载均衡到后端这些 pod 上面去。</p>

<p>当 pod 的生命周期有变化时，比如说其中一个 pod 销毁，service 就会自动从后端摘除这个 pod。这样实现了：就算 pod 的生命周期有变化，它访问的端点是不会发生变化的。</p>

<blockquote>
<p>访问service</p>
</blockquote>

<p><strong>集群内</strong></p>

<p>1、首先我们可以通过 service 的虚拟 IP 去访问，比如说刚创建的 my-service 这个服务，通过 kubectl get svc 或者 kubectl discribe service 都可以看到它的虚拟 IP 地址是 172.29.3.27，端口是 80，然后就可以通过这个虚拟 IP 及端口在 pod 里面直接访问到这个 service 的地址。</p>

<p>2、第二种方式直接访问服务名，依靠 DNS 解析，就是同一个 namespace 里 pod 可以直接通过 service 的名字去访问到刚才所声明的这个 service。不同的 namespace 里面，我们可以通过 service 名字加“.”，然后加 service 所在的哪个 namespace 去访问这个 service，例如我们直接用 curl 去访问，就是 my-service:80 就可以访问到这个 service。</p>

<p>3、第三种是通过环境变量访问，在同一个 namespace 里的 pod 启动时，K8s 会把 service 的一些 IP 地址、端口，以及一些简单的配置，通过环境变量的方式放到 K8s 的 pod 里面。在 K8s pod 的容器启动之后，通过读取系统的环境变量比读取到 namespace 里面其他 service 配置的一个地址，或者是它的端口号等等。比如在集群的某一个 pod 里面，可以直接通过 curl $ 取到一个环境变量的值，比如取到 MY_SERVICE_SERVICE_HOST 就是它的一个 IP 地址，MY_SERVICE 就是刚才我们声明的 MY_SERVICE，SERVICE_PORT 就是它的端口号，这样也可以请求到集群里面的 MY_SERVICE 这个 service。</p>

<p>Headless Service</p>

<p>service 有一个特别的形态就是 Headless Service。service 创建的时候可以指定 clusterIP:None，告诉 K8s 说我不需要 clusterIP（就是刚才所说的集群里面的一个虚拟 IP），然后 K8s 就不会分配给这个 service 一个虚拟 IP 地址，它没有虚拟 IP 地址怎么做到负载均衡以及统一的访问入口呢？</p>

<p>它是这样来操作的：pod 可以直接通过 service_name 用 DNS 的方式解析到所有后端 pod 的 IP 地址，通过 DNS 的 A 记录的方式会解析到所有后端的 Pod 的地址，由客户端选择一个后端的 IP 地址，这个 A 记录会随着 pod 的生命周期变化，返回的 A 记录列表也发生变化，这样就要求客户端应用要从 A 记录把所有 DNS 返回到 A 记录的列表里面 IP 地址中，客户端自己去选择一个合适的地址去访问 pod。</p>

<p><img src="/media/cloud/k8s/service3" alt="" /></p>

<p>可以从上图看一下跟刚才我们声明的模板的区别，就是在中间加了一个 clusterIP:None，即表明不需要虚拟 IP。实际效果就是集群的 pod 访问 my-service 时，会直接解析到所有的 service 对应 pod 的 IP 地址，返回给 pod，然后 pod 里面自己去选择一个 IP 地址去直接访问。</p>

<p><strong>集群外</strong></p>

<p>1、NodePort 的方式就是在集群的 node 上面（即集群的节点的宿主机上面）去暴露节点上的一个端口，这样相当于在节点的一个端口上面访问到之后就会再去做一层转发，转发到虚拟的 IP 地址上面，就是刚刚宿主机上面 service 虚拟 IP 地址。</p>

<p>3、也可以直接把容器的port直接映射到node上，hostNetWork=true</p>

<p>2、LoadBalancer 类型就是在 NodePort 上面又做了一层转换，刚才所说的 NodePort 其实是集群里面每个节点上面一个端口，LoadBalancer 是在所有的节点前又挂一个负载均衡。比如在阿里云上挂一个 SLB，这个负载均衡会提供一个统一的入口，并把所有它接触到的流量负载均衡到每一个集群节点的 node pod 上面去。然后 node pod 再转化成 ClusterIP，去访问到实际的 pod 上面。</p>

<blockquote>
<p>没有 selector 的 Service</p>
</blockquote>

<p>服务最常见的是抽象化对 Kubernetes Pod 的访问，但是它们也可以抽象化其他种类的后端。 实例:</p>

<pre><code>希望在生产环境中使用外部的数据库集群，但测试环境使用自己的数据库。
希望服务指向另一个 命名空间 中或其它集群中的服务。
您正在将工作负载迁移到 Kubernetes。 在评估该方法时，您仅在 Kubernetes 中运行一部分后端。
</code></pre>

<p>在任何这些场景中，都能够定义没有 selector 的 Service。 实例:</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
</code></pre>

<p>由于此服务没有选择器，因此 不会 自动创建相应的 Endpoint 对象。 您可以通过手动添加 Endpoint 对象，将服务手动映射到运行该服务的网络地址和端口：</p>

<pre><code>apiVersion: v1
kind: Endpoints
metadata:
  name: my-service
subsets:
  - addresses:
      - ip: 192.0.2.42
    ports:
      - port: 9376
</code></pre>

<p>访问没有 selector 的 Service，与有 selector 的 Service 的原理相同。 请求将被路由到用户定义的 Endpoint， YAML中为: 192.0.2.42:9376 (TCP)。</p>

<p>ExternalName Service 是 Service 的特例，它没有 selector，也没有使用 DNS 名称代替。</p>

<h2 id="volume">Volume</h2>

<p>volume存储卷是Pod中能够被多个容器访问的共享目录，用于持久化存储数据。</p>

<p>kubenetes中的volume与Pod的生命周期相同，但与容器生命周期不相关，当容器终止或者重启时，volume中的数据也不会丢失，最后Volume支持多种数据类型，比如：GlusterFS，Ceph等分布式文件系统</p>

<p>Kubernete 支持如下类型的volume:</p>

<pre><code>emptyDir
hostPath
gcePersistentDisk
awsElasticBlockStore
nfs
iscsi
glusterfs
rbd
gitRepo
secret
persistentVolumeClaim
</code></pre>

<p>其中最常见的就是EmptyDir，hostPath，NFS，Secret</p>

<blockquote>
<p>emptyDir</p>
</blockquote>

<p>一个emptyDir 第一次创建是在一个pod被指定到具体node的时候，并且会一直存在在pod的生命周期当中，正如它的名字一样，它初始化是一个空的目录，pod中的容器都可以读写这个目录，这个目录可以被挂在到各个容器相同或者不相同的的路径下。当一个pod因为任何原因被移除的时候，这些数据会被永久删除。注意：一个容器崩溃了不会导致数据的丢失，因为容器的崩溃并不移除pod.</p>

<pre><code>临时空间，例如用于某些应用程序运行时所需的临时目录，且无需永久保留
长时间任务的中间过程checkpoint的临时保存目录
一个容器需要从另一个容器中获取数据库的目录（多容器共享目录）
</code></pre>

<p>实例</p>

<pre><code>[root@master ~]# cat test.yaml
apiVersion: v1
kind: Service
metadata:
  name: serivce-mynginx
  namespace: default
spec:
  type: NodePort
  selector:
    app: mynginx
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 30080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mynginx
  template:
    metadata:
      labels:
        app: mynginx
    spec:
      containers:
      - name: mynginx
        image: lizhaoqwe/nginx:v1
        volumeMounts:
        - mountPath: /usr/share/nginx/html/
          name: share
        ports:
        - name: nginx
          containerPort: 80
      - name: busybox
        image: busybox
        command:
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;sleep 4444&quot;
        volumeMounts:
        - mountPath: /data/
          name: share
      volumes:
      - name: share
        emptyDir: {}
</code></pre>

<p>创建Pod</p>

<pre><code>[root@master ~]# kubectl create -f test.yaml
</code></pre>

<p>查看Pod</p>

<pre><code>[root@master ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
deploy-5cd657dd46-sx287   2/2     Running   0          2m1s
</code></pre>

<p>查看service</p>

<pre><code>[root@master ~]# kubectl get svc
NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes        ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        6d10h
serivce-mynginx   NodePort    10.99.110.43   &lt;none&gt;        80:30080/TCP   2m27s
</code></pre>

<p>我们进入到busybox容器当中创建一个index.html</p>

<pre><code>[root@master ~]# kubectl exec -it deploy-5cd657dd46-sx287 -c busybox -- /bin/sh

容器内部：
/data # cd /data
/data # echo &quot;fengzi&quot; &gt; index.html
</code></pre>

<p>打开浏览器验证一下。</p>

<p>结构</p>

<p><img src="/media/cloud/k8s/volume" alt="" /></p>

<blockquote>
<p>hostPath</p>
</blockquote>

<p>hostPath为在Pod上挂载宿主机上的文件或目录，它通常可以用于以下几方面：</p>

<pre><code>容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的告诉文件系统进行存储
需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部应用可以直接访问Docker的文件系统
</code></pre>

<p>在使用这种类型的volume时，需要注意以下几点：</p>

<pre><code>在不同的node上具有相同配置的Pod时，可能会因为宿主机上的目录和文件不同而导致对volume上的目录和文件访问结果不一致
如果使用了资源配置，则kubernetes无法将hostPath在宿主机上使用的资源纳入管理
</code></pre>

<p>实例</p>

<pre><code>[root@master ~]# cat test.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-deploy
  namespace: default
spec:
  selector:
    app: mynginx
  type: NodePort
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 31111

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynginx
  template:
    metadata:
      name: web
      labels:
        app: mynginx
    spec:
      containers:
      - name: mycontainer
        image: lizhaoqwe/nginx:v1
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: persistent-storage
        ports:
        - containerPort: 80
      volumes:
      - name: persistent-storage
        hostPath:
          type: DirectoryOrCreate
          path: /mydata
</code></pre>

<p>类型</p>

<pre><code>DirectoryOrCreate
Directory
FileOrCreate
File
Socket
CharDevice
BloakDevice
</code></pre>

<p>执行yaml文件</p>

<pre><code>[root@master ~]# kubectl create -f test.yaml
service/nginx-deploy created
deployment.apps/mydeploy created
</code></pre>

<p>在目录里操作，都是能够看到的</p>

<p><img src="/media/cloud/k8s/volume1" alt="" /></p>

<blockquote>
<p>NFS</p>
</blockquote>

<p>NFS是Network File System的缩写及网络文件系统。</p>

<p>主要功能是通过局域网络让不同的主机系统之间可以共享文件或目录。</p>

<p>实例</p>

<p>开启集群以外的另一台虚拟机，安装nfs-utils安装包</p>

<p>note：这里要注意的是需要在集群每个节点都安装nfs-utils安装包，不然挂载会失败！</p>

<pre><code>[root@master mnt]# yum install nfs-utils
已加载插件：fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.cn99.com
 * extras: mirrors.cn99.com
 * updates: mirrors.cn99.com
软件包 1:nfs-utils-1.3.0-0.61.el7.x86_64 已安装并且是最新版本
无须任何处理
</code></pre>

<p>编辑/etc/exports文件添加以下内容</p>

<pre><code>[root@localhost share]# vim /etc/exports
    /share  192.168.254.0/24(insecure,rw,no_root_squash)
</code></pre>

<p>重启nfs服务</p>

<pre><code>[root@localhost share]# service nfs restart
Redirecting to /bin/systemctl restart nfs.service
</code></pre>

<p>在/share目录中写一个index.html文件并且写入内容</p>

<pre><code>[root@localhost share]# echo &quot;nfs server&quot; &gt; /share/index.html
</code></pre>

<p>在kubernetes集群的master节点中创建yaml文件并写入</p>

<pre><code>[root@master ~]# cat test.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-deploy
  namespace: default
spec:
  selector:
    app: mynginx
  type: NodePort
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 31111

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynginx
  template:
    metadata:
      name: web
      labels:
        app: mynginx
    spec:
      containers:
      - name: mycontainer
        image: lizhaoqwe/nginx:v1
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: nfs
        ports:
        - containerPort: 80
      volumes:
      - name: nfs
        nfs:
          server: 192.168.254.11       #nfs服务器地址
          path: /share　　　　　　　　　　#nfs服务器共享目录
</code></pre>

<p>创建yaml文件</p>

<pre><code>[root@master ~]# kubectl create -f test.yaml
service/nginx-deploy created
deployment.apps/mydeploy created
</code></pre>

<p>验证访问OK，没问题！！！</p>

<p><img src="/media/cloud/k8s/volume2" alt="" /></p>

<h2 id="pv">pv</h2>

<p>pv可以理解成为kubernetes集群中的某个网络存储对应的一块存储，它与Volume类似，但有以下区别：</p>

<pre><code>pv只能是网络存储，不属于任何Node，但可以在每个Node上访问
pv并不是被定义在Pod上的，而是独立于Pod之外定义的
</code></pre>

<p>实例</p>

<p>在nfs server服务器上创建nfs卷的映射并重启</p>

<pre><code>[root@localhost ~]# cat /etc/exports
/share_v1  192.168.254.0/24(insecure,rw,no_root_squash)
/share_v2  192.168.254.0/24(insecure,rw,no_root_squash)
/share_v3  192.168.254.0/24(insecure,rw,no_root_squash)
/share_v4  192.168.254.0/24(insecure,rw,no_root_squash)
/share_v5  192.168.254.0/24(insecure,rw,no_root_squash)

[root@localhost ~]# service nfs restart
</code></pre>

<p>在nfs server服务器上创建响应目录</p>

<pre><code>[root@localhost /]# mkdir /share_v{1,2,3,4,5}
</code></pre>

<p>在kubernetes集群中的master节点上创建pv，我这里创建了5个pv对应nfs server当中映射出来的5个目录</p>

<pre><code>[root@master ~]# cat createpv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv01
spec:
  nfs:　　　　　　　　　　　　　　 #存储类型
    path: /share_v1　　　　　　 #要挂在的nfs服务器的目录位置
    server: 192.168.254.11　　 #nfs server地址，也可以是域名，前提是能被解析
  accessModes: 　　　　　　　　　#访问模式：
  - ReadWriteMany　　　　　　　　　　ReadWriteMany：读写权限，允许多个Node挂载 | ReadWriteOnce：读写权限，只能被单个Node挂在 | ReadOnlyMany：只读权限，允许被多个Node挂载
  - ReadWriteOnce　　　　　　　　　
  capacity:　　　　　　　　　　　　#存储容量　　　　　　　　　　　　
    storage: 10Gi　　　　　　　　 #pv存储卷为10G
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv02
spec:
  nfs:
    path: /share_v2
    server: 192.168.254.11
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 20Gi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv03
spec:
  nfs:
    path: /share_v3
    server: 192.168.254.11
  accessModes:
  - ReadWriteMany
  - ReadWriteOnce
  capacity:
    storage: 30Gi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv04
spec:
  nfs:
    path: /share_v4
    server: 192.168.254.11
  accessModes:
  - ReadWriteMany
  - ReadWriteOnce
  capacity:
    storage: 40Gi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv05
spec:
  nfs:
    path: /share_v5
    server: 192.168.254.11
  accessModes:
  - ReadWriteMany
  - ReadWriteOnce
  capacity:
    storage: 50Gi
</code></pre>

<p>执行yaml文件</p>

<pre><code>[root@master ~]# kubectl create -f createpv.yaml
persistentvolume/pv01 created
persistentvolume/pv02 created
persistentvolume/pv03 created
persistentvolume/pv04 created
persistentvolume/pv05 created
</code></pre>

<p>查看pv</p>

<pre><code>[root@master ~]# kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv01   10Gi       RWO,RWX        Retain           Available                                   5m10s
pv02   20Gi       RWX            Retain           Available                                   5m10s
pv03   30Gi       RWO,RWX        Retain           Available                                   5m9s
pv04   40Gi       RWO,RWX        Retain           Available                                   5m9s
pv05   50Gi       RWO,RWX        Retain           Available                                   5m9s
</code></pre>

<p>解析</p>

<pre><code>ACCESS MODES:
　　RWO:ReadWriteOnly
　　RWX:ReadWriteMany
　　ROX:ReadOnlyMany
RECLAIM POLICY:
　　Retain：保护pvc释放的pv及其上的数据，将不会被其他pvc绑定
　　recycle：保留pv但清空数据
　　delete：删除pvc释放的pv及后端存储volume
STATUS:
　　Available:空闲状态
　　Bound：已经绑定到某个pvc上
　　Released：对应的pvc已经被删除，但是资源没有被集群回收
　　Failed：pv自动回收失败
CLAIM:
　　被绑定到了那个pvc上面格式为：NAMESPACE/PVC_NAME
　　
</code></pre>

<p>有了pv之后我们就可以创建pvc了</p>

<pre><code>[root@master ~]# cat test.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-deploy
  namespace: default
spec:
  selector:
    app: mynginx
  type: NodePort
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 31111

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynginx
  template:
    metadata:
      name: web
      labels:
        app: mynginx
    spec:
      containers:
      - name: mycontainer
        image: nginx
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: html
        ports:
        - containerPort: 80
      volumes:
      - name: html
        persistentVolumeClaim:
          claimName: mypvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
  namespace: default
spec:
  accessMode:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
</code></pre>

<p>执行yaml文件</p>

<pre><code>[root@master ~]# kubectl create -f test.yaml
service/nginx-deploy created
deployment.apps/mydeploy created
persistentvolumeclaim/mypvc created
</code></pre>

<p>再次查看pv，已经显示pvc被绑定到了pv02上</p>

<pre><code>[root@master ~]# kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM           STORAGECLASS   REASON   AGE
pv01   10Gi       RWO,RWX        Retain           Available                                           22m
pv02   20Gi       RWX            Retain           Bound       default/mypvc                           22m
pv03   30Gi       RWO,RWX        Retain           Available                                           22m
pv04   40Gi       RWO,RWX        Retain           Available                                           22m
pv05   50Gi       RWO,RWX        Retain           Available                                           22m
</code></pre>

<p>查看pvc</p>

<pre><code>[root@master ~]# kubectl get pvc
NAME    STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mypvc   Bound    pv02     20Gi       RWX                           113s
</code></pre>

<p>验证</p>

<p>在nfs server服务器上找到相应的目录执行以下命令</p>

<pre><code>[root@localhost share_v1]# echo 'test pvc' &gt; index.html
</code></pre>

<p>然后打开浏览器，OK,没问题。</p>

<p><img src="/media/cloud/k8s/volume3" alt="" /></p>

<h2 id="configmap">configmap</h2>

<p>k8s提供了两种配置模式，一种就是正常的配置configmap，另外一种就是加密的secret。</p>

<blockquote>
<p>configmap的作用</p>
</blockquote>

<p>应用部署的一个最佳实战是将应用所需的配置信息与程序进行分离，这样可以使应用程序被更好的复用，通过不同的配置也能实现更灵活的功能，将应用打包为容器镜像后，可以通过环境变量或者外挂文件的方式在创建容器时进行配置注入，但在大规模容器集群的环境中，对多个容器进行不同的配置讲变得非常复杂，Kubernetes 1.2开始提供了一种统一的应用配置管理方案-configMap</p>

<blockquote>
<p>configmap的用法</p>
</blockquote>

<p>ConfigMap供容器使用的典型用法如下：</p>

<pre><code>生成为容器内的环境变量
设置容器启动命令的启动参数（需设置为环境变量）
以volume的形式挂载为容器内部的文件或者目录
</code></pre>

<p>1、configMap编写变量注入pod中</p>

<p>比如我们用configmap创建两个变量，一个是nginx_port=80，一个是nginx_server=192.168.254.13</p>

<pre><code>[root@master ~]# kubectl create configmap nginx-var --from-literal=nginx_port=80 --from-literal=nginx_server=192.168.254.13
configmap/nginx-var created
</code></pre>

<p>查看configmap</p>

<pre><code>[root@master ~]# kubectl get cm
NAME        DATA   AGE
nginx-var   2      5s


[root@master ~]# kubectl describe cm nginx-var
Name:         nginx-var
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
nginx_port:
----
80
nginx_server:
----
192.168.254.13
Events:  &lt;none&gt;
</code></pre>

<p>然后我们创建pod，把这2个变量注入到环境变量当中</p>

<pre><code>[root@master ~]# cat test2.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-nginx
  namespace: default
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 30080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: web
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - name: nginx
          containerPort: 80
        volumeMounts:
          - name: html
            mountPath: /user/share/nginx/html/
        env:
        - name: TEST_PORT
          valueFrom:
            configMapKeyRef:
              name: nginx-var
              key: nginx_port
        - name: TEST_HOST
          valueFrom:
            configMapKeyRef:
              name: nginx-var
              key: nginx_server
      volumes:
      - name: html
        emptyDir: {}
</code></pre>

<p>执行pod文件</p>

<pre><code>[root@master ~]# kubectl create -f test2.yaml
service/service-nginx created
</code></pre>

<p>查看pod</p>

<pre><code>[root@master ~]# kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
mydeploy-d975ff774-fzv7g   1/1     Running   0          19s
mydeploy-d975ff774-nmmqt   1/1     Running   0          19s
</code></pre>

<p>进入到容器中查看环境变量</p>

<pre><code>[root@master ~]# kubectl exec -it mydeploy-d975ff774-fzv7g -- /bin/sh


# printenv
SERVICE_NGINX_PORT_80_TCP_PORT=80
KUBERNETES_PORT=tcp://10.96.0.1:443
SERVICE_NGINX_PORT_80_TCP_PROTO=tcp
KUBERNETES_SERVICE_PORT=443
HOSTNAME=mydeploy-d975ff774-fzv7g
SERVICE_NGINX_SERVICE_PORT_NGINX=80
HOME=/root
PKG_RELEASE=1~buster
SERVICE_NGINX_PORT_80_TCP=tcp://10.99.184.186:80
TEST_HOST=192.168.254.13
TEST_PORT=80
TERM=xterm
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
NGINX_VERSION=1.17.3
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
KUBERNETES_PORT_443_TCP_PORT=443
NJS_VERSION=0.3.5
KUBERNETES_PORT_443_TCP_PROTO=tcp
SERVICE_NGINX_SERVICE_HOST=10.99.184.186
SERVICE_NGINX_PORT=tcp://10.99.184.186:80
SERVICE_NGINX_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_SERVICE_HOST=10.96.0.1
PWD=/
SERVICE_NGINX_PORT_80_TCP_ADDR=10.99.184.186
</code></pre>

<p>可以发现configMap当中的环境变量已经注入到了pod容器当中</p>

<p>这里要注意的是，如果是用这种环境变量的注入方式，pod启动后，如果在去修改configMap当中的变量，对于pod是无效的，如果是以卷的方式挂载，是可的实时更新的，这一点要清楚</p>

<p>2、用configMap以存储卷的形式挂载到pod中</p>

<p>上面说到了configMap以变量的形式虽然可以注入到pod当中，但是如果在修改变量的话pod是不会更新的，如果想让configMap中的配置跟pod内部的实时更新，就需要以存储卷的形式挂载</p>

<pre><code>[root@master ~]# cat test2.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-nginx
  namespace: default
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 30080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: web
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - name: nginx
          containerPort: 80
        volumeMounts:
          - name: html-config
            mountPath: /nginx/vars/
            readOnly: true
      volumes:
      - name: html-config
        configMap:
          name: nginx-var
</code></pre>

<p>执行yaml文件</p>

<pre><code>[root@master ~]# kubectl create -f test2.yaml
service/service-nginx created
deployment.apps/mydeploy created
</code></pre>

<p>查看pod</p>

<pre><code>[root@master ~]# kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
mydeploy-6f6b6c8d9d-pfzjs   1/1     Running   0          90s
mydeploy-6f6b6c8d9d-r9rz4   1/1     Running   0          90s
</code></pre>

<p>进入到容器中</p>

<pre><code>[root@master ~]# kubectl exec -it mydeploy-6f6b6c8d9d-pfzjs -- /bin/bash
</code></pre>

<p>在容器中查看configMap对应的配置</p>

<pre><code>root@mydeploy-6f6b6c8d9d-pfzjs:/# cd /nginx/vars
root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars# ls
nginx_port  nginx_server
root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars# cat nginx_port
80
root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars#
</code></pre>

<p>修改configMap中的配置，把端口号从80修改成8080</p>

<pre><code>[root@master ~]# kubectl edit cm nginx-var
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  nginx_port: &quot;8080&quot;
  nginx_server: 192.168.254.13
kind: ConfigMap
metadata:
  creationTimestamp: &quot;2019-09-13T14:22:20Z&quot;
  name: nginx-var
  namespace: default
  resourceVersion: &quot;248779&quot;
  selfLink: /api/v1/namespaces/default/configmaps/nginx-var
  uid: dfce8730-f028-4c57-b497-89b8f1854630
</code></pre>

<p>修改完稍等片刻查看文件档中的值，已然更新成8080</p>

<pre><code>root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars# cat nginx_port
8080
root@mydeploy-6f6b6c8d9d-pfzjs:/nginx/vars#
</code></pre>

<p>3、configMap创建配置文件注入到pod当中</p>

<p>这里以nginx配置文件为例子，我们在宿主机上配置好nginx的配置文件，创建configmap，最后通过configmap注入到容器中</p>

<p>创建nginx配置文件</p>

<pre><code>[root@master ~]# vim www.conf
server {
    server_name: 192.168.254.13;
    listen: 80;
    root /data/web/html/;
}
</code></pre>

<p>创建configMap</p>

<pre><code>[root@master ~]# kubectl create configmap nginx-config --from-file=/root/www.conf
configmap/nginx-config created
</code></pre>

<p>查看configMap</p>

<pre><code>[root@master ~]# kubectl get cm
NAME           DATA   AGE
nginx-config   1      3m3s
nginx-var      2      63m
</code></pre>

<p>创建pod并挂载configMap存储卷</p>

<pre><code>[root@master ~]# cat test2.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-nginx
  namespace: default
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - name: nginx
    port: 80
    targetPort: 80
    nodePort: 30080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeploy
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: web
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - name: nginx
          containerPort: 80
        volumeMounts:
          - name: html-config
            mountPath: /etc/nginx/conf.d/
            readOnly: true
      volumes:
      - name: html-config
        configMap:
          name: nginx-config
</code></pre>

<p>启动容器，并让容器启动的时候就加载configMap当中的配置</p>

<pre><code>[root@master ~]# kubectl create -f test2.yaml
service/service-nginx created
deployment.apps/mydeploy created
</code></pre>

<p>查看容器</p>

<pre><code>[root@master ~]# kubectl get pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
mydeploy-fd46f76d6-jkq52   1/1     Running   0          22s   10.244.1.46   node1   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>访问容器当中的网页，80端口是没问题的，8888端口访问不同</p>

<pre><code>[root@master ~]# curl 10.244.1.46
this is test web


[root@master ~]# curl 10.244.1.46:8888
curl: (7) Failed connect to 10.244.1.46:8888; 拒绝连接
</code></pre>

<p>接下来我们去修改configMap当中的内容，吧80端口修改成8888</p>

<pre><code>[root@master ~]# kubectl edit cm nginx-config
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  www.conf: |
    server {
        server_name 192.168.254.13;
        listen 8888;
        root /data/web/html/;
    }
kind: ConfigMap
metadata:
  creationTimestamp: &quot;2019-09-13T15:22:22Z&quot;
  name: nginx-config
  namespace: default
  resourceVersion: &quot;252615&quot;
  selfLink: /api/v1/namespaces/default/configmaps/nginx-config
  uid: f1881f87-5a91-4b8e-ab39-11a2f45733c2
</code></pre>

<p>进入到容器查看配置文件，可以发现配置文件已经修改过来了</p>

<pre><code>root@mydeploy-fd46f76d6-jkq52:/usr/bin# cat /etc/nginx/conf.d/www.conf
server {
    server_name 192.168.254.13;
    listen 8888;
    root /data/web/html/;
}
</code></pre>

<p>在去测试访问，发现还是报错，这是因为配置文件虽然已经修改了，但是nginx服务并没有加载配置文件，我们手动加载一下，以后可以用脚本形式自动完成加载文件</p>

<pre><code>[root@master ~]# curl 10.244.1.46
this is test web
[root@master ~]# curl 10.244.1.46:8888
curl: (7) Failed connect to 10.244.1.46:8888; 拒绝连接
</code></pre>

<p>在容器内部手动加载配置文件</p>

<pre><code>root@mydeploy-fd46f76d6-jkq52:/usr/bin# nginx -s reload
2019/09/13 16:04:12 [notice] 34#34: signal process started
</code></pre>

<p>再去测试访问，可以看到80端口已经访问不通，反而是我们修改的8888端口可以访问通</p>

<pre><code>[root@master ~]# curl 10.244.1.46
curl: (7) Failed connect to 10.244.1.46:80; 拒绝连接
[root@master ~]# curl 10.244.1.46:8888
this is test web
</code></pre>

<blockquote>
<p>configmap的实际应用</p>
</blockquote>

<p>1、我们经常使用的就是设置pod的环境变量，比如一些IP和端口的设置</p>

<pre><code>[root@001 ~]# kubectl get cm agent-config -n kube-system -o yaml
apiVersion: v1
data:
  voyage_agent_exporter_port: &quot;969&quot;
  voyage_agent_grpc_port: &quot;966&quot;
  voyage_agent_http_port: &quot;968&quot;
  voyage_agent_mulit_uplinks: '{&quot;ovs&quot;:[&quot;service0&quot;, &quot;service1&quot;]}'
  voyage_agent_netlink_timeout: &quot;10000&quot;
  voyage_agent_single_uplinks: service0,service1
  voyage_cni_config: |-
    {
      &quot;cniVersion&quot;: &quot;0.3.1&quot;,
      &quot;name&quot;: &quot;voyage-net&quot;,
      &quot;type&quot;: &quot;voyage-cni&quot;
    }
  voyage_server_grpc_port: &quot;961&quot;
  voyage_server_ip_list: 10.243.40.1,10.243.40.2,10.243.40.3
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:{&quot;voyage_agent_exporter_port&quot;:&quot;969&quot;,&quot;voyage_agent_grpc_port&quot;:&quot;966&quot;,&quot;voyage_agent_http_port&quot;:&quot;968&quot;,&quot;voyage_agent_mulit_uplinks&quot;:&quot;{\&quot;ovs\&quot;:[\&quot;service0\&quot;, \&quot;service1\&quot;]}&quot;,&quot;voyage_agent_netlink_timeout&quot;:&quot;10000&quot;,&quot;voyage_agent_single_uplinks&quot;:&quot;service0,service1&quot;,&quot;voyage_cni_config&quot;:&quot;{\n  \&quot;cniVersion\&quot;: \&quot;0.3.1\&quot;,\n  \&quot;name\&quot;: \&quot;voyage-net\&quot;,\n  \&quot;type\&quot;: \&quot;voyage-cni\&quot;\n}&quot;,&quot;voyage_server_grpc_port&quot;:&quot;961&quot;,&quot;voyage_server_ip_list&quot;:&quot;10.243.40.1,10.243.40.2,10.243.40.3&quot;},&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;voyage-agent-config&quot;,&quot;namespace&quot;:&quot;kube-system&quot;}}
  creationTimestamp: &quot;2020-01-19T18:03:55Z&quot;
  name: voyage-agent-config
  namespace: kube-system
  resourceVersion: &quot;692680451&quot;
  selfLink: /api/v1/namespaces/kube-system/configmaps/voyage-agent-config
  uid: 0ebf9112-3ae6-11ea-ad4e-6c92bf8d8058
</code></pre>

<p>然后在pod启动的时候可以设置这些探针，但是由于这种方式并不是实时的，如果修改还是需要重新发布，所以并不常用，对于这些配置我们设置一个配置中心，然后在每次发布的时候，拉去配置来设置环境变量，我们更多的是使用env，比如日志采集的环境变量</p>

<pre><code>spec:
  containers:
  - args:
    - --log.file=/opt/logs/app/test1.log
    - --log.interval=60s
    - --log.lineSize=500
    - --log.maxLines=10000000
    env:
    - name: test_log_app
      value: /opt/logs/app/*.log
    - name: test_log_app_prefix
      value: V1,ldcId,hostgroup,appId,ip,path,lid
    - name: appId
      value: loggen
    - name: test_log_app_brokerlist
      value: kafkasit02broker01.cnsuning.com:9092,kafkasit02broker02.cnsuning.com:9092,kafkasit02broker03.cnsuning.com:9092
    - name: test_log_app_topic
      value: ctdsa_nodejs_sit_njxz
    - name: KUBERNETES_POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    image: xgharborsit01.sncloud.com/sncloud/loggen:v0.0.1
    imagePullPolicy: IfNotPresent
    name: loggen
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /opt/logs
      name: log
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-gj6mr
      readOnly: true
  dnsPolicy: ClusterFirst
</code></pre>

<p>2、将应用的配置文件挂载，然后给应用程序启动使用，是我们最常用的，比如filebeat的yaml文件。</p>

<pre><code>[root@xgpcc01m010243040001 ~]# kubectl get cm filebeat-config -n kube-system -o yaml
apiVersion: v1
data:
  filebeat-k8slog.yml: &quot;filebeat.inputs:\n- type: log\n  enabled: true\n  close_eof:
    false\n  close_inactive: 5m\n  close_removed: false\n  close_renamed: false\n
    \ ignore_older: 48h\n  clean_inactive: 72h\n  clean_removed: true\n  paths:\n
    \ - \&quot;\&quot;\n  fields_under_root: true\n  fields:\n    brokerlist:\n    split: \&quot;
    \       \&quot;\noutput.kafka:\n  topic: \n  version: \&quot;0.8.2.2\&quot;\n  codec.format:\n
    \   ignoreNotFound: true\n    string: 'V1%{[split]}%{[ldc]}%{[split]}%{[hostgroup]}%{[split]}%{[appid]}%{[split]}%{[ip]}%{[split]}%{[path]}%{[split]}%{[lid]}%{[split]}%{[host.name]}%{[split]}%{[host.ip]}%{[split]}%{[@timestamp]}%{[split]}%{[message]}'\n&quot;
  filebeat.yml: |
    max_procs: 2
    queue:
      mem:
        events: 512
        flush.min_events: 256
    filebeat.inputs:
    - type: log
      enabled: false
      paths:
      - /var/log/filebeat-pause.log
    filebeat.config:
      inputs:
        enabled: true
        path: ${path.home}/inputs.d/*.yml
        reload.enabled: true
        reload.period: 10s
    output.kafka:
      topic: &quot;%{[topic]}&quot;
      version: &quot;0.8.2.2&quot;
      codec.format:
        ignoreNotFound: true
        string: '%{[message]}'
      metadata:
        retry.max: 2
        full: true
kind: ConfigMap
metadata:
  creationTimestamp: &quot;2020-04-23T13:58:00Z&quot;
  name: filebeat-config
  namespace: kube-system
  resourceVersion: &quot;654788331&quot;
  selfLink: /api/v1/namespaces/kube-system/configmaps/filebeat-config
  uid: 7153cf8e-856a-11ea-8bc6-6c92bf977c52
</code></pre>

<p>再来看filebeat的资源配置清单filebeat.yaml</p>

<pre><code>[root@xgpcc01m010243040001 ~]# kubectl get ds filebeat -n kube-system -o yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  creationTimestamp: &quot;2020-04-09T17:43:00Z&quot;
  generation: 2
  labels:
    addon: filebeat
    app: filebeat
    namespace: kube-system
  name: filebeat
  namespace: kube-system
  resourceVersion: &quot;741573898&quot;
  selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/filebeat
  uid: 8e6ee5ef-7a89-11ea-a446-6c92bf8d8058
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      addon: filebeat
      app: filebeat
      namespace: kube-system
  template:
    metadata:
      creationTimestamp: null
      labels:
        addon: filebeat
        app: filebeat
        namespace: kube-system
    spec:
      containers:
      - args:
        - --path.home=/opt/filebeats/filebeat
        - --path.config=/etc/filebeat
        - --httpprof=:6060
        command:
        - /opt/filebeats/bin/filebeat
        env:
        - name: ldc
          value: NJXZ
        - name: appid
          value: PCC
        - name: envType
          value: prd
        - name: system_MwType_serviceID
          value: PCC_filebeat_01
        - name: cloudHostGroupId
          value: &quot;01&quot;
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        image: xgharbor01.sncloud.com/sncloud/filebeat:7.5.2-4
        imagePullPolicy: Always
        name: filebeat
        resources:
          limits:
            cpu: &quot;3&quot;
            memory: 2Gi
          requests:
            cpu: &quot;1&quot;
            memory: 800Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /host/var/lib/kubelet/pods
          mountPropagation: HostToContainer
          name: kubeletpods
          readOnly: true
        - mountPath: /opt/filebeats/filebeat
          name: k8slog
          subPath: filebeats/filebeat
        - mountPath: /etc/filebeat
          name: config
      - args:
        - --path.home=/opt/filebeats/filebeat-k8slog
        - --path.config=/etc/filebeat
        - -c
        - filebeat-k8slog.yml
        command:
        - /opt/filebeats/bin/filebeat
        env:
        - name: ldc
          value: NJXZ
        - name: appid
          value: PCC
        - name: envType
          value: prd
        - name: system_MwType_serviceID
          value: PCC_filebeat_01
        - name: cloudHostGroupId
          value: &quot;01&quot;
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        image: xgharbor01.sncloud.com/sncloud/filebeat:7.5.2-4
        imagePullPolicy: Always
        name: filebeat-k8slog
        resources:
          limits:
            cpu: 250m
            memory: 512Mi
          requests:
            cpu: 125m
            memory: 125Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /opt/filebeats/filebeat-k8slog
          name: k8slog
          subPath: filebeats/filebeat-k8slog
        - mountPath: /k8s_log
          name: k8slog
        - mountPath: /etc/filebeat
          name: config
      - args:
        - --path.base=/host
        - --path.template=filebeat.tpl
        - --path.filebeat-home=/opt/filebeats/filebeat
        - --path.logs=/opt/log-pilot/logs
        - --logLevel=debug
        - --logPrefix=sn
        command:
        - /opt/log-pilot/bin/log-pilot
        env:
        - name: ldc
          value: NJXZ
        - name: appid
          value: PCC
        - name: envType
          value: prd
        - name: system_MwType_serviceID
          value: PCC_filebeat_01
        - name: cloudHostGroupId
          value: &quot;01&quot;
        - name: KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: xgharbor01.sncloud.com/sncloud/log-pilot:1.0.2
        imagePullPolicy: Always
        name: log-pilot
        resources:
          limits:
            cpu: 250m
            memory: 512Mi
          requests:
            cpu: 125m
            memory: 125Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /opt/filebeats/filebeat
          name: k8slog
          subPath: filebeats/filebeat
        - mountPath: /opt/log-pilot/logs
          name: k8slog
          subPath: log-pilot
        - mountPath: /host/var/lib/kubelet/pods
          mountPropagation: HostToContainer
          name: kubeletpods
          readOnly: true
        - mountPath: /var/run/docker.sock
          mountPropagation: HostToContainer
          name: docker-sock
          readOnly: true
      dnsPolicy: ClusterFirst
      hostNetwork: true
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      volumes:
      - configMap:
          defaultMode: 420
          name: filebeat-config
        name: config
      - hostPath:
          path: /var/lib/kubelet/pods
          type: &quot;&quot;
        name: kubeletpods
      - hostPath:
          path: /var/run/docker.sock
          type: &quot;&quot;
        name: docker-sock
      - hostPath:
          path: /k8s_log
          type: &quot;&quot;
        name: k8slog
  templateGeneration: 6
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 25%
    type: RollingUpdate
status:
  currentNumberScheduled: 128
  desiredNumberScheduled: 128
  numberAvailable: 128
  numberMisscheduled: 0
  numberReady: 128
  observedGeneration: 2
  updatedNumberScheduled: 128
</code></pre>

<p>实例</p>

<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm
</code></pre>

<p>创建</p>

<pre><code>$ kubectl create  -f  config.yaml
configmap &quot;special-config&quot; created
</code></pre>

<p><strong>用作环境变量</strong></p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot;]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
      envFrom:
        - configMapRef:
            name: env-config
  restartPolicy: Never
</code></pre>

<p>当 Pod 结束后会输出</p>

<pre><code>SPECIAL_LEVEL_KEY=very
SPECIAL_TYPE_KEY=charm
log_level=INFO
</code></pre>

<p><strong>用作命令行参数</strong></p>

<p>将 ConfigMap 用作命令行参数时，需要先把 ConfigMap 的数据保存在环境变量中，然后通过 $(VAR_NAME) 的方式引用环境变量.</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
  restartPolicy: Never
</code></pre>

<p>当 Pod 结束后会输出</p>

<pre><code>very charm
</code></pre>

<p><strong>使用 volume 将 ConfigMap 作为文件或目录直接挂载</strong></p>

<p>将创建的 ConfigMap 直接挂载至 Pod 的 / etc/config 目录下，其中每一个 key-value 键值对都会生成一个文件，key 为文件名，value 为内容</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: vol-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cat /etc/config/special.how&quot;]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
  restartPolicy: Never
</code></pre>

<p>当 Pod 结束后会输出</p>

<pre><code>very
</code></pre>

<p>将创建的 ConfigMap 中 special.how 这个 key 挂载到 / etc/config 目录下的一个相对路径 / keys/special.level。如果存在同名文件，直接覆盖。其他的 key 不挂载</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;cat /etc/config/keys/special.level&quot;]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: special.how
          path: keys/special.level
  restartPolicy: Never
</code></pre>

<p>当 Pod 结束后会输出</p>

<pre><code>very
</code></pre>

<p>ConfigMap 支持同一个目录下挂载多个 key 和多个目录。例如下面将 special.how 和 special.type 通过挂载到 / etc/config 下。并且还将 special.how 同时挂载到 / etc/config2 下。</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 36000&quot;]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
      - name: config-volume2
        mountPath: /etc/config2
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: special.how
          path: keys/special.level
        - key: special.type
          path: keys/special.type
    - name: config-volume2
      configMap:
        name: special-config
        items:
        - key: special.how
          path: keys/special.level
  restartPolicy: Never

# ls  /etc/config/keys/
special.level  special.type
# ls  /etc/config2/keys/
special.level
# cat  /etc/config/keys/special.level
very
# cat  /etc/config/keys/special.type
charm
</code></pre>

<p>使用 subpath 将 ConfigMap 作为单独的文件挂载到目录</p>

<p>在一般情况下 configmap 挂载文件时，会先覆盖掉挂载目录，然后再将 congfigmap 中的内容作为文件挂载进行。如果想不对原来的文件夹下的文件造成覆盖，只是将 configmap 中的每个 key，按照文件的方式挂载到目录下，可以使用 subpath 参数。</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: nginx
      command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 36000&quot;]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/nginx/special.how
        subPath: special.how
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: special.how
          path: special.how
  restartPolicy: Never


root@dapi-test-pod:/# ls /etc/nginx/
conf.d    fastcgi_params    koi-utf  koi-win  mime.types  modules  nginx.conf  scgi_params    special.how  uwsgi_params  win-utf
root@dapi-test-pod:/# cat /etc/nginx/special.how
very
root@dapi-test-pod:/#
</code></pre>

<h2 id="secret">secret</h2>

<p>Secret解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者Pod Spec中。Secret可以以Volume或者环境变量的方式使用。</p>

<p>Secret有三种类型：</p>

<pre><code>Service Account：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中；
Opaque：base64编码格式的Secret，用来存储密码、密钥等；
kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息。
</code></pre>

<blockquote>
<p>Opaque Secret</p>
</blockquote>

<p>Opaque类型的数据是一个map类型，要求value是base64编码格式：</p>

<pre><code>$ echo -n &quot;admin&quot; | base64
YWRtaW4=
$ echo -n &quot;1f2d1e2e67df&quot; | base64
MWYyZDFlMmU2N2Rm
</code></pre>

<p>secrets.yml</p>

<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: MWYyZDFlMmU2N2Rm
  username: YWRtaW4=
</code></pre>

<p>接着，就可以创建secret了：kubectl create -f secrets.yml。</p>

<p>创建好secret之后，有两种方式来使用它：</p>

<p>1、以Volume方式</p>

<p>将Secret挂载到Volume中</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    name: db
  name: db
spec:
  volumes:
  - name: secrets
    secret:
      secretName: mysecret
  containers:
  - image: gcr.io/my_project_id/pg:v1
    name: db
    volumeMounts:
    - name: secrets
      mountPath: &quot;/etc/secrets&quot;
      readOnly: true
    ports:
    - name: cp
      containerPort: 5432
      hostPort: 5432
</code></pre>

<p>2、以环境变量方式</p>

<p>将Secret导出到环境变量中</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: wordpress-deployment
spec:
  replicas: 2
  strategy:
      type: RollingUpdate
  template:
    metadata:
      labels:
        app: wordpress
        visualize: &quot;true&quot;
    spec:
      containers:
      - name: &quot;wordpress&quot;
        image: &quot;wordpress&quot;
        ports:
        - containerPort: 80
        env:
        - name: WORDPRESS_DB_USER
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: username
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: password
</code></pre>

<blockquote>
<p>kubernetes.io/dockerconfigjson</p>
</blockquote>

<p>可以直接用kubectl命令来创建用于docker registry认证的secret：</p>

<pre><code>$ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
secret &quot;myregistrykey&quot; created.
</code></pre>

<p>也可以直接读取~/.docker/config.json的内容来创建：</p>

<pre><code>$ cat ~/.docker/config.json | base64
$ cat &gt; myregistrykey.yaml &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: myregistrykey
data:
  .dockerconfigjson: UmVhbGx5IHJlYWxseSByZWVlZWVlZWVlZWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGx5eXl5eXl5eXl5eXl5eXl5eXl5eSBsbGxsbGxsbGxsbGxsbG9vb29vb29vb29vb29vb29vb29vb29vb29vb25ubm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg==
type: kubernetes.io/dockerconfigjson
EOF

$ kubectl create -f myregistrykey.yaml
</code></pre>

<p>在创建Pod的时候，通过imagePullSecrets来引用刚创建的myregistrykey:</p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers:
    - name: foo
      image: janedoe/awesomeapp:v1
  imagePullSecrets:
    - name: myregistrykey
</code></pre>

<blockquote>
<p>Service Account</p>
</blockquote>

<p>Service Account用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中。</p>

<pre><code>$ kubectl run nginx --image nginx
deployment &quot;nginx&quot; created
$ kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
nginx-3137573019-md1u2   1/1       Running   0          13s
$ kubectl exec nginx-3137573019-md1u2 ls /run/secrets/kubernetes.io/serviceaccount
ca.crt
namespace
token
</code></pre>

<h2 id="namespace">namespace</h2>

<p>namespace主要是实现资源的隔离，Namespace是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。比如我们将namespace命名为系统名，一个系统一个namespace。</p>

<p>初始化的namespace</p>

<p>在默认情况下，新的集群上有三个命名空间：</p>

<pre><code>default：向集群中添加对象而不提供命名空间，这样它会被放入默认的命名空间中。在创建替代的命名空间之前，该命名空间会充当用户新添加资源的主要目的地，无法删除。

kube-public：kube-public命名空间的目的是让所有具有或不具有身份验证的用户都能全局可读。这对于公开bootstrap组件所需的集群信息非常有用。它主要是由Kubernetes自己管理。

kube-system：kube-system命名空间用于Kubernetes管理的Kubernetes组件，一般规则是，避免向该命名空间添加普通的工作负载。它一般由系统直接管理，因此具有相对宽松的策略。
</code></pre>

<p>创建命名空间</p>

<p>方式一</p>

<pre><code>复制代码
vi ns.yaml
  apiVersion vl
  kind : Namespace
  metadata :
    name: custom-namespace     #这是命名空间的名称

kubectl create -f ns.yaml
</code></pre>

<p>方式二</p>

<pre><code>kubectl create namespace custom-namespace
</code></pre>

<h2 id="resource-quotas">Resource Quotas</h2>

<p>资源配额（Resource Quotas）是用来限制用户资源用量的一种机制。</p>

<p>它的工作原理为</p>

<pre><code>资源配额应用在Namespace上，并且每个Namespace最多只能有一个ResourceQuota对象
开启计算资源配额后，创建容器时必须配置计算资源请求或限制（也可以用LimitRange设置默认值）
用户超额后禁止创建新的资源
</code></pre>

<p>资源配额的启用</p>

<p>首先，在API Server启动时配置ResourceQuota adminssion control；然后在namespace中创建ResourceQuota对象即可。</p>

<p>资源配额的类型</p>

<p>1、计算资源，包括cpu和memory
    cpu, limits.cpu, requests.cpu
    memory, limits.memory, requests.memory</p>

<p>2、存储资源，包括存储资源的总量以及指定storage class的总量</p>

<pre><code>requests.storage：存储资源总量，如500Gi
persistentvolumeclaims：pvc的个数
.storageclass.storage.k8s.io/requests.storage
.storageclass.storage.k8s.io/persistentvolumeclaims
</code></pre>

<p>3、对象数，即可创建的对象的个数</p>

<pre><code>pods, replicationcontrollers, configmaps, secrets
resourcequotas, persistentvolumeclaims
services, services.loadbalancers, services.nodeports
</code></pre>

<p>计算资源示例</p>

<pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    pods: &quot;4&quot;
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
</code></pre>

<p>对象个数示例</p>

<pre><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
spec:
  hard:
    configmaps: &quot;10&quot;
    persistentvolumeclaims: &quot;4&quot;
    replicationcontrollers: &quot;20&quot;
    secrets: &quot;10&quot;
    services: &quot;10&quot;
    services.loadbalancers: &quot;2&quot;
</code></pre>

<blockquote>
<p>LimitRange</p>
</blockquote>

<p>默认情况下，Kubernetes中所有容器都没有任何CPU和内存限制。LimitRange用来给Namespace增加一个资源限制，包括最小、最大和默认资源。比如</p>

<pre><code>apiVersion: v1
kind: LimitRange
metadata:
  name: mylimits
spec:
  limits:
  - max:
      cpu: &quot;2&quot;
      memory: 1Gi
    min:
      cpu: 200m
      memory: 6Mi
    type: Pod
  - default:
      cpu: 300m
      memory: 200Mi
    defaultRequest:
      cpu: 200m
      memory: 100Mi
    max:
      cpu: &quot;2&quot;
      memory: 1Gi
    min:
      cpu: 100m
      memory: 3Mi
    type: Container
</code></pre>

<p>创建limitrange</p>

<pre><code>$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/limits.yaml --namespace=limit-example
limitrange &quot;mylimits&quot; created
$ kubectl describe limits mylimits --namespace=limit-example
Name:   mylimits
Namespace:  limit-example
Type        Resource      Min      Max      Default Request      Default Limit      Max Limit/Request Ratio
----        --------      ---      ---      ---------------      -------------      -----------------------
Pod         cpu           200m     2        -                    -                  -
Pod         memory        6Mi      1Gi      -                    -                  -
Container   cpu           100m     2        200m                 300m               -
Container   memory        3Mi      1Gi      100Mi                200Mi              -
</code></pre>

<blockquote>
<p>配额范围</p>
</blockquote>

<p>每个配额在创建时可以指定一系列的范围</p>

<pre><code>范围  说明
Terminating podSpec.ActiveDeadlineSeconds&gt;=0的Pod
NotTerminating  podSpec.activeDeadlineSeconds=nil的Pod
BestEffort  所有容器的requests和limits都没有设置的Pod（Best-Effort）
NotBestEffort   与BestEffort相反
</code></pre>

<h2 id="ingress">Ingress</h2>

<p>service和pod的IP仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到service在Node上暴露的NodePort上，然后再由kube-proxy将其转发给相关的Pod。</p>

<p>而Ingress就是为进入集群的请求提供路由规则的集合，如下图所示</p>

<pre><code>    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]
</code></pre>

<p>Ingress可以给service提供集群外部访问的URL、负载均衡、SSL终止、HTTP路由等。为了配置这些Ingress规则，集群管理员需要部署一个Ingress controller，它监听Ingress和service的变化，并根据规则配置负载均衡并提供访问入口。</p>

<p>Ingress格式</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        backend:
          serviceName: test
          servicePort: 80
</code></pre>

<p>每个Ingress都需要配置rules，目前Kubernetes仅支持http规则。上面的示例表示请求/testpath时转发到服务test的80端口。</p>

<p>根据Ingress Spec配置的不同，Ingress可以分为以下几种类型：</p>

<blockquote>
<p>单服务Ingress</p>
</blockquote>

<p>单服务Ingress即该Ingress仅指定一个没有任何规则的后端服务。</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  backend:
    serviceName: testsvc
    servicePort: 80
</code></pre>

<p>注：单个服务还可以通过设置Service.Type=NodePort或者Service.Type=LoadBalancer来对外暴露。</p>

<blockquote>
<p>路由到多服务的Ingress</p>
</blockquote>

<p>路由到多服务的Ingress即根据请求路径的不同转发到不同的后端服务上，比如</p>

<pre><code>foo.bar.com -&gt; 178.91.123.132 -&gt; / foo    s1:80
                                 / bar    s2:80
</code></pre>

<p>可以通过下面的Ingress来定义：</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        backend:
          serviceName: s1
          servicePort: 80
      - path: /bar
        backend:
          serviceName: s2
          servicePort: 80
</code></pre>

<p>使用kubectl create -f创建完ingress后：</p>

<pre><code>$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -
          foo.bar.com
          /foo          s1:80
          /bar          s2:80
</code></pre>

<blockquote>
<p>虚拟主机Ingress</p>
</blockquote>

<p>虚拟主机Ingress即根据名字的不同转发到不同的后端服务上，而他们共用同一个的IP地址，如下所示</p>

<pre><code>foo.bar.com --|                 |-&gt; foo.bar.com s1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-&gt; bar.foo.com s2:80
</code></pre>

<p>下面是一个基于Host header路由请求的Ingress：</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: s1
          servicePort: 80
  - host: bar.foo.com
    http:
      paths:
      - backend:
          serviceName: s2
          servicePort: 80
</code></pre>

<p>注：没有定义规则的后端服务称为默认后端服务，可以用来方便的处理404页面。</p>

<blockquote>
<p>TLS Ingress</p>
</blockquote>

<p>TLS Ingress通过Secret获取TLS私钥和证书(名为tls.crt和tls.key)，来执行TLS终止。如果Ingress中的TLS配置部分指定了不同的主机，则它们将根据通过SNI TLS扩展指定的主机名（假如Ingress controller支持SNI）在多个相同端口上进行复用。</p>

<p>定义一个包含tls.crt和tls.key的secret：</p>

<pre><code>apiVersion: v1
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
kind: Secret
metadata:
  name: testsecret
  namespace: default
type: Opaque
</code></pre>

<p>Ingress中引用secret：</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: no-rules-map
spec:
  tls:
    - secretName: testsecret
  backend:
    serviceName: s1
    servicePort: 80
</code></pre>

<p>注意，不同Ingress controller支持的TLS功能不尽相同。 请参阅有关nginx，GCE或任何其他Ingress controller的文档，以了解TLS的支持情况。</p>

<blockquote>
<p>更新Ingress</p>
</blockquote>

<p>可以通过kubectl edit ing name的方法来更新ingress：</p>

<pre><code>$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -                       178.91.123.132
          foo.bar.com
          /foo          s1:80

$ kubectl edit ing test
</code></pre>

<p>这会弹出一个包含已有IngressSpec yaml文件的编辑器，修改并保存就会将其更新到kubernetes API server，进而触发Ingress Controller重新配置负载均衡：</p>

<pre><code>spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: s1
          servicePort: 80
        path: /foo
  - host: bar.baz.com
    http:
      paths:
      - backend:
          serviceName: s2
          servicePort: 80
        path: /foo
..
</code></pre>

<p>更新后：</p>

<pre><code>$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -                       178.91.123.132
          foo.bar.com
          /foo          s1:80
          bar.baz.com
          /foo          s2:80
</code></pre>

<p>当然，也可以通过kubectl replace -f new-ingress.yaml命令来更新，其中new-ingress.yaml是修改过的Ingress yaml。</p>

<h2 id="dns">DNS</h2>

<p>kubernetes 提供了 service 的概念可以通过 VIP 访问 pod 提供的服务，但是在使用的时候还有一个问题：怎么知道某个应用的 VIP？比如我们有两个应用，一个 app，一个 是 db，每个应用使用 rc 进行管理，并通过 service 暴露出端口提供服务。app 需要连接到 db 应用，我们只知道 db 应用的名称，但是并不知道它的 VIP 地址。</p>

<p>最简单的办法是从 kubernetes 提供的 API 查询。但这是一个糟糕的做法，首先每个应用都要在启动的时候编写查询依赖服务的逻辑，这本身就是重复和增加应用的复杂度；其次这也导致应用需要依赖 kubernetes，不能够单独部署和运行（当然如果通过增加配置选项也是可以做到的，但这又是增加负责度）。</p>

<p>开始的时候，kubernetes 采用了 docker 使用过的方法——环境变量。每个 pod 启动时候，会把通过环境变量设置所有服务的 IP 和 port 信息，这样 pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息。这种方式服务和环境变量的匹配关系有一定的规范，使用起来也相对简单，但是有个很大的问题：依赖的服务必须在 pod 启动之前就存在，不然是不会出现在环境变量中的。</p>

<p>更理想的方案是：应用能够直接使用服务的名字，不需要关心它实际的 ip 地址，中间的转换能够自动完成。名字和 ip 之间的转换就是 DNS 系统的功能，因此 kubernetes 也提供了 DNS 方法来解决这个问题。</p>

<blockquote>
<p>方案</p>
</blockquote>

<p>DNS 有两种配置方式，在 1.3 之前使用 etcd + kube2sky + skydns 的方式，在 1.3 之后可以使用 kubedns + dnsmasq 的方式。从 Kubernetes v1.12 开始，CoreDNS 是推荐的 DNS 服务器，取代了kube-dns。</p>

<blockquote>
<p>使用</p>
</blockquote>

<p>不管以什么方式启动，对外的效果是一样的。要想使用 DNS 功能，还需要修改 kubelet 的启动配置项，告诉 kubelet，给每个启动的 pod 设置对应的 DNS 信息，一共有两个参数：&ndash;cluster_dns=10.10.10.10 &ndash;cluster_domain=cluster.local，分别是 DNS 在集群中的 vip 和域名后缀，要和 DNS rc 中保持一致。</p>

<h1 id="原理">原理</h1>

<h2 id="服务发现">服务发现</h2>

<p><img src="/media/cloud/k8s/service" alt="" /></p>

<p>如上图所示，K8s 服务发现以及 K8s Service 是这样整体的一个架构。</p>

<p>K8s 分为 master 节点和 worker 节点：</p>

<pre><code>master 里面主要是 K8s 管控的内容；
worker 节点里面是实际跑用户应用的一个地方。
</code></pre>

<p>在 K8s master 节点里面有 APIServer，就是统一管理 K8s 所有对象的地方，所有的组件都会注册到 APIServer 上面去监听这个对象的变化，比如说我们刚才的组件 pod 生命周期发生变化，这些事件。</p>

<p>这里面最关键的有三个组件：</p>

<pre><code>一个是 Cloud Controller Manager，负责去配置 LoadBalancer 的一个负载均衡器给外部去访问；
另外一个就是 Coredns，就是通过 Coredns 去观测 APIServer 里面的 service 后端 pod 的一个变化，去配置 service 的 DNS 解析，实现可以通过 service 的名字直接访问到 service 的虚拟 IP，或者是 Headless 类型的 Service 中的 IP 列表的解析；
然后在每个 node 里面会有 kube-proxy 这个组件，它通过监听 service 以及 pod 变化，然后实际去配置集群里面的 node pod 或者是虚拟 IP 地址的一个访问。
</code></pre>

<p>实际访问链路是什么样的呢？比如说从集群内部的一个 Client Pod3 去访问 Service，就类似于刚才所演示的一个效果。Client Pod3 首先通过 Coredns 这里去解析出 ServiceIP，Coredns 会返回给它 ServiceName 所对应的 service IP 是什么，这个 Client Pod3 就会拿这个 Service IP 去做请求，它的请求到宿主机的网络之后，就会被 kube-proxy 所配置的 iptables 或者 IPVS 去做一层拦截处理，之后去负载均衡到每一个实际的后端 pod 上面去，这样就实现了一个负载均衡以及服务发现。</p>

<p>对于外部的流量，比如说刚才通过公网访问的一个请求。它是通过外部的一个负载均衡器 Cloud Controller Manager 去监听 service 的变化之后，去配置的一个负载均衡器，然后转发到节点上的一个 NodePort 上面去，NodePort 也会经过 kube-proxy 的一个配置的一个 iptables，把 NodePort 的流量转换成 ClusterIP，紧接着转换成后端的一个 pod 的 IP 地址，去做负载均衡以及服务发现。这就是整个 K8s 服务发现以及 K8s Service 整体的结构。</p>

<blockquote>
<p>k8s提供了两种方式进行服务发现：</p>
</blockquote>

<p>1、环境变量： 当你创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。需要注意的是，要想一个Pod中注入某个Service的环境变量，则必须Service要先比该Pod创建。这一点，几乎使得这种方式进行服务发现不可用。</p>

<p>比如，一个ServiceName为redis-master的Service，对应的ClusterIP:Port为10.0.0.11:6379，则其对应的环境变量为：</p>

<pre><code>REDIS_MASTER_SERVICE_HOST=10.0.0.11
REDIS_MASTER_SERVICE_PORT=6379
REDIS_MASTER_PORT=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP_PROTO=tcp
REDIS_MASTER_PORT_6379_TCP_PORT=6379
REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11
</code></pre>

<p>2、DNS：这也是k8s官方强烈推荐的方式。可以通过cluster add-on的方式轻松的创建KubeDNS来对集群内的Service进行服务发现。更多关于DNS的内容在上面有说明。</p>

<h2 id="安全机制">安全机制</h2>

<p><img src="/media/cloud/k8s/safe" alt="" /></p>

<p>Kubernetes 官方文档给出了上面这张图。描述了用户在访问或变更资源的之前，需要经过 APIServer 的认证机制、授权机制以及准入控制机制。这三个机制可以这样理解，先检查是否合法用户，再检查该请求的行为是否有权限，最后做进一步的验证或添加默认参数。</p>

<blockquote>
<p>用户</p>
</blockquote>

<p>Kubernetes 中有两种用户，一种是内置“用户” ServiceAccount，另一种我称之为自然人。</p>

<p>1、Service Account</p>

<p>Service Account它并不是给kubernetes集群的用户使用的，而是给pod里面的进程使用的，它为pod提供必要的身份认证。</p>

<pre><code>kubectl get sa --all-namespaces

NAMESPACE     NAME          SECRETS   AGE
default       build-robot   1         1d
default       default       1         32d
default       kube-dns      1         31d
kube-public   default       1         32d
kube-system   dashboard     1         31d
kube-system   default       1         32d
kube-system   heapster      1         30d
kube-system   kube-dns      1         31d
</code></pre>

<p>如果kubernetes开启了ServiceAccount（–admission_control=…,ServiceAccount,… ）那么会在每个namespace下面都会创建一个默认的default的sa。其中最重要的就是secrets，它是每个sa下面都会拥有的一个加密的token</p>

<pre><code>kubectl get sa  default  -o yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2017-05-02T06:39:12Z
  name: default
  namespace: default
  resourceVersion: &quot;175&quot;
  selfLink: /api/v1/namespaces/default/serviceaccounts/default
  uid: 0de23575-2f02-11e7-98d0-5254c4628ad9
secrets:
- name: default-token-rsf8r
</code></pre>

<p>当用户再该namespace下创建pod的时候都会默认使用这个sa，kubernetes会把默认的sa挂载到容器内。</p>

<p>看一下这个secret</p>

<pre><code>kubectl get secret default-token-rsf8r -o yaml
apiVersion: v1
data:
  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR2akNDQXFhZ0F3SUJBZ0lVZlpvZDJtSzNsa3JiMzR3NDhhUmtOc0pVVDJjd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbAphVXBwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEUzTURVd01qQTNNekF3TUZvWERUSXlNRFV3TVRBM016QXdNRm93WlRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbGFVcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByZFdKbGNtNWxkR1Z6Ck1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBc2E5Zk1HVGd2MGl0YnlZcHoycXkKOThKWktXdWdFL0VPbXRYS2ExT0Y3ekUxSFh1cDFOVG8rNkhvUEFuR3hhVzg4Q0s0TENrbWhNSGFLdUxnT3IvVApOMGphdnc5YWlPeVdYR1hXUUxVN3U0aVhoaDV6a2N4bmZxRW9JOW9JV2dMTzVEL3hBL0tnZzRQZDRMeFdqMkFQCk4rcVdxQ2crU3BrdkpIQUZWL3IyTk1BbEIzNHBrK0t5djVQMDJSQmd6Y2xTeSs5OUxDWnlIQ1VocGl0TFFabHoKdUNmeGtBeUNoWFcxMWNKdVFtaDM4aFVKa0dhUW9OVDVSNmtoRTArenJDVjVkWnNVMVZuR0FydWxaWXpJY3kregpkeUZpYWYyaitITyt5blg4RUNySzR1TUF3Nk4zN1pnNjRHZVRtbk5EWmVDTTlPelk5czBOVzc1dHU5bHJPZTVqCnZRSURBUUFCbzJZd1pEQU9CZ05WSFE4QkFmOEVCQU1DQVFZd0VnWURWUjBUQVFIL0JBZ3dCZ0VCL3dJQkFqQWQKQmdOVkhRNEVGZ1FVK2RqMThRUkZyMWhKMVhGb1VyYUVVRnpEeVRBd0h3WURWUjBqQkJnd0ZvQVUrZGoxOFFSRgpyMWhKMVhGb1VyYUVVRnpEeVRBd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFBazQ4ODZBa0Fpa3VBVWRiOWU1CitldkVXVVFFaTIyTmc4REhmVTVSbXppU2ZhVllFQ1FuTlBUREprMmYvTm1Kb3RUVWxRZS9Ec3BkNEk1TFova1IKMGI2b1VoZkdmTkVOOXVObkkvZEgzOFBjUTNDaWtVeHhaeFRYTytaaldxcGNHZTRLNzZtaWd2ZWhQR2Z1VUNzQwp0UmZkZDM2YkhnRjN4MzRCWnc5MStDQ2VKQzBSWmNjVENqcHFHUEZFQlM3akJUVUlRVjNodnZycWJMV0hNeTJuCnFIck94UFI1eFkrRU5SQ0xzVWNSdk9icUhBK1g0c1BTdzBwMWpROXNtK1lWNG1ybW9Gd1RyS09kK2FqTVhzVXkKL3ZRYkRzNld4RWkxZ2ZvR3BxZFN6U1k0MS9IWHovMjZWNlFWazJBajdQd0FYZmszYk1wWHdDamRXRG4xODhNbQpXSHM9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
  namespace: ZGVmYXVsdA==
  token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbVJsWm1GMWJIUXRkRzlyWlc0dGNuTm1PSElpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWkdWbVlYVnNkQ0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJakJrWlRJek5UYzFMVEptTURJdE1URmxOeTA1T0dRd0xUVXlOVFJqTkRZeU9HRmtPU0lzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT21SbFptRjFiSFFpZlEuSmxuamM0Y0xNYkZrRlJVQjIyWGtFN2t4bTJ1dS1aQm9sUTh4VEdDNmdLOTdSZTVOMzBuY2V0SWJsanVOVWFlaDhtMDk2R19nMHE3cmRvWm5XMTV2OFBVXzNyM1hWWlBNc1lxbGRpZlNJbWtReXFqeEphVlBka3Izam5GWVBkVWNaTmk3MFF3cWtEdm5sMXB4SFRNZTZkTVNPTlExbUVoMHZSbHBhRTdjVWtTVlg5blRzaFVJVTVXWE9wRUxwdTVjVjBHV3ZGeDRDSzR6Umt3clNMdlV5X2d5UGZwLWdYVFZQWU80NkJKSWZtaVhlZGhVaW9nempPN285eGxDbUxQVkhyNkFIZGViNExiTVA1dkJ2MlBSZ2RrMW9keTR0VEdxLVRGU3M2VkNoMTZ4dk5IdTRtRVN5TjZmcXVObzJwYUFOelY4b251aTJuaU4yNTU1TzN4SFdR
kind: Secret
metadata:
  annotations:
    kubernetes.io/service-account.name: default
    kubernetes.io/service-account.uid: 0de23575-2f02-11e7-98d0-5254c4628ad9
  creationTimestamp: 2017-05-02T06:42:07Z
  name: default-token-rsf8r
  namespace: default
  resourceVersion: &quot;12551&quot;
  selfLink: /api/v1/namespaces/default/secrets/default-token-rsf8r
  uid: 75c0a236-2f02-11e7-98d0-5254c4628ad9
type: kubernetes.io/service-account-token
</code></pre>

<p>上面的内容是经过base64加密过后的，我们直接进入容器内</p>

<pre><code>~ # ls -l  /var/run/secrets/kubernetes.io/serviceaccount/
total 0
lrwxrwxrwx    1 root     root            13 May  4 23:57 ca.crt -&gt; ..data/ca.crt
lrwxrwxrwx    1 root     root            16 May  4 23:57 namespace -&gt; ..data/namespace
lrwxrwxrwx    1 root     root            12 May  4 23:57 token -&gt; ..data/token
</code></pre>

<p>可以看到已将ca.crt 、namespace和token放到容器内了，那么这个容器就可以通过https的请求访问apiserver了。</p>

<p>2、自然人</p>

<p>所谓自然人就是指区别于 pod 等资源概念的“人”，可以理解成实际操作 &ldquo;kubectl&rdquo; 命令的人，也就是我们的平常的用户。admin 可以分发私钥，但自然人可以储存类似 KeyStone 甚至包含账号密码的文件，所以 k8s 中没有对自然人以 API 对象描述之。</p>

<blockquote>
<p>认证机制（Authentication）</p>
</blockquote>

<p>k8s 中的认证机制，是在用户访问 APIServer 的第一步。通常是一个完整的 HTTP 请求打过来，但是这一步往往只检测请求头或客户端证书。</p>

<p>1、CA证书</p>

<p>基于https的证书认证，需要了解证书的机制，签发证书。比如</p>

<pre><code>X509 Client Certs: 客户端证书模式需要在 kubectl 命令中加入 --client-ca-file=&lt;SOMEFILE&gt; 参数，指明证书所在位置。
</code></pre>

<p>2、http token</p>

<pre><code>Static Token File: --token-auth-file=&lt;SOMEFILE&gt; 参数指明 bearer tokens 所在位置。
bearer tokens: 在 HTTP 请求头中加入 Authorization: Bearer &lt;TOKEN&gt;。
Bootstrap Tokens: 与 bearer tokens 一致，但 TOKEN 格式为 [a-z0-9]{6}.[a-z0-9]{16}。该方式称为 dynamically-managed Bearer token，以 secret 的方式保存在 kube-system namespace 中，可以被动态的创建和管理。同时，启用这种方式还需要在 APIServer 中打开 --enable-bootstrap-token-auth ，这种方式还处于 alpha 阶段。
</code></pre>

<p>3、http base</p>

<pre><code>Static Password File: 以参数 --basic-auth-file=&lt;SOMEFILE&gt; 指明 basic auth file 的位置。这个 basic auth file 以 csv 文件的形式存在，里面至少包含三个信息：password、username、user id，同时该模式在使用时需要在请求头中加入 Authorization: Basic BASE64ENCODED(USER:PASSWORD) 。
或者将用户名密码用base64加密后放在http request中的Authorization中
</code></pre>

<blockquote>
<p>授权机制（Authorization）</p>
</blockquote>

<p>当用户通过认证后，k8s 的授权机制将对用户的行为等进行授权检查。换句话说，就是对这个请求本身，是否对某资源、某 namespace、某操作有权限限制。</p>

<p>若要开启某种模式，需要在 APIServer 启动时，设置参数 &ndash;authorization-mode=RBAC。授权机制目前有 4 种模式：RBAC、ABAC、Node、Webhook。</p>

<blockquote>
<p>1、 RBAC</p>
</blockquote>

<p>基于角色的权限访问控制，就是对某个用户赋予某个角色，而这个角色通常决定了对哪些资源拥有怎样的权限。</p>

<p>首先需要用户，所以我们正常创建sa，然后给他授权</p>

<p>1、创建sa</p>

<p>创建一个 ServiceAccount 很简单，只需要指定其所在 namespace 和 name 即可。</p>

<pre><code>apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: hdls
  name: hdls-sa
</code></pre>

<p>2、定义角色</p>

<p>RBAC 中最重要的概念就是 Role 和 RoleBinding。</p>

<pre><code>Role 定义了一组对 Kubernetes API 对象的操作权限。
RoleBinding 则定义的是具体的 ServiceAccount 和 Role 的对应关系。
</code></pre>

<p>实例</p>

<pre><code>kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    namespace: hdls
    name: hdls-role
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods&quot;]
  verbs: [&quot;get&quot;, &quot;list&quot;]
</code></pre>

<p>1、namespace： 在这里仅限于逻辑上的“隔离”，并不会提供任何实际的隔离或者多租户能力；</p>

<p>2、rules：定义的是权限规则，允许“被作用者”，对 hdls 下面的 Pod 对象，进行 GET 和 LIST 操作；</p>

<pre><code>apiGroups：为 &quot;&quot; 代表 core API Group；
resources：指的是资源类型
verbs： 指的是具体的操作，当前 Kubernetes（v1.11）里能够对 API 对象进行的所有操作有 &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;。
</code></pre>

<p>3、绑定角色</p>

<p>RoleBinding 则定义的是具体的 ServiceAccount 和 Role 的对应关系。</p>

<pre><code>kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    name: hdls-rolebinding
    namespace: hdls
subjects:
- kind: ServiceAccount
    name: hdls-sa
    apiGroup: rbac.authorization.k8s.io
roleRef:
    kind: Role
    name: hdls-role
    apiGroup: rbac.authorization.k8s.io
</code></pre>

<p>1、这个 RoleBinding 对象里定义了一个 subjects 字段，即“被作用者”。它的类型是 ServiceAccount，就是上面创建的 sa。这个 subjects 还可以是 User 和 Group，User 是指 k8s 里的用户，而 Group 是指 ServiceAccounts。</p>

<p>2、roleRef 字段是用来直接通过名字，引用我们前面定义的 Role 对象（hdls-role），从而定义了 Subject 和 Role 之间的绑定关系。</p>

<p>我们再用 kubectl get sa -n hdls -o yaml 命令查看之前的 ServiceAccount，就可以看到 ServiceAccount.secret，这是因为 k8s 会为一个 ServiceAccount 自动创建并分配一个 Secret 对象，而这个 Secret 就是用来跟 APIServer 进行交互的授权文件： Token。Token 文件的内容一般是证书或者密码，以一个 Secret 对象的方式保存在 etcd 当中。
这个时候，我们在我们的 Pod 的 YAML 文件中定义字段 .spec.serviceAccountName 为上面的 ServiceAccount name 即可声明使用。</p>

<p>如果一个 Pod 没有声明 serviceAccountName，Kubernetes 会自动在它的 Namespace 下创建一个名叫 default 的默认 ServiceAccount，然后分配给这个 Pod。然而这个默认 ServiceAccount 并没有关联任何 Role。也就是说，此时它有访问 APIServer 的绝大多数权限。</p>

<p>4、cluster</p>

<p>Role 和 RoleBinding 对象都是 Namespaced 对象，它们只对自己的 Namespace 内的资源有效。</p>

<p>而某个 Role 需要对于非 Namespaced 对象（比如：Node），或者想要作用于所有的 Namespace 的时候，我们需要使用 ClusterRole 和 ClusterRoleBinding 去做授权。</p>

<p>这两个 API 对象的用法跟 Role 和 RoleBinding 完全一样。只不过，它们的定义里，没有了 Namespace 字段。</p>

<p>Kubernetes 已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。一般来说，这些系统级别的 ClusterRole，是绑定给 Kubernetes 系统组件对应的 ServiceAccount 使用的。</p>

<p>Kubernetes 还提供了四个内置的 ClusterRole 来供用户直接使用：</p>

<pre><code>cluster-admin：整个集群的最高权限。如果在 ClusterRoleBinding 中使用，意味着在这个集群中的所有 namespace 中的所有资源都拥有最高权限，为所欲为；如果在 RoleBinding 中使用，即在某个 namespace 中为所欲为。
admin：管理员权限。如果在 RoleBinding 中使用，意味着在某个 namespace 中，对大部分资源拥有读写权限，包括创建 Role 和 RoleBinding 的权限，但没有对资源 quota 和 namespace 本身的写权限。
edit：写权限。在某个 namespace 中，拥有对大部分资源的读写权限，但没有对 Role 和 RoleBinding 的读写权限。
view：读权限。在某个 namespace 中，仅拥有对大部分资源的读权限，没有对 Role 和 RoleBinding 的读权限，也没有对 seccrets 的读权限。
</code></pre>

<blockquote>
<p>2、ABAC</p>
</blockquote>

<p>基于属性的权限访问控制。若要开启该模式，需要在 APIServer 启动时，开启 &ndash;authorization-policy-file=<SOME_FILENAME> 和 &ndash;authorization-mode=ABAC 两个参数。</p>

<p>json 对象的格式来定义权限</p>

<p>与 Yaml 文件一致，必须描述的属性有 apiVersion、kind、spec，而 spec 里描述了具体的用户、资源和行为。看个例子：</p>

<pre><code>{&quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: {&quot;user&quot;: &quot;bob&quot;, &quot;namespace&quot;: &quot;projectCaribou&quot;, &quot;resource&quot;: &quot;pods&quot;, &quot;readonly&quot;: true}}
</code></pre>

<p>这就描述了用户 bob 只有在 namespace projectCaribou 下对 pod 的读权限。</p>

<blockquote>
<p>3、Node</p>
</blockquote>

<p>Node 授权机制是一种特殊的模式，是 kubelet 发起的请求授权。开启该模式，需要开启参数 &ndash;authorization-mode=Node。</p>

<p>通过启动 &ndash;enable-admission-plugins=&hellip;,NodeRestriction,&hellip;，来限制 kubelet 访问 node，endpoint、pod、service以及secret、configmap、PV 和 PVC 等相关的资源。</p>

<blockquote>
<p>4、Webhook</p>
</blockquote>

<p>Webhook 模式是一种 HTTP 回调模式，是一种通过 HTTP POST 方式实现的简单事件通知。该模式需要 APIServer 配置参数 –authorization-webhook-config-file=<SOME_FILENAME>，HTTP 配置文件的格式跟 kubeconfig 的格式类似。</p>

<blockquote>
<p>准入控制（Admission Controllers）</p>
</blockquote>

<p>在一个请求通过了认证机制和授权认证后，需要经过最后一层筛查，即准入控制。这个准入控制模块的代码通常在 APIServer 中，并被编译到二进制文件中被执行。这一层安全检查的意义在于，检查该请求是否达到系统的门槛，即是否满足系统的默认设置，并添加默认参数。</p>

<p>准入控制以插件的形式存在
开启的方式为：</p>

<pre><code>kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ...
</code></pre>

<p>关闭的方式为：</p>

<pre><code>kube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ...
</code></pre>

<p>常用的准入控制插件有：</p>

<pre><code>AlwaysAdmit：允许所有请求通过，被官方反对，因为没有实际意义；
AlwaysPullImages：将每个 pod 的 image pull policy 改为 always，在多租户的集群被使用；
AlwaysDeny：禁止所有请求通过，被官方反对，因为没有实际意义；
DefaultStorageClass：为每个 PersistentVolumeClaim 创建默认的 PV；
DefaultTolerationSeconds：如果 pod 对污点 node.kubernetes.io/not-ready:NoExecute 和 node.alpha.kubernetes.io/unreachable:NoExecute 没有容忍，为其创建默认的 5 分钟容忍 notready:NoExecute 和unreachable:NoExecute；
LimitRanger：确保每个请求都没有超过其 namespace 下的 LimitRange，如果在 Deployment 中使用了 LimitRange 对象，该准入控制插件必须开启；
NamespaceAutoProvision：检查请求中对应的 namespace 是否存在，若不存在自动创建；
NamespaceExists：检查请求中对应的 namespace 是否存在，若不存在拒绝该请求；
NamespaceLifecycle：保证被删除的 namespace 中不会创建新的资源；
NodeRestriction：不允许 kubelet 修改 Node 和 Pod 对象；
PodNodeSelector：通过读取 namespace 的注解和全局配置，来控制某 namespace 下哪些 label 选择器可被使用；
PodPreset：满足预先设置的标准的 pod 不允许被创建；
Priority：通过 priorityClassName 来决定优先级；
ResourceQuota：保证 namespace 下的资源配额；
ServiceAccount：保证 ServiceAccount 的自动创建，如果用到 ServiceAccount，建议开启；
</code></pre>

<h2 id="网路原理-todo">网路原理（todo）</h2>

<p>Kubernetes中有三种网络和三种IP。</p>

<p><img src="/media/cloud/k8s/network" alt="" /></p>

<p>为了便于理解，我把Node IP比作tcp/ip网络结构中的第二层地址，mac地址。通过它去寻找节点的物理IP地址，而这个地址通常通常对Kubernetes里的Pod来说是透明的，不用知道其他Pod的Node IP，通过Pod IP就能访问到。所以Pod IP被我看做是网络结构中的三层IP地址。而Cluster IP更像是一个域名，不用知道背后到底有哪些Pod，他们又分布在哪里。当然，这只是一个不太恰当的比喻，但是我觉得背后的设计逻辑还是有点相通的地方。</p>

<h3 id="docker网络实现-posts-cloud-container-docker-docker-principle"><a href="/posts/cloud/container/docker/docker-principle/">docker网络实现</a></h3>

<p>用过docker基本都知道，启动docker engine后，主机的网络设备里会有一个docker0的网关，而容器默认情况下会被分配在一个以docker0为网关的虚拟子网中。</p>

<pre><code>root@VM-66-197-ubuntu:/home/ubuntu# ifconfig
...
docker0   Link encap:Ethernet  HWaddr 02:42:ec:43:56:b2
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
...
root@VM-66-197-ubuntu:/home/ubuntu# docker inspect nginx
···
&quot;IPAddress&quot;: &quot;172.17.0.2&quot;,
···
</code></pre>

<p>为了实现上述功能，docker主要用到了linux的Bridge、Network Namespace、VETH。</p>

<pre><code>Bridge相当于是一个虚拟网桥，工作在第二层网络。也可以为它配置IP，工作在三层网络。docker0网关就是通过Bridge实现的。
Network Namespace是网络命名空间，通过Network Namespace可以建立一些完全隔离的网络栈。比如通过docker network create xxx就是在建立一个Network Namespace。
VETH是虚拟网卡的接口对，可以把两端分别接在两个不同的Network Namespace中，实现两个原本隔离的Network Namespace的通信。
</code></pre>

<p>所以总结起来就是：Network Namespace做了容器和宿主机的网络隔离，Bridge分别在容器和宿主机建立一个网关，然后再用VETH将容器和宿主机两个网络空间连接起来。</p>

<blockquote>
<p>CNM</p>
</blockquote>

<p><img src="/media/cloud/k8s/network2" alt="" /></p>

<p>基于上面的网络实现，docker的容器网络管理项目libnetwork提出了CNM（container network model）。</p>

<pre><code>Sandbox：每个沙盒包含一个容器网络栈(network stack)的配置，配置包括：容器的网口、路由表和DNS设置等。
Endpoint：通过Endpoint，沙盒可以被加入到一个Network里。
Network：一组能相互直接通信的Endpoints。
</code></pre>

<p>Sandbox对应于Network Namespace， Endpoint对应于VETH， Network对应于Bridge。</p>

<p>更多docker的网络实现可以参考：<a href="https://tonybai.com/2017/01/17/understanding-flannel-network-for-kubernetes/">https://tonybai.com/2017/01/17/understanding-flannel-network-for-kubernetes/</a>.</p>

<h3 id="pod-network">Pod Network</h3>

<p>Kubernetes的一个Pod中包含有多个容器，这些容器共享一个Network Namespace，更具体的说，是共享一个Network Namespace中的一个IP。创建Pod时，首先会生成一个pause容器，然后其他容器会共享pause容器的网络。</p>

<pre><code>root@kube-2:~# docker ps
CONTAINER ID        IMAGE                                            COMMAND                  CREATED             STATUS              PORTS               NAMES
d2dbb9e288e2        mirrorgooglecontainers/pause-amd64:3.0           &quot;/pause&quot;                 3 weeks ago         Up 3 weeks                              k8s_POD_frontend-647d9fdddf-n4x9w_default_9f275ea8-4853-11e8-8c42-fa163e4a07e5_0
cf1bfff28238        nginx                                            &quot;nginx -g 'daemon of…&quot;   3 weeks ago         Up 3 weeks                              k8s_nginx-demo_frontend-647d9fdddf-n4x9w_default_9f275ea8-4853-11e8-8c42-fa163e4a07e5_0

root@kube-2:~# docker inspect cf1bf
...
            &quot;NetworkMode&quot;: &quot;container:d2dbb9e288e26231759e28e8d4816862c6c57d4d2822a259bee7fcc9a2fd0b20&quot;,
...
</code></pre>

<p>可以看出，在这个Pod中，nginx容器通过&rdquo;NetworkMode&rdquo;: &ldquo;container:d2db&hellip;&ldquo;与pause容器共享了网络。这时候，相同容器之间的访问只需要用localhost+端口的形式，就像他们是部署在同一台物理机的不同进程一样，可以使用本地IPC进行通信。</p>

<p>然而pause的ip又是从哪里分配到的？如果还是用一个以docker0为网关的内网ip就会出现问题了。</p>

<pre><code>docker默认的网络是为同一台宿主机的docker容器通信设计的，Kubernetes的Pod需要跨主机与其他Pod通信，所以需要设计一套让不同Node的Pod实现透明通信（without NAT）的机制。
docker0的默认ip是172.17.0.1，docker启动的容器也默认被分配在172.17.0.1/16的网段里。跨主机的Pod通信要保证Pod的ip不能相同，所以还需要设计一套为Pod统一分配IP的机制。
</code></pre>

<p>以上两点，就是Kubernetes在Pod network这一层需要解决的问题。幸运的是有很多的网络工具已经实现了上述功能，所以Kubernetes选择了与这些工具结合。官方文档上可以看看有哪些网络插件可以使用：<a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this">https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this</a></p>

<blockquote>
<p>flannel网络插件</p>
</blockquote>

<p>我在部署Kubernetes的时候，使用的是flannel，所以就根据flannel来说一下。在第二篇中说到过，etcd在Node中充当网络插件的网络状态同步组件。而flannel的功能实现就依赖于etcd。</p>

<p><img src="/media/cloud/k8s/network3" alt="" /></p>

<p>首先在启动Kubernetes Controller Manager时，需要指定集群的pod ip范围：&ndash;cluster-cidr=172.16.0.0/16,并开启为Node分配ip选项：&ndash;allocate-node-cidrs=true(参考kubeasz的配置)。Controller Manager会把为每个Node分配的IP范围保存到etcd中。</p>

<p>新建Pod时，flannel会从etcd中取出属于该Node的ip，分配给Pod，再在etcd中记录下这个Pod的IP。这样etcd中就会存有一张Node IP与Pod IP对应的“路由表”。</p>

<p>当Pod需要跨Node通信时，数据包经过Node中的路由会到flannel中，flannel通过etcd查询到目的Pod IP的Node IP，使用flannel的Backends对数据包进行分装，发送给目的Node处理。目的Node拿到数据包后解开封装，拿到原始数据包，再通过Node的路由送到相应的Pod。</p>

<p>flannel的Backends有多种实现方式：VXLAN、UDP、gce&hellip;..具体参考官方文档。官方推荐的是VXLAN，之前介绍docker swarm时就提到过，swarm的overlay网络也是通过VXLAN实现的。关于vxlan的具体实现原理可以参考《vxlan 协议原理简介》。</p>

<p>flannel为Pod分配ip有不同的实现方式，Kubernetes推荐的是基于CNI，另一种是直接与docker结合，我觉得是在CNM的基础上做出的修改。</p>

<blockquote>
<p>基于CNI（container network interface）</p>
</blockquote>

<p>Container Network Interface (CNI) 最早是由CoreOS发起的容器网络规范，是Kubernetes网络插件的基础。其基本思想为：Container Runtime在创建容器时，先创建好network namespace，然后调用CNI插件为这个netns配置网络，其后再启动容器内的进程。现已加入CNCF，成为CNCF主推的网络模型。</p>

<p><img src="/media/cloud/k8s/network4" alt="" /></p>

<p>这个协议连接了两个组件：容器管理系统和网络插件。它们之间通过 JSON 格式的文件进行通信，实现容器的网络功能。具体的事情都是插件来实现的，包括：创建容器网络空间（network namespace）、把网络接口（interface）放到对应的网络空间、给网络接口分配 IP 等等。</p>

<p>使用CNI后，容器的IP分配就变成了如下步骤：</p>

<pre><code>kubelet 先创建pause容器生成network namespace
调用网络CNI driver
CNI driver 根据配置调用具体的cni 插件
cni 插件给pause 容器配置网络
pod 中其他的容器都使用 pause 容器的网络
</code></pre>

<p><img src="/media/cloud/k8s/network5" alt="" /></p>

<p>这时候Pod就直接以cni0作为了自己的网关，而不是docker默认的docker0。所以使用docker inspect查看某个pause容器时，是看不到它的网络信息的。</p>

<blockquote>
<p>基于docker CNM</p>
</blockquote>

<p>这种方式需要把/run/flannel/subnet.env中的内容写到docker的环境变量配置文件/run/flannel/docker中，然后在docker engine启动时带上相应参数EnvironmentFile=-/run/flannel/docker。这样docker0的ip地址就会是flannel为该Node分配的地址了。</p>

<p><img src="/media/cloud/k8s/network6" alt="" /></p>

<p>这样子Pod IP是docker engine分配的，Pod也是以docker0为网关，通过veth连接network namespace，符合CNM中的定义。</p>

<blockquote>
<p>比较</p>
</blockquote>

<pre><code>相比起来，明显是Kubernetes推荐的CNI模式要好一些。
</code></pre>

<p>CNI中，docker0的ip与Pod无关，Pod总是生成的时候才去动态的申请自己的IP，而CNM模式下，Pod的网段在docker engine启动时就已经决定。</p>

<p>CNI只是一个网络接口规范，各种功能都由插件实现，flannel只是插件的一种，而且docker也只是容器载体的一种选择，Kubernetes还可以使用其他的，比如rtk&hellip;官方博客也对此做过说明：Why Kubernetes doesn’t use libnetwork</p>

<h3 id="service-network">Service Network</h3>

<p>我觉得Service IP只能算一种假IP，除了在iptables中有相应的规则链，并没有任何网络链路的底层实现。所以我觉得它更像是一种集群内的“域名”，通过kube-proxy设置的iptables规则进行“DNS”解析，最终访问到对应的Pod。具体可以看看上一篇中关于kube-proxy的内容。</p>

<blockquote>
<p>外部访问Service</p>
</blockquote>

<p>另外这个虚拟的Service IP(Cluster IP)只能在集群内部才能访问到，如果要从外部访问，可以用一下几种方式：</p>

<p>1、Porxy API</p>

<p>这种是直接使用apiserver，通过Proxy API将访问请求转发到对应服务的ClusterIP上。比如访问grafana服务：</p>

<pre><code>http://localhost:8080/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy/
</code></pre>

<p><img src="/media/cloud/k8s/clusterip" alt="" /></p>

<p>2、NodePort</p>

<p>NodePort是把服务的端口映射到集群每一个Node上的某一个端口上，这样访问集群中任意一个Node的请求都可以被转发到对应的服务的ClusterIP上。</p>

<p><img src="/media/cloud/k8s/Nodeip" alt="" /></p>

<p>3、LoadBalancer</p>

<p>LoadBalancer是使用云服务提供商的负载均衡器，将来自外网的访问请求转发到ClusterIP上。</p>

<p><img src="/media/cloud/k8s/loadbanance" alt="" /></p>

<p>以上三中方式都是Service的type所支持的类型。但各有各的问题，ClusterIP通常只适合debug时访问，因为apiserver的权限过大。而NodePort是直接将集群的Node暴露在外，而且每一个服务就要占用一个端口，非常不便于管理。LoadBalancer一般都是服务提供商付费使用。</p>

<blockquote>
<p>Ingress</p>
</blockquote>

<p>Ingress感觉就是一个nginx反代，依据不同的server_name将请求转发给不同的Service。只需要暴露一个入口，就可以访问到集群内很多的服务。Ingress并不是Service的一种，而是对集群内Service入口的统一管理。</p>

<p><img src="/media/cloud/k8s/ingress" alt="" /></p>

<h2 id="k8s的pod创建流程">k8s的pod创建流程</h2>

<p><img src="/media/cloud/k8s/create" alt="" /></p>

<p>具体的创建步骤包括：</p>

<p>1、客户端提交创建请求，可以通过API Server的Restful API，也可以使用kubectl命令行工具。支持的数据类型包括JSON和YAML。</p>

<p>2、API Server处理用户请求，存储Pod数据到etcd。</p>

<p>3、调度器通过API Server查看未绑定的Pod。尝试为Pod分配主机。</p>

<p>4、过滤主机 (调度预选)：调度器用一组规则过滤掉不符合要求的主机。比如Pod指定了所需要的资源量，那么可用资源比Pod需要的资源量少的主机会被过滤掉。</p>

<p>5、主机打分(调度优选)：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把容一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等。</p>

<p>6、选择主机：选择打分最高的主机，进行binding操作，结果存储到etcd中。</p>

<p>7、kubelet根据调度结果执行Pod创建操作： 绑定成功后，scheduler会调用APIServer的API在etcd中创建一个boundpod对象，描述在一个工作节点上绑定运行的所有pod信息。运行在每个工作节点上的kubelet也会定期与etcd同步boundpod信息，一旦发现应该在该工作节点上运行的boundpod对象没有更新，则调用Docker API创建并启动pod内的容器。</p>

<blockquote>
<p>kubelet创建pod</p>
</blockquote>

<p><img src="/media/cloud/k8s/create1" alt="" /></p>

<p>1、获取Pod进行准入检查</p>

<p>kubelet的事件源主要包含两个部分：静态Pod和Apiserver，我们这里只考虑普通的Pod，则会直接将Pod加入到PodManager来进行管理，并且进行准入检查</p>

<p>准入检查主要包含两个关键的控制器：驱逐管理与预选检查驱逐管理主要是根据当前的资源压力，检测对应的Pod是否容忍当前的资源压力；预选检查则是根据当前活跃的容器和当前节点的信息来检查是否满足当前Pod的基础运行环境，例如亲和性检查，同时如果当前的Pod的优先级特别高或者是静态Pod，则会尝试为其进行资源抢占，会按照QOS等级逐级来进行抢占从而满足其运行环境</p>

<p>2、创建事件管道与容器管理主线程</p>

<p>kubelet接收到一个新创建的Pod首先会为其创建一个事件管道，并且启动一个容器管理的主线程消费管道里面的事件，并且会基于最后同步时间来等待当前kubelet中最新发生的事件(从本地的podCache中获取)，如果是一个新建的Pod，则主要是通过PLEG中更新时间操作，广播的默认空状态来作为最新的状态</p>

<p>3、同步最新状态</p>

<p>当从本地的podCache中获取到最新的状态信息和从事件源获取的Pod信息后，会结合当前当前statusManager和probeManager里面的Pod里面的容器状态来更新，从而获取当前感知到的最新的Pod状态</p>

<p>4、准入控制检查</p>

<p>之前的准入检查是Pod运行的资源硬性限制的检查，而这里的准入检查则是软状态即容器运行时和版本的一些软件运行环境检查，如果这里检查失败，则会讲对应的容器状态设置为Blocked</p>

<p>5、更新容器状态</p>

<p>在通过准入检查之后，会调用statusManager来进行POd最新状态的同步，此处可能会同步给apiserver</p>

<p>6、Cgroup配置</p>

<p>在更新完成状态之后会启动一个PodCOntainerManager主要作用则是为对应的Pod根据其QOS等级来进行Cgroup配置的更新</p>

<p>7、Pod基础运行环境准备</p>

<p>接下来kubelet会为Pod的创建准备基础的环境，包括Pod数据目录的创建、镜像秘钥的获取、等待volume挂载完成等操作创建Pod的数据目录主要是创建 Pod运行所需要的Pod、插件、Volume目录，并且会通过Pod配置的镜像拉取秘钥生成秘钥信息，到此kubelet创建容器的工作就已经基本完成</p>

<p>8、container创建</p>

<p><img src="/media/cloud/k8s/create2" alt="" /></p>

<pre><code>1、计算Pod容器变更
计算容器变更主要包括：Pod的sandbox是否变更、短声明周期容器、初始化容器是否完成、业务容器是否已经完成，相应的我们会得到一个几个对应的容器列表：需要被kill掉的容器列表、需要启动的容器列表，注意如果我们的初始化容器未完成，则不会进行将要运行的业务容器加入到需要启动的容器列表，可以看到这个地方是两个阶段

2、初始化失败尝试终止
如果之前检测到之前的初始化容器失败，则会检查当前Pod的所有容器和sandbox关联的容器如果有在运行的容器，会全部进行Kill操作，并且等待操作完成

3、未知状态容器补偿
当一些Pod的容器已经运行，但是其状态仍然是Unknow的时候，在这个地方会进行统一的处理，全部kill掉，从而为接下来的重新启动做清理操作，此处和3.2只会进行一个分支，但核心的目标都是清理那些运行失败或者无法获取状态的容器

4、创建容器沙箱
在启动Pod的容器之前，首先会为其创建一个sandbox容器，当前Pod的所有容器都和Pod对应的sandbox共享同一个namespace从而共享一个namespace里面的资源，创建Sandbox比较复杂，后续会继续介绍

5、启动Pod相关容器
Pod的容器目前分为三大类：短生命周期容器、初始化容器、业务容器，启动顺序也是从左到右依次进行,如果对于的容器创建失败，则会通过backoff机制来延缓容器的创建，这里我们顺便介绍下containerRuntime启动容器的流程

    1、检查容器镜像是否拉取
    镜像的拉取首先会进行对应容器镜像的拼接，然后将之前获取的拉取的秘钥信息和镜像信息，一起交给CRI运行时来进行底层容器镜像的拉取，当然这里也会各种backoff机制，从而避免频繁拉取失败影响kubelet的性能

    2、创建容器配置
    创建容器配置主要是为了容器的运行创建对应的配置数据，主要包括：Pod的主机名、域名、挂载的volume、configMap、secret、环境变量、挂载的设备信息、要挂载的目录信息、端口映射信息、根据环境生成执行的命令、日志目录等信息

    3、调用runtimeService完成容器的创建
    调用runtimeService传递容器的配置信息，调用CRI，并且最终调用容器的创建接口完成容器的状态

    4、调用runtimeService启动容器
    通过之前创建容器返回的容器ID，来进行对应的容器的启动，并且会为容器创建对应的日志目录

    5、执行容器的回调钩子
    如果容器配置了PostStart钩子，则会在此处进行对应钩子的执行，如果钩子的类型是Exec类则会调用CNI的EXec接口完成在容器内的执行
</code></pre>

<h1 id="源码解析">源码解析</h1>

<p><a href="/posts/cloud/container/kubernetes/k8s-code/">源码解析详解</a></p>
            </div>
            
            <div style="border: 1px dashed #e0e0e0; margin-bottom: 15px; padding: 10px 10px 10px 10px; background-color: #fffeee; background-repeat: no-repeat; background-attachment: scroll; background-position: 1% 50%; -moz-background-size: auto auto; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial;">
                <div>
                    <p style="margin-top:0px;">作者：<a target="_blank" href="http://blog.fatedier.com/">kingjcy</a>
                    <br />本文出处：<a target="_blank" href="https://kingjcy.github.io/post/cloud/paas/kubernetes/k8s-principle/">https://kingjcy.github.io/post/cloud/paas/kubernetes/k8s-principle/</a>
                    <br />
                    文章版权归本人所有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接，否则保留追究法律责任的权利。 </p>
                </div>
            </div>

            <aside>
                
                <ul class="list-inline post-tags">
                    
                    <li>
                        <a href="/tags/kubernetes/">
                            <i class="fa fa-tags"></i>
                            kubernetes
                        </a>
                    </li>
                    
                </ul>

                
                
                <h4 id="real-rels">相关文章</h4>
                <ul class="post-rels" id="real-rels"><li id="li-rels"><a href="/post/cloud/paas/kubernetes/k8s-apiserver/">K8s组件系列（一）---- K8s apiserver 详解</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2016年11月24日)</span></li><li id="li-rels"><a href="/post/cloud/paas/kubernetes/k8s-controller-manager/">K8s组件系列（三）---- K8s controller manager 详解</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2016年11月24日)</span></li><li id="li-rels"><a href="/post/cloud/paas/kubernetes/k8s-scheduler/">K8s组件系列（二）---- K8s scheduler 详解</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2016年11月24日)</span></li><li id="li-rels"><a href="/post/cloud/paas/kubernetes/k8s-proxy/">K8s组件系列（五）---- K8s proxy 详解</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2016年11月24日)</span></li><li id="li-rels"><a href="/post/cloud/paas/kubernetes/k8s-kubelet/">K8s组件系列（四）---- K8s kubelet 详解</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2016年11月24日)</span></li></ul>
            </aside>
                
            
            <footer>
                <nav>
                    <ul class="pager">

                        
                        <li class="previous"><a href="/post/linux/server/iptables/"><span aria-hidden="true">&larr;</span> Prev</a></li>
                        

                        <li><a href="/post/">All Posts</a></li>

                        
                        <li class="next"><a href="/post/cloud/paas/kubernetes/k8s-apiserver/">Next <span aria-hidden="true">&rarr;</span></a></li>
                        

                    </ul>
                </nav>
            </footer>

        </article>
    </div>
    <div class="col-md-4">
        
<aside>
        <div class="toc panel panel-default hidden-xs hidden-sm affix-top" data-spy="affix" data-offset-top="125" data-offset-bottom="300">
            <div class="panel-heading">
                <h2 class="panel-title">Catalog</h2>
            </div>

            <nav id="TableOfContents">
<ul>
<li><a href="#架构">架构</a>
<ul>
<li><a href="#核心组件">核心组件</a></li>
<li><a href="#分层架构">分层架构</a></li>
</ul></li>
<li><a href="#详细说明">详细说明</a>
<ul>
<li><a href="#master">master</a></li>
<li><a href="#node组件">node组件</a></li>
</ul></li>
<li><a href="#基本概念">基本概念</a>
<ul>
<li><a href="#pod">pod</a></li>
<li><a href="#rc">RC</a></li>
<li><a href="#depolymemt">depolymemt</a></li>
<li><a href="#statefulset">StatefulSet</a></li>
<li><a href="#daemonset">DaemonSet</a></li>
<li><a href="#cronjob">CronJob</a></li>
<li><a href="#job">job</a></li>
<li><a href="#label">label</a></li>
<li><a href="#service">service</a></li>
<li><a href="#volume">Volume</a></li>
<li><a href="#pv">pv</a></li>
<li><a href="#configmap">configmap</a></li>
<li><a href="#secret">secret</a></li>
<li><a href="#namespace">namespace</a></li>
<li><a href="#resource-quotas">Resource Quotas</a></li>
<li><a href="#ingress">Ingress</a></li>
<li><a href="#dns">DNS</a></li>
</ul></li>
<li><a href="#原理">原理</a>
<ul>
<li><a href="#服务发现">服务发现</a></li>
<li><a href="#安全机制">安全机制</a></li>
<li><a href="#网路原理-todo">网路原理（todo）</a>
<ul>
<li><a href="#docker网络实现-posts-cloud-container-docker-docker-principle"><a href="/posts/cloud/container/docker/docker-principle/">docker网络实现</a></a></li>
<li><a href="#pod-network">Pod Network</a></li>
<li><a href="#service-network">Service Network</a></li>
</ul></li>
<li><a href="#k8s的pod创建流程">k8s的pod创建流程</a></li>
</ul></li>
<li><a href="#源码解析">源码解析</a></li>
</ul>
</nav>
        </div>
</aside>

    </div>
</div>

</div>
<hr>

<footer class="container copy">
    <p>&copy; 2020  kingjcy blog </p>
	<p>Powered by <a href="https://gohugo.io" target="_blank">Hugo</a></p>
</footer>

<script>hljs.initHighlightingOnLoad();</script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?ace3ec99de96c4080ead1eb8d52db3b3";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92600390-2', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>


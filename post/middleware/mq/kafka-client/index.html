<!DOCTYPE html>

<html lang="zh-cn">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="author" content="fatedier">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="description" content="Go Kafka客户端简单示例">
<meta property="og:url" content="https://kingjcy.github.io/"><meta property="og:type" content="article">
<meta property="og:title" content="消息队列系列---- Kafka Client - kingjcy blog"><meta property="og:site_name" content="kingjcy blog">

<title>
    
    消息队列系列---- Kafka Client
    
</title>

<link rel="stylesheet" href="/onlyone/onlyone.css">
<link rel="shortcut icon" href="/assets/favicon.ico">
<script src="/onlyone/onlyone.js"></script>
<link rel="alternate" type="application/rss+xml" title="RSS" href="/index.xml">
</head>
<body>


<div class="container">
    <header class="nav">
        <nav class="navbar navbar-default">
            <div class="container-fluid">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/">kingjcy blog</a>
                </div>

                <div class="collapse navbar-collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav">
                        <li><a href="/categories/技术文章/">技术文章</a></li>
                        <li><a href="/categories/读书笔记/">读书笔记</a></li>
                        <li><a href="/categories/人生感悟/">人生感悟</a></li>
                        <li><a href="/tags/">分类</a></li>
                        <li><a href="/about/">关于我</a></li>
                        <li>
                            <form method="get" style="padding: 8px" action="https://www.google.com/search" target="_blank">
                                <input type="hidden" name="sitesearch" value="kingjcy.github.io"/>
                                <input type="text" class="form-control" name="q" placeholder="Press enter to search">
                            </form>
                        </li>
                    </ul>

                </div>
            </div>
        </nav>
    </header>


<div class="row">
    <div class="col-md-8">
        <article class="post single">

            <header>
                <div class="post-date">
                    2019年08月08日 
                </div>
                <h1 class="post-title">消息队列系列---- Kafka Client</h1>
            </header>

            <div class="post-content">
                <p>Go Kafka客户端简单示例</p>

<h1 id="客户端">客户端</h1>

<p>1.sarama目前使用最多的golang的client</p>

<pre><code>go get github.com/Shopify/sarama
</code></pre>

<p>该库要求kafka版本在0.8及以上，支持kafka定义的high-level API和low-level API，但不支持常用的consumer自动rebalance和offset追踪，所以一般得结合cluster版本使用。</p>

<p>2.sarama-cluster依赖库，弥补了上面了不足</p>

<pre><code>go get github.com/bsm/sarama-cluster
</code></pre>

<p>需要kafka 0.9及以上版本</p>

<p>实例</p>

<p>生产者</p>

<p>1.同步消息模式</p>

<pre><code>import (
    &quot;github.com/Shopify/sarama&quot;
    &quot;time&quot;
    &quot;log&quot;
    &quot;fmt&quot;
    &quot;os&quot;
    &quot;os/signal&quot;
    &quot;sync&quot;
)

var Address = []string{&quot;10.130.138.164:9092&quot;,&quot;10.130.138.164:9093&quot;,&quot;10.130.138.164:9094&quot;}

func main()  {
    syncProducer(Address)
    //asyncProducer1(Address)
}

//同步消息模式
func syncProducer(address []string)  {
    config := sarama.NewConfig()
    config.Producer.Return.Successes = true
    config.Producer.Timeout = 5 * time.Second
    p, err := sarama.NewSyncProducer(address, config)
    if err != nil {
        log.Printf(&quot;sarama.NewSyncProducer err, message=%s \n&quot;, err)
        return
    }
    defer p.Close()
    topic := &quot;test&quot;
    srcValue := &quot;sync: this is a message. index=%d&quot;
    for i:=0; i&lt;10; i++ {
        value := fmt.Sprintf(srcValue, i)
        msg := &amp;sarama.ProducerMessage{
            Topic:topic,
            Value:sarama.ByteEncoder(value),
        }
        part, offset, err := p.SendMessage(msg)
        if err != nil {
            log.Printf(&quot;send message(%s) err=%s \n&quot;, value, err)
        }else {
            fmt.Fprintf(os.Stdout, value + &quot;发送成功，partition=%d, offset=%d \n&quot;, part, offset)
        }
        time.Sleep(2*time.Second)
    }
}
</code></pre>

<p>2.异步消息之Goroutines</p>

<p>//异步消费者(Goroutines)：用不同的goroutine异步读取Successes和Errors channel</p>

<pre><code>func asyncProducer1(address []string)  {
    config := sarama.NewConfig()
    config.Producer.Return.Successes = true
    //config.Producer.Partitioner = 默认为message的hash
    p, err := sarama.NewAsyncProducer(address, config)
    if err != nil {
        log.Printf(&quot;sarama.NewSyncProducer err, message=%s \n&quot;, err)
        return
    }

    //Trap SIGINT to trigger a graceful shutdown.
    signals := make(chan os.Signal, 1)
    signal.Notify(signals, os.Interrupt)

    var wg sync.WaitGroup
    var enqueued, successes, errors int
    wg.Add(2) //2 goroutine

    // 发送成功message计数
    go func() {
        defer wg.Done()
        for range p.Successes() {
            successes++
        }
    }()

    // 发送失败计数
    go func() {
        defer wg.Done()
        for err := range p.Errors() {
            log.Printf(&quot;%s 发送失败，err：%s\n&quot;, err.Msg, err.Err)
            errors++
        }
    }()

    // 循环发送信息
    asrcValue := &quot;async-goroutine: this is a message. index=%d&quot;
    var i int
    Loop:
    for {
        i++
        value := fmt.Sprintf(asrcValue, i)
        msg := &amp;sarama.ProducerMessage{
            Topic:&quot;test&quot;,
            Value:sarama.ByteEncoder(value),
        }
        select {
        case p.Input() &lt;- msg: // 发送消息
            enqueued++
            fmt.Fprintln(os.Stdout, value)
        case &lt;-signals: // 中断信号
            p.AsyncClose()
            break Loop
        }
        time.Sleep(2 * time.Second)
    }
    wg.Wait()

    fmt.Fprintf(os.Stdout, &quot;发送数=%d，发送成功数=%d，发送失败数=%d \n&quot;, enqueued, successes, errors)

}
</code></pre>

<p>3.异步消息之Select</p>

<p>//异步消费者(Select)：同一线程内，通过select同时发送消息 和 处理errors计数。</p>

<p>//该方式效率较低，如果有大量消息发送， 很容易导致success和errors的case无法执行，从而阻塞一定时间。</p>

<p>//当然可以通过设置config.Producer.Return.Successes=false;config.Producer.Return.Errors=false来解决</p>

<pre><code>func asyncProducer2(address []string)  {
    config := sarama.NewConfig()
    config.Producer.Return.Errors = true
    p, err := sarama.NewAsyncProducer(address, config)
    if err != nil {
        log.Printf(&quot;sarama.NewSyncProducer err, message=%s \n&quot;, err)
        return
    }

    //Trap SIGINT to trigger a graceful shutdown.
    signals := make(chan os.Signal, 1)
    signal.Notify(signals, os.Interrupt)

    var enqueued, successes, errors int
    asrcValue := &quot;async-select: this is a message. index=%d&quot;
    var i int
    Loop:
    for {
        i++
        value := fmt.Sprintf(asrcValue, i)
        msg := &amp;sarama.ProducerMessage{
            Topic:&quot;test&quot;,
            Value:sarama.ByteEncoder(value),
        }
        select {
        case p.Input() &lt;- msg:
            fmt.Fprintln(os.Stdout, value)
            enqueued++
        case &lt;-p.Successes():
            successes++
        case err := &lt;-p.Errors():
            log.Printf(&quot;%s 发送失败，err：%s\n&quot;, err.Msg, err.Err)
            errors++
        case &lt;-signals:
            p.AsyncClose()
            break Loop
        }
        time.Sleep(2 * time.Second)
    }

    fmt.Fprintf(os.Stdout, &quot;发送数=%d，发送失败数=%d \n&quot;, enqueued, errors)
}
</code></pre>

<h1 id="源码解析">源码解析</h1>

<h2 id="syncproducer-和asyncproducer的关系">syncProducer 和asyncProducer的关系</h2>

<p>syncProducer 是所有功能都是由asyncProducer实现的，而syncProducer 之所以可以同步发送消息，答案就在SendMessage 函数中，源码如下</p>

<pre><code> func(sp *syncProducer)SendMessage(msg *ProducerMessage) (partitionint32,offsetint64,errerror) {

   expectation :=make(chan*ProducerError,1)

   msg.expectation = expectation

   sp.producer.Input() &lt;- msg

   if err := &lt;-expectation;err != nil {    // 阻塞等待返回结果

        return-1,-1,err.Err

    }

   return msg.Partition,msg.Offset,nil

}
</code></pre>

<p>而使用asyncProducer 时，只需要 直接将信息producer.Input()&lt;-&amp;ProducerMessage{} 放入进producer.Input(), 然后异步读取返回结果 chan*ProducerError</p>

<p>消息传递过程</p>

<pre><code> // one per topic

// partitions messages, then dispatches them by partition

type topicProducer struct{

    parent *asyncProducer

    topic string

    input &lt;-chan*ProducerMessage

    breaker *breaker.Breaker

    handlers map[int32] chan&lt;- *ProducerMessage

    partitioner Partitioner

}



type brokerProducer struct{

    parent *asyncProducer

    broker *Broker

    input  &lt;-chan*ProducerMessage

    output chan&lt;- *produceSet

    responses  &lt;-chan*brokerProducerResponse

    buffer *produceSet

    timer  &lt;-chantime.Time

    timerFired bool

    closing error

    currentRetries map[string]map[int32]error

}
</code></pre>

<p>由代码可以看出topicProducer，partitionProducer，brokerProducer的parent都是asyncProducer</p>

<p>消息传递过程：</p>

<p>asyncProducer.dispatcher -&gt;topicProducer.dispath -&gt; partitionProducer.dispatch -&gt; brokerProducer -&gt;produceSet</p>

<p>其中produceSet 对消息进行聚集，若配置了压缩的参数，则会压缩一个set中的所有的msg, 即批量压缩， 然后构建一个ProduceRequest ,然后由 broker.Produce 将请求发送出去，其中 broker 结构体代表一个kafka broker 的连接</p>

<p>partitionProducer 会选择leader broker地址 ,若选择失败，则会重新选择leader broker ，然后由这个连接发送消息根据kafka版本不同，消息会放入到不同的结构体中若版本大于V0.11，set.recordsToSend.RecordBatch.addRecord(rec) 将一个rec添加进去，否则将set.recordsToSend.MsgSet.addMessage(msgToSend)
 </p>

<p>在生成一个newBrokerProducer时，broker会开启消费output, 而output就是一个存放produceSet的channel,阻塞等待刷新ProduceRequest  并将其发送出去</p>

<h1 id="如何优雅的使用-kafka-生产者">如何优雅的使用 Kafka 生产者</h1>

<h2 id="发送流程">发送流程</h2>

<p>1.初始化以及真正发送消息的 kafka-producer-network-thread IO 线程。</p>

<p>2.将消息序列化。</p>

<p>3.得到需要发送的分区。</p>

<p>4.写入内部的一个缓存区中。</p>

<p>5.初始化的 IO 线程不断的消费这个缓存来发送消息。</p>

<h2 id="分区策略">分区策略</h2>

<ol>
<li><p>指定分区</p></li>

<li><p>自定义路由</p></li>

<li><p>默认策略，通常都是使用这种策略，其实就是一种对分区的轮询</p></li>
</ol>

<p>简单的来说分为以下几步：</p>

<pre><code>获取 Topic 分区数。
将内部维护的一个线程安全计数器 +1。
与分区数取模得到分区编号。
</code></pre>

<p>其实这就是很典型的轮询算法，所以只要分区数不频繁变动这种方式也会比较均匀。</p>

<h2 id="producer-参数解析">Producer 参数解析</h2>

<ol>
<li><p>acks</p>

<p>acks 是一个影响消息吞吐量的一个关键参数。</p>

<p>主要有 [all、-1, 0, 1] 这几个选项，默认为 1。</p>

<p>由于 Kafka 不是采取的主备模式，而是采用类似于 Zookeeper 的主备模式。</p>

<p>前提是 Topic 配置副本数量 replica &gt; 1。
当 acks = all/-1 时：</p>

<p>意味着会确保所有的 follower 副本都完成数据的写入才会返回。</p>

<p>这样可以保证消息不会丢失！</p>

<p>但同时性能和吞吐量却是最低的。
当 acks = 0 时：</p>

<p>producer 不会等待副本的任何响应，这样最容易丢失消息但同时性能却是最好的！</p>

<p>当 acks = 1 时：</p>

<p>这是一种折中的方案，它会等待副本 Leader 响应，但不会等到 follower 的响应。</p>

<p>一旦 Leader 挂掉消息就会丢失。但性能和消息安全性都得到了一定的保证。</p></li>

<li><p>Compression</p>

<p>压缩数据进行发送，选择不同支持的压缩方式</p></li>

<li><p>MaxMessages</p>

<p>这个参数看名称就知道是内部缓存区的大小限制，对他适当的调大可以提高吞吐量。</p>

<p>但也不能极端，调太大会浪费内存。小了也发挥不了作用，也是一个典型的时间和空间的权衡。</p></li>

<li><p>retries</p>

<p>retries 该参数主要是来做重试使用，当发生一些网络抖动都会造成重试。</p>

<p>这个参数也就是限制重试次数。</p>

<p>但也有一些其他问题。</p>

<p>因为是重发所以消息顺序可能不会一致，这也是上文提到就算是一个分区消息也不会是完全顺序的情况。</p>

<p>还是由于网络问题，本来消息已经成功写入了但是没有成功响应给 producer，进行重试时就可能会出现消息重复。这种只能是消费者进行幂等处理。</p></li>
</ol>

<h1 id="性能">性能</h1>

<p>生产速度</p>

<p>12c32G1000M&ndash;3台</p>

<p>单个生产者线程，单副本&ndash;821557个记录/秒（78.3 MB /秒）</p>

<p>单个生产者线程，三个副本，异步方式&ndash;786980 record / sec（75.1 MB / sec）</p>

<p>单个生产者线程，3个副本，同步复制&mdash;-428823条记录/秒（40.2 MB /秒）</p>

<p>三个生产者，3个副本，异步复制&mdash;-2024032个记录/秒（193.0 MB /秒）</p>

<p>消费者集群模实现</p>

<pre><code>func main()  {
    topic := []string{&quot;test&quot;}
    var wg = &amp;sync.WaitGroup{}
    wg.Add(2)
    //广播式消费：消费者1
    go clusterConsumer(wg, Address, topic, &quot;group-1&quot;)
    //广播式消费：消费者2
    go clusterConsumer(wg, Address, topic, &quot;group-2&quot;)

    wg.Wait()
}

// 支持brokers cluster的消费者
func clusterConsumer(wg *sync.WaitGroup,brokers, topics []string, groupId string)  {
    defer wg.Done()
    config := cluster.NewConfig()
    config.Consumer.Return.Errors = true
    config.Group.Return.Notifications = true
    config.Consumer.Offsets.Initial = sarama.OffsetNewest

    // init consumer
    consumer, err := cluster.NewConsumer(brokers, groupId, topics, config)
    if err != nil {
        log.Printf(&quot;%s: sarama.NewSyncProducer err, message=%s \n&quot;, groupId, err)
        return
    }
    defer consumer.Close()

    // trap SIGINT to trigger a shutdown
    signals := make(chan os.Signal, 1)
    signal.Notify(signals, os.Interrupt)

    // consume errors
    go func() {
        for err := range consumer.Errors() {
            log.Printf(&quot;%s:Error: %s\n&quot;, groupId, err.Error())
        }
    }()

    // consume notifications
    go func() {
        for ntf := range consumer.Notifications() {
            log.Printf(&quot;%s:Rebalanced: %+v \n&quot;, groupId, ntf)
        }
    }()

    // consume messages, watch signals
    var successes int
    Loop:
    for {
        select {
        case msg, ok := &lt;-consumer.Messages():
            if ok {
                fmt.Fprintf(os.Stdout, &quot;%s:%s/%d/%d\t%s\t%s\n&quot;, groupId, msg.Topic, msg.Partition, msg.Offset, msg.Key, msg.Value)
                consumer.MarkOffset(msg, &quot;&quot;)  // mark message as processed
                successes++
            }
        case &lt;-signals:
            break Loop
        }
    }
    fmt.Fprintf(os.Stdout, &quot;%s consume %d messages \n&quot;, groupId, successes)
}
</code></pre>

<p>关于同步和异步的说明已经性能测试</p>

<p>Shopify/sarama的producer有两种运行模式：</p>

<p>同步模式</p>

<pre><code>producer把消息发给kafka之后会等待结果返回。
</code></pre>

<p>实例</p>

<pre><code>config := sarama.NewConfig()
config.Producer.Return.Successes = true
client, err := sarama.NewClient([]{&quot;localhost:9092&quot;}, config)
if err != nil {
    log.Fatalf(&quot;unable to create kafka client: %q&quot;, err)
}

producer, err := sarama.NewSyncProducerFromClient(client)
if err != nil {
    log.Fatalf(&quot;unable to create kafka producer: %q&quot;, err)
}
defer producer.Close()

text := fmt.Sprintf(&quot;message %08d&quot;, i)
partition, offset , err := producer.SendMessage(&amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)})
if err != nil {
    log.Fatalf(&quot;unable to produce message: %q&quot;, err)
}
...
</code></pre>

<p>注意同步模式下，下面配置必须置上：</p>

<pre><code>config.Producer.Return.Successes = true
</code></pre>

<p>否则运行报错：</p>

<pre><code>2018/12/25 08:08:30 unable to create kafka producer: &quot;kafka:
invalid configuration (Producer.Return.Successes must be true to be used in a SyncProducer)&quot;
</code></pre>

<p>异步模式</p>

<pre><code>producer把消息发给kafka之后不会等待结果返回。
</code></pre>

<p>异步模式，顾名思义就是produce一个message之后不等待发送完成返回；这样调用者可以继续做其他的工作。</p>

<pre><code>config := sarama.NewConfig()
// config.Producer.Return.Successes = true
client, err := sarama.NewClient([]{&quot;localhost:9092&quot;}, config)
if err != nil {
    log.Fatalf(&quot;unable to create kafka client: %q&quot;, err)
}

producer, err := sarama.NewAsyncProducerFromClient
if err != nil {
    log.Fatalf(&quot;unable to create kafka producer: %q&quot;, err)
}
defer producer.Close()

text := fmt.Sprintf(&quot;message %08d&quot;, i)
producer.Input() &lt;- &amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
// wait response
select {
        //case msg := &lt;-producer.Successes():
        //    log.Printf(&quot;Produced message successes: [%s]\n&quot;,msg.Value)
        case err := &lt;-producer.Errors():
            log.Println(&quot;Produced message failure: &quot;, err)
        default:
            log.Println(&quot;Produced message default&quot;,)
}
...
</code></pre>

<p>关于异步producer有一个地方取药注意的。</p>

<p>异步模式produce一个消息后，缺省并不会报告成功状态。</p>

<pre><code>config.Producer.Return.Successes = false
...
producer.Input() &lt;- &amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
log.Printf(&quot;Produced message: [%s]\n&quot;,text)

// wait response
select {
    case msg := &lt;-producer.Successes():
        log.Printf(&quot;Produced message successes: [%s]\n&quot;,msg.Value)
    case err := &lt;-producer.Errors():
        log.Println(&quot;Produced message failure: &quot;, err)
}
</code></pre>

<p>则这段代码会挂住，因为设置没有要求返回成功config.Producer.Return.Successes = false，那么在select等待的时候producer.Successes()不会返回，producer.Errors()也不会返回(假设没有错误发生)，就挂在这儿。当然可以加一个default分支绕过去，就不会挂住了：</p>

<pre><code>select {
    case msg := &lt;-producer.Successes():
        log.Printf(&quot;Produced message successes: [%s]\n&quot;,msg.Value)
    case err := &lt;-producer.Errors():
        log.Println(&quot;Produced message failure: &quot;, err)
    default:
        log.Println(&quot;Produced message default&quot;)
}
</code></pre>

<p>如果打开了Return.Successes配置，则上述代码段等同于同步方式</p>

<pre><code>config.Producer.Return.Successes = true
...
producer.Input() &lt;- &amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
log.Printf(&quot;Produced message: [%s]\n&quot;,text)

// wait response
select {
    case msg := &lt;-producer.Successes():
        log.Printf(&quot;Produced message successes: [%s]\n&quot;,msg.Value)
    case err := &lt;-producer.Errors():
        log.Println(&quot;Produced message failure: &quot;, err)
}
</code></pre>

<p>从log可以看到，每发送一条消息，收到一条Return.Successes，类似于：</p>

<pre><code>2018/12/25 08:51:51 Produced message: [message 00002537]
2018/12/25 08:51:51 Produced message successes: [message 00002537]
2018/12/25 08:51:51 Produced message: [message 00002538]
2018/12/25 08:51:51 Produced message successes: [message 00002538]
2018/12/25 08:51:51 Produced message: [message 00002539]
2018/12/25 08:51:51 Produced message successes: [message 00002539]
2018/12/25 08:51:51 Produced message: [message 00002540]
2018/12/25 08:51:51 Produced message successes: [message 00002540]
2018/12/25 08:51:51 Produced message: [message 00002541]
2018/12/25 08:51:51 Produced message successes: [message 00002541]
2018/12/25 08:51:51 Produced message: [message 00002542]
2018/12/25 08:51:51 Produced message successes: [message 00002542]
2018/12/25 08:51:51 Produced message: [message 00002543]
2018/12/25 08:51:51 Produced message successes: [message 00002543]
...
</code></pre>

<p>就像是同步produce一样的行为了。</p>

<p>如果打开了Return.Successes配置，而又没有producer.Successes()提取，那么Successes()这个chan消息会被写满。
    config.Producer.Return.Successes = true
    &hellip;
    log.Printf(&ldquo;Reade to Produced message: [%s]\n&rdquo;,text)
    producer.Input() &lt;- &amp;sarama.ProducerMessage{Topic: topic, Key: nil, Value: sarama.StringEncoder(text)}
    log.Printf(&ldquo;Produced message: [%s]\n&rdquo;,text)</p>

<pre><code>// wait response
select {
    //case msg := &lt;-producer.Successes():
    //    log.Printf(&quot;Produced message successes: [%s]\n&quot;,msg.Value)
    case err := &lt;-producer.Errors():
        log.Println(&quot;Produced message failure: &quot;, err)
    default:
        log.Println(&quot;Produced message default&quot;,)
}
</code></pre>

<p>写满的结果就是不能再写入了，导致后面的Return.Successes消息丢失, 而且producer也会挂住，因为共享的buffer被占满了，大量的Return.Successes没有被消耗掉。</p>

<p>运行一段时间后：</p>

<pre><code>2018/12/25 08:58:24 Reade to Produced message: [message 00000603]
2018/12/25 08:58:24 Produced message: [message 00000603]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000604]
2018/12/25 08:58:24 Produced message: [message 00000604]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000605]
2018/12/25 08:58:24 Produced message: [message 00000605]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000606]
2018/12/25 08:58:24 Produced message: [message 00000606]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000607]
2018/12/25 08:58:24 Produced message: [message 00000607]
2018/12/25 08:58:24 Produced message default
2018/12/25 08:58:24 Reade to Produced message: [message 00000608]
</code></pre>

<p>在produce第00000608个message的时候被挂住了，因为消息缓冲满了；这个缓冲的大小是可配的(可能是这个MaxRequestSize?)，但是不管大小是多少，如果没有去提取Success消息最终都会被占满的。</p>

<p>结论就是说配置config.Producer.Return.Successes = true和操作&lt;-producer.Successes()必须配套使用；配置成true，那么就要去读取Successes，如果配置成false，则不能去读取Successes。</p>
            </div>
            
            <div style="border: 1px dashed #e0e0e0; margin-bottom: 15px; padding: 10px 10px 10px 10px; background-color: #fffeee; background-repeat: no-repeat; background-attachment: scroll; background-position: 1% 50%; -moz-background-size: auto auto; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial;">
                <div>
                    <p style="margin-top:0px;">作者：<a target="_blank" href="http://blog.fatedier.com/">kingjcy</a>
                    <br />本文出处：<a target="_blank" href="https://kingjcy.github.io/post/middleware/mq/kafka-client/">https://kingjcy.github.io/post/middleware/mq/kafka-client/</a>
                    <br />
                    文章版权归本人所有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接，否则保留追究法律责任的权利。 </p>
                </div>
            </div>

            <aside>
                
                <ul class="list-inline post-tags">
                    
                    <li>
                        <a href="/tags/mq/">
                            <i class="fa fa-tags"></i>
                            mq
                        </a>
                    </li>
                    
                    <li>
                        <a href="/tags/kafka/">
                            <i class="fa fa-tags"></i>
                            kafka
                        </a>
                    </li>
                    
                    <li>
                        <a href="/tags/client/">
                            <i class="fa fa-tags"></i>
                            client
                        </a>
                    </li>
                    
                </ul>

                
                
                <h4 id="real-rels">相关文章</h4>
                <ul class="post-rels" id="real-rels"><li id="li-rels"><a href="/post/middleware/mq/emq/">Activemq</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2020年03月19日)</span></li><li id="li-rels"><a href="/post/middleware/mq/mq-compare/">Mq Compare</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2018年04月21日)</span></li><li id="li-rels"><a href="/post/middleware/mq/rocketmq/">Rocketmq</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2018年03月28日)</span></li><li id="li-rels"><a href="/post/middleware/mq/activemq/">Activemq</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2018年03月19日)</span></li><li id="li-rels"><a href="/post/middleware/mq/kafka/">消息队列系列---- Kafka</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2017年07月19日)</span></li><li id="li-rels"><a href="/post/middleware/mq/nsq-principle/">Nsq Principle</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2017年06月20日)</span></li><li id="li-rels"><a href="/post/middleware/mq/nsq/">Nsq</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2017年06月19日)</span></li><li id="li-rels"><a href="/post/golang/go-clinet/">Go Clinet</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2016年04月24日)</span></li></ul>
            </aside>
                
            
            <footer>
                <nav>
                    <ul class="pager">

                        
                        <li class="previous"><a href="/post/database/etcd-client/"><span aria-hidden="true">&larr;</span> Prev</a></li>
                        

                        <li><a href="/post/">All Posts</a></li>

                        
                        <li class="next"><a href="/post/linux/system/system/">Next <span aria-hidden="true">&rarr;</span></a></li>
                        

                    </ul>
                </nav>
            </footer>

        </article>
    </div>
    <div class="col-md-4">
        
<aside>
        <div class="toc panel panel-default hidden-xs hidden-sm affix-top" data-spy="affix" data-offset-top="125" data-offset-bottom="300">
            <div class="panel-heading">
                <h2 class="panel-title">Catalog</h2>
            </div>

            <nav id="TableOfContents">
<ul>
<li><a href="#客户端">客户端</a></li>
<li><a href="#源码解析">源码解析</a>
<ul>
<li><a href="#syncproducer-和asyncproducer的关系">syncProducer 和asyncProducer的关系</a></li>
</ul></li>
<li><a href="#如何优雅的使用-kafka-生产者">如何优雅的使用 Kafka 生产者</a>
<ul>
<li><a href="#发送流程">发送流程</a></li>
<li><a href="#分区策略">分区策略</a></li>
<li><a href="#producer-参数解析">Producer 参数解析</a></li>
</ul></li>
<li><a href="#性能">性能</a></li>
</ul>
</nav>
        </div>
</aside>

    </div>
</div>

</div>
<hr>

<footer class="container copy">
    <p>&copy; 2020  kingjcy blog </p>
	<p>Powered by <a href="https://gohugo.io" target="_blank">Hugo</a></p>
</footer>

<script>hljs.initHighlightingOnLoad();</script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?ace3ec99de96c4080ead1eb8d52db3b3";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92600390-2', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>


<!DOCTYPE html>

<html lang="zh-cn">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="author" content="fatedier">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="description" content="现象 目前我们日志收集组件使用的是filebeat6.6.1，在某业务上线以后，发生了日志收集延迟的问题，最差的情况，延迟两天以上。严重影响了下游数据分析项目。
分析该业务日志之后，发现该业务日志量大，但是单日志filed非常少。
分析  看日志没有什么问题
 查看pprof的信息
  使用命令行
go tool pprof http://0.0.0.0:6060/debug/pprof/profile Showing top 10 nodes out of 197 flat flat% sum% cum cum% 21.45s 13.42% 13.42% 70.09s 43.85% runtime.gcDrain 15.49s 9.69% 23.11% 39.83s 24.92% runtime.scanobject 11.38s 7.12% 30.23% 11.38s 7.12% runtime.futex 7.86s 4.92% 35.15% 16.30s 10.20% runtime.greyobject 7.82s 4.89% 40.04% 7.82s 4.89% runtime.markBits.isMarked (inline) 5.59s 3.50% 43.53% 5.59s 3.50% runtime.(*lfstack).pop 5.51s 3.45% 46.98% 6.05s 3.78% runtime.">
<meta property="og:url" content="https://kingjcy.github.io/"><meta property="og:type" content="article">
<meta property="og:title" content="生产问题排查解决系列---- filebeat resource optimization - kingjcy blog"><meta property="og:site_name" content="kingjcy blog">

<title>
    
    生产问题排查解决系列---- filebeat resource optimization
    
</title>

<link rel="stylesheet" href="/onlyone/onlyone.css">
<link rel="shortcut icon" href="/assets/favicon.ico">
<script src="/onlyone/onlyone.js"></script>
<link rel="alternate" type="application/rss+xml" title="RSS" href="/index.xml">
</head>
<body>


<div class="container">
    <header class="nav">
        <nav class="navbar navbar-default">
            <div class="container-fluid">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/">kingjcy blog</a>
                </div>

                <div class="collapse navbar-collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav">
                        <li><a href="/categories/技术文章/">技术文章</a></li>
                        <li><a href="/categories/读书笔记/">读书笔记</a></li>
                        <li><a href="/categories/人生感悟/">人生感悟</a></li>
                        <li><a href="/tags/">分类</a></li>
                        <li><a href="/about/">关于我</a></li>
                        <li>
                            <form method="get" style="padding: 8px" action="https://www.google.com/search" target="_blank">
                                <input type="hidden" name="sitesearch" value="kingjcy.github.io"/>
                                <input type="text" class="form-control" name="q" placeholder="Press enter to search">
                            </form>
                        </li>
                    </ul>

                </div>
            </div>
        </nav>
    </header>


<div class="row">
    <div class="col-md-8">
        <article class="post single">

            <header>
                <div class="post-date">
                    2020年03月02日 
                </div>
                <h1 class="post-title">生产问题排查解决系列---- filebeat resource optimization</h1>
            </header>

            <div class="post-content">
                

<h1 id="现象">现象</h1>

<p>目前我们日志收集组件使用的是filebeat6.6.1，在某业务上线以后，发生了日志收集延迟的问题，最差的情况，延迟两天以上。严重影响了下游数据分析项目。</p>

<p>分析该业务日志之后，发现该业务日志量大，但是单日志filed非常少。</p>

<h1 id="分析">分析</h1>

<ol>
<li><p>看日志没有什么问题</p></li>

<li><p>查看pprof的信息</p></li>
</ol>

<p>使用命令行</p>

<pre><code>go tool pprof http://0.0.0.0:6060/debug/pprof/profile

Showing top 10 nodes out of 197
  flat  flat%   sum%        cum   cum%
21.45s 13.42% 13.42%     70.09s 43.85%  runtime.gcDrain
15.49s  9.69% 23.11%     39.83s 24.92%  runtime.scanobject
11.38s  7.12% 30.23%     11.38s  7.12%  runtime.futex
 7.86s  4.92% 35.15%     16.30s 10.20%  runtime.greyobject
 7.82s  4.89% 40.04%      7.82s  4.89%  runtime.markBits.isMarked (inline)
 5.59s  3.50% 43.53%      5.59s  3.50%  runtime.(*lfstack).pop
 5.51s  3.45% 46.98%      6.05s  3.78%  runtime.heapBitsForObject
 5.26s  3.29% 50.27%     13.92s  8.71%  runtime.sweepone
 4.04s  2.53% 52.80%      4.04s  2.53%  runtime.memclrNoHeapPointers
 3.37s  2.11% 54.91%      4.40s  2.75%  runtime.runqgrab
</code></pre>

<p>发现太多的cpu时间浪费在GC上，基本上可以肯定filebeat在小日志场景下，创建了大量的对象。所以需要不停的gc，所以需要查找那边调用创建这么大量的对象。第一个想到的就是发送多kafka</p>

<p>可以通过调用关系图可以看出根源在sarama 库，filebeat 通过sarama 来将message 写到kafka中。主要是encode方法（flate NewWriter）。我们都知道该方法是用来压缩的，我们的filebeat 默认是采用了gzip压缩。</p>

<ol>
<li>代码</li>
</ol>

<p>encode代码</p>

<pre><code>func (m *Message) encode(pe packetEncoder) error {
    pe.push(newCRC32Field(crcIEEE))

    pe.putInt8(m.Version)

    attributes := int8(m.Codec) &amp; compressionCodecMask
    pe.putInt8(attributes)

    if m.Version &gt;= 1 {
        if err := (Timestamp{&amp;m.Timestamp}).encode(pe); err != nil {
            return err
        }
    }

    err := pe.putBytes(m.Key)
    if err != nil {
        return err
    }

    var payload []byte

    if m.compressedCache != nil {
        payload = m.compressedCache
        m.compressedCache = nil
    } else if m.Value != nil {
        switch m.Codec {
        case CompressionNone:
            payload = m.Value
        case CompressionGZIP:
            var buf bytes.Buffer
            var writer *gzip.Writer
            if m.CompressionLevel != CompressionLevelDefault {
                writer, err = gzip.NewWriterLevel(&amp;buf, m.CompressionLevel)
                if err != nil {
                    return err
                }
            } else {
                writer = gzip.NewWriter(&amp;buf)
            }
            if _, err = writer.Write(m.Value); err != nil {
                return err
            }
            if err = writer.Close(); err != nil {
                return err
            }
            m.compressedCache = buf.Bytes()
            payload = m.compressedCache
        case CompressionSnappy:
            tmp := snappy.Encode(m.Value)
            m.compressedCache = tmp
            payload = m.compressedCache
        case CompressionLZ4:
            var buf bytes.Buffer
            writer := lz4.NewWriter(&amp;buf)
            if _, err = writer.Write(m.Value); err != nil {
                return err
            }
            if err = writer.Close(); err != nil {
                return err
            }
            m.compressedCache = buf.Bytes()
            payload = m.compressedCache

        default:
            return PacketEncodingError{fmt.Sprintf(&quot;unsupported compression codec (%d)&quot;, m.Codec)}
        }
        // Keep in mind the compressed payload size for metric gathering
        m.compressedSize = len(payload)
    }

    if err = pe.putBytes(payload); err != nil {
        return err
    }

    return pe.pop()
}
</code></pre>

<p>通过代码可以看出，gzip压缩的时候，使用了gzip.NewWriter方法。此时已经很明显了。</p>

<p>由于大量的小日志，在写到kafka之前，都在大量的gzip压缩，造成了大量的CPU时间浪费在了GC上。</p>

<h1 id="解决">解决</h1>

<p>此时对go熟悉的人都会想起使用sync.pool 复用对象，避免频繁GC。</p>

<p>sarama官方最新的代码：</p>

<pre><code>import (
    &quot;bytes&quot;
    &quot;compress/gzip&quot;
    &quot;fmt&quot;
    &quot;sync&quot;

    &quot;github.com/eapache/go-xerial-snappy&quot;
    &quot;github.com/pierrec/lz4&quot;
)

var (
    lz4WriterPool = sync.Pool{
        New: func() interface{} {
            return lz4.NewWriter(nil)
        },
    }

    gzipWriterPool = sync.Pool{
        New: func() interface{} {
            return gzip.NewWriter(nil)
        },
    }
)

func compress(cc CompressionCodec, level int, data []byte) ([]byte, error) {
    switch cc {
    case CompressionNone:
        return data, nil
    case CompressionGZIP:
        var (
            err    error
            buf    bytes.Buffer
            writer *gzip.Writer
        )
        if level != CompressionLevelDefault {
            writer, err = gzip.NewWriterLevel(&amp;buf, level)
            if err != nil {
                return nil, err
            }
        } else {
            writer = gzipWriterPool.Get().(*gzip.Writer)
            defer gzipWriterPool.Put(writer)
            writer.Reset(&amp;buf)
        }
        if _, err := writer.Write(data); err != nil {
            return nil, err
        }
        if err := writer.Close(); err != nil {
            return nil, err
        }
        return buf.Bytes(), nil
    case CompressionSnappy:
        return snappy.Encode(data), nil
    case CompressionLZ4:
        writer := lz4WriterPool.Get().(*lz4.Writer)
        defer lz4WriterPool.Put(writer)

        var buf bytes.Buffer
        writer.Reset(&amp;buf)

        if _, err := writer.Write(data); err != nil {
            return nil, err
        }
        if err := writer.Close(); err != nil {
            return nil, err
        }
        return buf.Bytes(), nil
    case CompressionZSTD:
        return zstdCompress(nil, data)
    default:
        return nil, PacketEncodingError{fmt.Sprintf(&quot;unsupported compression codec (%d)&quot;, cc)}
    }
}
</code></pre>

<p>通过最新的代码可以看出，官方只是在不启用gzip压缩的时候(compressionlevel=-1000)，会复用对象池。</p>

<p>这并不能满足我们的需求。</p>

<p>所以更改以后的代码如下：</p>

<pre><code>package sarama

import (

&quot;bytes&quot;

&quot;compress/gzip&quot;

&quot;fmt&quot;

&quot;sync&quot;

snappy &quot;github.com/eapache/go-xerial-snappy&quot;

&quot;github.com/pierrec/lz4&quot;

)

var (

lz4WriterPool = sync.Pool{

New: func() interface{} {

return lz4.NewWriter(nil)

},

}

gzipWriterPool = sync.Pool{

New: func() interface{} {

return gzip.NewWriter(nil)

},

}

gzipWriterPoolForCompressionLevel1 = sync.Pool{

New: func() interface{} {

gz, err := gzip.NewWriterLevel(nil, 1)

if err != nil {

panic(err)

}

return gz

},

}

gzipWriterPoolForCompressionLevel2 = sync.Pool{

New: func() interface{} {

gz, err := gzip.NewWriterLevel(nil, 2)

if err != nil {

panic(err)

}

return gz

},

}

gzipWriterPoolForCompressionLevel3 = sync.Pool{

New: func() interface{} {

gz, err := gzip.NewWriterLevel(nil, 3)

if err != nil {

panic(err)

}

return gz

},

}

gzipWriterPoolForCompressionLevel4 = sync.Pool{

New: func() interface{} {

gz, err := gzip.NewWriterLevel(nil, 4)

if err != nil {

panic(err)

}

return gz

},

}

gzipWriterPoolForCompressionLevel5 = sync.Pool{

New: func() interface{} {

gz, err := gzip.NewWriterLevel(nil, 5)

if err != nil {

panic(err)

}

return gz

},

}

gzipWriterPoolForCompressionLevel6 = sync.Pool{

New: func() interface{} {

gz, err := gzip.NewWriterLevel(nil, 6)

if err != nil {

panic(err)

}

return gz

},

}

gzipWriterPoolForCompressionLevel7 = sync.Pool{

New: func() interface{} {

gz, err := gzip.NewWriterLevel(nil, 7)

if err != nil {

panic(err)

}

return gz

},

}

gzipWriterPoolForCompressionLevel8 = sync.Pool{

New: func() interface{} {

gz, err := gzip.NewWriterLevel(nil, 8)

if err != nil {

panic(err)

}

return gz

},

}

gzipWriterPoolForCompressionLevel9 = sync.Pool{

New: func() interface{} {

gz, err := gzip.NewWriterLevel(nil, 9)

if err != nil {

panic(err)

}

return gz

},

}

)

func compress(cc CompressionCodec, level int, data \[\]byte) (\[\]byte, error) {

switch cc {

case CompressionNone:

return data, nil

case CompressionGZIP:

var (

err error

buf bytes.Buffer

writer \*gzip.Writer

)

switch level {

case CompressionLevelDefault:

writer = gzipWriterPool.Get().(\*gzip.Writer)

defer gzipWriterPool.Put(writer)

writer.Reset(&amp;buf)

case 1:

writer = gzipWriterPoolForCompressionLevel1.Get().(\*gzip.Writer)

defer gzipWriterPoolForCompressionLevel1.Put(writer)

writer.Reset(&amp;buf)

case 2:

writer = gzipWriterPoolForCompressionLevel2.Get().(\*gzip.Writer)

defer gzipWriterPoolForCompressionLevel2.Put(writer)

writer.Reset(&amp;buf)

case 3:

writer = gzipWriterPoolForCompressionLevel3.Get().(\*gzip.Writer)

defer gzipWriterPoolForCompressionLevel3.Put(writer)

writer.Reset(&amp;buf)

case 4:

writer = gzipWriterPoolForCompressionLevel4.Get().(\*gzip.Writer)

defer gzipWriterPoolForCompressionLevel4.Put(writer)

writer.Reset(&amp;buf)

case 5:

writer = gzipWriterPoolForCompressionLevel5.Get().(\*gzip.Writer)

defer gzipWriterPoolForCompressionLevel5.Put(writer)

writer.Reset(&amp;buf)

case 6:

writer = gzipWriterPoolForCompressionLevel6.Get().(\*gzip.Writer)

defer gzipWriterPoolForCompressionLevel6.Put(writer)

writer.Reset(&amp;buf)

case 7:

writer = gzipWriterPoolForCompressionLevel7.Get().(\*gzip.Writer)

defer gzipWriterPoolForCompressionLevel7.Put(writer)

writer.Reset(&amp;buf)

case 8:

writer = gzipWriterPoolForCompressionLevel8.Get().(\*gzip.Writer)

defer gzipWriterPoolForCompressionLevel8.Put(writer)

writer.Reset(&amp;buf)

case 9:

writer = gzipWriterPoolForCompressionLevel9.Get().(\*gzip.Writer)

defer gzipWriterPoolForCompressionLevel9.Put(writer)

writer.Reset(&amp;buf)

default:

writer, err = gzip.NewWriterLevel(&amp;buf, level)

if err != nil {

return nil, err

}

}

if \_, err := writer.Write(data); err != nil {

return nil, err

}

if err := writer.Close(); err != nil {

return nil, err

}

return buf.Bytes(), nil

case CompressionSnappy:

return snappy.Encode(data), nil

case CompressionLZ4:

writer := lz4WriterPool.Get().(\*lz4.Writer)

defer lz4WriterPool.Put(writer)

var buf bytes.Buffer

writer.Reset(&amp;buf)

if \_, err := writer.Write(data); err != nil {

return nil, err

}

if err := writer.Close(); err != nil {

return nil, err

}

return buf.Bytes(), nil

case CompressionZSTD:

return zstdCompress(nil, data)

default:

return nil, PacketEncodingError{fmt.Sprintf(&quot;unsupported compression codec (%d)&quot;, cc)}

}

}
</code></pre>

<h1 id="总结">总结</h1>

<p>实际结果cpu使用降低了一半，采集速度却提高了20%</p>

            </div>
            
            <div style="border: 1px dashed #e0e0e0; margin-bottom: 15px; padding: 10px 10px 10px 10px; background-color: #fffeee; background-repeat: no-repeat; background-attachment: scroll; background-position: 1% 50%; -moz-background-size: auto auto; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial;">
                <div>
                    <p style="margin-top:0px;">作者：<a target="_blank" href="http://blog.fatedier.com/">kingjcy</a>
                    <br />本文出处：<a target="_blank" href="https://kingjcy.github.io/post/product/filebeat_optimization/">https://kingjcy.github.io/post/product/filebeat_optimization/</a>
                    <br />
                    文章版权归本人所有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接，否则保留追究法律责任的权利。 </p>
                </div>
            </div>

            <aside>
                
                <ul class="list-inline post-tags">
                    
                    <li>
                        <a href="/tags/filebeat/">
                            <i class="fa fa-tags"></i>
                            filebeat
                        </a>
                    </li>
                    
                    <li>
                        <a href="/tags/leak/">
                            <i class="fa fa-tags"></i>
                            leak
                        </a>
                    </li>
                    
                    <li>
                        <a href="/tags/product-problem/">
                            <i class="fa fa-tags"></i>
                            product problem
                        </a>
                    </li>
                    
                </ul>

                
                
                <h4 id="real-rels">相关文章</h4>
                <ul class="post-rels" id="real-rels"><li id="li-rels"><a href="/post/product/filebeat_leak/">生产问题排查解决系列---- filebeat resource leak</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2020年03月02日)</span></li><li id="li-rels"><a href="/post/log/collect/filebeat/filebeat/">日志采集系列---- Filebeat</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2018年07月08日)</span></li><li id="li-rels"><a href="/post/log/collect/filebeat/filebeat-principle/">日志采集系列---- Filebeat原理</a>&nbsp;&nbsp;<span class="post-date" style="font-size:14px">&nbsp;(2018年07月08日)</span></li></ul>
            </aside>
                
            
            <footer>
                <nav>
                    <ul class="pager">

                        
                        <li class="previous"><a href="/post/product/filebeat_leak/"><span aria-hidden="true">&larr;</span> Prev</a></li>
                        

                        <li><a href="/post/">All Posts</a></li>

                        
                        <li class="next"><a href="/post/monitor/prometheus/exporter/log/grok_exporter/">Next <span aria-hidden="true">&rarr;</span></a></li>
                        

                    </ul>
                </nav>
            </footer>

        </article>
    </div>
    <div class="col-md-4">
        
<aside>
        <div class="toc panel panel-default hidden-xs hidden-sm affix-top" data-spy="affix" data-offset-top="125" data-offset-bottom="300">
            <div class="panel-heading">
                <h2 class="panel-title">Catalog</h2>
            </div>

            <nav id="TableOfContents">
<ul>
<li><a href="#现象">现象</a></li>
<li><a href="#分析">分析</a></li>
<li><a href="#解决">解决</a></li>
<li><a href="#总结">总结</a></li>
</ul>
</nav>
        </div>
</aside>

    </div>
</div>

</div>
<hr>

<footer class="container copy">
    <p>&copy; 2020  kingjcy blog </p>
	<p>Powered by <a href="https://gohugo.io" target="_blank">Hugo</a></p>
</footer>

<script>hljs.initHighlightingOnLoad();</script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?ace3ec99de96c4080ead1eb8d52db3b3";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92600390-2', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>


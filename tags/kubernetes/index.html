<!DOCTYPE html>

<html lang="zh-cn">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="author" content="fatedier">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="description" content="Kubernetes">
<meta property="og:url" content="https://kingjcy.github.io/"><meta property="og:title" content="Kubernetes - kingjcy blog"><meta property="og:site_name" content="kingjcy blog">

<title>
    
    Kubernetes - kingjcy blog
    
</title>

<link rel="stylesheet" href="/onlyone/onlyone.css">
<link rel="shortcut icon" href="/assets/favicon.ico">
<script src="/onlyone/onlyone.js"></script>
<link rel="alternate" type="application/rss+xml" title="RSS" href="/index.xml">
</head>
<body>


<div class="container">
    <header class="nav">
        <nav class="navbar navbar-default">
            <div class="container-fluid">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/">kingjcy blog</a>
                </div>

                <div class="collapse navbar-collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav">
                        <li><a href="/categories/技术文章/">技术文章</a></li>
                        <li><a href="/categories/读书笔记/">读书笔记</a></li>
                        <li><a href="/categories/人生感悟/">人生感悟</a></li>
                        <li><a href="/tags/">分类</a></li>
                        <li><a href="/about/">关于我</a></li>
                        <li>
                            <form method="get" style="padding: 8px" action="https://www.google.com/search" target="_blank">
                                <input type="hidden" name="sitesearch" value="kingjcy.github.io"/>
                                <input type="text" class="form-control" name="q" placeholder="Press enter to search">
                            </form>
                        </li>
                    </ul>

                </div>
            </div>
        </nav>
    </header>


<div class="posts">
    
    
<article class="post li">
    <header>
        <div class="post-date">
            2016年11月24日
        </div>
        <h2 class="post-title">
            <a href="/post/cloud/paas/kubernetes/k8s-principle/">K8s系列---- K8s Principle</a>
        </h2>
    </header>
    <div class="post-content">
        <p>kubernetes是一种以容器为核心的，自动化部署应用程序的分布式的容器管理系统。</p>

<p>直白的说，Kubernetes用户可以通过编写一个yaml或者json格式的配置文件，也可以通过工具/代码生成或直接请求kubernetes API创建应用，该配置文件中包含了用户想要应用程序保持的状态，不论整个kubernetes集群中的个别主机发生什么问题，都不会影响应用程序的状态，你还可以通过改变该配置文件或请求kubernetes API来改变应用程序的状态。</p>
    </div>
    </article>
<hr>

    
    
<article class="post li">
    <header>
        <div class="post-date">
            2016年11月24日
        </div>
        <h2 class="post-title">
            <a href="/post/cloud/paas/kubernetes/k8s-apiserver/">K8s组件系列（一）---- K8s apiserver 详解</a>
        </h2>
    </header>
    <div class="post-content">
        <p>apiserver是集群的核心，kubernetes API server的核心功能是提供了kubernetes各类资源对象（pod、RC 、service等）的增、删、改、查以及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。</p>
    </div>
    </article>
<hr>

    
    
<article class="post li">
    <header>
        <div class="post-date">
            2016年11月24日
        </div>
        <h2 class="post-title">
            <a href="/post/cloud/paas/kubernetes/k8s-controller-manager/">K8s组件系列（三）---- K8s controller manager 详解</a>
        </h2>
    </header>
    <div class="post-content">
        kube-controller-manager  controller-manager是管理器的控制者。使用是集群管理控制中心。内部对应控制器如下
他们通过API Server提供的接口实时监控整个集群里的每一个资源对象的当前状态，当发生各种故障导致系统状态发生变化，这些controller会尝试将系统从“现有装态”修正到“期望状态”。
 replication controller
 replication controller副本控制 ,副本控制器的核心作用是确保任何使用集群中的一个RC所关联的Pod副本数量保持预设的值。当然他是通过rc机制实现的。
RC中的Pod模板就像一个模具，模具制作出来的东西一旦离开模具，二者将毫无关系，一旦pod创建，无论模板如何变化都不会影响到已经创建的pod，并且删除一个RC 不会影响它所创建出来的Pod，当然如果想在RC控制下，删除所有的Pod，需要将RC中设置的pod的副本数该为0，这样才会自动删除所有的Pod。
1. 重新调度 就是上面说的能确保规定数量的pod运行 2. 弹性伸缩 可以通过spec.replicas来改变pod的数量 3. 滚动更新 新建一个rc为一个副本，然后减少原来的副本数量一个，一个增加，一个减少，直到原始的为0   node controller节点管理。
 首先我们需要了解kubelet通过apiserver向etcd中存储的节点信息（有节点健康状况，节点资源，节点名称地址，操作系统版本，docker版本，kubelet版本等等），其中一个节点健康状况分为三种True，false，unknown三种状态，也是最直接的节点状态
然后这个控制器就会重etcd中逐个节点读取这些状态，将来自kubelet状态来改变node controller中nodestatusmap中状态，对于状态不对的node节点加入一个队列，等待确认node是否有问题，有问题就进行信息同步，并且删除节点。
具体步骤
 - 如果controller manager在启动时设置了--cluster-cidr，那么为每一个没有设置spec.PodCIDR的节点生成一个CIDR地址，并用该地址设置节点的spec.PodCIDR属性。 - 逐个读取节点信息，此时node controller中有一个nodestatusMap，里面存储了信息，与新发送过来的节点信息做比较，并更新nodestatusMap中的节点信息。Kubelet发送过来的节点信息，有三种情况：未发送、发送但节点信息未变化、发送并且节点信息变化。此时node controller根据发送的节点信息，更新nodestatusMap，如果判断出在某段时间内没有接受到某个节点的信息，则设置节点状态为“未知”。 - 最后，将未就绪状态的节点加入到待删除队列中，待删除后，通过API Server将etcd中该节点的信息删除。如果节点为就绪状态，那么就向etcd中同步该节点信息。   resourcequota controller资源配额
 这一个功能十分必要，它确保任何对象任何时候都不会超量占用资源，确保来系统的稳定性。目前k8s支持三个层次的资源配额
1. 容器级别 可以限制cpu和memory 2. pod级别 对pod内所有容器的可用资源进行限制 3. namespace级别 pod数量，rc数量 service数量，rq数量，secret数量，persistent volume数量  实现机制：准入机制（admission caotrol），admission control当前提供了两种方式的配额约束，分别是limitRanger和resourceQuota。其中limitRanger作用于pod和容器上。ResourceQuota作用于namespace上，用于限定一个namespace里的各类资源的使用总额。
在etcd中会维护一个资源配额记录，每次用户通过apiserver进行请求时，这个控制器会先进行计算，如果资源不过就会拒绝请求。
从上图中，我们可以看出，大概有三条路线，resourceQuota controller在这三条路线中都起着重要的作用：
如果用户在定义pod时同时声明了limitranger，则用户通过API Server请求创建或者修改资源对象，这是admission control会计算当前配额的使用情况，不符合约束的则创建失败。（一、三） 对于定义了resource Quota的namespace，resourceQuota controller会定期统计和生成该namespace下的各类对象资源使用总量，统计结果包括：pod、service、RC、secret和PV等对象的实例个数，以及该namespace下所有的container实例所使用的资源量（CPU，memory），然后会将这些结果写入到etcd中，写入的内容为资源对象名称、配额制、使用值，然后admission control会根据统计结果判断是否超额，以确保相关namespace下的资源配置总量不会超过resource Quota的限定值。（二、三）   namespace controller
    </div>
    </article>
<hr>

    
    
<article class="post li">
    <header>
        <div class="post-date">
            2016年11月24日
        </div>
        <h2 class="post-title">
            <a href="/post/cloud/paas/kubernetes/k8s-scheduler/">K8s组件系列（二）---- K8s scheduler 详解</a>
        </h2>
    </header>
    <div class="post-content">
        kube-scheduler  scheduler负责资源的调度，按照预定的调度策略将待调度的Pod调度到相应的机器node上；并将信息写入到etcd中去。
 scheduler的工作流程
 核心：待调度的pod列表、可有的合适的node列表、调度算法和策略
1、遍历所有目标node，筛选出符合要求的候选节点。为此，kubernetes内置了多种预选策略
2、确定优先节点，在第1步的基础上，采用优选策略，计算出每一个节点候选的积分，积分最高者胜出
3、最后通过API Server将待调度的Pod，通知给最优node上的kubelet，将其创建并运行
 scheduler中可用的预选策略有很多
 1、NoDiskconflict：磁盘冲突 判断备选pod的gcePersistentDisk或者AWSElasticBlockStore和备选的节点中已存在的pod是否存在冲突具体检测过程如下：
 首先，读取备选pod的所有的volume信息，对每一个volume执行一下步骤的冲突检测 如果该volume是gcePersistentDisk，则将volume和备选节点上的所有pod的每个volume进行比较，如果发现相同的gcePersistentDisk，则返回false，表明磁盘冲突，检测结束，反馈给调度器该备选节点不合适作为备选的pod，如果volume是AWSElasticBlockStore，则将volume和备选节点上的所有pod的每个volume进行比较，如果发现相同的AWSElasticBlockStore，则返回false，表明磁盘冲突，检测结束，反馈给调度器该备选节点不合适作为备选的pod 最终，检查备选pod的所有的volume均为发现冲突，则返回true，表明不存在磁盘冲突，反馈给调度器该备选节点合适备选pod  2、podFistResources：资源要求 判断备选节点资源是否满足备选pod的需求，检测过程如下： - 计算备选pod和节点中已存在的pod的所有容器的需求资源（CPU 和内存）的总和 - 获得备选节点的状态信息，其中包括节点的资源信息 - 如果备选pod和节点中已存在pod的所有容器的需求资源（CPU和内存）的总和超出了备选节点拥有的资源，则返回false，表明备选节点不适合备选pod，否则返回true,表明备选节点适合备选pod
3、PodSelectorMatches：标签匹配 判断备选节点是否包含备选pod的标签选择器指定的标签： - 如果pod没有指定spec.nodeSelector标签选择器，则返回true - 如果获得备选节点的标签信息，判断节点是否包含备选pod的标签选择器所指的标签，如果包含返回true，不包含返回false
4、PodFitsHost：
判断备选pod的spec.nodeName域所指定的节点名称和备选节点的名称是否一致，如果一致返回true，否则返回false。
5、PodFitsPorts 判断备选pod所用的端口列表汇中的端口是否在备选节点中被占用，如果被占用，则返回false，否则返回true。
 Scheduler中的优选策略
 1、leastRequestedPriority 该策略用于从备选节点列表中选出资源消耗最小的节点： - 计算出所有备选节点上运行的pod和备选pod的CPU占用量 - 计算出所有备选节点上运行的pod和备选pod的memory占用量 - 根据特定的算法，计算每个节点的得分
2、CalculateNodeLabelPriority 如果用户在配置中指定了该策略，则scheduler会通过registerCustomPriorityFunction方法注册该策略。该策略用于判断策略列出的标签在备选节点中存在时，是否选择该备选节点。如果备选节点的标签在优选策略的标签列表中且优选策略的presence值为true，或者备选节点的标签不在优选策略的标签列表中且优选策略的presence值为false，则备选节点score=10，否则等于0。
3、BalancedResourceAllocation 该优选策略用于从备选节点列表中选出各项资源使用率最均衡的节点： - 计算出所有备选节点上运行的pod和备选pod的CPU占用量 - 计算出所有备选节点上运行的pod和备选pod的memory占用量 - 根据特定的算法，计算每个节点的得分
    </div>
    </article>
<hr>

    
    
<article class="post li">
    <header>
        <div class="post-date">
            2016年11月24日
        </div>
        <h2 class="post-title">
            <a href="/post/cloud/paas/kubernetes/k8s-proxy/">K8s组件系列（五）---- K8s proxy 详解</a>
        </h2>
    </header>
    <div class="post-content">
        kube-proxy  先看发展
 mode
 kube-proxy当前实现了两种proxyMode：userspace和iptables。其中userspace mode是v1.0及之前版本的默认模式，从v1.1版本中开始增加了iptables mode，在v1.2版本中正式替代userspace模式成为默认模式。但是目前比较流行的就是ipvs的模式，当时阿里使用的还是iptables。
1、userspace
userspace 模式下service的请求会先从用户空间进入内核iptables，然后再回到用户空间，由kube-proxy完成后端Endpoints的选择和代理工作，这样流量从用户空间进出内核带来的性能损耗是不可接受的。这也是k8s v1.0及之前版本中对kube-proxy质疑最大的一点，因此社区就开始研究iptables mode。
2、iptables
iptables mode因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗。另外，如果集群中存在上万的Service/Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。 这也导致，目前大部分企业用k8s上生产时，都不会直接用kube-proxy作为服务代理，而是通过自己开发或者通过Ingress Controller来集成HAProxy, Nginx来代替kube-proxy。
 核心功能
 kube-proxy负责为Service提供cluster内部的服务发现和负载均衡；service只是一个概念，而真正将service的作用落实的这是背后的kube-proxy服务进程。
其核心功能就是将到某个service的访问请求转发到后端的多个pod实例上。对每一个TCP类型的kubernetes service，kube-proxy都会在本地node上建立一个socketserver来负责接收请求，然后均匀发送到后端的某个pod的端口上，这个过程默认采用round robin负载均衡算法。另外，kubernetes也提供通过修改service的service.spec.sessionAffinity参数的值来实现会话保持特性的定向发送，如果设置的值为“clientIP”，那么则将来来自同一个clientIP的请求都转发到同一个后端的pod上。其实就是一个反向代理，类似于nginx，为每个service启动一个socket，指定虚拟ip和端口，然后连接来了进行轮训，目前也只是支持round robin算法，还提供session保持机制，就是sesssion没有过期的情况下，下次还是访问同一个后端pod。
service的clusterIP和nodePort等概念是kube-proxy服务通过Iptables的NAT转换实现的，kube-proxy在运行过程中动态创建于service相关的Iptable规则，这些规则实现了clusterIP以及nodePort的请求流量重定向到kube-proxy进程上对应的服务的代理端口的功能。由于Iptable机制针对的是本地的kube-proxy端口，所有每一个node上都要运行kube-proxy组件，这样一来，在kubernetes集群内部，我们可以在任意node上发起对service的访问。由此看来，由于kube-proxy的作用，在service的调用过程中客户端无序关心后端有几个pod，中间过程的通信，负载均衡以及故障恢复都是透明。
 实现细节
 kube-proxy通过查询和监听API Server中service与endpoint的变换，为每一个service都建立一个“服务代理对象“：kube-proxy程序内部的一种数据结构
kube-proxy内部也创建了一个负载均衡器—loadBalancer, loadBalancer上保存了service到对应的后端endpoint列表的动态路由转发表，而具体的路由选择则取决于round robin算法和service的session会话保持。
针对发生变化的service列表，kube-proxy会逐个处理，下面是具体的处理流程：
- 如果service没有设置集群IP，这不做任何处理，否则，获取该service的所有端口定义列表 - 逐个读取服务端口定义列表中的端口信息，根据端口名称、service名称和namespace判断本地是否已经存在对应的服务代理对象，如果不存在则创建，如果存在并且service端口被修改过，则先删除Iptables中和该service端口相关的规则，关闭服务代理对象，然后走新建流程并为该service创建相关的Iptables规则 - 更新负载均衡组件中对应service的转发地址列表，对于新建的service，确定转发时的会话保持策略 - 对于已删除的service则进行清理   实例
 1、创建一个service
apiVersion: v1 kind: Service metadata: labels: name: mysql role: service name: mysql-service spec: ports: - port: 3306 targetPort: 3306 nodePort: 30964 type: NodePort selector: mysql-service: &quot;true&quot;  mysql-service对应的nodePort暴露出来的端口为30964，对应的cluster IP(10.
    </div>
    </article>
<hr>

    
    
<article class="post li">
    <header>
        <div class="post-date">
            2016年11月24日
        </div>
        <h2 class="post-title">
            <a href="/post/cloud/paas/kubernetes/k8s-kubelet/">K8s组件系列（四）---- K8s kubelet 详解</a>
        </h2>
    </header>
    <div class="post-content">
        kubelet  该进程用于处理master节点下发到本节点的任务，管理Pod以及Pod中的容器。每个kubelet进程会在API Server上注册节点信息，定期向master节点汇报节点资源的使用情况，并通过cAdvisor监控容器和节点的资源。
 节点管理
 节点通过设置kubelet的启动参数“&ndash;register-node”来决定是否向API Server注册自己。如果该参数为true，那么kubelet将试着通过API Server注册自己。在自注册时，kubelet启动时还包括以下参数：
--api-servers：API Server的位置 --kubeconfing：kubeconfig文件，用于访问API Server的安全配置文件 --cloud-provider：云服务商地址，仅用于共有云环境  如果没有选择自注册模式，用户需要手动去配置node的资源信息，同时告知ndoe上的kubelet API Server的位置。Kubelet在启动时通过API Server注册节点信息，并定时向API Server发送节点新消息，API Server在接受到这些消息之后，将这些信息写入etcd中。通过kubelet的启动参数“&ndash;node-status-update-frequency”设置kubelet每个多长时间向API Server报告节点状态，默认为10s
 pod管理
 kubelet通过以下几种方式获取自身node上所要运行的pod清单：
1、文件：kubelet启动参数“&ndash;config”指定的配置文件目录下的文件（默认为“/etc/Kubernetes/manifests”）通过&ndash;file-check-frequency设置检查该文件的时间间隔，默认为20s
2、HTTP端点：通过“&ndash;manifest-url”参数设置。通过“&ndash;http-check-frequency”设置检查该HTTP端点数据的时间间隔，默认为20s。
3、API Server：kubelet通过API server监听etcd目录，同步pod列表
注意：这里static pod，不是被API Server创建的，而是被kubelet创建，之前文章中提到了静态的pod是在kubelet的配置文件中编写，并且总在kubelet所在node上运行。 Kubelet监听etcd，所有针对pod的操作将会被kubelet监听到。如果是新的绑定到本节点的pod，则按照pod清单的要求创建pod，如果是删除pod，则kubelet通过docker client去删除pod中的容器，并删除该pod。 具体的针对创建和修改pod任务，流程为：
- 为该pod创建一个目录 - 从API Server读取该pod清单 - 为该pod挂载外部volume - 下载pod用到的secret - 检查已经运行在节点中的pod,如果该pod没有容器或者Pause容器没有启动，则先停止pod里的所有容器的进程。如果pod中有需要删除的容器，则删除这些容器 - 为pod中的每个容器做如下操作 1、为容器计算一个hash值，然后用容器的名字去查询docker容器的hash值。若查找到容器，且两者得到hash不同，则停止docker中的容器的进程，并且停止与之关联pause容器的进程；若两个相同，则不做任何处理 2、如果容器被停止了，且容器没有指定restartPolicy(重启策略)，则不做任何处理 3、调用docker client 下载容器镜像，调用docker client 运行容器   容器的健康检查
 Pod通过两类探针来检查容器的健康状态。一个是livenessProbe探针，用于判断容器是否健康，告诉kubelet一个容器什么时候处于不健康状态，如果livenessProbe探针探测到容器不健康，则kubelet将删除该容器，并根据容器的重启策略做相应的处理；如果一个容器不包含livenessProbe探针，那么kubelet认为livenessProbe探针的返回值永远为“success”。另一个探针为ReadinessProbe，用于判断容器是否启动完成，且准备接受请求。如果ReadinessProbe探针检测到失败，则pod的状态将被修改，endpoint controller将从service的endpoints中删除包含该容器所在pod的IP地址的endpoint条目。
 cadvisor资源监控
 cadcisor是为容器监控而生的监控工具，目前集成在kubelet中，以4194端口进行暴露。
    </div>
    </article>
<hr>

    
</div>

</div>
<hr>

<footer class="container copy">
    <p>&copy; 2020  kingjcy blog </p>
	<p>Powered by <a href="https://gohugo.io" target="_blank">Hugo</a></p>
</footer>

<script>hljs.initHighlightingOnLoad();</script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?ace3ec99de96c4080ead1eb8d52db3b3";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92600390-2', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>

